[
  {
    "owner": "arize-ai",
    "repo": "phoenix",
    "content": "TITLE: Running Q&A Evaluation with Phoenix Evals and OpenAI GPT in Python\nDESCRIPTION: This Python snippet demonstrates the setup and execution of the Q&A evaluation using Phoenix evals and OpenAI GPT models. It imports necessary modules, initializes an OpenAI model with specified parameters, applies railings to constrain outputs to expected binary values, and classifies answers within a DataFrame against the Q&A prompt template. Key parameters include model name (\"gpt-4\"), zero temperature for determinism, the DataFrame of samples, and railings to remove extraneous output. The output is a DataFrame with classification labels and optional explanations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/q-and-a-on-retrieved-data.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport phoenix.evals.templates.default_templates as templates\nfrom phoenix.evals import (\n    OpenAIModel,\n    download_benchmark_dataset,\n    llm_classify,\n)\n\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\n\n#The rails fore the output to specific values of the template\n#It will remove text such as \",,,\" or \"...\", anything not the\n#binary value expected from the template\nrails = list(templates.QA_PROMPT_RAILS_MAP.values())\nQ_and_A_classifications = llm_classify(\n    dataframe=df_sample,\n    template=templates.QA_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    provide_explanation=True, #optional to generate explanations for the value produced by the eval LLM\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Phoenix OTEL Tracer Provider with Register in Python\nDESCRIPTION: Demonstrates how to import and use the phoenix.otel.register function to configure a global OpenTelemetry TracerProvider with Phoenix-aware defaults. The example sets a project name, enables batch span processing, and auto-instruments available OpenInference instrumentors. This setup facilitates sending span data to a Phoenix backend with minimal manual configuration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\ntracer_provider = register(\n    project_name=\"default\", # sets a project name for spans\n    batch=True, # uses a batch span processor\n    auto_instrument=True, # uses all installed OpenInference instrumentors\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application - Python\nDESCRIPTION: Initializes and launches the Arize Phoenix application. This starts a background process to collect trace data emitted by instrumented applications. The `.view()` method attempts to open the Phoenix UI in a browser.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_agent_tracing_tutorial.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n(session := px.launch_app()).view()\n```\n\n----------------------------------------\n\nTITLE: Setting up Ragas Evaluator Functions in Python\nDESCRIPTION: This Python snippet defines two asynchronous functions for evaluation: `tool_call_evaluator` and `goal_evaluator`. They use the `conversation_to_ragas_sample` helper and Ragas metrics (`ToolCallAccuracy`, `AgentGoalAccuracyWithReference`) to calculate evaluation scores based on agent input and output (messages). `goal_evaluator` requires an LLM (`gpt-4o`) to assess goal accuracy.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/ragas_agents_cookbook_phoenix.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_openai import ChatOpenAI\nfrom ragas.llms import LangchainLLMWrapper\nfrom ragas.metrics import AgentGoalAccuracyWithReference, ToolCallAccuracy\n\n\n# Setup evaluator LLM and metrics\nasync def tool_call_evaluator(input, output):\n    sample = conversation_to_ragas_sample(output[\"messages\"], reference_equation=input[\"question\"])\n    tool_call_accuracy = ToolCallAccuracy()\n    return await tool_call_accuracy.multi_turn_ascore(sample)\n\n\nasync def goal_evaluator(input, output):\n    sample = conversation_to_ragas_sample(\n        output[\"messages\"], reference_answer=output[\"final_output\"]\n    )\n    evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n    goal_accuracy = AgentGoalAccuracyWithReference(llm=evaluator_llm)\n    return await goal_accuracy.multi_turn_ascore(sample)\n\n\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Relevance of Retrieved Documents Using Phoenix Evals and OpenAI Model in Python\nDESCRIPTION: Imports Phoenix Evals classes and runs a relevance evaluation on the retrieved documents DataFrame using the RelevanceEvaluator backed by an OpenAI GPT-4 Turbo Preview model. The evaluation is run with explanations enabled and concurrency of 20 to speed up batch processing. The result is a DataFrame with relevance scores and explanations for each retrieved document, aiding debugging and evaluation analysis. It depends on the Phoenix evals module and OpenAI API credentials.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    RelevanceEvaluator,\n    run_evals,\n)\n\nrelevance_evaluator = RelevanceEvaluator(OpenAIModel(model_name=\"gpt-4-turbo-preview\"))\n\nretrieved_documents_relevance_df = run_evals(\n    evaluators=[relevance_evaluator],\n    dataframe=retrieved_documents_df,\n    provide_explanation=True,\n    concurrency=20,\n)[0]\n```\n\n----------------------------------------\n\nTITLE: Evaluating GPT-4 Turbo Classification Metrics in Python\nDESCRIPTION: Compares predicted and true labels for classification accuracy, printing detailed statistics and plotting the confusion matrix. This enables robust, apples-to-apples benchmarking of model variants and templates, enhancing iterative development and improvement cycles.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df_sample[\"relevant\"].map(RAG_RELEVANCY_PROMPT_RAILS_MAP).tolist()\n\nprint(classification_report(true_labels, relevance_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels, predict_vector=relevance_classifications, classes=rails\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Bare-Bones OpenAI Agent with Session Tracking (Python)\nDESCRIPTION: This snippet defines a simple agent that interacts with the OpenAI API. It uses OpenInference instrumentation to track the agent's interactions and associate them with a session ID. The `assistant` function takes a list of messages and a session ID, sends the messages to the OpenAI chat completions endpoint, and returns the response. It also sets attributes on the current span to record input and output values, as well as the session ID. The `using_session` context manager is used to propagate the session ID to spans created by the OpenAI instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_sessions_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n\nimport openai\nfrom openinference.instrumentation import using_session\nfrom openinference.semconv.trace import SpanAttributes\nfrom opentelemetry import trace\n\nclient = openai.Client()\nsession_id = str(uuid.uuid4())\n\ntracer = trace.get_tracer(__name__)\n\n\n@tracer.start_as_current_span(\n    name=\"agent\", attributes={SpanAttributes.OPENINFERENCE_SPAN_KIND: \"agent\"}\n)\ndef assistant(\n    messages: list[dict],\n    session_id: str = str,\n):\n    current_span = trace.get_current_span()\n    current_span.set_attribute(SpanAttributes.SESSION_ID, session_id)\n    current_span.set_attribute(SpanAttributes.INPUT_VALUE, messages[-1].get(\"content\"))\n\n    # Propagate the session_id down to spans crated by the OpenAI instrumentation\n    # This is not strictly necessary, but it helps to correlate the spans to the same session\n    with using_session(session_id):\n        response = (\n            client.chat.completions.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}] + messages,\n            )\n            .choices[0]\n            .message\n        )\n\n    current_span.set_attribute(SpanAttributes.OUTPUT_VALUE, response.content)\n    return response\n\nmessages = [{\"role\": \"user\", \"content\": \"hi! im bob\"}]\nresponse = assistant(\n    messages,\n    session_id=session_id,\n)\nmessages = messages + [response, {\"role\": \"user\", \"content\": \"what's my name?\"}]\nresponse = assistant(\n    messages,\n    session_id=session_id,\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating RAG Relevance with Phoenix and OpenAI in Python\nDESCRIPTION: Demonstrates using arize-phoenix-evals to assess RAG document relevance. The script imports necessary Phoenix modules, sets the OpenAI API key, downloads a benchmark dataset, configures an OpenAI model (GPT-4), executes the classification using `llm_classify` with predefined templates and rails, and prepares the predicted relevance labels (`y_pred`) and ground truth labels (`y_true`) for comparison using scikit-learn metrics. Requires `arize-phoenix-evals`, `openai`, `scikit-learn` packages, and a valid OpenAI API key set as an environment variable.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-evals/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom phoenix.evals import (\n    RAG_RELEVANCY_PROMPT_TEMPLATE,\n    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n    OpenAIModel,\n    download_benchmark_dataset,\n    llm_classify,\n)\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n\nos.environ[\"OPENAI_API_KEY\"] = \"<your-openai-key>\"\n\n# Download the benchmark golden dataset\ndf = download_benchmark_dataset(\n    task=\"binary-relevance-classification\", dataset_name=\"wiki_qa-train\"\n)\n# Sample and re-name the columns to match the template\ndf = df.sample(100)\ndf = df.rename(\n    columns={\n        \"query_text\": \"input\",\n        \"document_text\": \"reference\",\n    },\n)\nmodel = OpenAIModel(\n    model=\"gpt-4\",\n    temperature=0.0,\n)\n\n\nrails =list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\ndf[[\"eval_relevance\"]] = llm_classify(df, model, RAG_RELEVANCY_PROMPT_TEMPLATE, rails)\n#Golden dataset has True/False map to -> \"irrelevant\" / \"relevant\"\n#we can then scikit compare to output of template - same format\ny_true = df[\"relevant\"].map({True: \"relevant\", False: \"irrelevant\"})\ny_pred = df[\"eval_relevance\"]\n\n# Compute Per-Class Precision, Recall, F1 Score, Support\nprecision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix OpenTelemetry Tracer (Python)\nDESCRIPTION: Configures and registers the Phoenix OpenTelemetry tracer. The `auto_instrument=True` flag attempts to automatically instrument supported libraries like the OpenAI Agents SDK based on installed dependencies.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai-agents-sdk.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"agents\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Data, Building Nodes, and Creating a Vector Index (Python)\nDESCRIPTION: Loads text documents from the specified directory, initializes an OpenAI LLM instance, creates a SimpleNodeParser with a chunk size of 512, parses documents into nodes, and builds a VectorStoreIndex for similarity-based vector searches. Requires the downloaded dataset and OpenAI access; main input is the local data directory path. Outputs a vector index for subsequent querying.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndocuments = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n\n# Define an LLM\nllm = OpenAI(model=\"gpt-4\")\n\n# Build index with a chunk_size of 512\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=512)\nnodes = node_parser.get_nodes_from_documents(documents)\nvector_index = VectorStoreIndex(nodes)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Endpoint for Phoenix and Initializing PromptFlow Tracing in Python\nDESCRIPTION: This snippet sets up the OpenTelemetry OTLP endpoint environment variable to direct traces to the Phoenix collector's /v1/traces API. It uses PromptFlow's setup_exporter_from_environ function to initialize the tracing exporter leveraging the environment settings. This prepares PromptFlow flows to automatically send telemetry data to Phoenix for observability and analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/prompt-flow.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom opentelemetry.sdk.environment_variables import OTEL_EXPORTER_OTLP_ENDPOINT\nfrom promptflow.tracing._start_trace import setup_exporter_from_environ\n\nendpoint = f\"{os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"]}/v1/traces\"  # replace with your Phoenix endpoint if self-hosting\nos.environ[OTEL_EXPORTER_OTLP_ENDPOINT] = endpoint\nsetup_exporter_from_environ()\n```\n\n----------------------------------------\n\nTITLE: Running LLM Evaluations with run_evals Function in Python\nDESCRIPTION: Function to evaluate a pandas dataframe using LLM-powered evaluators to assess document relevance, hallucinations, toxicity, and other criteria. Returns a list of dataframes containing evaluation results for each evaluator.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evals-reference.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef run_evals(\n    dataframe: pd.DataFrame,\n    evaluators: List[LLMEvaluator],\n    provide_explanation: bool = False,\n    use_function_calling_if_available: bool = True,\n    verbose: bool = False,\n    concurrency: int = 20,\n) -> List[pd.DataFrame]\n```\n\n----------------------------------------\n\nTITLE: Aggregating Evaluation Metrics\nDESCRIPTION: Calculates the mean of all numeric evaluation metrics to get an overall assessment of RAG system performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n# Aggregate the scores across the retrievals\nresults = rag_evaluation_dataframe.mean(numeric_only=True)\nresults\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenAI API key with environment variables\nDESCRIPTION: Prompts the user to input their OpenAI API key securely if not already set in environment variables. Sets the API key as an environment variable for use in subsequent API calls, ensuring authenticated access to OpenAI services.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n\n```\n\n----------------------------------------\n\nTITLE: Running Basic OpenAI Agent Example (Python)\nDESCRIPTION: Demonstrates how to instantiate a simple OpenAI Agent and run a task using the `Runner`. This code, when executed with the registered tracer, will generate traces visible in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai-agents-sdk.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom agents import Agent, Runner\n\nagent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\nresult = Runner.run_sync(agent, \"Write a haiku about recursion in programming.\")\nprint(result.final_output)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Traced Express OpenAI Chat Server in Node.js (TypeScript)\nDESCRIPTION: This TypeScript snippet defines a simple Express server that provides a `/chat` endpoint. The endpoint accepts a `message` query parameter, invokes OpenAI's chat completions API, and replies with the LLM output. Before running, ensure all OpenTelemetry/OpenInference and Express dependencies are installed and the OpenAI API key is set via environment variables. The script expects Phoenix (or compatible OTLP collector) to be running to collect traces and supports Node.js v18+ with TypeScript or JS files. Input is a REST GET request to `/chat` with a message; output is the chat completion response text. Ensure instrumentation is loaded first for trace collection.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n// app.ts\nimport express from \"express\";\nimport OpenAI from \"openai\";\n\nconst PORT: number = parseInt(process.env.PORT || \"8080\");\nconst app = express();\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n});\n\napp.get(\"/chat\", async (req, res) => {\n  const message = req.query.message;\n  const chatCompletion = await openai.chat.completions.create({\n    messages: [{ role: \"user\", content: message }],\n    model: \"gpt-4o\",\n  });\n  res.send(chatCompletion.choices[0].message.content);\n});\n\napp.listen(PORT, () => {\n  console.log(`Listening for requests on http://localhost:${PORT}`);\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Phoenix AI Experimentation Framework in Python\nDESCRIPTION: This snippet shows how to launch the Phoenix application within a Jupyter notebook environment. The `px.launch_app()` call initializes the Phoenix server interface needed before running subsequent dataset and experiment operations. It requires that the Phoenix package be installed and imported.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/quickstart-datasets.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies with pip - Python\nDESCRIPTION: Installs all required packages for running the notebook, including OpenAI, Phoenix (with specific version requirements), OpenTelemetry, Dotenv, DuckDB, and OpenInference. No inputs or outputs are required; simply run the command in a notebook cell to prepare the environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q openai \"arize-phoenix>=8.8.0\" \"arize-phoenix-otel>=0.8.0\" openinference-instrumentation-openai python-dotenv duckdb \"openinference-instrumentation>=0.1.21\"\n```\n\n----------------------------------------\n\nTITLE: Evaluating Document Relevance using LLM\nDESCRIPTION: Uses Phoenix's LLM Evals to evaluate the relevance of retrieved documents to their corresponding queries, with explanations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    RelevanceEvaluator,\n    run_evals,\n)\n\nrelevance_evaluator = RelevanceEvaluator(OpenAIModel(model=\"gpt-4o\"))\n\nretrieved_documents_relevance_df = run_evals(\n    evaluators=[relevance_evaluator],\n    dataframe=retrieved_documents_df,\n    provide_explanation=True,\n    concurrency=20,\n)[0]\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI for Tracing with Phoenix\nDESCRIPTION: This Python code snippet sets up OpenInference instrumentation for OpenAI calls. It registers a tracer provider with Phoenix OTEL utilities and instruments the OpenAI client, allowing LLM calls made within the task function to be traced and linked to the experiment in the Phoenix UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/run-experiments.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register()\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Run Evals with Phoenix (Python)\nDESCRIPTION: This Python snippet demonstrates how to set up and run evaluations using Phoenix Evals.  It imports necessary modules, initializes an OpenAI model, defines evaluators for hallucination and question answering, prepares the data, and calls `run_evals`. It uses `nest_asyncio.apply()` for concurrency in notebook environments. It then renames the DataFrame columns to match the evaluators' requirements and runs the evaluation. The `provide_explanation=True` argument enables explanations for the evaluation results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/quickstart/evals.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.evals import HallucinationEvaluator, OpenAIModel, QAEvaluator, run_evals\n\nnest_asyncio.apply()  # This is needed for concurrency in notebook environments\n\n# Set your OpenAI API key\neval_model = OpenAIModel(model=\"gpt-4o\")\n\n# Define your evaluators\nhallucination_evaluator = HallucinationEvaluator(eval_model)\nqa_evaluator = QAEvaluator(eval_model)\n\n# We have to make some minor changes to our dataframe to use the column names expected by our evaluators\n# for `hallucination_evaluator` the input df needs to have columns 'output', 'input', 'context'\n# for `qa_evaluator` the input df needs to have columns 'output', 'input', 'reference'\ndf[\"context\"] = df[\"reference\"]\ndf.rename(columns={\"query\": \"input\", \"response\": \"output\"}, inplace=True)\nassert all(column in df.columns for column in [\"output\", \"input\", \"context\", \"reference\"])\n\n# Run the evaluators, each evaluator will return a dataframe with evaluation results\n# We upload the evaluation results to Phoenix in the next step\nhallucination_eval_df, qa_eval_df = run_evals(\n    dataframe=df, evaluators=[hallucination_evaluator, qa_evaluator], provide_explanation=True\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenAI API Key Environment Variable\nDESCRIPTION: Prompts the user to input their OpenAI API key securely using getpass and sets it in the environment variables if not already present. This is essential for authenticating requests to OpenAI services.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: User Input for Google Cloud Project ID and Location in Python\nDESCRIPTION: Prompts the user to input their Google Cloud project ID and location, which are necessary for making Vertex AI API calls. These values configure which project and regional endpoint the subsequent API clients should use.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_vertex_ai_tracing_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nproject_id = input(\"Enter your GCP project ID and press enter:\\n\")\nlocation = input(\"Enter your GCP location and press enter:\\n\")\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix API Key\nDESCRIPTION: Retrieves the Phoenix API key from an environment variable or prompts the user to enter it securely. The key is formatted and set in the `PHOENIX_CLIENT_HEADERS` environment variable, which is used by the Phoenix OpenTelemetry exporter to authenticate with the Phoenix backend.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/agentic_rag_tracing.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif not (phoenix_api_key := os.getenv(\"PHOENIX_API_KEY\")):\n    phoenix_api_key = getpass(\"ðŸ”‘ Enter your Phoenix API key: \")\n\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={phoenix_api_key}\"\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluation Types\nDESCRIPTION: Imports specific evaluator classes (`HallucinationEvaluator`, `QAEvaluator`, `RelevanceEvaluator`) from `phoenix.evals` and instantiates them. Each evaluator type is initialized with the previously configured language model (`eval_model`), allowing them to use the model's capabilities and pre-defined prompt templates to assess different aspects of the LLM application's performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_quickstart.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    HallucinationEvaluator,\n    QAEvaluator,\n    RelevanceEvaluator,\n)\n\nhallucination_evaluator = HallucinationEvaluator(eval_model);\nqa_correctness_evaluator = QAEvaluator(eval_model);\nrelevance_evaluator = RelevanceEvaluator(eval_model);\n```\n\n----------------------------------------\n\nTITLE: Displaying Phoenix UI URL - Python\nDESCRIPTION: Prints the web address for the Arize Phoenix user interface. This URL allows the user to access the UI in a browser to visualize and inspect the traces collected from the agent's execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_agent_tracing_tutorial.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nprint(f\"View the traces in phoenix: {session.url}\")\n```\n\n----------------------------------------\n\nTITLE: Tracing Tools with Context Managers\nDESCRIPTION: This snippet demonstrates tracing a tool execution using a context manager. It involves setting the input and output of the span, setting tool-specific attributes (name, description, parameters), and setting the status code.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry.trace import Status, StatusCode\n\nwith tracer.start_as_current_span(\n    \"tool-span\",\n    openinference_span_kind=\"tool\",\n) as span:\n    span.set_input(\"input\")\n    span.set_output(\"output\")\n    span.set_tool(\n        name=\"tool-name\",\n        description=\"tool-description\",\n        parameters={\"input\": \"input\"},\n    )\n    span.set_status(Status(StatusCode.OK))\n```\n\n----------------------------------------\n\nTITLE: Applying Tool Calling Prompt Template Replacement in Python\nDESCRIPTION: This snippet replaces the \"{tool_definitions}\" token in TOOL_CALLING_PROMPT_TEMPLATE.template with the JSON string of the tools object. This substitution prepares a complete prompt template for downstream LLM-based classification or decision-making involving tool use. The snippet requires TOOL_CALLING_PROMPT_TEMPLATE, tools, and the json library to be available in the environment. The input is dynamic template content and tool definitions; output is a text prompt ready for LLM consumption. Proper escaping for JSON values is required.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_58\n\nLANGUAGE: Python\nCODE:\n```\nTOOL_CALLING_PROMPT_TEMPLATE.template.replace(\"{tool_definitions}\", json.dumps(tools))\n```\n\n----------------------------------------\n\nTITLE: Performing LLM Relevance Classifications on Sample Data in Python\nDESCRIPTION: Uses Phoenixâ€™s llm_classify to run the LLM on a batch of input-reference pairs, classifying each as relevant or irrelevant. Applies a template, enforces allowed outputs (rails), and supports concurrent API requests for speed. Returns a list of predicted labels for further evaluation. Rails is set from RAG_RELEVANCY_PROMPT_RAILS_MAP.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# The rails is used to hold the output to specific values based on the template\n# It will remove text such as \",,,\" or \"...\"\n# Will ensure the binary value expected from the template is returned\nrails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\nrelevance_classifications = llm_classify(\n    dataframe=df_sample,\n    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    concurrency=20,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Creating an LLM-based Correctness Evaluator\nDESCRIPTION: Implements an evaluation function using an LLM classifier to determine if the agent's math solutions are correct, employing GPT-4.1 to assess responses and return a correctness score.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/openai_agents_cookbook.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nfrom phoenix.evals import OpenAIModel, llm_classify\n\n\ndef correctness_eval(input, output):\n    # Template for evaluating math problem solutions\n    MATH_EVAL_TEMPLATE = \"\"\"\n    You are evaluating whether a math problem was solved correctly.\n\n    [BEGIN DATA]\n    ************\n    [Question]: {question}\n    ************\n    [Response]: {response}\n    [END DATA]\n\n    Assess if the answer to the math problem is correct. First work out the correct answer yourself,\n    then compare with the provided response. Consider that there may be different ways to express the same answer\n    (e.g., \"43\" vs \"The answer is 43\" or \"5.0\" vs \"5\").\n\n    Your answer must be a single word, either \"correct\" or \"incorrect\"\n    \"\"\"\n\n    # Run the evaluation\n    rails = [\"correct\", \"incorrect\"]\n    eval_df = llm_classify(\n        data=pd.DataFrame([{\"question\": input[\"question\"], \"response\": output[\"final_output\"]}]),\n        template=MATH_EVAL_TEMPLATE,\n        model=OpenAIModel(model=\"gpt-4.1\"),\n        rails=rails,\n        provide_explanation=True,\n    )\n    label = eval_df[\"label\"][0]\n    score = 1 if label == \"correct\" else 0\n    return score\n```\n\n----------------------------------------\n\nTITLE: Running Relevance Evaluation with Phoenix run_evals\nDESCRIPTION: Executes the relevance evaluation using the `run_evals` function from the Phoenix library. It takes a list containing the `relevance_evaluator`, the input `retrieved_documents_df` DataFrame, requests explanations (`provide_explanation=True`), and sets concurrency to 20. The resulting DataFrame containing relevance scores and explanations is stored in `retrieved_documents_relevance_df`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nretrieved_documents_relevance_df = run_evals(\n    evaluators=[relevance_evaluator],\n    dataframe=retrieved_documents_df,\n    provide_explanation=True,\n    concurrency=20,\n)[0]\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Tools - Python\nDESCRIPTION: Defines the set of tools that the agent can utilize. In this case, it creates an `LLMMathChain` powered by the initialized `llm` and registers it as a `Tool` named \"Calculator\" with a description. This allows the agent to perform mathematical computations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_agent_tracing_tutorial.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nllm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)\n# Let's give the LLM access to math tools\ntools = [\n    Tool(\n        name=\"Calculator\",\n        func=llm_math_chain.run,\n        description=\"useful for when you need to answer questions about math\",\n    ),\n]\n```\n\n----------------------------------------\n\nTITLE: Defining Regex-based Code Evaluator for Link Detection in Python\nDESCRIPTION: This snippet defines a code evaluator that checks if an LLM output contains a link using a regular expression pattern. It leverages the MatchesRegex utility from phoenix.experiments and can be used as an evaluator input for experiment runs. The evaluator expects text output and returns a boolean indicating presence of links.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/using-evaluators.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments import run_experiment, MatchesRegex\n\n# This defines a code evaluator for links\ncontains_link = MatchesRegex(\n    pattern=r\"[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b([-a-zA-Z0-9()@:%_\\\\+.~#?&//=]*)\",\n    name=\"contains_link\"\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Relevance Classification with Explanations Using Phoenix and Python\nDESCRIPTION: This Python snippet shows how to use the Phoenix evals API to classify text relevance with explanations using OpenAI's GPT-4 model. It imports necessary components such as prompt templates, model wrappers, and utility functions. An OpenAIModel is instantiated with zero temperature for deterministic output. Rails are used to constrain model outputs to expected binary values, ensuring clean, interpretable results. The llm_classify function accepts a dataframe with input data, a prompt template, the model, rails for output constraints, and a flag to request explanations. The output is a dataframe containing both classification labels and corresponding textual explanations provided by the LLM, which aids in model debugging and result transparency.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/concepts-evals/evals-with-explanations.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n    RAG_RELEVANCY_PROMPT_TEMPLATE,\n    OpenAIModel,\n    download_benchmark_dataset,\n    llm_classify,\n)\n\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\n\n#The rails is used to hold the output to specific values based on the template\n#It will remove text such as \",,,\" or \"...\"\n#Will ensure the binary value expected from the template is returned\nrails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\nrelevance_classifications = llm_classify(\n    dataframe=df,\n    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    provide_explanation=True\n)\n#relevance_classifications is a Dataframe with columns 'label' and 'explanation'\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Running LLM-based Evaluations with Phoenix\nDESCRIPTION: Configures and runs three types of LLM-based evaluations (hallucination detection, QA correctness, and document relevance) using GPT-4 Turbo, then logs the evaluation results back to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_tracing_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\neval_model = OpenAIModel(\n    model=\"gpt-4-turbo-preview\",\n)\nhallucination_evaluator = HallucinationEvaluator(eval_model)\nqa_correctness_evaluator = QAEvaluator(eval_model)\nrelevance_evaluator = RelevanceEvaluator(eval_model)\n\nhallucination_eval_df, qa_correctness_eval_df = run_evals(\n    dataframe=queries_df,\n    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n    provide_explanation=True,\n)\nrelevance_eval_df = run_evals(\n    dataframe=retrieved_documents_df,\n    evaluators=[relevance_evaluator],\n    provide_explanation=True,\n)[0]\n\npx.Client().log_evaluations(\n    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df),\n    DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df),\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring LlamaIndex Settings and Loading Data (Python)\nDESCRIPTION: Sets global LlamaIndex configurations for the LLM and embedding model using the defined metadata, initializes a sentence transformer reranker, downloads a document, and builds a VectorStoreIndex from the document.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/run_experiments_with_llama_index.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nSettings.llm = OpenAI(model=experiment_metadata[\"llm\"])\nSettings.embed_model = OpenAIEmbedding(model=experiment_metadata[\"embed_model\"])\nreranker = SentenceTransformerRerank(model=experiment_metadata[\"reranker\"], top_n=2)\n\nessay = \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\"\nwith tempfile.NamedTemporaryFile() as tf:\n    urlretrieve(essay, tf.name)\n    documents = SimpleDirectoryReader(input_files=[tf.name]).load_data()\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Hallucination\nDESCRIPTION: This snippet evaluates potential hallucinations in the generated answers using an LLM classifier. It uses the `llm_classify` function with the hallucination prompt template and rails. The evaluation results are added to the DataFrame, and the head of the DataFrame is displayed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/span_query_language.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nhallucination_eval = llm_classify(\n    qa_df,\n    model,\n    HALLUCINATION_PROMPT_TEMPLATE,\n    list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n    provide_explanation=True,\n)\nhallucination_eval[\"score\"] = (\n    hallucination_eval.label[hallucination_eval.label.notnull()] == \"factual\"\n).astype(int)\nhallucination_eval.head()\n```\n\n----------------------------------------\n\nTITLE: Launching and Resetting Phoenix Application in Python\nDESCRIPTION: This snippet resets any existing Phoenix application instance by calling px.close_app(), then launches a new Phoenix app session with px.launch_app(). It sets up the environment to enable running queries and interacting with Phoenix either in the browser at http://localhost:6006/ or within a notebook. It requires the Phoenix Python client (px) imported and configured. No input/output data structures are involved directly.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# First things first, let's reset phoenix\npx.close_app()\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary Libraries - Python\nDESCRIPTION: This code imports various libraries, including `nest_asyncio`, `phoenix`, and modules from `phoenix.evals` and `phoenix.experiments`. `nest_asyncio` is used to allow asynchronous operations within a synchronous context, which is often needed in Jupyter notebooks. Phoenix is the main framework used for evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nimport nest_asyncio\n\nimport phoenix as px\nfrom phoenix.evals import TOOL_CALLING_PROMPT_TEMPLATE, OpenAIModel, llm_classify\nfrom phoenix.experiments import evaluate_experiment, run_experiment\nfrom phoenix.experiments.evaluators import create_evaluator\nfrom phoenix.experiments.types import Example\nfrom phoenix.trace import SpanEvaluations\nfrom phoenix.trace.dsl import SpanQuery\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Running Agent with Single Input - Python\nDESCRIPTION: This code snippet demonstrates how to run the agent with a single user query. The `start_main_span` function is called with a list containing a single message with the role 'user' and the content 'Create a line chart showing sales in 2021'. The result is then printed as Markdown.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nret = start_main_span([{\"role\": \"user\", \"content\": \"Create a line chart showing sales in 2021\"}])\nprint(Markdown(ret))\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Tracing Application\nDESCRIPTION: Initializes and launches the Phoenix application to collect trace data emitted by the instrumented LangChain application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_tracing_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n(session := px.launch_app()).view()\n```\n\n----------------------------------------\n\nTITLE: Sending Multimodal OpenAI Traces to Phoenix (Python)\nDESCRIPTION: Python script demonstrating how to send multimodal traces (text and image) from OpenAI's `gpt-4o` model to Phoenix. It conditionally launches or connects to Phoenix based on the `PHOENIX_API_KEY` environment variable, instruments OpenAI using OpenInference, and makes a chat completion request including an image URL. Requires the dependencies installed in the previous step and an OpenAI API key configured for the `openai` client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/advanced/multimodal-tracing.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Check if PHOENIX_API_KEY is present in the environment variables.\n# If it is, we'll use the cloud instance of Phoenix. If it's not, we'll start a local instance.\n# A third option is to connect to a docker or locally hosted instance.\n# See https://docs.arize.com/phoenix/setup/environments for more information.\n\n# Launch Phoenix\nimport os\nif \"PHOENIX_API_KEY\" in os.environ:\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\n    os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n\nelse:\n    import phoenix as px\n\n    px.launch_app().view()\n\n# Connect to Phoenix\nfrom phoenix.otel import register\ntracer_provider = register()\n\n# Instrument OpenAI calls in your application\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider, skip_dep_check=True)\n\n# Make a call to OpenAI with an image provided\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": \"Whatâ€™s in this image?\"},\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n          },\n        },\n      ],\n    }\n  ],\n  max_tokens=300,\n)\n```\n\n----------------------------------------\n\nTITLE: Running QA Correctness and Hallucination Evaluations\nDESCRIPTION: Sets up and executes two evaluators using OpenAI's gpt-4-turbo-preview model: one to assess QA correctness and another to detect hallucinations in the LLM's responses.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag_haystack.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    HallucinationEvaluator,\n    OpenAIModel,\n    QAEvaluator,\n    run_evals,\n)\n\nqa_evaluator = QAEvaluator(OpenAIModel(model=\"gpt-4-turbo-preview\"))\nhallucination_evaluator = HallucinationEvaluator(OpenAIModel(model=\"gpt-4-turbo-preview\"))\n\nqa_correctness_eval_df, hallucination_eval_df = run_evals(\n    evaluators=[qa_evaluator, hallucination_evaluator],\n    dataframe=qa_with_reference_df,\n    provide_explanation=True,\n    concurrency=20,\n)\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix in the Background (Python)\nDESCRIPTION: This code snippet launches the Phoenix application as a background process. This allows Phoenix to collect and display the trace data emitted by the instrumented OpenAI client in real-time. It's important to note that in a production environment, Phoenix should be run within a container for better resource management and isolation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_sessions_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Run Relevance Evaluation\nDESCRIPTION: This code evaluates the relevance of retrieved documents using Phoenix's LLM Evals. It defines a `RelevanceEvaluator` with the `MistralAIModel` and then runs the evaluations. The `provide_explanation=True` argument instructs the LLM to provide the reasoning behind the relevance scores, beneficial for debugging. The evaluations are run concurrently.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nrelevance_evaluator = RelevanceEvaluator(MistralAIModel(model=\"mistral-large-latest\"))\n\nretrieved_documents_relevance_df = run_evals(\n    evaluators=[relevance_evaluator],\n    dataframe=retrieved_documents_df,\n    provide_explanation=True,\n    concurrency=20,\n)[0]\n```\n\n----------------------------------------\n\nTITLE: Loading LLM Trace Data into Phoenix\nDESCRIPTION: This snippet downloads pre-existing trace data from a URL, reads it line by line, decodes it, converts the JSON lines into a pandas DataFrame, and then initializes a `TraceDataset` object with the data. This dataset is used as input for the Phoenix session.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_quickstart.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom urllib.request import urlopen\n\nfrom phoenix.trace.trace_dataset import TraceDataset\nfrom phoenix.trace.utils import json_lines_to_df\n\ntraces_url = \"https://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/trace.jsonl\"\nwith urlopen(traces_url) as response:\n    lines = [line.decode(\"utf-8\") for line in response.readlines()]\ntrace_ds = TraceDataset(json_lines_to_df(lines))\n```\n\n----------------------------------------\n\nTITLE: Evaluating GPT-3.5 summarization classification results\nDESCRIPTION: Evaluates the GPT-3.5 model's summarization classifications against ground truth labels, generating classification metrics and a confusion matrix for comparison with GPT-4.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df_sample[\"user_feedback\"].map(templates.SUMMARIZATION_PROMPT_RAILS_MAP).tolist()\n\nprint(classification_report(true_labels, summarization_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels,\n    predict_vector=summarization_classifications,\n    classes=rails,\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Tool Calling Prompt Template for LLM-as-Judge Evaluation in Python\nDESCRIPTION: Provides a multi-line string template for evaluating whether the tools called by an agent appropriately address a customer's question. This prompt template is designed for use with an LLM acting as a judge to classify tool call relevance as 'correct', 'mostly_correct', or 'incorrect'. It includes placeholders for the question and tool calls, and lists tool definitions to inform the judgment. This template is a core dependency for automated evaluation pipelines using language models to assess other model outputs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/react-prompting.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nTOOL_CALLING_PROMPT_TEMPLATE = \"\"\"\nYou are an evaluation assistant evaluating questions and tool calls to\ndetermine whether the tool called would reasonably help answer the question.\nThe tool calls have been generated by a separate agent, chosen from the list of\ntools provided below. Your job is to decide whether that agent's response was relevant to solving the customer's question.\n\n    [BEGIN DATA]\n    ************\n    [Question]: {question}\n    ************\n    [Tool Called]: {tool_calls}\n    [END DATA]\n\nYour response must be one of the following:\n1. **\\\"correct\\\"** â€“ The chosen tool(s) would sufficiently answer the question.\n2. **\\\"mostly_correct\\\"** â€“ The tool(s) are helpful, but a better selection could have been made (at most 1 missing or unnecessary tool).\n3. **\\\"incorrect\\\"** â€“ The tool(s) would not meaningfully help answer the question.\n\nExplain why you made your choice.\n\n    [Tool Definitions]:\n    product_comparison: Compare features of two products.\n    product_details: Get detailed features on one product.\n    apply_discount_code: Applies a discount code to an order.\n    customer_support: Get contact information for customer support regarding an issue.\n    track_package: Track the status of a package based on the tracking number.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Downloading All Spans as a DataFrame with Phoenix SDK - Python\nDESCRIPTION: This snippet illustrates how to download all tracing spans from your Phoenix project as a pandas DataFrame using the phoenix Python library. It demonstrates three approaches: fetching all spans from the default project, from a specific project using the project_name parameter, and with filtering using a UI-compatible boolean filter string. Requires Phoenix SDK, a working client connection, and optionally pandas for DataFrame manipulation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport phoenix as px\n\n# Download all spans from your default project\npx.Client().get_spans_dataframe()\n\n# Download all spans from a specific project\npx.Client().get_spans_dataframe(project_name='your project name')\n\n# You can query for spans with the same filter conditions as in the UI\npx.Client().get_spans_dataframe(\"span_kind == 'CHAIN'\")\n```\n\n----------------------------------------\n\nTITLE: Running Hallucination Classification with GPT-4\nDESCRIPTION: Executes the hallucination classification on the sampled dataset using GPT-4 with the predefined template and rails to enforce structured outputs. Uses concurrent requests for efficiency.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# The rails fore the output to specific values of the template\n# It will remove text such as \",,,\" or \"...\", anything not the\n# binary value expected from the template\nrails = list(HALLUCINATION_PROMPT_RAILS_MAP.values())\nhallucination_classifications = llm_classify(\n    dataframe=df, template=HALLUCINATION_PROMPT_TEMPLATE, model=model, rails=rails, concurrency=20\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Defining Prompt Template for Generating Questions in Python\nDESCRIPTION: Defines a multi-line string variable `GEN_TEMPLATE` containing detailed instructions for an LLM assistant. The prompt directs the LLM to generate complex, multi-faceted customer service questions suitable for testing tool-calling capabilities, focusing on scenarios involving multiple categories, vague details, mixed intentions, and indirect language related to e-commerce functions like product lookup and order tracking. It specifies the desired output format (list of 20 questions, one per line, no numbering).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nGEN_TEMPLATE = \"\"\"\nYou are an assistant that generates complex customer service questions. You will try to answer the question with the tool if possible,\ndo your best to answer, ask for more information only if needed.\nThe questions should often involve:\n\nPlease reference the product names, the product details, product IDS and product information.\n\nMultiple Categories: Questions that could logically fall into more than one category (e.g., combining product details with a discount code).\nVague Details: Questions with limited or vague information that require clarification to categorize correctly.\nMixed Intentions: Queries where the customerâ€™s goal or need is unclear or seems to conflict within the question itself.\nIndirect Language: Use of indirect or polite phrasing that obscures the direct need or request (e.g., using \"I was wondering if...\" or \"Perhaps you could help me with...\").\nFor specific categories:\n\nTrack Package: Include vague timing references (e.g., \"recently\" or \"a while ago\") instead of specific dates.\nProduct Comparison and Product Search: Include generic descriptors without specific product names or IDs (e.g., \"high-end smartphones\" or \"energy-efficient appliances\").\nApply Discount Code: Include questions about discounts that might apply to hypothetical or past situations, or without mentioning if they have made a purchase.\nProduct Details: Ask for comparisons or details that involve multiple products or categories ambiguously (e.g., \"Tell me about your range of electronics that are good for home office setups\").\nExamples of More Challenging Questions\nMultiple Categories\n\n\"I recently bought a samsung 106i smart phone, and I was wondering if there's a way to check what deals I might have missed or if my order is on its way?\"\n\"Could you tell me if the samsung 15H adapater in my last order are covered under warranty and if they have shipped yet?\"\nVague Details\n\n\"There's an issue with one of the Vizio 14Y TV I think I bought last monthâ€”what should I do?\"\n\"I need help with a iPhone 16H I ordered, or maybe I'm just looking for something new. Can you help?\"\nMixed Intentions\n\n\"I'm not sure if I should ask for a refund or just find out when it will arrive. What do you suggest?\"\n\"Could you help me decide whether to upgrade my product or just track the current one?\"\nIndirect Language\n\n\"I was wondering if you might assist me in figuring out a problem I have with an order, or maybe it's more of a query?\"\n\"Perhaps you could help me understand the benefits of your premium products compared to the regular ones?\"\n\nSome questions should be straightforward uses of the provided functions\n\nRespond with a list, one question per line. Do not include any numbering at the beginning of each line. Do not include any category headings.\nGenerate 20 questions.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Logging Document Relevance Evaluations to Phoenix\nDESCRIPTION: Logs the document relevance evaluation results to Phoenix for visualization and analysis, using the DocumentEvaluations class to structure the data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag_haystack.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import DocumentEvaluations, SpanEvaluations\n\npx.Client().log_evaluations(\n    DocumentEvaluations(dataframe=retrieved_documents_relevance_df, eval_name=\"relevance\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting LiteLLMModel with OpenInference for Request Tracing in Python\nDESCRIPTION: Imports and applies instrumentation to the LiteLLMModel to enable tracing of requests and responses for observability within the Phoenix evaluation UI. This snippet launches the Phoenix app locally and registers OpenTelemetry tracer provider to capture telemetry data. It requires the 'openinference.instrumentation.litellm' and Phoenix packages.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.litellm import LiteLLMInstrumentor\n\nimport phoenix as px\nfrom phoenix.otel import register\n\npx.launch_app()  # remove this line and run `pip install arize-phoenix` + `phoenix serve` in a terminal window to run Phoenix locally on your machine\n\ntp = register()\nLiteLLMInstrumentor().instrument(tracer_provider=tp)\n```\n\n----------------------------------------\n\nTITLE: Creating and Running Instrumented Smolagents (Python)\nDESCRIPTION: Imports necessary classes from `smolagents` (agents, tools, model API). It configures an `HfApiModel` for language model interactions, creates a `ToolCallingAgent` equipped with web search and webpage visiting tools, wraps it in a `ManagedAgent`, and sets up a `CodeAgent` as a manager. Finally, it executes the `manager_agent` with the query \"What are Smolagents?\", triggering the instrumented agent workflow, and sending traces to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/smolagents_tracing_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import (\n    CodeAgent,\n    DuckDuckGoSearchTool,\n    HfApiModel,\n    ManagedAgent,\n    ToolCallingAgent,\n    VisitWebpageTool,\n)\n\nmodel = HfApiModel()\n\nagent = ToolCallingAgent(\n    tools=[DuckDuckGoSearchTool(), VisitWebpageTool()],\n    model=model,\n)\nmanaged_agent = ManagedAgent(\n    agent=agent,\n    name=\"managed_agent\",\n    description=\"This is an agent that can do web search.\",\n)\nmanager_agent = CodeAgent(\n    tools=[],\n    model=model,\n    managed_agents=[managed_agent],\n)\n\nmanager_agent.run(\"What are Smolagents?\")\n```\n\n----------------------------------------\n\nTITLE: Executing Full Experiment with Evaluation Metrics\nDESCRIPTION: Runs the entire experiment, including task execution and evaluators, updating the experiment object with evaluation results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nexperiment = evaluate_experiment(experiment, evaluators)\n```\n\n----------------------------------------\n\nTITLE: Building RAG System with LlamaIndex and Ollama\nDESCRIPTION: Creates a Retrieval-Augmented Generation (RAG) system using LlamaIndex. Downloads Paul Graham essay dataset, configures LLM and embedding models, and builds a vector index for document retrieval.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import (\n    Settings,\n    VectorStoreIndex,\n)\nfrom llama_index.core.llama_dataset import download_llama_dataset\nfrom llama_index.embeddings.ollama import OllamaEmbedding\nfrom llama_index.llms.ollama import Ollama\n\n# download and install dependencies for benchmark dataset\nrag_dataset, documents = download_llama_dataset(\"PaulGrahamEssayDataset\", \"./data\")\n\nSettings.llm = Ollama(model=ollama_model)\nSettings.embed_model = OllamaEmbedding(model_name=ollama_embed_model)\n\n# build basic RAG system\nindex = VectorStoreIndex.from_documents(documents=documents)\nquery_engine = index.as_query_engine()\n```\n\n----------------------------------------\n\nTITLE: Evaluating RAG Relevancy\nDESCRIPTION: This snippet evaluates the relevancy of retrieved documents using an LLM classifier. It uses the `llm_classify` function with the RAG relevancy prompt template and rails. The evaluation results are added to the DataFrame, and the head of the DataFrame is displayed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/span_query_language.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndocs_eval = llm_classify(\n    docs_df,\n    model,\n    RAG_RELEVANCY_PROMPT_TEMPLATE,\n    list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n    provide_explanation=True,\n)\ndocs_eval[\"score\"] = (docs_eval.label[docs_eval.label.notnull()] == \"relevant\").astype(int)\ndocs_eval.head()\n```\n\n----------------------------------------\n\nTITLE: Executing Agent Evaluation Experiment with Phoenix in Python\nDESCRIPTION: Initiates a Phoenix experiment using the `run_experiment` function. It applies the `run_agent_and_track_path` function to each item in the specified `dataset`. The experiment is configured with a name ('Convergence Eval') and a description.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nexperiment = run_experiment(\n    dataset,\n    run_agent_and_track_path,\n    experiment_name=\"Convergence Eval\",\n    experiment_description=\"Evaluating the convergence of the agent\",\n)\n```\n\n----------------------------------------\n\nTITLE: Running Task and Evaluations Concurrently in Phoenix Python\nDESCRIPTION: Executes the experiment using `phoenix.experiments.run_experiment`, providing the `dataset`, the `task` function, and the `evaluators` dictionary simultaneously. This function runs the `task` for each example in the `dataset` and immediately runs all functions in `evaluators` on the output of the task. Both task results and evaluation results are stored in the returned experiment object. The result is assigned to `_`, indicating it's not immediately used.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n_ = run_experiment(dataset, task, evaluators)\n```\n\n----------------------------------------\n\nTITLE: Instantiating OpenAI Model (GPT-3.5 Turbo)\nDESCRIPTION: This code snippet instantiates an `OpenAIModel` with the GPT-3.5 Turbo model, setting the temperature to 0.0 and the request timeout to 20 seconds.  GPT-3.5 can significantly speed up the classification process.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_functionality_classifications.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model_name=\"gpt-3.5-turbo\", temperature=0.0, request_timeout=20)\n```\n\n----------------------------------------\n\nTITLE: Defining User Frustration Evaluation Prompt Template (Python Context)\nDESCRIPTION: This snippet defines the natural language prompt template intended for use with an LLM to evaluate user frustration in a conversation. It provides instructions on how to analyze the conversation provided within `[BEGIN DATA]` and `[END DATA]` markers, focusing on the user's state at the end. The template strictly requires the LLM's output to be a single word, either \"frustrated\" or \"ok\", ensuring a binary classification.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/user-frustration.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n  You are given a conversation where between a user and an assistant.\n  Here is the conversation:\n  [BEGIN DATA]\n  *****************\n  Conversation:\n  {conversation}\n  *****************\n  [END DATA]\n\n  Examine the conversation and determine whether or not the user got frustrated from the experience.\n  Frustration can range from midly frustrated to extremely frustrated. If the user seemed frustrated\n  at the beginning of the conversation but seemed satisfied at the end, they should not be deemed\n  as frustrated. Focus on how the user left the conversation.\n\n  Your response must be a single word, either \"frustrated\" or \"ok\", and should not\n  contain any text or characters aside from that word. \"frustrated\" means the user was left\n  frustrated as a result of the conversation. \"ok\" means that the user did not get frustrated\n  from the conversation.\n\n```\n\n----------------------------------------\n\nTITLE: Logging Evaluation Results to Phoenix\nDESCRIPTION: Logs the results of the evaluations (stored in the `hallucination_eval_df`, `qa_correctness_eval_df`, and `relevance_eval_df` dataframes) back into the active Phoenix session using the `px.Client().log_evaluations` method. `SpanEvaluations` and `DocumentEvaluations` are used to specify the type of data being logged and its association within the trace structure.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_quickstart.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import DocumentEvaluations, SpanEvaluations\n\npx.Client().log_evaluations(\n    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df),\n    DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df),\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix tracing with OpenInference and LlamaIndex instrumentation\nDESCRIPTION: Registers Phoenix trace endpoint and instruments LlamaIndex with OpenInference trace collection via `LlamaIndexInstrumentor`. This setup enables capturing LLM application traces for observability. Dependencies include `openinference` and Phoenix OTEL registration modules.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_openai_agent_tracing_tutorial.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Cloud Endpoint (Python)\nDESCRIPTION: Sets the Phoenix endpoint and API key as environment variables required for sending traces to a Phoenix Cloud instance. Replace \"ADD YOUR API KEY\" with your actual key found on the Phoenix dashboard.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai-agents-sdk.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Evaluators and Running LLM-based Evaluations in Phoenix - Python\nDESCRIPTION: Defines an evaluation model using OpenAIModel (e.g., 'gpt-4o') and instantiates hallucination, QA correctness, and relevance evaluators. Then, applies these evaluators to queries_df and retrieved_documents_df with 'run_evals', obtaining evaluation DataFrames. Requires Phoenix evaluation classes and a valid API/key for the LLM. Inputs are the previously exported DataFrames; outputs are evaluation summary DataFrames for quality assessment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\neval_model = OpenAIModel(\n    model=\"gpt-4o\",\n)\nhallucination_evaluator = HallucinationEvaluator(eval_model)\nqa_correctness_evaluator = QAEvaluator(eval_model)\nrelevance_evaluator = RelevanceEvaluator(eval_model)\n\nhallucination_eval_df, qa_correctness_eval_df = run_evals(\n    dataframe=queries_df,\n    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n    provide_explanation=True,\n)\nrelevance_eval_df = run_evals(\n    dataframe=retrieved_documents_df,\n    evaluators=[relevance_evaluator],\n    provide_explanation=True,\n)[0]\n\npx.Client().log_evaluations(\n    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df),\n    DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df),\n)\n```\n\n----------------------------------------\n\nTITLE: Launching - Phoenix UI - Python\nDESCRIPTION: Imports the Phoenix library and launches the local Phoenix web-based user interface. The `.view()` method typically opens the UI in the default web browser, allowing users to interact with their data and experiments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/datasets_and_experiments_quickstart.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\npx.launch_app().view()\n```\n\n----------------------------------------\n\nTITLE: Printing the Phoenix UI URL\nDESCRIPTION: Prints a formatted string that includes the URL of the running Phoenix session, accessed via `session.url`. This directs the user to the web interface where they can view the traces generated by the LlamaIndex application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"ðŸš€ Open the Phoenix UI if you haven't already: {session.url}\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating GPT-4 Turbo Classifications: Metrics and Confusion Matrix - Python\nDESCRIPTION: Assesses GPT-4 Turbo model's predicted labels against human groundtruth. Prints scikit-learn classification metrics and plots a normalized confusion matrix using pycm. Requires 'true_labels', 'relevance_classifications', and 'rails' to be set.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df[\"true_value\"].map(HUMAN_VS_AI_PROMPT_RAILS_MAP).tolist()\n\nprint(classification_report(true_labels, relevance_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels, predict_vector=relevance_classifications, classes=rails\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Store Sales Dataset from Remote Parquet - Python\nDESCRIPTION: Loads the store sales and price/promotions data from a remote Parquet file using pandas, making it available as a DataFrame for further querying and analysis. Requires network access and pandas. The resulting DataFrame is stored in 'store_sales_df', providing tabular data for all downstream tools.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nstore_sales_df = pd.read_parquet(\n    \"https://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/llama-index/Store_Sales_Price_Elasticity_Promotions_Data.parquet\"\n)\nstore_sales_df.head()\n```\n\n----------------------------------------\n\nTITLE: Generating Evaluation Questions Using Phoenix LLM Generate - Python\nDESCRIPTION: Uses Phoenix's `llm_generate` function to programmatically create questions from the document chunks DataFrame. It applies the defined prompt template via an OpenAI model (`gpt-3.5-turbo`) and uses a custom output parser to handle the expected JSON format, generating a new DataFrame containing the generated questions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport json\\n\\nfrom phoenix.evals import OpenAIModel, llm_generate\\n\\n\\ndef output_parser(response: str, index: int):\\n    try:\\n        return json.loads(response)\\n    except json.JSONDecodeError as e:\\n        return {\"__error__\": str(e)}\\n\\n\\nquestions_df = llm_generate(\\n    dataframe=document_chunks_df,\\n    template=generate_questions_template,\\n    model=OpenAIModel(\\n        model_name=\"gpt-3.5-turbo\",\\n    ),\\n    output_parser=output_parser,\\n    concurrency=20,\\n)\n```\n\n----------------------------------------\n\nTITLE: Logging Evaluation Results into Phoenix with Span and Document Evaluations in Python\nDESCRIPTION: Imports DocumentEvaluations and SpanEvaluations, then logs evaluation DataFrames (hallucination, QA correctness, and relevance) into Phoenix via px.Client().log_evaluations. Inputs are eval DataFrames; outputs are stored evaluations for interactive exploration within Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import DocumentEvaluations, SpanEvaluations\n\npx.Client().log_evaluations(\n    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_eval_df),\n    DocumentEvaluations(\n        eval_name=\"Retrieval Relevance\", dataframe=retrieved_documents_relevance_df\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Safely Executing Dynamic Python Code (for Visualization) - Python\nDESCRIPTION: Implements a Phoenix-traced tool that executes arbitrary Python code in a highly restricted environment, limiting available built-ins and explicitly importing required plotting/data analysis libraries. Used for securely running code generated for data visualization. Inputs: Python code as a string. Outputs: success message or any generated plot output. Catches exceptions and reports error messages.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@tracer.tool()\ndef run_python_code(code: str) -> str:\n    \"\"\"Execute Python code in a restricted environment\"\"\"\n    # Create restricted globals/locals dictionaries with plotting libraries\n    restricted_globals = {\n        \"__builtins__\": {\n            \"print\": print,\n            \"len\": len,\n            \"range\": range,\n            \"sum\": sum,\n            \"min\": min,\n            \"max\": max,\n            \"int\": int,\n            \"float\": float,\n            \"str\": str,\n            \"list\": list,\n            \"dict\": dict,\n            \"tuple\": tuple,\n            \"set\": set,\n            \"round\": round,\n            \"__import__\": __import__,\n            \"json\": __import__(\"json\"),\n        },\n        \"plt\": __import__(\"matplotlib.pyplot\"),\n        \"pd\": __import__(\"pandas\"),\n        \"np\": __import__(\"numpy\"),\n        \"sns\": __import__(\"seaborn\"),\n    }\n\n    try:\n        # Execute code in restricted environment\n        exec_locals = {}\n        exec(code, restricted_globals, exec_locals)\n\n        # Capture any printed output or return the plot\n        exec_locals.get(\"__builtins__\", {}).get(\"_\", \"\")\n        if \"plt\" in exec_locals:\n            return exec_locals[\"plt\"]\n\n        # Try to parse output as JSON before returning\n        return \"Code executed successfully\"\n\n    except Exception as e:\n        return f\"Error executing code: {str(e)}\"\n```\n\n----------------------------------------\n\nTITLE: Defining - Custom Code Evaluator - Python\nDESCRIPTION: Shows how to define a custom evaluation function using standard Python code. This function calculates the Jaccard similarity between the task output and the expected answer from the dataset, providing a numerical similarity score.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/datasets_and_experiments_quickstart.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict\n\n\ndef jaccard_similarity(output: str, expected: Dict[str, Any]) -> float:\n    # https://en.wikipedia.org/wiki/Jaccard_index\n    actual_words = set(output.lower().split(\" \"))\n    expected_words = set(expected[\"answer\"].lower().split(\" \"))\n    words_in_common = actual_words.intersection(expected_words)\n    all_words = actual_words.union(expected_words)\n    return len(words_in_common) / len(all_words)\n```\n\n----------------------------------------\n\nTITLE: Running a Zero-Shot Chain-of-Thought Prompt Experiment and Extracting Answers with Phoenix in Python\nDESCRIPTION: Runs the zero-shot CoT prompt as a task by sending inputs to OpenAI and retrieving the full reasoning output along with the final numerical answer extracted from the last line. The evaluation function verifies correctness by checking the integer match against ground truth. Uses Phoenix's run_experiment to track this CoT prompt performance on the dataset, enabling analysis of the reasoning process and improvements over baseline.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/chain_of_thought_prompting.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport re\n\n\ndef zero_shot_COT_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **zero_shot_COT.format(variables={\"Problem\": input[\"Word Problem\"]})\n    )\n    response_text = resp.choices[0].message.content.strip()\n    lines = response_text.split(\"\\n\")\n    final_answer = lines[-1].strip()\n    final_answer = re.sub(r\"^\\*\\*(\\d+)\\*\\*$\", r\"\\1\", final_answer)\n    return {\"full_response\": response_text, \"final_answer\": final_answer}\n\n\ndef evaluate_response(output, expected):\n    final_answer = output[\"final_answer\"]\n    if not final_answer.isdigit():\n        return False\n    return int(final_answer) == int(expected[\"Answer\"])\n\n\ninitial_experiment = run_experiment(\n    dataset,\n    task=zero_shot_COT_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Zero-Shot COT Prompt\",\n    experiment_name=\"zero-shot-cot-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + zero_shot_COT.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Span Processing Manually with OpenTelemetry SDK in Python\nDESCRIPTION: Demonstrates how to manually configure the `BatchSpanProcessor` using the standard OpenTelemetry SDK in Python. This involves creating an instance of `BatchSpanProcessor` with an appropriate exporter (e.g., `OTLPSpanExporter`) and adding it to the `tracer_provider`. This method offers more control compared to library helpers.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor, BatchSpanProcessor\n\ntracer_provider.add_span_processor(BatchSpanProcessor(OTLPSpanExporter(endpoint)))\n```\n\n----------------------------------------\n\nTITLE: Installing OpenInference Python Instrumentation Package (sh)\nDESCRIPTION: Installs the core `openinference-instrumentation` package for Python using pip. This package is required to use the context managers (`using_session`, `using_user`, etc.) for adding attributes to OpenTelemetry spans in Python applications.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install openinference-instrumentation\n```\n\n----------------------------------------\n\nTITLE: Defining LLM Judge Prompt Templates for Clarity and Entity Accuracy in Python\nDESCRIPTION: This snippet creates two multi-line string constants: CLARITY_LLM_JUDGE_PROMPT and ENTITY_CORRECTNESS_LLM_JUDGE_PROMPT, which are used as prompt templates for LLM-based judgment of response clarity and entity correctness. The prompts set a standard format for LLM assessors to follow, instructing them on criteria and output structure. There are no parameterized inputs except for formatted {query} and {response} values, but use of these prompts assumes integration with an LLM classification utility. There are no dependencies except standard Python string handling.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_57\n\nLANGUAGE: Python\nCODE:\n```\nCLARITY_LLM_JUDGE_PROMPT = \"\"\"\nIn this task, you will be presented with a query and an answer. Your objective is to evaluate the clarity\nof the answer in addressing the query. A clear response is one that is precise, coherent, and directly\naddresses the query without introducing unnecessary complexity or ambiguity. An unclear response is one\nthat is vague, disorganized, or difficult to understand, even if it may be factually correct.\n\nYour response should be a single word: either \\\"clear\\\" or \\\"unclear,\\\" and it should not include any other\ntext or characters. \\\"clear\\\" indicates that the answer is well-structured, easy to understand, and\nappropriately addresses the query. \\\"unclear\\\" indicates that the answer is ambiguous, poorly organized, or\nnot effectively communicated. Please carefully consider the query and answer before determining your\nresponse.\n\nAfter analyzing the query and the answer, you must write a detailed explanation of your reasoning to\njustify why you chose either \\\"clear\\\" or \\\"unclear.\\\" Avoid stating the final label at the beginning of your\nexplanation. Your reasoning should include specific points about how the answer does or does not meet the\ncriteria for clarity.\n\n[BEGIN DATA]\nQuery: {query}\nAnswer: {response}\n[END DATA]\nPlease analyze the data carefully and provide an explanation followed by your response.\n\nEXPLANATION: Provide your reasoning step by step, evaluating the clarity of the answer based on the query.\nLABEL: \\\"clear\\\" or \\\"unclear\\\"\n\"\"\"\n\nENTITY_CORRECTNESS_LLM_JUDGE_PROMPT = \"\"\"\nIn this task, you will be presented with a query and an answer. Your objective is to determine whether all\nthe entities mentioned in the answer are correctly identified and accurately match those in the query. An\nentity refers to any specific person, place, organization, date, or other proper noun. Your evaluation\nshould focus on whether the entities in the answer are correctly named and appropriately associated with\nthe context in the query.\n\nYour response should be a single word: either \\\"correct\\\" or \\\"incorrect,\\\" and it should not include any\nother text or characters. \\\"correct\\\" indicates that all entities mentioned in the answer match those in the\nquery and are properly identified. \\\"incorrect\\\" indicates that the answer contains errors or mismatches in\nthe entities referenced compared to the query.\n\nAfter analyzing the query and the answer, you must write a detailed explanation of your reasoning to\njustify why you chose either \\\"correct\\\" or \\\"incorrect.\\\" Avoid stating the final label at the beginning of\nyour explanation. Your reasoning should include specific points about how the entities in the answer do or\ndo not match the entities in the query.\n\n[BEGIN DATA]\nQuery: {query}\nAnswer: {response}\n[END DATA]\nPlease analyze the data carefully and provide an explanation followed by your response.\n\nEXPLANATION: Provide your reasoning step by step, evaluating whether the entities in the answer are\ncorrect and consistent with the query.\nLABEL: \\\"correct\\\" or \\\"incorrect\\\"\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix OpenTelemetry Tracing - Python\nDESCRIPTION: Imports the 'register' function from 'phoenix.otel' and configures a tracer provider for monitoring LLM API calls. Set 'project_name' to specify the project context (defaults to 'default') and enable automatic instrumentation with 'auto_instrument=True'. The resulting tracer provider can be used with OpenTelemetry-based monitoring setups.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/litellm.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n)\n\n```\n\n----------------------------------------\n\nTITLE: Defining RAG Schema with Retrieval and Embedding Column Names in Python\nDESCRIPTION: This snippet shows how to define a schema for Phoenix that specifies column mappings for RAG data, including the prediction ID, embedding vector, raw query data, retrieved document IDs, and relevance scores.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/retrieval-rag.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nprimary_schema = Schema(\n    prediction_id_column_name=\"id\",\n    prompt_column_names=RetrievalEmbeddingColumnNames(\n        vector_column_name=\"embedding\",\n        raw_data_column_name=\"query\",\n        context_retrieval_ids_column_name=\"retrieved_document_ids\",\n        context_retrieval_scores_column_name=\"relevance_scores\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Environment - Python\nDESCRIPTION: Obtains the OpenAI API key securely from the environment variable OPENAI_API_KEY or prompts the user to enter it interactively. Sets the API key in the OS environment for subsequent use by OpenAI-powered evaluators. Requires the standard 'os' and 'getpass' modules and should only be run in an environment where user input is possible if the key is not already set.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/evals.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"\\ud83d\\udd11 Enter your OpenAI API key: \")\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Run QA and Hallucination Evaluation\nDESCRIPTION: Runs QA correctness and hallucination evaluations on the QA dataset using `run_evals` and two evaluators: `QAEvaluator` and `HallucinationEvaluator`. It uses `MistralAIModel` for evaluation and provides explanations. The results are stored in separate dataframes for QA correctness and hallucination assessment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    HallucinationEvaluator,\n    MistralAIModel,\n    QAEvaluator,\n    run_evals,\n)\n\nqa_evaluator = QAEvaluator(MistralAIModel(model=\"mistral-large-latest\"))\nhallucination_evaluator = HallucinationEvaluator(MistralAIModel(model=\"mistral-large-latest\"))\n\nqa_correctness_eval_df, hallucination_eval_df = run_evals(\n    evaluators=[qa_evaluator, hallucination_evaluator],\n    dataframe=qa_with_reference_df,\n    provide_explanation=True,\n    concurrency=20,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Router Step Function\nDESCRIPTION: This function simulates a router step within an agent. It takes an `Example` object as input, constructs a system message and user message based on the question in the example, calls the LLM client, and extracts and returns a list of called tools.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef run_router_step(example: Example) -> str:\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n        }\n    ]\n    messages.append({\"role\": \"user\", \"content\": example.input.get(\"question\")})\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        tools=tools,\n    )\n    tool_calls = []\n    for tool_call in response.choices[0].message.tool_calls:\n        tool_calls.append(tool_call.function.name)\n    return tool_calls\n```\n\n----------------------------------------\n\nTITLE: Evaluating Retriever Context Relevance using LLM-assisted Classification in Phoenix (Python)\nDESCRIPTION: Uses Phoenix evaluation utilities to classify the relevance of each piece of retrieved context to the query. Copies over queries and context columns into dedicated evaluation columns, applies the llm_classify method with a relevancy prompt template and OpenAI LLM, and stores the LLM-based relevance predictions. Requires phoenix.evals, an OpenAI-compatible model (e.g., gpt-4), and proper DataFrame structure. Useful for advanced retrieval evaluation beyond cosine similarity.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/qdrant_langchain_instrumentation_search_and_retrieval_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n    RAG_RELEVANCY_PROMPT_TEMPLATE,\n    OpenAIModel,\n    llm_classify,\n)\n\n# create evaluation dataframes with \"input\" and \"reference\" columns\ncontext0_eval_df = query_df.copy()\ncontext0_eval_df[\"input\"] = context0_eval_df[\"text\"]\ncontext0_eval_df[\"reference\"] = context0_eval_df[\"context_text_0\"]\n\ncontext1_eval_df = query_df.copy()\ncontext1_eval_df[\"input\"] = context1_eval_df[\"text\"]\ncontext1_eval_df[\"reference\"] = context1_eval_df[\"context_text_1\"]\n\nmodel = OpenAIModel(model=\"gpt-4\")\ncontext0_relevance = llm_classify(\n    context0_eval_df,\n    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n    provide_explanation=True,\n    model=model,\n)\ncontext1_relevance = llm_classify(\n    context1_eval_df,\n    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n    provide_explanation=True,\n    model=model,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Phoenix and Langchain\nDESCRIPTION: Installs required Python packages using pip, including Arize Phoenix, OpenTelemetry instrumentation for Phoenix, LlamaIndex OpenAI integration, OpenAI client, Google Cloud Storage filesystem, nest_asyncio, Langchain core and OpenAI integration, and OpenInference instrumentation for Langchain. The '-qq' flag ensures a quiet installation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qq \"arize-phoenix>=8.8.0\" \"arize-phoenix-otel>=0.8.0\" llama-index-llms-openai openai gcsfs nest_asyncio langchain langchain-openai openinference-instrumentation-langchain\n```\n\n----------------------------------------\n\nTITLE: Classifying Tool Call Responses with LLM and Prompt Rails - Python\nDESCRIPTION: Generates rails from TOOL_CALLING_PROMPT_RAILS_MAP and classifies responses using the llm_classify method. Applies a prompt template, model, binary classification rails, and instructs the LLM to provide explanations for each classification. The resulting DataFrame is further postprocessed to assign a numeric score of 1 for 'correct' and 0 otherwise. Inputs: trace DataFrame, prompt template, model instance, rails, explanation flag. Outputs: response_classifications DataFrame with label and score columns.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nrails = list(TOOL_CALLING_PROMPT_RAILS_MAP.values())\n\nresponse_classifications = llm_classify(\n    dataframe=trace_df,\n    template=TOOL_CALLING_PROMPT_TEMPLATE,\n    model=eval_model,\n    rails=rails,\n    provide_explanation=True,\n)\nresponse_classifications[\"score\"] = response_classifications.apply(\n    lambda x: 1 if x[\"label\"] == \"correct\" else 0, axis=1\n)\n```\n\n----------------------------------------\n\nTITLE: Defining RAG Schema in Arize Phoenix Python\nDESCRIPTION: This snippet defines the data schema required by Arize Phoenix for RAG data. It maps the dataframe columns containing the prediction ID, query embedding, raw query text, retrieved document IDs, and their relevance scores to the appropriate schema fields. This schema is essential for Phoenix to correctly interpret and display the RAG data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/define-your-schema/retrieval-rag.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nprimary_schema = Schema(\n    prediction_id_column_name=\"id\",\n    prompt_column_names=RetrievalEmbeddingColumnNames(\n        vector_column_name=\"embedding\",\n        raw_data_column_name=\"query\",\n        context_retrieval_ids_column_name=\"retrieved_document_ids\",\n        context_retrieval_scores_column_name=\"relevance_scores\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating GPT-3.5-Turbo Hallucination Detection Performance\nDESCRIPTION: Compares GPT-3.5-Turbo's hallucination classifications against ground-truth labels, generating classification metrics and visualizing a confusion matrix to assess model performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df[\"is_hallucination\"].map(HALLUCINATION_PROMPT_RAILS_MAP).tolist()\n\nprint(classification_report(true_labels, hallucination_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels,\n    predict_vector=hallucination_classifications,\n    classes=rails,\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Filter Traces with Retrieved Documents - Python\nDESCRIPTION: This snippet filters the traces DataFrame to include only those spans that have retrieved documents. It is useful for specifically analyzing the retrieval stage of the RAG pipeline and for assessing the relevance of the retrieved content.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nspans_with_docs_df = spans_df[spans_df[\"attributes.retrieval.documents\"].notnull()]\n```\n\n----------------------------------------\n\nTITLE: Evaluating Response Using LLM as a Judge\nDESCRIPTION: This snippet defines a function to evaluate model outputs by classifying their relevance through an LLM-based classifier, utilizing a DataFrame for input data and applying the classification results to score answers as correct or incorrect. It automates response evaluation in experimentation workflows.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/react_prompting.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndef evaluate_response(input, output):\n    response_classifications = llm_classify(\n        dataframe=pd.DataFrame([{\"question\": input[\"Questions\"], \"tool_calls\": output}]),\n        template=TOOL_CALLING_PROMPT_TEMPLATE,\n        model=OpenAIModel(model=\"gpt-3.5-turbo\"),\n        rails=list(TOOL_CALLING_PROMPT_RAILS_MAP.values()),\n        provide_explanation=True,\n    )\n    score = response_classifications.apply(lambda x: 0 if x[\"label\"] == \"incorrect\" else 1, axis=1)\n    return score\n\n```\n\n----------------------------------------\n\nTITLE: Evaluating Classifications with GPT-4 Turbo using Report and Matrix\nDESCRIPTION: This snippet evaluates the performance of GPT-4 Turbo by comparing classifications against ground truth. A classification report and confusion matrix are generated to quantify performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_functionality_classifications.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df[\"is_well_coded\"].map(CODE_FUNCTIONALITY_PROMPT_RAILS_MAP).tolist()\n\nprint(classification_report(true_labels, classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels, predict_vector=classifications, classes=rails\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Applying nest_asyncio for Improved Async Performance in Notebook Environments\nDESCRIPTION: Enables re-entrant event loops in notebook environments using nest_asyncio to significantly speed up evaluation submissions, typically improving performance by 5x.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key in Python\nDESCRIPTION: Retrieves the OpenAI API key from environment variables. If not found, it securely prompts the user to enter the key using `getpass`. The retrieved key is then set as an environment variable `OPENAI_API_KEY` for subsequent use by OpenAI clients.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Running a Text2SQL Evaluation Experiment with Phoenix in Python\nDESCRIPTION: Runs an experiment over the uploaded question dataset, applying the text2sql pipeline to each question and evaluating using the specified evaluators (no_error and has_results). Relies on Phoenix's run_experiment function for orchestration. Dependencies: px, run_experiment from phoenix.experiments, ds dataset, task function, and evaluator functions. Output: experiment object with results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\nfrom phoenix.experiments import run_experiment\n\n\n# Define the task to run text2sql on the input question\ndef task(input):\n    return text2sql(input[\"question\"])\n\n\nexperiment = run_experiment(\n    ds, task=task, evaluators=[no_error, has_results], experiment_metadata=CONFIG\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Phoenix Integration\nDESCRIPTION: Imports necessary libraries for working with Phoenix, OpenAI, and data manipulation. Includes libraries for asynchronous operations, dataframes, OpenAI API, tracing, and Phoenix-specific modules for client interaction, prompt management, evaluation, and experimentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/react_prompting.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\nimport pandas as pd\nfrom openai import OpenAI\nfrom openai.types.chat.completion_create_params import CompletionCreateParamsBase\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nimport phoenix as px\nfrom phoenix.client import Client as PhoenixClient\nfrom phoenix.client.types import PromptVersion\nfrom phoenix.evals import (\n    TOOL_CALLING_PROMPT_RAILS_MAP,\n    OpenAIModel,\n    llm_classify,\n)\nfrom phoenix.experiments import run_experiment\nfrom phoenix.otel import register\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Exporting Trace Data as Pandas DataFrame - Python\nDESCRIPTION: Connects to the running Phoenix application using `px.Client()` and retrieves all collected trace spans. The data is returned as a pandas DataFrame, enabling programmatic analysis, evaluation, or further processing of the trace information.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_agent_tracing_tutorial.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ntrace_df = px.Client().get_spans_dataframe()\ntrace_df\n```\n\n----------------------------------------\n\nTITLE: Setting Parent ID as Index in Phoenix SpanQuery Output (Python)\nDESCRIPTION: Shows how to set the `parent_id` of a span as the index of the resulting DataFrame. This is done by selecting `parent_id` and renaming it to the special column name `span_id` within the `SpanQuery().select()` method. This technique is crucial for joining child spans to their parent spans.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nquery = SpanQuery().select(\n    span_id=\"parent_id\",\n    output=\"output.value\",\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Task Function for Few Shot Prompt - Python\nDESCRIPTION: Defines an updated 'test_prompt' function that invokes the OpenAI API using the newly created few shot prompt. Extracts the relevant example from input, formats the prompt variables, and returns the model's stripped output. This function is tied to the specific prompt instance and should be used for experiments evaluating few shot optimization. Dependencies: OpenAI, few_shot_prompt.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef test_prompt(input):\n    client = OpenAI()\n    prompt_vars = {\"prompt\": input[\"prompt\"]}\n    resp = client.chat.completions.create(**few_shot_prompt.format(variables=prompt_vars))\n    return resp.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Running the Initial LLM as a Judge Experiment with Phoenix in Python\nDESCRIPTION: Applies 'nest_asyncio' to allow nested event loops and runs a Phoenix experiment comparing the initial prompt's predicted empathy scores against dataset ground truth. 'run_experiment' executes the evaluation pipeline with the given dataset, task, and evaluator, logging results under the experiment name 'initial_prompt'. This snippet automates large-scale prompt evaluation and accuracy measurement.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.experiments import run_experiment\n\nnest_asyncio.apply()\n\ninitial_experiment = run_experiment(\n    dataset, task=initial_prompt, evaluators=[evaluate_response], experiment_name=\"initial_prompt\"\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating LLM Application Responses for Hallucinations Using Phoenix Evals in Python\nDESCRIPTION: Applies Phoenix's LLM evals framework backed by OpenAI models to classify LLM responses for hallucinations and correctness. Uses predefined prompt templates and rails for hallucination detection. Applies nest_asyncio for asynchronous API call optimization. The classification returns labels along with explanation and a binary factual score.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.evals import (\n    HALLUCINATION_PROMPT_RAILS_MAP,\n    HALLUCINATION_PROMPT_TEMPLATE,\n    QA_PROMPT_RAILS_MAP,\n    QA_PROMPT_TEMPLATE,\n    OpenAIModel,\n    llm_classify,\n)\n\nnest_asyncio.apply()  # Speeds up OpenAI API calls\n\n# Check if the application has any indications of hallucinations\nhallucination_eval = llm_classify(\n    dataframe=queries_df,\n    model=OpenAIModel(model=\"gpt-4o\", temperature=0.0),\n    template=HALLUCINATION_PROMPT_TEMPLATE,\n    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n    provide_explanation=True,  # Makes the LLM explain its reasoning\n)\nhallucination_eval[\"score\"] = (\n    hallucination_eval.label[~hallucination_eval.label.isna()] == \"factual\"\n).astype(int)\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Schema with Implicit Features in Python\nDESCRIPTION: Shows how to create a `phoenix.Schema` where feature columns are implicitly detected. By only specifying essential columns like prediction and actual labels and omitting `feature_column_names` (leaving it as `None`), Phoenix automatically treats all remaining columns in the DataFrame as features. This simplifies schema definition for datasets with a large number of features.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/define-your-schema/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nschema = px.Schema(\n    prediction_label_column_name=\"predicted\",\n    actual_label_column_name=\"target\",\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluate Experiment with LLM Judge Python\nDESCRIPTION: Introduces an advanced evaluation method using an LLM (GPT) as a judge. It creates an `LLMCriteriaEvaluator` named 'is_sql' that uses an OpenAI model to assess whether the generated output is a valid and executable SQL query based on a specified criterion. It then uses `evaluate_experiment` to apply this LLM-based evaluator to the results of the previous experiment, providing an alternative, potentially more nuanced assessment of query quality.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.evals.models import OpenAIModel\nfrom phoenix.experiments import evaluate_experiment\nfrom phoenix.experiments.evaluators.llm_evaluators import LLMCriteriaEvaluator\n\nllm_evaluator = LLMCriteriaEvaluator(\n    name=\"is_sql\",\n    criteria=\"is_sql\",\n    description=\"the output is a valid SQL query and that it executes without errors\",\n    model=OpenAIModel(),\n)\n\nevaluate_experiment(experiment, evaluators=[llm_evaluator])\n```\n\n----------------------------------------\n\nTITLE: Running Zero-Shot Experiment\nDESCRIPTION: Runs the zero-shot prompting experiment using the `run_experiment` function from the Phoenix library. It passes the dataset, the `zero_shot_prompt` task, and the `evaluate_response` evaluator. The results of the experiment, including task outputs and evaluation results, are then uploaded to Phoenix for analysis and visualization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/few_shot_prompting.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.experiments import run_experiment\n\nnest_asyncio.apply()\n\ninitial_experiment = run_experiment(\n    dataset,\n    task=zero_shot_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Zero-Shot Prompt\",\n    experiment_name=\"zero-shot-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Building and Indexing Chroma DB for Employees\nDESCRIPTION: Creates or gets a separate Chroma collection named 'agentic-rag-demo-company-employees', also using the OpenAI embedding function. It adds sample documents containing employee information. Similar to the policies database, a `ChromaVectorStore`, `VectorStoreIndex`, and LlamaIndex query engine are created for this employee data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/agentic_rag_tracing.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nchroma_client = chromadb.Client()\nchroma_collection = chroma_client.get_or_create_collection(\n    \"agentic-rag-demo-company-employees\", embedding_function=openai_ef\n)\n\nchroma_collection.add(\n    ids=[\"1\", \"2\", \"3\"],\n    documents=[\n        \"John Smith is a Software Engineer in the Engineering department who started on 2023-01-15\",\n        \"Sarah Johnson is a Marketing Manager in the Marketing department who started on 2022-08-01\",\n        \"Michael Williams is a Sales Director in the Sales department who started on 2021-03-22\",\n    ],\n)\n\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n\nchroma_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\nchroma_engine_employees = chroma_index.as_query_engine(similarity_top_k=1)\n```\n\n----------------------------------------\n\nTITLE: Logging Evaluations to Phoenix\nDESCRIPTION: This code logs the Ragas evaluation scores to the Phoenix platform, which allows for visualizing the scores alongside the application spans. It iterates through the columns of the `eval_scores_df` DataFrame, creates `SpanEvaluations` objects for each evaluation metric, and uses the Phoenix client to log these evaluations.  Requires the Phoenix client library.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import SpanEvaluations\n\n# Log the evaluations to Phoenix under the project \"llama-index\"\n# This will allow you to visualize the scores alongside the spans in the UI\nfor eval_name in eval_scores_df.columns:\n    evals_df = eval_scores_df[[eval_name]].rename(columns={eval_name: \"score\"})\n    evals = SpanEvaluations(eval_name, evals_df)\n    px.Client().log_evaluations(evals)\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Endpoint and API Key\nDESCRIPTION: This snippet configures the environment variables required to connect to a Phoenix instance. It sets the `PHOENIX_COLLECTOR_ENDPOINT` to the Phoenix URL and prompts the user for their Phoenix API key, which is then used to set the `PHOENIX_API_KEY` and `OTEL_EXPORTER_OTLP_HEADERS` environment variables. This enables the OpenTelemetry exporter to authenticate with the Phoenix collector.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\n# Change the following line if you're self-hosting\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com/\"\n\n# Remove the following lines if you're self-hosting\nos.environ[\"PHOENIX_API_KEY\"] = getpass(\"Enter your Phoenix API key: \")\nos.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\n```\n\n----------------------------------------\n\nTITLE: Applying nest_asyncio for asynchronous request handling\nDESCRIPTION: Applies nest_asyncio to enable event loops to be re-entrant in notebook environments, which significantly improves the speed of evaluation submissions (up to 5x faster).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables and API Keys for Phoenix and OpenAI in Python\nDESCRIPTION: Sets environment variables required to authenticate and connect to Phoenix Cloud and OpenAI services. Uses 'getpass' to securely prompt the user for their Phoenix API key and OpenAI API key if not already set. Configures 'PHOENIX_COLLECTOR_ENDPOINT' to target the Phoenix cloud instance. This snippet enables secure access to APIs necessary for all downstream operations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\nif not os.environ.get(\"PHOENIX_CLIENT_HEADERS\"):\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = \"api_key=\" + getpass(\"Enter your Phoenix API key: \")\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Running - Phoenix Experiment - Python\nDESCRIPTION: Initiates a Phoenix experiment. It takes the uploaded dataset, the defined task function, an experiment name, and a list of evaluators (custom code and custom LLM in this case). Phoenix executes the task on each data point and applies the specified evaluators.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/datasets_and_experiments_quickstart.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments import run_experiment\n\nexperiment = run_experiment(\n    dataset,\n    task,\n    experiment_name=\"initial-experiment\",\n    evaluators=[jaccard_similarity, accuracy],\n)\n```\n\n----------------------------------------\n\nTITLE: Tracing OpenAI Calls Automatically (Python)\nDESCRIPTION: This snippet demonstrates how Phoenix can automatically trace calls made to the OpenAI library. After installing the `openinference-instrumentation-openai` package and setting the OpenAI API key, Phoenix captures the requests and responses when the OpenAI API is called.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-python.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Add OpenAI API Key\nimport os\nimport openai\n\nos.environ[\"OPENAI_API_KEY\"] = \"ADD YOUR OPENAI API KEY\"\n\nclient = openai.OpenAI()\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a haiku.\"}],\n)\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Uploading Evaluations to Phoenix (Python)\nDESCRIPTION: Imports `SpanEvaluations` from `phoenix.trace`, creates a Phoenix client instance, and uses `client.log_evaluations()` to upload the evaluation results. It wraps the formatted `eval_df` in a `SpanEvaluations` object, providing a name ('Duplicate') for this set of evaluations. The DataFrame's index (span_id) links these evaluations to the specific traces in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evaluating_traces_quickstart.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import SpanEvaluations\n\npx.Client().log_evaluations(SpanEvaluations(eval_name=\"Duplicate\", dataframe=eval_df))\n```\n\n----------------------------------------\n\nTITLE: Creating a Prompt with Phoenix Client (Python)\nDESCRIPTION: This snippet demonstrates how to create a prompt using the Phoenix client in Python. It defines a prompt for summarizing articles into bullet points, then uses the `px.Client().prompts.create` method to store this prompt in the Phoenix server with the name \"article-bullet-summarizer\".  It requires the `phoenix` library to be installed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/how-to-prompts/create-a-prompt.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\nfrom phoenix.client.types import PromptVersion\n\ncontent = \"\"\"\\\nYou're an expert educator in {{ topic }}. Summarize the following article\nin a few concise bullet points that are easy for beginners to understand.\n\n{{ article }}\n\"\"\"\n\nprompt_name = \"article-bullet-summarizer\"\nprompt = px.Client().prompts.create(\n    name=prompt_name,\n    version=PromptVersion(\n        [{\"role\": \"user\", \"content\": content}],\n        model_name=\"gpt-4o-mini\",\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Running a Phoenix Experiment with the Few-Shot Prompt in Python\nDESCRIPTION: Executes an experiment using the `run_experiment` function (presumably from the Phoenix library or a related utility). It applies the `few_shot_prompt_template` function to each item in the `dataset`, evaluates the generated responses using the `evaluate_response` evaluator, and logs the experiment to Phoenix with a description, name, and metadata including the associated prompt ID. Requires the `dataset` object, the `few_shot_prompt_template` function, an `evaluate_response` function, and the `few_shot_prompt` object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/few_shot_prompting.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfew_shot_experiment = run_experiment(\n    dataset,\n    task=few_shot_prompt_template,\n    evaluators=[evaluate_response],\n    experiment_description=\"Few Shot Prompting\",\n    experiment_name=\"few-shot-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + few_shot_prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Calling OpenAI API with Chart Instructions and Images\nDESCRIPTION: Iterates through selected rows (index 1 to 24) of the ChartMimic DataFrame. For each row, it extracts the instruction text and the input figure bytes. The image bytes are base64 encoded. It then constructs a message payload containing the text instruction and the base64 encoded image data URL. This payload is sent to the OpenAI Chat Completions API using the 'gpt-4o-mini' model.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/vision.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfor _, instruction, input_figure in (\n    df.loc[:, [\"Instruction\", \"InputFigurePreview\"]].iloc[1:25].itertuples()\n):\n    bytes = input_figure[\"bytes\"]\n    encoded_string = b64encode(bytes).decode()\n    message = {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": instruction},\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:image/png;base64,{encoded_string}\", \"detail\": \"low\"},\n            },\n        ],\n    }\n    client.chat.completions.create(model=\"gpt-4o-mini\", messages=[message], max_tokens=1000)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application and Registering OTel Endpoint (Python)\nDESCRIPTION: Initializes and launches the Arize Phoenix application instance. It then registers the local Phoenix instance as the OpenTelemetry endpoint, directing traces from instrumented libraries to this instance for collection and visualization. Requires the `arize-phoenix` library.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/groq_tracing_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\nfrom phoenix.otel import register\n\nsession = px.launch_app()\nregister()\n```\n\n----------------------------------------\n\nTITLE: Creating Inferences Object in Arize Phoenix Python\nDESCRIPTION: This code snippet constructs an `Inferences` object using the specified dataframe containing the RAG data and the previously defined schema. The `Inferences` object encapsulates the data and its structure, making it ready for ingestion and analysis within the Arize Phoenix application. This step is crucial for preparing the data for visualization and monitoring.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/define-your-schema/retrieval-rag.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprimary_inferences = px.Inferences(primary_dataframe, primary_schema)\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedding Feature Columns with px.EmbeddingColumnNames in Phoenix (Python)\nDESCRIPTION: This snippet illustrates how to define an embedding feature column within a px.Schema by mapping a user-defined embedding name (e.g., \"transaction_embeddings\") to a px.EmbeddingColumnNames object that specifies the vector_column_name parameter. Dependencies include the Phoenix library (imported as px). The embedding_feature_column_names argument is a dictionary, with keys for embedding feature names and values set to EmbeddingColumnNames objects. The code expects that the DataFrame contains a column with vector data matching the specified vector_column_name and that all vectors are one-dimensional arrays of identical length. Incorrect or inconsistent shapes will result in Phoenix raising an error.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/define-your-schema/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nschema = px.Schema(\n    prediction_label_column_name=\"predicted\",\n    actual_label_column_name=\"target\",\n    embedding_feature_column_names={\n        \"transaction_embeddings\": px.EmbeddingColumnNames(\n            vector_column_name=\"embedding_vector\"\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Querying Employees Query Engine\nDESCRIPTION: Tests the query engine created for the employee Chroma database by asking for the name of the Sales Director. This verifies the retrieval functionality of the employee data store.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/agentic_rag_tracing.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nchroma_engine_employees.query(\"What is the name of the Sales Director?\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for DSPy, Phoenix, and OpenInference\nDESCRIPTION: Installs the necessary Python packages for DSPy framework, Phoenix, OpenInference instrumentation, and related libraries to set up the environment for building and observing the DSPy application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/dspy_tracing_tutorial.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n!pip install arize-phoenix \"dspy>=2.5.0\" \"openinference-instrumentation-dspy>=0.1.13\" openinference-instrumentation-litellm opentelemetry-exporter-otlp 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Defining Langchain Tools for E-commerce Agent in Python\nDESCRIPTION: Defines several Python functions decorated with Langchain's `@tool`. These functions simulate tools an e-commerce chatbot might use: `product_comparison`, `product_details`, `apply_discount_code`, `product_search`, `customer_support`, and `track_package`. Each function includes type hints for parameters (leveraging Pydantic validation implicitly), a docstring explaining its purpose and parameters (used by the LLM agent), basic input validation returning an error dictionary, and placeholder return values. Finally, all defined tool functions are collected into a list named `tools`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n## function definitions using pydantic decorator\n\n\n@tool\ndef product_comparison(product_a_id: str, product_b_id: str) -> dict:\n    \"\"\"\n    Compare features of two products.\n\n    Parameters:\n    product_a_id (str): The unique identifier of Product A.\n    product_b_id (str): The unique identifier of Product B.\n\n    Returns:\n    dict: A dictionary containing the comparison of the two products.\n    \"\"\"\n\n    if product_a_id == \"\" or product_b_id == \"\":\n        return {\"error\": \"missing product id\"}\n\n    # Implement the function logic here\n    return {\"comparison\": \"Similar\"}\n\n\n@tool\ndef product_details(product_id: str) -> dict:\n    \"\"\"\n    Get detailed features on one product.\n\n    Parameters:\n    product_id (str): The unique identifier of the Product.\n\n    Returns:\n    dict: A dictionary containing product details.\n    \"\"\"\n\n    if product_id == \"\":\n        return {\"error\": \"missing product id\"}\n\n    # Implement the function logic here\n    return {\"name\": \"Product Name\", \"price\": \"$12.50\", \"Availability\": \"In Stock\"}\n\n\n@tool\ndef apply_discount_code(order_id: int, discount_code: str) -> dict:\n    \"\"\"\n    Applies a discount code to an order.\n\n    Parameters:\n    order_id (str): The unique identifier of the order.\n    discount_code (str): The discount code to apply.\n\n    Returns:\n    dict: A dictionary containing the updated order details.\n    \"\"\"\n\n    if order_id == \"\" or discount_code == \"\":\n        return {\"error\": \"missing order id or discount code\"}\n\n    # Implement the function logic here\n    return {\"applied\": \"True\"}\n\n\n@tool\ndef product_search(\n    query: str,\n    category: str = None,\n    min_price: float = 0.0,\n    max_price: float = None,\n    page: int = 1,\n    page_size: int = 20,\n) -> dict:\n    \"\"\"\n    Search for products based on criteria.\n\n    Parameters:\n    query (str): The search query string.\n    category (str, optional): The category to filter the search. Default is None.\n    min_price (float, optional): The minimum price of the products to search. Default is 0.\n    max_price (float, optional): The maximum price of the products to search. Default is None.\n    page (int, optional): The page number for pagination. Default is 1.\n    page_size (int, optional): The number of results per page. Default is 20.\n\n    Returns:\n    dict: A dictionary containing the search results and pagination info.\n    \"\"\"\n\n    if query == \"\":\n        return {\"error\": \"missing query\"}\n\n    # Implement the function logic here\n    return {\"results\": [], \"pagination\": {\"total\": 0, \"page\": 1, \"page_size\": 20}}\n\n\n@tool\ndef customer_support(issue_type: str) -> dict:\n    \"\"\"\n    Get contact information for customer support regarding an issue.\n\n    Parameters:\n    issue_type (str): The type of issue (e.g., billing, technical support).\n\n    Returns:\n    dict: A dictionary containing the contact information for customer support.\n    \"\"\"\n\n    if issue_type == \"\":\n        return {\"error\": \"missing issue type\"}\n\n    # Implement the function logic here\n    return {\"contact\": issue_type}\n\n\n@tool\ndef track_package(tracking_number: int) -> dict:\n    \"\"\"\n    Track the status of a package based on the tracking number.\n\n    Parameters:\n    tracking_number (str): The tracking number of the package.\n\n    Returns:\n    dict: A dictionary containing the tracking status of the package.\n    \"\"\"\n    if tracking_number == \"\":\n        return {\"error\": \"missing tracking number\"}\n\n    # Implement the function logic here\n    return {\"status\": \"Delivered\"}\n\n\ntools = [\n    product_comparison,\n    product_search,\n    customer_support,\n    track_package,\n    apply_discount_code,\n    product_details,\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Tracer for Console and Phoenix in Python\nDESCRIPTION: This configures the OpenTelemetry tracer by adding two SpanProcessors.  One SpanProcessor prints traces to the console using ConsoleSpanExporter. The other sends traces to Phoenix using SimpleSpanProcessor and the phoenix_otlp_endpoint. register() initializes and returns the tracer provider.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/bedrock_tracing_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nphoenix_otlp_endpoint = urljoin(session_url, \"v1/traces\")\ntracer_provider = register()\ntracer_provider.add_span_processor(SimpleSpanProcessor(span_exporter=ConsoleSpanExporter()))\ntracer_provider.add_span_processor(SimpleSpanProcessor(endpoint=phoenix_otlp_endpoint))\n```\n\n----------------------------------------\n\nTITLE: Sampling and Renaming Data for Evaluation in Python\nDESCRIPTION: Randomly samples a subset (size N_EVAL_SAMPLE_SIZE) from the loaded benchmark dataset and renames columns to match template variable names ('input' for queries, 'reference' for documents). This prepares a DataFrame for LLM-based classification, reducing compute and facilitating multiple experiment iterations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf_sample = df.sample(n=N_EVAL_SAMPLE_SIZE).reset_index(drop=True)\ndf_sample = df_sample.rename(\n    columns={\n        \"query_text\": \"input\",\n        \"document_text\": \"reference\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix SDK and OpenAI Client via pip\nDESCRIPTION: This snippet shows how to install the required libraries for Phoenix SDK and OpenAI within your Python environment. It prepares dependencies for prompt management and completion tasks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/quickstart-prompts/quickstart-prompts-python.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-client openai\n```\n\n----------------------------------------\n\nTITLE: Defining Visualization Configuration and Chart Generation Tools - Python\nDESCRIPTION: Defines a Pydantic model for chart configuration, and two Phoenix-traced toolsâ€”one for extracting a chart configuration with the help of the LLM, and another for generating Python chart code from that configuration. A third tool (generate_visualization) chains both steps, producing code for visualizing a dataset with respect to a user goal. Required dependencies: pydantic, LLM access, and prior setup of the Phoenix tracer. Typically receives structured or string data and a visualization goal; outputs Python chart code as a string.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass VisualizationConfig(BaseModel):\n    chart_type: str = Field(..., description=\"Type of chart to generate\")\n    x_axis: str = Field(..., description=\"Name of the x-axis column\")\n    y_axis: str = Field(..., description=\"Name of the y-axis column\")\n    title: str = Field(..., description=\"Title of the chart\")\n\n\n@tracer.chain()\ndef extract_chart_config(data: str, visualization_goal: str) -> dict:\n    \"\"\"Generate chart visualization configuration\n\n    Args:\n        data: String containing the data to visualize\n        visualization_goal: Description of what the visualization should show\n\n    Returns:\n        Dictionary containing line chart configuration\n    \"\"\"\n    prompt = f\"\"\"Generate a chart configuration based on this data: {data}\n    The goal is to show: {visualization_goal}\"\"\"\n\n    response = client.beta.chat.completions.parse(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        response_format=VisualizationConfig,\n    )\n\n    try:\n        # Extract axis and title info from response\n        content = response.choices[0].message.content\n\n        # Return structured chart config\n        return {\n            \"chart_type\": content.chart_type,\n            \"x_axis\": content.x_axis,\n            \"y_axis\": content.y_axis,\n            \"title\": content.title,\n            \"data\": data,\n        }\n    except Exception:\n        return {\n            \"chart_type\": \"line\",\n            \"x_axis\": \"date\",\n            \"y_axis\": \"value\",\n            \"title\": visualization_goal,\n            \"data\": data,\n        }\n\n\n@tracer.chain()\ndef create_chart(config: VisualizationConfig) -> str:\n    \"\"\"Create a chart based on the configuration\"\"\"\n    prompt = f\"\"\"Write python code to create a chart based on the following configuration.\n    Only return the code, no other text.\n    config: {config}\"\"\"\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n    )\n\n    code = response.choices[0].message.content\n    code = code.replace(\"```python\", \"\").replace(\"```\", \"\")\n    code = code.strip()\n\n    return code\n\n\n@tracer.tool()\ndef generate_visualization(data: str, visualization_goal: str) -> str:\n    \"\"\"Generate a visualization based on the data and goal\"\"\"\n    config = extract_chart_config(data, visualization_goal)\n    code = create_chart(config)\n    return code\n```\n\n----------------------------------------\n\nTITLE: Instantiating OpenAIModel for GPT-4 Relevance Evaluation in Python\nDESCRIPTION: Creates an instance of Phoenix's OpenAIModel set to use GPT-4 with temperature 0.0. This model is used for generating relevance classifications with minimal randomness, supporting high consistency in experiments. Ensure OpenAI API key is configured before use.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\n```\n\n----------------------------------------\n\nTITLE: Instantiating GPT-4 Model for Hallucination Detection\nDESCRIPTION: Creates an instance of OpenAIModel configured to use GPT-4 with zero temperature for deterministic results in hallucination classification.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating GPT-3.5 Classifications: Metrics and Confusion Matrix - Python\nDESCRIPTION: Evaluates performance of GPT-3.5 Turbo-based classification compared to human groundtruth. Prints classification metrics and visualizes the result as a normalized confusion matrix. Assumes 'true_labels', 'relevance_classifications', and 'rails' variables are defined.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df[\"true_value\"].map(HUMAN_VS_AI_PROMPT_RAILS_MAP).tolist()\n\nprint(classification_report(true_labels, relevance_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels, predict_vector=relevance_classifications, classes=rails\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix OTEL Tracing With Python Application\nDESCRIPTION: Imports the Phoenix OTEL register function and creates a tracer provider configuration for the application. The 'project_name' parameter labels the tracing project, while 'auto_instrument' enables automatic instrumentation of supported dependencies. This is necessary to forward API call traces to Phoenix for monitoring.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving trace dataset as a pandas DataFrame\nDESCRIPTION: Acquires trace data via Phoenix client and displays the first few rows of the dataset. Facilitates detailed inspection of recorded LLM spans and function call details for debugging and monitoring.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_openai_agent_tracing_tutorial.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nds = px.Client().get_trace_dataset()\n\nds.dataframe.head()\n```\n\n----------------------------------------\n\nTITLE: Invoking Agent with Multiple Queries - Python\nDESCRIPTION: Runs the agent multiple times with a list of different input queries. Each invocation is performed within a loop, demonstrating how the agent handles various inputs and potentially uses its tools. Each query execution generates a separate trace in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_agent_tracing_tutorial.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nqueries = [\n    \"What is (121 * 3) + 42?\",\n    \"what is 3 * 3?\",\n    \"what is 4 * 4?\",\n    \"what is 75 * (3 + 4)?\",\n    \"what is 23 times 87\",\n]\n\nfor query in queries:\n    print(f\"> {query}\")\n    response = agent_executor.invoke({\"input\": query})\n    print(response)\n    print(\"---\")\n```\n\n----------------------------------------\n\nTITLE: Launching Arize Phoenix App\nDESCRIPTION: Launches the Arize Phoenix application, which serves as an OTEL collector for the generated spans.  This allows for tracing and observability of the LlamaIndex application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_multimodal_image_reasoning.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Defining Corpus Schema with px.Schema and EmbeddingColumnNames\nDESCRIPTION: This snippet shows the setup of a schema for corpus data, specifying the identification and embedding columns. It uses `px.Schema` with `EmbeddingColumnNames` to prepare data for inference operations in Phoenix. Dependencies include the `px` library, which provides schema and column definitions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/corpus-data.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ncorpus_schema = px.Schema(\n    id_column_name=\"id\",\n    document_column_names=EmbeddingColumnNames(\n        vector_column_name=\"embedding\",\n        raw_data_column_name=\"text\",\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Schemas and Launching App with Query and Corpus Embeddings in Python\nDESCRIPTION: Defines the schema for query and corpus datasets specifying which columns contain prompt text, embeddings vectors, responses, and evaluation tags. Then closes any existing Phoenix app and launches a new session with the processed inference data for queries and corpus embeddings. This enables interactive visualization and inspection of embedding clusters, query results, and associated evaluation metrics via Phoenix's UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nquery_schema = px.Schema(\n    prompt_column_names=px.EmbeddingColumnNames(\n        raw_data_column_name=\"query\", vector_column_name=\"vector\"\n    ),\n    response_column_names=\"response\",\n    tag_column_names=[\n        \"hallucination_label\",\n        \"hallucination_score\",\n        \"qa_correctness_label\",\n        \"qa_correctness_score\",\n    ],\n)\ncorpus_schema = px.Schema(\n    prompt_column_names=px.EmbeddingColumnNames(\n        raw_data_column_name=\"text\", vector_column_name=\"vector\"\n    )\n)\n# relaunch phoenix with a primary and corpus dataset to view embeddings\npx.close_app()\nsession = px.launch_app(\n    primary=px.Inferences(query_df, query_schema, \"query\"),\n    corpus=px.Inferences(corpus_df.reset_index(drop=True), corpus_schema, \"corpus\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Tracing Functions with Decorators (Python)\nDESCRIPTION: This code demonstrates how to use the `@tracer.chain` decorator to trace a function. The decorator automatically captures the input arguments and return value as attributes of the trace span.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-python.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@tracer.chain\ndef my_func(input: str) -> str:\n    return \"output\"\n```\n\n----------------------------------------\n\nTITLE: Installing OpenInference and OpenAI Instrumentation (Bash)\nDESCRIPTION: This bash snippet uses a package manager like npm to install OpenInference instrumentation libraries and the OpenAI SDK, which are required for capturing traces of OpenAI API calls with semantic context. It specifically adds the OpenAI client, OpenInference's OpenAI instrumentation, and semantic conventions package to facilitate standardized trace labeling and export. Run this command prior to initializing tracing in your application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# npm, pnpm, yarn, etc\nnpm install openai @arizeai/openinference-instrumentation-openai @arizeai/openinference-semantic-conventions\n\n```\n\n----------------------------------------\n\nTITLE: Appending Few-Shot Examples to Empathy Evaluation Prompt - Python\nDESCRIPTION: This snippet appends a multi-example few-shot training string to the standard empathy evaluation prompt template. By augmenting the prompt, it supplies labeled examples at varying scores (low, medium, high) to improve the LLM's discernment during automated judging. It requires the previously defined variable EMPATHY_EVALUATION_PROMPT_TEMPLATE and does not perform any computation itself; rather, it prepares the improved prompt for downstream usage.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfew_shot_examples = \"\"\"\n---\nTo help guide your reasoning, compare the text to the following examples and scorings:\n\nExample 1: \"You should try to focus and get your work done. Everyone has tough days.\"\n  Score: 2\n  Explanation: No acknowledgment of the userâ€™s emotions, dismissive and offers no real support\n\nExample 2: \"That sounds really stressful. Have you considered talking to a supervisor or taking breaks?\"\n  Score: 5\n  Explanation: Acknowledges stress, but in a generic way. Provides advice, but not very personal. Could be warmer in tone.\n\nExample 3: \"Iâ€™m really sorry youâ€™re feeling this way. Itâ€™s completely understandable to feel overwhelmed. Youâ€™re not alone in this. Have you had a chance to take a break or talk to someone who can support you?\"\n  Score: 9\n  Explanation: Validates emotions, reassures the user, and offers support\n\"\"\"\nEMPATHY_EVALUATION_PROMPT_TEMPLATE_IMPROVED = EMPATHY_EVALUATION_PROMPT_TEMPLATE + few_shot_examples\n```\n\n----------------------------------------\n\nTITLE: Basic usage of Phoenix Inferences in Python\nDESCRIPTION: Creates an Inferences object from a pandas DataFrame and schema. This is a simple example of initializing the inferences object without providing a custom name.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/inference-and-schema.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nds = px.Inferences(df, schema)\n```\n\n----------------------------------------\n\nTITLE: Logging Evaluations to Phoenix\nDESCRIPTION: Sends the evaluation metrics to Phoenix for visualization alongside the previously captured traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import DocumentEvaluations, SpanEvaluations\n\npx.Client().log_evaluations(\n    SpanEvaluations(dataframe=ndcg_at_2, eval_name=\"ndcg@2\"),\n    SpanEvaluations(dataframe=precision_at_2, eval_name=\"precision@2\"),\n    DocumentEvaluations(dataframe=retrieved_documents_relevance_df, eval_name=\"relevance\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Computing Retrieval Hit Rate (Correct Document Retrieved) in Python\nDESCRIPTION: Determines whether a correct document (hit) was retrieved at all for each query by checking if the sum of the top two evaluation scores exceeds zero. Grouped by span ID, this boolean hit metric indicates overall retrieval success per query, aiding in downstream performance assessments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nhit = pd.DataFrame(\n    {\n        \"hit\": documents_with_relevance_df.groupby(\"context.span_id\").apply(\n            lambda x: x.eval_score[:2].sum(skipna=False) > 0\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating LLM Responses for QA Correctness and Hallucination\nDESCRIPTION: This snippet sets up and runs evaluations for QA correctness and hallucination on the generated LLM responses. It initializes `QAEvaluator` and `HallucinationEvaluator` from `phoenix.evals`, configuring them to use an OpenAI model (`gpt-4-turbo-preview`). The `run_evals` function executes these evaluators concurrently on the `qa_with_reference_df`, providing explanations for the scores.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    HallucinationEvaluator,\n    OpenAIModel,\n    QAEvaluator,\n    run_evals,\n)\n\nqa_evaluator = QAEvaluator(OpenAIModel(model_name=\"gpt-4-turbo-preview\"))\nhallucination_evaluator = HallucinationEvaluator(OpenAIModel(model_name=\"gpt-4-turbo-preview\"))\n\nqa_correctness_eval_df, hallucination_eval_df = run_evals(\n    evaluators=[qa_evaluator, hallucination_evaluator],\n    dataframe=qa_with_reference_df,\n    provide_explanation=True,\n    concurrency=20,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Arize Phoenix Evaluation Notebook - Python\nDESCRIPTION: Installs required Python packages including arize-phoenix, openai, matplotlib, pycm, scikit-learn, tiktoken, openinference-instrumentation-openai, and a specific httpx version. Run this cell as the first step in the notebook; dependencies must be installed for subsequent code to work. No input parameters required, but must be run in an environment with pip access, such as Colab.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Requires arize-phoenix as it usees UI / tracing\n!pip install -qq \"arize-phoenix\" \"openai>=1\" ipython matplotlib pycm scikit-learn tiktoken openinference-instrumentation-openai 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Initializing Phoenix OpenAIModel in Python\nDESCRIPTION: Creates an instance of the `OpenAIModel` class from the `phoenix.evals` library. It configures the instance to use the 'gpt-4o' model and sets a maximum token limit of 1300 for the generated responses.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model=\"gpt-4o\", max_tokens=1300)\n```\n\n----------------------------------------\n\nTITLE: Installing Libraries with Pip in Bash\nDESCRIPTION: This snippet installs the necessary Python libraries for tracing, evaluation, and agent interaction. It includes Arize Phoenix, OpenTelemetry instrumentation for OpenAI and OpenAI Agents, Ragas, and Langchain-OpenAI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/ragas_agents_cookbook_phoenix.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install -q \"arize-phoenix>=8.0.0\" arize-phoenix-otel openinference-instrumentation-openai-agents openinference-instrumentation-openai arize-phoenix-evals \"arize[Datasets]\" --upgrade\n\n!pip install langchain-openai\n\n!pip install -q openai opentelemetry-sdk opentelemetry-exporter-otlp gcsfs nest_asyncio ragas openai-agents --upgrade\n```\n\n----------------------------------------\n\nTITLE: Running LlamaIndex Query Engine and Capturing Inference Data (Python)\nDESCRIPTION: Initializes an `OpenInferenceCallbackHandler` to automatically capture inference data in the OpenInference format for observability. Configures a LlamaIndex `ServiceContext` using OpenAI models for LLM and embeddings, incorporating the callback handler. Creates a `VectorStoreIndex` using the configured Milvus vector store and service context, and then builds a `query_engine` from the index. Finally, it iterates through a list of example queries, executes each using the query engine, and prints the query and the generated response, demonstrating the functional Q&A service.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/milvus_llamaindex_search_and_retrieval_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncallback_handler = OpenInferenceCallbackHandler()\n\nservice_context = ServiceContext.from_defaults(\n    llm=OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n    embed_model=OpenAIEmbedding(model=\"text-embedding-ada-002\"),\n    callback_manager=CallbackManager(handlers=[callback_handler]),\n)\n\nindex = VectorStoreIndex.from_vector_store(vector_store, service_context=service_context);\n\nquery_engine = index.as_query_engine()\n\nmax_line_length = 80\nfor query in [\n    \"How do I get an Arize API key?\",\n    \"Can I create monitors with an API?\",\n    \"How do I need to format timestamps?\",\n    \"What is the price of the Arize platform\",\n]:\n    print(\"Query\")\n    print(\"=====\")\n    print()\n    print(textwrap.fill(query, max_line_length))\n    print()\n    response = query_engine.query(query)\n    print(\"Response\")\n    print(\"======%%\")\n    print()\n    print(textwrap.fill(str(response), max_line_length))\n    print()\n```\n\n----------------------------------------\n\nTITLE: Defining and Saving a Base Prompt Template in Phoenix - Python\nDESCRIPTION: Defines a base system prompt and parameters for GPT 3.5 Turbo using OpenAI's CompletionCreateParamsBase, and persists the template in Phoenix for version tracking and reuse. The prompt instructs the model to classify input as 'benign' or 'jailbreak.' Dependencies: openai, phoenix.client. Key parameters include model name, temperature (set to 0), and message template. Outputs a prompt object stored in Phoenix, linked to a unique identifier.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom openai.types.chat.completion_create_params import CompletionCreateParamsBase\n\nfrom phoenix.client.types import PromptVersion\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an evaluator that decides whether a given prompt is a jailbreak risk. Only output 'benign' or 'jailbreak', no other words.\",\n        },\n        {\"role\": \"user\", \"content\": \"{{prompt}}\"},\n    ],\n)\n\nprompt_identifier = \"jailbreak-classification\"\n\nprompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"A prompt for classifying whether a given prompt is a jailbreak risk.\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Printing Phoenix Session URL\nDESCRIPTION: Prints the URL of the active Phoenix UI session. This allows the user to easily access the UI in a web browser to visualize the traces and the newly added evaluation annotations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_quickstart.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"ðŸ”¥ðŸ¦ Open back up Phoenix in case you closed it: {session.url}\")\n```\n\n----------------------------------------\n\nTITLE: Running Extractions Python\nDESCRIPTION: Iterates through the list of `travel_requests`, calls the `extract_raw_travel_request_attributes_string` function for each request to extract the attributes, and prints both the input and the extracted raw attributes.  This step demonstrates the core functionality of the structured data extraction.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/anthropic_tracing_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nraw_travel_attributes_column = []\nfor travel_request in travel_requests:\n    print(\"Travel request:\")\n    print(\"===============\")\n    print(travel_request)\n    print()\n    raw_travel_attributes = extract_raw_travel_request_attributes_string(\n        travel_request, tool_schema, system_message, client\n    )\n    raw_travel_attributes_column.append(raw_travel_attributes)\n    print(\"Raw Travel Attributes:\")\n    print(\"=====================\")\n    print(raw_travel_attributes)\n    print()\n    print()\n\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Schema with Feature Columns and Labels in Python\nDESCRIPTION: This snippet demonstrates how to define a Phoenix schema using the `px.Schema` constructor. It specifies the feature columns, the actual label column, and the prediction label column. This schema is then used to tell Phoenix what the data in the dataframe means. The columns listed under `feature_column_names` will be treated as features by Phoenix, `actual_label_column_name` will be treated as the ground truth, and `prediction_label_column_name` will be treated as predictions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/inferences.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nschema = px.Schema(\n    feature_column_names=[\n        \"sepal_length\",\n        \"sepal_width\",\n        \"petal_length\",\n        \"petal_width\",\n    ],\n    actual_label_column_name=\"target\",\n    prediction_label_column_name=\"prediction\",\n)\n```\n\n----------------------------------------\n\nTITLE: Making OpenAI API Call in Deno with Tracing\nDESCRIPTION: Creates an instance of the OpenAI client, which automatically uses the `OPENAI_API_KEY` environment variable for authentication by default. It then asynchronously makes a simple chat completion request with a basic user message. This API call will be automatically traced by the instrumentation set up previously. The code includes basic error handling and prints the resulting message to the console.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/tracing_openai_node_tutorial.ipynb#_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst client = new OpenAI({\n  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted\n});\n\nasync function main() {\n try {\n  const chatCompletion = await client.chat.completions.create({\n    messages: [{ role: 'user', content: 'Say this is a test' }],\n    model: 'gpt-3.5-turbo',\n  });\n  console.dir(chatCompletion.choices[0].message);\n } catch (e) {\n   console.error(e);\n }\n}\n\nawait main();\n```\n\n----------------------------------------\n\nTITLE: Instantiating - Phoenix Code Evaluator - Python\nDESCRIPTION: Demonstrates how to instantiate a pre-built evaluator provided by Phoenix that runs code-based logic. This example uses the 'ContainsAnyKeyword' evaluator, configured to check if the task output contains specific keywords.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/datasets_and_experiments_quickstart.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments.evaluators import ContainsAnyKeyword\n\ncontains_keyword = ContainsAnyKeyword(keywords=[\"Y Combinator\", \"YC\"])\n```\n\n----------------------------------------\n\nTITLE: Defining Chart Config Evaluator\nDESCRIPTION: Defines an evaluator for comparing the chart configuration generated by the agent against the expected configuration. This function compares the `chart_config` field within the output object with the expected `chart_config`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_chart_config(output: str, expected: str) -> bool:\n    return output.get(\"chart_config\") == expected.get(\"chart_config\")\n```\n\n----------------------------------------\n\nTITLE: Aligning Query and Database Embeddings in Python\nDESCRIPTION: Centers the query and database embedding distributions by subtracting their respective centroids. This technique makes the two distributions overlap in the embedding space, which is important for visualizing relationships in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_search_and_retrieval_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndatabase_centroid = database_df[\"text_vector\"].mean()\ndatabase_df[\"text_vector\"] = database_df[\"text_vector\"].apply(lambda x: x - database_centroid)\n\nquery_centroid = query_df[\":feature.[float].embedding:prompt\"].mean()\nquery_df[\":feature.[float].embedding:prompt\"] = query_df[\":feature.[float].embedding:prompt\"].apply(\n    lambda x: x - query_centroid\n)\n```\n\n----------------------------------------\n\nTITLE: Combining Questions and Document Chunks for Evaluation Dataset - Python\nDESCRIPTION: Combines the DataFrame of generated questions with the original document chunks DataFrame. It then reshapes the data using `melt` to create pairs of (question, document chunk text), suitable for use as an evaluation dataset where each question is paired with its source context.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Construct a dataframe of the questions and the document chunks\\nquestions_with_document_chunk_df = pd.concat([questions_df, document_chunks_df], axis=1)\\nquestions_with_document_chunk_df = questions_with_document_chunk_df.melt(\\n    id_vars=[\"text\"], value_name=\"question\"\\n).drop(\"variable\", axis=1)\n```\n\n----------------------------------------\n\nTITLE: Importing Basic Libraries for Data Processing and Visualization\nDESCRIPTION: Imports standard Python libraries for operating system interaction, user input, data visualization, and dataframe manipulation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport matplotlib.pyplot as plt\nimport openai\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Loading Data and Building Index - Python\nDESCRIPTION: This code loads data from a text file, builds a VectorStoreIndex using LlamaIndex, and configures the LLM and embedding model to use MistralAI. The index is created with chunk size 512. The `using_project` context manager is used to collect traces under a specific Phoenix project name.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\nfrom urllib.request import urlretrieve\n\nwith tempfile.NamedTemporaryFile() as tf:\n    urlretrieve(\n        \"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/main/data/paul_graham/paul_graham_essay.txt\",\n        tf.name,\n    )\n    documents = SimpleDirectoryReader(input_files=[tf.name]).load_data()\n\n# Define an LLM\nllm = MistralAI(model=\"mistral-large-latest\")\nSettings.llm = llm\nSettings.embed_model = MistralAIEmbedding()\n\nwith using_project(INDEXING_PROJECT):  # Collect traces under the project \"indexing\"\n    # Build index with a chunk_size of 512\n    node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n    nodes = node_parser.get_nodes_from_documents(documents)\n    vector_index = VectorStoreIndex(nodes)\n```\n\n----------------------------------------\n\nTITLE: Logging Evaluations to Phoenix\nDESCRIPTION: Logs evaluation results to Phoenix for visualization. It uses the `phoenix.trace` module to create `SpanEvaluations` and `DocumentEvaluations` objects from Pandas DataFrames and sends them to Phoenix using `px.Client().log_evaluations()`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import DocumentEvaluations, SpanEvaluations\n\npx.Client().log_evaluations(\n    SpanEvaluations(dataframe=ndcg_at_2, eval_name=\"ndcg@2\"),\n    SpanEvaluations(dataframe=precision_at_2, eval_name=\"precision@2\"),\n    DocumentEvaluations(dataframe=retrieved_documents_relevance_df, eval_name=\"relevance\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Log Evaluations to Phoenix\nDESCRIPTION: This snippet logs the retrieval evaluation results to Phoenix for visualization. It uses the `px.Client().log_evaluations` function, specifying the dataframes, evaluation names and types (SpanEvaluations and DocumentEvaluations). This step makes the evaluation results available within the Phoenix UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import DocumentEvaluations, SpanEvaluations\n\npx.Client().log_evaluations(\n    SpanEvaluations(dataframe=ndcg_at_2, eval_name=\"ndcg@2\"),\n    SpanEvaluations(dataframe=precision_at_2, eval_name=\"precision@2\"),\n    DocumentEvaluations(dataframe=retrieved_documents_relevance_df, eval_name=\"relevance\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating LLM Response Quality\nDESCRIPTION: Uses QA and Hallucination evaluators to assess the correctness of LLM responses and detect potential hallucinations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    HallucinationEvaluator,\n    OpenAIModel,\n    QAEvaluator,\n    run_evals,\n)\n\nqa_evaluator = QAEvaluator(OpenAIModel(model=\"gpt-4-turbo-preview\"))\nhallucination_evaluator = HallucinationEvaluator(OpenAIModel(model=\"gpt-4-turbo-preview\"))\n\nqa_correctness_eval_df, hallucination_eval_df = run_evals(\n    evaluators=[qa_evaluator, hallucination_evaluator],\n    dataframe=qa_with_reference_df,\n    provide_explanation=True,\n    concurrency=20,\n)\n```\n\n----------------------------------------\n\nTITLE: Formatting evaluation dataframe for upload (Python)\nDESCRIPTION: Ensures the 'score' and 'label' columns in the evaluation DataFrame have the correct data types (integer for score, string for label) before uploading to Phoenix. This step is crucial for proper interpretation and display in the UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/feedback-and-annotations/evaluating-phoenix-traces.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\neval_df[\"score\"] = eval_df[\"score\"].astype(int)\neval_df[\"label\"] = eval_df[\"label\"].astype(str)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry and Phoenix Dependencies\nDESCRIPTION: Command to install necessary npm packages for OpenTelemetry and Phoenix integration, including tracing SDKs and semantic conventions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-ts.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# npm, pnpm, yarn, etc\nnpm install @arizeai/openinference-semantic-conventions @opentelemetry/semantic-conventions @opentelemetry/api @opentelemetry/instrumentation @opentelemetry/resources @opentelemetry/sdk-trace-base @opentelemetry/sdk-trace-node @opentelemetry/exporter-trace-otlp-proto\n```\n\n----------------------------------------\n\nTITLE: Using a Pre-built Conciseness Evaluator with OpenAIModel in Python\nDESCRIPTION: This snippet creates a conciseness evaluator that leverages an OpenAI language model for evaluation. It imports `ConcisenessEvaluator` and wraps an OpenAI GPT-4o model, enabling natural language-based grading on task outputs. This requires Phoenix's evaluation framework and OpenAI access.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/quickstart-datasets.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments.evaluators import ConcisenessEvaluator\nfrom phoenix.evals.models import OpenAIModel\n\nmodel = OpenAIModel(model=\"gpt-4o\")\nconciseness = ConcisenessEvaluator(model=model)\n```\n\n----------------------------------------\n\nTITLE: Generating Ragas Evaluation Dataset (Python)\nDESCRIPTION: Defines functions `generate_response` to query the RAG engine and extract answer/contexts, and `generate_ragas_dataset` to iterate through test questions, call the query engine, and format the results into a `datasets.Dataset` object compatible with Ragas evaluation metrics, including ground truth.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\n\nfrom phoenix.trace import using_project\n\n\ndef generate_response(query_engine, question):\n    response = query_engine.query(question)\n    return {\n        \"answer\": response.response,\n        \"contexts\": [c.node.get_content() for c in response.source_nodes],\n    }\n\n\ndef generate_ragas_dataset(query_engine, test_df):\n    test_questions = test_df[\"question\"].values\n    responses = [generate_response(query_engine, q) for q in tqdm(test_questions)]\n\n    dataset_dict = {\n        \"question\": test_questions,\n        \"answer\": [response[\"answer\"] for response in responses],\n        \"contexts\": [response[\"contexts\"] for response in responses],\n        \"ground_truth\": test_df[\"ground_truth\"].values.tolist(),\n    }\n    ds = Dataset.from_dict(dataset_dict)\n    return ds\n\n\nwith using_project(\"llama-index\"):\n    ragas_eval_dataset = generate_ragas_dataset(query_engine, test_df)\n\nragas_evals_df = pd.DataFrame(ragas_eval_dataset)\nragas_evals_df.head(2)\n```\n\n----------------------------------------\n\nTITLE: Uploading Synthetic Dataset to Phoenix in Python\nDESCRIPTION: Initializes an Arize Phoenix client using `px.Client()`. Uploads the previously created `generated_dataset` to Phoenix using the `upload_dataset` method. Assigns the dataset name \"nba-golden-synthetic\" and structures the data by providing list comprehensions for inputs (containing the 'question') and outputs (containing the 'expected' dictionary with results, error, and query).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nsynthetic_dataset = px.Client().upload_dataset(\n    dataset_name=\"nba-golden-synthetic\",\n    inputs=[{\"question\": example[\"input\"]} for example in generated_dataset],\n    outputs=[example[\"expected\"] for example in generated_dataset],\n);\n```\n\n----------------------------------------\n\nTITLE: Defining the Initial LLM as a Judge Evaluation Task and Evaluator in Python\nDESCRIPTION: Defines two core functions: 'initial_prompt' which uses Phoenix's 'llm_classify' to get a score (1-10) from the LLM based on the empathy evaluation prompt template, and 'evaluate_response' which compares predicted scores against ground-truth empathy scores allowing a tolerance of Â±2. This setup specifies how model outputs will be scored and evaluated against dataset labels during experiments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nfrom phoenix.evals import (\n    OpenAIModel,\n    llm_classify,\n)\n\n\ndef initial_prompt(input):\n    response_classifications = llm_classify(\n        dataframe=pd.DataFrame([{\"AI_Response\": input[\"AI_Response\"]}]),\n        template=EMPATHY_EVALUATION_PROMPT_TEMPLATE,\n        model=OpenAIModel(model=\"gpt-4\"),\n        rails=list(map(str, range(1, 11))),\n        provide_explanation=True,\n    )\n    score = response_classifications.iloc[0][\"label\"]\n    return int(score)\n\n\ndef evaluate_response(input, output):\n    expected_score = input[\"EI_Empathy_Score\"]\n    predicted_score = output\n    return abs(expected_score - predicted_score) <= 2\n```\n\n----------------------------------------\n\nTITLE: Viewing Classifications with Explanations in Pandas DataFrame\nDESCRIPTION: This snippet merges the original data sample with the classifications and explanations generated by the LLM. The relevant columns - `coding_instruction`, `code`, `label`, and `explanation` - are then displayed in a Pandas DataFrame to view the classification details and the LLM's reasoning.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_functionality_classifications.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Let's view the data\nmerged_df = pd.merge(\n    small_df_sample, relevance_classifications_df, left_index=True, right_index=True\n)\nmerged_df[[\"coding_instruction\", \"code\", \"label\", \"explanation\"]].head()\n```\n\n----------------------------------------\n\nTITLE: Using llm_classify for Categorical Evaluation Supports Binary and Multi-Class\nDESCRIPTION: This code demonstrates how to perform classification-based evaluation using Phoenix's llm_classify function with an OpenAI model. The setup includes defining allowed output classes ('relevant' and 'irrelevant'), integrating a custom data frame, and configuring the model. It ensures the LLM output is mapped to predefined classes, handling cases where the output may be ambiguous or unparsable.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/bring-your-own-evaluator.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.evals import (\n    llm_classify,\n    OpenAIModel # see https://docs.arize.com/phoenix/evaluation/evaluation-models\n)\n\n# The rails is used to hold the output to specific values based on the template\n# It will remove text such as \",,,\" or \"...\"\n# Will ensure the binary value expected from the template is returned\nrails = [\"irrelevant\", \"relevant\"]\n#MultiClass would be rails = [\"irrelevant\", \"relevant\", \"semi-relevant\"]\nrelevance_classifications = llm_classify(\n    dataframe=<YOUR_DATAFRAME_GOES_HERE>,\n    template=CATEGORICAL_TEMPLATE,\n    model=OpenAIModel('gpt-4o', api_key=''),\n    rails=rails\n)\n```\n\n----------------------------------------\n\nTITLE: Preparing Data for Response Quality Evaluation\nDESCRIPTION: Sets up the evaluation phase by generating question-context pairs, which can be used to assess the effectiveness of the RAG pipeline in producing accurate responses based on retrieved data. Utilizes Phoenix's `llm_generate` utility for this purpose.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\n\n```\n\n----------------------------------------\n\nTITLE: Defining the Correctness Evaluation Function Using a Template and DataFrame\nDESCRIPTION: This function, correctness_eval, assesses whether a math problem's solution is correct by utilizing a predefined template to prompt a language model. It constructs a DataFrame with the question and response, applies the template for evaluation, and returns the evaluation DataFrame which includes labels and explanations. Dependencies include pandas and the llm_classify function, along with the OpenAIModel class. It outputs a DataFrame indicating correctness with explanations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/openai_agents_cookbook.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ndef correctness_eval(input, output):\n    # Template for evaluating math problem solutions\n    MATH_EVAL_TEMPLATE = \"\"\"\n    You are evaluating whether a math problem was solved correctly.\n\n    [BEGIN DATA]\n    ************\n    [Question]: {question}\n    ************\n    [Response]: {response}\n    [END DATA]\n\n    Assess if the answer to the math problem is correct. First work out the correct answer yourself,\n    then compare with the provided response. Consider that there may be different ways to express the same answer\n    (e.g., \"43\" vs \"The answer is 43\" or \"5.0\" vs \"5\").\n\n    Your answer must be a single word, either \"correct\" or \"incorrect\"\n    \"\"\"\n\n    # Run the evaluation\n    rails = [\"correct\", \"incorrect\"]\n    eval_df = llm_classify(\n        data=pd.DataFrame([{\"question\": input[\"question\"], \"response\": output[\"final_output\"]}]),\n        template=MATH_EVAL_TEMPLATE,\n        model=OpenAIModel(model=\"gpt-4.1\"),\n        rails=rails,\n        provide_explanation=True,\n    )\n\n    return eval_df\n```\n\n----------------------------------------\n\nTITLE: Instrumenting LlamaIndex with OpenInference and Phoenix\nDESCRIPTION: This code configures LlamaIndex instrumentation using OpenInference and Phoenix. It sets up a trace exporter to send trace data to a local endpoint, creates a TracerProvider, and instruments LlamaIndex to capture tracing data during execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\n\nendpoint = \"http://127.0.0.1:6006/v1/traces\"\ntracer_provider = TracerProvider()\ntracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n\nLlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI Client\nDESCRIPTION: This code instruments the OpenAI client to emit telemetry data in OpenInference format using `OpenAIInstrumentor`. This allows for capturing traces of interactions with the OpenAI API. The `tracer_provider` is registered and used to instrument the OpenAI client. This is essential for capturing LLM application traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_tracing_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register()\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Schema for Multimodal Data (Python)\nDESCRIPTION: This snippet defines a Phoenix schema for a multimodal dataset, combining text description and image embeddings for e-commerce products. It uses the 'name' column as a tag. It specifies two distinct embedding features: 'description_embedding' linking 'description_vector' to the raw 'description' text, and 'image_embedding' linking 'image_vector' to the 'image' URL, allowing visualization of both modalities in the app.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nschema = px.Schema(\n    tag_column_names=[\"name\"],\n    embedding_feature_column_names={\n        \"description_embedding\": px.EmbeddingColumnNames(\n            vector_column_name=\"description_vector\",\n            raw_data_column_name=\"description\",\n        ),\n        \"image_embedding\": px.EmbeddingColumnNames(\n            vector_column_name=\"image_vector\",\n            link_to_data_column_name=\"image\",\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Defining generate_response and send_feedback Functions\nDESCRIPTION: This code defines two core functions: `generate_response` and `send_feedback`. `generate_response` sends a request to the OpenAI API, manually instruments the API call using OpenTelemetry spans and attributes. It returns the API response and the span ID. `send_feedback` sends user feedback (thumbs up/down) to the Phoenix feedback endpoint, associated with a specific span ID. These functions manage the interaction with the OpenAI API, instrumentation, and feedback submission.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/human_feedback/chatbot_with_human_feedback.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nhttp_client = httpx.Client()\n\n\ndef generate_response(\n    input_text: str, model: str = \"gpt-4o\", temperature: float = 0.1\n) -> Dict[str, Any]:\n    user_message = {\"role\": \"user\", \"content\": input_text, \"uuid\": str(uuid4())}\n    invocation_parameters = {\"temperature\": temperature}\n    payload = {\n        \"model\": model,\n        **invocation_parameters,\n        \"messages\": [user_message],\n    }\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {openai_api_key}\",\n    }\n    with TRACER.start_as_current_span(\"chatbot with feedback example\") as span:\n        span.set_attribute(\n            SpanAttributes.OPENINFERENCE_SPAN_KIND, OpenInferenceSpanKindValues.LLM.value\n        )\n        span.set_attribute(SpanAttributes.LLM_MODEL_NAME, payload[\"model\"])\n        span.set_attribute(SpanAttributes.INPUT_VALUE, json.dumps(payload[\"messages\"][0]))\n        span.set_attribute(SpanAttributes.INPUT_MIME_TYPE, OpenInferenceMimeTypeValues.JSON.value)\n\n        # get the active hex-encoded spanID\n        span_id = span.get_span_context().span_id.to_bytes(8, \"big\").hex()\n        print(span_id)\n\n        response = http_client.post(OPENAI_API_URL, headers=headers, json=payload)\n\n        if not (200 <= response.status_code < 300):\n            raise Exception(f\"Failed to call OpenAI API: {response.text}\")\n        response_json = response.json()\n\n        span.set_attribute(SpanAttributes.OUTPUT_VALUE, json.dumps(response_json))\n        span.set_attribute(SpanAttributes.OUTPUT_MIME_TYPE, OpenInferenceMimeTypeValues.JSON.value)\n\n    return response_json, span_id\n\n\ndef send_feedback(span_id: str, feedback: int) -> None:\n    label = \"ðŸ‘\" if feedback == 1 else \"ðŸ‘Ž\"\n    request_body = {\n        \"data\": [\n            {\n                \"span_id\": span_id,\n                \"name\": \"user_feedback\",\n                \"annotator_kind\": \"HUMAN\",\n                \"result\": {\"label\": label, \"score\": feedback},\n                \"metadata\": {},\n            }\n        ]\n    }\n\n    try:\n        response = http_client.post(FEEDBACK_ENDPOINT, json=request_body)\n        if not (200 <= response.status_code < 300):\n            raise Exception(f\"Failed to send feedback: {response.text}\")\n        print(f\"Feedback sent for span_id {span_id}: {label}\")\n    except httpx.ConnectError:\n        warnings.warn(\"Could not connect to feedback server.\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Evaluators and Running LLM Data Evaluations with Phoenix - Python\nDESCRIPTION: Initializes evaluation models and evaluators for hallucination detection and Q&A correctness using Phoenix with OpenAI's gpt-4o. Prepares the DataFrame with required columns, applies concurrency patch for notebooks with nest_asyncio, and executes the evaluations. Expects the DataFrame to have specific columns for each evaluator. Returns evaluation DataFrames with result labels and explanations. Required dependencies: 'phoenix.evals', 'openai', 'nest_asyncio', and a valid OpenAI API key.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/evals.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.evals import HallucinationEvaluator, OpenAIModel, QAEvaluator, run_evals\n\nnest_asyncio.apply()  # This is needed for concurrency in notebook environments\n\n# Set your OpenAI API key\neval_model = OpenAIModel(model=\"gpt-4o\")\n\n# Define your evaluators\nhallucination_evaluator = HallucinationEvaluator(eval_model)\nqa_evaluator = QAEvaluator(eval_model)\n\n# We have to make some minor changes to our dataframe to use the column names expected by our evaluators\n# for `hallucination_evaluator` the input df needs to have columns 'output', 'input', 'context'\n# for `qa_evaluator` the input df needs to have columns 'output', 'input', 'reference'\ndf[\"context\"] = df[\"reference\"]\ndf.rename(columns={\"query\": \"input\", \"response\": \"output\"}, inplace=True)\nassert all(column in df.columns for column in [\"output\", \"input\", \"context\", \"reference\"])\n\n# Run the evaluators, each evaluator will return a dataframe with evaluation results\n# We upload the evaluation results to Phoenix in the next step\nhallucination_eval_df, qa_eval_df = run_evals(\n    dataframe=df, evaluators=[hallucination_evaluator, qa_evaluator], provide_explanation=True\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Text from Retrieved Source Nodes (Python)\nDESCRIPTION: Retrieves the raw text content from the first node in the list of similar or relevant nodes returned by the RAG system. Useful for inspecting the original context retrieved for the answer. Expects 'response_vector' to contain a list attribute 'source_nodes'; index [0] accesses the first node.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresponse_vector.source_nodes[0].get_text()\n```\n\n----------------------------------------\n\nTITLE: Running Q&A Classification with GPT-4 Turbo\nDESCRIPTION: Executes the classification task using GPT-4 Turbo to compare its performance with the standard GPT-4 and GPT-3.5 Turbo models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nQ_and_A_classifications = llm_classify(\n    dataframe=df_sample,\n    template=QA_PROMPT_TEMPLATE,\n    model=model,\n    rails=list(QA_PROMPT_RAILS_MAP.values()),\n    concurrency=20,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Running Experiment with Meta Prompt\nDESCRIPTION: The code executes an experiment within the `Phoenix` framework, using the `test_prompt` function. It calls `run_experiment` with parameters like the dataset, the task function, and an evaluator. The experiment description is set to describe the technique, and the name will be used to identify the run. The experiment_metadata includes the prompt ID from Phoenix, associating the experiment with the meta prompt generated.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nmeta_prompting_experiment = run_experiment(\n    dataset,\n    task=test_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Prompt Optimization Technique #2: Meta Prompting\",\n    experiment_name=\"meta-prompting\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + meta_prompt_result.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenAI API key with environment variable fallback\nDESCRIPTION: Retrieves the OpenAI API key from environment variables or prompts the user securely if not set. Sets necessary environment variables to authenticate API requests for subsequent agent operations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_openai_agent_tracing_tutorial.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix and LiteLLM Dependencies in Python\nDESCRIPTION: Installs the required Python packages 'arize-phoenix', 'litellm', and 'openinference-instrumentation-litellm' using pip, enabling the use of Phoenix evaluation tools and LiteLLM model instrumentation. This snippet should be run prior to importing and using these packages.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qq \"arize-phoenix\" \"litellm\" openinference-instrumentation-litellm\n```\n\n----------------------------------------\n\nTITLE: Creating a New Prompt with Phoenix Client in Python\nDESCRIPTION: This snippet demonstrates how to create a new prompt with a specified name, description, and version using the Phoenix SDK in Python. It defines the prompt content with variables, associates it with a language model, and registers it under a given name for future use.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/quickstart-prompts/quickstart-prompts-python.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.client import Client\nfrom phoenix.client.types import PromptVersion\n\ncontent = \"\"\"\\nYou're an expert educator in {{ topic }}. Summarize the following article\\nin a few concise bullet points that are easy for beginners to understand.\n\n{{ article }}\"\"\"\n\nprompt_name = \"article-bullet-summarizer\"\nprompt = Client().prompts.create(\n    name=prompt_name,\n    prompt_description=\"Summarize an article in a few bullet points\",\n    version=PromptVersion(\n        [{\"role\": \"user\", \"content\": content}],\n        model_name=\"gpt-4o-mini\",\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Instrumentation with OpenTelemetry in TypeScript\nDESCRIPTION: Demonstrates how to import and register the `OpenAIInstrumentation` with the OpenTelemetry Node.js SDK. It initializes a `NodeTracerProvider` and uses `registerInstrumentations` to enable automatic tracing for OpenAI API calls. Depends on `@opentelemetry/sdk-trace-node`, `@arizeai/openinference-instrumentation-openai`, and `@opentelemetry/instrumentation`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai-node-sdk.md#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { NodeTracerProvider } from \"@opentelemetry/sdk-trace-node\";\nimport {\n  OpenAIInstrumentation,\n} from \"@arizeai/openinference-instrumentation-openai\";\nimport { registerInstrumentations } from \"@opentelemetry/instrumentation\";\n\nconst provider = new NodeTracerProvider();\nprovider.register();\n\nregisterInstrumentations({\n  instrumentations: [new OpenAIInstrumentation()],\n});\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix Tracer in Python\nDESCRIPTION: Initializes and configures the Phoenix tracer for LLM observability by calling 'register' with a specified project name and enabling auto-instrumentation. Requires 'phoenix.otel' to be importable and any required environment variables to be set. Outputs a tracer provider object for use in further observability flows.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/groq.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Applying Agent Executor to DataFrame for Inference - Python\nDESCRIPTION: Applies the agent_executor's invoke method to each question in the 'questions' column of the questions_df dataframe, storing the resulting responses in a new column named 'response'. Requires a previously defined agent_executor object, and an existing questions_df pandas DataFrame with a 'questions' column. Input is the dataframe; output is a new 'response' column containing the agent-invoked results per question.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nquestions_df[\"response\"] = questions_df[\"questions\"].apply(\n    lambda x: agent_executor.invoke({\"input\": x})\n)\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Evaluator to Phoenix Experiment in Python\nDESCRIPTION: Applies the custom `evaluate_path_length` evaluator to the results stored in the `experiment` object using the `evaluate_experiment` function from the Phoenix framework. This adds the calculated evaluation scores to the experiment data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nexperiment = evaluate_experiment(experiment, evaluators=[evaluate_path_length])\n```\n\n----------------------------------------\n\nTITLE: Downloading and Uploading a Sample Llama Dataset to Phoenix in Python\nDESCRIPTION: Downloads a specific Llama dataset locally to a temporary directory and then uploads a random sample of its entries to the Phoenix client as a dataframe dataset. The sample size and dataset name are configurable. The dataset upload includes generating a unique dataset name using a timestamp.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsample_size = 7\ndataset_name = \"EvaluatingLlmSurveyPaperDataset\"\nwith tempfile.TemporaryDirectory() as dir_name:\n    rag_dataset, documents = download_llama_dataset(dataset_name, dir_name)\ndataset = px.Client().upload_dataset(\n    dataset_name=f\"{dataset_name}_{time_ns()}\",\n    dataframe=rag_dataset.to_pandas().sample(sample_size, random_state=42),\n)\n```\n\n----------------------------------------\n\nTITLE: Building a Haystack RAG Pipeline\nDESCRIPTION: Creates a Retrieval-Augmented Generation pipeline using Haystack components: a document store with sample data, a BM25 retriever, a prompt builder with template, and an OpenAI LLM generator.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag_haystack.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom haystack import Document, Pipeline\nfrom haystack.components.builders.prompt_builder import PromptBuilder\nfrom haystack.components.generators import OpenAIGenerator\nfrom haystack.components.retrievers.in_memory import InMemoryBM25Retriever\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\n\n# Write documents to InMemoryDocumentStore\ndocument_store = InMemoryDocumentStore()\ndocument_store.write_documents(\n    [\n        Document(content=\"My name is Jean and I live in Paris.\"),\n        Document(content=\"My name is Mark and I live in Berlin.\"),\n        Document(content=\"My name is Giorgio and I live in Rome.\"),\n    ]\n)\n\n# Build a RAG pipeline\nprompt_template = \"\"\"\nGiven these documents, answer the question.\nDocuments:\n{% for doc in documents %}\n    {{ doc.content }}\n{% endfor %}\nQuestion: {{question}}\nAnswer:\n\"\"\"\n\nretriever = InMemoryBM25Retriever(document_store=document_store)\nprompt_builder = PromptBuilder(template=prompt_template)\nllm = OpenAIGenerator(model=\"gpt-3.5-turbo\")\n\nrag_pipeline = Pipeline()\nrag_pipeline.add_component(\"retriever\", retriever)\nrag_pipeline.add_component(\"prompt_builder\", prompt_builder)\nrag_pipeline.add_component(\"llm\", llm)\nrag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\nrag_pipeline.connect(\"prompt_builder\", \"llm\")\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix Server Using pip - Bash\nDESCRIPTION: Installs the core arize-phoenix package for launching a Phoenix server locally. Required for users who want to self-host the Phoenix observability platform and send trace data from their Python LLM applications. Execute in your CLI before starting the server.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for LlamaIndex and Phoenix Monitoring\nDESCRIPTION: Imports modules such as json, os, getpass for environment management, Faker for generating user data, GCSFileSystem for cloud storage access, LlamaIndex classes for index handling, OpenAI API interfaces, and Phoenix instrumentation tools. Sets up the environment to build and trace the application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/project_sessions_llama_index_query_engine.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport json\nimport os\nfrom getpass import getpass\nfrom random import sample\nfrom urllib.request import urlopen\nfrom uuid import uuid4\n\nfrom faker import Faker\nfrom gcsfs import GCSFileSystem\nfrom llama_index.core import (\n    Settings,\n    StorageContext,\n    load_index_from_storage,\n)\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nfrom openinference.instrumentation import using_session, using_user\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\nfrom tqdm import tqdm\n\nimport phoenix as px\nfrom phoenix.otel import register\n```\n\n----------------------------------------\n\nTITLE: Instantiating BedrockModel with Boto3 Client in Python\nDESCRIPTION: This snippet demonstrates how to create a Bedrock runtime client using a previously established Boto3 session (potentially one obtained after assuming a role) and then use this client to instantiate the `BedrockModel` object for use within the Phoenix evaluation framework.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evaluation-models.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclient_bedrock = assumed_role_session.client(\"bedrock-runtime\")\n# Arize Model Object - Bedrock ClaudV2 by default\nmodel = BedrockModel(client=client_bedrock)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application from Python Notebook\nDESCRIPTION: This Python snippet imports the phoenix module and launches the Phoenix application interface, providing interactive access to Phoenix functionality within a notebook session. Note that notebook environments lack persistent storage, so trace data will be ephemeral.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/prompt-flow.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI LLM and Building Vector Index with Chunking\nDESCRIPTION: Creates an OpenAI GPT-4 model instance and constructs a vector index from loaded documents using a node parser with specified chunk size. Facilitates efficient semantic search and retrieval for the RAG system.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nllm = OpenAI(model=\"gpt-4o\")\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=512)\nnodes = node_parser.get_nodes_from_documents(documents)\nvector_index = VectorStoreIndex(nodes)\n\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Data Schema for DataFrames - Python\nDESCRIPTION: Creates a Phoenix Schema object to map DataFrame column names to Phoenix concepts such as prediction, target, score, timestamp, features, tags, and embeddings. Embedding columns are specified via a dictionary mapping to px.EmbeddingColumnNames. This schema is critical for correct ingestion and visualization in Phoenix. Requires the phoenix Python library.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/credit_card_fraud_tutorial.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nschema = px.Schema(\n    prediction_id_column_name=\"prediction_id\",\n    prediction_label_column_name=\"predicted_label\",\n    prediction_score_column_name=\"predicted_score\",\n    actual_label_column_name=\"actual_label\",\n    timestamp_column_name=\"prediction_timestamp\",\n    feature_column_names=feature_column_names,\n    tag_column_names=[\"age\"],\n    embedding_feature_column_names={\n        \"tabular_embedding\": px.EmbeddingColumnNames(\n            vector_column_name=\"tabular_vector\",\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Running Overall Experiment with Evaluators in Python\nDESCRIPTION: This snippet demonstrates how to run an overall experiment using a set of evaluators. It defines an experiment name and description to provide context for the evaluation process. The evaluators likely assess different aspects of the agent's performance, such as function calling, SQL result evaluation, clarity, entity correctness, and code runnability.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_61\n\nLANGUAGE: Python\nCODE:\n```\nrun_overall_experiment,\n    evaluators=[\n        function_calling_eval,\n        evaluate_sql_result,\n        evaluate_clarity,\n        evaluate_entity_correctness,\n        code_is_runnable,\n    ],\n    experiment_name=\"Overall Experiment\",\n    experiment_description=\"Evaluating the overall experiment\",\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Chatbot Tools for Tool-Calling with OpenAI Python\nDESCRIPTION: This snippet defines three function-type tools for use by OpenAI LLMs: product_details_tool for product lookup, product_search_tool for querying general categories or prices, and customer_support_tool for support escalation. All tools are structured using the ChatCompletionToolParam format. It creates a list of tool definitions (tools) and prints them as formatted JSON. Inputs are hard-coded tool configs, and outputs are displayed JSON specs for model consumption. Dependencies include the OpenAI Python library; no external API calls occur here.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/demos/playground_demo.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nproduct_details_tool: ChatCompletionToolParam = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"product_details\",\n        \"description\": \"Searches for a product by name and returns important details such as price and availability\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"name\": {\"type\": \"string\", \"description\": \"The name of the product being searched\"}\n            },\n            \"required\": [],\n        },\n    },\n}\nproduct_search_tool: ChatCompletionToolParam = {\n    \"function\": {\n        \"name\": \"product_search\",\n        \"description\": 'Searches for products by generic descriptions without specific product names (e.g., \"high-end smartphones\" or \"energy-efficient appliances\")',\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\"type\": \"string\", \"description\": \"The search query string.\"},\n                \"category\": {\n                    \"type\": \"string\",\n                    \"description\": \"The category to filter the search.\",\n                    \"default\": None,\n                },\n                \"min_price\": {\n                    \"type\": \"number\",\n                    \"description\": \"The minimum price of the products to search.\",\n                    \"default\": 0,\n                },\n                \"max_price\": {\n                    \"type\": \"number\",\n                    \"description\": \"The maximum price of the products to search.\",\n                    \"default\": None,\n                },\n            },\n            \"required\": [\"query\"],\n        },\n    },\n    \"type\": \"function\",\n}\ncustomer_support_tool: ChatCompletionToolParam = {\n    \"function\": {\n        \"name\": \"customer_support\",\n        \"description\": \"Escalates to a human customer support agent\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"issue_type\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"billing\", \"account_management\", \"technical_support\", \"other\"],\n                    \"description\": \"The type of issue\",\n                },\n                \"issue_description\": {\n                    \"type\": \"string\",\n                    \"description\": \"A description of the issue\",\n                },\n            },\n            \"required\": [\"issue_type\", \"issue_description\"],\n        },\n    },\n    \"type\": \"function\",\n}\ntools = [\n    product_details_tool,\n    product_search_tool,\n    customer_support_tool,\n]\nprint(json.dumps(tools, indent=2))\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Schema for Production Data (Python)\nDESCRIPTION: Creates a `phoenix.Schema` object for the production DataFrame (`prod_df`). This schema is similar to the training schema but omits the `actual_label_column_name` because the production data lacks ground truth labels.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/image_classification_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprod_schema = px.Schema(\n    timestamp_column_name=\"prediction_ts\",\n    prediction_label_column_name=\"predicted_action\",\n    embedding_feature_column_names={\n        \"image_embedding\": px.EmbeddingColumnNames(\n            vector_column_name=\"image_vector\",\n            link_to_data_column_name=\"url\",\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Defining One-Shot Prompt Structure with an Example\nDESCRIPTION: Creates the configuration for a one-shot prompt. It defines a system message template that includes a placeholder for the sampled example (`{examples}`). The example is formatted into the template, and the updated prompt parameters (including the example) are used to create a new version of the prompt in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/few-shot-prompting.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\none_shot_template = \"\"\"\n\"You are an evaluator who assesses the sentiment of a review. Output if the review positive, negative, or neutral. Only respond with one of these classifications.\"\n\nHere is one example of a review and the sentiment:\n\n{examples}\n\"\"\"\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"system\", \"content\": one_shot_template.format(examples=one_shot_example)},\n        {\"role\": \"user\", \"content\": \"{{Review}}\"},\n    ],\n)\n\none_shot_prompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"One-shot prompt for classifying reviews based on sentiment.\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Saving LLM Prompt in Phoenix using Python\nDESCRIPTION: This snippet registers the constructed prompt in Phoenix, assigning a unique prompt identifier and storing metadata such as the prompt description and version. It uses the Phoenix client and the PromptVersion constructed from OpenAI parameters. The 'prompt_identifier' must be unique and alphanumeric (or use hyphens/underscores). Inputs are the identifier and prompt details; the output is the registered prompt in Phoenix. The snippet requires the 'Client', 'PromptVersion', and OpenAI params objects as dependencies.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\n# prompt identifier should contain only alphanumeric characters, hyphens or underscores\nprompt_identifier = \"convert-customer-service-policy\"\n\nprompt = Client().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Convert customer service policy into a routine\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Instantiating OpenAIModel with GPT-3.5 Turbo Parameters - Python\nDESCRIPTION: Creates another OpenAIModel instance, this time with model_name set to 'gpt-3.5-turbo' and explicit timeout. Useful for faster, less expensive classification runs at potential cost to accuracy. Assumes prior steps have properly configured the environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model_name=\"gpt-3.5-turbo\", temperature=0.0, request_timeout=20)\n```\n\n----------------------------------------\n\nTITLE: Installing required Python dependencies for OpenAI, LlamaIndex, Phoenix, and HTTPx\nDESCRIPTION: Installs essential Python packages with specific version constraints needed for the LlamaIndex agent, OpenAI API, Phoenix tracing, and HTTP communication. These packages are prerequisites for the subsequent setup and agent interactions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_openai_agent_tracing_tutorial.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n!pip install \"openai>=1\" \"arize-phoenix>=4.29.0\" \"llama_index>=0.11.0\" 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Sampling Few Shot Examples from Test Set - Python\nDESCRIPTION: Loads the test split from the jailbreak-classification dataset and randomly samples 10 examples to be used as few shot exemplars. These examples guide the model in prompt optimization by being included in the prompt definition. Dependencies: datasets, pandas. Outputs a DataFrame of 10 examples for use in prompt construction.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nds_test = load_dataset(\"jackhhao/jailbreak-classification\")[\n    \"test\"\n]  # this time, load in the test set instead of the training set\nfew_shot_examples = ds_test.to_pandas().sample(10)\n```\n\n----------------------------------------\n\nTITLE: Importing and Setting Up LLM APIs and Environment Variables in Python\nDESCRIPTION: This snippet imports SDKs for multiple large language model providers and loads environment variables from a .env file for configuration. It initializes the generative AI SDKs (including OpenAI, HuggingFace, Gemini, Anthropic, etc.) and prepares the Phoenix client for prompt management. Dependencies include installing all mentioned SDKs and having a .env file in the user home directory for secure credentials. No function parameters are used, but environment variables are critical for SDK authentication.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/examples/prompts/prompts_for_various_SDKs.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nfrom pathlib import Path\n\nimport anthropic\nimport google.generativeai as genai\nimport huggingface_hub\nimport mistralai\nimport ollama\nimport openai\nfrom dotenv import load_dotenv\nfrom google.generativeai import GenerativeModel\n\nfrom phoenix.client import Client\nfrom phoenix.client.utils import to_chat_messages_and_kwargs\n\nload_dotenv(Path.home() / \".env\")\ngenai.configure()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for CrewAI - Bash\nDESCRIPTION: This command installs the necessary Python packages for working with CrewAI and its integrations. It includes `openinference-instrumentation-crewai`, `crewai` and `crewai-tools`. These dependencies enable the core CrewAI functionality and the instrumentation necessary for tracing with OpenInference.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/crewai.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-crewai crewai crewai-tools\n```\n\n----------------------------------------\n\nTITLE: Defining Schema object for inference data (CV model)\nDESCRIPTION: Creates a Schema object to map DataFrame columns to inference schema fields, including timestamp, prediction label, actual label, and embedding features with links to data. This schema informs Phoenix how to interpret DataFrame columns.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/phoenix-inferences.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Define Schema to indicate which columns in train_df should map to each field\ntrain_schema = px.Schema(\n    timestamp_column_name=\"prediction_ts\",\n    prediction_label_column_name=\"predicted_action\",\n    actual_label_column_name=\"actual_action\",\n    embedding_feature_column_names={\n        \"image_embedding\": px.EmbeddingColumnNames(\n            vector_column_name=\"image_vector\",\n            link_to_data_column_name=\"url\",\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Telemetry and Tracing in Python\nDESCRIPTION: Configures Arize Phoenix for telemetry capture. It retrieves the Phoenix API key from environment variables or prompts the user if not found, sets it as an environment variable, defines the Phoenix collector endpoint (pointing to the cloud service), and sets necessary client headers including the API key. It also specifies a project name ('Tool Calling Eval') for organizing traces in Phoenix. Finally, it calls `phoenix.otel.register` to initialize OpenTelemetry tracing with auto-instrumentation enabled for the specified project, storing the tracer provider.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nif not (phoenix_api_key := os.getenv(\"PHOENIX_API_KEY\")):\n    phoenix_api_key = getpass(\"ðŸ”‘ Enter your Phoenix API key: \")\n\nos.environ[\"PHOENIX_API_KEY\"] = phoenix_api_key\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com/\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={phoenix_api_key}\"\n\nos.environ[\"PHOENIX_PROJECT_NAME\"] = \"Tool Calling Eval\"\n\ntracer_provider = register(auto_instrument=True, project_name=\"Tool Calling Eval\")\n```\n\n----------------------------------------\n\nTITLE: Defining Convergence Questions Dataset in Python\nDESCRIPTION: Creates a list of questions related to average sales quantities, converts it into a Pandas DataFrame, and uploads it to Phoenix as a dataset named 'convergence_questions'. The 'question' column is designated as the input key for the dataset. Requires pandas (pd) and a configured Phoenix client (px_client).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n# Replace this with a human annotated set of ground truth data, instead of generated examples\n\nconvergence_questions = [\n    \"What was the average quantity sold per transaction?\",\n    \"What is the mean number of items per sale?\",\n    \"Calculate the typical quantity per transaction\",\n    \"Show me the average number of units sold in each transaction\",\n    \"What's the mean transaction size in terms of quantity?\",\n    \"On average, how many items were purchased per transaction?\",\n    \"What is the average basket size per sale?\",\n    \"Calculate the mean number of products per purchase\",\n    \"What's the typical number of units per order?\",\n    \"Find the average quantity of items in each transaction\",\n    \"What is the average number of products bought per purchase?\",\n    \"Tell me the mean quantity of items in a typical transaction\",\n    \"How many items does a customer buy on average per transaction?\",\n    \"What's the usual number of units in each sale?\",\n    \"Calculate the average basket quantity per order\",\n    \"What is the typical amount of products per transaction?\",\n    \"Show the mean number of items customers purchase per visit\",\n    \"What's the average quantity of units per shopping trip?\",\n    \"How many products do customers typically buy in one transaction?\",\n    \"What is the standard basket size in terms of quantity?\",\n]\n\nconvergence_df = pd.DataFrame({\"question\": convergence_questions})\n\ndataset = px_client.upload_dataset(\n    dataframe=convergence_df, dataset_name=\"convergence_questions\", input_keys=[\"question\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for LangChain and Phoenix\nDESCRIPTION: Installs necessary Python packages including LangChain, OpenAI, Phoenix, and related libraries for tracing and evaluation capabilities.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_tracing_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"langchain>=0.1.0\" langchain-community langchain-openai \"openai>=1\" 'httpx<0.28' \"arize-phoenix[evals]\" tiktoken nest-asyncio openinference-instrumentation-langchain\n```\n\n----------------------------------------\n\nTITLE: Running an Experiment with Phoenix Framework (Python)\nDESCRIPTION: This snippet defines a task function that generates prompts based on questions, then runs a Phoenix experiment with specified evaluators such as routing, function call, and parameter extraction evaluations. It requires the 'phoenix' library and assumes 'dataset' and 'questions' are defined.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nimport phoenix as px\nfrom phoenix.experiments import run_experiment\n\ndef prompt_gen_task(input):\n    return run_prompt(input[\"question\"])\n\n\neperiment = run_experiment(\n    dataset=dataset,\n    task=prompt_gen_task,\n    evaluators=[routing_eval, function_call_eval, parameter_extraction_eval],\n    experiment_name=\"agents-cookbook\",\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running QA & Hallucination Evaluations\nDESCRIPTION: Imports necessary evaluators (`HallucinationEvaluator`, `QAEvaluator`, `run_evals`) from `phoenix.evals`. Initializes the QA and Hallucination evaluators with a specified `eval_model`. Runs both evaluations simultaneously on the `qa_with_reference_df` DataFrame using `run_evals`, requesting explanations and setting concurrency. Stores the results in separate DataFrames: `qa_correctness_eval_df` and `hallucination_eval_df`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    HallucinationEvaluator,\n    QAEvaluator,\n    run_evals,\n)\n\n# Initialize the built in Q&A evaluator\nqa_evaluator = QAEvaluator(eval_model)\n\n# Initialize the built in Hallucination evaluator\nhallucination_evaluator = HallucinationEvaluator(eval_model)\n\n# Run the evaluation\nqa_correctness_eval_df, hallucination_eval_df = run_evals(\n    evaluators=[qa_evaluator, hallucination_evaluator],\n    dataframe=qa_with_reference_df,\n    provide_explanation=True,\n    concurrency=20,\n)\n```\n\n----------------------------------------\n\nTITLE: Running Evals with Default Error Handling (Python)\nDESCRIPTION: Executes the `llm_classify` function on the sampled dataframe using the buggy model and standard relevance rails. With `include_exceptions=True`, this run demonstrates the default error handling behavior, including retries and early exit on terminal errors (like missing input), capturing detailed exception and status information.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nrails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\nevals_with_exception_info = llm_classify(\n    dataframe=df_sample,\n    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n    model=funny_model,\n    rails=rails,\n    concurrency=3,\n    include_exceptions=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Asynchronous Query Task Using Llama-Index in Python\nDESCRIPTION: Creates an in-memory VectorStoreIndex from preloaded documents and converts it into a query engine. Defines an async function called `task` which accepts an input dictionary with a 'query' key, performs an asynchronous query using the engine, and returns a dictionary containing the retrieved context texts and the response. This task function forms the core LLM interaction logic used by experiments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nindex = VectorStoreIndex.from_documents(documents=documents)\nquery_engine = index.as_query_engine()\n\n\nasync def task(input):\n    ans = await query_engine.aquery(input[\"query\"])\n    return {\n        \"contexts\": [node.text for node in ans.source_nodes],\n        \"response\": ans.response,\n    }\n```\n\n----------------------------------------\n\nTITLE: Loading Computer Vision Inference Data into a Pandas DataFrame using Python\nDESCRIPTION: Demonstrates downloading and loading a parquet file containing human action classification inferences into a pandas DataFrame. The dataset serves as input containing features like embeddings, predictions, and metadata, which will be visualized in Phoenix. This snippet also imports the Phoenix library to prepare for subsequent schema definitions and inference wrapping.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/inferences_quickstart.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nimport phoenix as px\n\ntrain_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/cv/human-actions/human_actions_training.parquet\"\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Prompt for Hallucination Detection in Phoenix Using Python\nDESCRIPTION: Defines a prompt template describing a task to classify generated answers as 'factual' or 'hallucinated' based on a query and reference text. Sends this prompt to the Phoenix server creating it with a unique name, a detailed description, a user role message, and specifies the model version 'gpt-4o-mini'. The prompt supports variable replacement for evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprompt_name = f\"hallu-eval-{token_hex(4)}\"  # adding a random suffix for demo purposes\n\ncontent = \"\"\"\\\nIn this task, you will be presented with a query, a reference text and an answer. The answer is\ngenerated to the question based on the reference text. The answer may contain false information. You\nmust use the reference text to determine if the answer to the question contains false information,\nif the answer is a hallucination of facts. Your objective is to determine whether the answer text\ncontains factual information and is not a hallucination. A 'hallucination' refers to\nan answer that is not based on the reference text or assumes information that is not available in\nthe reference text. Your response should be a single word: either \"factual\" or \"hallucinated\", and\nit should not include any other text or characters. \"hallucinated\" indicates that the answer\nprovides factually inaccurate information to the query based on the reference text. \"factual\"\nindicates that the answer to the question is correct relative to the reference text, and does not\ncontain made up information. Please read the query and reference text carefully before determining\nyour response.\n\n[BEGIN DATA]\n************\n[Query]: {{ question }}\n************\n[Reference text]: {{ knowledge }}\n************\n[Answer]: {{ answer }}\n************\n[END DATA]\n\nIs the answer above factual or hallucinated based on the query and reference text?\n\"\"\"\n_ = Client().prompts.create(\n    name=prompt_name,\n    prompt_description=\"Determining if an answer is factual or hallucinated based on a query and reference text\",\n    version=PromptVersion(\n        [{\"role\": \"user\", \"content\": content}],\n        model_name=\"gpt-4o-mini\",\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Querying Spans and Extracting Tool Calls - Python\nDESCRIPTION: This code defines a query to retrieve LLM spans from Phoenix, then extracts the called tool names from the `output_messages` of each span. It creates a Pandas DataFrame from the query results and defines a function `get_tool_call` to parse the `output_messages` and extract the tool name, handling cases where no tool was used.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nquery = (\n    SpanQuery()\n    .where(\n        \"span_kind == 'LLM'\",\n    )\n    .select(question=\"input.value\", output_messages=\"llm.output_messages\")\n)\n\n# The Phoenix Client can take this query and return the dataframe.\ntool_calls_df = px.Client().query_spans(query, project_name=project_name, timeout=None)\ntool_calls_df.dropna(subset=[\"output_messages\"], inplace=True)\n\n\ndef get_tool_call(outputs):\n    if outputs[0].get(\"message\").get(\"tool_calls\"):\n        return (\n            outputs[0]\n            .get(\"message\")\n            .get(\"tool_calls\")[0]\n            .get(\"tool_call\")\n            .get(\"function\")\n            .get(\"name\")\n        )\n    else:\n        return \"No tool used\"\n\n\ntool_calls_df[\"tool_call\"] = tool_calls_df[\"output_messages\"].apply(get_tool_call)\ntool_calls_df.head()\n```\n\n----------------------------------------\n\nTITLE: Creating QA Dataset with Input/Output Messages - Python\nDESCRIPTION: Defines a pandas DataFrame representing input user messages and expected assistant outputs, forming a two-row evaluation set for RAG tasks. No dependencies beyond pandas. Inputs: Hardcoded messages. Outputs: DataFrame variable 'df' for downstream data upload and experimentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/run_experiments_with_llama_index.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"input_messages\": [\n            [{\"role\": \"user\", \"content\": \"Which grad schools did the author apply for and why?\"}],\n            [{\"role\": \"user\", \"content\": \"What did the author do growing up?\"}],\n        ],\n        \"output_message\": [\n            {\n                \"role\": \"assistant\",\n                \"content\": \"The author applied to three grad schools: MIT and Yale, which were renowned for AI at the time, and Harvard, which the author had visited because a friend went there and it was also home to Bill Woods, who had invented the type of parser the author used in his SHRDLU clone. The author chose these schools because he wanted to learn about AI and Lisp, and these schools were known for their expertise in these areas.\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"The author took a painting class at Harvard with Idelle Weber and later became her de facto studio assistant. Additionally, the author worked on several different projects, including writing essays, developing spam filters, and painting.\",\n            },\n        ],\n    }\n)\ndf\n```\n\n----------------------------------------\n\nTITLE: Instrumenting LangChain with OpenInference Tracer\nDESCRIPTION: Configures OpenInference instrumentation for LangChain to capture trace data during application execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_tracing_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntracer_provider = register()\nLangChainInstrumentor(tracer_provider=tracer_provider).instrument(skip_dep_check=True)\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom LLM Eval String Template in Python\nDESCRIPTION: Defines a custom evaluation prompt template as a multi-line string variable `MY_CUSTOM_TEMPLATE`. This template instructs an LLM on how to evaluate responses based on questions, specifying input placeholders (`{question}`, `{response}`) and requiring a single-word output (\"positive\" or \"negative\"). This serves as the core instruction set for the custom evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/concepts-evals/building-your-own-evals.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nMY_CUSTOM_TEMPLATE = '''\n    You are evaluating the positivity or negativity of the responses to questions.\n    [BEGIN DATA]\n    ************\n    [Question]: {question}\n    ************\n    [Response]: {response}\n    [END DATA]\n\n\n    Please focus on the tone of the response.\n    Your answer must be single word, either \"positive\" or \"negative\"\n    '''\n```\n\n----------------------------------------\n\nTITLE: Running Q&A Classification with GPT-4\nDESCRIPTION: Executes the main evaluation function that classifies answers using GPT-4, the prompt template, and output rails to ensure consistent response formats.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# The rails force the output to specific values of the template\n# It will remove text such as \",,,\" or \"...\", anything not the\n# binary value expected from the template\nrails = list(QA_PROMPT_RAILS_MAP.values())\nQ_and_A_classifications = llm_classify(\n    dataframe=df_sample,\n    template=QA_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    concurrency=20,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Running Citation Evaluation with Phoenix Evals Python API\nDESCRIPTION: This snippet shows how to execute citation/reference link evaluation using the Phoenix Evals API. Required dependencies: phoenix.evals library, a dataframe (df) containing the data, and a supported OpenAI API key if using the OpenAIModel. The code initializes a language model with specific parameters, sets up output rails to ensure the categorical response format, and calls llm_classify to perform the evaluation. Inputs: dataframe (df) with conversation and document_text columns; Outputs: relevance_classifications with binary assessment and optional explanations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/reference-link-evals.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.evals import (\n    REF_LINK_EVAL_PROMPT_RAILS_MAP,\n    REF_LINK_EVAL_PROMPT_TEMPLATE_STR,\n    OpenAIModel,\n    download_benchmark_dataset,\n    llm_classify,\n)\n\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\n\n#The rails is used to hold the output to specific values based on the template\n#It will remove text such as \",,,\" or \"...\"\n#Will ensure the binary value expected from the template is returned\nrails = list(REF_LINK_EVAL_PROMPT_RAILS_MAP.values())\nrelevance_classifications = llm_classify(\n    dataframe=df,\n    template=REF_LINK_EVAL_PROMPT_TEMPLATE_STR,\n    model=model,\n    rails=rails,\n    provide_explanation=True, #optional to generate explanations for the value produced by the eval LLM\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix and Dependencies using Pip\nDESCRIPTION: Installs or upgrades the `arize-phoenix` library with embeddings support and the `arize` library with AutoEmbeddings using pip. This command prepares the environment for running the subsequent code in the tutorial.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/sentiment_classification_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uq \"arize[AutoEmbeddings]\" \"arize-phoenix[embeddings]\"\n```\n\n----------------------------------------\n\nTITLE: Applying nest_asyncio for Async Request Submission in Notebooks\nDESCRIPTION: Applies nest_asyncio to enable asynchronous request submission in notebook environments, which can significantly improve evaluation speed (up to 5x faster).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Tracer Provider for Phoenix in Python\nDESCRIPTION: Registers an OpenTelemetry tracer provider configured to export trace data to the Phoenix backend via an OTLP endpoint. The register function is called with the Phoenix OTLP collector URL which listens on localhost port 6006. This setup enables trace data from instrumented libraries to be captured and sent to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/crewai_tracing_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\ntracer_provider = register(endpoint=\"http://localhost:6006/v1/traces\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating Relevance of Retrieved Documents (RAG)\nDESCRIPTION: This snippet evaluates the relevance of retrieved documents using Phoenix's LLM Evals. It creates a `RelevanceEvaluator` and runs the evaluations on the `retrieved_documents_df` DataFrame. The `provide_explanation=True` argument enables LLM explanations for reasoning. It depends on the `phoenix` and `openai` libraries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    RelevanceEvaluator,\n    run_evals,\n)\n\nrelevance_evaluator = RelevanceEvaluator(OpenAIModel(model_name=\"gpt-4-turbo-preview\"))\n\nretrieved_documents_relevance_df = run_evals(\n    evaluators=[relevance_evaluator],\n    dataframe=retrieved_documents_df,\n    provide_explanation=True,\n    concurrency=20,\n)[0]\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Instrumentation\nDESCRIPTION: This command installs the `openinference-instrumentation-llama_index` package. This package contains the necessary instrumentation code to trace LlamaIndex Workflows and send the traces to the configured OpenTelemetry collector, which in this context is the Phoenix server.  It is a crucial dependency for enabling tracing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex-1.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-llama_index\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Eval Dependencies (Bash)\nDESCRIPTION: Installs the necessary Python packages required to run Phoenix evaluations and related utilities. This includes `arize-phoenix-evals`, `openai`, `ipython`, `matplotlib`, `pycm`, `scikit-learn`, `tiktoken`, `nest_asyncio` (for async support in notebooks), and `httpx`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n!pip install -qq \"arize-phoenix-evals\" \"openai>=1\" ipython matplotlib pycm scikit-learn tiktoken nest_asyncio 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Schema for Production Inferences without Ground Truth in Python\nDESCRIPTION: Creates a Schema object for the production dataset that lacks an actual label column. The schema specifies the timestamp and prediction label columns along with embeddings metadata. Differentiating schemas per dataset is required when data structures differ, such as absence of ground truth in production data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/inferences_quickstart.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprod_schema = px.Schema(\n    timestamp_column_name=\"prediction_ts\",\n    prediction_label_column_name=\"predicted_action\",\n    embedding_feature_column_names={\n        \"image_embedding\": px.EmbeddingColumnNames(\n            vector_column_name=\"image_vector\",\n            link_to_data_column_name=\"url\",\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Retrieved Documents DataFrame from Phoenix Session in Python\nDESCRIPTION: Imports the get_retrieved_documents function and calls it to extract all retrieved documents from the active Phoenix session. The resulting pandas DataFrame includes trace IDs, inputs, references, and document scores related to the retrieval. This snippet is useful for analyzing which documents the RAG system retrieved per query. It requires a running Phoenix session accessible via px.active_session().\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_retrieved_documents\n\nretrieved_documents_df = get_retrieved_documents(px.active_session())\nretrieved_documents_df\n```\n\n----------------------------------------\n\nTITLE: Logging Span Evaluation Results into Phoenix - Python\nDESCRIPTION: Sends the evaluated results back into Phoenix using px.Client().log_evaluations, wrapping the response_classifications DataFrame in a SpanEvaluations object with an assigned eval_name. Inputs: DataFrame of classifications, evaluation name string. Outputs: logs results for visualization and historical tracking in Phoenix. Requires Phoenix client and the relevant evaluation classes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\npx.Client().log_evaluations(\n    SpanEvaluations(eval_name=\"Tool Calling Eval\", dataframe=response_classifications),\n)\n```\n\n----------------------------------------\n\nTITLE: Importing libraries for summarization evaluation\nDESCRIPTION: Imports the necessary Python libraries for data handling, visualization, model evaluation, and interaction with the Phoenix evaluation framework.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport matplotlib.pyplot as plt\nimport openai\nimport pandas as pd\nfrom pycm import ConfusionMatrix\nfrom sklearn.metrics import classification_report\n\nimport phoenix.evals.default_templates as templates\nfrom phoenix.evals import (\n    OpenAIModel,\n    download_benchmark_dataset,\n    llm_classify,\n)\n\npd.set_option(\"display.max_colwidth\", None)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Instrumenting LangChain with OpenInference Tracing Using Python\nDESCRIPTION: Creates an OpenTelemetry tracer provider configured to export spans to the Phoenix applicationâ€™s HTTP endpoint. Sets up the LangChainInstrumentor to automatically instrument LangChain calls to emit OpenInference trace spans with the configured tracer, allowing detailed trace data collection during chain execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_vertex_ai_tracing_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nendpoint = \"http://127.0.0.1:6006/v1/traces\"\ntracer_provider = TracerProvider()\ntracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n\nLangChainInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Combining Retrieval Metrics into Summary DataFrame in Python\nDESCRIPTION: Combines the retrieval trace DataFrame filtered by span kind with the computed NDCG@2, Precision@2, and hit rate metrics into a single DataFrame for comprehensive retrieval evaluation. This aggregation uses the Phoenix active session's method to get relevant spans and concatenates all metrics with query inputs for easy inspection of retrieval performance on each query.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nretrievals_df = px.active_session().get_spans_dataframe(\"span_kind == 'RETRIEVER'\")\nrag_evaluation_dataframe = pd.concat(\n    [\n        retrievals_df[\"attributes.input.value\"],\n        ndcg_at_2.add_prefix(\"ncdg@2_\"),\n        precision_at_2.add_prefix(\"precision@2_\"),\n        hit,\n    ],\n    axis=1,\n)\nrag_evaluation_dataframe\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for RAG Experiment Setup\nDESCRIPTION: Imports all necessary modules and classes from standard libraries, Phoenix, LlamaIndex, and OpenInference for building and running the RAG experiment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/run_experiments_with_llama_index.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\nfrom datetime import datetime, timezone\nfrom time import sleep\nfrom urllib.request import urlretrieve\n\nimport nest_asyncio\nimport pandas as pd\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.postprocessor import SentenceTransformerRerank\nfrom llama_index.core.settings import Settings\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nimport phoenix as px\nfrom phoenix.evals import OpenAIModel\nfrom phoenix.experiments import run_experiment\nfrom phoenix.experiments.evaluators import ConcisenessEvaluator\nfrom phoenix.experiments.types import EvaluationResult, Example, ExperimentRun\nfrom phoenix.otel import register\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI SDK for Tracing in Deno\nDESCRIPTION: Imports the necessary OpenAI SDK and the specific OpenInference instrumentation library designed for it using Deno's `npm:` specifiers. It creates an instance of the `OpenAIInstrumentation` class. It then manually applies this instrumentation to the imported OpenAI library, enabling automatic tracing of subsequent API calls made through client instances.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/tracing_openai_node_tutorial.ipynb#_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport OpenAI from 'npm:openai';\nimport { OpenAIInstrumentation } from \"npm:@arizeai/openinference-instrumentation-openai\";\n\nconst oaiInstrumentor = new OpenAIInstrumentation();\noaiInstrumentor.manuallyInstrument(OpenAI);\n```\n\n----------------------------------------\n\nTITLE: Downloading SQuAD 2.0 Benchmark Dataset for Q&A Evaluation\nDESCRIPTION: Downloads the Stanford Question Answering Dataset (SQuAD 2.0) with supplemental incorrect answers for evaluating Q&A classification performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf = download_benchmark_dataset(task=\"qa-classification\", dataset_name=\"qa_generated_dataset\")\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluators for Experiment Run in Python\nDESCRIPTION: This code defines two asynchronous evaluators, `answer_relevancy` and `context_relevancy`, which assess the quality of the generated answers and retrieved contexts. They use LlamaIndex's `AnswerRelevancyEvaluator` and `ContextRelevancyEvaluator` with an OpenAI model to evaluate the response based on the input query. The evaluators return a score and feedback.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nasync def answer_relevancy(output, input) -> Tuple[Score, Explanation]:\n    ans = await AnswerRelevancyEvaluator(\n        llm=OpenAI(temperature=0, model=\"gpt-4o\"),\n    ).aevaluate(input[\"query\"], response=output[\"response\"])\n    return ans.score, ans.feedback\n\n\nasync def context_relevancy(output, input) -> Tuple[Score, Explanation]:\n    ans = await ContextRelevancyEvaluator(\n        llm=OpenAI(temperature=0, model=\"gpt-4o\"),\n    ).aevaluate(input[\"query\"], contexts=output[\"contexts\"])\n    return ans.score, ans.feedback\n\n\nevaluators = [answer_relevancy, context_relevancy]\n```\n\n----------------------------------------\n\nTITLE: Querying Llama-Index RAG Engine for Answers in Python\nDESCRIPTION: Iterates through each row of the questions DataFrame, submits questions to the query_engine, and prints both question and response. Inputs: questions_df and an initialized QueryEngine. Outputs: question-answer pairs to the console.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Loop over the questions and generate the answers\nfor i, row in questions_df.iterrows():\n    question = row[\"Prompt/ Question\"]\n    response_vector = query_engine.query(question)\n    print(f\"Question: {question}\\nAnswer: {response_vector.response}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Calculating QA Correctness and Hallucination Scores with Pandas DataFrames\nDESCRIPTION: These snippets calculate the mean scores for QA correctness and hallucinations using pandas DataFrames, providing quantitative metrics for model evaluation. They require pandas dataframes 'qa_correctness_eval_df' and 'hallucination_eval_df'.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_42\n\nLANGUAGE: Python\nCODE:\n```\nqa_correctness_eval_df.mean(numeric_only=True)\n```\n\nLANGUAGE: Python\nCODE:\n```\nhallucination_eval_df.mean(numeric_only=True)\n```\n\n----------------------------------------\n\nTITLE: Improve Prompt with Few-Shot Example Python\nDESCRIPTION: Enhances the system prompt for the LLM by including a sample row from the 'nba' table. This provides the model with a concrete example of the data format, aiming to improve its understanding of column values (like date formats) and potentially reduce errors. The system prompt is reconstructed to include a 'Column | Type | Example' format. The `generate_query` function is redefined (using the same async structure but the new prompt), and a specific question known to cause issues is tested with the improved prompt.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nsamples = conn.query(\"SELECT * FROM nba LIMIT 1\").to_df().to_dict(orient=\"records\")[0]\nsample_rows = \"\\n\".join(\n    f\"{column['column_name']} | {column['column_type']} | {samples[column['column_name']]}\"\n    for column in columns\n)\nsystem_prompt = (\n    \"You are a SQL expert, and you are given a single table named nba with the following columns:\\n\\n\"\n    \"Column | Type | Example\\n\"\n    \"-------|------|--------\\n\"\n    f\"{sample_rows}\\n\"\n    \"\\n\"\n    \"Write a DuckDB SQL query corresponding to the user's request. \"\n    \"Return just the query text, with no formatting (backticks, markdown, etc.).\"\n)\n\n\nasync def generate_query(input):\n    response = await client.chat.completions.create(\n        model=TASK_MODEL,\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": input,\n            },\n        ],\n    )\n    return response.choices[0].message.content\n\n\nprint(await generate_query(\"Which team won the most games in 2015?\"))\n```\n\n----------------------------------------\n\nTITLE: Running Initial Customer Support Experiment\nDESCRIPTION: This code executes an experiment by applying the prompt task and response evaluator over a dataset, with metadata describing the prompt used. It enables performance measurement and comparison of different prompts through systematic experimentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/react_prompting.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ninitial_experiment = run_experiment(\n    dataset,\n    task=prompt_task,\n    evaluators=[evaluate_response],\n    experiment_description=\"Customer Support Prompt\",\n    experiment_name=\"initial-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + prompt.id},\n)\n\n```\n\n----------------------------------------\n\nTITLE: Uploading Evaluation Results to Phoenix\nDESCRIPTION: Code to log the trustworthiness evaluation results back to Phoenix for visualization and analysis. This ensures proper data types for the scores and explanations before uploading the evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/integrations/cleanlab.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import SpanEvaluations\n\neval_df[\"score\"] = eval_df[\"score\"].astype(float)\neval_df[\"explanation\"] = eval_df[\"explanation\"].astype(str)\n\npx.Client().log_evaluations(SpanEvaluations(eval_name=\"Trustworthiness\", dataframe=eval_df))\n```\n\n----------------------------------------\n\nTITLE: Creating a Query Engine from the Index for User Queries\nDESCRIPTION: Transforms the vector index into a query engine to enable natural language querying over indexed content. This allows interactive querying to retrieve relevant data chunks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nquery_engine = vector_index.as_query_engine()\n\n```\n\n----------------------------------------\n\nTITLE: Running Code Generation Experiment\nDESCRIPTION: Runs a Phoenix experiment to evaluate the code generation step of the agent. It uses the dataset of questions, `run_code_generation` as the task, and two evaluators: `code_is_runnable` and `evaluate_chart_config`. The experiment tests both the code's ability to run and its resulting configuration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nexperiment = run_experiment(\n    dataset,\n    run_code_generation,\n    evaluators=[code_is_runnable, evaluate_chart_config],\n    experiment_name=\"Code Generation Eval\",\n    experiment_description=\"Evaluating the code generation step of the agent\",\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Evaluators for LLM Output Assessment in Python\nDESCRIPTION: Example demonstrating how to initialize evaluators by passing a model to be used during evaluation. Shows setup for hallucination and QA correctness evaluation using OpenAI models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evals-reference.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    OpenAIModel,\n    HallucinationEvaluator,\n    QAEvaluator,\n    run_evals,\n)\n\napi_key = None  # set your api key here or with the OPENAI_API_KEY environment variable\neval_model = OpenAIModel(model_name=\"gpt-4-turbo-preview\", api_key=api_key)\n\nhallucination_evaluator = HallucinationEvaluator(eval_model)\nqa_correctness_evaluator = QAEvaluator(eval_model)\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-Built LlamaIndex Index from Cloud Storage\nDESCRIPTION: Creates a GCSFileSystem object connected to a public Google Cloud Storage bucket. Sets the persistence directory path and loads an existing LlamaIndex index stored in GCS into a storage context object, preparing for querying.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/project_sessions_llama_index_query_engine.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfile_system = GCSFileSystem(project=\"public-assets-275721\")\npersist_dir = \"arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/index/\"\nstorage_context = StorageContext.from_defaults(fs=file_system, persist_dir=persist_dir)\n```\n\n----------------------------------------\n\nTITLE: Displaying Evals Results with Exception Info (Python)\nDESCRIPTION: Displays the dataframe returned by the `llm_classify` function when `include_exceptions=True` is set. This output includes columns showing the outcome of each evaluation row, including any exceptions encountered, the number of retries attempted, and the final execution status (e.g., SUCCEEDED, FAILED, DID NOT RUN).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nevals_with_exception_info\n```\n\n----------------------------------------\n\nTITLE: Defining Asynchronous Pairwise Evaluation Function\nDESCRIPTION: Creates an async evaluator that compares model outputs with expected responses using Llama-Index's PairwiseComparisonEvaluator, returning score and feedback. Uses GPT-4 with temperature=0 for deterministic evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nllm = OpenAI(temperature=0, model=\"gpt-4o\")\n\nasync def pairwise(output, input, expected) -> Tuple[Score, Explanation]:\n    ans = await PairwiseComparisonEvaluator(llm=llm).aevaluate(\n        query=input[\"instruction\"],\n        response=output,\n        second_response=expected[\"response\"],\n    )\n    return ans.score, ans.feedback\n\n evaluators = [pairwise]\n```\n\n----------------------------------------\n\nTITLE: Importing necessary libraries for agent, tracing, and data handling\nDESCRIPTION: Imports core Python modules and specialized libraries including OpenAI, pandas, LlamaIndex components, and Phoenix. Sets pandas display options to manage DataFrame visualization, establishing dependencies for agent creation, tracing, and data analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_openai_agent_tracing_tutorial.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport openai\nimport pandas as pd\nfrom llama_index.agent.openai import OpenAIAgent\nfrom llama_index.core.prompts.system import SHAKESPEARE_WRITING_ASSISTANT\nfrom llama_index.core.tools import FunctionTool\nfrom llama_index.llms.openai import OpenAI\n\nimport phoenix as px\n\npd.set_option(\"display.max_colwidth\", 1000)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenInference LangChain Instrumentation\nDESCRIPTION: Installs the OpenInference LangChain instrumentation package needed to enable Phoenix tracing within LangChain applications.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langgraph.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-langchain\n```\n\n----------------------------------------\n\nTITLE: Defining SQL Result Evaluator\nDESCRIPTION: This function defines an evaluator to compare the agent's SQL query results with the expected results. It extracts numerical values from both the agent's output and the expected output, and compares them to determine if they match.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_sql_result(output: str, expected: str) -> bool:\n    # Extract just the numbers from both strings\n    result_nums = \"\".join(filter(str.isdigit, output))\n    expected_nums = \"\".join(filter(str.isdigit, expected.get(\"expected_result\")))\n    return result_nums == expected_nums\n```\n\n----------------------------------------\n\nTITLE: Adding Span Attributes with Individual Context Managers in Python\nDESCRIPTION: Shows the equivalent way to add multiple span attributes using individual OpenInference context managers (`using_session`, `using_user`, `using_metadata`, `using_tags`, `using_prompt_template`). This example produces the same result as the `using_attributes` context manager but requires nesting multiple `with` statements. It illustrates the convenience provided by `using_attributes`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nwith (\n    using_session(\"my-session-id\"),\n    using_user(\"my-user-id\"),\n    using_metadata(metadata),\n    using_tags(tags),\n    using_prompt_template(\n        template=prompt_template,\n        version=prompt_template_version,\n        variables=prompt_template_variables,\n    ),\n):\n    # Calls within this block will generate spans with the attributes:\n    # \"session.id\" = \"my-session-id\"\n    # \"user.id\" = \"my-user-id\"\n    # \"metadata\" = \"{\\\"key-1\\\": value_1, \\\"key-2\\\": value_2, ... }\" # JSON serialized\n    # \"tag.tags\" = \"[\\\"tag_1\\\",\\\"tag_2\\\",...]\"\n    # \"llm.prompt_template.template\" = \"Please describe the weather forecast for {city} on {date}\"\n    # \"llm.prompt_template.variables\" = \"{\\\"city\\\": \\\"Johannesburg\\\", \\\"date\\\": \\\"July 11\\\"}\" # JSON serialized\n    # \"llm.prompt_template.version \" = \"v1.0\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Tracing Chains with Decorators\nDESCRIPTION: This snippet showcases tracing a chain function using a decorator. The function takes a string input and returns a string output. Input and output attributes are automatically derived from the function signature.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@tracer.chain\ndef decorated_chain_with_plain_text_output(input: str) -> str:\n    return \"output\"\n\ndecorated_chain_with_plain_text_output(\"input\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Modules for Llama-Index and Phoenix Experimentation in Python\nDESCRIPTION: Imports a set of standard and third-party Python modules required for dataset loading, asynchronous execution, evaluation, and instrumentation integration. This includes Llama-Index classes for indexing and evaluation, Phoenix experiment utilities, and OpenTelemetry instrumentation. The snippet also applies nest_asyncio patching to allow nested event loops in environments like Jupyter.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport tempfile\nfrom textwrap import shorten\nfrom time import time_ns\nfrom typing import Tuple\n\nimport nest_asyncio\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.core.evaluation import AnswerRelevancyEvaluator, ContextRelevancyEvaluator\nfrom llama_index.core.llama_dataset import download_llama_dataset\nfrom llama_index.llms.openai import OpenAI\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nimport phoenix as px\nfrom phoenix.experiments import evaluate_experiment, run_experiment\nfrom phoenix.experiments.types import Explanation, Score\nfrom phoenix.otel import register\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for RAG Pipeline - Python\nDESCRIPTION: This snippet installs the necessary Python packages for building the RAG pipeline, including Arize Phoenix, LlamaIndex, Mistral AI integrations, and other utility libraries. These dependencies are essential for setting up the environment for data loading, indexing, querying, and evaluation of the RAG system.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uqq \"arize-phoenix[embeddings]\" \"arize-phoenix-evals>=0.5.0\" \"llama-index==0.10.19\" \"llama-index-llms-mistralai\" \"llama-index-embeddings-mistralai\"  \"openinference-instrumentation-mistralai>=0.0.2\" \"openinference-instrumentation-llama-index>=1.0.0\" \"llama-index-callbacks-arize-phoenix>=0.1.2\" gcsfs nest_asyncio 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Upload Dataset to Phoenix Python\nDESCRIPTION: Uploads the list of sample questions to Phoenix as a versioned dataset named 'nba-questions'. This allows Phoenix to manage the test cases for evaluation experiments. The questions are converted into a pandas DataFrame before uploading. `input_keys` specifies 'question' as the input column, and `output_keys` is left empty as the output is not yet generated.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\n\nds = px.Client().upload_dataset(\n    dataset_name=\"nba-questions\",\n    dataframe=pd.DataFrame([{\"question\": question} for question in questions]),\n    input_keys=[\"question\"],\n    output_keys=[],\n)\n\n# If you have already uploaded the dataset, you can fetch it using the following line\n# ds = px.Client().get_dataset(name=\"nba-questions\")\n```\n\n----------------------------------------\n\nTITLE: Patching asyncio Event Loop with nest_asyncio in Python\nDESCRIPTION: Imports nest_asyncio and applies its patch to the global asyncio event loop, enabling nested event loops in notebook environments like Jupyter/Colab. This is necessary to avoid runtime errors with async functions and to significantly speed up eval submission by enabling concurrent processing. Required only in interactive environments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Running Task and Evaluations Together in Python\nDESCRIPTION: This snippet executes the task and the evaluations together in a single experiment run, streamlining the process and potentially optimizing resource usage.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n_ = run_experiment(dataset, task, evaluators)\n```\n\n----------------------------------------\n\nTITLE: Running Application with TS-Node - Shell\nDESCRIPTION: Shell command to execute the application file (`app.ts`) using `ts-node`, requiring a separate instrumentation file (`instrumentation.ts`). This is how the example application is intended to be run to enable tracing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nts-node --require ./instrumentation.ts app.ts\n```\n\n----------------------------------------\n\nTITLE: Invoking Sales Data Lookup Tool - Python\nDESCRIPTION: Demonstrates invoking the Phoenix-traced sales data lookup tool with a sample prompt for sales details. The function returns sales data corresponding to store 1320 on November 1st, 2021. Input: string prompt. Output: formatted query result.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nexample_data = lookup_sales_data(\"Show me all the sales for store 1320 on November 1st, 2021\")\nexample_data\n```\n\n----------------------------------------\n\nTITLE: Displaying Evals Results from exit_on_error=False Run (Python)\nDESCRIPTION: Displays the dataframe containing the results from the evaluation run configured with `exit_on_error=False`. This output shows the final status and outcome for every row in the input dataframe, illustrating how `exit_on_error=False` prevents early termination and allows inspection of all row outcomes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nall_evals\n```\n\n----------------------------------------\n\nTITLE: Classifying Data with LLM using Phoenix Evals\nDESCRIPTION: This code uses the `llm_classify` function from the `phoenix.evals` module to classify the question-answer pairs in the DataFrame. It sets up an OpenAI model, applies a specified template, and designates acceptable rails for the LLM's output.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.evals import OpenAIModel, llm_classify\n\nnest_asyncio.apply()\n\nmodel = OpenAIModel(model=\"gpt-4\", temperature=0.0)\n\nQ_and_A_classifications = llm_classify(\n    data=evals_copy,\n    template=qa_template,\n    model=model,\n    rails=[\"correct\", \"incorrect\"],\n    provide_explanation=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Querying LLM Trace Data with Phoenix DSL - Python\nDESCRIPTION: Defines a SpanQuery filter for 'LLM' span kinds and selects the 'llm.input_messages' and 'llm.output_messages' as 'question' and 'outputs' respectively. Executes the query using Phoenix client on a specified project, returning a DataFrame for further tool call evaluation. Inputs: project_name string and Phoenix client instance; Outputs: DataFrame with queried columns. Dependencies: Phoenix, pandas, and a running Phoenix service.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nquery = (\n    SpanQuery()\n    .where(\n        # Filter for the `LLM` span kind.\n        # The filter condition is a string of valid Python boolean expression.\n        \"span_kind == 'LLM'\",\n    )\n    .select(\n        # Extract and rename the following span attributes\n        question=\"llm.input_messages\",\n        outputs=\"llm.output_messages\",\n    )\n)\ntrace_df = px.Client().query_spans(query, project_name=\"Tool Calling Eval\")\n# trace_df[\"tool_call\"] = trace_df[\"tool_call\"].fillna(\"No tool used\")\n```\n\n----------------------------------------\n\nTITLE: Defining llm_generate Function - Phoenix\nDESCRIPTION: This code defines the `llm_generate` function.  It is designed to generate text using a specified LLM and a template. The function takes a pandas DataFrame, a prompt template, an LLM model instance, an optional system instruction and output parser. It returns a dataframe where each row represents the generated output. The dataframe will include a column for the generated text.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evals-reference.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef llm_generate(\n    dataframe: pd.DataFrame,\n    template: Union[PromptTemplate, str],\n    model: Optional[BaseEvalModel] = None,\n    system_instruction: Optional[str] = None,\n    output_parser: Optional[Callable[[str, int], Dict[str, Any]]] = None,\n) -> List[str]\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix Packages (Cloud)\nDESCRIPTION: This command installs the `arize-phoenix-otel` package, which is necessary to enable tracing with Arize Phoenix. This package provides OpenTelemetry instrumentation for sending traces to the Arize Phoenix platform.  It is a prerequisite for tracing LlamaIndex workflows within Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex-1.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluation Function with GPT-4o Optimized Classifier - Python\nDESCRIPTION: Defines a new evaluation function test_dspy_prompt tailored to the classifier optimized via GPT-4o. It invokes optimized_classifier_using_gpt_4o with the input prompt and returns the result's label. Requires the optimized classifier with GPT-4o, and expects dictionary input with a 'prompt' field. Intended to be passed as a task to subsequent experiment runs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\n# Create evaluation function using optimized classifier\ndef test_dspy_prompt(input):\n    result = optimized_classifier_using_gpt_4o(prompt=input[\"prompt\"])\n    return result.label\n```\n\n----------------------------------------\n\nTITLE: Import Dependencies and Apply nest_asyncio Python\nDESCRIPTION: Imports various Python libraries required for the tutorial, including standard libraries like 'json', 'functools', 'textwrap', 'time', and 'typing', as well as external libraries like 'nest_asyncio', 'phoenix', 'datasets', 'llama_index', 'openinference', and 'opentelemetry'. 'nest_asyncio.apply()' is called to patch asyncio, enabling its use in environments that might otherwise have issues with nested event loops.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom functools import partial\nfrom textwrap import shorten\nfrom time import time_ns\nfrom typing import Tuple\n\nimport nest_asyncio\nimport phoenix as px\nfrom datasets import load_dataset\nfrom llama_index.core.evaluation import GuidelineEvaluator\nfrom llama_index.llms.openai import OpenAI\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom phoenix.experiments import evaluate_experiment, run_experiment\nfrom phoenix.experiments.types import Explanation, Score\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Defining Tool Schema Python\nDESCRIPTION: Defines the JSON schema for the tool (function) that the Anthropic API will use to extract structured data. The schema specifies the `name`, `description`, and `input_schema` of the tool, which defines the structure of the extracted data including the `location`, `budget_level`, and `purpose` of the trip, along with data types and enumerated values.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/anthropic_tracing_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntool_schema = {\n    \"name\": \"record_travel_request_attributes\",\n    \"description\": \"Records the attributes of a travel request\",\n    \"input_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": 'The desired destination location. Use city, state, and country format when possible. If no destination is provided, return \"not_stated\".',\n            },\n            \"budget_level\": {\n                \"type\": \"string\",\n                \"enum\": [\"low\", \"medium\", \"high\", \"not_stated\"],\n                \"description\": 'The desired budget level. If no budget level is provided, return \"not_stated\".',\n            },\n            \"purpose\": {\n                \"type\": \"string\",\n                \"enum\": [\"business\", \"pleasure\", \"other\", \"not_stated\"],\n                \"description\": 'The purpose of the trip. If no purpose is provided, return \"not_stated\".',\n            },\n        },\n        \"required\": [\"location\", \"budget_level\", \"purpose\"],\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Tool Call Names from Output Messages - Python\nDESCRIPTION: Defines a function get_tool_call that extracts the name of the called function from output messages, returning the function name if present or 'No tool used' if not. Applies this function to the 'outputs' column of trace_df, creating a new 'tool_call' column. Inputs: outputs list/dict from each trace; Outputs: string representing tool function name or default string. Requires output messages structurally containing a 'tool_calls' list under 'message'.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\ndef get_tool_call(outputs):\n    if outputs[0].get(\"message\").get(\"tool_calls\"):\n        return (\n            outputs[0]\n            .get(\"message\")\n            .get(\"tool_calls\")[0]\n            .get(\"tool_call\")\n            .get(\"function\")\n            .get(\"name\")\n        )\n    else:\n        return \"No tool used\"\n\n\ntrace_df[\"tool_call\"] = trace_df[\"outputs\"].apply(get_tool_call)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Creating LlamaIndex Query Engine\nDESCRIPTION: Configures the global LlamaIndex `Settings` to use specific OpenAI models: `gpt-4o` for the language model (`llm`) and `text-embedding-ada-002` for the embedding model (`embed_model`). It then loads the index from the previously configured `storage_context` and creates a `RetrieverQueryEngine` based on this index, ready to answer questions using RAG.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nSettings.llm = OpenAI(model=\"gpt-4o\")\nSettings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\nindex = load_index_from_storage(\n    storage_context,\n)\nquery_engine = index.as_query_engine()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Classifications with Classification Report and Confusion Matrix\nDESCRIPTION: This code snippet evaluates the predicted classifications against the ground truth labels. It generates a classification report using `sklearn.metrics.classification_report` and plots a confusion matrix using `pycm.ConfusionMatrix` and `matplotlib.pyplot`. The true labels are mapped from the DataFrame's 'is_well_coded' column using the provided prompt rails map.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_functionality_classifications.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df[\"is_well_coded\"].map(CODE_FUNCTIONALITY_PROMPT_RAILS_MAP).tolist()\n\nprint(classification_report(true_labels, relevance_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels, predict_vector=relevance_classifications, classes=rails\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Guideline-Based Evaluators in Python\nDESCRIPTION: Initializes a Llama-Index OpenAI LLM instance ('gpt-4o', temperature 0 for deterministic output). Defines a `guidelines` dictionary mapping evaluator names to descriptive evaluation criteria. Creates an asynchronous helper function `adapt` that wraps the Llama-Index `GuidelineEvaluator.aevaluate` method, taking the evaluator function (`fn`), task `output`, and task `input` as arguments. It calls the underlying evaluator with query, response, and contexts extracted from the input/output, then returns a tuple containing the evaluation score (`ans.passing`) and explanation (`ans.feedback`). Finally, it builds an `evaluators` dictionary using a dictionary comprehension, creating a partially applied `adapt` function for each guideline, bound to a `GuidelineEvaluator` initialized with the specific guideline and the 'gpt-4o' LLM.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nllm = OpenAI(temperature=0, model=\"gpt-4o\")\nguidelines = {\n    \"answer_fully\": \"The response should fully answer the query.\",\n    \"unambiguous\": \"The response should avoid being vague or ambiguous.\",\n    \"use_numbers\": \"The response should be specific and use statistics or numbers when possible.\",\n}\n\n\nasync def adapt(fn, output, input) -> Tuple[Score, Explanation]:\n    ans = await fn(\n        query=input[\"messages\"][0][\"content\"],\n        response=output,\n        contexts=[input[\"document\"]],\n    )\n    return ans.passing, ans.feedback\n\n\nevaluators = {\n    name: partial(adapt, GuidelineEvaluator(llm=llm, guidelines=guideline).aevaluate)\n    for name, guideline in guidelines.items()\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenInference Instrumentation (Python)\nDESCRIPTION: Sets up OpenInference tracing for LlamaIndex and LangChain. It registers a tracer provider pointing to the local Phoenix endpoint and instruments the libraries, ensuring that LLM application execution traces and spans are automatically collected by Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.langchain import LangChainInstrumentor\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\nLangChainInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Running Experiment - Phoenix DSPy - Python\nDESCRIPTION: Executes an experiment using the Arize Phoenix `run_experiment` function. It utilizes a specified `dataset`, the `test_dspy_prompt` function as the task, and a list of `evaluators`. Metadata about the specific DSPy prompt ID and descriptions for the experiment are included for tracking and analysis in the Phoenix UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndspy_experiment = run_experiment(\n    dataset,\n    task=test_dspy_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Prompt Optimization Technique #4: DSPy Prompt Tuning\",\n    experiment_name=\"dspy-optimization\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + dspy_prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Defining AnthropicModel Parameters in Python\nDESCRIPTION: This snippet defines the `AnthropicModel` class, inheriting from `BaseModel`. It specifies default parameters for interacting with Anthropic models, including model name, temperature, max tokens, sampling probabilities, stop sequences, and extra parameters.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evaluation-models.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass AnthropicModel(BaseModel):\n    model: str = \"claude-2.1\"\n    \"\"\"The model name to use.\"\"\"\n    temperature: float = 0.0\n    \"\"\"What sampling temperature to use.\"\"\"\n    max_tokens: int = 256\n    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n    top_p: float = 1\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n    top_k: int = 256\n    \"\"\"The cutoff where the model no longer selects the words.\"\"\"\n    stop_sequences: List[str] = field(default_factory=list)\n    \"\"\"If the model encounters a stop sequence, it stops generating further tokens.\"\"\"\n    extra_parameters: Dict[str, Any] = field(default_factory=dict)\n    \"\"\"Any extra parameters to add to the request body (e.g., countPenalty for a21 models)\"\"\"\n    max_content_size: Optional[int] = None\n    \"\"\"If you're using a fine-tuned model, set this to the maximum content size\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Code Generation Run Function\nDESCRIPTION: This function defines the task for the code generation agent, taking an Example object as input and extracting the question and data, executing the code generation logic using `extract_chart_config` and `generate_visualization` methods, and returning the generated code and chart configuration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ndef run_code_generation(example: Example) -> str:\n    with suppress_tracing():\n        chart_config = extract_chart_config(\n            data=example.input.get(\"example_data\"), visualization_goal=example.input.get(\"question\")\n        )\n        code = generate_visualization(\n            visualization_goal=example.input.get(\"question\"), data=example.input.get(\"example_data\")\n        )\n\n    return {\"code\": code, \"chart_config\": chart_config}\n```\n\n----------------------------------------\n\nTITLE: Displaying QA Correctness Evaluation Results\nDESCRIPTION: Displays the first few rows of the DataFrame containing the QA correctness evaluation results. This allows for a quick inspection of the scores and explanations generated by the `QAEvaluator`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nqa_correctness_eval_df.head()\n```\n\n----------------------------------------\n\nTITLE: Configuring DSPy Optimizer - GPT-4o - Python\nDESCRIPTION: Configures and compiles a DSPy pipeline optimizer (`MIPROv2`) using GPT-4o as the model for generating prompts (`prompt_model`) and a separate model (referenced by `turbo`) for executing the task (`task_model`). The optimizer is then used to `compile` the base `classifier` module against the `trainset`, producing a new `optimized_classifier_using_gpt_4o`. This demonstrates separating the optimization and task execution models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nprompt_gen_lm = dspy.LM(\"gpt-4o\")\ntp = dspy.MIPROv2(\n    metric=validate_classification, auto=\"light\", prompt_model=prompt_gen_lm, task_model=turbo\n)\noptimized_classifier_using_gpt_4o = tp.compile(classifier, trainset=train_data)\n```\n\n----------------------------------------\n\nTITLE: Calling OpenAI API and returning the result in JSON format\nDESCRIPTION: This helper function sends requests to the OpenAI API and returns the result in JSON format.  It takes a user prompt and user input as strings. It initializes an OpenAI client, calls the chat completion create method with a specific model, sets a system role for the prompt and returns the response as a dictionary. It handles any exceptions during the API call and returns an empty dictionary on error. It depends on the `openai` package.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndef call_openai_api(user_prompt: str, user_input: str) -> dict:\n    \"\"\"Issue requests to the OpenAI API\n\n    Parameters\n    ----------\n    user_prompt : str\n        Prompt template for OpenAI API\n    user_input : str\n        Prompt input for OpenAI API\n\n    Returns\n    -------\n    dict\n        Dictionary of response from OpenAI API\n    \"\"\"\n    client = OpenAI()\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            response_format={\"type\": \"json_object\"},\n            messages=[\n                {\"role\": \"system\", \"content\": user_prompt},\n                {\"role\": \"user\", \"content\": user_input},\n            ],\n        )\n        return json.loads(response.choices[0].message.content)\n    except Exception as e:\n        print(f\"Error calling OpenAI API: {e}\")\n        return {}\n```\n\n----------------------------------------\n\nTITLE: Viewing Traces with Document Retrieval Metadata\nDESCRIPTION: Displays a subset of the spans DataFrame focusing on trace name, kind, input values, and documents retrieved during queries, aiding in understanding retrieval behavior and trace details.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nspans_df[\"name\", \"span_kind\", \"attributes.input.value\", \"attributes.retrieval.documents\"].head()\n\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application - Python\nDESCRIPTION: This snippet launches the Phoenix application, providing a user interface for monitoring and evaluating the RAG pipeline. It allows the user to view traces, monitor pipeline performance, and understand the flow of data through various components.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n(session := px.launch_app()).view()\n```\n\n----------------------------------------\n\nTITLE: Defining Code Readability Evaluation Prompt Template\nDESCRIPTION: This snippet defines the prompt template used by a Large Language Model (LLM) to evaluate the readability of generated code. It instructs the LLM to act as a senior software engineer and judge the provided `code` snippet based on the given `query` (task assignment). The template explicitly limits the LLM's output to 'readable' or 'unreadable' for classification purposes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/code-generation-eval.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nYou are a stern but practical senior software engineer who cares a lot about simplicity and\nreadability of code. Can you review the following code that was written by another engineer?\nFocus on readability of the code. Respond with \"readable\" if you think the code is readable,\nor \"unreadable\" if the code is unreadable or needlessly complex for what it's trying\nto accomplish.\n\nONLY respond with \"readable\" or \"unreadable\"\n\nTask Assignment:\n```\n{query}\n```\n\nImplementation to Evaluate:\n```\n{code}\n```\n```\n\n----------------------------------------\n\nTITLE: Creating a Function Tool for the OpenAI Agent\nDESCRIPTION: Defines a math equation solver function tool using the OpenAI agents function_tool decorator, allowing the agent to evaluate mathematical expressions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/openai_agents_cookbook.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom agents import Runner, function_tool\n\n\n@function_tool\ndef solve_equation(equation: str) -> str:\n    \"\"\"Use python to evaluate the math equation, instead of thinking about it yourself.\n\n    Args:\n       equation: string which to pass into eval() in python\n    \"\"\"\n    return str(eval(equation))\n```\n\n----------------------------------------\n\nTITLE: Define Few-Shot CoT Prompt Template and Create Phoenix Prompt (Python)\nDESCRIPTION: Defines a multi-line string template for a Few-Shot Chain of Thought prompt, including instructions and a placeholder for examples. It formats the template by inserting the previously sampled `few_shot_examples` DataFrame. It then uses the Phoenix Client to create a new prompt version based on this formatted template and OpenAI's `gpt-3.5-turbo` model.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/chain-of-thought-prompting.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nfew_shot_COT_template = \"\"\nYou are an evaluator who outputs the answer to a math word problem. You must always think through the problem logically before providing an answer. Show some of your reasoning.\n\nFinally, output the integer answer ONLY on a final new line. In this final answer, be sure not include words, commas, labels, or units and round all decimals answers.\n\nHere are some examples of word problems, step by step explanations, and solutions to guide your reasoning:\n\n{examples}\n\"\"\"\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"system\", \"content\": few_shot_COT_template.format(examples=few_shot_examples)},\n        {\"role\": \"user\", \"content\": \"{{Problem}}\"},\n    ],\n)\n\nfew_shot_COT = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Few Shot COT prompt\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Running Configured Evaluations\nDESCRIPTION: Executes the defined evaluators on the exported dataframes (`queries_df`, `retrieved_documents_df`). The `run_evals` function orchestrates the evaluation process using the specified models and data. `nest_asyncio.apply()` is included to enable asynchronous operations within notebook environments. `provide_explanation=True` requests the LLM to generate explanations for its evaluation decisions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_quickstart.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.evals import (\n    run_evals,\n)\n\nnest_asyncio.apply()  # needed for concurrency in notebook environments\n\nhallucination_eval_df, qa_correctness_eval_df = run_evals(\n    dataframe=queries_df,\n    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n    provide_explanation=True,\n)\nrelevance_eval_df = run_evals(\n    dataframe=retrieved_documents_df,\n    evaluators=[relevance_evaluator],\n    provide_explanation=True,\n)[0]\n```\n\n----------------------------------------\n\nTITLE: Viewing Evaluation Results (Python)\nDESCRIPTION: Retrieves the evaluation results that were added to the `experiment` object by `evaluate_experiment`. This allows inspecting the scores and feedback generated by the evaluators for each experiment run.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nexperiment.get_evaluations()\n```\n\n----------------------------------------\n\nTITLE: Instrumenting LangChain with OpenInference - Python\nDESCRIPTION: Sets up instrumentation for LangChain operations using the OpenInference library. It registers a tracer provider (configured by Phoenix) and then uses the `LangChainInstrumentor` to automatically capture trace spans for LangChain activities, sending them to the configured provider.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_agent_tracing_tutorial.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ntracer_provider = register()\nLangChainInstrumentor(tracer_provider=tracer_provider).instrument(skip_dep_check=True)\n```\n\n----------------------------------------\n\nTITLE: Logging LLM Response Evaluations to Phoenix\nDESCRIPTION: Logs the calculated QA correctness and hallucination evaluations to the active Phoenix session. It uses `px.Client().log_evaluations` with `SpanEvaluations` for each evaluation type, allowing visualization and analysis alongside traces in the Phoenix UI. Assumes `qa_correctness_eval_df` and `hallucination_eval_df` contain the evaluation results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import SpanEvaluations\n\npx.Client().log_evaluations(\n    SpanEvaluations(dataframe=qa_correctness_eval_df, eval_name=\"Q&A Correctness\"),\n    SpanEvaluations(dataframe=hallucination_eval_df, eval_name=\"Hallucination\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Logging Span Evaluations to Phoenix\nDESCRIPTION: This code snippet logs the `hallucination_eval` and `qa_correctness_eval` DataFrames as span evaluations to the Phoenix server using the `log_evaluations` method. It uses the `SpanEvaluations` object to specify the evaluation name and DataFrame for each evaluation. This allows viewing evaluations within the Phoenix UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import SpanEvaluations\n\npx.Client().log_evaluations(\n    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval),\n    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval),\n)\n```\n\n----------------------------------------\n\nTITLE: Running Experiment with Gradient Prompt\nDESCRIPTION: This code snippet runs an experiment using the gradient optimized prompt. The `run_experiment` function is called, taking the dataset, the `test_gradient_prompt` function (defined to test the gradient based prompt), and an evaluator. Metadata including the description, experiment name, and the associated prompt ID are provided for experiment tracking and analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ngradient_experiment = run_experiment(\n    dataset,\n    task=test_gradient_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Prompt Optimization Technique #3: Prompt Gradients\",\n    experiment_name=\"gradient-optimization\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + gradient_prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Uploading a Dataset to Phoenix Using Python and Pandas\nDESCRIPTION: This snippet uploads a dataset to Phoenix by creating a pandas DataFrame containing a question-answer pair along with metadata. It initializes a Phoenix client to call `upload_dataset` providing the DataFrame, dataset name, and keys identifying inputs, outputs, and metadata. Inputs must be iterable, and outputs define the expected answers for evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/quickstart-datasets.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport phoenix as px\n\ndf = pd.DataFrame(\n    [\n        {\n            \"question\": \"What is Paul Graham known for?\",\n            \"answer\": \"Co-founding Y Combinator and writing on startups and techology.\",\n            \"metadata\": {\"topic\": \"tech\"},\n        }\n    ]\n)\nphoenix_client = px.Client()\ndataset = phoenix_client.upload_dataset(\n    dataframe=df,\n    dataset_name=\"test-dataset\",\n    input_keys=[\"question\"],\n    output_keys=[\"answer\"],\n    metadata_keys=[\"metadata\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix in a Notebook\nDESCRIPTION: Python code to launch the Phoenix app directly from a notebook.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langchain.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies with pip (bash)\nDESCRIPTION: This snippet installs the necessary Python packages required to run the tutorial. It includes `arize-phoenix` for interacting with the Phoenix platform, `openinference-instrumentation-openai` and `openai` for using and tracing the OpenAI API, `datasets` for loading sample data, and `httpx` with a version constraint.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n%%bash\npip install -q \"arize-phoenix>=4.29.0\" openinference-instrumentation-openai openai datasets 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Running Hallucination Classification with GPT-4-Turbo\nDESCRIPTION: Tests the preview version of GPT-4-Turbo for hallucination detection, using the same evaluation framework to compare with other models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nrails = list(HALLUCINATION_PROMPT_RAILS_MAP.values())\nhallucination_classifications = llm_classify(\n    dataframe=df,\n    template=HALLUCINATION_PROMPT_TEMPLATE,\n    model=OpenAIModel(model_name=\"gpt-4-turbo-preview\", temperature=0.0),\n    rails=rails,\n    concurrency=20,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Making an OpenAI Chat Completion Request\nDESCRIPTION: Initializes an OpenAI client, sets the desired model and message, then executes a chat completion request. The response is printed, with traces automatically collected and reported to Phoenix due to prior instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/tracing_quickstart_openai.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nclient = openai.OpenAI()\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a haiku.\"}],\n)\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Running the Agent - Python\nDESCRIPTION: The `run_agent` function encapsulates the core logic of the agent. It handles message formatting, adds a system prompt if needed, and then enters a loop where it calls a language model, processes tool calls if present, and returns the final response. This function assumes the existence of a `client` object and predefined `tools` and `model` variables.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ndef run_agent(messages):\n    print(\"Running agent with messages:\", messages)\n    if isinstance(messages, str):\n        messages = [{\"role\": \"user\", \"content\": messages}]\n        print(\"Converted string message to list format\")\n\n    # Check and add system prompt if needed\n    if not any(\n        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n    ):\n        system_prompt = {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n        }\n        messages.append(system_prompt)\n        print(\"Added system prompt to messages\")\n\n    while True:\n        # Router call span\n        print(\"Starting router call span\")\n        with tracer.start_as_current_span(\n            \"router_call\",\n            openinference_span_kind=\"chain\",\n        ) as span:\n            span.set_input(value=messages)\n\n            response = client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n            )\n\n            messages.append(response.choices[0].message.model_dump())\n            tool_calls = response.choices[0].message.tool_calls\n            print(\"Received response with tool calls:\", bool(tool_calls))\n            span.set_status(StatusCode.OK)\n\n            if tool_calls:\n                # Tool calls span\n                print(\"Processing tool calls\")\n                messages = handle_tool_calls(tool_calls, messages)\n                span.set_output(value=tool_calls)\n            else:\n                print(\"No tool calls, returning final response\")\n                span.set_output(value=response.choices[0].message.content)\n\n                return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Running LLM-as-a-Judge Experiment with Few-Shot Prompt - Python\nDESCRIPTION: Defines a judge function that uses the improved few-shot prompt for response evaluation, and runs an experiment over a dataset. The function passes the user response into a DataFrame, applies the improved empathy prompt template (with few-shot examples), specifies the model (OpenAIModel with GPT-4), and sets evaluation rails (1-10). The experiment is initialized with the judge task, response evaluator, and a descriptive experiment name. Requires llm_classify, OpenAIModel, EMPATHY_EVALUATION_PROMPT_TEMPLATE_IMPROVED, and evaluate_response to be defined elsewhere.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef llm_as_a_judge(input):\n    response_classifications = llm_classify(\n        dataframe=pd.DataFrame([{\"AI_Response\": input[\"AI_Response\"]}]),\n        template=EMPATHY_EVALUATION_PROMPT_TEMPLATE_IMPROVED,\n        model=OpenAIModel(model=\"gpt-4\"),\n        rails=list(map(str, range(1, 11))),\n        provide_explanation=True,\n    )\n    score = response_classifications.iloc[0][\"label\"]\n    return int(score)\n\n\nexperiment = run_experiment(\n    dataset,\n    task=llm_as_a_judge,\n    evaluators=[evaluate_response],\n    experiment_name=\"few_shot_examples\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix and LlamaIndex\nDESCRIPTION: Installs the necessary Python packages for running LlamaIndex RAG experiments with Arize Phoenix, including specific versions and instrumentation libraries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/run_experiments_with_llama_index.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uqqq \"arize-phoenix[llama-index]>=4.6\" sentence-transformers torch openinference-instrumentation-llama_index\n```\n\n----------------------------------------\n\nTITLE: Defining a Function Tool for OpenAI Agents in Python\nDESCRIPTION: This Python function `solve_equation` is decorated with `@function_tool` to make it available as a tool for the OpenAI Agent. It takes a string equation and uses Python's `eval()` to compute the result, returning it as a string. The docstring serves as the tool's description for the agent.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/ragas_agents_cookbook_phoenix.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom agents import Runner, function_tool\n\n\n@function_tool\ndef solve_equation(equation: str) -> str:\n    \"\"\"Use python to evaluate the math equation, instead of thinking about it yourself.\n\n    Args:\"\n       equation: string which to pass into eval() in python\n    \"\"\"\n    return str(eval(equation))\n```\n\n----------------------------------------\n\nTITLE: Constructing and Executing Span Queries with Phoenix DSL - Python\nDESCRIPTION: This snippet shows how to use the Phoenix query DSL (Domain Specific Language) to build and run a custom span query that filters for RETRIEVER spans and selects the input value attribute, renaming it in the result DataFrame. It uses the SpanQuery builder for readable query composition, and the result is a pandas DataFrame indexed by span_id. Requires the phoenix Python package with phoenix.trace.dsl.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport phoenix as px\nfrom phoenix.trace.dsl import SpanQuery\n\nquery = SpanQuery().where(\n    # Filter for the `RETRIEVER` span kind.\n    # The filter condition is a string of valid Python boolean expression.\n    \"span_kind == 'RETRIEVER'\",\n).select(\n    # Extract the span attribute `input.value` which contains the query for the\n    # retriever. Rename it as the `input` column in the output dataframe.\n    input=\"input.value\",\n)\n\n# The Phoenix Client can take this query and return the dataframe.\npx.Client().query_spans(query)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Document Retrieval Relevance with LLM\nDESCRIPTION: Uses LLM classification to evaluate the relevance of retrieved documents to queries, which helps identify if retrieval issues are causing hallucinations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n    RAG_RELEVANCY_PROMPT_TEMPLATE,\n    OpenAIModel,\n    llm_classify,\n)\n\nretrieved_documents_eval = llm_classify(\n    dataframe=retrieved_documents_df,\n    model=OpenAIModel(model=\"gpt-4o\", temperature=0.0),\n    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n    provide_explanation=True,\n)\n\nretrieved_documents_eval[\"score\"] = (\n    retrieved_documents_eval.label[~retrieved_documents_eval.label.isna()] == \"relevant\"\n).astype(int)\n```\n\n----------------------------------------\n\nTITLE: Defining ROUGE-1 and Token Count Evaluators - Python\nDESCRIPTION: Implements helper functions for calculating ROUGE-1 precision, recall, and F1 score using the `rouge` library. Defines evaluator functions (`rouge_1_f1_score`, `rouge_1_precision`, `rouge_1_recall`, `num_tokens`) that conform to the Phoenix evaluator signature. The `num_tokens` function uses `tiktoken` to count tokens based on the specified model encoding. Finally, it creates a list `EVALUATORS` containing these evaluator functions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/summarization.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport tiktoken\nfrom rouge import Rouge\n\n\n# convenience functions\ndef _rouge_1(hypothesis: str, reference: str) -> Dict[str, Any]:\n    scores = Rouge().get_scores(hypothesis, reference)\n    return scores[0][\"rouge-1\"]\n\n\ndef _rouge_1_f1_score(hypothesis: str, reference: str) -> float:\n    return _rouge_1(hypothesis, reference)[\"f\"]\n\n\ndef _rouge_1_precision(hypothesis: str, reference: str) -> float:\n    return _rouge_1(hypothesis, reference)[\"p\"]\n\n\ndef _rouge_1_recall(hypothesis: str, reference: str) -> float:\n    return _rouge_1(hypothesis, reference)[\"r\"]\n\n\n# evaluators\ndef rouge_1_f1_score(output: str, expected: Dict[str, Any]) -> float:\n    return _rouge_1_f1_score(hypothesis=output, reference=expected[\"summary\"])\n\n\ndef rouge_1_precision(output: str, expected: Dict[str, Any]) -> float:\n    return _rouge_1_precision(hypothesis=output, reference=expected[\"summary\"])\n\n\ndef rouge_1_recall(output: str, expected: Dict[str, Any]) -> float:\n    return _rouge_1_recall(hypothesis=output, reference=expected[\"summary\"])\n\n\ndef num_tokens(output: str) -> int:\n    encoding = tiktoken.encoding_for_model(gpt_4o)\n    return len(encoding.encode(output))\n\n\nEVALUATORS = [rouge_1_f1_score, rouge_1_precision, rouge_1_recall, num_tokens]\n```\n\n----------------------------------------\n\nTITLE: Logging Evaluations to Phoenix - Python\nDESCRIPTION: This code logs the evaluation results to Phoenix using `px.Client().log_evaluations`. It creates a `SpanEvaluations` object with the eval name \"Tool Calling Eval\" and the evaluation DataFrame, `tool_call_eval`. This allows for visualization and analysis of the evaluation results within the Phoenix platform.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\npx.Client().log_evaluations(\n    SpanEvaluations(eval_name=\"Tool Calling Eval\", dataframe=tool_call_eval),\n)\n```\n\n----------------------------------------\n\nTITLE: Running Experiment - Phoenix DSPy GPT-4o - Python\nDESCRIPTION: Executes a second experiment using Arize Phoenix with the classifier optimized using GPT-4o. It uses the same `dataset` and `evaluators` as the first experiment but utilizes the redefined `test_dspy_prompt` which calls the GPT-4o optimized classifier. This experiment is configured with a unique name and description to differentiate it in the Phoenix UI, allowing comparison between optimization techniques.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\n# Run experiment with DSPy-optimized classifier\ndspy_experiment_using_gpt_4o = run_experiment(\n    dataset,\n    task=test_dspy_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Prompt Optimization Technique #5: DSPy Prompt Tuning with GPT-4o\",\n    experiment_name=\"dspy-optimization-gpt-4o\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + dspy_prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Using llm_generate for Numeric Score Evaluation with Templates\nDESCRIPTION: This snippet illustrates how to perform a regression-based evaluation using Phoenix's llm_generate function with a score template. It demonstrates setting up the template to assess grammatical errors as a continuous score, passing a pandas dataframe for evaluation, and including a custom output parser callback to extract the numeric score from the LLM output. It emphasizes careful regex parsing to ensure accurate score extraction.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/bring-your-own-evaluator.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.evals import (\n    llm_generate,\n    OpenAIModel # see https://docs.arize.com/phoenix/evaluation/evaluation-models\n)\n\n# Define the score template\nSCORE_TEMPLATE = \"\"\"\nYou are a helpful AI bot that checks for grammatical, spelling and typing errors\nin a document context. You are going to return a continous score for the\ndocument based on the percent of grammatical and typing errors. The score should be\nbetween 10 and 1. A score of 1 will be no grammatical errors in any word,\na score of 2 will be 20% of words have errors, a 5 score will be 50% errors,\na 7 is 70%, and a 10 score will be all words in the context have a\ngrammatical errors.\n\nThe following is the document context.\n\n#CONTEXT\n{context}\n#ENDCONTEXT\n\n#QUESTION\nPlease return a score between 10 and 1.\nYou will return no other text or language besides the score. Only return the score.\nPlease return in a format that is \"the score is: 10\" or \"the score is: 1\"\n\"\"\"\n\n# Run evaluation with llm_generate\n\nresults = llm_generate(\n    dataframe=<YOUR_DATAFRAME_GOES_HERE>,\n    template=SCORE_TEMPLATE,\n    model=OpenAIModel('gpt-4o', api_key=''),\n    verbose=True,\n    output_parser=numeric_score_eval,\n    include_prompt=True,\n    include_response=True,\n)\n\ndef numeric_score_eval(output, row_index):\n    # Extract score from output string using regex\n    row = df.iloc[row_index]\n    score = self.find_score(output)\n    return {\"score\": score}\n\ndef find_score(self, output):\n    pattern = r\"score is.*?([+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?)]\"\n    match = re.search(pattern, output, re.IGNORECASE)\n    if match:\n        return float(match.group(1))\n    else:\n        return None\n```\n\n----------------------------------------\n\nTITLE: Building a RetrievalQA Chain Using LangChain and VertexAI Embeddings/Chat in Python\nDESCRIPTION: Defines embedding and chat models using VertexAIâ€™s 'textembedding-gecko' for vector embeddings and 'chat-bison' for conversational responses. Loads a dataset of document vectors and texts, creates a KNN retriever for similarity search, and instantiates a RetrievalQA chain that orchestrates retrieval and question answering over the Arize documentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_vertex_ai_tracing_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nembeddings = VertexAIEmbeddings(\n    model_name=\"textembedding-gecko\",\n    project=project_id,\n    location=location,\n)\ndatabase_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/langchain-vertexai/database.parquet\"\n)\nknn_retriever = KNNRetriever(\n    index=np.stack(database_df[\"text_vector\"]),\n    texts=database_df[\"text\"].tolist(),\n    embeddings=embeddings,\n)\nllm = ChatVertexAI(\n    model_name=\"chat-bison\",\n    project=project_id,\n    location=location,\n)\nchain = RetrievalQA.from_chain_type(\n    llm=llm,\n    retriever=knn_retriever,\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying Full Endpoint Directly for Phoenix OTEL with gRPC Transport in Python\nDESCRIPTION: Demonstrates specifying a fully qualified gRPC collector endpoint directly when initializing the tracer provider. The gRPC protocol is inferred from the endpoint URL, enabling direct span export to a Phoenix gRPC server without relying on environment variables.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\ntracer_provider = register(endpoint=\"http://localhost:4317\")\n\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix with LlamaIndex (Python, Shell)\nDESCRIPTION: Installs the arize-phoenix Python package with experimental and LlamaIndex features, ensuring all pipeline and evaluation capabilities are available. This step must be run in a terminal or notebook cell before proceeding. Requires an active Python environment and network access.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n!pip install -qq \"arize-phoenix[experimental,llama-index]>=2.0\"\n```\n\n----------------------------------------\n\nTITLE: Using Pre-built Keyword Matching Evaluator in Phoenix with Python\nDESCRIPTION: This snippet demonstrates instantiating a pre-built evaluator to check if any specified keywords appear in the task outputs. It imports the `ContainsAnyKeyword` evaluator from Phoenix, sets keywords like \"Y Combinator\" and \"YC\", allowing automated grading based on keyword presence. This evaluator works on string outputs and requires the Phoenix evaluators module.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/quickstart-datasets.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments.evaluators import ContainsAnyKeyword\n\ncontains_keyword = ContainsAnyKeyword(keywords=[\"Y Combinator\", \"YC\"])\n```\n\n----------------------------------------\n\nTITLE: Evaluating GPT-4 Turbo Performance with Confusion Matrix\nDESCRIPTION: Analyzes classification results from GPT-4 Turbo, comparing to ground truth, and visualizes confusion matrix to assess consistency with other models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\ntrue_labels = df[\"readable\"].map(CODE_READABILITY_PROMPT_RAILS_MAP).tolist()\n\nprint(classification_report(true_labels, readability_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels, predict_vector=readability_classifications, classes=rails\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix Experiment\nDESCRIPTION: This snippet demonstrates how to install the necessary Python packages to run a Phoenix experiment, including the Phoenix library itself, the OpenAI instrumentation, and the OpenAI library for making LLM calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/run-experiments.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix openinference-instrumentation-openai openai\n```\n\n----------------------------------------\n\nTITLE: Evaluating Tool Calls with LLM-as-a-Judge - Python\nDESCRIPTION: This code uses `llm_classify` to evaluate tool calls by another LLM. It applies a prompt template to each row of the `tool_calls_df` DataFrame, classifying whether the tool call was \"correct\" or \"incorrect\". It also calculates a score based on the classification result. The template checks if agent router chooses the tool in accordance to defined list of tools.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\ntool_call_eval = llm_classify(\n    dataframe=tool_calls_df,\n    template=TOOL_CALLING_PROMPT_TEMPLATE.template.replace(\n        \"{tool_definitions}\",\n        \"generate_visualization, lookup_sales_data, analyze_sales_data, run_python_code\",\n    ),\n    rails=[\"correct\", \"incorrect\"],\n    model=eval_model,\n    provide_explanation=True,\n)\n\ntool_call_eval[\"score\"] = tool_call_eval.apply(\n    lambda x: 1 if x[\"label\"] == \"correct\" else 0, axis=1\n)\n\ntool_call_eval.head()\n```\n\n----------------------------------------\n\nTITLE: Launch Phoenix App - Python\nDESCRIPTION: This snippet launches the Phoenix application with the primary and reference inferences. It uses the `px.launch_app` function, passing in the `primary` and `reference` attributes from the `inferences` object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/use-example-inferences.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npx.launch_app(inferences.primary, inferences.reference)\n```\n\n----------------------------------------\n\nTITLE: Running Agent and Tracking Conversation Path in Python\nDESCRIPTION: Defines two functions: `run_agent_and_track_path` wraps the agent execution for a single Phoenix `Example`, calling `run_agent_messages` and returning the path length and formatted steps. `run_agent_messages` manages the conversation loop with an OpenAI model (`client.chat.completions.create`), ensures a system prompt exists, invokes the model with tools, handles potential tool calls via `handle_tool_calls` (not shown), and returns the complete message history upon completion. Includes print statements for debugging.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\ndef run_agent_and_track_path(example: Example) -> str:\n    print(\"Starting main span with messages:\", example.input.get(\"question\"))\n    messages = [{\"role\": \"user\", \"content\": example.input.get(\"question\")}]\n    ret = run_agent_messages(messages)\n    return {\"path_length\": len(ret), \"messages\": format_message_steps(ret)}\n\n\ndef run_agent_messages(messages):\n    print(\"Running agent with messages:\", messages)\n    if isinstance(messages, str):\n        messages = [{\"role\": \"user\", \"content\": messages}]\n        print(\"Converted string message to list format\")\n\n    # Check and add system prompt if needed\n    if not any(\n        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n    ):\n        system_prompt = {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n        }\n        messages.append(system_prompt)\n        print(\"Added system prompt to messages\")\n\n    while True:\n        # Router call span\n        print(\"Starting router\")\n\n        response = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            tools=tools,\n        )\n\n        messages.append(response.choices[0].message.model_dump())\n        tool_calls = response.choices[0].message.tool_calls\n        print(\"Received response with tool calls:\", bool(tool_calls))\n\n        if tool_calls:\n            # Tool calls span\n            print(\"Processing tool calls\")\n            tool_calls = response.choices[0].message.tool_calls\n            messages = handle_tool_calls(tool_calls, messages)\n        else:\n            print(\"No tool calls, returning final response\")\n            return messages\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix OTel with Customization - Python\nDESCRIPTION: This Python code configures the Phoenix OTel with `project_name`, `headers` and `batch` options using the `register` function. It utilizes a custom authorization header, sets a project name, and specifies batch processing.  It requires the `phoenix.otel` module.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-otel/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\ntracer_provider = register(\n    project_name=\"otel-test\", headers={\"Authorization\": \"Bearer TOKEN\"}, batch=True\n)\n```\n\n----------------------------------------\n\nTITLE: Running the Agent with Sample User Message\nDESCRIPTION: Executes the compiled workflow app with a user prompt, retrieves the final answer from the agent's tool call, and outputs the result, demonstrating an end-to-end query processing example.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nmessages = app.invoke({\"messages\": [(\"user\", \"Which sales agent made the most in sales in 2009?\")]})\njson_str = messages[\"messages\"][-1].tool_calls[0][\"args\"][\"final_answer\"]\njson_str\n```\n\n----------------------------------------\n\nTITLE: View Experiment Results as DataFrame Python\nDESCRIPTION: Calls the 'as_dataframe()' method on the experiment object returned by 'run_experiment'. This method converts the experiment's run results (including inputs, outputs, and potentially tracing information) into a Pandas DataFrame. This provides a tabular view for easy inspection and analysis of the experiment's outcomes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nexperiment.as_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Toxicity Evaluation using OpenAI Model\nDESCRIPTION: This Python snippet demonstrates how to perform a toxicity evaluation using an OpenAI model. It initializes the model, sets parameters, and uses the llm_classify function to classify the toxicity of text within a dataframe. The rails are used to ensure the output is a single word, either \"toxic\" or \"non-toxic\".\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/toxicity.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    TOXICITY_PROMPT_RAILS_MAP,\n    TOXICITY_PROMPT_TEMPLATE,\n    OpenAIModel,\n    download_benchmark_dataset,\n    llm_classify,\n)\n\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\n\n#The rails is used to hold the output to specific values based on the template\n#It will remove text such as \",,,\" or \"...\"\n#Will ensure the binary value expected from the template is returned \nails = list(TOXICITY_PROMPT_RAILS_MAP.values())\ntoxic_classifications = llm_classify(\n    dataframe=df_sample,\n    template=TOXICITY_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    provide_explanation=True, #optional to generate explanations for the value produced by the eval LLM\n)\n```\n\n----------------------------------------\n\nTITLE: Manually Instrumenting LangChain Callbacks\nDESCRIPTION: This TypeScript code snippet demonstrates how to manually instrument the `@langchain/core/callbacks/manager` module in LangChain using the `LangChainInstrumentation`.  It initializes a `NodeTracerProvider`, registers it, creates an instance of `LangChainInstrumentation`, and then manually instruments the `CallbackManagerModule`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langchain.js.md#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { NodeTracerProvider } from \"@opentelemetry/sdk-trace-node\";\nimport {\n  LangChainInstrumentation\n} from \"@arizeai/openinference-instrumentation-langchain\";\nimport * as CallbackManagerModule from \"@langchain/core/callbacks/manager\";\n\nconst provider = new NodeTracerProvider();\nprovider.register();\n\nconst lcInstrumentation = new LangChainInstrumentation();\n// LangChain must be manually instrumented as it doesn't have \n// a traditional module structure\nlcInstrumentation.manuallyInstrument(CallbackManagerModule);\n```\n\n----------------------------------------\n\nTITLE: Centering Embedding Distributions\nDESCRIPTION: Computes a centroid (mean) for the embedding column within the database and query dataframes and then applies a transformation to center the data around these centroids.  This is essential for aligning the distributions of query and context embeddings for accurate analysis in Phoenix. The transformation subtracts the centroid from each data point. Dependencies include `pandas` and the initialized database and query DataFrames. The output is the modified DataFrames with centered embedding vectors.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/milvus_llamaindex_search_and_retrieval_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndatabase_embedding_column_name = \"text_vector\"\ndatabase_centroid = database_df[database_embedding_column_name].mean()\ndatabase_df[database_embedding_column_name] = database_df[database_embedding_column_name].apply(\n    lambda x: x - database_centroid\n)\nquery_embedding_column_name = \":feature.[float].embedding:prompt\"\nquery_centroid = query_df[query_embedding_column_name].mean()\nquery_df[query_embedding_column_name] = query_df[query_embedding_column_name].apply(\n    lambda x: x - query_centroid\n)\n```\n\n----------------------------------------\n\nTITLE: Logging Document Evaluations to Phoenix\nDESCRIPTION: Logs the document relevance evaluations to Phoenix, which automatically calculates retrieval metrics like precision, NDCG, and hit rate.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import DocumentEvaluations\n\npx.Client().log_evaluations(\n    DocumentEvaluations(eval_name=\"Relevance\", dataframe=retrieved_documents_eval)\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Schema with Features and Tags in Python\nDESCRIPTION: Creates a Phoenix schema that explicitly defines DataFrame columns for predicted labels, actual labels, model input features, and descriptive tags. Relies on the Phoenix (`px`) library. The parameters include `prediction_label_column_name`, `actual_label_column_name`, a list of `feature_column_names`, and a list of `tag_column_names`. Useful for datasets where features and metadata need to be differentiatedâ€”features are used as model inputs, while tags are metadata for cohort analysis. The DataFrame must contain columns matching all specified feature and tag names.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nschema = px.Schema(\n    prediction_label_column_name=\"predicted\",\n    actual_label_column_name=\"target\",\n    feature_column_names=[\n        \"fico_score\",\n        \"merchant_id\",\n        \"loan_amount\",\n        \"annual_income\",\n        \"home_ownership\",\n        \"num_credit_lines\",\n        \"inquests_in_last_6_months\",\n        \"months_since_last_delinquency\",\n    ],\n    tag_column_names=[\n        \"age\",\n        \"gender\",\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application with LLM Inferences (Python)\nDESCRIPTION: Launches the interactive Phoenix web application using `px.launch_app`. It takes the previously created `primary_inferences` object, which contains the LLM prompt/response data and its schema, as input. This function starts a local web server and opens the Phoenix UI in a browser for exploring the inferences.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/prompt-and-response-llm.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsession = px.launch_app(primary_inferences)\n```\n\n----------------------------------------\n\nTITLE: Defining Task and Evaluation Functions for Zero-Shot Prompting in Python\nDESCRIPTION: Defines two functions: `zero_shot_prompt` which takes an input dictionary, formats the zero-shot prompt with the review, calls the OpenAI API, and returns the model's sentiment prediction. `evaluate_response` compares the model's output to the expected sentiment label (case-insensitive).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/few-shot-prompting.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef zero_shot_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(**prompt.format(variables={\"Review\": input[\"Review\"]}))\n    return resp.choices[0].message.content.strip()\n\n\ndef evaluate_response(output, expected):\n    return output.lower() == expected[\"Sentiment\"].lower()\n```\n\n----------------------------------------\n\nTITLE: Defining utility functions and creating FunctionTool instances\nDESCRIPTION: Implements simple arithmetic functions `multiply` and `add`, each annotated with docstrings. Converts these functions into `FunctionTool` objects suitable for use by the agent, providing reusable tools for calculation tasks within conversations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_openai_agent_tracing_tutorial.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two integers and return the result.\"\"\"\n    return a * b\n\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\n\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two integers and return the result.\"\"\"\n    return a + b\n\nadd_tool = FunctionTool.from_defaults(fn=add)\n```\n\n----------------------------------------\n\nTITLE: AI-Powered Sales Data Analysis Tool - Python\nDESCRIPTION: Defines a Phoenix-traced tool that leverages an LLM via the OpenAI API to analyze a subset of sales data according to a user question/prompt. Constructs a formatted prompt for the LLM, sends it, and extracts the model's analytic response. Requires a loaded LLM client, project model, and tracing. Inputs: user analysis prompt and sales data string; output: scalar string analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@tracer.tool()\ndef analyze_sales_data(prompt: str, data: str) -> str:\n    \"\"\"Implementation of AI-powered sales data analysis\"\"\"\n    # Construct prompt based on analysis type and data subset\n    prompt = f\"\"\"Analyze the following data: {data}\n    Your job is to answer the following question: {prompt}\"\"\"\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n    )\n\n    analysis = response.choices[0].message.content\n    return analysis if analysis else \"No analysis could be generated\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Phoenix Client and Eval Model - Python\nDESCRIPTION: This code initializes a Phoenix client, `px_client`, and defines an evaluation model, `eval_model`, using `OpenAIModel` with the \"gpt-4o-mini\" model. The Phoenix client is used for interacting with the Phoenix platform, and the evaluation model is used for LLM-based evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\npx_client = px.Client()\neval_model = OpenAIModel(model=\"gpt-4o-mini\")\n```\n\n----------------------------------------\n\nTITLE: Defining Asynchronous Evaluators for Answer and Context Relevancy in Python\nDESCRIPTION: Defines two async evaluator functions using Llama-Index's `AnswerRelevancyEvaluator` and `ContextRelevancyEvaluator` classes backed by OpenAI models. Each function asynchronously evaluates either the task's answer or retrieved contexts for relevancy against the query input, returning a score and explanatory feedback. These can be applied during experiment evaluation to assess output quality.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nasync def answer_relevancy(output, input) -> Tuple[Score, Explanation]:\n    ans = await AnswerRelevancyEvaluator(\n        llm=OpenAI(temperature=0, model=\"gpt-4o\"),\n    ).aevaluate(input[\"query\"], response=output[\"response\"])\n    return ans.score, ans.feedback\n\n\nasync def context_relevancy(output, input) -> Tuple[Score, Explanation]:\n    ans = await ContextRelevancyEvaluator(\n        llm=OpenAI(temperature=0, model=\"gpt-4o\"),\n    ).aevaluate(input[\"query\"], contexts=output[\"contexts\"])\n    return ans.score, ans.feedback\n\n\nevaluators = [answer_relevancy, context_relevancy]\n```\n\n----------------------------------------\n\nTITLE: Setting Up LLM Evaluation for Hallucination Detection\nDESCRIPTION: Configures and runs LLM-based evaluation to detect hallucinations in the RAG system responses using predefined templates and the Ollama model, with explanations for each classification.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.evals import (\n    HALLUCINATION_PROMPT_RAILS_MAP,\n    HALLUCINATION_PROMPT_TEMPLATE,\n    QA_PROMPT_RAILS_MAP,\n    QA_PROMPT_TEMPLATE,\n    LiteLLMModel,\n    llm_classify,\n)\n\nnest_asyncio.apply()\n\n# Check if the application has any indications of hallucinations\nhallucination_eval = llm_classify(\n    dataframe=queries_df,\n    model=LiteLLMModel(model=\"ollama/\" + ollama_model),\n    template=HALLUCINATION_PROMPT_TEMPLATE,\n    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n    provide_explanation=True,  # Makes the LLM explain its reasoning\n)\nhallucination_eval[\"score\"] = (\n    hallucination_eval.label[~hallucination_eval.label.isna()] == \"factual\"\n).astype(int)\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Schema for Text Embeddings (Python)\nDESCRIPTION: This code defines a schema for a dataset containing text embeddings for product reviews. It identifies 'sentiment' as the ground truth label, 'category' as a standard feature, and 'name' as a tag column. It configures an embedding feature 'product_review_embeddings', connecting the 'text_vector' column to the 'text' column via `raw_data_column_name`, enabling Phoenix to display the original text alongside its embedding.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nschema = px.Schema(\n    actual_label_column_name=\"sentiment\",\n    feature_column_names=[\n        \"category\",\n    ],\n    tag_column_names=[\n        \"name\",\n    ],\n    embedding_feature_column_names={\n        \"product_review_embeddings\": px.EmbeddingColumnNames(\n            vector_column_name=\"text_vector\",\n            raw_data_column_name=\"text\",\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Running Evaluations on Phoenix Experiment Results in Python\nDESCRIPTION: Uses the `phoenix.experiments.evaluate_experiment` function to apply the defined `evaluators` to the existing `experiment` object. This function iterates through each run in the `experiment`, executes the corresponding evaluation functions defined in the `evaluators` dictionary using the run's input and output, and stores the resulting scores and explanations back into the `experiment` object. The updated `experiment` object is reassigned.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nexperiment = evaluate_experiment(experiment, evaluators)\n```\n\n----------------------------------------\n\nTITLE: Importing Modules and Applying Nest AsyncIO in Python\nDESCRIPTION: Imports core Python modules and LLM SDKs required for dataset handling, API interactions, and Phoenix integrations. Applies nest_asyncio to allow nested event loops necessary for asynchronous calls in some environments. Sets up core dependencies for subsequent operations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\nfrom secrets import token_hex\n\nimport anthropic\nimport google.generativeai as genai\nimport groq\nimport nest_asyncio\nimport openai\nimport pandas as pd\nfrom google.generativeai.generative_models import GenerativeModel\nfrom openinference.instrumentation.anthropic import AnthropicInstrumentor\nfrom openinference.instrumentation.groq import GroqInstrumentor\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nfrom openinference.instrumentation.vertexai import VertexAIInstrumentor\nfrom sklearn.metrics import accuracy_score\n\nimport phoenix as px\nfrom phoenix.client import Client\nfrom phoenix.client.types import PromptVersion\nfrom phoenix.experiments import run_experiment\nfrom phoenix.otel import register\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Searching for Purchase Item using Pandas and OpenTelemetry\nDESCRIPTION: This function creates a span to search for a purchase item using a Pandas DataFrame. It takes a user payload in JSON format, a Pandas DataFrame containing item data, and an OpenTelemetry tracer as input.  It starts a span named \"Search for Purchase Item\", loads the user payload, calls `update_payload_with_search_results` to update the payload with search results, sets custom attributes (shopping_category, Item, Stars), defines the span type as \"CHAIN\", sets the status code, and returns the updated payload as a JSON string. It depends on `pandas` and `opentelemetry` packages.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef item_search(\n    user_payload_json: str,\n    items_df: pd.DataFrame,\n    tracer: opentelemetry.sdk.trace.Tracer,\n) -> str:\n    \"\"\"Create span of the search for a purchase item\n\n    Parameters\n    ----------\n    user_payload_json : str\n        Item payload JSON string\n    items_df : pd.DataFrame\n        DataFrame containting item data\n    tracer : opentelemetry.sdk.trace.Tracer\n        Tracer to handle span creation\n\n    Returns\n    -------\n    str\n        JSON formatted string of the item payload\n    \"\"\"\n    # Define Span Name & Start\n    with tracer.start_as_current_span(\"Search for Purchase Item\") as span:\n        trace_api.get_current_span()\n        user_payload_dict = json.loads(user_payload_json)\n        updated_dict = update_payload_with_search_results(\n            user_payload_dict, items_df, \"Best Use\", \"shopping_category\"\n        )\n\n        # Define Custom Attribute String - Shopping Category String\n        span.set_attribute(\"shopping_category.name\", updated_dict[\"shopping_category\"])\n\n        # Define Custom Attribute String - Item String\n        span.set_attribute(\"Item.name\", updated_dict[\"Item\"])\n\n        # Define Custom Attribute Value - Stars Value\n        span.set_attribute(\"Stars.value\", updated_dict[\"Stars\"])\n\n        # Define Span Type as \"CHAIN\"\n        span.set_attribute(\n            SpanAttributes.OPENINFERENCE_SPAN_KIND, OpenInferenceSpanKindValues.CHAIN.value\n        )\n\n        # Set Status Code\n        span.set_status(trace_api.StatusCode.OK)\n\n        return json.dumps(updated_dict)\n```\n\n----------------------------------------\n\nTITLE: Importing Python Libraries and Phoenix Evals Modules\nDESCRIPTION: Imports core Python libraries and Phoenix evaluation components. Includes modules for environment setup (os, getpass), data plotting (matplotlib, pandas), evaluation metrics (pycm, scikit-learn), and Phoenix's key eval functions and templates. Sets pandas display options for large text columns. All dependencies must be installed beforehand.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pycm import ConfusionMatrix\nfrom sklearn.metrics import classification_report\n\nfrom phoenix.evals import (\n    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n    RAG_RELEVANCY_PROMPT_TEMPLATE,\n    OpenAIModel,\n    download_benchmark_dataset,\n    llm_classify,\n)\n\npd.set_option(\"display.max_colwidth\", None)\n```\n\n----------------------------------------\n\nTITLE: Viewing Hallucination Evaluation Results\nDESCRIPTION: Displays the head of the dataframe containing hallucination evaluations to identify instances of LLM fabrication.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nhallucination_eval_df.head()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Pip\nDESCRIPTION: Installs the necessary Python packages required for the tutorial, including LangGraph, LangChain components for OpenAI and community utilities, Arize Phoenix, and the OpenInference instrumentation for LangChain.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -U langgraph langchain_openai langchain_community arize-phoenix openinference-instrumentation-langchain\n```\n\n----------------------------------------\n\nTITLE: Chaining Context Setters with `setAttributes` in TypeScript\nDESCRIPTION: Shows how to chain multiple context-setting functions in TypeScript. It uses `setAttributes` nested with another setter (like a hypothetical `setSession`) to add multiple attributes (`myAttribute` and `session.id`) to the context active within the `context.with` callback. Each setter returns a new context, allowing them to be composed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport { context } from \"@opentelemetry/api\"\nimport { setAttributes } from \"@openinference-core\"\n\ncontext.with(\n  setAttributes(\n    setSession(context.active(), { sessionId: \"session-id\"}),\n    { myAttribute: \"test\" }\n  ),\n  () => {\n      // Calls within this block will generate spans with the attributes:\n      // \"myAttribute\" = \"test\"\n      // \"session.id\" = \"session-id\"\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Building and Indexing Chroma DB for Company Policies\nDESCRIPTION: Creates or gets a Chroma collection named 'agentic-rag-demo-company-policies', using the previously initialized OpenAI embedding function. It adds sample documents representing company policies to the collection. A `ChromaVectorStore` is created from the collection, and finally, a `VectorStoreIndex` and a LlamaIndex query engine are built from the vector store, configured to retrieve the top 1 similar document.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/agentic_rag_tracing.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nchroma_client = chromadb.Client()\nchroma_collection = chroma_client.get_or_create_collection(\n    \"agentic-rag-demo-company-policies\", embedding_function=openai_ef\n)\n\nchroma_collection.add(\n    ids=[\"1\", \"2\", \"3\"],\n    documents=[\n        \"The travel policy is: Employees must book travel through the company portal. Economy class flights and standard hotel rooms are covered. Meals during travel are reimbursed up to $75/day. All expenses require receipts.\",\n        \"The pto policy is: Full-time employees receive 20 days of paid time off per year, accrued monthly. PTO requests must be submitted at least 2 weeks in advance through the HR portal. Unused PTO can carry over up to 5 days into the next year.\",\n        \"The dress code is: Business casual attire is required in the office. This includes collared shirts, slacks or knee-length skirts, and closed-toe shoes. Jeans are permitted on Fridays. No athletic wear or overly casual clothing.\",\n    ],\n)\n\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n\nchroma_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\nchroma_engine_policy = chroma_index.as_query_engine(similarity_top_k=1)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI API Client and Basic Parameters - Python\nDESCRIPTION: Prompts the user for an OpenAI API key if not already present in the environment, creates the OpenAI client, and specifies the model and project name for all further requests. Prerequisites include obtaining a valid OpenAI API key and using an available model (defaults to 'gpt-4o-mini'). Expects no direct parameters except user entry on missing API keys.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif os.getenv(\"OPENAI_API_KEY\") is None:\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n\nclient = OpenAI()\nmodel = \"gpt-4o-mini\"\nproject_name = \"talk-to-your-data-agent\"\n```\n\n----------------------------------------\n\nTITLE: Processing Model Response into Pandas DataFrame in Python\nDESCRIPTION: Takes the raw string response (`resp`) from the OpenAI model, removes leading/trailing whitespace using `strip()`, and splits it into a list of individual questions based on newline characters (`\\n`). This list is then used to create a Pandas DataFrame named `questions_df` with a single column named 'questions'. Finally, it prints the resulting DataFrame to the console.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsplit_response = resp.strip().split(\"\\n\")\n\nquestions_df = pd.DataFrame(split_response, columns=[\"questions\"])\nprint(questions_df)\n```\n\n----------------------------------------\n\nTITLE: Run Phoenix Experiment with Task and Evaluators (Python)\nDESCRIPTION: Executes the main Phoenix experiment workflow. It uses `nest_asyncio` to allow asyncio loops (often needed in environments like notebooks). The `run_experiment` function takes the defined `task` function, a list of `evaluators`, the `dataset` (uploaded test cases), a description, and optional metadata. It runs the task on each dataset item, applies evaluators, and uploads results to Phoenix. Required dependencies: `nest_asyncio`, `phoenix.experiments.run_experiment`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.experiments import run_experiment\n\nnest_asyncio.apply()\n\nrun_experiment(\n    task=task,\n    evaluators=[matches_expected_label],\n    dataset=test_cases,\n    experiment_description=\"Image classification experiment\",\n    experiment_metadata={\"model\": \"gpt-4o\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Summarization Prompt with OpenAI and Phoenix\nDESCRIPTION: Creates a prompt for summarizing articles about inventions, utilizing the structured response format, and registers it in Phoenix for reuse with dynamic input variables.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nparams = CompletionCreateParamsBase(\n    model=\"gpt-4o-mini\",\n    temperature=0.2,\n    messages=[\n        {\"role\": \"system\", \"content\": dedent(summarization_prompt)},\n        {\"role\": \"user\", \"content\": \"{{text}}\"},\n    ],\n    response_format=response_format,\n)\n```\n\n----------------------------------------\n\nTITLE: Running Evaluation Experiment with Phoenix in Python\nDESCRIPTION: Executes an evaluation experiment using a predefined `run_experiment` function. This function takes the `synthetic_dataset` uploaded to Phoenix, a `task` object (presumably defining the text-to-SQL task), a list of evaluators (`no_error`, `has_results`), and experiment metadata (`CONFIG`) as input. The purpose is to evaluate the performance of the text-to-SQL application on the generated dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nrun_experiment(\n    synthetic_dataset, task=task, evaluators=[no_error, has_results], experiment_metadata=CONFIG\n)\n```\n\n----------------------------------------\n\nTITLE: Testing Evaluator Functions (Python)\nDESCRIPTION: Selects the first run result from the experiment, retrieves the corresponding dataset example, and iterates through the defined `evaluators` list. For each evaluator, it calls the function with the run's data and prints its name and a shortened JSON representation of the evaluation result (score and explanation). This verifies that the evaluators are working correctly.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nrun = experiment[0]\nexample = dataset.examples[run.dataset_example_id]\nfor fn in evaluators:\n    _ = await fn(run.output, example.input, example.output)\n    print(fn.__qualname__)\n    print(shorten(json.dumps(_), width=80))\n```\n\n----------------------------------------\n\nTITLE: Creating and Uploading Dataset to Phoenix\nDESCRIPTION: This code snippet creates a pandas DataFrame from the collected conversation data (`conversations`). It then uploads this DataFrame to the Phoenix platform using the `px.Client().upload_dataset` function. The input and output keys are specified for proper dataset schema definition in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/ragas_agents_cookbook_phoenix.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\ndataset_df = pd.DataFrame(\n    {\n        \"question\": [conv[\"question\"] for conv in conversations],\n        \"final_output\": [conv[\"final_output\"] for conv in conversations],\n    }\n)\n\ndataset = px.Client().upload_dataset(\n    dataframe=dataset_df,\n    dataset_name=\"math-questions\",\n    input_keys=[\"question\"],\n    output_keys=[\"final_output\"],\n)\n\nprint(dataset)\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama API Base and Initializing LiteLLMModel in Python\nDESCRIPTION: Sets the environment variable 'OLLAMA_API_BASE' to point to the local Ollama server's URL and port (default 11434). Then, initializes a LiteLLMModel instance using the specified Ollama model. This setup enables Python code to send requests to the local Ollama LLM.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom phoenix.evals import LiteLLMModel\n\nos.environ[\"OLLAMA_API_BASE\"] = \"http://localhost:11434\"\n\nmodel = LiteLLMModel(model=\"ollama/llama3.2:1b\")\n```\n\n----------------------------------------\n\nTITLE: Viewing Evaluation Results for Experiment Runs as DataFrame in Python\nDESCRIPTION: Retrieves the evaluations associated with an experiment and presents them as a pandas DataFrame. This allows users to analyze, filter, or visualize evaluation scores and explanations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nexperiment.get_evaluations()\n```\n\n----------------------------------------\n\nTITLE: Evaluating QA Correctness with LLM Classification\nDESCRIPTION: Uses LLM classification to evaluate whether a RAG application answers questions correctly, applying the GPT-4o model with a QA prompt template and converting results to numerical scores.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nqa_correctness_eval = llm_classify(\n    dataframe=queries_df,\n    model=OpenAIModel(model=\"gpt-4o\", temperature=0.0),\n    template=QA_PROMPT_TEMPLATE,\n    rails=list(QA_PROMPT_RAILS_MAP.values()),\n    provide_explanation=True,  # Makes the LLM explain its reasoning\n    concurrency=4,\n)\n\nqa_correctness_eval[\"score\"] = (\n    qa_correctness_eval.label[~qa_correctness_eval.label.isna()] == \"correct\"\n).astype(int)\n```\n\n----------------------------------------\n\nTITLE: Defining Tool Functions for LLM in ReAct Prompting\nDESCRIPTION: Creates five function tools that the LLM can use to respond to customer queries: product comparison, product details, discount application, customer support, and package tracking.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/react-prompting.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"product_comparison\",\n            \"description\": \"Compare features of two products.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"product_a_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unique identifier of Product A.\",\n                    },\n                    \"product_b_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unique identifier of Product B.\",\n                    },\n                },\n                \"required\": [\"product_a_id\", \"product_b_id\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"product_details\",\n            \"description\": \"Get detailed features on one product.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"product_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unique identifier of the Product.\",\n                    }\n                },\n                \"required\": [\"product_id\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"apply_discount_code\",\n            \"description\": \"Checks for discounts and promotions. Applies a discount code to an order.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"order_id\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The unique identifier of the order.\",\n                    },\n                    \"discount_code\": {\n                        \"type\": \"string\",\n                        \"description\": \"The discount code to apply.\",\n                    },\n                },\n                \"required\": [\"order_id\", \"discount_code\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"customer_support\",\n            \"description\": \"Get contact information for customer support regarding an issue.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"issue_type\": {\n                        \"type\": \"string\",\n                        \"description\": \"The type of issue (e.g., billing, technical support).\",\n                    }\n                },\n                \"required\": [\"issue_type\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"track_package\",\n            \"description\": \"Track the status of a package based on the tracking number.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"tracking_number\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The tracking number of the package.\",\n                    }\n                },\n                \"required\": [\"tracking_number\"],\n            },\n        },\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Running LLM Classifications with Explanations Enabled in Python\nDESCRIPTION: Runs llm_classify on a small random sample of data with the provide_explanation flag set to True, so the LLM outputs concise reasoning for each classification. This mode increases compute time and token usage but provides valuable insight for debugging and analysis. Verbose output is enabled for progress visibility.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsmall_df_sample = df_sample.copy().sample(n=5).reset_index(drop=True)\nrelevance_classifications_df = llm_classify(\n    dataframe=small_df_sample,\n    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    provide_explanation=True,\n    verbose=True,\n    concurrency=20,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Task Function on Examples in Python\nDESCRIPTION: This code defines an asynchronous task function that queries a LlamaIndex query engine with an input query. The function returns a dictionary containing the contexts (source node texts) and the response from the query engine. It uses a VectorStoreIndex and its associated query engine to process the query.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nindex = VectorStoreIndex.from_documents(documents=documents)\nquery_engine = index.as_query_engine()\n\n\nasync def task(input):\n    ans = await query_engine.aquery(input[\"query\"])\n    return {\n        \"contexts\": [node.text for node in ans.source_nodes],\n        \"response\": ans.response,\n    }\n```\n\n----------------------------------------\n\nTITLE: LLM-Assisted Relevance Evaluation\nDESCRIPTION: Uses a Large Language Model (LLM), such as GPT-3.5 or GPT-4, to classify the relevance of retrieved documents to their corresponding queries.  This involves extracting query texts and document texts, then using the `classify_relevance` function (not defined in this snippet) to evaluate relevance. Dependencies include `sample_query_df`, `database_df`, the chosen LLM, and the `classify_relevance` function. The output is the `sample_query_df` modified to include relevance scores.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/milvus_llamaindex_search_and_retrieval_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nevals_model_name = \"gpt-3.5-turbo\"\n# evals_model_name = \"gpt-4\"  # use GPT-4 if you have access\nquery_texts = sample_query_df[\":feature.text:prompt\"].tolist()\nlist_of_document_id_lists = sample_query_df[\":feature.[str].retrieved_document_ids:prompt\"].tolist()\ndocument_id_to_text = dict(zip(database_df[\"document_id\"].to_list(), database_df[\"text\"].to_list()))\ndoc_texts = []\nfor document_index in [0]:\n    for document_ids in list_of_document_id_lists:\n        doc_texts.append(document_id_to_text[document_ids[document_index]])\n\nrelevance = []\nfor query_text, document_text in zip(query_texts, doc_texts):\n    relevance.append(classify_relevance(query_text, document_text, evals_model_name))\n\nsample_query_df = sample_query_df.assign(\n    retrieved_document_text_0=doc_texts,\n    relevance_0=relevance,\n)\nsample_query_df[\n    [\n        \":feature.text:prompt\",\n        \"retrieved_document_text_0\",\n        \"relevance_0\",\n    ]\n].rename(columns={\":feature.text:prompt\": \"query_text\"})\n```\n\n----------------------------------------\n\nTITLE: Opening Phoenix Session View in Python\nDESCRIPTION: Invokes the Phoenix session's view method to launch or refresh the UI, allowing users to observe traces and evaluations interactively. Takes no parameters and expects the session object from px.launch_app().\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nsession.view()\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI instrumentation (Bash)\nDESCRIPTION: Installs the OpenInference instrumentation package specifically for the OpenAI library. This package is needed to automatically capture traces from OpenAI API calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/feedback-and-annotations/evaluating-phoenix-traces.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n%%bash\npip install -q openinference-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Saving Gradient Optimized Prompt to Phoenix\nDESCRIPTION: This snippet saves the gradient-optimized prompt to Phoenix as a new prompt version.  It formats the prompt with few-shot examples if necessary, then saves it as an OpenAI prompt in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nif r\"\\{examples\\}\" in gradient_prompt:\n    gradient_prompt = gradient_prompt.format(examples=few_shot_examples)\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": gradient_prompt,\n        },  # if your meta prompt includes few shot examples, make sure to include them here\n        {\"role\": \"user\", \"content\": \"{{prompt}}\"},\n    ],\n)\n\ngradient_prompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Gradient prompt result\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Text2SQL Generation with OpenAI in Python\nDESCRIPTION: Implements the core logic for converting natural language to SQL with OpenAI's async client. It constructs a system prompt using the NBA table's schema and defines an async function to generate a SQL query from user input. Dependencies: openai, database connection, and the NBA table columns previously registered. Inputs: a user string with a natural language request. Output: the corresponding SQL query string. Limitations: relies on network access to OpenAI API and requires async execution context.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport openai\n\nclient = openai.AsyncClient()\n\ncolumns = conn.query(\"DESCRIBE nba\").to_df().to_dict(orient=\"records\")\n\n# We will use GPT4o to start\nTASK_MODEL = \"gpt-4o\"\nCONFIG = {\"model\": TASK_MODEL}\n\n\nsystem_prompt = (\n    \"You are a SQL expert, and you are given a single table named nba with the following columns:\\n\"\n    f'{\",\".join(column[\"column_name\"] + \": \" + column[\"column_type\"] for column in columns)}\\n'\n    \"Write a SQL query corresponding to the user's request. Return just the query text, \"\n    \"with no formatting (backticks, markdown, etc.).\"\n)\n\n\nasync def generate_query(input):\n    response = await client.chat.completions.create(\n        model=TASK_MODEL,\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": input,\n            },\n        ],\n    )\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Downloading Paul Graham Essay\nDESCRIPTION: This code downloads a Paul Graham essay from a specified URL and saves it to a local file.  It uses `curl` to retrieve the essay text and saves it to 'data/paul_graham/paul_graham_essay.txt'.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n!mkdir -p 'data/paul_graham/'\n!curl 'https://raw.githubusercontent.com/Arize-ai/phoenix-assets/main/data/paul_graham/paul_graham_essay.txt' -o 'data/paul_graham/paul_graham_essay.txt'\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Client with Options Object\nDESCRIPTION: TypeScript example showing how to configure the Phoenix client by passing options directly to the createClient function.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/packages/phoenix-client/README.md#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst phoenix = createClient({\n  options: {\n    baseUrl: \"http://localhost:6006\",\n    headers: {\n      Authorization: \"Bearer xxxxxx\",\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Fetching Spans from a Specific Project in Phoenix - Python\nDESCRIPTION: This snippet provides two methods for obtaining spans from a specific project in Phoenix, either by passing the project_name directly to get_spans_dataframe, or using SpanQuery with query_spans. It is crucial to specify the correct project name to scope query results appropriately. Requires a valid Phoenix client and the appropriate project access.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport phoenix as px\nfrom phoenix.trace.dsl import SpanQuery\n\n# Get spans from a project\npx.Client().get_spans_dataframe(project_name=\"<my-project>\")\n\n# Using the query DSL\nquery = SpanQuery().where(\"span_kind == 'CHAIN'\").select(input=\"input.value\")\npx.Client().query_spans(query, project_name=\"<my-project>\")\n```\n\n----------------------------------------\n\nTITLE: Defining Agents, Tasks, and Running CrewAI Sequential Workflow in Python\nDESCRIPTION: Demonstrates setting up CrewAI agents with specific roles, goals, and backstories to simulate realistic collaboration scenarios. The researcher uses SerperDevTool for search capabilities, and the writer transforms research insights into engaging content. Tasks are created and assigned to agents with expected output formats. A sequential Crew process is instantiated for the agents and tasks, and kickoff() runs the workflow producing results, which are printed out. This snippet illustrates orchestrating an AI-powered multi-agent system with integrated tracing enabled.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/crewai_tracing_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom crewai import Agent, Crew, Process, Task\nfrom crewai_tools import SerperDevTool\n\nsearch_tool = SerperDevTool()\n\n# Define your agents with roles and goals\nresearcher = Agent(\n    role=\"Senior Research Analyst\",\n    goal=\"Uncover cutting-edge developments in AI and data science\",\n    backstory=\"\"\"You work at a leading tech think tank.\n  Your expertise lies in identifying emerging trends.\n  You have a knack for dissecting complex data and presenting actionable insights.\"\"\",\n    verbose=True,\n    allow_delegation=False,\n    # You can pass an optional llm attribute specifying what model you wanna use.\n    # llm=ChatOpenAI(model_name=\"gpt-3.5\", temperature=0.7),\n    tools=[search_tool],\n)\nwriter = Agent(\n    role=\"Tech Content Strategist\",\n    goal=\"Craft compelling content on tech advancements\",\n    backstory=\"\"\"You are a renowned Content Strategist, known for your insightful and engaging articles.\n  You transform complex concepts into compelling narratives.\"\"\",\n    verbose=True,\n    allow_delegation=True,\n)\n\n# Create tasks for your agents\ntask1 = Task(\n    description=\"\"\"Conduct a comprehensive analysis of the latest advancements in AI in 2024.\n  Identify key trends, breakthrough technologies, and potential industry impacts.\"\"\",\n    expected_output=\"Full analysis report in bullet points\",\n    agent=researcher,\n)\n\ntask2 = Task(\n    description=\"\"\"Using the insights provided, develop an engaging blog\n  post that highlights the most significant AI advancements.\n  Your post should be informative yet accessible, catering to a tech-savvy audience.\n  Make it sound cool, avoid complex words so it doesn't sound like AI.\"\"\",\n    expected_output=\"Full blog post of at least 4 paragraphs\",\n    agent=writer,\n)\n\n# Instantiate your crew with a sequential process\ncrew = Crew(\n    agents=[researcher, writer], tasks=[task1, task2], verbose=1, process=Process.sequential\n)\n\n# Get your crew to work!\nresult = crew.kickoff()\n\nprint(\"######################\")\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Building LangChain RetrievalQA Application\nDESCRIPTION: Creates a RetrievalQA chain using a pre-built KNN index of Arize documentation with OpenAI embeddings and a ChatOpenAI model for question answering.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_tracing_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/\"\n    \"unstructured/llm/context-retrieval/langchain/database.parquet\"\n)\nknn_retriever = KNNRetriever(\n    index=np.stack(df[\"text_vector\"]),\n    texts=df[\"text\"].tolist(),\n    embeddings=OpenAIEmbeddings(),\n)\nchain_type = \"stuff\"  # stuff, refine, map_reduce, and map_rerank\nchat_model_name = \"gpt-3.5-turbo\"\nllm = ChatOpenAI(model_name=chat_model_name)\nchain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=chain_type,\n    retriever=knn_retriever,\n    metadata={\"application_type\": \"question_answering\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Prompt by Tag (Python)\nDESCRIPTION: This snippet demonstrates how to retrieve a specific version of a prompt from the Phoenix server using a tag. It uses the `px.Client().prompts.get` method with the `prompt_identifier` and `tag` parameters.  In this case, it retrieves the version tagged with \"production\".\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/how-to-prompts/create-a-prompt.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprompt = px.Client().prompts.get(prompt_identifier=prompt_name, tag=\"production\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Images from Tesla's Website\nDESCRIPTION: Downloads a set of images from Google Drive URLs using `wget` and stores them in the './input_images/' directory. The `%%capture` cell magic suppresses the output of the `wget` commands.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_multimodal_image_reasoning.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!wget \"https://docs.google.com/uc?export=download&id=1nUhsBRiSWxcVQv8t8Cvvro8HJZ88LCzj\" -O ./input_images/long_range_spec.png\n!wget \"https://docs.google.com/uc?export=download&id=19pLwx0nVqsop7lo0ubUSYTzQfMtKJJtJ\" -O ./input_images/model_y.png\n!wget \"https://docs.google.com/uc?export=download&id=1utu3iD9XEgR5Sb7PrbtMf1qw8T1WdNmF\" -O ./input_images/performance_spec.png\n!wget \"https://docs.google.com/uc?export=download&id=1dpUakWMqaXR4Jjn1kHuZfB0pAXvjn2-i\" -O ./input_images/price.png\n!wget \"https://docs.google.com/uc?export=download&id=1qNeT201QAesnAP5va1ty0Ky5Q_jKkguV\" -O ./input_images/real_wheel_spec.png\n```\n\n----------------------------------------\n\nTITLE: Exporting Trace Data into Pandas DataFrames for Evaluation in Python\nDESCRIPTION: Retrieves query-level and document-level evaluation data from Phoenixâ€™s client session by extracting QA references and retrieved documents respectively. These DataFrames prepare the trace data for subsequent automated evaluation of spans using LLM-based evaluators.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_vertex_ai_tracing_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nqueries_df = get_qa_with_reference(px.Client())\nretrieved_documents_df = get_retrieved_documents(px.Client())\n```\n\n----------------------------------------\n\nTITLE: Launching the Phoenix Visualization App with Primary Inference Data using Python\nDESCRIPTION: Fires up the Phoenix interactive visualization app with the training inference dataset as the primary input. This session object allows users to open the Phoenix UI either inline in notebooks or via a provided browser link to analyze model performance, embeddings, and clusters.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/inferences_quickstart.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsession = px.launch_app(primary=train_ds)\n```\n\n----------------------------------------\n\nTITLE: Creating Pandas DataFrame for RAG Experiment Dataset (Python)\nDESCRIPTION: Constructs a Pandas DataFrame containing example RAG input messages (user queries) and corresponding expected output messages (assistant responses) for evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/run_experiments_with_llama_index.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"input_messages\": [\n            [{\"role\": \"user\", \"content\": \"Which grad schools did the author apply for and why?\"}],\n            [{\"role\": \"user\", \"content\": \"What did the author do growing up?\"}],\n        ],\n        \"output_message\": [\n            {\n                \"role\": \"assistant\",\n                \"content\": \"The author applied to three grad schools: MIT and Yale, which were renowned for AI at the time, and Harvard, which the author had visited because a friend went there and it was also home to Bill Woods, who had invented the type of parser the author used in his SHRDLU clone. The author chose these schools because he wanted to learn about AI and Lisp, and these schools were known for their expertise in these areas.\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"The author took a painting class at Harvard with Idelle Weber and later became her de facto studio assistant. Additionally, the author worked on several different projects, including writing essays, developing spam filters, and painting.\",\n            },\n        ],\n    }\n)\ndf\n```\n\n----------------------------------------\n\nTITLE: Invoking OpenAI ChatCompletion for Structured Extraction in Python\nDESCRIPTION: This Python snippet demonstrates how to use the OpenAI ChatCompletion API to extract structured fields from user input according to a predefined schema. It sets system and user messages and provides functions metadata, forcing the LLM to call the registered function and output data matching the schema. The only dependency is the 'openai' Python SDK. Required parameters include a model name, the system message string, the user input string, the function schema object, and a function call directive. The output is a response object with structured extraction results; the function_call parameter ensures strict schema adherence.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/use-cases-tracing/structured-extraction.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresponse = openai.ChatCompletion.create(\n    model=model,\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": travel_request},\n    ],\n    functions=[function_schema],\n    # By default, the LLM will choose whether or not to call a function given the conversation context.\n    # The line below forces the LLM to call the function so that the output conforms to the schema.\n    function_call={\"name\": function_schema[\"name\"]},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Query Engine\nDESCRIPTION: This code creates a query engine from the previously built vector index. The query engine is used to perform queries against the indexed data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nquery_engine = vector_index.as_query_engine()\n```\n\n----------------------------------------\n\nTITLE: Retrieving QA Reference Data from Phoenix in Python\nDESCRIPTION: Retrieves questions and corresponding reference answers for QA evaluations using get_qa_with_reference. px.active_session() is assumed to point to the active Phoenix tracing session. Returns queries_df containing questions with references for downstream evals.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_qa_with_reference\n\nqueries_df = get_qa_with_reference(px.active_session())\nqueries_df\n```\n\n----------------------------------------\n\nTITLE: Viewing Experiment Results as DataFrame in Python\nDESCRIPTION: Retrieves the results from the completed Phoenix `experiment` object and converts them into a Pandas DataFrame using the `as_dataframe()` method. This allows for tabular inspection and analysis of the experiment's inputs and outputs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nexperiment.as_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Running the Initial Experiment with Base Prompt - Python\nDESCRIPTION: Runs the first experiment using Phoenix, employing the dataset, the base task, and the evaluator defined previously. Uses the nest_asyncio library to allow for nested event loops if running in Jupyter or notebook environments. The experiment is labeled and described for reproducibility and tracked in Phoenix with associated prompt version metadata. Dependencies: nest_asyncio, phoenix.experiments. Outputs an experiment object and logs results in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.experiments import run_experiment\n\nnest_asyncio.apply()\n\ninitial_experiment = run_experiment(\n    dataset,\n    task=test_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Initial base prompt\",\n    experiment_name=\"initial-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running LLM-Based Evaluators on Trace Data in Python\nDESCRIPTION: Initializes a VertexAI evaluation model and three evaluators to assess hallucination, QA correctness, and relevance of retrieved documents. Runs evaluations on the exported DataFrames, generating evaluation results with optional explanations. These evaluations aid in identifying quality issues in LLM application spans.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_vertex_ai_tracing_tutorial.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\neval_model = VertexAIModel(\n    project=project_id,\n    location=location,\n    model=\"text-bison\",\n    temperature=0.0,\n)\nhallucination_evaluator = HallucinationEvaluator(eval_model)\nqa_correctness_evaluator = QAEvaluator(eval_model)\nrelevance_evaluator = RelevanceEvaluator(eval_model)\n\nhallucination_eval_df, qa_correctness_eval_df = run_evals(\n    dataframe=queries_df,\n    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n    provide_explanation=True,\n)\nrelevance_eval_df = run_evals(\n    dataframe=retrieved_documents_df,\n    evaluators=[relevance_evaluator],\n    provide_explanation=True,\n)[0]\n```\n\n----------------------------------------\n\nTITLE: Running Agent and Tracking Path (Python)\nDESCRIPTION: The `run_agent_and_track_path_combined` function orchestrates running the agent and tracking the messages exchanged. It takes an `Example` object as input, extracts the question from the input, and passes it to the `run_agent_messages_combined` function. The `run_agent_messages_combined` then runs the agent with the given messages, potentially adding a system prompt if none exists, and handles tool calls iteratively until a final response is received. The function relies on external variables such as `client`, `model`, and `tools` for agent interaction and the `handle_tool_calls` function to manage the tool calls. Finally `process_messages` is called to extract all the needed information.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\ndef run_agent_and_track_path_combined(example: Example) -> str:\n    print(\"Starting main span with messages:\", example.input.get(\"question\"))\n    messages = [{\"role\": \"user\", \"content\": example.input.get(\"question\")}]\n    ret = run_agent_messages_combined(messages)\n    return process_messages(ret)\n\n\ndef run_agent_messages_combined(messages):\n    print(\"Running agent with messages:\", messages)\n    if isinstance(messages, str):\n        messages = [{\"role\": \"user\", \"content\": messages}]\n        print(\"Converted string message to list format\")\n\n    # Check and add system prompt if needed\n    if not any(\n        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n    ):\n        system_prompt = {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n        }\n        messages.append(system_prompt)\n        print(\"Added system prompt to messages\")\n\n    while True:\n        # Router call span\n        print(\"Starting router\")\n\n        response = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            tools=tools,\n        )\n\n        messages.append(response.choices[0].message.model_dump())\n        tool_calls = response.choices[0].message.tool_calls\n        print(\"Received response with tool calls:\", bool(tool_calls))\n\n        if tool_calls:\n            # Tool calls span\n            print(\"Processing tool calls\")\n            tool_calls = response.choices[0].message.tool_calls\n            messages = handle_tool_calls(tool_calls, messages)\n        else:\n            print(\"No tool calls, returning final response\")\n            return messages\n```\n\n----------------------------------------\n\nTITLE: Performing Basic LLM Inference Call in Python\nDESCRIPTION: This snippet demonstrates the basic method for getting a response from an instantiated LLM evaluation model object in Phoenix. By calling the model instance directly with a text string, you perform a simple inference request.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evaluation-models.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# model = Instantiate your model here\nmodel(\"Hello there, how are you?\")\n# Output: \"As an artificial intelligence, I don't have feelings,\n#          but I'm here and ready to assist you. How can I help you today?\"\n```\n\n----------------------------------------\n\nTITLE: Configuring LlamaIndex with OpenAI and MongoDB Atlas\nDESCRIPTION: Sets up the core components of the LlamaIndex application. It configures the global `Settings` with specific OpenAI models (`gpt-4o` for LLM, `text-embedding-ada-002` for embeddings). It then initializes a `SimpleMongoReader` to load documents ('text' field) from the specified MongoDB collection. A `MongoDBAtlasVectorSearch` instance is created as the vector store, linked to the MongoDB client and collection. Finally, it creates a `VectorStoreIndex` from the loaded documents, using the MongoDB vector store within a `StorageContext`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_with_mongodb_and_llama_index.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndb_name = \"phoenix\"  # Replace with your database name\ncollection_name = \"phoenix-docs\"  # Replace with your collection name\nvector_index_name = \"vector_index\"  # Replace with your vector index name\nSettings.llm = OpenAI(model=\"gpt-4o\", temperature=0.0)\nSettings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n\ndb = client[db_name]\ncollection = db[collection_name]\n\n# You can obtain your uri @... format directly in mongo atlas\nuri = f\"mongodb+srv://{mongo_username}:{mongo_password}@cluster0.lq406.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n\nquery_dict = {}\nreader = SimpleMongoReader(uri=uri)\ndocuments = reader.load_data(\n    db_name,\n    collection_name,\n    field_names=[\"text\"],\n    query_dict=query_dict,\n)\n\n# Create a new client and connect to the server\nclient = MongoClient(uri, server_api=ServerApi(\"1\"))\n\n# create Atlas as a vector store\nstore = MongoDBAtlasVectorSearch(\n    client, db_name=db_name, collection_name=collection_name, vector_index_name=vector_index_name\n)\n\nstorage_context = StorageContext.from_defaults(vector_store=store)\n\nindex = VectorStoreIndex.from_documents(\n    documents, storage_context=storage_context, show_progress=True\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating Phoenix Prompts with OpenAI SDK in TypeScript\nDESCRIPTION: This code demonstrates how to use a fetched prompt with the OpenAI SDK. It involves transforming the prompt into SDK-compatible parameters with variable substitution, then making a chat completion request. The process requires initializing the OpenAI client, retrieving the prompt, converting it via toSDK, and handling API responses, enabling seamless AI model integrations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/quickstart-prompts/quickstart-prompts-ts.md#_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { getPrompt, toSDK } from \"@arizeai/phoenix-client/prompts\";\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY\n});\n\nconst prompt = await getPrompt({\n  prompt: {\n    name: \"article-summarizer\"\n  }\n});\n\nif (!prompt) {\n  throw new Error(\"Prompt not found\");\n}\n\nconst openaiParameters = toSDK({\n  sdk: \"openai\", \n  prompt,\n  variables: {\n    topic: \"technology\",\n    article:\n      \"Artificial intelligence has seen rapid advancement in recent years. Large language models like GPT-4 can now generate human-like text, code, and even create images from descriptions. This technology is being integrated into many industries, from healthcare to finance, transforming how businesses operate and people work.\"\n  }\n});\n\nif (!openaiParameters) {\n  throw new Error(\"OpenAI parameters not found\");\n}\n\nconst response = await openai.chat.completions.create({\n  ...openaiParameters,\n  model: \"gpt-4o-mini\",\n  stream: false\n});\n\nconsole.log(\"Summary:\", response.choices[0].message.content);\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Anthropic with OpenTelemetry (Python)\nDESCRIPTION: Registers the Phoenix OpenTelemetry tracer provider and instruments the Anthropic client, enabling automatic tracing of API calls to be sent to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntracer_provider = register()\nAnthropicInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Function to Solve Math Problems and Log Evaluation Results\nDESCRIPTION: This async function, solve_math_problem, takes a dataset row with a math question, executes the problem solving asynchronously via Runner.run, and uses tracing spans for monitoring. It evaluates the solution's correctness using correctness_eval, then logs the evaluation label, explanation, and a score (1 for correct, 0 for incorrect) into a DataFrame for further analysis. Dependencies include asyncio, pandas, and tracing libraries. The function returns the final result with messages and output.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/openai_agents_cookbook.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n# This is our modified task function.\nasync def solve_math_problem(dataset_row: dict):\n    with tracer.start_as_current_span(name=\"agent\", openinference_span_kind=\"agent\") as agent_span:\n        question = dataset_row.get(\"question\")\n        agent_span.set_input(question)\n        agent_span.set_status(StatusCode.OK)\n\n        result = await Runner.run(agent, question)\n        agent_span.set_output(result.final_output)\n\n        task_result = {\n            \"final_output\": result.final_output,\n            \"messages\": result.to_input_list(),\n        }\n\n        # Evaluation span for correctness\n        with tracer.start_as_current_span(\n            \"correctness-evaluator\",\n            openinference_span_kind=\"evaluator\",\n        ) as eval_span:\n            evaluation_result = correctness_eval(dataset_row, task_result)\n            eval_span.set_attribute(\"eval.label\", evaluation_result[\"label\"][0])\n            eval_span.set_attribute(\"eval.explanation\", evaluation_result[\"explanation\"][0])\n\n        # Logging our evaluation\n        span_id = format_span_id(eval_span.get_span_context().span_id)\n        score = 1 if evaluation_result[\"label\"][0] == \"correct\" else 0\n        eval_data = {\n            \"span_id\": span_id,\n            \"label\": evaluation_result[\"label\"][0],\n            \"score\": score,\n            \"explanation\": evaluation_result[\"explanation\"][0],\n        }\n        df = pd.DataFrame([eval_data])\n        px.Client().log_evaluations(\n            SpanEvaluations(\n                dataframe=df,\n                eval_name=\"correctness\",\n            ),\n        )\n\n    return task_result\n\n# Example usage\ndataset_row = {\"question\": \"What is 15 + 28?\"}\nresult = asyncio.run(solve_math_problem(dataset_row))\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenTelemetry Tracing with OpenInference in Node.js (TypeScript)\nDESCRIPTION: This TypeScript script initializes OpenTelemetry tracing in a Node.js application by configuring a TracerProvider, adding a BatchSpanProcessor for exporting traces to Phoenix, and registering OpenInference instrumentation for the OpenAI client. It sets logging for diagnostics, constructs the resource with service and project names, and explicitly instruments the OpenAI SDK. Before execution, install all packages listed in the dependencies section. Expected input includes environment variables for Phoenix and OpenAI API keys, and the script must be loaded before the application to ensure instrumentation is active. This snippet targets Node.js v18+ with support for ESM and CommonJS; for development, you may use DEBUG logs. Limitations include ensuring the Phoenix server is reachable at the configured URL.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// instrumentation.ts\nimport { registerInstrumentations } from \"@opentelemetry/instrumentation\";\nimport { OpenAIInstrumentation } from \"@arizeai/openinference-instrumentation-openai\";\nimport { diag, DiagConsoleLogger, DiagLogLevel } from \"@opentelemetry/api\";\nimport { OTLPTraceExporter } from \"@opentelemetry/exporter-trace-otlp-proto\";\nimport { resourceFromAttributes } from \"@opentelemetry/resources\";\nimport { BatchSpanProcessor } from \"@opentelemetry/sdk-trace-base\";\nimport { NodeTracerProvider } from \"@opentelemetry/sdk-trace-node\";\nimport { ATTR_SERVICE_NAME } from \"@opentelemetry/semantic-conventions\";\nimport { SEMRESATTRS_PROJECT_NAME } from \"@arizeai/openinference-semantic-conventions\";\nimport OpenAI from \"openai\";\n\n// For troubleshooting, set the log level to DiagLogLevel.DEBUG\ndiag.setLogger(new DiagConsoleLogger(), DiagLogLevel.DEBUG);\n\nconst tracerProvider = new NodeTracerProvider({\n  resource: resourceFromAttributes({\n    [ATTR_SERVICE_NAME]: \"openai-service\",\n    // Project name in Phoenix, defaults to \"default\"\n    [SEMRESATTRS_PROJECT_NAME]: \"openai-service\",\n  }),\n  spanProcessors: [\n    // BatchSpanProcessor will flush spans in batches after some time,\n    // this is recommended in production. For development or testing purposes\n    // you may try SimpleSpanProcessor for instant span flushing to the Phoenix UI.\n    new BatchSpanProcessor(\n      new OTLPTraceExporter({\n        url: `http://localhost:6006/v1/traces`,\n        // (optional) if connecting to Phoenix Cloud\n        // headers: { \"api_key\": process.env.PHOENIX_API_KEY },\n        // (optional) if connecting to self-hosted Phoenix with Authentication enabled\n        // headers: { \"Authorization\": `Bearer ${process.env.PHOENIX_API_KEY}` }\n      })\n    ),\n  ],\n});\ntracerProvider.register();\n\nconst instrumentation = new OpenAIInstrumentation();\ninstrumentation.manuallyInstrument(OpenAI);\n\nregisterInstrumentations({\n  instrumentations: [instrumentation],\n});\n\nconsole.log(\"\\uD83D\\uDC40 OpenInference initialized\");\n```\n\n----------------------------------------\n\nTITLE: Downloading Training and Production Data using Pandas in Python\nDESCRIPTION: Uses the Pandas `read_parquet` function to load training and production datasets related to sentiment classification from remote URLs into `train_df` and `prod_df` DataFrames respectively. The index is reset after loading.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/sentiment_classification_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/nlp/sentiment-classification-language-drift/sentiment_classification_language_drift_training.parquet\",\n).reset_index(drop=True)\nprod_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/nlp/sentiment-classification-language-drift/sentiment_classification_language_drift_production.parquet\",\n).reset_index(drop=True)\n```\n\n----------------------------------------\n\nTITLE: Defining a Math Problem Generation Template in Text\nDESCRIPTION: This text snippet provides a detailed prompt template designed for an AI model to generate diverse math problems. It specifies required problem types (basic, complex, exponents, fractions, word problems, etc.) and constraints (no solutions, list format, no numbering, 10 unique problems).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/ragas_agents_cookbook_phoenix.ipynb#_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nMATH_GEN_TEMPLATE = \"\"\"\nYou are an assistant that generates diverse math problems for testing a math solver agent.\nThe problems should include:\n\nBasic Operations: Simple addition, subtraction, multiplication, division problems.\nComplex Arithmetic: Problems with multiple operations and parentheses following order of operations.\nExponents and Roots: Problems involving powers, square roots, and other nth roots.\nPercentages: Problems involving calculating percentages of numbers or finding percentage changes.\nFractions: Problems with addition, subtraction, multiplication, or division of fractions.\nAlgebra: Simple algebraic expressions that can be evaluated with specific values.\nSequences: Finding sums, products, or averages of number sequences.\nWord Problems: Converting word problems into mathematical equations.\n\nDo not include any solutions in your generated problems.\n\nRespond with a list, one math problem per line. Do not include any numbering at the beginning of each line.\nGenerate 10 diverse math problems. Ensure there are no duplicate problems.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Getting Spans Dataframe from Phoenix\nDESCRIPTION: This code retrieves a DataFrame containing span data from Phoenix, using the Phoenix client. The `project_name` parameter specifies the project from which to retrieve the spans. The resulting DataFrame will contain all traces logged to the specified project.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nspans_df = px.Client().get_spans_dataframe(project_name=\"default\")\nspans_df.head()\n```\n\n----------------------------------------\n\nTITLE: Extracting Retrieval and Question-Answer Dataframes from Phoenix Session in Python\nDESCRIPTION: Utilizes Phoenix session evaluation utilities to convert telemetry traces into structured dataframes for analysis. Retrieves the dataframe containing documents fetched during retrieval steps and a dataframe containing paired questions and reference answers. These datasets prepare the application data for subsequent automated evaluation using LLM evals.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n\nretrieved_documents_df = get_retrieved_documents(px.active_session())\nqueries_df = get_qa_with_reference(px.active_session())\n```\n\n----------------------------------------\n\nTITLE: Instrumenting LlamaIndex for Observability with OpenTelemetry (Python)\nDESCRIPTION: Configures LlamaIndex for instrumentation, enabling detailed tracing with OpenTelemetry and Phoenix. Sets up a custom tracer provider, endpoint, and span processor, and applies the instrumentation to all subsequent LlamaIndex operations. Requires 'opentelemetry' Python packages and a running Phoenix trace endpoint; 'endpoint' specifies the trace collector.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\n\nendpoint = \"http://127.0.0.1:6006/v1/traces\"\ntracer_provider = TracerProvider()\ntracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n\nLlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries\nDESCRIPTION: Imports necessary libraries for the project. Imports modules for type hinting, LangChain message handling, Google VertexAI Chat, LangGraph graph definition, and Vertex AI agent engine functionalities.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/google_agent_engine_tracing_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nfrom langchain_core.messages import BaseMessage, HumanMessage\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langgraph.graph import END, MessageGraph\nfrom langgraph.prebuilt import ToolNode\nfrom vertexai import agent_engines\n```\n\n----------------------------------------\n\nTITLE: Checking Task Runs Successfully in Python\nDESCRIPTION: This snippet demonstrates how to run the defined task function on an example from the dataset and prints a shortened version of the output. This confirms the task function is correctly processing the data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nexample = dataset[0]\ntask_output = await task(example.input)\nprint(shorten(json.dumps(task_output), width=80))\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Local Endpoint - Python\nDESCRIPTION: Sets the PHOENIX_COLLECTOR_ENDPOINT environment variable to connect the instrumentation client with a locally running Phoenix server. This configuration is required when running Phoenix in a local environment for test or development purposes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/mistralai.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Phoenix Client and Uploading Dataset in Python\nDESCRIPTION: Initializes the Phoenix client and uploads the loaded pandas DataFrame (`ds`) as a Phoenix dataset. It specifies the input ('Review') and output ('Sentiment') keys and assigns a unique name to the dataset using a UUID.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/few-shot-prompting.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n\nimport phoenix as px\nfrom phoenix.client import Client as PhoenixClient\n\nunique_id = uuid.uuid4()\n\n# Upload the dataset to Phoenix\ndataset = px.Client().upload_dataset(\n    dataframe=ds,\n    input_keys=[\"Review\"],\n    output_keys=[\"Sentiment\"],\n    dataset_name=f\"review-classification-{unique_id}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating Tracer in Express App - TypeScript\nDESCRIPTION: Example of acquiring a tracer within the scope of an Express.js application file. This snippet also shows basic server setup, dependency imports (OpenAI), and a simple route handler, although span creation is not included in this specific block.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\n// app.ts\nimport { trace } from '@opentelemetry/api';\nimport express from 'express';\nimport OpenAI from \"openai\";\n\nconst tracer = trace.getTracer('llm-server', '0.1.0');\n\nconst PORT: number = parseInt(process.env.PORT || \"8080\");\nconst app = express();\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n});\n\napp.get(\"/chat\", async (req, res) => {\n  const message = req.query.message;\n  const chatCompletion = await openai.chat.completions.create({\n    messages: [{ role: \"user\", content: message }],\n    model: \"gpt-4o\",\n  });\n  res.send(chatCompletion.choices[0].message.content);\n});\n\napp.listen(PORT, () => {\n  console.log(`Listening for requests on http://localhost:${PORT}`);\n});\n```\n\n----------------------------------------\n\nTITLE: Optimizing Prompt Using Gradient\nDESCRIPTION: This snippet defines the `optimize_prompt` function. The function converts a given base prompt into an embedding, then moves in the gradient direction, adjusting the prompt's embedding. It calls the OpenAI API to generate a new prompt using the modified embedding direction. The new prompt aims to maintain the original meaning, but to move toward the direction of successful prompts as indicated by the calculated gradient.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Function to optimize a prompt using the gradient\ndef optimize_prompt(base_prompt, gradient, step_size=0.1):\n    # Get base embedding\n    base_embedding = get_embedding(base_prompt)\n\n    # Move in gradient direction\n    optimized_embedding = base_embedding + step_size * gradient\n\n    # Use GPT to convert the optimized embedding back to text\n    client = OpenAI()\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are helping to optimize prompts. Given the original prompt and its embedding, generate a new version that maintains the core meaning but moves in the direction of the optimized embedding.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Original prompt: {base_prompt}\\nOptimized embedding direction: {optimized_embedding[:10]}...\\nPlease generate an improved version that moves in this embedding direction.\",\n            },\n        ],\n    )\n    return response.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Combine Eval Results with Original Data\nDESCRIPTION: This Python snippet merges the evaluation results (hallucination and QA evaluations) with the original DataFrame. It adds new columns for evaluation labels and explanations from both evaluators.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evals_quickstart.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresults_df = df.copy()\nresults_df[\"hallucination_eval\"] = hallucination_eval_df[\"label\"]\nresults_df[\"hallucination_explanation\"] = hallucination_eval_df[\"explanation\"]\nresults_df[\"qa_eval\"] = qa_eval_df[\"label\"]\nresults_df[\"qa_explanation\"] = qa_eval_df[\"explanation\"]\nresults_df.head()\n```\n\n----------------------------------------\n\nTITLE: Running Retrieval Relevance Evals with Phoenix and OpenAI Python\nDESCRIPTION: Performs LLM-based classification using OpenAI to determine the relevance of individual retrieved document chunks to the user query. The results are stored in a DataFrame and can be logged back to the Phoenix UI, where Phoenix also calculates aggregate metrics like MRR and NDCG.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/retrieval/quickstart-retrieval.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\nfrom phoenix.evals import (\n    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n    RAG_RELEVANCY_PROMPT_TEMPLATE,\n    OpenAIModel,\n    llm_classify,\n)\n\nretrieved_documents_eval = llm_classify(\n    dataframe=retrieved_documents_df,\n    model=OpenAIModel(\"gpt-4-turbo-preview\", temperature=0.0),\n    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n    provide_explanation=True,\n)\n\nretrieved_documents_eval[\"score\"] = (\n    retrieved_documents_eval.label[~retrieved_documents_eval.label.isna()] == \"relevant\"\n).astype(int)\n\npx.Client().log_evaluations(DocumentEvaluations(eval_name=\"Relevance\", dataframe=retrieved_documents_eval))\n\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Span Attributes with Phoenix SpanQuery (Python)\nDESCRIPTION: Illustrates selecting specific attributes (`input.value`, `output.value`) from Phoenix spans using the `SpanQuery().select()` method. The selected attributes will become columns in the resulting DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nquery = SpanQuery().select(\n    \"input.value\",\n    \"output.value\",\n)\n```\n\n----------------------------------------\n\nTITLE: Formatting and Using a Phoenix Prompt in Python\nDESCRIPTION: This snippet showcases how to format a prompt with variable values and send a completion request to OpenAI's API. It involves setting variables for placeholders, formatting the prompt, and invoking the chat completion API to generate a response based on the prompt.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/quickstart-prompts/quickstart-prompts-python.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nprompt_vars = {\"topic\": \"Sports\", \"article\": \"Surrey have signed Australia all-rounder Moises Henriques for this summer's NatWest T20 Blast. Henriques will join Surrey immediately after the Indian Premier League season concludes at the end of next month and will be with them throughout their Blast campaign and also as overseas cover for Kumar Sangakkara - depending on the veteran Sri Lanka batsman's Test commitments in the second half of the summer. Australian all-rounder Moises Henriques has signed a deal to play in the T20 Blast for Surrey . Henriques, pictured in the Big Bash (left) and in ODI action for Australia (right), will join after the IPL . Twenty-eight-year-old Henriques, capped by his country in all formats but not selected for the forthcoming Ashes, said: 'I'm really looking forward to playing for Surrey this season. It's a club with a proud history and an exciting squad, and I hope to play my part in achieving success this summer. 'I've seen some of the names that are coming to England to be involved in the NatWest T20 Blast this summer, so am looking forward to testing myself against some of the best players in the world.' Surrey director of cricket Alec Stewart added: 'Moises is a fine all-round cricketer and will add great depth to our squad.'\"}\nformatted_prompt = prompt.format(variables=prompt_vars)\n\n# Make a request with your Prompt\noai_client = OpenAI()\nresp = oai_client.chat.completions.create(**formatted_prompt)\n```\n\n----------------------------------------\n\nTITLE: Setup OpenAI Tracing Python\nDESCRIPTION: Configures OpenInference instrumentation to automatically trace calls made to the OpenAI API. This allows Phoenix to capture the prompts, responses, and metadata of LLM interactions, which is crucial for analyzing the Text2SQL pipeline. It registers a tracer provider with Phoenix and instruments the OpenAI client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register()\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Instrument OpenAI Client with OpenInference (Python)\nDESCRIPTION: Applies OpenInference instrumentation to the OpenAI Python library. This enables tracing of all API calls made through the OpenAI client, capturing details like prompts, responses, and latency, which are then sent to the registered tracer provider. Required dependencies: `openinference.instrumentation.openai.OpenAIInstrumentor`, `tracer_provider` (defined earlier).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider, skip_dep_check=True)\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Schema with Predictions and Actuals in Python\nDESCRIPTION: Specifies a Phoenix schema mapping DataFrame columns to their respective prediction score, predicted label, actual (ground truth) label, and timestamp. The Phoenix (`px`) library is required. Key parameters: `timestamp_column_name`, `prediction_score_column_name`, `prediction_label_column_name`, and `actual_label_column_name` are set to match corresponding columns in the input DataFrame. This configuration is suitable for binary classification inferences with both prediction scores and labels. Outputs a Schema object for initializing a Phoenix dataset; columns in the DataFrame must exist with these exact names.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nschema = px.Schema(\n    timestamp_column_name=\"timestamp\",\n    prediction_score_column_name=\"prediction_score\",\n    prediction_label_column_name=\"prediction\",\n    actual_label_column_name=\"target\",\n)\n```\n\n----------------------------------------\n\nTITLE: Preparing for Document Evaluation in Phoenix\nDESCRIPTION: Sets up the environment for evaluating retrieved documents by applying nest_asyncio and initializing the Phoenix client to retrieve document data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag_haystack.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nimport phoenix as px\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Defining Prompt Task and Evaluation Functions for LLM-as-Judge in Python\nDESCRIPTION: Implements two Python functions: 'prompt_task' sends a formatted chat completion request to OpenAI with a customer's question; 'evaluate_response' uses an LLM-as-Judge approach to classify if the tools called are appropriate for the input question, leveraging pandas DataFrame, a pre-defined evaluation template, and an OpenAIModel instance. It returns a binary score based on classification labels. Dependencies include OpenAI Python SDK, llm_classify utilities, pandas, and the TOOL_CALLING_PROMPT_TEMPLATE and TOOL_CALLING_PROMPT_RAILS_MAP. Inputs are input dictionaries containing questions and corresponding LLM outputs; outputs are numerical scores indicating evaluation results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/react-prompting.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef prompt_task(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **prompt.format(variables={\"questions\": input[\"Questions\"]})\n    )\n    return resp\n\n\ndef evaluate_response(input, output):\n    response_classifications = llm_classify(\n        dataframe=pd.DataFrame([{\"question\": input[\"Questions\"], \"tool_calls\": output}]),\n        template=TOOL_CALLING_PROMPT_TEMPLATE,\n        model=OpenAIModel(model=\"gpt-3.5-turbo\"),\n        rails=list(TOOL_CALLING_PROMPT_RAILS_MAP.values()),\n        provide_explanation=True,\n    )\n    score = response_classifications.apply(lambda x: 0 if x[\"label\"] == \"incorrect\" else 1, axis=1)\n    return score\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: This snippet retrieves the OpenAI API key from the environment variables or prompts the user to enter it. It then sets the API key for the OpenAI library and the environment variable for later use.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/span_query_language.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport openai\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Configuring an OpenTelemetry Tracer with Phoenix (python)\nDESCRIPTION: Initializes the OpenTelemetry tracer for manual instrumentation in a Python application, specifying a resource representing the project, creating a TracerProvider, setting the tracer provider, creating a tracer object, configuring an OTLP HTTP span exporter with Phoenix as the collector, and attaching a SimpleSpanProcessor. Dependencies: openinference-semconv, opentelemetry-api, opentelemetry-sdk, opentelemetry-exporter-otlp, and Phoenix's configuration utilities. Inputs: project name (string), host/port via phoenix.config.get_env_host/get_env_port; Output: initialized tracer and span exporter configured to route spans to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/custom-spans.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.semconv.resource import ResourceAttributes\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom phoenix.config import get_env_host, get_env_port\n\nresource = Resource(attributes={\n    ResourceAttributes.PROJECT_NAME: '<your-project-name>'\n})\ntracer_provider = TracerProvider(resource=resource)\ntrace.set_tracer_provider(tracer_provider)\ntracer = trace.get_tracer(__name__)\ncollector_endpoint = f\"http://{get_env_host()}:{get_env_port()}/v1/traces\"\nspan_exporter = OTLPSpanExporter(endpoint=collector_endpoint)\nsimple_span_processor = SimpleSpanProcessor(span_exporter=span_exporter)\ntrace.get_tracer_provider().add_span_processor(simple_span_processor)\n```\n\n----------------------------------------\n\nTITLE: Quickstart Phoenix OTel - Python\nDESCRIPTION: This snippet showcases the simplest way to start using OTel with Phoenix, by registering the tracer. It utilizes the `register` function to set up a global `TracerProvider`. The function defaults to sending spans to `http://localhost` using gRPC.  It requires the `phoenix.otel` module.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-otel/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\ntracer_provider = register()\n```\n\n----------------------------------------\n\nTITLE: Creating OpenTelemetry Tracer Provider Configuration\nDESCRIPTION: Implementation of an OpenTelemetry TraceProvider in TypeScript that configures trace collection and exports to Phoenix, including service name and project attributes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-ts.md#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// instrumentation.ts\nimport { diag, DiagConsoleLogger, DiagLogLevel } from \"@opentelemetry/api\";\nimport { OTLPTraceExporter } from \"@opentelemetry/exporter-trace-otlp-proto\";\nimport { resourceFromAttributes } from \"@opentelemetry/resources\";\nimport { BatchSpanProcessor } from \"@opentelemetry/sdk-trace-base\";\nimport { NodeTracerProvider } from \"@opentelemetry/sdk-trace-node\";\nimport { ATTR_SERVICE_NAME } from \"@opentelemetry/semantic-conventions\";\n\nimport { SEMRESATTRS_PROJECT_NAME } from \"@arizeai/openinference-semantic-conventions\";\n\ndiag.setLogger(new DiagConsoleLogger(), DiagLogLevel.ERROR);\n\nconst COLLECTOR_ENDPOINT = process.env.PHOENIX_COLLECTOR_ENDPOINT;\nconst SERVICE_NAME = \"my-llm-app\";\n\nconst provider = new NodeTracerProvider({\n  resource: resourceFromAttributes({\n    [ATTR_SERVICE_NAME]: SERVICE_NAME,\n    // defaults to \"default\" in the Phoenix UI\n    [SEMRESATTRS_PROJECT_NAME]: SERVICE_NAME,\n  }),\n  spanProcessors: [\n    // BatchSpanProcessor will flush spans in batches after some time,\n    // this is recommended in production. For development or testing purposes\n    // you may try SimpleSpanProcessor for instant span flushing to the Phoenix UI.\n    new BatchSpanProcessor(\n      new OTLPTraceExporter({\n        url: `${COLLECTOR_ENDPOINT}/v1/traces`,\n        // (optional) if connecting to Phoenix Cloud\n        // headers: { \"api_key\": process.env.PHOENIX_API_KEY },\n        // (optional) if connecting to self-hosted Phoenix with Authentication enabled\n        // headers: { \"Authorization\": `Bearer ${process.env.PHOENIX_API_KEY}` }\n      })\n    ),\n  ],\n});\n\nprovider.register();\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application Session\nDESCRIPTION: Initializes and launches the Arize Phoenix application UI. This function starts a local Phoenix server session, which allows for visualizing traces and interacting with evaluation data generated by the application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_with_mongodb_and_llama_index.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsession = px.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluators\nDESCRIPTION: This code defines two asynchronous evaluators: `answer_relevancy` and `context_relevancy`. Each evaluator utilizes OpenAI's GPT-4o model, using the `AnswerRelevancyEvaluator` and `ContextRelevancyEvaluator` from `llama_index.core.evaluation` respectively, to evaluate the answer and context relevancy.  Each evaluator returns a score and explanation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nasync def answer_relevancy(output, input) -> Tuple[Score, Explanation]:\n    ans = await AnswerRelevancyEvaluator(\n        llm=OpenAI(temperature=0, model=\"gpt-4o\"),\n    ).aevaluate(input[\"query\"], response=output[\"response\"])\n    return ans.score, ans.feedback\n\n\nasync def context_relevancy(output, input) -> Tuple[Score, Explanation]:\n    ans = await ContextRelevancyEvaluator(\n        llm=OpenAI(temperature=0, model=\"gpt-4o\"),\n    ).aevaluate(input[\"query\"], contexts=output[\"contexts\"])\n    return ans.score, ans.feedback\n\n\nevaluators = [answer_relevancy, context_relevancy]\n```\n\n----------------------------------------\n\nTITLE: Evaluating Agent Plans with Prompt Template\nDESCRIPTION: Defines a prompt for an AI evaluation assistant to assess agent plans. It takes the user task, available tools, and the plan as input, applies specific criteria (validity, sufficiency, efficiency, tool usage), and outputs 'ideal', 'valid', or 'invalid' as a single word.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/agent-planning.md#_snippet_0\n\nLANGUAGE: Prompt Template\nCODE:\n```\nYou are an evaluation assistant. Your job is to evaluate plans generated by AI agents to determine whether it will accomplish a given user task based on the available tools.\n\nHere is the data:\n    [BEGIN DATA]\n    ************\n    [User task]: {task}\n    ************\n    [Tools]: {tool_definitions}\n    ************\n    [Plan]: {plan}\n    [END DATA]\n\nHere is the criteria for evaluation\n1. Does the plan include only valid and applicable tools for the task?  \n2. Are the tools used in the plan sufficient to accomplish the task?  \n3. Will the plan, as outlined, successfully achieve the desired outcome?  \n4. Is this the shortest and most efficient plan to accomplish the task?\n\nRespond with a single word, \"ideal\", \"valid\", or \"invalid\", and should not contain any text or characters aside from that word.\n\n\"ideal\" means the plan generated is valid, uses only available tools, is the shortest possible plan, and will likely accomplish the task.\n\n\"valid\" means the plan generated is valid and uses only available tools, but has doubts on whether it can successfully accomplish the task.\n\n\"invalid\" means the plan generated includes invalid steps that cannot be used based on the available tools.\n```\n\n----------------------------------------\n\nTITLE: Defining Customer Intent Classification Prompt for OpenAI in Python\nDESCRIPTION: Provides a multi-line string prompt designed to classify customer input text into purchase or query intents using OpenAI. Specifies criteria for distinguishing purchases (item-related intent) versus queries (policy/general inquiries). Also requests categorical labels for supplementary shopping or query categories. This prompt is used as input for an AI model to parse and interpret customer messages for targeted assistance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ncustomer_intent_prompt = \"\"\"\nYou are a helpful assistant designed to output JSON. Classify the following customer text as either a 'purchase' or 'query'.\n\nTo help define the difference between purchae and query:\n- A purchase is a customer asking about a specific item or function of an item with the intent to purchase.\n- A query is likely asking about policies on returns, shipping, order modifications, and general inquiries outside of seeking to purchase an item.\n\nChoose the \\\"purchase\\\" category if you see both purchase and query intent.\nkey: customer_intent\nvalue: 'purchase' or a 'query'\n\nIf intent is purchase, append another key 'shopping_category' and the value should be one of the following:\n['Fitness and health',\n 'Gym and travel',\n 'Home and gym',\n 'Home automation',\n 'Kitchen',\n 'Office and home',\n 'Outdoor activities',\n 'Reading',\n 'Travel']\n\nIf intent is query, append another key 'query_category' and the value should be one of the following:\n['Damaged or Incorrect Orders',\n 'Delivery Time',\n 'Gift Services',\n 'International Shipping',\n 'Order Modification',\n 'Order Tracking',\n 'Payment Options',\n 'Return Policy',\n 'Security',\n 'Sustainability']\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Building a RouterQueryEngine to Route Queries Between SQL and Vector Tools Using LlamaIndex in Python\nDESCRIPTION: Initializes a RouterQueryEngine with an LLM-driven selector (LLMSingleSelector) that chooses the appropriate tool to answer each query. The router is supplied with query engine tools consisting of a SQL query engine for technical camera data and one or more vector store query engines for general knowledge.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_sql_retriever_tutorial.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nquery_engine = RouterQueryEngine(\n    selector=LLMSingleSelector.from_defaults(),\n    query_engine_tools=([sql_tool] + vector_tools),\n)\n```\n\n----------------------------------------\n\nTITLE: Applying Article Summarization Tool Prompt from Phoenix (Python)\nDESCRIPTION: Fetches the saved summarization prompt from Phoenix, applies it to downloaded articles, extracts the structured JSON output from the tool calls, and displays the results as an HTML table using pandas.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nprompt = Client().prompts.get(prompt_identifier=prompt_identifier)\n\ndef summarize(input: dict[str, str]):\n    response = Anthropic().messages.create(**prompt.format(variables=input))\n    for content in response.content:\n        if content.type == \"tool_use\":\n            yield content.input\n\n\nres = pd.json_normalize(chain.from_iterable(map(summarize, articles)))\ndisplay(HTML(res.to_html()))\n```\n\n----------------------------------------\n\nTITLE: Filtering Spans by Evaluation Results using 'evals' Keyword in Python Expression\nDESCRIPTION: Illustrates the specific syntax for filtering spans based on evaluation results. This Python boolean expression uses the special `evals` keyword, indexed by the evaluation name ('correctness'), to access evaluation attributes like `label` and compare them to a desired value ('incorrect'). This syntax works in the Phoenix UI search bar and `SpanQuery.where()`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/filter-spans.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nevals['correctness'].label == 'incorrect'\n```\n\n----------------------------------------\n\nTITLE: Loading and Launching Phoenix App\nDESCRIPTION: This snippet loads an example trace dataset ('llama_index_rag') and launches the Phoenix application with the trace data. It also opens the Phoenix UI in a browser window.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/span_query_language.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nds = px.load_example_traces(\"llama_index_rag\")\n(session := px.launch_app(trace=ds)).view()\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenTelemetry for BeeAI Instrumentation (instrumentation.js)\nDESCRIPTION: Demonstrates how to create an `instrumentation.js` file to configure the OpenTelemetry Node.js SDK. It initializes `BeeAIInstrumentation`, sets up resources (service and project names), configures an OTLP trace exporter, adds the instrumentation, starts the SDK provider, and manually instruments the `beeaiFramework` module if it's not loaded via `require`. Dependencies include `@opentelemetry/sdk-node`, `@opentelemetry/exporter-trace-otlp-proto`, semantic conventions, and the BeeAI instrumentation/framework packages.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/beeai.md#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// instrumentation.js\nimport { NodeSDK, node, resources } from \"@opentelemetry/sdk-node\";\nimport { OTLPTraceExporter } from \"@opentelemetry/exporter-trace-otlp-proto\";\nimport { ATTR_SERVICE_NAME } from \"@opentelemetry/semantic-conventions\";\nimport { SEMRESATTRS_PROJECT_NAME } from \"@arizeai/openinference-semantic-conventions\";\nimport { BeeAIInstrumentation } from \"@arizeai/openinference-instrumentation-beeai\";\nimport * as beeaiFramework from \"beeai-framework\";\n\n// Initialize Instrumentation Manually\nconst beeAIInstrumentation = new BeeAIInstrumentation();\n\nconst provider = new NodeSDK({\n  resource: new resources.Resource({\n    [ATTR_SERVICE_NAME]: \"beeai\",\n    [SEMRESATTRS_PROJECT_NAME]: \"beeai-project\",\n  }),\n  spanProcessors: [\n    new node.SimpleSpanProcessor(\n      new OTLPTraceExporter({\n        url: \"http://localhost:6006/v1/traces\",\n      }),\n    ),\n  ],\n  instrumentations: [beeAIInstrumentation],\n});\n\nawait provider.start();\n\n// Manually Patch BeeAgent (This is needed when the module is not loaded via require (commonjs))\nconsole.log(\"ðŸ”§ Manually instrumenting BeeAgent...\");\nbeeAIInstrumentation.manuallyInstrument(beeaiFramework);\nconsole.log(\"âœ… BeeAgent manually instrumented.\");\n\n// eslint-disable-next-line no-console\nconsole.log(\"ðŸ‘€ OpenInference initialized\");\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI Calls with Phoenix Telemetry in Python\nDESCRIPTION: Sets up instrumentation to collect telemetry data by registering Phoenix OpenTelemetry tracing with the project name 'prompt-optimization'. Then instruments OpenAI calls using OpenAIInstrumentor so that model call metadata and performance are tracked automatically in Phoenix during experiments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(project_name=\"prompt-optimization\")\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App and Setting Up Tracer\nDESCRIPTION: This code launches the Phoenix application and registers the OpenTelemetry tracer. It sets up the necessary components for collecting and visualizing traces and spans within the Phoenix UI.  The tracer provider is initialized to send trace data to a specified endpoint.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/human_feedback/chatbot_with_human_feedback.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npx.launch_app()\n```\n\nLANGUAGE: python\nCODE:\n```\ntracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Tracing for OpenAI Agents\nDESCRIPTION: Sets up the OpenTelemetry tracer provider to enable automatic instrumentation of OpenAI agent activities, registering the project with Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/openai_agents_cookbook.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# Setup Tracing\ntracer_provider = register(\n    project_name=\"openai-agents-cookbook\",\n    endpoint=\"https://app.phoenix.arize.com/v1/traces\",\n    auto_instrument=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying First 10 Rows of DataFrame (Pandas)\nDESCRIPTION: This snippet uses the `head()` method of a Pandas DataFrame to display the first 10 rows of the `questions_with_document_chunk_df`. This allows for a quick visual inspection of the data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nquestions_with_document_chunk_df.head(10)\n```\n\n----------------------------------------\n\nTITLE: Querying OpenAI Chat API with Phoenix Prompt Utilities in Python\nDESCRIPTION: This code obtains a specific prompt version from the Phoenix client, formats it along with variables into the OpenAI chat message schema, and sends it to the OpenAI API using the Python SDK. It requires the openai Python SDK, and credentials should be set in environment variables. The prompt_version_id selects the prompt, while 'variables' holds input data for templating. Returns a structured response object from OpenAI, output as JSON.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/examples/prompts/prompts_for_various_SDKs.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprompt_version_id = \"UHJvbXB0VmVyc2lvbjoy\"\nprompt_version = Client().prompts.get(prompt_version_id=prompt_version_id)\nprint(f\"prompt_version = {prompt_version.model_dump_json(indent=2)}\\n{'-'*100}\")\n\nmessages, kwargs = to_chat_messages_and_kwargs(\n    prompt_version, variables={\"question\": \"Who made you?\"}\n)\nprint(f\"messages = {json.dumps(messages, indent=2)}\\n{'-'*100}\")\nprint(f\"kwargs = {json.dumps(kwargs, indent=2)}\\n{'-'*100}\")\n\nresponse = openai.OpenAI().chat.completions.create(messages=messages, **kwargs)\nprint(f\"response = {response.model_dump_json(indent=2)}\")\n```\n\n----------------------------------------\n\nTITLE: Defining Code Execution Evaluator\nDESCRIPTION: Defines an evaluator to check if the generated Python code is runnable. This uses the `exec()` function to attempt to execute the generated code after cleaning it to ensure it doesn't include extraneous characters. A boolean result is returned.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndef code_is_runnable(output: str) -> bool:\n    \"\"\"Check if the code is runnable\"\"\"\n    output = output.get(\"code\")\n    output = output.strip()\n    output = output.replace(\"```python\", \"\").replace(\"```\", \"\")\n    try:\n        exec(output)\n        return True\n    except Exception:\n        return False\n```\n\n----------------------------------------\n\nTITLE: Instrument Llama-Index for Tracing Python\nDESCRIPTION: Configures OpenTelemetry tracing for LlamaIndex operations. It defines an OTLP endpoint (defaulting to localhost:4317) where trace spans will be sent. A 'TracerProvider' is set up with a 'SimpleSpanProcessor' that exports spans to this endpoint using 'OTLPSpanExporter'. The 'LlamaIndexInstrumentor' is then used to automatically capture traces from LlamaIndex calls and associate them with this provider.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nendpoint = \"http://127.0.0.1:4317\"\n(tracer_provider := TracerProvider()).add_span_processor(\n    SimpleSpanProcessor(OTLPSpanExporter(endpoint))\n)\nLlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Get QA with Reference Data\nDESCRIPTION: Retrieves question-answer pairs with reference data from Phoenix. It fetches data including input questions, context, and the LLM-generated output, setting the stage for QA correctness and hallucination evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nqa_with_reference_df = get_qa_with_reference(px.Client())\nqa_with_reference_df\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix Packages (Notebook)\nDESCRIPTION: This command installs the `arize-phoenix` package, which is necessary for using Phoenix features.  This package is a prerequisite for launching and using the Phoenix Notebook integration, which allows for visualizing traces within a Jupyter Notebook environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex-1.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Customizing Phoenix Client HTTP Headers Directly in Python\nDESCRIPTION: Shows creating a Phoenix client instance with custom HTTP headers passed as a dictionary via the headers parameter, allowing flexibility for authentication mechanisms beyond bearer tokens such as using the 'api-key' header.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.client import Client\n\nclient = Client(headers={\"api-key\": \"your-api-key\"})  # use `api-key` for Phoenix Cloud\n```\n\n----------------------------------------\n\nTITLE: Connecting to Phoenix Endpoint with Environment Variables in Python\nDESCRIPTION: This code sets up the Phoenix API endpoint and client headers using environment variables in Python. It includes configurable endpoint URL and API key management needed for authenticating and connecting to the Phoenix service.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/quickstart-prompts/quickstart-prompts-python.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# If you're self-hosting Phoenix, change this value:\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n\nPHOENIX_API_KEY = enter your api key\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Initial Experiment Results and Sampling Data for Meta Prompting in Python\nDESCRIPTION: Converts the initial experiment results to a pandas DataFrame and selects the first 10 entries to form a dataset for the meta prompting technique. This subset contains input, output, and evaluation data used to guide prompt generation for subsequent optimization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Access the experiment results from the first round as a dataframe\nground_truth_df = initial_experiment.as_dataframe()\n\n# Sample 10 examples to use as meta prompting examples\nground_truth_df = ground_truth_df[:10]\n```\n\n----------------------------------------\n\nTITLE: Running the Experiment with GPT-3.5-turbo in Phoenix\nDESCRIPTION: Runs the experiment in Phoenix, specifying the dataset and the GPT-3.5-turbo task function. This initiates the model comparison with GPT-3.5-turbo, allowing Phoenix to handle and orchestrate model evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nexperiment = run_experiment(dataset, task)\n```\n\n----------------------------------------\n\nTITLE: Opening Phoenix UI in Notebook in Python\nDESCRIPTION: Embeds the Phoenix UI directly into the notebook as an inline frame. The height parameter can be adjusted (defaults to 1000 pixels).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/manage-the-app.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsession.view()\n```\n\n----------------------------------------\n\nTITLE: Classifying with Explanations using Phoenix llm_classify (with nest_asyncio) - Python\nDESCRIPTION: Runs classification using the Phoenix 'llm_classify' function, with explanations, on the loaded DataFrame. Enables nest_asyncio for compatibility with Jupyter environments. Restricts output labels (rails), sets high concurrency for speed, and returns classifications and explanations, which assist in troubleshooting. Expects 'df', model, HUMAN_VS_AI_PROMPT_TEMPLATE, and rails to be defined.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n# The rails is used to hold the output to specific values based on the template\n# It will remove text such as \",,,\" or \"...\"\n# Will ensure the binary value expected from the template is returned\nrails = list(HUMAN_VS_AI_PROMPT_RAILS_MAP.values())\nrelevance_classifications = llm_classify(\n    dataframe=df,\n    template=HUMAN_VS_AI_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    verbose=False,\n    provide_explanation=True,\n    concurrency=50,\n)\n```\n\n----------------------------------------\n\nTITLE: Appending Self-Refinement Steps to Empathy Evaluation Prompt - Python\nDESCRIPTION: Appends instruction text to the empathy evaluation template instructing the LLM to perform self-critique and refinement after scoring, promoting iterative evaluation. The guidance prompts reflection, consideration of bias, and final score adjustment, specifying that only a number (1-10) should be output. This strategy enhances the reliability of automated judgment. Requires EMPATHY_EVALUATION_PROMPT_TEMPLATE to be present; no computation is performed directly.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nrefinement_text = \"\"\"\n---\nAfter you have done the evaluation, follow these two steps:\n1. Self-Critique\nReview your initial score:\n- Was it too harsh or lenient?\n- Did it consider the full context?\n- Would others agree with your score?\nExplain any inconsistencies briefly.\n\n2. Final Refinement\nBased on your critique, adjust your score if necessary.\n- Only output a number (1-10)\n\"\"\"\nEMPATHY_EVALUATION_PROMPT_TEMPLATE_IMPROVED = EMPATHY_EVALUATION_PROMPT_TEMPLATE + refinement_text\n```\n\n----------------------------------------\n\nTITLE: Evaluating Tool Call Appropriateness with Async OpenAI and Prompt Engineering Python\nDESCRIPTION: This snippet defines a detailed evaluation prompt for judging whether selected tool calls could precisely answer a given question, then implements an asynchronous function to run this evaluation using OpenAI's LLM in tool-augmented mode. The function checks the structure of model outputs, builds a test prompt, performs an async chat completion, and parses the response as 'correct' or 'incorrect.' Dependencies include AsyncOpenAI, Python async syntax, and valid input data structures. Inputs: dict describing the question and another with LLM-generated messages; Output: correctness as a single word.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/demos/playground_demo.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nasync_llm_client = AsyncOpenAI()\n\ntool_calling_evaluation_prompt_template = \"\"\"\nYou are an evaluation assistant evaluating questions and tool calls to\ndetermine whether the tool(s) called answer the question. The tool\ncalls have been generated by a separate agent, and chosen from the list of\ntools provided below. It is your job to decide whether that agent chose\nthe right tool(s) to call.\n\n    [BEGIN DATA]\n    ************\n    [Question]: {question}\n    ************\n    [Tools Called]: {tool_calls}\n    [END DATA]\n\nYour response must be single word, either \"correct\" or \"incorrect\",\nand should not contain any text or characters aside from that word.\n\"incorrect\" means that the chosen tool would not answer the question,\nthe tool includes information that is not presented in the question,\nor that the tool signature includes parameter values that don't match\nthe formats specified in the tool signatures below.\n\n\"correct\" means the correct tool call(s) were chosen, the correct parameters\nwere extracted from the question, the tool call generated is runnable and correct,\nand that no outside information not present in the question was used\nin the generated question.\n\n    [Tool Definitions]: {tool_definitions}\n\"\"\"\n\n\nasync def judged_correct(input: dict[str, Any], output: dict[str, Any]) -> str:\n    if len(messages := output.get(\"messages\", {})) != 1:\n        raise ValueError(\"expected exactly one message\")\n    tool_calls = messages[0].get(\"tool_calls\", [])\n    evaluation_prompt = tool_calling_evaluation_prompt_template.format(\n        question=input[\"question\"],\n        tool_calls=json.dumps(tool_calls),\n        tool_definitions=json.dumps(tools),\n    )\n    response = await async_llm_client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"content\": evaluation_prompt, \"role\": \"user\"}],\n        tools=tools,\n    )\n    assert isinstance(response_content := response.choices[0].message.content, str)\n    response_content = response_content.strip()\n    if response_content not in (\"correct\", \"incorrect\"):\n        raise ValueError(f\"expected 'correct' or 'incorrect', got {response_content}\")\n    return response_content\n```\n\n----------------------------------------\n\nTITLE: Instrumenting DSPy\nDESCRIPTION: The provided code uses `openinference-instrumentation-dspy` to instrument the DSPy library. This instrumentation will send all calls made to DSPy to Phoenix. This enhances observability by tracking the interactions with DSPy, providing insights into the prompts being optimized and their execution. This setup enables detailed logging of the actions taken by DSPy during prompt optimization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.dspy import DSPyInstrumentor\n\nDSPyInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Logging Evaluation Metrics to Phoenix for Visualization\nDESCRIPTION: This snippet sends QA correctness and hallucination evaluation data to Phoenix using the Phoenix client and SpanEvaluations objects, enabling visualization of model performance. Dependencies include 'phoenix.trace.SpanEvaluations' and 'px.Client()'.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_43\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.trace import SpanEvaluations\n\npx.Client().log_evaluations(\n    SpanEvaluations(dataframe=qa_correctness_eval_df, eval_name=\"Q&A Correctness\"),\n    SpanEvaluations(dataframe=hallucination_eval_df, eval_name=\"Hallucination\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Full Phoenix Library for LLM Evaluations in Python\nDESCRIPTION: Installs the complete Phoenix SDK package required for advanced features such as running evaluations and detailed span queries, beyond the lighter OTEL tracing library.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/amazon_bedrock_agents_tracing_and_evals.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Preparing Training Data\nDESCRIPTION: This code prepares training data for the DSPy optimizer. It iterates over the rows of the `ground_truth_df` DataFrame and creates `dspy.Example` objects. Each example contains the `prompt` from the `input` column and the `label` (derived from the `expected` column). This training set is then used by the MIPROv2 optimizer for prompt tuning and model optimization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n# Prepare training data from previous examples\ntrain_data = []\nfor _, row in ground_truth_df.iterrows():\n    example = dspy.Example(\n        prompt=row[\"input\"][\"prompt\"], label=row[\"expected\"][\"type\"]\n    ).with_inputs(\"prompt\")\n    train_data.append(example)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting LlamaIndex and OpenAI with OpenTelemetry - Python\nDESCRIPTION: Configures and applies OpenTelemetry tracing to LlamaIndex and OpenAI API interactions using an OTLP endpoint. Instantiates a TracerProvider and attaches a SimpleSpanProcessor for exporting traces. Dependencies: opentelemetry, openinference. Inputs: Custom tracing endpoint URL. Outputs: Instrumented LlamaIndex/OpenAI operations with trace telemetry.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/run_experiments_with_llama_index.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nendpoint = \"http://127.0.0.1:6006/v1/traces\"\ntracer_provider = trace_sdk.TracerProvider()\ntracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n\nLlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Tracer for Phoenix\nDESCRIPTION: This code configures the OpenTelemetry tracer by adding a SpanProcessor that exports traces to Phoenix. It registers the tracer, then adds a `SimpleSpanProcessor` that sends traces to the specified Phoenix endpoint. This is essential for collecting and viewing the generated traces in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nphoenix_otlp_endpoint = \"https://app.phoenix.arize.com/v1/traces\"\ntracer_provider = register()\ntracer_provider.add_span_processor(SimpleSpanProcessor(endpoint=phoenix_otlp_endpoint))\n```\n\n----------------------------------------\n\nTITLE: Performing Basic Anthropic Text Generation (Python)\nDESCRIPTION: Demonstrates a simple call to the Anthropic API using `MessageCreateParamsBase` to generate text based on a system prompt and user message, then prints the output.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nparams = MessageCreateParamsBase(\n    model=\"claude-3-5-haiku-latest\",\n    max_tokens=128,\n    temperature=0,\n    system=\"You're a coding poet\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a haiku about recursion in programming.\"}],\n)\nresp = Anthropic().messages.create(**params)\nprint(resp.content[0].text)\n```\n\n----------------------------------------\n\nTITLE: Connect to Phoenix Client (Python)\nDESCRIPTION: This code block initializes the connection to the Phoenix platform. It checks for the presence of a `PHOENIX_API_KEY` environment variable to determine if it should connect to a cloud instance or launch a local Phoenix server. Required dependencies: `os`, `phoenix` (if launching locally).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Check if PHOENIX_API_KEY is present in the environment variables.\n# If it is, we'll use the cloud instance of Phoenix. If it's not, we'll start a local instance.\n# A third option is to connect to a docker or locally hosted instance.\n# See https://docs.arize.com/phoenix/setup/environments for more information.\n\nimport os\n\nif \"PHOENIX_API_KEY\" in os.environ:\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\n    os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n\nelse:\n    import phoenix as px\n\n    px.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Invoking Bedrock Model with Instrumented Client\nDESCRIPTION: This snippet invokes the Bedrock model using the `instrumented_client`. Because the client is instrumented, this call generates traces that will be printed to the console (due to the configured SpanProcessor). The prompt is sent to the model, and the completion from the response is printed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresponse = instrumented_client.invoke_model(modelId=\"anthropic.claude-v2:1\", body=prompt)\nresponse_body = json.loads(response.get(\"body\").read())\nprint(response_body[\"completion\"])\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame, Uploading Dataset\nDESCRIPTION: This snippet creates a Pandas DataFrame from the `agent_tool_responses` dictionary, then uploads it to the Phoenix client. The uploaded dataset is used as ground truth to test and evaluate agent tool calling.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ntool_calling_df = pd.DataFrame(agent_tool_responses.items(), columns=[\"question\", \"tool_calls\"])\ndataset = px_client.upload_dataset(\n    dataframe=tool_calling_df,\n    dataset_name=f\"tool_calling_ground_truth_{id}\",\n    input_keys=[\"question\"],\n    output_keys=[\"tool_calls\"],\n)\n\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Reinstalling Shapely for Compatibility in Python\nDESCRIPTION: This snippet forcibly reinstalls an older version of the `shapely` library (version less than 2.0.0) to avoid version compatibility issues in the Colab environment. It also installs Phoenix, LangChain, Google Cloud AI Platform SDK, and related packages needed for building and tracing LangChain + VertexAI applications.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_vertex_ai_tracing_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install --force-reinstall \"shapely<2.0.0\"\n```\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"arize-phoenix[evals]\" google-api-python-client \"google-cloud-aiplatform[preview]\" langchain langchain-community nest-asyncio langchain-google-vertexai\n```\n\n----------------------------------------\n\nTITLE: Running an Experiment to Evaluate Agent Performance\nDESCRIPTION: Executes an evaluation experiment on the math solver agent using the generated dataset and correctness evaluator, tracking results in Phoenix for analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/openai_agents_cookbook.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments import run_experiment\n\ninitial_experiment = run_experiment(\n    dataset,\n    task=solve_math_problem,\n    evaluators=[correctness_eval],\n    experiment_description=\"Solve Math Problems\",\n    experiment_name=f\"solve-math-questions-{str(uuid.uuid4())[:5]}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Tracing Environment Variables - Python\nDESCRIPTION: Prompts for the user's Phoenix API key if not set, and then configures necessary environment variables for Phoenix tracing in the cloud. Dependencies include an active Phoenix account for API keys and network access for logging. Inputs include secret user entry for the API key, with outputs being modified environment variables for use by the remainder of the application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif os.getenv(\"PHOENIX_API_KEY\") is None:\n    os.environ[\"PHOENIX_API_KEY\"] = getpass(\"Enter your Phoenix API key: \")\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com/\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.getenv('PHOENIX_API_KEY')}\"\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix Tracer Provider with Automatic Instrumentation in Python\nDESCRIPTION: Registers Phoenix OpenTelemetry tracer provider with the specified project name and activates automatic instrumentation for supported openinference packages, such as Bedrock. This ensures that API calls to Bedrock agents are instrumented and trace data is sent automatically to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/amazon_bedrock_agents_tracing_and_evals.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nproject_name = \"Amazon Bedrock Agent Example\"\n\ntracer_provider = register(project_name=project_name, auto_instrument=True)\n```\n\n----------------------------------------\n\nTITLE: Querying the Remote LangGraph App\nDESCRIPTION: This snippet demonstrates querying the deployed LangGraph app running on Agent Engine using the `remote_agent.query()` method. The queries are identical to the local testing phase, allowing verification of the application's functionality in the remote environment.  It uses the same prompts to validate deployment and remote accessibility.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/google_agent_engine_tracing_tutorial.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nremote_agent.query(message=\"Get product details for shoes\")\n```\n\nLANGUAGE: python\nCODE:\n```\nremote_agent.query(message=\"Get product details for coffee\")\n```\n\nLANGUAGE: python\nCODE:\n```\nremote_agent.query(message=\"Get product details for smartphone\")\n```\n\nLANGUAGE: python\nCODE:\n```\nremote_agent.query(message=\"Tell me about the weather\")\n```\n\n----------------------------------------\n\nTITLE: Setting up Phoenix Auto-Instrumentation for Haystack (Python)\nDESCRIPTION: Initializes and configures the Phoenix tracer using `phoenix.otel.register`. It sets a project name for organization within Phoenix and enables `auto_instrument=True`, which automatically patches installed libraries (like Haystack via `openinference-instrumentation-haystack`) to capture traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/haystack.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Logging Evaluation Results to Phoenix Platform\nDESCRIPTION: This code demonstrates how to push evaluation results, whether classification or scoring, into the Phoenix platform using the SpanEvaluations API. It requires that the dataframe includes a 'context.span_id' for trace association. By creating a SpanEvaluations object and calling log_evaluations, users can visualize aggregate evaluation metrics within Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/bring-your-own-evaluator.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.trace import SpanEvaluations\n\npx.Client().log_evaluations(\n    SpanEvaluations(eval_name=\"Your Eval Display Name\", dataframe=test_results)\n)\n```\n\n----------------------------------------\n\nTITLE: Running an Experiment Using Phoenix run_experiment Function in Python\nDESCRIPTION: This snippet demonstrates running a full experiment using the uploaded dataset and defined task function while applying multiple evaluators. The `run_experiment` method returns an experiment object containing results. Parameters include the dataset, task, experiment name, and a list of evaluators for grading. This requires Phoenix's experiment module and prior task and evaluator definitions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/quickstart-datasets.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments import run_experiment\n\nexperiment = run_experiment(\n    dataset,\n    task,\n    experiment_name=\"initial-experiment\",\n    evaluators=[jaccard_similarity, accuracy],\n)\n```\n\n----------------------------------------\n\nTITLE: Querying for Q&A Data with Retrieved Documents using Phoenix (Python)\nDESCRIPTION: Presents a complex query pattern to extract data suitable for Q&A evaluations on retrieved documents. It defines two queries: `query_for_root_span` filters for root spans (`parent_id is None`) and selects input/output; `query_for_retrieved_documents` filters for 'RETRIEVER' spans, sets the index to `parent_id` (renamed as `span_id`), and concatenates document content into a 'reference' column. The results are joined using `pandas.concat()` after executing both queries with `px.Client().query_spans()`. Requires `pandas` library.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom phoenix.trace.dsl import SpanQuery\n\nquery_for_root_span = SpanQuery().where(\n    \"parent_id is None\",   # Filter for root spans\n).select(\n    input=\"input.value\",   # Input contains the user's question\n    output=\"output.value\", # Output contains the LLM's answer\n)\n\nquery_for_retrieved_documents = SpanQuery().where(\n    \"span_kind == 'RETRIEVER'\",  # Filter for RETRIEVER span\n).select(\n    # Rename parent_id as span_id. This turns the parent_id\n    # values into the index of the output dataframe.\n    span_id=\"parent_id\",\n).concat(\n    \"retrieval.documents\",\n    reference=\"document.content\",\n)\n\n# Perform an inner join on the two sets of spans.\npd.concat(\n    px.Client().query_spans(\n        query_for_root_span,\n        query_for_retrieved_documents,\n    ),\n    axis=1,\n    join=\"inner\",\n)\n```\n\n----------------------------------------\n\nTITLE: Running LLM-as-a-Judge Experiment with Style-Invariant Prompt - Python\nDESCRIPTION: Implements the judge function and experiment workflow using the style-invariant prompt template. The function sends the response and updated prompt into the LLM classifier, evaluating over a 1-10 scale. The experiment is then run with the judge as the task and a specified evaluator. Dependencies include llm_classify, EMPATHY_EVALUATION_PROMPT_TEMPLATE_IMPROVED, OpenAIModel, evaluate_response, and run_experiment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef llm_as_a_judge(input):\n    response_classifications = llm_classify(\n        dataframe=pd.DataFrame([{\"AI_Response\": input[\"AI_Response\"]}]),\n        template=EMPATHY_EVALUATION_PROMPT_TEMPLATE_IMPROVED,\n        model=OpenAIModel(model=\"gpt-4\"),\n        rails=list(map(str, range(1, 11))),\n        provide_explanation=True,\n    )\n    score = response_classifications.iloc[0][\"label\"]\n    return int(score)\n\n\nexperiment = run_experiment(\n    dataset, task=llm_as_a_judge, evaluators=[evaluate_response], experiment_name=\"style_invariant\"\n)\n```\n\n----------------------------------------\n\nTITLE: Formatting and Displaying Tool Definitions for Evaluation - Python\nDESCRIPTION: Concatenates tool definitions from a tools iterable, combining each tool's name and description into a formatted string. Prints the resulting string to standard output and prepares a variable to be used in later evaluation. Inputs: tools iterable with .name and .description attributes. Outputs: tool_definitions string. Assumes tools are defined elsewhere in the code.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\ntool_definitions = \"\"\n\nfor current_tool in tools:\n    tool_definitions += f\"\"\"\n    {current_tool.name}: {current_tool.description}\n    \"\"\"\n\nprint(tool_definitions)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting for OpenInference and Tracing - Python\nDESCRIPTION: This snippet configures OpenInference instrumentation for Mistral AI and LlamaIndex, allowing for tracing of operations within the RAG pipeline. It registers the tracing provider to capture the interactions and data flow within the pipeline, which can then be analyzed using Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\nfrom openinference.instrumentation.mistralai import MistralAIInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\nMistralAIInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix API Key and Endpoint (Cloud)\nDESCRIPTION: This Python snippet configures the environment variables required to connect to the Phoenix Cloud instance.  It sets the `PHOENIX_API_KEY` and `PHOENIX_COLLECTOR_ENDPOINT` environment variables using the provided API key and endpoint.  The API key can be found on the Phoenix dashboard.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/hfsmolagents.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix Tracer and Instrumenting LlamaIndex\nDESCRIPTION: Initializes Phoenix tracing endpoint via 'register', obtaining a tracer provider. Instruments LlamaIndex with Phoenix, enabling trace collection for LlamaIndex operations and user sessions through OpenInference's instrumentation module.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/project_sessions_llama_index_query_engine.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ntracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Testing the Task Function on an Example in Python\nDESCRIPTION: Retrieves the first example from the `dataset`. Executes the previously defined `task` function using the `input` attribute of this example. The output of the task is then converted to a JSON string and shortened to a maximum width of 80 characters using `textwrap.shorten` before being printed. This serves as a quick check to verify that the `task` function runs successfully and produces output.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nexample = dataset[0]\ntask_output = task(example.input)\nprint(shorten(json.dumps(task_output), width=80))\n```\n\n----------------------------------------\n\nTITLE: Evaluating with Ragas\nDESCRIPTION: This code snippet evaluates a dataset using specified Ragas metrics. It uses the `evaluate` function to compute evaluation results based on the `ragas_eval_dataset` and a list of metrics, and then converts the results into a pandas DataFrame for further processing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nwith using_project(\"ragas-evals\"):\n    evaluation_result = evaluate(\n        dataset=ragas_eval_dataset,\n        metrics=[faithfulness, answer_correctness, context_recall, context_precision],\n    )\neval_scores_df = pd.DataFrame(evaluation_result.scores)\n```\n\n----------------------------------------\n\nTITLE: Initializing Langchain Tool-Calling Agent in Python\nDESCRIPTION: Initializes the components for a Langchain agent. It creates a `ChatOpenAI` instance using the 'gpt-4o' model. It pulls a standard OpenAI functions agent prompt from Langchain Hub (`hwchase17/openai-functions-agent`). It then uses `create_tool_calling_agent` to construct the agent logic, providing the LLM, the list of defined `tools`, and the prompt. Finally, it sets up an `AgentExecutor` to run the agent, passing the agent logic, the tools list, and enabling verbose output (`verbose=True`) for detailed logging during execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nllm = ChatOpenAI(model=\"gpt-4o\")\nprompt = hub.pull(\"hwchase17/openai-functions-agent\")\nagent = create_tool_calling_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI Model Calls for Phoenix Tracking - Python\nDESCRIPTION: Instruments OpenAI API calls so that all model invocations are automatically tracked and logged in Phoenix via OpenTelemetry. Sets up the Phoenix tracer provider, registers it with the project, and instruments OpenAI API calls accordingly. Dependencies: openinference.instrumentation.openai, phoenix.otel. No parameters. This is optional for experiment execution, but provides full traceability in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(project_name=\"prompt-optimization\")\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables and API Keys for Phoenix and OpenAI in Python\nDESCRIPTION: Configures environment variables required to connect with the Phoenix Cloud API and OpenAI API. It uses getpass to securely prompt users for their Phoenix API key and OpenAI API key if these are not already set as environment variables. These keys enable authenticated access to Phoenix services and OpenAI language models for prompt execution and experiment tracking.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/chain_of_thought_prompting.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\nif not os.environ.get(\"PHOENIX_CLIENT_HEADERS\"):\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = \"api_key=\" + getpass(\"Enter your Phoenix API key: \")\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix locally via Command Line\nDESCRIPTION: This code installs the Phoenix package and starts a local Phoenix server for testing and development. It involves pip installation and executing the 'phoenix serve' command. Customization options are referenced for terminal deployment. Useful for local testing and debugging.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/anthropic.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Load Document Index and Set up Query Engine\nDESCRIPTION: Retrieves a document knowledge base from Google Cloud Storage, sets up storage and service contexts for LlamaIndex, and prepares a query engine to answer questions about Arize documentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_application_tracing_evaluating_and_analysis.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfile_system = GCSFileSystem(project=\"public-assets-275721\")\nindex_path = \"arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/index/\"\nstorage_context = StorageContext.from_defaults(\n    fs=file_system,\n    persist_dir=index_path,\n    graph_store=SimpleGraphStore(),  # prevents unauthorized request to GCS\n)\nservice_context = ServiceContext.from_defaults(\n    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.0),\n    embed_model=OpenAIEmbedding(model=\"text-embedding-ada-002\"),\n)\nindex = load_index_from_storage(\n    storage_context,\n    service_context=service_context,\n)\nquery_engine = index.as_query_engine()\n\n# Asking the Application questions about the Arize product\nqueries = [\n    \"How can I query for a monitor's status using GraphQL?\",\n    \"How do I delete a model?\",\n    \"How much does an enterprise license of Arize cost?\",\n    \"How do I log a prediction using the python SDK?\",\n]\n\nfor query in tqdm(queries):\n    response = query_engine.query(query)\n    print(f\"Query: {query}\")\n    print(f\"Response: {response}\")\n```\n\n----------------------------------------\n\nTITLE: Creating a New Alembic Migration Revision (Bash)\nDESCRIPTION: This command initializes a new Alembic migration script with a descriptive message. Replace 'your_revision_name' with a meaningful identifier for the migration purpose. Requires Alembic to be installed and the user to be within the correct directory. No immediate changes are made to the database; further editing of the created file is required.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/src/phoenix/db/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nalembic revision -m \"your_revision_name\"\n```\n\n----------------------------------------\n\nTITLE: Instantiating and Using OpenAI Model - Python\nDESCRIPTION: This Python code snippet demonstrates how to instantiate an OpenAI model with a specific model name and temperature, and then use the model to generate a response to a given question. It requires the `OpenAIModel` class (presumably from a specific library) and the OpenAI API key setup. The output is a string.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nmodel = OpenAIModel(model_name=\"gpt-4\",temperature=0.6)\nmodel(\"What is the largest costal city in France?\")\n```\n```\n\n----------------------------------------\n\nTITLE: Running Document Relevance Evaluations Using Phoenix in Python\nDESCRIPTION: Executes run_evals using the RelevanceEvaluator over the retrieved documents DataFrame, requesting explanations for each judgment. The function is parallelized with concurrency=20. Produces a DataFrame with document-level evaluation results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nretrieved_documents_relevance_df = run_evals(\n    evaluators=[relevance_evaluator],\n    dataframe=retrieved_documents_df,\n    provide_explanation=True,\n    concurrency=20,\n)[0]\nretrieved_documents_relevance_df\n```\n\n----------------------------------------\n\nTITLE: Defining RAG Relevance Evaluation Prompt Template in Python\nDESCRIPTION: This Python multi-line string defines a prompt template instructing an LLM to compare a reference text and a question, determining if the reference can answer the question. The template expects the model to respond with a single word ('relevant' or 'unrelated'), enforcing output constraints for downstream evaluation. No external dependencies are required, but correct formatting of `{query}` and `{reference}` placeholders is needed for formatting prior to LLM usage. Inputs are the question and reference text; output is a one-word relevance classification.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/retrieval-rag-relevance.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nYou are comparing a reference text to a question and trying to determine if the reference text\ncontains information relevant to answering the question. Here is the data:\n    [BEGIN DATA]\n    ************\n    [Question]: {query}\n    ************\n    [Reference text]: {reference}\n    [END DATA]\n\nCompare the Question above to the Reference text. You must determine whether the Reference text\ncontains information that can answer the Question. Please focus on whether the very specific\nquestion can be answered by the information in the Reference text.\nYour response must be single word, either \"relevant\" or \"unrelated\",\nand should not contain any text or characters aside from that word.\n\"unrelated\" means that the reference text does not contain an answer to the Question.\n\"relevant\" means the reference text contains an answer to the Question.\n```\n\n----------------------------------------\n\nTITLE: Defining Asynchronous OpenAI Summarization Task - Python\nDESCRIPTION: Creates an asynchronous function `summarize_article_openai` that takes an `Example` object, a `prompt_template` string, and a model name. It formats the prompt using the article content from the example, makes an asynchronous call to the OpenAI chat completions endpoint, and returns the generated summary text.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/summarization.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import AsyncOpenAI\nfrom phoenix.experiments import Example\n\nopenai_client = AsyncOpenAI()\n\n\nasync def summarize_article_openai(example: Example, prompt_template: str, model: str) -> str:\n    formatted_prompt_template = prompt_template.format(article=example.input[\"article\"])\n    response = await openai_client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"assistant\", \"content\": formatted_prompt_template},\n        ],\n    )\n    assert response.choices\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Instantiating GPT-4 Turbo model for summarization evaluation\nDESCRIPTION: Creates an instance of the OpenAI GPT-4 Turbo model with zero temperature, which offers a balance of speed and accuracy compared to GPT-4 and GPT-3.5.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model_name=\"gpt-4-turbo-preview\", temperature=0.0)\n```\n\n----------------------------------------\n\nTITLE: Querying Retriever Spans\nDESCRIPTION: This snippet queries spans associated with the retriever component of the RAG system. It selects the input, explodes the retrieval documents array, and filters for spans where IS_RETRIEVER is true.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/span_query_language.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsession.query_spans(\n    SpanQuery()\n    .select(**INPUT)\n    .explode(RETRIEVAL_DOCUMENTS, reference=DOCUMENT_CONTENT, score=DOCUMENT_SCORE)\n    .where(IS_RETRIEVER)\n)\n```\n\n----------------------------------------\n\nTITLE: Running Agent Pipeline for Natural Language Queries in Python\nDESCRIPTION: Creates a QueryPipelineAgentWorker from the prepared pipeline and initializes an AgentRunner with this worker. Executes a chat query to check if 'Aerosmith' is in the database and prints the textual agent response. Requires prior pipeline setup and agent framework context for AgentRunner and QueryPipelineAgentWorker classes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nagent_worker = QueryPipelineAgentWorker(qp)\nagent = AgentRunner(agent_worker)\nresponse = agent.chat(\"Is Aerosmith in this database?\")\nprint(str(response))\n```\n\n----------------------------------------\n\nTITLE: Launching Arize Phoenix Application (Python)\nDESCRIPTION: Launches the Arize Phoenix application in the background, providing a web interface for viewing traces, datasets, and experiment results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/run_experiments_with_llama_index.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npx.launch_app().view()\n```\n\n----------------------------------------\n\nTITLE: Defining VertexAIModel Parameters - Python\nDESCRIPTION: Declares the VertexAIModel class attributes for configuration and authentication of Google's Vertex AI LLMs. The model requires the 'google-cloud-aiplatform>=1.33.0' dependency. Parameters such as 'project', 'location', and optionally 'credentials' are necessary for API access, while other fields like 'model', 'temperature', 'max_tokens', 'top_p', and 'top_k' govern generation and sampling. Inputs depend on your Google Cloud Project settings; outputs are generated completions via the Vertex AI backend.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evaluation-models.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass VertexAIModel:\n    project: Optional[str] = None\n    location: Optional[str] = None\n    credentials: Optional[\"Credentials\"] = None\n    model: str = \"text-bison\"\n    tuned_model: Optional[str] = None\n    temperature: float = 0.0\n    max_tokens: int = 256\n    top_p: float = 0.95\n    top_k: int = 40\n```\n\n----------------------------------------\n\nTITLE: Defining Jarowinkler Similarity Evaluator Function in Python\nDESCRIPTION: Defines a function to compute the Jarowinkler similarity metric between the stringified JSON representations of the model output and the expected output. It sorts keys to ensure stable comparisons. This evaluator returns a float similarity score used later to assess experiment performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/langchain_email_extraction.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef jarowinkler_similarity(output, expected) -> float:\n    return jarowinkler.jarowinkler_similarity(\n        json.dumps(output, sort_keys=True),\n        json.dumps(expected, sort_keys=True),\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Tracer and Instrumenting OpenAI\nDESCRIPTION: Sets up OpenTelemetry tracing using Phoenix. It registers a tracer provider pointing to a local OpenTelemetry collector endpoint (localhost:4317) and specifies a project name ('vision-fixture'). It then configures the trace settings, specifically increasing the maximum length for base64 encoded images, and instruments the OpenAI library to automatically capture traces for API calls using the configured tracer provider and settings.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/vision.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\ntracer_provider = register(endpoint=\"http://127.0.0.1:4317\", project_name=\"vision-fixture\")\nconfig = TraceConfig(base64_image_max_length=1_000_000_000)\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider, config=config)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable (Python)\nDESCRIPTION: Imports necessary libraries (`os`, `getpass`, `dotenv`), loads environment variables from a `.env` file if present, and securely retrieves the OpenAI API key either from the environment or by prompting the user. It then sets the `OPENAI_API_KEY` environment variable, which is required for interacting with the OpenAI API.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evaluating_traces_quickstart.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport dotenv\n\ndotenv.load_dotenv()\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Initializing Bedrock Client with boto3\nDESCRIPTION: This snippet demonstrates how to initialize a boto3 session and create a Bedrock runtime client.  It's important to instrument boto3 *before* creating the bedrock-runtime client to ensure all calls to `invoke_model` are traced. The `session.client` method returns a Bedrock runtime client used to interact with the Bedrock service.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/bedrock.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport boto3\n\nsession = boto3.session.Session()\nclient = session.client(\"bedrock-runtime\")\n```\n\n----------------------------------------\n\nTITLE: Viewing RAG Relevancy Classification Prompt Template in Python\nDESCRIPTION: Prints the default prompt template string (RAG_RELEVANCY_PROMPT_TEMPLATE) used for LLM relevance classification. The template contains variables (input, reference, output) that are formatted per each evaluated sample. This template can be modified to tune LLM behavior during experiments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(RAG_RELEVANCY_PROMPT_TEMPLATE)\n```\n\n----------------------------------------\n\nTITLE: Setting Up LangChain Chat Model with Custom Functions in Python\nDESCRIPTION: Configures a ChatOpenAI language model instance (GPT-4o) bound to a function schema defined in the registry for the email extraction task, forcing the model to call a specified OpenAI function. Chains the LangChain prompt instructions, the LLM response, and a JSON output parser for structured extraction results. This chain object enables invoking the model with function calling capability.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/langchain_email_extraction.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nllm = ChatOpenAI(model=\"gpt-4o\").bind_functions(\n    functions=[registry[dataset_name].schema],\n    function_call=registry[dataset_name].schema.schema()[\"title\"],\n)\noutput_parser = JsonOutputFunctionsParser()\nextraction_chain = registry[dataset_name].instructions | llm | output_parser\n```\n\n----------------------------------------\n\nTITLE: Accessing and Displaying the Query Response\nDESCRIPTION: Outputs the direct response generated by the language model based on the query, allowing assessment of retrieval relevance and answer quality.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nresponse_vector.response\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenAI Provider with Vercel AI SDK\nDESCRIPTION: Configures the OpenAI provider by prompting for an API key and initializing the createOpenAI utility from Vercel AI SDK. This establishes the connection to OpenAI's API services.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_vercel_ai_sdk.ipynb#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst apiKey = prompt(\"Enter your OpenAI API key:\")\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst openai = createOpenAI({ apiKey: apiKey });\n```\n\n----------------------------------------\n\nTITLE: Enabling Llama-Index Instrumentation with OpenInference and Phoenix in Python\nDESCRIPTION: This code enables tracing of Llama-Index internals using LlamaIndexInstrumentor and OpenInference-compliant trace providers registered to Phoenix. Requires phoenix and openinference.instrumentation.llama_index; expects a local or remote Phoenix endpoint. Returns an instrumented state for effective observability.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\nLlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Configuring Pandas (Python)\nDESCRIPTION: Imports standard and third-party libraries essential for the notebook, including `os`, `collections`, `getpass`, and `pandas`, as well as specific evaluation components from the `phoenix.evals` module. It also configures pandas to display the full content of dataframe columns.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom collections import Counter\nfrom getpass import getpass\n\nimport pandas as pd\n\nfrom phoenix.evals import (\n    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n    RAG_RELEVANCY_PROMPT_TEMPLATE,\n    OpenAIModel,\n    download_benchmark_dataset,\n    llm_classify,\n)\n\npd.set_option(\"display.max_colwidth\", None)\n```\n\n----------------------------------------\n\nTITLE: Install Phoenix Evals Dependencies\nDESCRIPTION: This snippet installs the necessary Python packages for using Phoenix Evals.  It utilizes `pip` to install the `arize-phoenix`, `openai`, and `nest_asyncio` packages.  The `-q` flag ensures a quiet installation.  These packages are crucial for setting up and running evaluations within the Phoenix framework.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/quickstart/evals.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n%%bash\npip install -q \"arize-phoenix>=4.29.0\"\npip install -q openai nest_asyncio\n```\n\n----------------------------------------\n\nTITLE: Formatting Agent Message History in Python\nDESCRIPTION: Defines a Python function `format_message_steps` that takes a list of message objects (typically from an OpenAI API interaction) and converts them into a formatted, human-readable string. It identifies user, system, assistant (including tool calls), and tool response messages to reconstruct the conversation flow.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\ndef format_message_steps(messages):\n    \"\"\"\n    Convert a list of message objects into a readable format that shows the steps taken.\n\n    Args:\n        messages (list): A list of message objects containing role, content, tool calls, etc.\n\n    Returns:\n        str: A readable string showing the steps taken.\n    \"\"\"\n    steps = []\n    for message in messages:\n        role = message.get(\"role\")\n        if role == \"user\":\n            steps.append(f\"User: {message.get('content')}\")\n        elif role == \"system\":\n            steps.append(\"System: Provided context\")\n        elif role == \"assistant\":\n            if message.get(\"tool_calls\"):\n                for tool_call in message[\"tool_calls\"]:\n                    tool_name = tool_call[\"function\"][\"name\"]\n                    steps.append(f\"Assistant: Called tool '{tool_name}'\")\n            else:\n                steps.append(f\"Assistant: {message.get('content')}\")\n        elif role == \"tool\":\n            steps.append(f\"Tool response: {message.get('content')}\")\n\n    return \"\\n\".join(steps)\n```\n\n----------------------------------------\n\nTITLE: Appending Style-Invariant Examples to Evaluation Prompt - Python\nDESCRIPTION: This snippet augments the empathy evaluation prompt to emphasize style-invariant evaluation, illustrating how different writing tones can yield similar empathy scores. It concatenates a style-diverse scenario block to the existing prompt, encouraging the LLM to focus on content over stylistic differences. Relies on the variable EMPATHY_EVALUATION_PROMPT_TEMPLATE being defined earlier and produces a revised prompt for further use.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nstyle_invariant = \"\"\"\n----\nTo help guide your reasoning, below is an example of how different response styles and tones can achieve similar scores:\n\n#### Scenario: Customer Support Handling a Late Order\nUser: \"My order is late, and I needed it for an important event. This is really frustrating.\"\n\nResponse A (Formal): \"I sincerely apologize for the delay...\"\nResponse B (Casual): \"Oh no, thatâ€™s really frustrating!...\"\nResponse C (Direct): \"Sorry about that. Iâ€™ll check...\"\n\"\"\"\nEMPATHY_EVALUATION_PROMPT_TEMPLATE_IMPROVED = EMPATHY_EVALUATION_PROMPT_TEMPLATE + style_invariant\n```\n\n----------------------------------------\n\nTITLE: Set OpenAI API Key (Python)\nDESCRIPTION: This Python snippet retrieves and sets the OpenAI API key from the environment or prompts the user to enter it.  It first checks if the `OPENAI_API_KEY` environment variable is already set. If not, it prompts the user for input using `getpass`. The key is then set in the environment. This is a prerequisite for using OpenAI models in the evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/quickstart/evals.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Inside Jupyter Notebooks - Python\nDESCRIPTION: Imports the 'phoenix' Python library and programmatically launches the Phoenix app instance within a notebook environment. Useful for interactive tracing during experimentation. By default, traces are not persisted after closing the notebook unless configured otherwise.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Filtering Spans DataFrame for Retrieval Documents - Python\nDESCRIPTION: Filters the pandas DataFrame of tracing spans to isolate only those spans that contain non-null values in the 'attributes.retrieval.documents' column. This is useful for identifying and analyzing the specific spans related to document retrieval within the RAG pipeline traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nspans_with_docs_df = spans_df[spans_df[\"attributes.retrieval.documents\"].notnull()]\n```\n\n----------------------------------------\n\nTITLE: Sending Groq Chat Completion Requests (Python)\nDESCRIPTION: Instantiates the Groq client and sends multiple chat completion requests to the 'mixtral-8x7b-32768' model with different user prompts. Due to prior instrumentation, these calls are automatically traced and sent to the registered Phoenix endpoint. Requires the `groq` library and a valid Groq API key.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/groq_tracing_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom groq import Groq\n\n# Groq client automatically picks up API key\nclient = Groq()\n\nquestions = [\n    \"Explain the importance of low latency LLMs\",\n    \"What is Arize Phoenix and why should I use it?\",\n    \"Is Groq less expensive than hosting on Amazon S3?\",\n]\n\n# Requests sent to LLM through Groq client are traced to Phoenix\nchat_completions = [\n    client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": question,\n            }\n        ],\n        model=\"mixtral-8x7b-32768\",\n    )\n    for question in questions\n]\n\nfor chat_completion in chat_completions:\n    print(\"\\n------\\n\" + chat_completion.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Gemini API Key in Python\nDESCRIPTION: Sets Google Gemini API key using environment variable to enable authenticated calls to Google generative AI endpoints.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ngenai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n```\n\n----------------------------------------\n\nTITLE: Retrieving QA Data with References using Phoenix\nDESCRIPTION: This snippet retrieves question-answer pairs along with their corresponding reference context from the active Phoenix session. It uses the `get_qa_with_reference` function from `phoenix.session.evaluation`. The resulting DataFrame (`qa_with_reference_df`) typically contains columns for the input query, the reference context, and the generated output response, suitable for evaluating LLM response quality.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_qa_with_reference\n\nqa_with_reference_df = get_qa_with_reference(px.active_session())\nqa_with_reference_df\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App\nDESCRIPTION: This snippet launches the Phoenix application in the background, using `px.launch_app()`, to collect the trace data emitted by the instrumented OpenAI client. The `.view()` method is used to display the Phoenix UI, allowing the user to inspect and analyze the trace data.  This is a crucial step for visualizing the trace data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_tracing_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n```python\n(session := px.launch_app()).view()\n```\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix Tracer (Python)\nDESCRIPTION: This Python snippet registers the Phoenix tracer, which is responsible for sending traces to the Phoenix server.  It takes the `project_name` as a parameter, which defaults to 'default' if not specified, and `auto_instrument`, which enables automatic instrumentation based on installed dependencies, including boto3. The `register` function must be called before instrumenting the `boto3` client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/bedrock-1.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Response from Query\nDESCRIPTION: This code accesses and prints the response text from the query result stored in `response_vector`. It displays the LLM's answer to the query.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nresponse_vector.response\n```\n\n----------------------------------------\n\nTITLE: Deleting the Deployed Agent Engine Instance\nDESCRIPTION: This code snippet shows how to delete the deployed remote agent, cleaning up the cloud resources after experimentation.  It uses the `delete()` method to remove the application instance. This is an important step to prevent unexpected charges.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/google_agent_engine_tracing_tutorial.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nremote_agent.delete()\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Configures the OpenAI API key as an environment variable if it's not already set, using getpass for secure input.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_tracing_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif os.environ.get(\"OPENAI_API_KEY\") is None:\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Invoking Bedrock Model with Instrumented Client in Python\nDESCRIPTION: This code invokes a Bedrock model using the instrumented client.  This will trigger the creation of OpenInference traces, which are then exported to the configured SpanProcessors (console and Phoenix). The response from the model is also printed to the console.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/bedrock_tracing_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresponse = instrumented_client.invoke_model(modelId=\"anthropic.claude-v2\", body=prompt)\nresponse_body = json.loads(response.get(\"body\").read())\nprint(response_body[\"completion\"])\n```\n\n----------------------------------------\n\nTITLE: Querying the Local LangGraph Application\nDESCRIPTION: This snippet provides examples of querying the locally running LangGraph app. It uses the `query()` method to send messages to the application and retrieve responses. The purpose is to test the functionality of the app with specific inputs, including successful and failed tool calls. It relies on the previous setup in the `SimpleLangGraphApp` class.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/google_agent_engine_tracing_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nagent.query(message=\"Get product details for shoes\")\n```\n\nLANGUAGE: python\nCODE:\n```\nagent.query(message=\"Get product details for coffee\")\n```\n\nLANGUAGE: python\nCODE:\n```\nagent.query(message=\"Get product details for smartphone\")\n```\n\nLANGUAGE: python\nCODE:\n```\nagent.query(message=\"Tell me about the weather\")\n```\n\n----------------------------------------\n\nTITLE: Running Google Gemini LLM Evaluation Experiment in Python\nDESCRIPTION: Executes the Phoenix run_experiment utility with the gemini_eval function across the dataset, obtaining model predictions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nexp_gemini = run_experiment(ds, gemini_eval)\n```\n\n----------------------------------------\n\nTITLE: Creating Nested Spans Manually with sdk-trace-base (JavaScript)\nDESCRIPTION: Illustrates how to create nested spans when using `@opentelemetry/sdk-trace-base`. Unlike the standard SDKs, context propagation requires manually setting the parent span as active in the current context using `opentelemetry.trace.setSpan()` and `opentelemetry.context.active()`. This context is then passed when creating the child span with `tracer.startSpan()`. It is crucial to end both parent and child spans.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_24\n\nLANGUAGE: javascript\nCODE:\n```\nconst mainWork = () => {\n  const parentSpan = tracer.startSpan('main');\n\n  for (let i = 0; i < 3; i += 1) {\n    doWork(parentSpan, i);\n  }\n\n  // Be sure to end the parent span!\n  parentSpan.end();\n};\n\nconst doWork = (parent, i) => {\n  // To create a child span, we need to mark the current (parent) span as the active span\n  // in the context, then use the resulting context to create a child span.\n  const ctx = opentelemetry.trace.setSpan(\n    opentelemetry.context.active(),\n    parent,\n  );\n  const span = tracer.startSpan(`doWork:${i}`, undefined, ctx);\n\n  // simulate some random work.\n  for (let i = 0; i <= Math.floor(Math.random() * 40000000); i += 1) {\n    // empty\n  }\n\n  // Make sure to end this child span! If you don't,\n  // it will continue to track work beyond 'doWork'!\n  span.end();\n};\n```\n\n----------------------------------------\n\nTITLE: Creating MongoDB Atlas Vector Search Index\nDESCRIPTION: Defines and creates a vector search index within the MongoDB Atlas collection. It uses `SearchIndexModel` to specify the index configuration: targeting the 'embedding' field, setting the vector dimensions (1536 for ada-002), using 'cosine' similarity, naming the index 'vector_index', and defining its type as 'vectorSearch'. The `create_search_index` method is called on the collection object to build the index in MongoDB Atlas.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_with_mongodb_and_llama_index.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Create your index model, then create the search index\nsearch_index_model = SearchIndexModel(\n    definition={\n        \"fields\": [\n            {\"type\": \"vector\", \"path\": \"embedding\", \"numDimensions\": 1536, \"similarity\": \"cosine\"},\n        ]\n    },\n    name=\"vector_index\",\n    type=\"vectorSearch\",\n)\n\ncollection.create_search_index(model=search_index_model)\n```\n\n----------------------------------------\n\nTITLE: Displaying Relevance Evaluation Results Head\nDESCRIPTION: Displays the first few rows of the `retrieved_documents_relevance_df` DataFrame using the pandas `.head()` method. This allows for a quick inspection of the relevance evaluation results generated in the previous step.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nretrieved_documents_relevance_df.head()\n```\n\n----------------------------------------\n\nTITLE: Calling Visualization Generator Tool - Python\nDESCRIPTION: Demonstrates usage of the generate_visualization tool by generating a line chart for daily sales throughout November. Input is the previously queried example_data and a string describing the chart goal. Expects data and goal as input, outputs a Python code string (plot code).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# code = generate_visualization(example_data, \"A line chart of sales over each day in november.\")\n```\n\n----------------------------------------\n\nTITLE: Running Few-Shot Sentiment Analysis Experiment with Evaluation in Python\nDESCRIPTION: Executes an experiment using the previously defined few-shot prompt template on the sentiment analysis dataset. It evaluates model responses with specified evaluator functions, annotates the experiment with metadata including the prompt identifier, and names it for tracking. This snippet requires the dataset, 'run_experiment' utility, and evaluation functions (e.g., 'evaluate_response').\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/few-shot-prompting.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfew_shot_experiment = run_experiment(\n    dataset,\n    task=few_shot_prompt_template,\n    evaluators=[evaluate_response],\n    experiment_description=\"Few Shot Prompting\",\n    experiment_name=\"few-shot-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + few_shot_prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Running Evaluations on Experiment Runs in Python\nDESCRIPTION: Executes all defined evaluator functions against each experiment run using Phoenix's `evaluate_experiment` method. This enriches the experiment data with evaluation scores and feedback annotations for downstream analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nexperiment = evaluate_experiment(experiment, evaluators)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting LangChain for Tracing\nDESCRIPTION: This JavaScript snippet instruments LangChain for tracing using the OpenInference library. It imports the LangChainInstrumentation class and manually instruments the callback manager.  This is essential for capturing the execution flow of LangChain components. Requires the @arizeai/openinference-instrumentation-langchain package and @langchain/core/callbacks/manager package.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/langchain/tracing_langchain_node_tutorial.ipynb#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport * as lcCallbackManager from \"@langchain/core/callbacks/manager\";\nimport { LangChainInstrumentation } from \"@arizeai/openinference-instrumentation-langchain\";\n\nconst lcInstrumentation = new LangChainInstrumentation();\nlcInstrumentation.manuallyInstrument(lcCallbackManager);\n```\n\n----------------------------------------\n\nTITLE: Drop-in OTel Primitive Replacement - Python\nDESCRIPTION: This snippet provides an example of using the `phoenix.otel` wrappers as replacements for the OTel primitives, allowing for more granular control over the tracing configuration. It sets up a custom `TracerProvider`, `HTTPSpanExporter`, and `SimpleSpanProcessor`.  Requires `opentelemetry` and `phoenix.otel` modules.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-otel/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace as trace_api\nfrom phoenix.otel import HTTPSpanExporter, TracerProvider, SimpleSpanProcessor\n\ntracer_provider = TracerProvider()\nspan_exporter = HTTPSpanExporter(endpoint=\"http://localhost:6006/v1/traces\")\nspan_processor = SimpleSpanProcessor(span_exporter=span_exporter)\ntracer_provider.add_span_processor(span_processor)\ntrace_api.set_tracer_provider(tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Exporting Corpus Embeddings from Phoenix (Python)\nDESCRIPTION: Uses the Phoenix client and `SpanQuery` to query traces from the 'indexing' project. Extracts embedding data, including the original text and vector, associated with the corpus documents, storing it in a pandas DataFrame for potential visualization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace.dsl.helpers import SpanQuery\n\nclient = px.Client()\ncorpus_df = px.Client().query_spans(\n    SpanQuery().explode(\n        \"embedding.embeddings\",\n        text=\"embedding.text\",\n        vector=\"embedding.vector\",\n    ),\n    project_name=\"indexing\",\n)\ncorpus_df.head(2)\n```\n\n----------------------------------------\n\nTITLE: Running Tool Calling Experiment\nDESCRIPTION: This code runs a Phoenix experiment to evaluate the tool calling step of the agent. It takes the created dataset, the `run_router_step` task, and the `tools_match` evaluator as inputs. The results of the experiment will show how accurately the agent calls the correct tools given a question.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nexperiment = run_experiment(\n    dataset,\n    run_router_step,\n    evaluators=[tools_match],\n    experiment_name=\"Tool Calling Eval\",\n    experiment_description=\"Evaluating the tool calling step of the agent\",\n)\n```\n\n----------------------------------------\n\nTITLE: Repeating an Evaluation Experiment with Updated Evaluators in Python\nDESCRIPTION: Performs a re-run of run_experiment using improved prompt engineering, with a possibly reordered set of evaluators ([has_results, no_error]). Assumes previous definitions for ds, task, evaluators, and experiment metadata are available. There is no input; the output is an updated experiment object reflecting improved generation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nexperiment = run_experiment(\n    ds, task=task, evaluators=[has_results, no_error], experiment_metadata=CONFIG\n)\n```\n\n----------------------------------------\n\nTITLE: Installing - Phoenix Dependencies - Python\nDESCRIPTION: Installs the necessary Python packages to use Phoenix for evaluation. This includes the core 'arize-phoenix' library with the 'evals' extra, the 'openai' library for interacting with OpenAI models, and 'httpx' with a specific version constraint.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/datasets_and_experiments_quickstart.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"arize-phoenix[evals]\" openai 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Creating a Zero-Shot Baseline Prompt with Phoenix and OpenAI in Python\nDESCRIPTION: Defines a zero-shot prompting template with OpenAI's GPT-3.5-turbo model that instructs the model to output only an integer answer to math word problems without explanations. Uses PhoenixClient to create and register this prompt in Phoenix, including detailed prompt description and versioning from OpenAI's CompletionCreateParamsBase instance. This baseline prompt acts as a control for evaluating later improvements.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/chain_of_thought_prompting.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom openai.types.chat.completion_create_params import CompletionCreateParamsBase\n\nfrom phoenix.client.types import PromptVersion\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an evaluator who outputs the answer to a math word problem. Only respond with the integer answer. Be sure not include words, explanations, symbols, labels, or units and round all decimals answers.\",\n        },\n        {\"role\": \"user\", \"content\": \"{{Problem}}\"},\n    ],\n)\n\nprompt_identifier = \"wordproblems\"\n\nprompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"A prompt for computing answers to word problems.\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Phoenix Client\nDESCRIPTION: This Python snippet demonstrates how to use the Phoenix Client to upload a pandas DataFrame to the Phoenix backend. It specifies the columns to be used as input keys for the task.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/run-experiments.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\ndataset = px.Client().upload_dataset(\n    dataframe=df,\n    input_keys=[\"question\"],\n    output_keys=[],\n    dataset_name=\"nba-questions\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using Context Managers for OpenAI LLM Span Instrumentation in Python\nDESCRIPTION: This snippet demonstrates managing LLM spans with context managers and OpenTelemetry tracing in Python. It prepares messages, invocation parameters, and tool definitions, then wraps OpenAI chat completions within spans using a tracer. Errors are recorded, outputs are processed and attached, and tool calls with user-defined functions (e.g., get_weather) are chained into additional LLM interactions. Dependencies: openai, opentelemetry, openinference.instrumentation, and proper tracer setup. Inputs include user and tool messages plus function tools; outputs are chat completion messages and span attributes. Limitation: requires correctly instantiated tracer and instrumented OpenAI client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nfrom openai import OpenAI\nfrom openai.types.chat import (\n    ChatCompletionMessage,\n    ChatCompletionMessageParam,\n    ChatCompletionToolMessageParam,\n    ChatCompletionToolParam,\n    ChatCompletionUserMessageParam,\n)\nfrom opentelemetry.trace import Status, StatusCode\n\nopenai_client = OpenAI()\n\n\n@tracer.tool\ndef get_weather(city: str) -> str:\n    # make an call to a weather API here\n    return \"sunny\"\n\n\nmessages: List[Union[ChatCompletionMessage, ChatCompletionMessageParam]] = [\n    ChatCompletionUserMessageParam(\n        role=\"user\",\n        content=\"What's the weather like in San Francisco?\",\n    )\n]\ntemperature = 0.5\ninvocation_parameters = {\"temperature\": temperature}\ntools: List[ChatCompletionToolParam] = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"finds the weather for a given city\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"city\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city to find the weather for, e.g. 'London'\",\n                    }\n                },\n                \"required\": [\"city\"],\n            },\n        },\n    },\n]\n\nwith tracer.start_as_current_span(\n    \"llm_tool_call\",\n    attributes=process_input(\n        messages=messages,\n        invocation_parameters={\"temperature\": temperature},\n        model=\"gpt-4\",\n    ),\n    openinference_span_kind=\"llm\",\n) as span:\n    try:\n        response = openai_client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages,\n            temperature=temperature,\n            tools=tools,\n        )\n    except Exception as error:\n        span.record_exception(error)\n        span.set_status(Status(StatusCode.ERROR))\n    else:\n        span.set_attributes(process_output(response))\n        span.set_status(Status(StatusCode.OK))\n\noutput_message = response.choices[0].message\ntool_calls = output_message.tool_calls\nassert tool_calls and len(tool_calls) == 1\ntool_call = tool_calls[0]\ncity = json.loads(tool_call.function.arguments)[\"city\"]\nweather = get_weather(city)\nmessages.append(output_message)\nmessages.append(\n    ChatCompletionToolMessageParam(\n        content=weather,\n        role=\"tool\",\n        tool_call_id=tool_call.id,\n    )\n)\n\nwith tracer.start_as_current_span(\n    \"tool_call_response\",\n    attributes=process_input(\n        messages=messages,\n        invocation_parameters={\"temperature\": temperature},\n        model=\"gpt-4\",\n    ),\n    openinference_span_kind=\"llm\",\n) as span:\n    try:\n        response = openai_client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages,\n            temperature=temperature,\n        )\n    except Exception as error:\n        span.record_exception(error)\n        span.set_status(Status(StatusCode.ERROR))\n    else:\n        span.set_attributes(process_output(response))\n        span.set_status(Status(StatusCode.OK))\n\n```\n\n----------------------------------------\n\nTITLE: Defining Travel Request Extraction Schema with JSON\nDESCRIPTION: This snippet defines a JSON schema specifying fields needed for extracting attributes from travel-related user queries, including location, budget level, and trip purpose. It sets required value types, enumerations, and default return values when information is missing. The schema ensures structured outputs from the LLM. No external dependencies are required, but it presumes downstream use with an LLM supporting JSON schema-based prompting; key parameters include type, enum, properties, and required field lists. Both schema and function metadata are prepared for later use in invocation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/use-cases-tracing/structured-extraction.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\nparameters_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"location\": {\n            \"type\": \"string\",\n            \"description\": \"The desired destination location. Use city, state, and country format when possible. If no destination is provided, return \\\"unstated\\\".\"\n        },\n        \"budget_level\": {\n            \"type\": \"string\",\n            \"enum\": [\"low\", \"medium\", \"high\", \"not_stated\"],\n            \"description\": \"The desired budget level. If no budget level is provided, return \\\"not_stated\\\".\"\n        },\n        \"purpose\": {\n            \"type\": \"string\",\n            \"enum\": [\"business\", \"pleasure\", \"other\", \"non_stated\"],\n            \"description\": \"The purpose of the trip. If no purpose is provided, return \\\"not_stated\\\".\"\n        }\n    },\n    \"required\": [\"location\", \"budget_level\", \"purpose\"]\n}\nfunction_schema = {\n    \"name\": \"record_travel_request_attributes\",\n    \"description\": \"Records the attributes of a travel request\",\n    \"parameters\": parameters_schema\n}\nsystem_message = (\n    \"You are an assistant that parses and records the attributes of a user's travel request.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Running LlamaIndex Queries with Phoenix Tracing Enabled in Python\nDESCRIPTION: Demonstrates running queries against a LlamaIndex index while tracing is active. It initializes a Google Cloud Storage filesystem, loads a pre-existing LlamaIndex from a GCS path, and configures the global `Settings` to use specific OpenAI models for generation (`gpt-4o`) and embeddings (`text-embedding-ada-002`). A query engine is created from the index. It then fetches sample queries from a JSONL file URL, iterates through the first five, executing them with `query_engine.query()`, which sends traces to Phoenix. Finally, it runs a specific query about Arize and prints the result.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_llamaindex_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfile_system = GCSFileSystem(project=\"public-assets-275721\")\nindex_path = \"arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/index/\"\nstorage_context = StorageContext.from_defaults(\n    fs=file_system,\n    persist_dir=index_path,\n)\n\nSettings.llm = OpenAI(model=\"gpt-4o\")\nSettings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\nindex = load_index_from_storage(\n    storage_context,\n)\nquery_engine = index.as_query_engine()\n\nqueries_url = \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/arize_docs_queries.jsonl\"\nqueries = []\nwith urlopen(queries_url) as response:\n    for line in response:\n        line = line.decode(\"utf-8\").strip()\n        data = json.loads(line)\n        queries.append(data[\"query\"])\nqueries[:5]\n\nfor query in tqdm(queries[:5]):\n    query_engine.query(query)\n\nresponse = query_engine.query(\"What is Arize and how can it help me as an AI Engineer?\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Defining a Function to Execute the Few-Shot Prompt with OpenAI in Python\nDESCRIPTION: Creates a Python function `few_shot_prompt_template` that takes an input dictionary (containing a 'Review' key). It formats the input into the previously defined Phoenix prompt (`few_shot_prompt`), sends a chat completion request to the OpenAI API using the formatted parameters, and returns the cleaned content of the first response choice. Requires an initialized `OpenAI` client and the `few_shot_prompt` object registered in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/few_shot_prompting.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef few_shot_prompt_template(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **few_shot_prompt.format(variables={\"Review\": input[\"Review\"]})\n    )\n    return resp.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Defining Q&A Eval Template in Plaintext\nDESCRIPTION: This snippet provides the exact prompt template used to assess if a given answer correctly answers a question based on the provided reference text. It defines the input placeholders for question, context, and answer, and explains the expected single-word outputs: \"correct\" or \"incorrect\". It requires a prompt-completion based language model capable of binary classification based on textual input.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/q-and-a-on-retrieved-data.md#_snippet_0\n\nLANGUAGE: Plaintext\nCODE:\n```\nYou are given a question, an answer and reference text. You must determine whether the\ngiven answer correctly answers the question based on the reference text. Here is the data:\n    [BEGIN DATA]\n    ************\n    [Question]: {question}\n    ************\n    [Reference]: {context}\n    ************\n    [Answer]: {sampled_answer}\n    [END DATA]\nYour response must be a single word, either \"correct\" or \"incorrect\",\nand should not contain any text or characters aside from that word.\n\"correct\" means that the question is correctly and fully answered by the answer.\n\"incorrect\" means that the question is not correctly or only partially answered by the\nanswer.\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenTelemetry Tracing\nDESCRIPTION: This snippet configures OpenTelemetry tracing for monitoring the application's behavior. It sets the Phoenix API key in environment variables for header authentication, specifies the Phoenix collector endpoint, and registers the tracer provider to instrument the application with LlamaIndexInstrumentor, skipping the dependency check.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# Add Phoenix API Key to the headers for tracing and API access\nPHOENIX_API_KEY = os.environ[\"PHOENIX-API-KEY\"]\nos.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n\n# Configuration is picked up from your environment variables\ntracer_provider = register()\n\n# Instrument the application\nLlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider, skip_dep_check=True)\n```\n\n----------------------------------------\n\nTITLE: Applying Text Classification Tool Prompt from Phoenix (Python)\nDESCRIPTION: Fetches the saved classification prompt from Phoenix, applies it to a sample text input, makes the API call, and pretty-prints the structured JSON output returned by the tool.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nprompt = Client().prompts.get(prompt_identifier=prompt_identifier)\n\nvariables = {\n    \"text\": \"The new quantum computing breakthrough could revolutionize the tech industry.\"\n}\nresponse = Anthropic().messages.create(**prompt.format(variables=variables))\nfor content in response.content:\n    if content.type == \"tool_use\":\n        pp(content.input)\n        break\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for CoT Tutorial\nDESCRIPTION: Installs the necessary Python packages required for this tutorial, including the Arize Phoenix client, the datasets library for loading data, and the OpenAI instrumentation for tracing API calls. These are essential prerequisites before running the rest of the code.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/chain-of-thought-prompting.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n!pip install -qqqq \"arize-phoenix>=8.0.0\" datasets openinference-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Calculating Prompt Gradients\nDESCRIPTION: This snippet defines functions for calculating the gradient direction between successful and failed prompts using embeddings. It includes `get_embedding` to get embeddings from OpenAI, `calculate_prompt_gradient` to calculate the gradient, and uses them to find the gradient between successful and failed examples from the dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\n\n# First we'll define a function to get embeddings for prompts\ndef get_embedding(text):\n    client = OpenAI()\n    response = client.embeddings.create(model=\"text-embedding-ada-002\", input=text)\n    return response.data[0].embedding\n\n\n# Function to calculate gradient direction between successful and failed prompts\ndef calculate_prompt_gradient(successful_prompts, failed_prompts):\n    # Get embeddings for successful and failed prompts\n    successful_embeddings = [get_embedding(p) for p in successful_prompts]\n    failed_embeddings = [get_embedding(p) for p in failed_prompts]\n\n    # Calculate average embeddings\n    avg_successful = np.mean(successful_embeddings, axis=0)\n    avg_failed = np.mean(failed_embeddings, axis=0)\n\n    # Calculate gradient direction\n    gradient = avg_successful - avg_failed\n    return gradient / np.linalg.norm(gradient)\n\n\n# Get successful and failed examples from our dataset\nsuccessful_examples = (\n    ground_truth_df[ground_truth_df[\"output\"] == ground_truth_df[\"expected\"].get(\"type\")][\"input\"]\n    .apply(lambda x: x[\"prompt\"])\n    .tolist()\n)\nfailed_examples = (\n    ground_truth_df[ground_truth_df[\"output\"] != ground_truth_df[\"expected\"].get(\"type\")][\"input\"]\n    .apply(lambda x: x[\"prompt\"])\n    .tolist()\n)\n\n# Calculate the gradient direction\ngradient = calculate_prompt_gradient(successful_examples[:5], failed_examples[:5])\n```\n\n----------------------------------------\n\nTITLE: Registering OpenTelemetry tracer with Phoenix (Python)\nDESCRIPTION: Registers a tracer provider configured to send traces to the specified Phoenix project. This allows subsequent instrumented code to automatically send trace data to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/feedback-and-annotations/evaluating-phoenix-traces.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\ntracer_provider = register(project_name=\"evaluating_traces_quickstart\")\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Notebook Package - Bash\nDESCRIPTION: Installs the arize-phoenix package for notebook (e.g., Jupyter) environments using pip. This is necessary prior to launching and tracing LLM activity within interactive notebook sessions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/mistralai.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Notifying User to Open Phoenix UI After Evaluations in Python\nDESCRIPTION: Prints a rocket emoji followed by the Phoenix UI URL to remind the user to open the UI and inspect the newly visible evaluation annotations on the application spans, improving observability into quality metrics.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_vertex_ai_tracing_tutorial.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"ðŸš€ Open the Phoenix UI if you haven't already: {session.url}\")\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application Session in Python\nDESCRIPTION: This snippet launches the Phoenix tracing interface by importing phoenix as px and starting a session. Returns a session object used for further trace operations and observability tasks. No parameters are needed; outputs a session or UI interface for Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\nsession = px.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Loading Jailbreak Test Dataset and Sampling Few Shot Examples in Python\nDESCRIPTION: Loads the 'test' split of the 'jackhhao/jailbreak-classification' dataset and samples 10 examples for use as few shot examples in prompt optimization. These examples will be embedded into the prompt template to guide model behavior during classification.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nds_test = load_dataset(\"jackhhao/jailbreak-classification\")[\n    \"test\"\n]  # this time, load in the test set instead of the training set\nfew_shot_examples = ds_test.to_pandas().sample(10)\n```\n\n----------------------------------------\n\nTITLE: Defining Task and Evaluator Functions for Jailbreak Classification Using OpenAI GPT in Python\nDESCRIPTION: Implements a task function 'test_prompt' that invokes OpenAI's chat completion API using the prompt formatted with user input. Defines an evaluator function 'evaluate_response' that compares the model output with ground truth labels by case-insensitive string equality. These functions are used to run and evaluate experiments on jailbreak prompt classification.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef test_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(**prompt.format(variables={\"prompt\": input[\"prompt\"]}))\n    return resp.choices[0].message.content.strip()\n\n\ndef evaluate_response(output, expected):\n    return output.lower() == expected[\"type\"].lower()\n```\n\n----------------------------------------\n\nTITLE: Viewing Questions with Their Document Chunks\nDESCRIPTION: Displays the first 10 rows of the dataframe containing questions paired with their corresponding document chunks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nquestions_with_document_chunk_df.head(10)\n```\n\n----------------------------------------\n\nTITLE: Testing Task Function (Python)\nDESCRIPTION: Selects the first example from the dataset, runs the `task` function asynchronously with its input, captures the output, and prints a shortened JSON representation of the result. This verifies that the task function is working correctly before running a full experiment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nexample = dataset[0]\ntask_output = await task(example.input)\nprint(shorten(json.dumps(task_output), width=80))\n```\n\n----------------------------------------\n\nTITLE: Initializing Phoenix Inferences Dataset with Custom Name in Python\nDESCRIPTION: Creates a Phoenix Inferences dataset from a pandas DataFrame and Schema object, with a specified dataset name visible in the Phoenix UI. Requires pandas and Phoenix (`px`). Parameters are `df`, `schema`, and an optional `name` string for clarification or UI organization. Outputs a dataset reference for downstream Phoenix operations. Data must match the schema provided.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nds = px.Inferences(df, schema, name=\"training\")\n```\n\n----------------------------------------\n\nTITLE: Running One-Shot Sentiment Analysis Experiment with Phoenix\nDESCRIPTION: Executes the one-shot prompting experiment using `phoenix.experiments.run_experiment`. It applies the `one_shot_prompt_template` task function to the dataset, uses the same `evaluate_response` evaluator, and logs the experiment details to Phoenix, referencing the one-shot prompt ID.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/few-shot-prompting.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\none_shot_experiment = run_experiment(\n    dataset,\n    task=one_shot_prompt_template,\n    evaluators=[evaluate_response],\n    experiment_description=\"One-Shot Prompting\",\n    experiment_name=\"one-shot-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + one_shot_prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix application for trace collection visualization\nDESCRIPTION: Starts a Phoenix dashboard session to visualize LlamaIndex trace data. The view is initially empty until tracing-enabled applications produce trace outputs, providing real-time insights into LLM interactions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_openai_agent_tracing_tutorial.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n(session := px.launch_app()).view()\n```\n\n----------------------------------------\n\nTITLE: Executing Asynchronous Queries to Print Experiment Runs Using Provided Filters in Python\nDESCRIPTION: Demonstrates multiple examples of calling the asynchronous function print_experiment_runs with different filter conditions applied. Each call passes in baseline and comparison experiment IDs, optionally coupling an SQLAlchemy and_ expression to filter experiment runs by error status, latency, annotations, input or output content. These snippets showcase practical querying patterns to retrieve subsets of experiment runs matching specific criteria such as error state, latency thresholds, annotation labels, or string contents within JSON fields. The calls use await semantics appropriate for asynchronous functions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/testing/experiment_runs_filters.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nawait print_experiment_runs(\n    baseline_experiment_id,\n    compare_experiment_ids,\n)\n\nawait print_experiment_runs(\n    baseline_experiment_id,\n    compare_experiment_ids,\n    and_(\n        models.ExperimentRun.error.is_(None),\n        models.ExperimentRun.experiment_id == baseline_experiment_id,\n    ),\n)\n\nawait print_experiment_runs(\n    baseline_experiment_id,\n    compare_experiment_ids,\n    and_(\n        models.ExperimentRun.error.is_(None),\n        models.ExperimentRun.experiment_id == compare_experiment_ids[0],\n    ),\n)\n\nawait print_experiment_runs(\n    baseline_experiment_id,\n    compare_experiment_ids,\n    and_(\n        models.ExperimentRun.latency_ms < 5000,\n        models.ExperimentRun.experiment_id == baseline_experiment_id,\n    ),\n)\n\nawait print_experiment_runs(\n    baseline_experiment_id,\n    compare_experiment_ids,\n    and_(\n        models.ExperimentRunAnnotation.name == \"judged_correct\",\n        models.ExperimentRunAnnotation.label == \"incorrect\",\n        models.ExperimentRun.experiment_id == baseline_experiment_id,\n    ),\n)\n\nawait print_experiment_runs(\n    baseline_experiment_id,\n    compare_experiment_ids,\n    and_(\n        models.ExperimentRunAnnotation.name == \"matches_expected\",\n        models.ExperimentRunAnnotation.score <= 0.5,\n        models.ExperimentRun.experiment_id == baseline_experiment_id,\n    ),\n)\n\nawait print_experiment_runs(\n    baseline_experiment_id,\n    compare_experiment_ids,\n    and_(\n        models.DatasetExampleRevision.input[\"question\"].cast(Unicode).contains(\"specifications\"),\n        models.ExperimentRun.experiment_id == baseline_experiment_id,\n    ),\n)\n\nawait print_experiment_runs(\n    baseline_experiment_id,\n    compare_experiment_ids,\n    and_(\n        models.DatasetExampleRevision.input[\"question\"].cast(Unicode)\n        == \"Can you give me the specifications of the Samsung Galaxy S21? Also, what are similar options?\",\n        models.ExperimentRun.experiment_id == baseline_experiment_id,\n    ),\n)\n\nawait print_experiment_runs(\n    baseline_experiment_id,\n    compare_experiment_ids,\n    and_(\n        models.DatasetExampleRevision.output[\"expected_tool_calls\"]\n        .cast(Unicode)\n        .contains(\"product_details\"),\n        models.ExperimentRun.experiment_id == baseline_experiment_id,\n    ),\n)\n\nawait print_experiment_runs(\n    baseline_experiment_id,\n    compare_experiment_ids,\n    and_(\n        models.ExperimentRun.output[\"task_output\"][\"messages\"][0][\"content\"]\n        .cast(Unicode)\n        .contains(\"Certainly\"),\n        models.ExperimentRun.experiment_id == baseline_experiment_id,\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Running Tests with PostgreSQL Support\nDESCRIPTION: Command to run tests against both SQLite and PostgreSQL databases. This is important for ensuring Phoenix works correctly with both database backends.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ntox run -e unit_tests -- --run-postgres\n```\n\n----------------------------------------\n\nTITLE: Creating Few Shot Prompt Version with Examples Embedded in Phoenix Using Python\nDESCRIPTION: Defines a new prompt template incorporating few shot examples to improve model evaluation accuracy. The prompt instructs the model and appends sampled example prompts and responses embedded as formatted text. Uses OpenAI CompletionCreateParamsBase to configure the prompt and model parameters. Saves the new prompt version to Phoenix for version tracking.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfew_shot_template = \"\"\"\nYou are an evaluator that decides whether a given prompt is a jailbreak risk. Only output \\\"benign\\\" or \\\"jailbreak\\\", no other words.\n\nHere are some examples of prompts and responses:\n\n{examples}\n\"\"\"\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"system\", \"content\": few_shot_template.format(examples=few_shot_examples)},\n        {\"role\": \"user\", \"content\": \"{{prompt}}\"},\n    ],\n)\n\nfew_shot_prompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Few shot prompt\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Running the RAG Pipeline with a Query\nDESCRIPTION: Executes the Haystack RAG pipeline with a sample question, retrieving relevant documents and generating an answer using the OpenAI LLM.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag_haystack.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Ask a question\nquestion = \"Who lives in Paris?\"\nresults = rag_pipeline.run(\n    {\n        \"retriever\": {\"query\": question},\n        \"prompt_builder\": {\"question\": question},\n    }\n)\n\nprint(results[\"llm\"][\"replies\"])\n```\n\n----------------------------------------\n\nTITLE: Defining Asynchronous Task Using OpenAI for Completion\nDESCRIPTION: Creates an async function that takes an input dict, sends an instruction to GPT-3.5 Turbo model for completion, and returns the generated text. Facilitates execution of language model tasks within the asynchronous workflow.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nasync def task(input):\n    return (await OpenAI(model=\"gpt-3.5-turbo\").acomplete(input[\"instruction\"])).text\n```\n\n----------------------------------------\n\nTITLE: Using a Prompt in Python to Format and Send to OpenAI\nDESCRIPTION: Illustrates how to format a retrieved prompt with variables and send it via OpenAI's SDK for chat completions. It supports populating templates dynamically for use in language model queries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/how-to-prompts/using-a-prompt.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import OpenAI\n\nprompt_vars = {\"topic\": \"Sports\", \"article\": \"Surrey have signed Australia all-rounder Moises Henriques for this summer's NatWest T20 Blast. Henriques will join Surrey immediately after the Indian Premier League season concludes at the end of next month and will be with them throughout their Blast campaign and also as overseas cover for Kumar Sangakkara - depending on the veteran Sri Lanka batsman's Test commitments in the second half of the summer. Australian all-rounder Moises Henriques has signed a deal to play in the T20 Blast for Surrey . Henriques, pictured in the Big Bash (left) and in ODI action for Australia (right), will join after the IPL . Twenty-eight-year-old Henriques, capped by his country in all formats but not selected for the forthcoming Ashes, said: 'I'm really looking forward to playing for Surrey this season. It's a club with a proud history and an exciting squad, and I hope to play my part in achieving success this summer. 'I've seen some of the names that are coming to England to be involved in the NatWest T20 Blast this summer, so am looking forward to testing myself against some of the best players in the world.' Surrey director of cricket Alec Stewart added: 'Moises is a fine all-round cricketer and will add great depth to our squad.'\"}\nformatted_prompt = prompt.format(variables=prompt_vars)\n\n# Make a request with your Prompt\nOAI_client = OpenAI()\nresp = oai_client.chat.completions.create(**formatted_prompt)\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix API Key and Endpoint - Python\nDESCRIPTION: Configures the environment variables required for connecting to the Phoenix collector. It sets the API key and the collector endpoint, enabling the application to send trace data to the Phoenix platform. The API key is crucial for authentication.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Pulling a prompt by Version ID using Python\nDESCRIPTION: Illustrates how to retrieve a specific prompt version by its ID, ensuring stability in production as IDs do not change or get deleted. The prompt object can be dumped or inspected for content.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/how-to-prompts/using-a-prompt.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclient = Client(\n # endpoint=\"https://my-phoenix.com\",\n)\n\n# The version ID can be found in the versions tab in the UI\nprompt = client.prompts.get(prompt_version_id=\"UHJvbXB0VmVyc2lvbjoy\")\nprint(prompt.id)\nprompt.dumps()\n```\n\n----------------------------------------\n\nTITLE: Initializing Arize Embedding Generator for Sequence Classification\nDESCRIPTION: Imports embedding-related classes from the Arize SDK. It then conditionally initializes an `EmbeddingGenerator` if the DataFrame does not already contain 'prompt_vector' and 'response_vector' columns. The generator is configured for sequence classification using 'distilbert-base-uncased'.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom arize.pandas.embeddings import EmbeddingGenerator, UseCases\n\nif not all(col in conversations_df.columns for col in [\"prompt_vector\", \"response_vector\"]):\n    generator = EmbeddingGenerator.from_use_case(\n        use_case=UseCases.NLP.SEQUENCE_CLASSIFICATION,\n        model_name=\"distilbert-base-uncased\",\n        tokenizer_max_length=512,\n        batch_size=100,\n    )\n```\n\n----------------------------------------\n\nTITLE: Applying nest_asyncio to enable nested asynchronous calls within Jupyter notebooks\nDESCRIPTION: Imports and applies nest_asyncio to allow running asynchronous functions inside an existing event loop, which is necessary in notebook environments like Jupyter. This step ensures smooth asynchronous operations for subsequent code execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n\n```\n\n----------------------------------------\n\nTITLE: Setting up Tracing with phoenix.otel.register\nDESCRIPTION: This snippet demonstrates how to configure the tracer using `phoenix.otel.register`. It initializes the tracer provider and retrieves the tracer instance, specifying the protocol and project name.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\ntracer_provider = register(protocol=\"http/protobuf\", project_name=\"your project name\")\ntracer = tracer_provider.get_tracer(__name__)\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame of Document Chunks - Python\nDESCRIPTION: Constructs a pandas DataFrame from a list of document nodes, extracting the text content from each node. This creates a structured representation of the source documents or document chunks used in the RAG system, which can then be used for tasks like generating evaluation questions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Let's construct a dataframe of just the documents that are in our index\ndocument_chunks_df = pd.DataFrame({\"text\": [node.get_text() for node in nodes]})\n```\n\n----------------------------------------\n\nTITLE: Computing Tabular Embeddings for DataFrames - Python (GPU Required)\nDESCRIPTION: Instantiates an EmbeddingGeneratorForTabularFeatures using a pre-trained language model ('distilbert-base-uncased') and generates embedding vectors for both training and production DataFrames. The embeddings are stored in a new 'tabular_vector' column. This procedure requires a compatible GPU, sufficient memory, and that the huggingface transformers models are available (either cached or downloadable).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/credit_card_fraud_tutorial.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ngenerator = EmbeddingGeneratorForTabularFeatures(\n    model_name=\"distilbert-base-uncased\",\n)\ntrain_df[\"tabular_vector\"] = generator.generate_embeddings(\n    train_df,\n    selected_columns=feature_column_names,\n)\nprod_df[\"tabular_vector\"] = generator.generate_embeddings(\n    prod_df,\n    selected_columns=feature_column_names,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Constants - Python\nDESCRIPTION: This snippet imports necessary modules and configures the environment for the RAG pipeline. It sets up tracing using Phoenix, initializes constants for project names and maximum queries. The nest_asyncio is important for running async functions within a Jupyter notebook.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\nimport nest_asyncio\n\n# Setup projects to collect tracing under\nos.environ[\"PHOENIX_PROJECT_NAME\"] = \"mistral-rag\"  # Collect traces under the project \"mistral-rag\"\nINDEXING_PROJECT = \"indexing\"  # For llama-index indexing\nTESTSET_PROJECT = \"testset\"  # For capturing synthetic testset traces\nMAX_QUERIES = 10  # Maximum number of queries to run.\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Register Phoenix OpenTelemetry Tracer Python\nDESCRIPTION: Registers the Phoenix OpenTelemetry tracer provider. Setting `auto_instrument=True` automatically instruments supported libraries (like VertexAI if installed) upon registration. `project_name` allows organizing traces within Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/vertexai.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Querying LLM Spans from Phoenix Traces with SpanQuery in Python\nDESCRIPTION: Creates and executes a span query filtering for spans of kind 'LLM', then selects relevant columns 'question' and 'outputs' from span attributes. The query returns a DataFrame with filtered traces from the Phoenix project to process further for evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/amazon_bedrock_agents_tracing_and_evals.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nquery = (\n    SpanQuery()\n    .where(\n        # Filter for the `LLM` span kind.\n        # The filter condition is a string of valid Python boolean expression.\n        \"span_kind == 'LLM'\",\n    )\n    .select(\n        question=\"input.value\",\n        outputs=\"output.value\",\n    )\n)\ntrace_df = px.Client().query_spans(query, project_name=project_name)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies (Python)\nDESCRIPTION: Installs necessary Python libraries using the `%pip` magic command, common in notebook environments. Key dependencies include `ragas`, `pypdf`, `arize-phoenix`, `openai`, `pandas`, and `httpx`, essential for the RAG evaluation workflow.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uq \"ragas==0.1.4\" pypdf \"arize-phoenix[llama-index,embeddings]\" \"openai>=1.0.0\" pandas 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenTelemetry Tracing in LLM Application\nDESCRIPTION: Defines a function that implements manual instrumentation of an LLM application with OpenTelemetry tracing. It handles user intents, performs searches based on user queries, and adds appropriate span attributes following the OpenInference semantic conventions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef run_llm_app(\n    row_json: str, customer_intent_prompt: str, tracer: opentelemetry.sdk.trace.Tracer\n) -> dict:\n    \"\"\"Run manual instrumentation of the LLM application\n\n    Parameters\n    ----------\n    row_json : str\n        JSON formatted string of row data\n    customer_intent_prompt : str\n        Customer intent prompt (is customer asking a about purchases or a separate query)\n    tracer : opentelemetry.sdk.trace.Tracer\n        Tracer to handle span creation\n\n    Returns\n    -------\n    dict\n        Dictionary of response results\n    \"\"\"\n    # Define Span Name & Start\n    with tracer.start_as_current_span(\"Customer Session\") as span:\n        # Define Open Inference Semanantic Convention - Input\n        span.set_attribute(\"input.value\", row[\"Customer Input\"])\n\n        if not isinstance(row_json, str):\n            row_json = row_json.to_json()\n\n        intent_response_json = openai_classify_user_intent(\n            customer_intent_prompt, row_json, tracer=tracer\n        )\n        intent_response_dict = json.loads(intent_response_json)\n\n        intent = intent_response_dict.get(\"customer_intent\")\n        if intent == \"purchase\":\n            result_purchase_json = item_search(intent_response_json, items_df, tracer=tracer)\n            result_purchase_dict = json.loads(result_purchase_json)\n            return_result_response_json = item_search_response(\n                json.dumps(result_purchase_dict), item_search_prompt, tracer=tracer\n            )\n\n        elif intent == \"query\":\n            result_query_json = answer_search(intent_response_json, policy_df, tracer=tracer)\n            result_query_dict = json.loads(result_query_json)\n            return_result_response_json = query_search_response(\n                json.dumps(result_query_dict), customer_qa_prompt, tracer=tracer\n            )\n\n        else:\n            return_result_response_json = json.dumps(\n                {\n                    \"message\": \"Sorry, I couldn't help out. Please reach out to support for more help.\"\n                }\n            )\n\n        result_response_dict = json.loads(return_result_response_json)\n\n        # Define Open Inference Semanantic Convention - Output\n        span.set_attribute(\"output.value\", result_response_dict[\"customer_response\"])\n\n        # Define Custom Attribute String - Customer ID\n        span.set_attribute(\"customerID.name\", result_response_dict[\"Customer ID\"])\n\n        # Define Custom Attribute String - Premium Customer Bool String\n        span.set_attribute(\"premiumCustomer.name\", result_response_dict[\"Premium Customer\"])\n\n        # Define Span Type as \"CHAIN\"\n        span.set_attribute(\n            SpanAttributes.OPENINFERENCE_SPAN_KIND, OpenInferenceSpanKindValues.CHAIN.value\n        )\n\n        # Set Status Code\n        span.set_status(trace_api.StatusCode.OK)\n\n        return result_response_dict\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for Articles and Summaries\nDESCRIPTION: Creates embeddings for articles and summaries using the DistilBERT model through Arize's EmbeddingGenerator, which is configured for a summarization use case.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_summarization_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ngenerator = EmbeddingGenerator.from_use_case(\n    use_case=UseCases.NLP.SUMMARIZATION,\n    model_name=\"distilbert-base-uncased\",\n)\nbaseline_df[\"article_vector\"] = generator.generate_embeddings(text_col=baseline_df[\"article\"])\nbaseline_df[\"summary_vector\"] = generator.generate_embeddings(text_col=baseline_df[\"summary\"])\nrecent_df[\"article_vector\"] = generator.generate_embeddings(text_col=recent_df[\"article\"])\nrecent_df[\"summary_vector\"] = generator.generate_embeddings(text_col=recent_df[\"summary\"])\n```\n\n----------------------------------------\n\nTITLE: Defining Path Length Evaluation Function for Phoenix in Python\nDESCRIPTION: Defines a custom code evaluator function `evaluate_path_length` for Phoenix using the `@create_evaluator` decorator. This function takes the structured output (containing 'path_length') from an experiment run and calculates a score by dividing the pre-calculated `optimal_path_length` by the run's actual path length. It returns 0 if the path length is missing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\n@create_evaluator(name=\"Convergence Eval\", kind=\"CODE\")\ndef evaluate_path_length(output: str) -> float:\n    if output and output.get(\"path_length\"):\n        return optimal_path_length / float(output.get(\"path_length\"))\n    else:\n        return 0\n```\n\n----------------------------------------\n\nTITLE: Creating a RAG Pipeline with Haystack\nDESCRIPTION: This code defines a Retrieval-Augmented Generation (RAG) pipeline using Haystack components. It creates an InMemoryDocumentStore, populates it with documents, defines a prompt template, and sets up a pipeline with a retriever, prompt builder, and OpenAI LLM. Finally, it runs a question through the pipeline.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/haystack_tracing_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom haystack import Document, Pipeline\nfrom haystack.components.builders.prompt_builder import PromptBuilder\nfrom haystack.components.generators import OpenAIGenerator\nfrom haystack.components.retrievers.in_memory import InMemoryBM25Retriever\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\n\ndocument_store = InMemoryDocumentStore()\ndocument_store.write_documents(\n    [\n        Document(content=\"My name is Jean and I live in Paris.\"),\n        Document(content=\"My name is Mark and I live in Berlin.\"),\n        Document(content=\"My name is Giorgio and I live in Rome.\"),\n    ]\n)\n\nprompt_template = \"\"\"\nGiven these documents, answer the question.\nDocuments:\n{% for doc in documents %}\n    {{ doc.content }}\n{% endfor %}\nQuestion: {{question}}\nAnswer:\n\"\"\"\n\nretriever = InMemoryBM25Retriever(document_store=document_store)\nprompt_builder = PromptBuilder(template=prompt_template)\nllm = OpenAIGenerator()\n\nrag_pipeline = Pipeline()\nrag_pipeline.add_component(\"retriever\", retriever)\nrag_pipeline.add_component(\"prompt_builder\", prompt_builder)\nrag_pipeline.add_component(\"llm\", llm)\nrag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\nrag_pipeline.connect(\"prompt_builder\", \"llm\")\n\nquestion = \"Who lives in Paris?\"\nresults = rag_pipeline.run(\n    {\n        \"retriever\": {\"query\": question},\n        \"prompt_builder\": {\"question\": question},\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Viewing QA Correctness Evaluation Results\nDESCRIPTION: Displays the head of the QA correctness evaluation dataframe to review the assessment of answer correctness.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nqa_correctness_eval.head()\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix as a Docker Container - Bash\nDESCRIPTION: Launches the Phoenix server in a Docker container and binds the host port 6006 to the container, allowing access to the Phoenix dashboard from the host system. Execute after pulling the Phoenix image for containerized local deployments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Defining and Versioning a Base Prompt in Phoenix Using OpenAI Completion Parameters in Python\nDESCRIPTION: Defines a base prompt template configured for the GPT-3.5 Turbo model where the system message sets instructions for classification output. Uses OpenAI's CompletionCreateParamsBase to specify model parameters and prompt messages. The prompt is saved to Phoenix under a given identifier, enabling version control and reuse in experiments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom openai.types.chat.completion_create_params import CompletionCreateParamsBase\n\nfrom phoenix.client.types import PromptVersion\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an evaluator that decides whether a given prompt is a jailbreak risk. Only output 'benign' or 'jailbreak', no other words.\",\n        },\n        {\"role\": \"user\", \"content\": \"{{prompt}}\"},\n    ],\n)\n\nprompt_identifier = \"jailbreak-classification\"\n\nprompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"A prompt for classifying whether a given prompt is a jailbreak risk.\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Generating SQL Queries and Questions with GPT-4o in Python\nDESCRIPTION: Defines Pydantic models (`Question`, `Questions`) for structuring SQL queries and natural language questions. Constructs a detailed prompt using table schema information (`sample_rows`) to guide GPT-4o. Calls the OpenAI Chat Completions API (`client.chat.completions.create`) specifying 'gpt-4o', using function calling with the `generate_questions` function to ensure structured JSON output matching the `Questions` schema. Parses the response to extract the list of generated question-query pairs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom typing import List\n\nfrom pydantic import BaseModel\n\n\nclass Question(BaseModel):\n    sql: str\n    question: str\n\n\nclass Questions(BaseModel):\n    questions: List[Question]\n\n\nsample_rows = \"\\n\".join(\n    f\"{column['column_name']} | {column['column_type']} | {samples[column['column_name']]}\"\n    for column in columns\n)\nsynthetic_data_prompt = f\"\"\"You are a SQL expert, and you are given a single table named nba with the following columns:\n\nColumn | Type | Example\n-------|------|--------\n{sample_rows}\n\nGenerate SQL queries that would be interesting to ask about this table. Return the SQL query as a string, as well as the\nquestion that the query answers.\"\"\"\n\nresponse = await client.chat.completions.create(\n    model=\"gpt-4o\",\n    temperature=0,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": synthetic_data_prompt,\n        }\n    ],\n    tools=[\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"generate_questions\",\n                \"description\": \"Generate SQL queries that would be interesting to ask about this table.\",\n                \"parameters\": Questions.model_json_schema(),\n            },\n        }\n    ],\n    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"generate_questions\"}},\n)\n\ngenerated_questions = json.loads(response.choices[0].message.tool_calls[0].function.arguments)[\n    \"questions\"\n]\ngenerated_questions[0]\n```\n\n----------------------------------------\n\nTITLE: Setting Anthropic LLM Model Name in Python\nDESCRIPTION: Defines the model name string for use with the Anthropic Claude 3.5 Haiku latest LLM. Used as a parameter in Anthropic API calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nanthropic_model = \"claude-3-5-haiku-latest\"  # @param {type: \"string\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Model Instrumentation\nDESCRIPTION: Installs necessary Python packages for instrumenting your model calls with OpenInference and Phoenix, enabling automatic tracing and data collection during agent execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/autogen-support.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\npip install openinference-instrumentation-openai arize-phoenix-otel arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Applying Retrieved Prompt to Article Data and Collecting LLM Results in Python\nDESCRIPTION: This snippet retrieves a stored prompt from Phoenix using its identifier, defines a 'process_article' function to send article content to the OpenAI chat completions API with the prompt applied, and collects LLM-generated routines along with their original content and policy. The results are normalized into a pandas DataFrame and displayed as HTML. Key dependencies include 'Client', 'OpenAI', 'pd', and 'HTML'. Input is a record or DataFrame of customer service articles; output is a DataFrame of the policy, article content, and generated routine. This facilitates bulk or batched processing of help articles into standardized routines.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nprompt = Client().prompts.get(prompt_identifier=prompt_identifier)\n\n\ndef process_article(input: dict[str, str]):\n    resp = OpenAI().chat.completions.create(**prompt.format(variables=input))\n    routine = resp.choices[0].message.content\n    return {\"policy\": input[\"policy\"], \"content\": input[\"content\"], \"routine\": routine}\n\n\n# Collect results into a DataFrame.\nres = pd.json_normalize(map(process_article, articles.to_dict(orient=\"records\")))\ndisplay(HTML(res.to_html()))\n```\n\n----------------------------------------\n\nTITLE: Defining an Agent Task Function for Evaluation\nDESCRIPTION: Creates a function for testing the agent that takes a question from a dataset, runs the agent, and returns both the final output and conversation history.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/openai_agents_cookbook.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\n\n# This is our task function. It takes a question and returns the final output and the messages recorded to generate the final output.\nasync def solve_math_problem(dataset_row: dict):\n    result = await Runner.run(agent, dataset_row.get(\"question\"))\n    return {\n        \"final_output\": result.final_output,\n        \"messages\": result.to_input_list(),\n    }\n\n\ndataset_row = {\"question\": \"What is 15 + 28?\"}\n\nresult = asyncio.run(solve_math_problem(dataset_row))\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Adding A-Frame Sphere Primitive (HTML)\nDESCRIPTION: Defines a sphere geometry in the 3D scene using the A-Frame primitive element `<a-sphere>`. Attributes set its `position` (x, y, z coordinates), `radius`, and `color`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_3\n\nLANGUAGE: HTML\nCODE:\n```\n<a-sphere position=\"0 1.25 -5\" radius=\"1.25\" color=\"#EF2D5E\"></a-sphere>\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application UI in Python\nDESCRIPTION: Starts and launches the Phoenix web application user interface using the px.launch_app() method. The resulting UI allows interactive viewing and exploration of telemetry data and traces related to the LLM-powered application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\npx.launch_app().view()\n```\n\n----------------------------------------\n\nTITLE: Preparing Training DataFrame from Parquet File in Python\nDESCRIPTION: Loads training inferences data for a computer vision model from a remote Parquet file into a Pandas DataFrame. This operation depends on pandas and requires the remote dataset URL to be accessible. The resulting DataFrame 'train_df' will contain one row per inference, including columns for predictions, labels, metadata, etc. Key parameter: file URL. Output: Pandas DataFrame with structured inference data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/quickstart/phoenix-inferences/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ntrain_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/cv/human-actions/human_actions_training.parquet\"\n)\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix MCP Server\nDESCRIPTION: This JSON configuration sets up the Phoenix MCP server. It specifies the command to run (npx), the package to execute (@arizeai/phoenix-mcp@latest), and arguments including the Phoenix collector endpoint and API key.  Users must replace placeholders for `baseUrl` and `apiKey` with their actual values.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/integrations/phoenix-mcp-server.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"mcpServers\": {\n    \"phoenix\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@arizeai/phoenix-mcp@latest\",\n        \"--baseUrl\",\n        \"https://my-phoenix.com\",\n        \"--apiKey\",\n        \"your-api-key\"\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Execute RAG Pipeline with Tracing - Python\nDESCRIPTION: Defines the `rag_pipeline` function. Uses the `@tracer.chain` decorator to create a top-level span for the entire pipeline execution. Orchestrates the RAG process by calling the previously defined functions: `query_weaviate`, `format_context`, `create_prompt`, and `query_openai`. Prints the retrieved context during execution and returns the final generated answer.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@tracer.chain\ndef rag_pipeline(query):\n    # Execute the query\n    weaviate_results = query_weaviate(query)\n    context = format_context(weaviate_results)\n    print(\"Retrieved context:\")\n    print(context)\n\n    # Create a prompt with the retrieved information\n    final_prompt = create_prompt(query, context)\n\n    # Execute the OpenAI query\n    final_answer = query_openai(final_prompt)\n\n    return final_answer\n```\n\n----------------------------------------\n\nTITLE: Exporting Arize Model Data Using Python Export Client\nDESCRIPTION: Python code to export model data from Arize to a pandas DataFrame. This method requires setting the API key as an environment variable, specifying the space ID, model name, environment, and date range for the data export.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/integrations/bring-production-data-to-notebook-for-eda-or-retraining.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nos.environ['ARIZE_API_KEY'] = ARIZE_API_KEY\n\nfrom datetime import datetime\n\nfrom arize.exporter import ArizeExportClient\nfrom arize.utils.types import Environments\n\nclient = ArizeExportClient()\n\nprimary_df = client.export_model_to_df(\n    space_id='U3BhY2U6NzU0',\n    model_name='test_home_prices_LLM',\n    environment=Environments.PRODUCTION,\n    start_time=datetime.fromisoformat('2023-02-11T07:00:00.000+00:00'),\n    end_time=datetime.fromisoformat('2023-03-14T00:59:59.999+00:00'),\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key\nDESCRIPTION: Sets up the OpenAI API key by either retrieving it from environment variables or prompting the user for input, which is necessary for using OpenAI models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App with Datasets Python\nDESCRIPTION: Launches the Phoenix application, optionally providing one or two datasets (instances of `phoenix.Dataset` or `Inferences` as mentioned in older docs). The function returns a `Session` object that represents the running Phoenix instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/api_reference/source/api/session.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nsession = px.launch_app(prim_inf_, ref_inf_)\n```\n\nLANGUAGE: Python\nCODE:\n```\nsession = px.launch_app(inf)\n```\n\nLANGUAGE: Python\nCODE:\n```\nsession = px.launch_app(prim_inf, ref_inf)\n```\n\nLANGUAGE: Python\nCODE:\n```\nsession = px.launch_app(inf)\n```\n\nLANGUAGE: Python\nCODE:\n```\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Specifying Full Endpoint Directly for Phoenix OTEL with HTTP Transport in Python\nDESCRIPTION: Shows explicit configuration of the tracer provider by specifying the fully qualified HTTP collector endpoint directly in the register function call. The HTTP transport protocol is automatically inferred from the endpoint URL, enabling direct and explicit span export destination control.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\ntracer_provider = register(endpoint=\"http://localhost:6006/v1/traces\")\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Phoenix AI Client in Python\nDESCRIPTION: This snippet imports the Phoenix client library and initializes a Client instance for interacting with the Phoenix API. The client instance `pxc` is required as a dependency for subsequent function calls that retrieve and manipulate project data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/llamaindex-workflows-research-agent/evaluate_traces.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\npxc = px.Client()\n```\n\n----------------------------------------\n\nTITLE: Defining SQL Query Generation Prompt Template in LangChain\nDESCRIPTION: Creates a system prompt template to instruct the language model (GPT-4) to generate correct SQLite queries based on user questions, limiting results and enforcing best practices. This template is used within a ChatPromptTemplate for structured message handling.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nquery_gen_system = \"\"\"You are a SQL expert with a strong attention to detail.\n\nGiven an input question, output a syntactically correct SQLite query to run, then look at the results of the query and return the answer.\n\nDO NOT call any tool besides SubmitFinalAnswer to submit the final answer.\n\nWhen generating the query:\n\nOutput the SQL query that answers the input question without a tool call.\n\nUnless the user specifies a specific number of examples they wish to obtain, always limit your query to at most 5 results.\nYou can order the results by a relevant column to return the most interesting examples in the database.\nNever query for all the columns from a specific table, only ask for the relevant columns given the question.\n\nIf you get an error while executing a query, rewrite the query and try again.\n\nIf you get an empty result set, you should try to rewrite the query to get a non-empty result set.\nNEVER make stuff up if you don't have enough information to answer the query... just say you don't have enough information.\n\nIf you have enough information to answer the input question, simply invoke the appropriate tool to submit the final answer to the user.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Generating Readability Classification with Explanations Enabled\nDESCRIPTION: Runs classification with explanations included, outputting both labels and reasoning for troubleshooting or interpretability, at the cost of additional tokens and slower execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nsmall_df_sample = df.copy().sample(n=5).reset_index(drop=True)\nreadability_classifications_df = llm_classify(\n    dataframe=small_df_sample,\n    template=CODE_READABILITY_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    provide_explanation=True,\n    verbose=True,\n    concurrency=20,\n)\n```\n\n----------------------------------------\n\nTITLE: Running Experiment with Few Shot Prompt - Python\nDESCRIPTION: Launches a new experiment in Phoenix using the dataset, the few shot prompt task function, and the previous evaluator(s). Describes and names the experiment for clarity and attaches the prompt version metadata for reproducibility and comparison. Dependencies: phoenix.experiments. Output is an experiment object tracked and visualized within Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfew_shot_experiment = run_experiment(\n    dataset,\n    task=test_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Prompt Optimization Technique #1: Few Shot Examples\",\n    experiment_name=\"few-shot-examples\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + few_shot_prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting the Application for Tracing\nDESCRIPTION: Registers the application with Phoenix for tracing, enabling the collection of performance data. It sets up a tracer provider and instruments the OpenAI library to capture traces of OpenAI API calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/react_prompting.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntracer_provider = register(\n    project_name=\"ReAct-examples\", endpoint=\"https://app.phoenix.arize.com/v1/traces\"\n)\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Production Evaluation Components\nDESCRIPTION: Imports and initializes components for production-time evaluation, including OpenTelemetry tracing and Phoenix span evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/openai_agents_cookbook.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry.trace import StatusCode, format_span_id\n\nfrom phoenix.trace import SpanEvaluations\n\ntracer = tracer_provider.get_tracer(__name__)\n```\n\n----------------------------------------\n\nTITLE: Classifying Code Readability with GPT-4 and Post-Processing\nDESCRIPTION: Uses llm_classify to perform batch classification of code snippets in the dataset, utilizing predefined prompt templates and rails for binary labels. Results are stored as a list of predicted labels.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nrails = list(CODE_READABILITY_PROMPT_RAILS_MAP.values())\nreadability_classifications = llm_classify(\n    dataframe=df,\n    template=CODE_READABILITY_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    concurrency=20,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Computing and Visualizing Classification Metrics in Python\nDESCRIPTION: Calculates true/false relevance metrics by comparing LLM predicted relevance labels to human-labeled ground-truth. Produces a classification report (precision, recall, F1) and a confusion matrix plot using matplotlib and pycm. Ensures normalized display with label mapping derived from the dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df_sample[\"relevant\"].map(RAG_RELEVANCY_PROMPT_RAILS_MAP).tolist()\n\nprint(classification_report(true_labels, relevance_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels, predict_vector=relevance_classifications, classes=rails\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application (Python)\nDESCRIPTION: Starts the Phoenix observability application which allows tracing and evaluation of the RAG pipeline. This setup step must precede instrumentation and any pipeline operations to enable tracing functionality. No input parameters are required; it launches a local or remote Phoenix application instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Launching the Phoenix Application Session\nDESCRIPTION: Initializes and launches the Phoenix application in the background using `px.launch_app()`. This instance will collect trace data emitted by the instrumented LlamaIndex application. The `session.view()` method typically opens or displays the URL to the Phoenix UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n(session := px.launch_app()).view()\n```\n\n----------------------------------------\n\nTITLE: Importing Required Python Libraries for LangChain, VertexAI, Phoenix, and Telemetry\nDESCRIPTION: Imports multiple libraries used to build and trace the LangChain application with VertexAI embeddings and chat models. Includes numpy, pandas for data management, LangChain components like RetrievalQA and KNNRetriever, Phoenix instrumentation and evaluation tools, OpenTelemetry for trace exporting, and utilities for asynchronous execution and HTTP requests. Also contains logic to detect if running in Google Colab to enable authentication.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_vertex_ai_tracing_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom urllib.request import urlopen\n\nimport nest_asyncio\nimport numpy as np\nimport pandas as pd\nfrom langchain.chains import RetrievalQA\nfrom langchain.retrievers import KNNRetriever\nfrom langchain_google_vertexai import ChatVertexAI, VertexAIEmbeddings\nfrom openinference.instrumentation.langchain import LangChainInstrumentor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom tqdm import tqdm\n\nimport phoenix as px\nfrom phoenix.evals import (\n    HallucinationEvaluator,\n    QAEvaluator,\n    RelevanceEvaluator,\n    VertexAIModel,\n    run_evals,\n)\nfrom phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\nfrom phoenix.trace import DocumentEvaluations, SpanEvaluations\n\nnest_asyncio.apply()  # needed for concurrent evals in notebook environments\n\ntry:\n    from google.colab.auth import authenticate_user\n\n    IS_COLAB = True\nexcept ImportError:\n    IS_COLAB = False\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Schema with Explicit Features and Tags in Python\nDESCRIPTION: Illustrates defining a `phoenix.Schema` that explicitly lists model input features (`feature_column_names`) and descriptive tags (`tag_column_names`), alongside prediction and actual labels. Features are used by the model, while tags provide metadata for filtering and cohort analysis within Phoenix. This example maps 'predicted' and 'target' columns and specifies lists for feature and tag column names.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/define-your-schema/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nschema = px.Schema(\n    prediction_label_column_name=\"predicted\",\n    actual_label_column_name=\"target\",\n    feature_column_names=[\n        \"fico_score\",\n        \"merchant_id\",\n        \"loan_amount\",\n        \"annual_income\",\n        \"home_ownership\",\n        \"num_credit_lines\",\n        \"inquests_in_last_6_months\",\n        \"months_since_last_delinquency\",\n    ],\n    tag_column_names=[\n        \"age\",\n        \"gender\",\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Exporting Retriever Span DataFrames using Phoenix - Python\nDESCRIPTION: Exports the 'retriever' spans into two pandas DataFrames using Phoenix's px.Client: one for query-level aggregation (queries_df) and another with exploded documents for per-document evaluation (retrieved_documents_df). Requires the px.Client from the Phoenix library and assumes prior traces exist for extraction. Both functions fetch and format trace data for downstream evaluation and must be run in a Python environment with Phoenix installed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nqueries_df = get_qa_with_reference(px.Client())\nretrieved_documents_df = get_retrieved_documents(px.Client())\n```\n\n----------------------------------------\n\nTITLE: Launching Arize Phoenix Application in Python\nDESCRIPTION: Imports the Phoenix library and launches an Arize Phoenix tracing session by calling launch_app(). This initializes the Phoenix UI and backend services for visualizing telemetry data. The returned session object contains connection details such as the URL where traces can be viewed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/crewai_tracing_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\nsession = px.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Adding Attributes to a Span (python)\nDESCRIPTION: Shows how to attach custom key/value pairs (attributes) to the current span using set_attribute, supporting types such as int, string, and list. Requires: opentelemetry.trace; a current active span object. Keys should ideally be vendor-prefixed to avoid conflicts. Output: enriched span with custom attributes; input: attribute names and values. No output returned directly.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/custom-spans.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace\n\ncurrent_span = trace.get_current_span()\n\ncurrent_span.set_attribute(\"operation.value\", 1)\ncurrent_span.set_attribute(\"operation.name\", \"Saying hello!\")\ncurrent_span.set_attribute(\"operation.other-stuff\", [1, 2, 3])\n```\n\n----------------------------------------\n\nTITLE: Implementing a Function Router for the Customer Support Agent\nDESCRIPTION: Creates a function that processes user input and routes it to the appropriate tool using OpenAI's function calling capability. Uses gpt-4o-mini model with temperature=0 for deterministic responses and handles tool calls appropriately.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport openai\n\n\ndef run_prompt(input):\n    client = openai.Client()\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        temperature=0,\n        tools=tools,\n        tool_choice=\"auto\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \" \",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": input,\n            },\n        ],\n    )\n\n    if (\n        hasattr(response.choices[0].message, \"tool_calls\")\n        and response.choices[0].message.tool_calls is not None\n        and len(response.choices[0].message.tool_calls) > 0\n    ):\n        tool_calls = response.choices[0].message.tool_calls\n    else:\n        tool_calls = []\n\n    if response.choices[0].message.content is None:\n        response.choices[0].message.content = \"\"\n    if response.choices[0].message.content:\n        return response.choices[0].message.content\n    else:\n        return tool_calls\n```\n\n----------------------------------------\n\nTITLE: Create Weaviate Collection - Python\nDESCRIPTION: Imports required Weaviate modules ('weaviate', 'Configure', 'Auth'). Connects to Weaviate Cloud. Creates a new collection named 'Question' configured with a text2vec-weaviate vectorizer for embeddings and a Cohere generative integration. Closes the client connection.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport weaviate\nfrom weaviate.classes.config import Configure\nfrom weaviate.classes.init import Auth\n\n# Best practice: store your credentials in environment variables\nwcd_url = os.environ[\"WEAVIATE_URL\"]\nwcd_api_key = os.environ[\"WEAVIATE_API_KEY\"]\n\nclient = weaviate.connect_to_weaviate_cloud(\n    cluster_url=wcd_url,  # Replace with your Weaviate Cloud URL\n    auth_credentials=Auth.api_key(wcd_api_key),  # Replace with your Weaviate Cloud key\n)\n\nquestions = client.collections.create(\n    name=\"Question\",\n    vectorizer_config=Configure.Vectorizer.text2vec_weaviate(),  # Configure the Weaviate Embeddings integration\n    generative_config=Configure.Generative.cohere(),  # Configure the Cohere generative AI integration\n)\n\nclient.close()  # Free up resources\n```\n\n----------------------------------------\n\nTITLE: Importing Phoenix Evaluation and Query Libraries in Python\nDESCRIPTION: Imports core Phoenix package, evaluation utilities including prompt rails map, template, Bedrock model wrapper, classification function, and span querying DSL to enable querying traced spans and performing LLM-based evaluation classification.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/amazon_bedrock_agents_tracing_and_evals.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nimport phoenix as px\nfrom phoenix.evals import (\n    TOOL_CALLING_PROMPT_RAILS_MAP,\n    TOOL_CALLING_PROMPT_TEMPLATE,\n    BedrockModel,\n    llm_classify,\n)\nfrom phoenix.trace import SpanEvaluations\nfrom phoenix.trace.dsl import SpanQuery\n```\n\n----------------------------------------\n\nTITLE: Inspect Evaluation DataFrames\nDESCRIPTION: This snippet displays the resulting DataFrames from the Phoenix evaluations for responses correctness, function selection appropriateness, and parameter matching, assisting in performance analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nrouter_eval_df\n```\n\n----------------------------------------\n\nTITLE: Import Key Modules and Configure Asyncio Compatibility\nDESCRIPTION: Imports standard and third-party libraries including json, pandas, time, nested asyncio, and modules from Llama-Index, Phoenix, and OpenAI. Applies nest_asyncio to allow nested asyncio event loops, facilitating async operations within notebooks or scripts.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport json\nfrom textwrap import shorten\nfrom time import time_ns\nfrom typing import Tuple\n\nimport nest_asyncio\nimport pandas as pd\nfrom llama_index.core.evaluation import (\n    PairwiseComparisonEvaluator,\n)\nfrom llama_index.llms.openai import OpenAI\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nimport phoenix as px\nfrom phoenix.experiments import evaluate_experiment, run_experiment\nfrom phoenix.experiments.types import Explanation, Score\nfrom phoenix.otel import register\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Adding Descriptive Tool Definitions Column to Phoenix Trace Data Frame in Python\nDESCRIPTION: Adds a constant textual description to the dataframe under the column 'tool_definitions' explaining the meaning of various Phoenix trace and experiment sources. This description supplements evaluation context.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/amazon_bedrock_agents_tracing_and_evals.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntrace_df[\"tool_definitions\"] = (\n    \"phoenix-traces retrieves the latest trace information from Phoenix, phoenix-experiments retrieves the latest experiment information from Phoenix, phoenix-datasets retrieves the latest dataset information from Phoenix\"\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Evaluation DataFrame (Python)\nDESCRIPTION: Displays the contents of the `eval_df` Pandas DataFrame. This DataFrame at this stage contains the span IDs as index and a boolean 'label' column indicating if the corresponding joke was a duplicate.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evaluating_traces_quickstart.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\neval_df\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Bedrock with OpenInference in Python\nDESCRIPTION: This code snippet instruments Bedrock clients using the OpenInference Bedrock instrumentor. After calling `.instrument()`, all Bedrock clients created from the session will automatically generate OpenInference traces when `invoke_model` is called.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/bedrock_tracing_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nBedrockInstrumentor().instrument(skip_dep_check=True)\ninstrumented_client = session.client(\"bedrock-runtime\")\n```\n\n----------------------------------------\n\nTITLE: Defining Query Engines as LlamaIndex Tools\nDESCRIPTION: Creates a list of `QueryEngineTool` objects for LlamaIndex. Each tool wraps one of the previously created Chroma query engines (policies and employees) and provides `ToolMetadata` including a descriptive name and description. These tools will be available for the agent to use when answering queries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/agentic_rag_tracing.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=chroma_engine_employees,\n        metadata=ToolMetadata(\n            name=\"ChromaEmployees\",\n            description=(\n                \"Provides information about an employee's department, start date, and name from a relational database.\"\n                \"Use a detailed plain text question as input to the tool.\"\n            ),\n        ),\n    ),\n    QueryEngineTool(\n        query_engine=chroma_engine_policy,\n        metadata=ToolMetadata(\n            name=\"ChromaPolicy\",\n            description=(\n                \"Provides information about company policies and preocedures. Use this to get more detailed information about company policies.\"\n                \"Use a detailed plain text statement about a specific policy as input to the tool.\"\n            ),\n        ),\n    ),\n]\n```\n\n----------------------------------------\n\nTITLE: Displaying the DataFrame with Responses - Python\nDESCRIPTION: Displays the updated questions_df DataFrame, which now contains a 'response' column resulting from agent invocation on each question. This step assumes questions_df is a pandas DataFrame and is typically used for inspection or further analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nquestions_df\n```\n\n----------------------------------------\n\nTITLE: Experiment for DSPy Classifier\nDESCRIPTION: Runs an experiment with the DSPy optimized classifier. Defines a `test_gradient_prompt` function. The `run_experiment` function is then called, which executes the experiment using the dataset and the `test_gradient_prompt` which is used to test the prompt on the `dataset`. It also takes an evaluator, as well as a description, and the experiment name. In addition, it stores metadata, including the associated prompt id. This tracks the results of the DSPy optimized classifier and its performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndef test_gradient_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **gradient_prompt.format(variables={\"prompt\": input[\"prompt\"]})\n    )\n    return resp.choices[0].message.content.strip()\n```\n\nLANGUAGE: python\nCODE:\n```\ngradient_experiment = run_experiment(\n    dataset,\n    task=test_gradient_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Prompt Optimization Technique #3: Prompt Gradients\",\n    experiment_name=\"gradient-optimization\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + gradient_prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Launching the Phoenix Web App with Python\nDESCRIPTION: Starts the Phoenix web application server using its Python API. This allows interactive use of the Phoenix platform for experiment tracking and evaluation visualization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Haystack with OpenInference and Phoenix\nDESCRIPTION: Sets up OpenTelemetry tracing for Haystack using the HaystackInstrumentor, enabling automatic trace collection in the Haystack application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag_haystack.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.haystack import HaystackInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register()\n\n# Use Phoenix's autoinstrumentor to automatically track traces from Haystack\nHaystackInstrumentor().instrument(tracer_provider=tracer_provider, skip_dep_check=True)\n```\n\n----------------------------------------\n\nTITLE: Assigning Span IDs for Phoenix\nDESCRIPTION: This snippet assigns span IDs to Ragas evaluation scores and merges them with other DataFrames to prepare the data for visualization in Phoenix. It's crucial for linking the evaluation results with the application spans, allowing for annotations and analysis within the Phoenix UI.  It uses `merge` to combine dataframes based on the 'question' column and sets the index to 'context.span_id'.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nspan_questions = (\n    spans_dataframe[['input']]\n    .sort_values('input')\n    .drop_duplicates(subset=['input'], keep='first')\n    .reset_index()\n    .rename({'input': 'question'}, axis=1)\n)\nragas_evals_df = ragas_evals_df.merge(span_questions, on='question').set_index('context.span_id')\ntest_df = test_df.merge(span_questions, on='question').set_index('context.span_id')\neval_data_df = pd.DataFrame(evaluation_result.dataset)\neval_data_df = eval_data_df.merge(span_questions, on='question').set_index('context.span_id')\neval_scores_df.index = eval_data_df.index\n\nquery_embeddings_df = (\n    query_embeddings_df.sort_values('text')\n    .drop_duplicates(subset=['text'])\n    .rename({'text': 'question'}, axis=1)\n    .merge(span_questions, on='question')\n    .set_index('context.span_id')\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix Python SDK\nDESCRIPTION: Installs the Arize Phoenix Python package with embeddings support via pip. This setup is required prior to importing and using the Arize Phoenix SDK in a Python environment such as Jupyter notebooks or Google Colab. Using the `-Uq` flag ensures a quiet and upgraded installation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/inferences_quickstart.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uq 'arize-phoenix[embeddings]'\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluation and Agent Utility Functions for LLM-Based QA with Python\nDESCRIPTION: This code block provides several utility functions for evaluating various aspects of LLM responses in a QA pipeline using sales data. function_calling_eval computes a correctness score for predicted tool calls using LLM-based classification; code_is_runnable attempts to execute generated code from tool outputs to verify if it runs without error; evaluate_sql_result compares the numeric outputs of LLM-synthesized SQL queries; evaluate_clarity and evaluate_entity_correctness respectively use LLM classifiers and prompt templates to judge clarity and entity alignment in final responses. All functions assume dependencies on pandas, a custom llm_classify function, prompt templates, and the eval_model identifier. Inputs are dicts containing question/answer data, and outputs are numerical scores or booleans. Caution should be taken with exec usage inside code_is_runnable due to potential security risks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_59\n\nLANGUAGE: Python\nCODE:\n```\ndef function_calling_eval(input: str, output: str) -> float:\n    function_calls = output.get(\"tool_calls\")\n    if function_calls:\n        eval_df = pd.DataFrame(\n            {\"question\": [input.get(\"question\")] * len(function_calls), \"tool_call\": function_calls}\n        )\n\n        tool_call_eval = llm_classify(\n            dataframe=eval_df,\n            template=TOOL_CALLING_PROMPT_TEMPLATE.template.replace(\n                \"{tool_definitions}\", json.dumps(tools).replace(\"{\", '\"').replace(\"}\", '\"')\n            ),\n            rails=[\"correct\", \"incorrect\"],\n            model=eval_model,\n            provide_explanation=True,\n        )\n\n        tool_call_eval[\"score\"] = tool_call_eval.apply(\n            lambda x: 1 if x[\"label\"] == \"correct\" else 0, axis=1\n        )\n        return tool_call_eval[\"score\"].mean()\n    else:\n        return 0\n\n\ndef code_is_runnable(output: str) -> bool:\n    \"\"\"Check if the code is runnable\"\"\"\n    generated_code = output.get(\"tool_responses\")\n    if not generated_code:\n        return True\n\n    # Find first lookup_sales_data response\n    generated_code = next(\n        (r for r in generated_code if r.get(\"tool_name\") == \"generate_visualization\"), None\n    )\n    if not generated_code:\n        return True\n\n    # Get the first response\n    generated_code = generated_code.get(\"tool_response\", \"\")\n    generated_code = generated_code.strip()\n    generated_code = generated_code.replace(\"```python\", \"\").replace(\"```\", \"\")\n    try:\n        exec(generated_code)\n        return True\n    except Exception:\n        return False\n\n\ndef evaluate_sql_result(output, expected) -> bool:\n    sql_result = output.get(\"tool_responses\")\n    if not sql_result:\n        return True\n\n    # Find first lookup_sales_data response\n    sql_result = next((r for r in sql_result if r.get(\"tool_name\") == \"lookup_sales_data\"), None)\n    if not sql_result:\n        return True\n\n    # Get the first response\n    sql_result = sql_result.get(\"tool_response\", \"\")\n\n    # Extract just the numbers from both strings\n    result_nums = \"\".join(filter(str.isdigit, sql_result))\n    expected_nums = \"\".join(filter(str.isdigit, expected.get(\"sql_result\")))\n    return result_nums == expected_nums\n\n\ndef evaluate_clarity(output: str, input: str) -> bool:\n    df = pd.DataFrame({\"query\": [input.get(\"question\")], \"response\": [output.get(\"final_output\")]})\n    response = llm_classify(\n        dataframe=df,\n        template=CLARITY_LLM_JUDGE_PROMPT,\n        rails=[\"clear\", \"unclear\"],\n        model=eval_model,\n        provide_explanation=True,\n    )\n    return response[\"label\"] == \"clear\"\n\n\ndef evaluate_entity_correctness(output: str, input: str) -> bool:\n    df = pd.DataFrame({\"query\": [input.get(\"question\")], \"response\": [output.get(\"final_output\")]})\n    response = llm_classify(\n        dataframe=df,\n        template=ENTITY_CORRECTNESS_LLM_JUDGE_PROMPT,\n        rails=[\"correct\", \"incorrect\"],\n        model=eval_model,\n        provide_explanation=True,\n    )\n    return response[\"label\"] == \"correct\"\n```\n\n----------------------------------------\n\nTITLE: Defining Tools and Creating Prompt (Python)\nDESCRIPTION: This snippet defines a tool called `print_all_characteristics` designed to handle an arbitrary number of characteristics.  It also constructs the prompt content and parameters using `MessageCreateParamsBase` for the Anthropic model. It takes a description as input (`desc`) and formats it into a prompt for the language model to generate the character's attributes. This involves setting the model, maximum token limit, tools, and tool choice within `params`. The prompt identifier can only contain alphanumeric characters, hyphens, or underscores. Prerequisites include installing the Anthropic and Phoenix libraries and having access to Anthropic models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ntools = [\n    {\n        \"name\": \"print_all_characteristics\",\n        \"description\": \"Prints all characteristics which are provided.\",\n        \"input_schema\": {\"type\": \"object\", \"additionalProperties\": True},\n    }\n]\n\ncontent = \"\"\"\n    Given a description of a character, your task is to extract all the characteristics of the character and print them using the print_all_characteristics tool.\n\n    The print_all_characteristics tool takes an arbitrary number of inputs where the key is the characteristic name and the value is the characteristic value (age: 28 or eye_color: green).\n\n    <description>\n    {{desc}}\n    </description>\n\n    Now use the print_all_characteristics tool.\n\"\"\"\n\nparams = MessageCreateParamsBase(\n    model=\"claude-3-5-haiku-latest\",\n    max_tokens=4096,\n    tools=tools,\n    tool_choice={\"type\": \"tool\", \"name\": \"print_all_characteristics\"},\n    messages=[{\"role\": \"user\", \"content\": dedent(content)}],\n)\n\n```\n\n----------------------------------------\n\nTITLE: Generating Questions from Document Chunks using LLM\nDESCRIPTION: Uses the Phoenix evals module to generate questions from document chunks, implementing a JSON output parser to handle LLM responses.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nfrom phoenix.evals import OpenAIModel, llm_generate\n\n\ndef output_parser(response: str, index: int):\n    try:\n        return json.loads(response)\n    except json.JSONDecodeError as e:\n        return {\"__error__\": str(e)}\n\n\nquestions_df = llm_generate(\n    dataframe=document_chunks_df,\n    template=generate_questions_template,\n    model=OpenAIModel(model=\"gpt-3.5-turbo\"),\n    output_parser=output_parser,\n    concurrency=20,\n)\n```\n\n----------------------------------------\n\nTITLE: Calculate Agent Path Convergence in Python\nDESCRIPTION: This Python code calculates the convergence score for agent runs based on the length of the paths taken. It iterates through a list of agent outputs, determines the optimal (minimum) path length, and calculates the ratio of the optimal path length to the actual run length for each output. The convergence score is then calculated as the average of these ratios. Requires a list of agent output messages, where each output represents a path taken by the agent.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/agent-path-convergence.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Assume you have an output which has a list of messages, which is the path taken\nall_outputs = [\n]\n\noptimal_path_length = 999\nratios_sum = 0\n\nfor output in all_outputs:\n    run_length = len(output)\n    optimal_path_length = min(run_length, optimal_path_length)\n    ratio = optimal_path_length / run_length\n    ratios_sum += ratio\n\n# Calculate the average ratio\nif len(all_outputs) > 0:\n    convergence = ratios_sum / len(all_outputs)\nelse:\n    convergence = 0\n\nprint(f\"The optimal path length is {optimal_path_length}\")\nprint(f\"The convergence is {convergence}\")\n```\n\n----------------------------------------\n\nTITLE: Instrumenting code with OpenInference\nDESCRIPTION: Instruments the code with OpenInference and registers the tracer provider with Phoenix. This step enables trace visualization for deeper insights into the model's behavior.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/few_shot_prompting.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(project_name=\"few-shot-examples\")\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Task Function for Experiment - Python\nDESCRIPTION: Defines an asynchronous Python function `solve_math_problem` that serves as the task for the experiment. It takes input, runs the defined `agent` using `Runner.run`, and returns a dictionary containing the agent's final output and the message history, formatted for subsequent evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/integrations/ragas.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom agents import Runner\n\nasync def solve_math_problem(input):\n    if isinstance(input, dict):\n        input = next(iter(input.values()))\n    result = await Runner.run(agent, input)\n    return {\"final_output\": result.final_output, \"messages\": result.to_input_list()}\n```\n\n----------------------------------------\n\nTITLE: Initialize Phoenix Tracer Provider and Tracing - Python\nDESCRIPTION: Imports the `register` function from `phoenix.otel`. Sets a project name for the Phoenix trace session. Registers the tracer provider, enabling auto-instrumentation for libraries like OpenAI. Retrieves a tracer instance specifically for manual instrumentation within the RAG pipeline functions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\nphoenix_project_name = \"weaviate-rag-pipeline\"\n\n# Because you've install the openinference openai package, the call below will auto-instrument OpenAI calls\ntracer_provider = register(project_name=phoenix_project_name, auto_instrument=True)\n\n# Retrieve a tracer for manual instrumentation\ntracer = tracer_provider.get_tracer(__name__)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Embedding Function for Chroma\nDESCRIPTION: Initializes the OpenAI embedding function provided by the `chromadb.utils.embedding_functions` module. This function is configured with the user's OpenAI API key and specifies the 'text-embedding-3-small' model, which will be used by Chroma to generate embeddings for text documents.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/agentic_rag_tracing.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nopenai_ef = embedding_functions.OpenAIEmbeddingFunction(\n    api_key=os.environ[\"OPENAI_API_KEY\"], model_name=\"text-embedding-3-small\"\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading Phoenix Benchmark Dataset (Python)\nDESCRIPTION: Downloads a specific benchmark dataset from the Phoenix library for use in evaluations. The function is configured to download the 'wiki_qa-train' dataset for a 'binary-relevance-classification' task, providing a standard dataset for testing evaluation functionality.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = download_benchmark_dataset(\n    task=\"binary-relevance-classification\", dataset_name=\"wiki_qa-train\"\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Documents Dataframe\nDESCRIPTION: This snippet retrieves the documents from the Phoenix client and displays the head of the DataFrame containing document information. It uses the `get_retrieved_documents` helper function.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/span_query_language.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndocs_df = get_retrieved_documents(px.Client())\ndocs_df.head()\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Python Environment\nDESCRIPTION: Checks for an existing OpenAI API key in the environment variable OPENAI_API_KEY. If not found, prompts the user to securely input their OpenAI API key via a password prompt and sets it in the environment. This key is required for authenticating OpenAI API requests throughout the application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix in a Notebook - Python\nDESCRIPTION: Imports Phoenix in an interactive Python notebook and launches the web application interface using 'px.launch_app()'. The 'phoenix' package must be installed in the environment. Suitable for quick, ephemeral instrumented tracing in notebook workflows, but does not save traces persistently.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/litellm.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n\n```\n\n----------------------------------------\n\nTITLE: Filtering Spans by Substring using 'in' Operator in Python Expression\nDESCRIPTION: Provides an example of a Python boolean expression string used for filtering spans. This expression checks if the substring 'programming' is present within the 'output.value' attribute of a span using the `in` operator. This syntax is applicable in both the Phoenix UI search bar and the `.where()` method of `SpanQuery`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/filter-spans.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n'programming' in output.value\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Tracer (Python)\nDESCRIPTION: This snippet configures the Phoenix tracer provider using the `register` function from the `phoenix.otel` module.  It sets the project name and enables automatic instrumentation for supported libraries. The `tracer` object is then obtained for creating custom spans.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-python.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # See 'Trace all calls made to a library' below\n)\ntracer = tracer_provider.get_tracer(__name__)\n```\n\n----------------------------------------\n\nTITLE: Creating Combined Evaluation Dataframe\nDESCRIPTION: Combines all evaluation metrics into a single dataframe for comprehensive analysis of the RAG system's retrieval performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nretrievals_df = px.Client().get_spans_dataframe(\n    \"span_kind == 'RETRIEVER' and input.value is not None\"\n)\nrag_evaluation_dataframe = pd.concat(\n    [\n        retrievals_df[\"attributes.input.value\"],\n        ndcg_at_2.add_prefix(\"ncdg@2_\"),\n        precision_at_2.add_prefix(\"precision@2_\"),\n        hit,\n    ],\n    axis=1,\n)\nrag_evaluation_dataframe\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key\nDESCRIPTION: This code snippet configures the OpenAI API key by retrieving it from an environment variable or prompting the user to enter it. It then sets the API key as an environment variable for use by the OpenAI models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_functionality_classifications.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Experiment Questions and SQL Generation (Python)\nDESCRIPTION: This snippet defines a list of questions related to store sales data. It then iterates through this list and uses the `generate_sql_query` function (not defined in the provided context) to generate a SQL query for each question, storing the generated query in the `sql_generated` field of each question dictionary. The generated SQL queries are used to answer the questions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\noverall_experiment_questions = [\n    {\n        \"question\": \"What was the most popular product SKU?\",\n        \"sql_result\": \"   SKU_Coded  Total_Qty_Sold 0    6200700         52262.0\",\n    },\n    {\n        \"question\": \"What was the total revenue across all stores?\",\n        \"sql_result\": \"   Total_Revenue 0   1.327264e+07\",\n    },\n    {\n        \"question\": \"Which store had the highest sales volume?\",\n        \"sql_result\": \"   Store_Number  Total_Sales_Volume 0          2970             59322.0\",\n    },\n    {\n        \"question\": \"Create a bar chart showing total sales by store\",\n        \"sql_result\": \"    Store_Number    Total_Sales 0            880  420302.088397 1           1650  580443.007953 2           4180  272208.118542 3            550  229727.498752 4           1100  497509.528013 5           3300  619660.167018 6           3190  335035.018792 7           2970  836341.327191 8           3740  359729.808228 9           2530  324046.518720 10          4400   95745.620250 11          1210  508393.767785 12           330  370503.687331 13          2750  453664.808068 14          1980  242290.828499 15          1760  350747.617798 16          3410  410567.848126 17           990  378433.018639 18          4730  239711.708869 19          4070  322307.968330 20          3080  495458.238811 21          2090  309996.247965 22          1320  592832.067579 23          2640  308990.318559 24          1540  427777.427815 25          4840  389056.668316 26          2860  132320.519487 27          2420  406715.767402 28           770  292968.918642 29          3520  145701.079372 30           660  343594.978075 31          3630  405034.547846 32          2310  412579.388504 33          2200  361173.288199 34          1870  401070.997685\",\n    },\n    {\n        \"question\": \"What percentage of items were sold on promotion?\",\n        \"sql_result\": \"   Promotion_Percentage 0              0.625596\",\n    },\n    {\n        \"question\": \"What was the average transaction value?\",\n        \"sql_result\": \"   Average_Transaction_Value 0                  19.018132\",\n    },\n    {\n        \"question\": \"Create a line chart showing sales in 2021\",\n        \"sql_result\": \"  sale_month  total_quantity_sold  total_sales_value 0 2021-11-01              43056.0      499984.428193 1 2021-12-01              75724.0      910982.118423\",\n    },\n]\n\noverall_experiment_questions[0][\"sql_generated\"] = generate_sql_query(\n    overall_experiment_questions[0][\"question\"], store_sales_df.columns, \"sales\"\n)\noverall_experiment_questions[1][\"sql_generated\"] = generate_sql_query(\n    overall_experiment_questions[1][\"question\"], store_sales_df.columns, \"sales\"\n)\noverall_experiment_questions[2][\"sql_generated\"] = generate_sql_query(\n    overall_experiment_questions[2][\"question\"], store_sales_df.columns, \"sales\"\n)\noverall_experiment_questions[3][\"sql_generated\"] = generate_sql_query(\n    overall_experiment_questions[3][\"question\"], store_sales_df.columns, \"sales\"\n)\noverall_experiment_questions[4][\"sql_generated\"] = generate_sql_query(\n    overall_experiment_questions[4][\"question\"], store_sales_df.columns, \"sales\"\n)\noverall_experiment_questions[5][\"sql_generated\"] = generate_sql_query(\n    overall_experiment_questions[5][\"question\"], store_sales_df.columns, \"sales\"\n)\noverall_experiment_questions[6][\"sql_generated\"] = generate_sql_query(\n    overall_experiment_questions[6][\"question\"], store_sales_df.columns, \"sales\"\n)\n\nprint(overall_experiment_questions[6])\n```\n\n----------------------------------------\n\nTITLE: Custom Evaluator Class for Substring Presence - Python\nDESCRIPTION: Implements a synchronous/asynchronous evaluator that assigns score based on the presence of a specified substring in the experiment output. Inputs: Example and ExperimentRun objects; substring (set at init). Outputs: EvaluationResult (score/explanation). Used for automatic labeling in experiment runs. Requires Phoenix experiment types.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/run_experiments_with_llama_index.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass ContainsSubstring:\n    name = \"contains_substring\"\n    annotator_kind = \"CODE\"\n\n    def __init__(self, substring: str):\n        self.substring = substring\n\n    def evaluate(self, _: Example, exp_run: ExperimentRun) -> EvaluationResult:\n        result = exp_run.output.result\n        score = int(isinstance(result, str) and self.substring in result)\n        return EvaluationResult(\n            score=score,\n            explanation=f\"the substring `{repr(self.substring)}` was in the output\",\n        )\n\n    async def async_evaluate(self, _: Example, exp_run: ExperimentRun) -> EvaluationResult:\n        return self.evaluate(_, exp_run)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting LlamaIndex and OpenAI with OpenInference (Python)\nDESCRIPTION: Registers a tracer provider pointing to the Phoenix endpoint (default 6006) and instruments the LlamaIndex and OpenAI libraries to automatically capture traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/run_experiments_with_llama_index.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\nOpenAIInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Printing Code Functionality Prompt Template\nDESCRIPTION: This snippet prints the default code functionality prompt template.  The template is used by the LLM to evaluate code functionality. The prompt includes variables `coding_instruction` and `code`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_functionality_classifications.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(CODE_FUNCTIONALITY_PROMPT_TEMPLATE)\n```\n\n----------------------------------------\n\nTITLE: Defining Gemini Evaluation Function and Running Phoenix Experiment in Python\nDESCRIPTION: Defines an evaluation function to send formatted prompts to Google Gemini and extract labels. Runs the experiment on the dataset to generate predictions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef gemini_eval(input):\n    formatted_prompt = prompt.format(variables=variables, sdk=\"google_generativeai\")\n    response = (\n        GenerativeModel(**{**formatted_prompt.kwargs, \"model_name\": gemini_model})\n        .start_chat(history=formatted_prompt.messages[:-1])\n        .send_message(formatted_prompt.messages[-1])\n    )\n    return {\"label\": response.candidates[0].content.parts[0].text}\n```\n\n----------------------------------------\n\nTITLE: Defining Tool Implementations - Python\nDESCRIPTION: This code defines a dictionary that maps the names of the tools defined earlier to their corresponding implementations.  `lookup_sales_data`, `analyze_sales_data`, and `generate_visualization` are assumed to be functions defined elsewhere in the code, providing the actual logic for each tool. This mapping allows the agent to call the correct function based on the tool name.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ntool_implementations = {\n    \"lookup_sales_data\": lookup_sales_data,\n    \"analyze_sales_data\": analyze_sales_data,\n    \"generate_visualization\": generate_visualization,\n    # \"run_python_code\": run_python_code\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Example String\nDESCRIPTION: This snippet creates a new column 'example' in a Pandas DataFrame by concatenating the 'input', 'output', and 'expected' columns into a single string. It then displays the first few rows of the updated DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nground_truth_df[\"example\"] = ground_truth_df.apply(\n    lambda row: f\"Input: {row['input']}\\nOutput: {row['output']}\\nExpected Output: {row['expected']}\",\n    axis=1,\n)\nground_truth_df.head()\n```\n\n----------------------------------------\n\nTITLE: Classifying Code Readability with GPT-3.5 and Post-Processing\nDESCRIPTION: Performs the same classification task using GPT-3.5-turbo model, demonstrating model flexibility; results are processed similarly as with GPT-4.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nrails = list(CODE_READABILITY_PROMPT_RAILS_MAP.values())\nreadability_classifications = llm_classify(\n    dataframe=df,\n    template=CODE_READABILITY_PROMPT_TEMPLATE,\n    model=OpenAIModel(model=\"gpt-3.5-turbo\", temperature=0.0),\n    rails=rails,\n    concurrency=20,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Instrumentation\nDESCRIPTION: This command installs the @arizeai/openinference-instrumentation-langchain package as a dependency to your project. This package provides automatic instrumentation for LangChain.js.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langchain.js.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install --save @arizeai/openinference-instrumentation-langchain\n```\n\n----------------------------------------\n\nTITLE: Defining the Phoenix OpenTelemetry Trace Provider (Python)\nDESCRIPTION: Imports the `register` function from `phoenix.otel`. It initializes and registers an OpenTelemetry TracerProvider configured to send trace data to the specified Phoenix project ('my-agent-app'). This `tracer_provider` object will be used to instrument the application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/smolagents_tracing_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\ntracer_provider = register(\n    project_name=\"my-agent-app\",  # Default is 'default'\n)\n```\n\n----------------------------------------\n\nTITLE: Setting the Global Tracer Provider for OpenTelemetry in Python\nDESCRIPTION: Sets the previously configured `TracerProvider` (containing both the conditional console and Phoenix exporters) as the global tracer provider for the application using `trace_api.set_tracer_provider`. All subsequent trace operations will use this provider.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/span_filtering_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Set the tracer provider\ntrace_api.set_tracer_provider(tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Calling OpenAI API to Extract Travel Request Attributes Using Function Calling in Python\nDESCRIPTION: Defines a function that sends travel request text to the OpenAI API with function calling enabled, forcing the model to return structured attributes conforming to the predefined schema. It captures the tool call arguments from the API response, effectively extracting the structured data as a JSON string. Requires an OpenAI client instance, a model name (defaulting to \"gpt-4o\"), and the function schema.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_tracing_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef extract_raw_travel_request_attributes_string(\n    travel_request: str,\n    tool_schema: Dict[str, Any],\n    system_message: str,\n    client: OpenAI,\n    model: str = \"gpt-4o\",\n) -> str:\n    chat_completion = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": travel_request},\n        ],\n        tools=[tool_schema],\n        # By default, the LLM will choose whether or not to call a function given the conversation context.\n        # The line below forces the LLM to call the function so that the output conforms to the schema.\n        tool_choice={\"type\": \"function\", \"function\": {\"name\": tool_schema[\"function\"][\"name\"]}},\n    )\n    tool_call = chat_completion.choices[0].message.tool_calls[0]\n    return tool_call.function.arguments\n```\n\n----------------------------------------\n\nTITLE: Uploading Evaluation Questions as a Versioned Dataset with Phoenix in Python\nDESCRIPTION: Uploads the question data as a versioned dataset to Phoenix for experiment tracking and retrieval. Uses px.Client().upload_dataset to upload a DataFrame, specifying input and output keys for later use in experiment evaluation. Dependencies: pandas and Phoenix client. Input: list of questions. Output: dataset object ds. If previously uploaded, px.Client().get_dataset(name=\"nba-questions\") can be used to retrieve it.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nds = px.Client().upload_dataset(\n    dataset_name=\"nba-questions\",\n    dataframe=pd.DataFrame([{\"question\": question} for question in questions]),\n    input_keys=[\"question\"],\n    output_keys=[],\n)\n\n# If you have already uploaded the dataset, you can fetch it using the following line\n# ds = px.Client().get_dataset(name=\"nba-questions\")\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix API Key and Endpoint - Python\nDESCRIPTION: Configures environment variables for authenticating and directing traces to the Phoenix Cloud using your API key. Requires Python's 'os' module. Set 'PHOENIX_CLIENT_HEADERS' with your API key and 'PHOENIX_COLLECTOR_ENDPOINT' with the Phoenix Cloud URL before running any instrumented code. Replace 'ADD YOUR API KEY' with your actual Phoenix API key.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/litellm.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n\n```\n\n----------------------------------------\n\nTITLE: Processing and Converting OpenAI LLM Input and Output for OpenInference (Python)\nDESCRIPTION: This code defines utility functions to transform OpenAI Chat API messages, tools, and response objects into OpenInference-compatible attributes using the openinference-instrumentation library. Dependencies: openai, opentelemetry, and openinference.instrumentation. Key functions include process_input (converts messages, model, tools, and parameters to input attributes), convert_openai_message_to_oi_message and convert_openai_tool_param_to_oi_tool (helpers for message/tool conversion), and process_output (extracts and formats output attributes and token usage from chat completions). Inputs are chat messages and tool parameters; output is a dictionary of attributes for LLM tracing and analysis. Limitations: assumes well-formed OpenAI message objects and function-typed tools.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom openai.types.chat import (\n    ChatCompletion,\n    ChatCompletionMessage,\n    ChatCompletionMessageParam,\n    ChatCompletionToolParam,\n)\nfrom opentelemetry.util.types import AttributeValue\n\nimport openinference.instrumentation as oi\nfrom openinference.instrumentation import (\n    get_input_attributes,\n    get_llm_attributes,\n    get_output_attributes,\n)\n\n\ndef process_input(\n    messages: List[ChatCompletionMessageParam],\n    model: str,\n    temperature: Optional[float] = None,\n    tools: Optional[List[ChatCompletionToolParam]] = None,\n    **kwargs: Any,\n) -> Dict[str, AttributeValue]:\n    oi_messages = [convert_openai_message_to_oi_message(message) for message in messages]\n    oi_tools = [convert_openai_tool_param_to_oi_tool(tool) for tool in tools or []]\n    return {\n        **get_input_attributes(\n            {\n                \"messages\": messages,\n                \"model\": model,\n                \"temperature\": temperature,\n                \"tools\": tools,\n                **kwargs,\n            }\n        ),\n        **get_llm_attributes(\n            provider=\"openai\",\n            system=\"openai\",\n            model_name=model,\n            input_messages=oi_messages,\n            invocation_parameters={\"temperature\": temperature},\n            tools=oi_tools,\n        ),\n    }\n\n\ndef convert_openai_message_to_oi_message(\n    message_param: Union[ChatCompletionMessageParam, ChatCompletionMessage],\n) -> oi.Message:\n    if isinstance(message_param, ChatCompletionMessage):\n        role: str = message_param.role\n        oi_message = oi.Message(role=role)\n        if isinstance(content := message_param.content, str):\n            oi_message[\"content\"] = content\n        if message_param.tool_calls is not None:\n            oi_tool_calls: List[oi.ToolCall] = []\n            for tool_call in message_param.tool_calls:\n                function = tool_call.function\n                oi_tool_calls.append(\n                    oi.ToolCall(\n                        id=tool_call.id,\n                        function=oi.ToolCallFunction(\n                            name=function.name,\n                            arguments=function.arguments,\n                        ),\n                    )\n                )\n            oi_message[\"tool_calls\"] = oi_tool_calls\n        return oi_message\n\n    role = message_param[\"role\"]\n    assert isinstance(message_param[\"content\"], str)\n    content = message_param[\"content\"]\n    return oi.Message(role=role, content=content)\n\n\ndef convert_openai_tool_param_to_oi_tool(tool_param: ChatCompletionToolParam) -> oi.Tool:\n    assert tool_param[\"type\"] == \"function\"\n    return oi.Tool(json_schema=dict(tool_param))\n\n\ndef process_output(response: ChatCompletion) -> Dict[str, AttributeValue]:\n    message = response.choices[0].message\n    role = message.role\n    oi_message = oi.Message(role=role)\n    if isinstance(message.content, str):\n        oi_message[\"content\"] = message.content\n    if isinstance(message.tool_calls, list):\n        oi_tool_calls: List[oi.ToolCall] = []\n        for tool_call in message.tool_calls:\n            tool_call_id = tool_call.id\n            function_name = tool_call.function.name\n            function_arguments = tool_call.function.arguments\n            oi_tool_calls.append(\n                oi.ToolCall(\n                    id=tool_call_id,\n                    function=oi.ToolCallFunction(\n                        name=function_name,\n                        arguments=function_arguments,\n                    ),\n                )\n            )\n        oi_message[\"tool_calls\"] = oi_tool_calls\n    output_messages = [oi_message]\n    token_usage = response.usage\n    oi_token_count: Optional[oi.TokenCount] = None\n    if token_usage is not None:\n        prompt_tokens = token_usage.prompt_tokens\n        completion_tokens = token_usage.completion_tokens\n        oi_token_count = oi.TokenCount(\n            prompt=prompt_tokens,\n            completion=completion_tokens,\n        )\n    return {\n        **get_llm_attributes(\n            output_messages=output_messages,\n            token_count=oi_token_count,\n        ),\n        **get_output_attributes(response),\n    }\n```\n\n----------------------------------------\n\nTITLE: Displaying First Few Rows of Training DataFrame (Python/Pandas)\nDESCRIPTION: Shows the first few rows of the training data DataFrame (`train_df`) using the `.head()` method. This allows for a quick inspection of the data's structure, columns (including image URLs, embeddings, predictions, and actual labels), and sample values.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/image_classification_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_df.head()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages for Phoenix and OpenAI Integration\nDESCRIPTION: Installs the necessary Python libraries including 'arize-phoenix' for Phoenix SDK, 'datasets' for dataset loading, and 'openinference-instrumentation-openai' for OpenAI telemetry instrumentation. Essential for setting up the environment to run subsequent code involving Phoenix and OpenAI integrations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q \"arize-phoenix>=8.0.0\" datasets openinference-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Simulating a Conversation with OpenAI LLM\nDESCRIPTION: Simulates a conversation with the OpenAI LLM by passing the previous response as a message from the assistant and then asking a follow-up question. The LLM receives the initial message with the images, the assistant's response, and the user's follow-up question along with the images.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_multimodal_image_reasoning.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmessage_2 = generate_openai_multi_modal_chat_message(\n    prompt=response_1.message.content,\n    role=\"assistant\",\n)\n\nmessage_3 = generate_openai_multi_modal_chat_message(\n    prompt=\"Can you tell me what the price of each spec as well?\",\n    role=\"user\",\n    image_documents=image_documents,\n)\nresponse_2 = openai_mm_llm.chat(\n    messages=[\n        message_1,\n        message_2,\n        message_3,\n    ],\n)\n\nprint(response_2)\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Execute Bedrock Agent with Tracing in Python\nDESCRIPTION: Defines a function `run` which sends input text to the Bedrock agent runtime client by invoking the agent with session and trace attributes enabled. The function streams and decodes response chunks and prints trace events to provide real-time feedback of agent execution and traced data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/amazon_bedrock_agents_tracing_and_evals.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef run(input_text):\n    session_id = f\"default-session1_{int(time.time())}\"\n\n    attributes = dict(\n        inputText=input_text,\n        agentId=AGENT_ID,\n        agentAliasId=AGENT_ALIAS_ID,\n        sessionId=session_id,\n        enableTrace=True,\n    )\n    response = bedrock_agent_runtime.invoke_agent(**attributes)\n\n    # Stream the response\n    for _, event in enumerate(response[\"completion\"]):\n        if \"chunk\" in event:\n            print(event)\n            chunk_data = event[\"chunk\"]\n            if \"bytes\" in chunk_data:\n                output_text = chunk_data[\"bytes\"].decode(\"utf8\")\n                print(output_text)\n        elif \"trace\" in event:\n            print(event[\"trace\"])\n```\n\n----------------------------------------\n\nTITLE: Implement Few-Shot CoT Task and Evaluation (Python)\nDESCRIPTION: Defines a task function `few_shot_COT_prompt` that calls the OpenAI API using the Few-Shot Phoenix prompt, parses the response to extract the final answer from the last line, similar to the Self-Consistency task. It includes the `evaluate_response` function again (identical to the previous version but included for completeness within this section's setup). It imports `nest_asyncio` and applies it before running an experiment with the dataset, the Few-Shot task function, and the evaluator.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/chain-of-thought-prompting.md#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ndef few_shot_COT_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **few_shot_COT.format(variables={\"Problem\": input[\"Word Problem\"]})\n    )\n    response_text = resp.choices[0].message.content.strip()\n    lines = response_text.split(\"\\n\")\n    final_answer = lines[-1].strip()\n    final_answer = re.sub(r\"^\\*\\*(\\d+)\\*\\*$\", r\"\\1\", final_answer)\n    return {\"full_response\": response_text, \"final_answer\": final_answer}\n\n\ndef evaluate_response(output, expected):\n    final_answer = output[\"final_answer\"]\n    if not final_answer.isdigit():\n        return False\n    return int(final_answer) == int(expected[\"Answer\"])\n\n\nimport nest_asyncio\n\nfrom phoenix.experiments import run_experiment\n\nnest_asyncio.apply()\n\ninitial_experiment = run_experiment(\n    dataset,\n    task=few_shot_COT_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Few-Shot COT Prompt\",\n    experiment_name=\"few-shot-cot-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + few_shot_COT.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Defining MistralAIModel Parameters in Python\nDESCRIPTION: This snippet defines the `MistralAIModel` class, inheriting from `BaseModel`. It specifies parameters for interacting with MistralAI models, including model name, temperature, top_p, random seed, response format, and safety modes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evaluation-models.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass MistralAIModel(BaseModel):\n    model: str = \"mistral-large-latest\"\n    temperature: float = 0\n    top_p: Optional[float] = None\n    random_seed: Optional[int] = None\n    response_format: Optional[Dict[str, str]] = None\n    safe_mode: bool = False\n    safe_prompt: bool = False\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Anthropic Client Python\nDESCRIPTION: Instruments the Anthropic client to emit telemetry data in OpenInference format. It uses `AnthropicInstrumentor` to instrument the client with a tracer and `register` to register the tracer with Phoenix, enabling monitoring and debugging of the API calls and interactions within the Phoenix platform.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/anthropic_tracing_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.anthropic import AnthropicInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(project_name=\"anthropic-tools\")\nAnthropicInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Defining Conditions for Span Filtering in Python\nDESCRIPTION: Defines two example functions, `console_condition` and `phoenix_condition`, to be used with `ConditionalSpanProcessor`. `console_condition` returns true if the span name contains 'console'. `phoenix_condition` returns true if the span does *not* meet the `console_condition` (i.e., its name doesn't contain 'console'). These functions determine the export destination.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/span_filtering_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Define conditions for sending spans to specific exporters\ndef console_condition(span):\n    return \"console\" in span.name  # Example: send to Console if \"console\" is in the span name\n\n\ndef phoenix_condition(span):\n    # return \"phoenix\" in span.name  # Example: send to Phoenix if \"phoenix\" is in the span name\n    return not console_condition(\n        span\n    )  # Example: send to Phoenix if \"console\" is not in the span name\n```\n\n----------------------------------------\n\nTITLE: Adding Session ID to Spans in Python\nDESCRIPTION: Demonstrates adding a `session.id` attribute to OpenTelemetry spans using the `using_session` context manager or decorator from `openinference.instrumentation`. Requires the `openinference-instrumentation` package and a non-empty string session ID. Spans created within the context or decorated function will inherit this attribute.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation import using_session\n\nwith using_session(session_id=\"my-session-id\"):\n    # Calls within this block will generate spans with the attributes:\n    # \"session.id\" = \"my-session-id\"\n    ...\n```\n\nLANGUAGE: python\nCODE:\n```\n@using_session(session_id=\"my-session-id\")\ndef call_fn(*args, **kwargs):\n    # Calls within this function will generate spans with the attributes:\n    # \"session.id\" = \"my-session-id\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Evaluating GPT-4-Turbo Hallucination Detection Performance\nDESCRIPTION: Compares GPT-4-Turbo's hallucination classifications against ground-truth labels, generating classification metrics and visualizing a confusion matrix to assess model performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nprint(classification_report(true_labels, hallucination_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels,\n    predict_vector=hallucination_classifications,\n    classes=rails,\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Using a Prompt in TypeScript to Format and Send to OpenAI\nDESCRIPTION: Demonstrates how to convert a prompt with variables into SDK-specific format and make an OpenAI chat completion request using the Phoenix SDK in TypeScript. Supports customizing parameters like model and streaming options.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/how-to-prompts/using-a-prompt.md#_snippet_7\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { getPrompt, toSDK } from \"@arizeai/phoenix-client/prompts\";\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI()\nconst prompt = await getPrompt({ name: \"my-prompt\" });\n\n// openaiParameters is fully typed, and safe to use directly in the openai client\nconst openaiParameters = toSDK({\n  // sdk does not have to match the provider saved in your prompt\n  sdk: \"openai\",\n  prompt: questionAskerPrompt,\n  // variables within your prompt template can be replaced across messages\n  variables: { question: \"How do I write 'Hello World' in JavaScript?\" }\n});\n\nconst response = await openai.chat.completions.create({\n  ...openaiParameters,\n  // you can still override any of the invocation parameters as needed\n  // for example, you can change the model or stream the response\n  model: \"gpt-4o-mini\",\n  stream: false\n})\n```\n\n----------------------------------------\n\nTITLE: Temporarily Pausing Phoenix Tracing with Context Manager in Python\nDESCRIPTION: Illustrates how to temporarily pause tracing for specific code sections using the `suppress_tracing` context manager from `phoenix.trace`. Code executed within the `with` block will not generate traces, useful for excluding processes like document chunking or LLM evaluations. Tracing automatically resumes after the block.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/concepts-tracing/faqs-tracing.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import suppress_tracing\n\nwith suppress_tracing():\n    # Code running inside this block doesn't generate traces.\n    # For example, running LLM evals here won't generate additional traces.\n    ...\n# Tracing will resume outside the block.\n...\n```\n\n----------------------------------------\n\nTITLE: Waiting for Data Availability (Python)\nDESCRIPTION: Pauses execution for a brief period (2 seconds) to allow tracing data from the indexing process to be fully ingested and made available within the Phoenix application before attempting to query it.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom time import sleep\n\n# Wait a little bit in case data hasn't beomme fully available\nsleep(2)\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary Python Modules\nDESCRIPTION: Imports standard libraries (`json`, `functools`, `textwrap`, `time`, `typing`), asynchronous utilities (`nest_asyncio`), data handling (`datasets`), Llama-Index components (`GuidelineEvaluator`, `OpenAI`), OpenInference instrumentation (`LlamaIndexInstrumentor`), and Phoenix components (`px`, `evaluate_experiment`, `run_experiment`, `Explanation`, `Score`, `register`). `nest_asyncio.apply()` is called to allow nested asyncio event loops, often needed in notebook environments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom functools import partial\nfrom textwrap import shorten\nfrom time import time_ns\nfrom typing import Tuple\n\nimport nest_asyncio\nfrom datasets import load_dataset\nfrom llama_index.core.evaluation import GuidelineEvaluator\nfrom llama_index.llms.openai import OpenAI\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nimport phoenix as px\nfrom phoenix.experiments import evaluate_experiment, run_experiment\nfrom phoenix.experiments.types import Explanation, Score\nfrom phoenix.otel import register\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix - Bash\nDESCRIPTION: Installs the arize-phoenix package, which is needed for launching Phoenix locally or in a notebook. This package includes the necessary components to interact with the Phoenix platform.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Displaying the Generated Prompt\nDESCRIPTION: This snippet simply displays the value of the `new_prompt` variable generated in the previous code snippet.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nnew_prompt\n```\n\n----------------------------------------\n\nTITLE: Loading LlamaIndex from Google Cloud Storage and Initializing Models in Python\nDESCRIPTION: Connects to Google Cloud Storage via gcsfs to access a persisted LlamaIndex data index. Initializes default storage context with GCS filesystem and directory path. Configures LlamaIndex Settings to use OpenAI models: GPT-4 Turbo Preview for LLM queries and text-embedding-ada-002 for embeddings. Loads an existing LlamaIndex from persistent storage and creates a query engine interface for Q&A retrieval.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom gcsfs import GCSFileSystem\nfrom llama_index.core import (\n    Settings,\n    StorageContext,\n    load_index_from_storage,\n)\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\n\nimport phoenix as px\n\nfile_system = GCSFileSystem(project=\"public-assets-275721\")\nindex_path = \"arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/index/\"\nstorage_context = StorageContext.from_defaults(\n    fs=file_system,\n    persist_dir=index_path,\n)\n\nSettings.llm = OpenAI(model=\"gpt-4-turbo-preview\")\nSettings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\nindex = load_index_from_storage(\n    storage_context,\n)\nquery_engine = index.as_query_engine()\n```\n\n----------------------------------------\n\nTITLE: Installing OpenInference Bedrock Instrumentation\nDESCRIPTION: This command installs the necessary packages for instrumenting AWS Bedrock with OpenInference, including the bedrock instrumentation package and the OpenTelemetry OTLP exporter.  The OTLP exporter sends trace data to a configured backend.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/bedrock.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-bedrock opentelemetry-exporter-otlp\n```\n\n----------------------------------------\n\nTITLE: Submitting Customer Queries to LLM Application\nDESCRIPTION: Iterates through customer input data, converts each row to JSON, and calls the LLM application function for each customer query. Results are formatted and displayed in the notebook.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nfor _, row in customer_inputs_df.iterrows():\n    row_json = row.to_json()\n    result = run_llm_app(row_json, customer_intent_prompt, tracer=tracer)\n    pretty_print_result(result)\n```\n\n----------------------------------------\n\nTITLE: Logging Filtered Traces Back to Phoenix with TraceDataset in Python\nDESCRIPTION: Imports and uses TraceDataset to log previously filtered and saved traces (from Parquet) back into Phoenix. The input is a Parquet file with trace data, and px.Client().log_traces stores the traces for visualization and analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix import TraceDataset\n\npx.Client().log_traces(TraceDataset(pd.read_parquet(\"demo_traces.parquet\")))\n```\n\n----------------------------------------\n\nTITLE: Filtering DataFrame for Valid Questions (Pandas)\nDESCRIPTION: This snippet filters a Pandas DataFrame named `questions_with_document_chunk_df` to remove rows where the 'question' column has null values. This ensures that only valid questions are used in subsequent evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nquestions_with_document_chunk_df = questions_with_document_chunk_df[\n    questions_with_document_chunk_df[\"question\"].notnull()\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Cloud environment variables\nDESCRIPTION: Python code to set environment variables for Phoenix Cloud authentication and endpoint configuration. Requires an API key from the Phoenix dashboard.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/dspy.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Query for Extracting Specific Message Content from ExperimentRun Output in Python\nDESCRIPTION: This snippet performs an asynchronous database query to extract and print the first message's content from the 'task_output' field within the output JSON column of the ExperimentRun model. It demonstrates label aliasing in selection and iterates over query results to print non-empty values.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/testing/experiment_runs_filters.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nasync with engine.connect() as conn:\n    results = await conn.execute(\n        select(models.ExperimentRun.output[\"task_output\"][\"messages\"][0][\"content\"].label(\"value\")),\n    )\n    for result in results:\n        if result.value:\n            print(result.value)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Logged Data as DataFrames from Phoenix Python\nDESCRIPTION: Fetches logged tracing data and specific retrieval-related information (retrieved documents and QA pairs) from the active Phoenix session into pandas DataFrames. This prepares the data for subsequent evaluation steps.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/retrieval/quickstart-retrieval.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Get traces from Phoenix into dataframe \n\nspans_df = px.active_session().get_spans_dataframe()\nspans_df[[\"name\", \"span_kind\", \"attributes.input.value\", \"attributes.retrieval.documents\"]].head()\n\nfrom phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n\nretrieved_documents_df = get_retrieved_documents(px.active_session())\nqueries_df = get_qa_with_reference(px.active_session())\n\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Instrumentation for Automatic Tracing\nDESCRIPTION: Installation of the OpenAI-specific instrumentation package that enables automatic tracing of all OpenAI API calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-ts.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# npm, pnpm, yarn, etc\nnpm install openai @arizeai/openinference-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Running Sample Queries through LlamaIndex Engine\nDESCRIPTION: Iterates through the first 5 queries stored in the `queries` list. For each query, it calls the `query` method of the previously created `query_engine`. A `tqdm` progress bar is used to show the execution progress. Each call to `query_engine.query` generates trace data that is automatically sent to the running Phoenix instance due to the prior instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfor query in tqdm(queries[:5]):\n    query_engine.query(query)\n```\n\n----------------------------------------\n\nTITLE: Accessing Retrieved Node Text in RAG Pipeline - Python\nDESCRIPTION: This snippet demonstrates how to access the raw text content of a specific source node from a RAG query response. It targets the text of the second retrieved document node (index 1), typically obtained from a response object containing structured information about the retrieval process.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresponse_vector.source_nodes[1].get_text()\n```\n\n----------------------------------------\n\nTITLE: Constructing a QueryEngine with Cohere Rerank Postprocessor in Python\nDESCRIPTION: Initializes a CohereRerank postprocessor using an API key and incorporates it into a Llama-Index QueryEngine configured to return the top-5 similar chunks and rerank with Cohere. Inputs are the vector index and credentials. The QueryEngine serves for querying with reranked results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.postprocessor.cohere_rerank import CohereRerank\n\ncohere_api_key = os.environ[\"COHERE_API_KEY\"]\ncohere_rerank = CohereRerank(api_key=cohere_api_key, top_n=2)\n\nquery_engine = vector_index.as_query_engine(\n    similarity_top_k=5,\n    node_postprocessors=[cohere_rerank],\n)\n```\n\n----------------------------------------\n\nTITLE: Querying and Printing Experiment Runs with Optional Filtering using SQLAlchemy Async in Python\nDESCRIPTION: Defines an asynchronous function to query experiment runs for a given baseline experiment and comparison experiment IDs. The function builds a complex SQLAlchemy query joining several tables including Experiment, DatasetExample, DatasetExampleRevision, ExperimentRun, and ExperimentRunAnnotation to retrieve relevant dataset examples. An optional filter_condition parameter allows filtering on the SQL query with arbitrary binary expressions. The results are ordered and printed by example ID. It uses asynchronous connection management with an SQLAlchemy async engine and relies on Phoenix DB ORM models. This function facilitates detailed fetching of experiment data with flexible filtering for monitoring or debugging experiment runs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/testing/experiment_runs_filters.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nasync def print_experiment_runs(\n    baseline_experiment_id: int,\n    compare_experiment_ids: list[int],\n    filter_condition: Optional[BinaryExpression[Any]] = None,\n) -> None:\n    async with engine.connect() as conn:\n        baseline_experiment = (\n            await conn.execute(\n                select(models.Experiment).where(models.Experiment.id == baseline_experiment_id)\n            )\n        ).first()\n        assert baseline_experiment is not None\n        dataset_id = baseline_experiment.dataset_id\n        version_id = baseline_experiment.dataset_version_id\n\n        revision_ids = (\n            select(func.max(models.DatasetExampleRevision.id))\n            .join(\n                models.DatasetExample,\n                models.DatasetExample.id == models.DatasetExampleRevision.dataset_example_id,\n            )\n            .where(\n                and_(\n                    models.DatasetExampleRevision.dataset_version_id <= version_id,\n                    models.DatasetExample.dataset_id == dataset_id,\n                )\n            )\n            .group_by(models.DatasetExampleRevision.dataset_example_id)\n            .scalar_subquery()\n        )\n        examples = (\n            select(models.DatasetExample)\n            .join(\n                models.DatasetExampleRevision,\n                models.DatasetExample.id == models.DatasetExampleRevision.dataset_example_id,\n            )\n            .join(\n                models.ExperimentRun,\n                onclause=models.ExperimentRun.dataset_example_id == models.DatasetExample.id,\n            )\n            .join(\n                models.ExperimentRunAnnotation,\n                onclause=models.ExperimentRunAnnotation.experiment_run_id\n                == models.ExperimentRun.id,\n                isouter=True,\n            )\n            .where(\n                and_(\n                    models.DatasetExampleRevision.id.in_(revision_ids),\n                    models.DatasetExampleRevision.revision_kind != \"DELETE\",\n                )\n            )\n            .order_by(models.DatasetExampleRevision.dataset_example_id.desc())\n        )\n        if filter_condition is not None:\n            examples = examples.where(filter_condition)\n        print(examples.compile(compile_kwargs={\"literal_binds\": True}))\n        results = await conn.execute(examples)\n        for result in results:\n            print(result.id)\n```\n\n----------------------------------------\n\nTITLE: Analyze and Combine Results (Python)\nDESCRIPTION: This Python snippet combines the original dataset with the evaluation results, including labels and explanations. It creates a copy of the original DataFrame and adds new columns from the evaluation dataframes, namely, hallucination evaluation results (`hallucination_eval`, `hallucination_explanation`) and question answering evaluation results (`qa_eval`, `qa_explanation`).  Finally it displays the first few rows of the combined result DataFrame using `head()`.  This allows for a comprehensive view of the original data alongside the evaluation metrics.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/quickstart/evals.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresults_df = df.copy()\nresults_df[\"hallucination_eval\"] = hallucination_eval_df[\"label\"]\nresults_df[\"hallucination_explanation\"] = hallucination_eval_df[\"explanation\"]\nresults_df[\"qa_eval\"] = qa_eval_df[\"label\"]\nresults_df[\"qa_explanation\"] = qa_eval_df[\"explanation\"]\nresults_df.head()\n```\n\n----------------------------------------\n\nTITLE: Test Evaluators on Sample Run Python\nDESCRIPTION: Retrieves a sample experiment run and its corresponding dataset example. It then iterates through the defined 'evaluators' dictionary. For each evaluator, it calls the evaluation function asynchronously with the sample run's output and the example's input. The resulting evaluation score and feedback are printed (shortened) to confirm the evaluators are configured and working as expected.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nrun = experiment[0]\nexample = dataset.examples[run.dataset_example_id]\nfor name, fn in evaluators.items():\n    _ = await fn(run.output, example.input)\n    print(name)\n    print(shorten(json.dumps(_), width=80))\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Model Response\nDESCRIPTION: This code snippet takes the model's response, splits it into individual math problems, and creates a pandas DataFrame. It removes any empty questions from the DataFrame and prints the resulting DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/ragas_agents_cookbook_phoenix.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsplit_response = resp.strip().split(\"\\n\")\nmath_problems_df = pd.DataFrame(split_response, columns=[\"question\"])\nmath_problems_df = math_problems_df[math_problems_df[\"question\"].str.strip() != \"\"]\n\nprint(math_problems_df)\n```\n\n----------------------------------------\n\nTITLE: Running Code Functionality Classifications with GPT-3.5 Turbo\nDESCRIPTION: This code snippet runs code functionality classifications on the DataFrame using the `llm_classify` function and the GPT-3.5 Turbo model. It extracts the 'label' column from the results and converts it to a list.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_functionality_classifications.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nrails = list(CODE_FUNCTIONALITY_PROMPT_RAILS_MAP.values())\nrelevance_classifications = llm_classify(\n    dataframe=df,\n    template=CODE_FUNCTIONALITY_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    concurrency=20,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Cloud Endpoint and API Key (Python)\nDESCRIPTION: Sets environment variables required to connect to Arize Phoenix Cloud. `PHOENIX_API_KEY` stores your unique API key (obtained from the Phoenix dashboard), and `PHOENIX_COLLECTOR_ENDPOINT` specifies the cloud endpoint URL. These variables configure the `arize-phoenix-otel` package.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/haystack.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Defining RAG Task Function for Experiment (Python)\nDESCRIPTION: Defines a Python function `rag_with_reranker` that takes an input dictionary (from the dataset), converts the user message to a query, initializes a LlamaIndex chat engine with the index and reranker, performs the chat query, and returns the response string.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/run_experiments_with_llama_index.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef rag_with_reranker(input) -> str:\n    chat_engine = index.as_chat_engine(similarity_top_k=10, node_postprocessors=[reranker])\n    response = chat_engine.chat(input[\"input_messages\"][-1][\"content\"])\n    return str(response)\n```\n\n----------------------------------------\n\nTITLE: Fetching Q&A Data with Phoenix get_qa_with_reference\nDESCRIPTION: Imports the `get_qa_with_reference` helper function from `phoenix.session.evaluation` and uses it to retrieve Question and Answer data along with reference information from a specified Phoenix project. Requires an active Phoenix client (`px.Client()`) instance and the project name (`phoenix_project_name`). The fetched data is stored in the `qa_with_reference_df` DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_qa_with_reference\n\n# Get the Question and Answer with reference data from Phoenix using this helper function\nqa_with_reference_df = get_qa_with_reference(px.Client(), project_name=phoenix_project_name)\nqa_with_reference_df\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries Python\nDESCRIPTION: Imports the required Python libraries. These libraries include `os` and `getpass` for environment variable and password handling; `typing` for type hinting; `pandas` for data manipulation; and `phoenix` for interacting with Phoenix and visualization, `AnthropicInstrumentor` for instrumenting the Anthropic client and `register` for registering the instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/anthropic_tracing_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\nfrom typing import Any, Dict\n\nimport pandas as pd\n\nimport phoenix as px\n\npd.set_option(\"display.max_colwidth\", None)\n```\n\n----------------------------------------\n\nTITLE: Extracting Retrieved Documents from Phoenix (RAG)\nDESCRIPTION: This snippet extracts retrieved documents from a Phoenix active session using the `get_retrieved_documents` function from the `phoenix.session.evaluation` module. It depends on the `phoenix` library. The resulting DataFrame contains information about the retrieved documents, their scores, and related trace IDs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_retrieved_documents\n\nretrieved_documents_df = get_retrieved_documents(px.active_session())\nretrieved_documents_df\n```\n\n----------------------------------------\n\nTITLE: Importing libraries for data handling and indexing with LlamaIndex\nDESCRIPTION: Imports necessary modules from pandas, LlamaIndex, and Phoenix for data loading, processing, and tracing instrumentation. Prepares the environment for loading data, building indices, and instrumenting tracing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.node_parser import SimpleNodeParser\nfrom llama_index.llms.openai import OpenAI\n\nimport phoenix as px\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Phoenix Inferences Dataset in Python\nDESCRIPTION: Demonstrates the basic instantiation of a Phoenix `Inferences` dataset using a pandas DataFrame (`df`) and a defined `Schema` (`schema`). Optionally, a `name` parameter can be provided to assign a name to the dataset for identification in the Phoenix UI. Requires the `phoenix` library (imported as `px`) and a pre-existing pandas DataFrame and `px.Schema` object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/define-your-schema/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nds = px.Inferences(df, schema)\n```\n\nLANGUAGE: python\nCODE:\n```\nds = px.Inferences(df, schema, name=\"training\")\n```\n\n----------------------------------------\n\nTITLE: Viewing Relevance Evaluation Results\nDESCRIPTION: Displays the head of the dataframe containing relevance evaluations for retrieved documents.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nretrieved_documents_relevance_df.head()\n```\n\n----------------------------------------\n\nTITLE: Evaluation Function for Self-Consistency CoT Prompt\nDESCRIPTION: Defines a function that sends the math problem to the OpenAI API using the self-consistency prompt, extracts the final answer, and evaluates if it matches the expected solution. Used to run experiments with the Phoenix framework.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/chain_of_thought_prompting.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef self_consistency_COT_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **self_consistency_COT.format(variables={\"Problem\": input[\"Word Problem\"]})\n    )\n    response_text = resp.choices[0].message.content.strip()\n    lines = response_text.split(\"\\n\")\n    final_answer = lines[-1].strip()\n    final_answer = re.sub(r\"^\\*\\*(\\d+)\\*\\*$\", r\"\\1\", final_answer)\n    return {\"full_response\": response_text, \"final_answer\": final_answer}\n\n\ndef evaluate_response(output, expected):\n    final_answer = output[\"final_answer\"]\n    if not final_answer.isdigit():\n        return False\n    return int(final_answer) == int(expected[\"Answer\"])\n\n\ninitial_experiment = run_experiment(\n    dataset,\n    task=self_consistency_COT_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Self Consistency COT Prompt\",\n    experiment_name=\"self-consistency-cot-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + self_consistency_COT.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Printing Experiment Question at Index Using Python\nDESCRIPTION: This snippet prints the seventh entry in the overall_experiment_questions list (assumed to be a collection of question objects or dicts). It requires the overall_experiment_questions variable to be defined and populated with objects supporting indexing. The expected input is that overall_experiment_questions[6] returns a printable object, and the output is the printed representation of that object. There are no external dependencies beyond standard Python.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_56\n\nLANGUAGE: Python\nCODE:\n```\nprint(overall_experiment_questions[6])\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Phoenix and LlamaIndex\nDESCRIPTION: Installs the necessary Python packages including Phoenix, LlamaIndex components, and OpenInference instrumentation to build a traced LLM application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qq arize-phoenix==7.3.2 llama-index-core==0.12.8 llama-index-llms-ollama==0.5.0 llama-index-embeddings-ollama==0.5.0 llama-index-readers-file==0.4.1 gcsfs==2024.12.0 nest_asyncio==1.6.0 openinference-instrumentation-llama_index==3.1.2 litellm==1.55.12\n```\n\n----------------------------------------\n\nTITLE: Loading Math Word Problems Dataset and Uploading to Phoenix in Python\nDESCRIPTION: Loads the 'syeddula/math_word_problems' dataset using the datasets library and converts it to a pandas DataFrame. Generates a unique UUID to name the dataset uniquely for tracking within Phoenix. Uploads this dataset to Phoenix using the SDK client, specifying input and output columns, allowing Phoenix to manage and version datasets relevant for prompt experiments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/chain_of_thought_prompting.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n\nfrom datasets import load_dataset\n\nimport phoenix as px\nfrom phoenix.client import Client as PhoenixClient\n\nds = load_dataset(\"syeddula/math_word_problems\")[\"train\"]\nds = ds.to_pandas()\nds.head()\n\nunique_id = uuid.uuid4()\n\n# Upload the dataset to Phoenix\ndataset = px.Client().upload_dataset(\n    dataframe=ds,\n    input_keys=[\"Word Problem\"],\n    output_keys=[\"Answer\"],\n    dataset_name=f\"wordproblems-{unique_id}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean of Eval Results\nDESCRIPTION: Calculates and prints the mean score for QA correctness and hallucination evaluation results. This provides an aggregate view of the LLM's performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nqa_correctness_eval_df.mean(numeric_only=True)\n```\n\nLANGUAGE: python\nCODE:\n```\nhallucination_eval_df.mean(numeric_only=True)\n```\n\n----------------------------------------\n\nTITLE: Defining Explicitly Excluded Columns with px.Schema in Python\nDESCRIPTION: This snippet demonstrates how to create a px.Schema object in Python specifying which columns are prediction and actual labels, while explicitly excluding certain columns from being considered model features. Dependencies include the Phoenix library (imported as px). The excluded_column_names parameter is set to a list of string column names such as \"hospital\" and \"insurance_provider\". The output is a configured Schema object, and any columns not listed as excluded may be implicitly inferred as features. Ensure that the DataFrame columns match the names provided.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/define-your-schema/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nschema = px.Schema(\n    prediction_label_column_name=\"predicted\",\n    actual_label_column_name=\"target\",\n    excluded_column_names=[\n        \"hospital\",\n        \"insurance_provider\",\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix Packages (Docker)\nDESCRIPTION: This command installs the `arize-phoenix-otel` package, which is necessary to enable tracing with Arize Phoenix. This package provides OpenTelemetry instrumentation for sending traces to the Arize Phoenix platform.  It is a prerequisite for tracing LlamaIndex workflows within Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex-1.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting up Environment\nDESCRIPTION: Imports necessary modules from standard library, third-party libraries, and local modules for the agent implementation. Sets up pandas display options and applies nest_asyncio to enable asynchronous operations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Standard library imports\nimport re\n\n# Third-party library imports\nfrom typing import Any, Dict, List, Optional, Set, Tuple, cast\n\nimport nest_asyncio\nimport pandas as pd\n\n# Local module imports\nfrom llama_index.core import SQLDatabase\nfrom llama_index.core.agent import (\n    AgentChatResponse,\n    AgentRunner,\n    QueryPipelineAgentWorker,\n    ReActChatFormatter,\n    Task,\n)\nfrom llama_index.core.agent.react.output_parser import ReActOutputParser\nfrom llama_index.core.agent.react.types import (\n    ObservationReasoningStep,\n    ResponseReasoningStep,\n)\nfrom llama_index.core.llms import ChatMessage, MessageRole\nfrom llama_index.core.query_engine import NLSQLTableQueryEngine\nfrom llama_index.core.query_pipeline import (\n    AgentFnComponent,\n    AgentInputComponent,\n    CustomAgentComponent,\n    QueryComponent,\n    QueryPipeline,\n    ToolRunnerComponent,\n)\nfrom llama_index.core.tools import BaseTool, QueryEngineTool\nfrom llama_index.llms.openai import OpenAI\nfrom sqlalchemy import create_engine\nfrom tqdm import tqdm\n\nimport phoenix as px\n\n# Apply settings and initializations\npd.set_option(\"display.max_colwidth\", 1000)\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Sample Invocations of Amazon Bedrock Agent using Run Function in Python\nDESCRIPTION: Executes the defined `run` function with sample natural language input prompts to instruct the Bedrock agent to retrieve information about recent Phoenix traces, how to run Phoenix evaluations, and recent Phoenix experiments. These calls illustrate how the agent can be queried interactively.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/amazon_bedrock_agents_tracing_and_evals.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nrun(\"Tell me about my recent Phoenix traces\")\n```\n\nLANGUAGE: python\nCODE:\n```\nrun(\"How do I run evaluations in Arize Phoenix?\")\n```\n\nLANGUAGE: python\nCODE:\n```\nrun(\"Tell me about my recent Phoenix experiments\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Phoenix Tracing with Environment Variables in Python\nDESCRIPTION: Shows how to configure a LangChain application to send traces to a remote Phoenix instance using `PHOENIX_HOST` and `PHOENIX_PORT` environment variables. Imports `os` and `LangChainInstrumentor`, sets the environment variables for the remote host and port, and then instruments the application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/concepts-tracing/faqs-tracing.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom phoenix.trace import LangChainInstrumentor\n\n# assume phoenix is running at 162.159.135.42:6007\nos.environ[\"PHOENIX_HOST\"] = \"162.159.135.42\"\nos.environ[\"PHOENIX_PORT\"] = \"6007\"\n\nLangChainInstrumentor().instrument()  # logs to http://162.159.135.42:6007\n\n# run your LangChain application\n```\n\n----------------------------------------\n\nTITLE: Defining Test Function\nDESCRIPTION: This code defines a `test_prompt` function that interacts with the OpenAI API using the meta_prompt_result. It sends the input prompt to OpenAI and returns the generated completion. This will be the function passed to an experiment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef test_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **meta_prompt_result.format(variables={\"prompt\": input[\"prompt\"]})\n    )\n    return resp.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Creating Example Column in DataFrame\nDESCRIPTION: This code snippet creates a new column named 'example' in the 'ground_truth_df' DataFrame. It uses the 'apply' method with a lambda function to generate a string for each row. The string concatenates the 'input', 'output', and 'expected' values from the row, formatted with specific labels and newlines. This is likely used to create a dataset where each example is represented as a single string.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nground_truth_df[\"example\"] = ground_truth_df.apply(\n    lambda row: f\"Input: {row['input']}\\nOutput: {row['output']}\\nExpected Output: {row['expected']}\",\n    axis=1,\n)\nground_truth_df.head()\n```\n\n----------------------------------------\n\nTITLE: Defining Embedding Features in Phoenix Schema\nDESCRIPTION: This Python code shows how to define embedding features in Phoenix by specifying the vector data column. It creates a schema with transaction_embeddings as an embedding feature that uses the embedding_vector column from the dataframe as its source.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nschema = px.Schema(\n    prediction_label_column_name=\"predicted\",\n    actual_label_column_name=\"target\",\n    embedding_feature_column_names={\n        \"transaction_embeddings\": px.EmbeddingColumnNames(\n            vector_column_name=\"embedding_vector\"\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Few-Shot Prompt Template and Registering in Arize Phoenix Using Python\nDESCRIPTION: Defines a multi-line string template that instructs an evaluator model to classify sentiment as positive, negative, or neutral, prefaced by several labeled examples for few-shot prompting. It then constructs parameter objects compatible with OpenAI's GPT-3.5-turbo chat completion API and registers this prompt with the Phoenix client to version-control and track prompt iterations. Dependencies include the PhoenixClient class and CompletionCreateParamsBase for prompt parameters.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/few-shot-prompting.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfew_shot_template = \"\"\"\n\"You are an evaluator who assesses the sentiment of a review. Output if the review positive, negative, or neutral. Only respond with one of these classifications.\"\n\nHere are examples of a review and the sentiment:\n\n{examples}\n\"\"\"\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"system\", \"content\": few_shot_template.format(examples=few_shot_examples)},\n        {\"role\": \"user\", \"content\": \"{{Review}}\"},\n    ],\n)\n\nfew_shot_prompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Few-shot prompt for classifying reviews based on sentiment.\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenTelemetry Tracer for Deno\nDESCRIPTION: This JavaScript snippet initializes the OpenTelemetry tracer using NodeTracerProvider, OTLPTraceExporter, and SimpleSpanProcessor. It sets the project name as 'deno-langchain' and configures the exporter to send trace data to a local Phoenix server running at localhost:6006. It registers the provider to start tracing. This is a prerequisite for sending trace data to the Phoenix server for monitoring and analysis.  Dependencies include @opentelemetry/sdk-trace-node, @opentelemetry/resources, and @opentelemetry/exporter-trace-otlp-proto.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/langchain/tracing_langchain_node_tutorial.ipynb#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport {\n  NodeTracerProvider,\n  SimpleSpanProcessor,\n} from \"@opentelemetry/sdk-trace-node\";\nimport { Resource } from \"@opentelemetry/resources\";\nimport { OTLPTraceExporter } from \"@opentelemetry/exporter-trace-otlp-proto\";\nimport { SEMRESATTRS_PROJECT_NAME } from \"@arizeai/openinference-semantic-conventions\";\n\nconst provider = new NodeTracerProvider({\n  resource: new Resource({\n    [SEMRESATTRS_PROJECT_NAME]: \"deno-langchain\",\n  }),\n});\n\nprovider.addSpanProcessor(\n  new SimpleSpanProcessor(\n    new OTLPTraceExporter({\n      url: \"http://localhost:6006/v1/traces\",\n    })\n  )\n);\n\nprovider.register();\n\nconsole.log(\"ðŸ‘€ OpenInference initialized\");\n```\n\n----------------------------------------\n\nTITLE: Defining LangChain SQLDatabaseToolkit Tools\nDESCRIPTION: Initializes a `SQLDatabaseToolkit` using the previously created `db` instance and a `ChatOpenAI` language model. It retrieves the standard `sql_db_list_tables` and `sql_db_schema` tools provided by the toolkit and demonstrates invoking them.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\nfrom langchain_openai import ChatOpenAI\n\ntoolkit = SQLDatabaseToolkit(db=db, llm=ChatOpenAI(model=\"gpt-4o\"))\ntools = toolkit.get_tools()\n\nlist_tables_tool = next(tool for tool in tools if tool.name == \"sql_db_list_tables\")\nget_schema_tool = next(tool for tool in tools if tool.name == \"sql_db_schema\")\n\nprint(list_tables_tool.invoke(\"\"))\n\nprint(get_schema_tool.invoke(\"Artist\"))\n```\n\n----------------------------------------\n\nTITLE: Loading Documents with LlamaIndex (Python)\nDESCRIPTION: Uses LlamaIndex's `SimpleDirectoryReader` to load documents from the specified local directory (`./prompt-engineering-papers`). `num_files_limit` is set to 2 for demonstration purposes, limiting the number of files processed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import SimpleDirectoryReader\n\ndir_path = \"./prompt-engineering-papers\"\nreader = SimpleDirectoryReader(dir_path, num_files_limit=2)\ndocuments = reader.load_data()\n```\n\n----------------------------------------\n\nTITLE: Enabling OpenInference Tracing for LlamaIndex Instrumentation in Python\nDESCRIPTION: Imports the LlamaIndexInstrumentor from the OpenInference SDK and Phoenix's OpenTelemetry register function. Registers a tracer provider with the local Phoenix collector endpoint and instruments all LlamaIndex calls to produce OpenInference-compatible tracing spans for observability. The instrumentation is enabled with dependency checks skipped.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_sql_retriever_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Context String\nDESCRIPTION: Defines a multi-line string that serves as the system prompt or context for the ReAct agent. It instructs the agent on its persona, capabilities (access to two databases via tools), and guidelines for answering questions, including using available context and handling inability to answer.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/agentic_rag_tracing.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nCONTEXT = \"\"\"\nYou are a chatbot designed to answer questions about the company's employees and policies.\nYou have access to a Chroma database with information about the company's employees and their departmental information.\nYou also have access to a Chroma database with information about the company's policies. Use provided context to help answer\nthe question. Make sure that you have all the context required to answer the question and if you don't, check if there are\nother tools that can help you answer the question. If you still can't answer the question, ask the user for more information and\napologize that you can't answer.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Performing Relevance Classification with Phoenix Eval Functions in Python\nDESCRIPTION: Samples 100 entries from the downloaded dataset and renames columns to match expected input format. Defines a set of prompt rails and runs the 'llm_classify' function with a relevancy prompt template and the local LiteLLM model. This concurrenctly performs classification with up to 20 parallel calls, producing a DataFrame with classification results for evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n    RAG_RELEVANCY_PROMPT_TEMPLATE,\n    llm_classify,\n)\n\nN_EVAL_SAMPLE_SIZE = 100\n\ndf_sample = df.sample(n=N_EVAL_SAMPLE_SIZE)\n\ndf_sample = df_sample.rename(\n    columns={\n        \"query_text\": \"input\",\n        \"document_text\": \"reference\",\n    },\n)\n\nrails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\n\nrelevance_df = llm_classify(\n    dataframe=df_sample,\n    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    concurrency=20,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Sample Document Chunks Dataframe\nDESCRIPTION: Constructs a dataframe of document chunks from nodes and samples 10 random entries for testing purposes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndocument_chunks_df = pd.DataFrame({\"text\": [node.get_text() for node in nodes]})\ndocument_chunks_df = document_chunks_df.sample(10, random_state=42)\ndocument_chunks_df.head()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix and OpenInference in Python\nDESCRIPTION: This command installs the necessary Python packages: arize-phoenix for the Phoenix monitoring platform, boto3 for interacting with AWS services, and openinference-instrumentation-bedrock for automatic tracing of Bedrock model calls using OpenInference.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/bedrock_tracing_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install arize-phoenix boto3 openinference-instrumentation-bedrock\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI API Calls with OpenInference\nDESCRIPTION: Sets up telemetry to track OpenAI API calls, allowing monitoring and analysis of ReAct prompting interactions through the Phoenix platform.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/react-prompting.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntracer_provider = register(\n    project_name=\"ReAct-examples\", endpoint=\"https://app.phoenix.arize.com/v1/traces\"\n)\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix Tracer with Application in Python\nDESCRIPTION: This code configures the Phoenix telemetry tracer with your project name and enables automatic instrumentation based on installed dependencies. It uses the 'register' function from 'phoenix.otel' to set up observability for LLM applications. Requires prior installation of the instrumentation package.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/anthropic.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Function Tools for Customer Support Agent\nDESCRIPTION: Creates a list of function definitions that the agent can call to handle various customer support tasks. Each function includes a name, description, and parameter specifications with their types and requirements.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"product_comparison\",\n            \"description\": \"Compare features of two products.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"product_a_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unique identifier of Product A.\",\n                    },\n                    \"product_b_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unique identifier of Product B.\",\n                    },\n                },\n                \"required\": [\"product_a_id\", \"product_b_id\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"product_search\",\n            \"description\": \"Search for products based on criteria.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": \"The search query string.\",\n                    },\n                    \"category\": {\n                        \"type\": \"string\",\n                        \"description\": \"The category to filter the search.\",\n                    },\n                    \"min_price\": {\n                        \"type\": \"number\",\n                        \"description\": \"The minimum price of the products to search.\",\n                        \"default\": 0,\n                    },\n                    \"max_price\": {\n                        \"type\": \"number\",\n                        \"description\": \"The maximum price of the products to search.\",\n                    },\n                    \"page\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The page number for pagination.\",\n                        \"default\": 1,\n                    },\n                    \"page_size\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The number of results per page.\",\n                        \"default\": 20,\n                    },\n                },\n                \"required\": [\"query\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"customer_support\",\n            \"description\": \"Get contact information for customer support regarding an issue.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"issue_type\": {\n                        \"type\": \"string\",\n                        \"description\": \"The type of issue (e.g., billing, technical support).\",\n                    }\n                },\n                \"required\": [\"issue_type\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"track_package\",\n            \"description\": \"Track the status of a package based on the tracking number.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"tracking_number\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The tracking number of the package.\",\n                    }\n                },\n                \"required\": [\"tracking_number\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"product_details\",\n            \"description\": \"Returns details for a given product id\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"product_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The id of a product to look up.\",\n                    }\n                },\n                \"required\": [\"product_id\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"apply_discount_code\",\n            \"description\": \"Applies the discount code to a given order.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"order_id\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The id of the order to apply the discount code to.\",\n                    },\n                    \"discount_code\": {\n                        \"type\": \"string\",\n                        \"description\": \"The discount code to apply\",\n                    },\n                },\n                \"required\": [\"order_id\", \"discount_code\"],\n            },\n        },\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Load Data into DuckDB Python\nDESCRIPTION: Downloads the 'suzyanil/nba-data' dataset from Hugging Face using the `datasets` library. It then loads the data into an in-memory DuckDB database connection. The pandas DataFrame representation of the dataset is registered as a virtual table named 'nba' in the DuckDB instance. Finally, it queries and prints the first record as a dictionary to show the data structure.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport duckdb\nfrom datasets import load_dataset\n\ndata = load_dataset(\"suzyanil/nba-data\")[\"train\"]\n\nconn = duckdb.connect(database=\":memory:\", read_only=False)\nconn.register(\"nba\", data.to_pandas())\n\nconn.query(\"SELECT * FROM nba LIMIT 5\").to_df().to_dict(orient=\"records\")[0]\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Phoenix Prompt by Name, Version, or Tag in TypeScript\nDESCRIPTION: This snippet illustrates how to fetch existing prompts from the Phoenix platform using different identifiers such as name, specific version ID, or tag. It employs the getPrompt function from the Phoenix prompts module and showcases retrieval methods for managing prompt versions and environments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/quickstart-prompts/quickstart-prompts-ts.md#_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { getPrompt } from \"@arizeai/phoenix-client/prompts\";\n\nconst latestPrompt = await getPrompt({\n  prompt: {\n    name: \"article-summarizer\"\n  }\n});\n\nconst specificVersionPrompt = await getPrompt({ \n  prompt: {\n    versionId: \"abcd1234\"\n  }\n});\n\nconst productionPrompt = await getPrompt({ \n  prompt: {\n    name: \"article-summarizer\", \n    tag: \"production\"\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Attributes During Span Creation (Object) - JavaScript\nDESCRIPTION: Shows how to provide a set of attributes when starting a new span by passing an options object as the second argument to `tracer.startActiveSpan()`. This allows attributes to be included from the span's inception.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\ntracer.startActiveSpan(\n  'app.new-span',\n  { attributes: { attribute1: 'value1' } },\n  (span) => {\n    // do some work...\n\n    span.end();\n  },\n);\n```\n\n----------------------------------------\n\nTITLE: Calculating Edit Distance Between Output and Expected Value in Python\nDESCRIPTION: This example demonstrates a custom evaluator that uses the editdistance library to compute the edit distance between the experiment output and expected value. Input and expected values are serialized as JSON strings for comparison to ensure type-agnostic distance calculation. Requires the editdistance library and json module; install editdistance using 'pip install editdistance'. The function returns an integer edit distance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/using-evaluators.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npip install editdistance\n```\n\nLANGUAGE: python\nCODE:\n```\ndef edit_distance(output, expected) -> int:\n    return editdistance.eval(\n        json.dumps(output, sort_keys=True), json.dumps(expected, sort_keys=True)\n    )\n```\n\n----------------------------------------\n\nTITLE: Uploading DataFrame to Phoenix\nDESCRIPTION: This Python snippet uploads a Pandas DataFrame to the Phoenix platform to create a dataset.  It defines example queries and responses, creates a DataFrame from them, launches the Phoenix application, and then uses the `upload_dataset` method to upload the DataFrame. The `input_keys` and `output_keys` parameters are specified to designate the columns as input and output.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-datasets/creating-datasets.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nimport phoenix as px\n\nqueries = [\n    \"What are the 9 planets in the solar system?\",\n    \"How many generations of fundamental particles have we observed?\",\n    \"Is Aluminum a superconductor?\",\n]\nresponses = [\n    \"There are 8 planets in the solar system.\",\n    \"We have observed 3 generations of fundamental particles.\",\n    \"Yes, Aluminum becomes a superconductor at 1.2 degrees Kelvin.\",\n]\n\ndataset_df = pd.DataFrame(data={\n    \"query\": queries,\n    \"responses\": responses\n})\n\npx.launch_app()\nclient = px.Client()\ndataset = client.upload_dataset(\n    dataframe=dataset_df,\n    dataset_name=\"physics-questions\",\n    input_keys=[\"query\"],\n    output_keys=[\"responses\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Phoenix Query Dataset from OpenInference Format in Python\nDESCRIPTION: This code creates a Phoenix query dataset from a dataframe in OpenInference format. The from_open_inference method automatically infers column meanings without requiring a user-defined schema.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_search_and_retrieval_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nquery_ds = px.Inferences.from_open_inference(query_df)\n```\n\n----------------------------------------\n\nTITLE: Building and Configuring the RetrievalQA Chain in LangChain (Python)\nDESCRIPTION: Defines a RetrievalQA chain using a Qdrant retriever (configured for MMR and limiting the number of retrieved docs) and a ChatOpenAI LLM. The chain is set up for question answering over Arize documentation, with observability callbacks for logging. Key parameters include the retrieval model, retrieval k, and the LLM temperature. Requires documentation to be indexed in Qdrant and all LangChain modules installed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/qdrant_langchain_instrumentation_search_and_retrieval_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nhandler = StdOutCallbackHandler()\n\n\nnum_retrieved_documents = 2\nretriever = qdrant.as_retriever(\n    search_type=\"mmr\", search_kwargs={\"k\": num_retrieved_documents}, enable_limit=True\n)\nchain_type = \"stuff\"  # stuff, refine, map_reduce, and map_rerank\nchat_model_name = \"gpt-4-turbo-preview\"\nllm = ChatOpenAI(model_name=chat_model_name, temperature=0.0)\nchain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=chain_type,\n    retriever=retriever,\n    metadata={\"application_type\": \"question_answering\"},\n    callbacks=[handler],\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for RAG - Python\nDESCRIPTION: This code imports essential libraries for building and evaluating a RAG pipeline using LlamaIndex, including the Mistral AI LLM and embeddings. It also imports the Phoenix library for monitoring and tracing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.node_parser import SimpleNodeParser\nfrom llama_index.embeddings.mistralai import MistralAIEmbedding\nfrom llama_index.llms.mistralai import MistralAI\n\nimport phoenix as px\nfrom phoenix.trace import using_project\n```\n\n----------------------------------------\n\nTITLE: Classifying QA Correctness using Phoenix\nDESCRIPTION: This snippet uses `llm_classify` from the Phoenix library to evaluate the correctness of question-answering responses. It leverages a pre-defined prompt and LLM model, specifying rails to guide the classification. The output includes a 'score' based on the classification result, indicating correctness.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nqa_correctness_eval = llm_classify(\n    dataframe=queries_df,\n    model=LiteLLMModel(model=\"ollama/\" + ollama_model),\n    template=QA_PROMPT_TEMPLATE,\n    rails=list(QA_PROMPT_RAILS_MAP.values()),\n    provide_explanation=True,  # Makes the LLM explain its reasoning\n)\n\nqa_correctness_eval[\"score\"] = (\n    qa_correctness_eval.label[~qa_correctness_eval.label.isna()] == \"correct\"\n).astype(int)\n```\n\n----------------------------------------\n\nTITLE: Creating an OpenAI Client and Task Wrapper for Chat Completions in Python\nDESCRIPTION: Initializes an OpenAI-compatible chat client that points to the locally running mock server. Defines a 'task' function that accepts an input dictionary with a 'messages' field and calls the OpenAI API's chat completion endpoint, returning the assistant's message content. Requires 'openai', 'portpicker', and a compatible chat completions endpoint. Inputs are dictionaries containing message lists; outputs are completion strings.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/internal/run_experiments.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport openai\n\nport = pick_unused_port()\nclient = openai.OpenAI(api_key=\"sk-\", base_url=f\"http://localhost:{port}/v1/\")\n\n\ndef task(input):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=input[\"messages\"],\n        max_tokens=20,\n    )\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Classifying with GPT-3.5 Turbo: Phoenix llm_classify - Python\nDESCRIPTION: Runs Phoenix 'llm_classify' on the DataFrame using the GPT-3.5 Turbo model for classification. Uses templates and rails as in previous runs. Enables explanation generation for deeper analysis. Returns a DataFrame with classification results and explanations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nrails = list(HUMAN_VS_AI_PROMPT_RAILS_MAP.values())\nrelevance_classifications_df = llm_classify(\n    dataframe=df,\n    template=HUMAN_VS_AI_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    concurrency=50,\n    provide_explanation=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App and Retrieving Session URL in Python\nDESCRIPTION: This snippet launches the Phoenix application, which is used for visualizing and analyzing traces. It then retrieves the URL of the active Phoenix session, which will be used to configure the OpenTelemetry exporter to send traces to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/bedrock_tracing_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npx.launch_app().view()\nsession_url = px.active_session().url\n```\n\n----------------------------------------\n\nTITLE: Running Hallucination Classification with GPT-3.5-Turbo\nDESCRIPTION: Executes the hallucination classification on the sampled dataset using GPT-3.5-Turbo with the predefined template and rails. Uses concurrent requests for efficiency.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nrails = list(HALLUCINATION_PROMPT_RAILS_MAP.values())\nhallucination_classifications = llm_classify(\n    dataframe=df, template=HALLUCINATION_PROMPT_TEMPLATE, model=model, rails=rails, concurrency=20\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Mustache Prompt Example\nDESCRIPTION: This code snippet demonstrates a prompt template using Mustache formatting.  The placeholder {{name}} will be replaced with the provided value at runtime. Mustache offers more flexibility for conditional logic and loops.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/internal_docs/specs/prompts.md#_snippet_1\n\nLANGUAGE: mustache\nCODE:\n```\nHello, {{name}}!\n```\n\n----------------------------------------\n\nTITLE: Implementing a Starlette-based Chat Completion Mock Server in Python\nDESCRIPTION: Defines an asynchronous Starlette handler and a 'Receiver' class for spinning up a mock chat completion REST API endpoint compatible with OpenAI's client. Dependencies include 'starlette', 'uvicorn', 'portpicker', and standard libraries. The server exposes a '/v1/chat/completions' endpoint returning a fixed-completion response, and includes context manager facilities to start/stop the server in a thread, useful for isolated tests. Inputs are HTTP POST requests; output is a JSON chat completion object. Intended for use as a local OpenAI API substitute during testing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/internal/run_experiments.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom contextlib import contextmanager\nfrom threading import Thread\nfrom time import sleep, time\nfrom typing import Awaitable, Callable, Generator\n\nfrom portpicker import pick_unused_port\nfrom starlette.applications import Starlette\nfrom starlette.responses import JSONResponse, Response\nfrom starlette.routing import Request, Route\nfrom uvicorn import Config, Server\n\n\nasync def hello(_ : Request) -> Response:\n    return JSONResponse(\n        content={\n            \"id\": \"chatcmpl-123\",\n            \"object\": \"chat.completion\",\n            \"created\": 1677652288,\n            \"model\": \"gpt-3.5-turbo-0125\",\n            \"system_fingerprint\": \"fp_44709d6fcb\",\n            \"choices\": [\n                {\n                    \"index\": 0,\n                    \"message\": {\n                        \"role\": \"assistant\",\n                        \"content\": \"\\n\\nHello there, how may I assist you today?\",\n                    },\n                    \"finish_reason\": \"stop\",\n                }\n            ],\n            \"usage\": {\"prompt_tokens\": 9, \"completion_tokens\": 12, \"total_tokens\": 21},\n        }\n    )\n\n\nclass Receiver:\n    def __init__(self, chat_completion: Callable[[Request], Awaitable[Response]]) -> None:\n        self.app = Starlette(\n            routes=[\n                Route(\"/v1/chat/completions\", chat_completion, methods=[\"POST\"]),\n            ]\n        )\n\n    def install_signal_handlers(self) -> None:\n        pass\n\n    @contextmanager\n    def run_in_thread(self, port: int) -> Generator[Thread, None, None]:\n        \"\"\"A coroutine to keep the server running in a thread.\"\"\"\n        config = Config(app=self.app, port=port, loop=\"asyncio\", log_level=\"critical\")\n        server = Server(config=config)\n        thread = Thread(target=server.run)\n        thread.start()\n        time_limit = time() + 5  # 5 seconds\n        try:\n            while not server.started and thread.is_alive() and time() < time_limit:\n                sleep(1e-3)\n            if time() > time_limit:\n                raise RuntimeError(\"server took too long to start\")\n            yield\n        finally:\n            server.should_exit = True\n            thread.join(timeout=5)\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Core Package and Launching Locally - Bash\nDESCRIPTION: Installs the core Phoenix package and starts the Phoenix server locally using the command line. The Phoenix server provides the local observability environment for tracing application events. Useful for development and testing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/mistralai.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Importing UTC Datetime Tools in Python\nDESCRIPTION: Imports the 'datetime' and 'timezone' objects from the Python standard library, essential for generating and formatting timestamped dataset names or records throughout the workflow. No external dependencies are required beyond Python's standard library. The primary use is to create unique, timezone-aware dataset names.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/internal/run_experiments.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom datetime import datetime, timezone\n```\n\n----------------------------------------\n\nTITLE: Patching OpenAI Client Methods\nDESCRIPTION: This snippet demonstrates how to patch the `create` method of the OpenAI client to automatically trace LLM calls. It reassigns the client method to the decorated version, providing transparent instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nopenai_client = OpenAI()\n\n# patch the create method\nwrapper = tracer.llm\nopenai_client.chat.completions.create = wrapper(openai_client.chat.completions.create)\n```\n\n----------------------------------------\n\nTITLE: Generate Synthetic Questions Using GPT-4 Model\nDESCRIPTION: This snippet defines a template for generating diverse customer support questions, sets up the necessary imports and model initialization, and generates 25 questions using the GPT-4 model. It prepares data for downstream evaluation and response analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport nest_asyncio\nimport pandas as pd\n\nnest_asyncio.apply()\nfrom phoenix.evals import OpenAIModel\n\npd.set_option(\"display.max_colwidth\", 500)\n\nmodel = OpenAIModel(model=\"gpt-4o\", max_tokens=1300)\n\nresp = model(GEN_TEMPLATE)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Data Handling and Model Evaluation\nDESCRIPTION: Imports libraries for environment setup: OS and getpass for API key handling, matplotlib for plotting, openai SDK for model interaction, pandas for data processing, pycm and scikit-learn for performance metrics, and specific modules from phoenix.evals for dataset download and classification.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport matplotlib.pyplot as plt\nimport openai\nimport pandas as pd\nfrom pycm import ConfusionMatrix\nfrom sklearn.metrics import classification_report\n\nfrom phoenix.evals import (\n    CODE_READABILITY_PROMPT_RAILS_MAP,\n    CODE_READABILITY_PROMPT_TEMPLATE,\n    OpenAIModel,\n    download_benchmark_dataset,\n    llm_classify,\n)\n\npd.set_option(\"display.max_colwidth\", None)\n```\n\n----------------------------------------\n\nTITLE: Instantiating LLM Helpfulness Evaluator with Phoenix Model Wrapper in Python\nDESCRIPTION: This snippet demonstrates how to instantiate a helpfulness evaluator using Phoenix's built-in LLM evaluators and model wrappers. The evaluator uses the OpenAIModel backend and can be used to score experiment runs based on their helpfulness. Requires the phoenix.experiments.evaluators and phoenix.evals.models modules. The evaluator object expects LLM-compatible input/outputs for assessment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/using-evaluators.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments.evaluators import HelpfulnessEvaluator\nfrom phoenix.evals.models import OpenAIModel\n\nhelpfulness_evaluator = HelpfulnessEvaluator(model=OpenAIModel())\n```\n\n----------------------------------------\n\nTITLE: Running summarization classification with GPT-3.5\nDESCRIPTION: Executes the summarization classification evaluation using GPT-3.5, following the same approach as with GPT-4 for comparison purposes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nrails = list(templates.SUMMARIZATION_PROMPT_RAILS_MAP.values())\nsummarization_classifications = llm_classify(\n    dataframe=df_sample,\n    template=templates.SUMMARIZATION_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    concurrency=20,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Saving Text Classification Tool Prompt to Phoenix (Python)\nDESCRIPTION: Saves the parameters for the text classification task, including the tool definition and category list, into the Phoenix prompt registry.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# prompt identifier should contain only alphanumeric characters, hyphens or underscores\nprompt_identifier = \"document-classification\"\n\nprompt = Client().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Document classification\",\n    version=PromptVersion.from_anthropic(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Using suppress tracing context manager (Python)\nDESCRIPTION: A context manager `suppress_tracing()` prevents tracing within its scope. Inside the `with` block, spans created will not generate trace data, useful for excluding certain operations from traces and avoiding clutter.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\nwith suppress_tracing():\n    # this trace will not be recorded\n    with tracer.start_as_current_span(\n        \"THIS-SPAN-SHOULD-NOT-BE-TRACED\",\n        openinference_span_kind=\"chain\",\n    ) as span:\n        span.set_input(\"input\")\n        span.set_output(\"output\")\n        span.set_status(Status(StatusCode.OK))\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenTelemetry Tracing with sdk-trace-base (TypeScript)\nDESCRIPTION: Demonstrates initializing OpenTelemetry tracing using `@opentelemetry/sdk-trace-base`. This approach is used when the full Node.js or Web SDK is not suitable. It involves setting up a `BasicTracerProvider`, configuring a `BatchSpanProcessor` with an exporter (e.g., `ConsoleSpanExporter`), registering the provider, and obtaining a tracer instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_23\n\nLANGUAGE: typescript\nCODE:\n```\nimport opentelemetry from '@opentelemetry/api';\nimport {\n  BasicTracerProvider,\n  BatchSpanProcessor,\n  ConsoleSpanExporter,\n} from '@opentelemetry/sdk-trace-base';\n\nconst provider = new BasicTracerProvider();\n\n// Configure span processor to send spans to the exporter\nprovider.addSpanProcessor(new BatchSpanProcessor(new ConsoleSpanExporter()));\nprovider.register();\n\n// This is what we'll access in all instrumentation code\nconst tracer = opentelemetry.trace.getTracer('example-basic-tracer-node');\n```\n\n----------------------------------------\n\nTITLE: Concatenating Span Queries\nDESCRIPTION: This snippet demonstrates concatenating the results of two span queries to combine root spans with their corresponding retrieved documents.  It joins the results based on the inner join and displays the concatenated DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/span_query_language.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npd.concat(\n    session.query_spans(\n        SpanQuery().select(**IO).where(IS_ROOT),\n        SpanQuery()\n        .select(span_id=\"parent_id\")\n        .concat(RETRIEVAL_DOCUMENTS, reference=DOCUMENT_CONTENT),\n    ),\n    axis=1,\n    join=\"inner\",\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Query Search Response using OpenAI and OpenTelemetry\nDESCRIPTION: This function generates a query search response by using the OpenAI API. It takes the user payload as a JSON string, the customer Q&A prompt, and an OpenTelemetry tracer. It starts a span named \"Query Search Response\", loads the user payload, extracts the customer input, calls the OpenAI API, updates the payload, sets the span type as \"CHAIN\", sets the status code, and returns the updated payload as a JSON string. It requires the `openai` and `opentelemetry` packages.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndef query_search_response(\n    user_payload_json: str,\n    customer_qa_prompt: str,\n    tracer: opentelemetry.sdk.trace.Tracer,\n) -> str:\n    \"\"\"Query response when customer has an inquiry\n\n    Parameters\n    ----------\n    user_payload_json : str\n        JSON formatted string for Q&A prompt template input\n    customer_qa_prompt : str\n        Customer Q&A prompt template\n    tracer : opentelemetry.sdk.trace.Tracer\n        Tracer to handle span creation\n\n    Returns\n    -------\n    str\n        JSON formatted string of query search payload\n    \"\"\"\n    # Define Span Name & Start\n    with tracer.start_as_current_span(\"Query Search Response\") as span:\n        user_payload_dict = json.loads(user_payload_json)\n        customer_input = user_payload_dict.get(\"Customer Input\", \"\")\n        response_dict = call_openai_api(customer_qa_prompt, customer_input)\n        user_payload_dict.update(response_dict)\n\n        # Define Span Type as \"CHAIN\"\n        span.set_attribute(\n            SpanAttributes.OPENINFERENCE_SPAN_KIND, OpenInferenceSpanKindValues.CHAIN.value\n        )\n\n        # Set Status Code\n        span.set_status(trace_api.StatusCode.OK)\n\n        return json.dumps(user_payload_dict)\n```\n\n----------------------------------------\n\nTITLE: Running Evaluations\nDESCRIPTION: This code snippet runs the evaluations using `evaluate_experiment`, which applies the evaluators defined previously to the experiment. It takes the experiment and evaluators as input, and performs the evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nexperiment = evaluate_experiment(experiment, evaluators)\n```\n\n----------------------------------------\n\nTITLE: Creating ReAct Customer Support Prompt with Step-By-Step Reasoning Using Phoenix in Python\nDESCRIPTION: Constructs a ReAct prompt to enhance customer service question answering by encouraging the model to think step-by-step and select up to three tools in a specific order with parameters. This improves upon the initial prompt by instructing the model to perform logical decomposition before tool invocation. The snippet uses GPT-4 with controlled temperature and requires tool usage, incorporating detailed system messages. It also registers the prompt in Phoenix with version control, similar to the initial prompt creation, enabling iterative improvements in prompt design.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/react-prompting.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nparams = CompletionCreateParamsBase(\n    model=\"gpt-4\",\n    temperature=0.5,\n    tools=tools,\n    tool_choice=\"required\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"\n              You are a helpful customer service agent. Carefully analyze the customerâ€™s question to fully understand their request.\n              Step 1: Think step-by-step. Identify the key pieces of information needed to answer the question. Consider any dependencies between these pieces of information.\n              Step 2: Decide which tools to use. Choose up to 3 tools that will best retrieve the required information. If multiple tools are needed, determine the correct order to call them.\n              Step 3: Output the chosen tools and any relevant parameters.\n\n            \"\"\",\n        },\n        {\"role\": \"user\", \"content\": \"{{questions}}\"},\n    ],\n)\n\nprompt_identifier = \"customer-support\"\n\nprompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Customer Support ReAct Prompt\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Running Overall QA Experiment Orchestration in Python\nDESCRIPTION: This snippet defines run_overall_experiment, which uses suppress_tracing to temporarily disable tracing and runs run_agent_and_track_path_combined on the provided Example input, returning the result. The second part shows an assignment of experiment to the result of run_experiment with dataset as input. These operations depend on the context where Example, suppress_tracing, run_agent_and_track_path_combined, run_experiment, and dataset are defined. The main input is an Example object representing a QA task instance, and the output is either the processed response or a handle to the experiment run.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_60\n\nLANGUAGE: Python\nCODE:\n```\ndef run_overall_experiment(example: Example) -> str:\n    with suppress_tracing():\n        return run_agent_and_track_path_combined(example)\n\n\nexperiment = run_experiment(\n    dataset,\n```\n\n----------------------------------------\n\nTITLE: Enhancing the OpenAI Prompt with Few-Shot Examples for Text2SQL in Python\nDESCRIPTION: Improves the system prompt for OpenAI by including column example values from the NBA dataset, which can help the LLM disambiguate data types and expected input formats. Regenerates system_prompt, then redefines generate_query accordingly. Dependencies: access to NBA table's columns and a data sample. Inputs: none needed directlyâ€”uses database schema. Outputs: prints improved SQL generation for a sample query.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nsamples = conn.query(\"SELECT * FROM nba LIMIT 1\").to_df().to_dict(orient=\"records\")[0]\nsample_rows = \"\\n\".join(\n    f\"{column['column_name']} | {column['column_type']} | {samples[column['column_name']]}\"\n    for column in columns\n)\nsystem_prompt = (\n    \"You are a SQL expert, and you are given a single table named nba with the following columns:\\n\\n\"\n    \"Column | Type | Example\\n\"\n    \"-------|------|--------\\n\"\n    f\"{sample_rows}\\n\"\n    \"\\n\"\n    \"Write a DuckDB SQL query corresponding to the user's request. \"\n    \"Return just the query text, with no formatting (backticks, markdown, etc.).\"\n)\n\n\nasync def generate_query(input):\n    response = await client.chat.completions.create(\n        model=TASK_MODEL,\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": input,\n            },\n        ],\n    )\n    return response.choices[0].message.content\n\n\nprint(await generate_query(\"Which team won the most games in 2015?\"))\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix UI and Registering OpenTelemetry Tracing - Python\nDESCRIPTION: Initializes the Phoenix application for local visualization of LLM evaluation runs. Sets up OpenTelemetry tracing for OpenAI instrumentation to enable tracing of LLM calls within Phoenix. Requires Phoenix and openinference.instrumentation.openai packages. No input parameters; must be run locally with dashboard access.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nimport phoenix as px\nfrom phoenix.otel import register\n\npx.launch_app().view()\n\ntracer_provider = register()\nOpenAIInstrumentor(tracer_provider=tracer_provider).instrument()\n```\n\n----------------------------------------\n\nTITLE: Format Retrieved Context with Tracing - Python\nDESCRIPTION: Defines the `format_context` function. Uses the `@tracer.chain` decorator to automatically create a span for this function, representing a step in the processing chain. Iterates through the results from the Weaviate query and formats the properties ('question', 'answer', 'category') into a single string for use as context in the generation prompt. Returns the formatted context string.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Process and format the retrieved results\n@tracer.chain  # This will create a chain span for the function, same as the with tracer.start_as_current_span() in the query_weaviate function\ndef format_context(results):\n    context = \"\"\n    for item in results.objects:\n        properties = item.properties\n        context += f\"Question: {properties['question']}\\n\"\n        context += f\"Answer: {properties['answer']}\\n\"\n        context += f\"Category: {properties['category']}\\n\\n\"\n    return context\n```\n\n----------------------------------------\n\nTITLE: Running a Zero-Shot Prompt Experiment and Evaluating Outputs with Phoenix in Python\nDESCRIPTION: Defines a task function that sends math problem inputs to the OpenAI chat completion API using the zero-shot baseline prompt and captures model outputs. An evaluation function verifies if the output is numeric and matches the expected answer. Uses the Phoenix run_experiment method to execute this evaluation on the uploaded dataset with proper asynchronous context. This setup enables automated performance tracking of the baseline prompt in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/chain_of_thought_prompting.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.experiments import run_experiment\n\nnest_asyncio.apply()\n\n\ndef zero_shot_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **prompt.format(variables={\"Problem\": input[\"Word Problem\"]})\n    )\n    return resp.choices[0].message.content.strip()\n\n\ndef evaluate_response(output, expected):\n    if not output.isdigit():\n        return False\n    return int(output) == int(expected[\"Answer\"])\n\n\ninitial_experiment = run_experiment(\n    dataset,\n    task=zero_shot_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Zero-Shot Prompt\",\n    experiment_name=\"zero-shot-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Task Function\nDESCRIPTION: This snippet defines an asynchronous task function.  This function takes an input, queries an `VectorStoreIndex` using the `query_engine`, and returns the contexts and response in a dictionary. This is the core logic that the experiment will apply.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nindex = VectorStoreIndex.from_documents(documents=documents)\nquery_engine = index.as_query_engine()\n\n\nasync def task(input):\n    ans = await query_engine.aquery(input[\"query\"])\n    return {\n        \"contexts\": [node.text for node in ans.source_nodes],\n        \"response\": ans.response,\n    }\n```\n\n----------------------------------------\n\nTITLE: Defining Pairwise Evaluator Function (Python)\nDESCRIPTION: Defines an asynchronous function `pairwise` that takes `output`, `input`, and `expected` from an `ExperimentRun`. It uses `llama_index.core.evaluation.PairwiseComparisonEvaluator` with `gpt-4o` to compare the `output` (LLM response) against the `expected[\"response\"]` (ground truth) based on the `input[\"instruction\"]` (query). It returns the score and feedback. A list `evaluators` is created containing this function.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nllm = OpenAI(temperature=0, model=\"gpt-4o\")\n\n\nasync def pairwise(output, input, expected) -> Tuple[Score, Explanation]:\n    ans = await PairwiseComparisonEvaluator(llm=llm).aevaluate(\n        query=input[\"instruction\"],\n        response=output,\n        second_response=expected[\"response\"],\n    )\n    return ans.score, ans.feedback\n\nevaluators = [pairwise]\n```\n\n----------------------------------------\n\nTITLE: Creating SQL-Based NLSQL Query Engine Over Camera Database Using LlamaIndex in Python\nDESCRIPTION: Wraps the in-memory SQLite engine as an SQLDatabase including only the 'cameras' table. Constructs an NLSQLTableQueryEngine that translates natural language queries into SQL queries specifically targeting the 'cameras' table schema. A QueryEngineTool is created with a detailed description of the table columns to provide context for the queries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_sql_retriever_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsql_database = SQLDatabase(engine, include_tables=[\"cameras\"])\n\nsql_query_engine = NLSQLTableQueryEngine(\n    sql_database=sql_database,\n    tables=[\"cameras\"],\n)\nsql_tool = QueryEngineTool.from_defaults(\n    query_engine=sql_query_engine,\n    description=(\n        \"Useful for translating a natural language query into a SQL query over\"\n        \" a table containing technical details about specific digital camera models: Model,\"\n        \" Release date, Max resolution, Low resolution, Effective pixels, Zoom wide (W),\"\n        \" Zoom tele (T), Normal focus range, Macro focus range, Storage included,\"\n        \" Weight (inc. batteries), Dimensions, Price\"\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Span Status to Error - TypeScript\nDESCRIPTION: Illustrates how to explicitly set the status of a span, specifically marking it as `ERROR` using `span.setStatus()` and providing an optional message. This signals that the operation represented by the span did not complete successfully.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_21\n\nLANGUAGE: typescript\nCODE:\n```\nimport opentelemetry, { SpanStatusCode } from '@opentelemetry/api';\n\n// ...\n\ntracer.startActiveSpan('app.doWork', (span) => {\n  for (let i = 0; i <= Math.floor(Math.random() * 40000000); i += 1) {\n    if (i > 10000) {\n      span.setStatus({\n        code: SpanStatusCode.ERROR,\n        message: 'Error',\n      });\n    }\n  }\n\n  span.end();\n});\n```\n\n----------------------------------------\n\nTITLE: Gathering documents into chunks using tiktoken\nDESCRIPTION: This code defines a function that splits a list of documents into chunks, ensuring each chunk's token count is below a specified maximum. It uses tiktoken to count tokens and adds a separator between documents in a chunk.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/long_context_summary_evals.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nencoding = tiktoken.get_encoding(\"cl100k_base\")\n\n\ndef gather_documents_into_chunks(\n    documents: List[str],\n    max_tokens_per_chunk: int,\n    separator=\"\\n\\n======\\n\\n\",\n) -> List[str]:\n    chunks = []\n    current_chunk_documents = []\n    current_chunk_tokens = 0\n    num_tokens_in_separator = len(encoding.encode(separator))\n    for document in documents:\n        document_tokens = len(encoding.encode(document))\n        tokens_to_add = document_tokens + (\n            num_tokens_in_separator if current_chunk_documents else 0\n        )\n        if current_chunk_tokens + tokens_to_add <= max_tokens_per_chunk:\n            current_chunk_documents.append(document)\n            current_chunk_tokens += tokens_to_add\n        else:\n            if current_chunk_documents:\n                chunks.append(separator.join(current_chunk_documents))\n            current_chunk_documents = [document]\n            current_chunk_tokens = document_tokens\n    if current_chunk_documents:\n        chunks.append(separator.join(current_chunk_documents))\n    return chunks\n```\n\n----------------------------------------\n\nTITLE: Instantiating OpenAI Model (GPT-4)\nDESCRIPTION: This code snippet instantiates an `OpenAIModel` with the GPT-4 model, setting the temperature to 0.0. It then tests the model by passing a simple string and printing the response.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_functionality_classifications.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nmodel(\"Hello world, this is a test if you are working?\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Hallucination Classification Results with Explanations\nDESCRIPTION: Combines the original data with classification results and explanations to display comprehensive information about each sample, showing inputs, references, outputs, and the LLM's reasoning.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Let's view the data\nmerged_df = pd.merge(\n    small_df_sample, hallucination_classifications_df, left_index=True, right_index=True\n)\nmerged_df[[\"input\", \"reference\", \"output\", \"is_hallucination\", \"label\", \"explanation\"]].head()\n```\n\n----------------------------------------\n\nTITLE: Querying Ollama Chat API with Phoenix Prompt Utilities in Python\nDESCRIPTION: This snippet demonstrates using the Ollama client to interact with a locally hosted model, formatting the request with the Phoenix prompt utilities. Requires the ollama Python SDK and that the specified model is available locally. The prompt_version_id and variables determine the message content; Python environment variables may be used for configuration. Returns the chat completion as structured JSON.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/examples/prompts/prompts_for_various_SDKs.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# ollama run hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:Q2_K\nmodel = \"hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:Q2_K\"\n\nprompt_version_id = \"UHJvbXB0VmVyc2lvbjox\"\nprompt_version = Client().prompts.get(prompt_version_id=prompt_version_id)\nprint(f\"prompt_version = {prompt_version.model_dump_json(indent=2)}\\n{'-'*100}\")\n\nmessages, kwargs = to_chat_messages_and_kwargs(\n    prompt_version, variables={\"question\": \"Who made you?\"}\n)\nprint(f\"messages = {json.dumps(messages, indent=2)}\\n{'-'*100}\")\nprint(f\"kwargs = {json.dumps(kwargs, indent=2)}\\n{'-'*100}\")\n\nresponse = ollama.chat(model=model, messages=messages)\nprint(f\"response = {response.model_dump_json(indent=2)}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Environment Variables (Cloud)\nDESCRIPTION: This Python code snippet sets up the necessary environment variables for connecting to the Arize Phoenix platform. `PHOENIX_API_KEY` is used for authentication and `PHOENIX_COLLECTOR_ENDPOINT` specifies the address of the Phoenix server. These variables are crucial to ensure that the generated traces are correctly sent to your Phoenix instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex-1.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Inspect Single Dataset Example Python\nDESCRIPTION: Accesses a specific example within the loaded Phoenix dataset by index (in this case, the first example at index 0). This is used to examine the structure and content of individual data records, which is helpful for understanding the format of inputs that will be passed to the task function.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndataset[0]\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Phoenix for Experimentation\nDESCRIPTION: Creates and uploads the generated math problems dataset to Phoenix, assigning it a unique identifier for use in experiments and evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/openai_agents_cookbook.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n\nimport phoenix as px\n\nunique_id = uuid.uuid4()\n\ndataset_name = \"math-questions-\" + str(uuid.uuid4())[:5]\n\n# Upload the dataset to Phoenix\ndataset = px.Client().upload_dataset(\n    dataframe=math_problems_df,\n    input_keys=[\"question\"],\n    dataset_name=f\"math-questions-{unique_id}\",\n)\nprint(dataset)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting LlamaIndex with OpenInference\nDESCRIPTION: Instruments the LlamaIndex application using OpenInference's auto-instrumentor. This configures tracing for LlamaIndex, specifying a tracer provider and trace configuration, allowing for observability of the LlamaIndex application's execution flow.  `skip_dep_check=True` disables dependency checks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_multimodal_image_reasoning.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation import TraceConfig\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nconfig = TraceConfig(base64_image_max_length=100_000_000)\nLlamaIndexInstrumentor().instrument(\n    tracer_provider=tracer_provider,\n    config=config,\n    skip_dep_check=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Pulling Phoenix Docker Image - Bash\nDESCRIPTION: Downloads the latest Phoenix Docker image from Docker Hub. This image is used to run a containerized instance of Phoenix, providing a self-hosted tracing solution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Phoenix via px.Client in Python\nDESCRIPTION: Creates a unique dataset name using the current UTC time and uploads the sampled DataFrame to the Phoenix platform using the Phoenix Client API. The 'upload_dataset' method requires specifying input and output keys, which define the schema for experiment runs. Dependencies include the previously created DataFrame, Phoenix client, and datetime objects. Inputs are the DataFrame and schema details; output is successful dataset registration in the Phoenix system.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/internal/run_experiments.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndataset_name = \"nvidia/ChatQA-Training-Data\" + \"-\" + datetime.now(timezone.utc).isoformat()\npx.Client().upload_dataset(\n    dataset_name=dataset_name,\n    dataframe=df,\n    input_keys=(\"messages\", \"document\"),\n    output_keys=(\"answers\",),\n)\n```\n\n----------------------------------------\n\nTITLE: Previewing DataFrame containing inference data\nDESCRIPTION: Displays the first few rows of the DataFrame to verify that inference data, including features, predictions, and metadata, has been loaded correctly. This is critical before defining schema and logging data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/phoenix-inferences.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ntrain_df.head()\n```\n\n----------------------------------------\n\nTITLE: Converting Traces to Datasets for Evaluation\nDESCRIPTION: Transforms trace data into structured datasets for evaluation, extracting retrieved documents and Q&A pairs with references for analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n\nretrieved_documents_df = get_retrieved_documents(px.Client())\nqueries_df = get_qa_with_reference(px.Client())\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies - Bash\nDESCRIPTION: Installs the necessary Python packages, including the `openinference-instrumentation-llama_index` and `llama-index` package. The llama-index package is explicitly set to version 0.11.0 or higher.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-llama_index llama-index>=0.11.0\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Phoenix and Bedrock Integration in Python\nDESCRIPTION: Installs necessary Python packages including Phoenix OTEL SDK, boto3 for AWS SDK interactions, and instrumentation libraries for Bedrock. These dependencies enable tracing, evaluation, and connection to Amazon Bedrock services and Phoenix observability platform.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/amazon_bedrock_agents_tracing_and_evals.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q uv\n!uv pip install -q arize-phoenix-otel boto3 anthropic openinference-instrumentation-bedrock\n```\n\n----------------------------------------\n\nTITLE: Matching Tool Calls to Expected Outputs for Evaluation Python\nDESCRIPTION: This snippet defines a function for evaluating whether the tool call(s) generated in response output match exactly the expected tools listed in the test specification. It extracts tool call names from model output, compares against expected, and returns 'matches' or 'does not match.' Requires OpenAI output message format and presence of 'expected_tool_calls'. Input: dict with model output, dict with expected tool calls. Output: match result as a string.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/demos/playground_demo.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef matches_expected(output: dict[str, Any], expected: dict[str, Any]) -> str:\n    if len(messages := output.get(\"messages\", {})) != 1:\n        raise ValueError(\"expected exactly one message\")\n    tool_calls = [\n        tool_call.get(\"function\", {}).get(\"name\") for tool_call in messages[0].get(\"tool_calls\", [])\n    ]\n    expected_tool_calls = expected[\"expected_tool_calls\"]\n    return \"matches\" if set(tool_calls) == set(expected_tool_calls) else \"does not match\"\n```\n\n----------------------------------------\n\nTITLE: Fetching Retrieved Documents from Phoenix\nDESCRIPTION: Retrieves the documents returned by the Haystack retriever component from the Phoenix trace data for evaluation purposes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag_haystack.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_retrieved_documents\n\nclient = px.Client()\n\nretrieved_documents_df = get_retrieved_documents(px.Client())\nretrieved_documents_df.head()\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix Tracer\nDESCRIPTION: This code snippet registers the Phoenix tracer provider. It configures the tracer with a project name and enables auto-instrumentation to automatically instrument the application based on installed OpenInference dependencies. The `register` function initializes and configures the OpenTelemetry tracer provider to send traces to a Phoenix instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/bedrock.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Spans as Pandas DataFrame from Phoenix Session - Python\nDESCRIPTION: Fetches all tracing spans recorded in the active Arize Phoenix session and loads them into a pandas DataFrame. This allows for programmatic analysis, filtering, and inspection of the captured traces, such as examining different stages (spans) of the RAG pipeline like retrieval and synthesis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nspans_df = px.active_session().get_spans_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Defining a Phoenix Schema for Corpus Data in Python\nDESCRIPTION: This snippet shows how to define a Phoenix schema for corpus data, specifically for information retrieval. It defines a schema for a corpus inference set with columns for document ID, document text (raw data), and document embeddings.  `EmbeddingColumnNames` is used to specify the vector column and the raw data column related to embeddings.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/inferences.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncorpus_schema=Schema(\n    id_column_name=\"id\",\n    document_column_names=EmbeddingColumnNames(\n        vector_column_name=\"embedding\",\n        raw_data_column_name=\"text\",\n    ),\n),\ncorpus_ds = px.Inferences(corpus_df, corpus_schema)\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Exporter with Conditional Processing in Python\nDESCRIPTION: Configures the exporter for Arize Phoenix. It sets the Phoenix API key in the `PHOENIX_CLIENT_HEADERS` environment variable. Then, it creates an `OTLPSpanExporter` configured with the Phoenix endpoint URL and registers it with the `TracerProvider` using `ConditionalSpanProcessor` and the `phoenix_condition` function, ensuring only non-console spans are sent to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/span_filtering_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Add Phoenix API Key to the headers for tracing and API access\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={phoenix_api_key}\"\n\n# Create the Phoenix exporter\notlp_exporter = OTLPSpanExporter(endpoint=\"https://app.phoenix.arize.com/v1/traces\")\n\n# Add the Phoenix exporter to the tracer provider\ntracer_provider.add_span_processor(ConditionalSpanProcessor(otlp_exporter, phoenix_condition))\n```\n\n----------------------------------------\n\nTITLE: Initializing DSPy Prompt Optimizer with GPT-4o Model - Python\nDESCRIPTION: Configures the DSPy MIPROv2 optimization pipeline to use GPT-4o as the prompt generation model and another model ('turbo') as the classifier for pipeline execution. The snippet creates a prompt optimizer, validates classification through a metric, and compiles an optimized classifier trained on the training data. Requires DSPy, prompt and task models, a validate_classification metric, and annotated training data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\nprompt_gen_lm = dspy.LM(\"gpt-4o\")\ntp = dspy.MIPROv2(\n    metric=validate_classification, auto=\"light\", prompt_model=prompt_gen_lm, task_model=turbo\n)\noptimized_classifier_using_gpt_4o = tp.compile(classifier, trainset=train_data)\n```\n\n----------------------------------------\n\nTITLE: Launching the Phoenix App\nDESCRIPTION: This snippet initializes and launches the Phoenix application. It imports the phoenix library and calls the launch_app() function to start the Phoenix UI for visualizing traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/haystack_tracing_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key using getpass in Python\nDESCRIPTION: This code snippet imports the `os` and `getpass` modules to retrieve the OpenAI API key. It checks if the `OPENAI_API_KEY` environment variable is already set. If not, it prompts the user to enter the API key using `getpass` and sets it as an environment variable. This avoids hardcoding the API key directly in the script.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Instantiating GPT-3.5 Turbo Model for Q&A Classification\nDESCRIPTION: Creates an instance of the OpenAI GPT-3.5 Turbo model with zero temperature and a timeout setting for Q&A classification comparison.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model_name=\"gpt-3.5-turbo\", temperature=0.0, request_timeout=20)\n```\n\n----------------------------------------\n\nTITLE: Uninstrumenting OpenAI Instrumentor - Python\nDESCRIPTION: This line of code uninstruments the OpenAI instrumentor. This is done to prevent capturing LLM calls made during judge evaluation in the same project where the agent LLM is being traced. This separation is important for isolating and analyzing the judge's performance independently.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nOpenAIInstrumentor().uninstrument()  # Uninstrument the OpenAI client to avoid capturing LLM as a Judge evaluation calls in your same project.\n```\n\n----------------------------------------\n\nTITLE: TypeScript: Send Feedback Annotation to Phoenix via Fetch API\nDESCRIPTION: This function sends a feedback annotation to Phoenixâ€™s API endpoint in TypeScript using fetch. It takes the span ID as input, constructs a JSON payload with feedback data, and performs a POST request with appropriate headers. The snippet assumes a valid API key is provided.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/feedback-and-annotations/capture-feedback.md#_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\nasync function postFeedback(spanId: string) {\n  // ...\n  await fetch(\"https://app.phoenix.arize.com/v1/span_annotations?sync=false\", {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n      accept: \"application/json\",\n      \"api_key\": \"<your phoenix api key>\",\n    },\n    body: JSON.stringify({\n      data: [\n        {\n          span_id: spanId,\n          annotator_kind: \"HUMAN\",\n          name: \"feedback\",\n          result: {\n            label: \"thumbs_up\",\n            score: 1,\n            explanation: \"A good response\",\n          },\n        },\n      ],\n    }),\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Loading the Empathy Scores Dataset and Uploading It to Phoenix Using Python\nDESCRIPTION: Loads a test dataset containing chatbot responses with empathy scores using the Huggingface 'datasets' library, then converts it to a pandas DataFrame. It generates a unique dataset name with UUID and uploads the DataFrame to Phoenix using the Phoenix client SDK, specifying input and output keys. This prepares the data for downstream LLM evaluation experiments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nds = load_dataset(\"syeddula/empathy_scores\")[\"test\"]\nds = ds.to_pandas()\nds.head()\n\nimport uuid\n\nimport phoenix as px\n\nunique_id = uuid.uuid4()\n\n# Upload the dataset to Phoenix\ndataset = px.Client().upload_dataset(\n    dataframe=ds,\n    input_keys=[\"AI_Response\", \"EI_Empathy_Score\"],\n    output_keys=[\"EI_Empathy_Score\"],\n    dataset_name=f\"empathy-{unique_id}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Define Output Parser Function\nDESCRIPTION: This Python function is designed to parse the LLM's output, which is expected to be a JSON string. It attempts to load the response as JSON and returns the parsed data. If a JSONDecodeError occurs, it returns an error dictionary. It is crucial for processing LLM responses in a structured way.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef output_parser(response: str, index: int):\n    try:\n        return json.loads(response)\n    except json.JSONDecodeError as e:\n        return {\"__error__\": str(e)}\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Phoenix - Python\nDESCRIPTION: Uploads a dataset of questions and reference answers (presumably from a Pandas DataFrame) to Arize Phoenix. It initializes a Phoenix client, creates a dataset named 'math-questions', specifies the input ('question') and output ('final_output') keys, making it available for running experiments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/integrations/ragas.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\ndataset_df = pd.DataFrame(\n    {\n        \"question\": [conv[\"question\"] for conv in conversations],\n        \"final_output\": [conv[\"final_output\"] for conv in conversations],\n    }\n)\n\ndataset = px.Client().upload_dataset(\n    dataframe=dataset_df,\n    dataset_name=\"math-questions\",\n    input_keys=[\"question\"],\n    output_keys=[\"final_output\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Sample Travel Requests in Python\nDESCRIPTION: This snippet initializes a list of ten unstructured string travel requests representing various user travel needs. These serve as the input data for downstream structured attribute extraction. No special dependencies are required besides Python itself.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_tracing_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntravel_requests = [\n    \"Can you recommend a luxury hotel in Tokyo with a view of Mount Fuji for a romantic honeymoon?\",\n    \"I'm looking for a mid-range hotel in London with easy access to public transportation for a solo backpacking trip. Any suggestions?\",\n    \"I need a budget-friendly hotel in San Francisco close to the Golden Gate Bridge for a family vacation. What do you recommend?\",\n    \"Can you help me find a boutique hotel in New York City with a rooftop bar for a cultural exploration trip?\",\n    \"I'm planning a business trip to Tokyo and I need a hotel near the financial district. What options are available?\",\n    \"I'm traveling to London for a solo vacation and I want to stay in a trendy neighborhood with great shopping and dining options. Any recommendations for hotels?\",\n    \"I'm searching for a luxury beachfront resort in San Francisco for a relaxing family vacation. Can you suggest any options?\",\n    \"I need a mid-range hotel in New York City with a fitness center and conference facilities for a business trip. Any suggestions?\",\n    \"I'm looking for a budget-friendly hotel in Tokyo with easy access to public transportation for a backpacking trip. What do you recommend?\",\n    \"I'm planning a honeymoon in London and I want a luxurious hotel with a spa and romantic atmosphere. Can you suggest some options?\",\n]\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Prompts using TypeScript Function\nDESCRIPTION: Illustrates using the `setPromptTemplate` function from `@openinference-core` in conjunction with `context.with` from `@opentelemetry/api` to set prompt template details (template string, variables object, version string) on the active OpenTelemetry context. OpenInference auto-instrumentations pick up these details and add them as `llm.prompt_template.*` attributes to spans created within the `context.with` callback scope. All parameters (`template`, `variables`, `version`) are optional.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/instrumenting-prompt-templates-and-prompt-variables.md#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { context } from \"@opentelemetry/api\"\nimport { setPromptTemplate } from \"@openinference-core\"\n\ncontext.with(\n  setPromptTemplate(\n    context.active(),\n    { \n      template: \"hello {{name}}\",\n      variables: { name: \"world\" },\n      version: \"v1.0\"\n    }\n  ),\n  () => {\n      // Calls within this block will generate spans with the attributes:\n      // \"llm.prompt_template.template\" = \"hello {{name}}\"\n      // \"llm.prompt_template.version\" = \"v1.0\"\n      // \"llm.prompt_template.variables\" = '{ \"name\": \"world\" }'\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Displaying Text from Second Source Node\nDESCRIPTION: Extracts and prints raw text from the second source node used in the response, enabling examination of the context that influenced the answer.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nresponse_vector.source_nodes[1].get_text()\n\n```\n\n----------------------------------------\n\nTITLE: Re-running the Experiment with GPT-3.5-turbo\nDESCRIPTION: Redefines the LangChain agent to use the GPT-3.5-turbo model, keeping the same dataset, evaluator and task functions. This allows for a direct comparison between GPT-4o and GPT-3.5-turbo on the email extraction task.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel = \"gpt-3.5-turbo\"\n\nllm = ChatOpenAI(model=model).bind_functions(\n    functions=[registry[dataset_name].schema],\n    function_call=registry[dataset_name].schema.schema()[\"title\"],\n)\nextraction_chain = registry[dataset_name].instructions | llm | output_parser\n```\n\n----------------------------------------\n\nTITLE: Defining Zero-Shot CoT Prompt Configuration (OpenAI)\nDESCRIPTION: Creates a new system message template that explicitly asks the model to show its reasoning step-by-step before providing the final answer, implementing the Zero-Shot CoT technique. This template is used to define a new OpenAI chat completion configuration, which is then saved as a new prompt version in Phoenix for comparative analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/chain-of-thought-prompting.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nzero_shot_COT_template = \"\"\"\nYou are an evaluator who outputs the answer to a math word problem.\n\nYou must always think through the problem logically before providing an answer.\n\nFirst, show some of your reasoning.\n\nThen output the integer answer ONLY on a final new line. In this final answer, be sure not include words, commas, labels, or units and round all decimals answers.\n\n\"\"\"\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"system\", \"content\": zero_shot_COT_template},\n        {\"role\": \"user\", \"content\": \"{{Problem}}\"},\n    ],\n)\n\nzero_shot_COT = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Zero Shot COT prompt\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Configure Local Phoenix Endpoint Python Env Var\nDESCRIPTION: Sets the `PHOENIX_COLLECTOR_ENDPOINT` environment variable in Python to point to the local Phoenix instance, which is typically running on `http://localhost:6006`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/vertexai.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Installing arize-phoenix Package - Bash\nDESCRIPTION: This command installs the `arize-phoenix` package to install Phoenix in a Notebook environment. This allows the user to launch Phoenix directly within the notebook environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/crewai.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Phoenix with Python Client\nDESCRIPTION: Uploads the sampled pandas DataFrame dataset to Phoenix using the Phoenix Python client. The upload specifies 'prompt' as the input key and 'type' as the output key, and names the dataset with a unique UUID to track experiments distinctly. This enables dataset management and versioning within Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n\nimport phoenix as px\nfrom phoenix.client import Client as PhoenixClient\n\nunique_id = uuid.uuid4()\n\n# Upload the dataset to Phoenix\ndataset = px.Client().upload_dataset(\n    dataframe=ds,\n    input_keys=[\"prompt\"],\n    output_keys=[\"type\"],\n    dataset_name=f\"jailbreak-classification-{unique_id}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Inspecting a Single Experiment Run Structure\nDESCRIPTION: Accesses and examines the data attributes of a specific experiment run, providing insights into individual evaluation results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nexperiment[0]\n```\n\n----------------------------------------\n\nTITLE: Defining Task Function for Email Extraction\nDESCRIPTION: Defines the task function that invokes the LangChain agent with an email input. The task takes an input email and returns the output of the extraction chain as a string.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef task(input) -> str:\n    return extraction_chain.invoke(input)\n```\n\n----------------------------------------\n\nTITLE: Generating a Condensed Empathy Evaluation Prompt Using GPT-4 - Python\nDESCRIPTION: Defines a function that leverages a GPT-4-chat API to condense an empathy prompt, optimizing it for brevity while retaining scoring and criteria details. The function instructs the model to strip redundancies and focus on conciseness, returning only the optimized prompt text. Optional error handling returns any exception as an error dict. Relies on an initialized OpenAI API client, access to the original prompt in EMPATHY_EVALUATION_PROMPT_TEMPLATE, and models with chat.completions.create().\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef generate_condensed_template():\n    meta_prompt = \"\"\"\n    You are an expert in prompt engineering and LLM evaluation. Your task is to optimize a given LLM-as-a-judge prompt by reducing its word count significantly while maintaining all essential information, including evaluation criteria, scoring system, and purpose.\n\n    Requirements:\n    Preserve all key details such as metrics, scoring guidelines, and judgment criteria.\n\n    Eliminate redundant phrasing and unnecessary explanations.\n\n    Ensure clarity and conciseness without losing meaning.\n\n    Maintain the promptâ€™s effectiveness for consistent evaluations.\n\n    Output Format:\n    Return only the optimized prompt as plain text, with no explanations or commentary.\n\n    \"\"\"\n\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Provided LLM-as-a-judge prompt\"\n                    + EMPATHY_EVALUATION_PROMPT_TEMPLATE,\n                },\n                {\"role\": \"user\", \"content\": meta_prompt},\n            ],\n            temperature=0.9,  # High temperature for more creativity\n        )\n\n        return response.choices[0].message.content\n    except Exception as e:\n        return {\"error\": str(e)}\n\n\nprint(\"Generating condensed evaluation template...\")\nEMPATHY_EVALUATION_PROMPT_TEMPLATE_CONDENSED = generate_condensed_template()\nprint(\"Template generated successfully!\")\nprint(EMPATHY_EVALUATION_PROMPT_TEMPLATE_CONDENSED)\n```\n\n----------------------------------------\n\nTITLE: Running Few-Shot Learning Experiment for Summarization in Python\nDESCRIPTION: Executes an experiment using the few-shot prompt template created earlier, which includes example summaries to guide the model in generating more appropriate concise summaries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/summarization.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntask = partial(summarize_article_openai, prompt_template=template, model=gpt_4o)\nexperiment_results = run_experiment(\n    dataset,\n    task,\n    experiment_name=\"few-shot-template\",\n    experiment_description=\"include examples\",\n    experiment_metadata={\"vendor\": \"openai\", \"model\": gpt_4o},\n    evaluators=EVALUATORS,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Zero-Shot Prompt Structure for Sentiment Analysis in Python\nDESCRIPTION: Defines the structure for a zero-shot sentiment analysis prompt using OpenAI's 'gpt-3.5-turbo' model. It sets up system and user messages, with the user message containing a placeholder `{{Review}}`. The prompt configuration is then registered with Phoenix using `PhoenixClient().prompts.create`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/few-shot-prompting.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom openai.types.chat.completion_create_params import CompletionCreateParamsBase\n\nfrom phoenix.client.types import PromptVersion\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an evaluator who assesses the sentiment of a review. Output if the review positive, negative, or neutral. Only respond with one of these classifications.\",\n        },\n        {\"role\": \"user\", \"content\": \"{{Review}}\"},\n    ],\n)\n\nprompt_identifier = \"fridge-sentiment-reviews\"\n\nprompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"A prompt for classifying reviews based on sentiment.\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing LlamaIndex StorageContext from GCS\nDESCRIPTION: Initializes a Google Cloud Storage file system object using `GCSFileSystem`. Specifies the GCS path to a pre-built LlamaIndex index directory. Creates a `StorageContext` using this file system and path, allowing LlamaIndex to load the index data directly from cloud storage.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfile_system = GCSFileSystem(project=\"public-assets-275721\")\nindex_path = \"arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/index/\"\nstorage_context = StorageContext.from_defaults(\n    fs=file_system,\n    persist_dir=index_path,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI LLM Model Parameters in LlamaIndex Using Python\nDESCRIPTION: Configures the global LLM setting in LlamaIndex to use OpenAI's GPT-4o model with temperature set to 0.0 for deterministic outputs. This affects subsequent natural language query processing to produce consistent answers with no randomness.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_sql_retriever_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nSettings.llm = OpenAI(temperature=0.0, model=\"gpt-4o\")\n```\n\n----------------------------------------\n\nTITLE: Specifying Project Name with Phoenix register() Function (Python)\nDESCRIPTION: This snippet shows how to directly assign a project name when initializing the Phoenix tracer using the 'register' function. Requires the 'phoenix.otel' module. The 'project_name' parameter sets the project under which all subsequent traces will be grouped. Expects a string for 'project_name', and returns a configured tracer provider instance. Must be run before other instrumentation steps.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/setup-projects.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.otel import register\n\ntracer_provider = register(\n    project_name=\"my-project-name\",\n    ....\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Zero-Shot Prompt\nDESCRIPTION: Defines a zero-shot prompt for sentiment analysis using the OpenAI API. The prompt instructs the model to classify reviews as positive, negative, or neutral and then creates a prompt in Phoenix using the PhoenixClient.  The prompt is structured as a chat completion with system and user roles.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/few_shot_prompting.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom openai.types.chat.completion_create_params import CompletionCreateParamsBase\n\nfrom phoenix.client.types import PromptVersion\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an evaluator who assesses the sentiment of a review. Output if the review positive, negative, or neutral. Only respond with one of these classifications.\",\n        },\n        {\"role\": \"user\", \"content\": \"{{Review}}\"},\n    ],\n)\n\nprompt_identifier = \"fridge-sentiment-reviews\"\n\nprompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"A prompt for classifying reviews based on sentiment.\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App with Training Inferences in Python\nDESCRIPTION: Starts a Phoenix session with the training inferences as the primary dataset. Requires a Phoenix import as 'px' and an initialized 'train_ds' Inferences object. Parameter 'primary' specifies the main data to visualize. Running this command launches the app's UI which can be accessed via the provided link or in-notebook viewer. Returns a session object representing the Phoenix application instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/quickstart/phoenix-inferences/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsession = px.launch_app(primary=train_ds)\n\n```\n\n----------------------------------------\n\nTITLE: Logging Sessions and Traces with OpenInference Core Library in TypeScript\nDESCRIPTION: Illustrates how to instrument session-based tracing in a TypeScript application using OpenTelemetry and the `@arizeai/openinference-core` library. Session-specific semantic attributes are set on spans, and the `setSession` context utility is used to propagate the session identifier through async calls, including interactions with OpenAI's chat completion API. Prerequisites include installing `@arizeai/openinference-core` and setting up OpenTelemetry along with an OpenAI client. The main inputs are chat message arrays and a session ID string, producing traced and attributed OpenAI chat responses.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/setup-sessions.md#_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { trace } from \"@opentelemetry/api\";\nimport { SemanticConventions } from \"@arizeai/openinference-semantic-conventions\";\nimport { context } from \"@opentelemetry/api\";\nimport { setSession } from \"@arizeai/openinference-core\";\n\nconst tracer = trace.getTracer(\"agent\");\n\nconst client = new OpenAI({\n  apiKey: process.env[\"OPENAI_API_KEY\"], // This is the default and can be omitted\n});\n\nasync function assistant(params: {\n  messages: { role: string; content: string }[];\n  sessionId: string;\n}) {\n  return tracer.startActiveSpan(\"agent\", async (span: Span) => {\n    span.setAttribute(SemanticConventions.OPENINFERENCE_SPAN_KIND, \"agent\");\n    span.setAttribute(SemanticConventions.SESSION_ID, params.sessionId);\n    span.setAttribute(\n      SemanticConventions.INPUT_VALUE,\n      messages[messages.length - 1].content,\n    );\n    try {\n      // This is not strictly necessary but it helps propagate the session ID\n      // to all child spans\n      return context.with(\n        setSession(context.active(), { sessionId: params.sessionId }),\n        async () => {\n          // Calls within this block will generate spans with the session ID set\n          const chatCompletion = await client.chat.completions.create({\n            messages: params.messages,\n            model: \"gpt-3.5-turbo\",\n          });\n          const response = chatCompletion.choices[0].message;\n          span.setAttribute(SemanticConventions.OUTPUT_VALUE, response.content);\n          span.end();\n          return response;\n        },\n      );\n    } catch (e) {\n      span.error(e);\n    }\n  });\n}\n\nconst sessionId = crypto.randomUUID();\n\nlet messages = [{ role: \"user\", content: \"hi! im Tim\" }];\n\nconst res = await assistant({\n  messages,\n  sessionId: sessionId,\n});\n\nmessages = [res, { role: \"assistant\", content: \"What is my name?\" }];\n\nawait assistant({\n  messages,\n  sessionId: sessionId,\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Ollama API Environment Variable\nDESCRIPTION: Sets the OLLAMA_API_BASE environment variable to point to the local Ollama instance running on the default port 11434.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OLLAMA_API_BASE\"] = \"http://localhost:11434\"\n```\n\n----------------------------------------\n\nTITLE: Installing Jupyter for Testing Phoenix\nDESCRIPTION: Command to install Jupyter for testing Phoenix by running tutorial notebooks. This helps verify that a built version of Phoenix works correctly.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\npip install jupyter\n```\n\n----------------------------------------\n\nTITLE: Installing OpenInference Bedrock Instrumentation (bash)\nDESCRIPTION: This command installs the `openinference-instrumentation-bedrock` package using pip. This package is required for instrumenting AWS Bedrock calls. The command is executed in a terminal environment, typically as part of the setup process.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/bedrock-1.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-bedrock\n```\n\n----------------------------------------\n\nTITLE: Creating Customer Query Response Prompt for OpenAI in Python\nDESCRIPTION: Defines a prompt string instructing the AI to generate an objective, detailed JSON response answering customer queries about e-commerce policies like returns, shipping, and order changes. The assistant is advised to provide complete guidance tailored to the input question without unnecessary additions, supporting enhanced automated customer service.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ncustomer_qa_prompt = \"\"\"\nYou are a helpful assistant designed to output JSON. Assist with answering customer queries about policies on returns, shipping, order modifications, and general inquiries for an e-commerce shop.\n\nWhen responding to a customer query, carefully consider the context of their question and provide a clear, detailed response. Your response should informatively guide the customer on the next steps they can take or the information they're seeking.\n\nOutput JSON where the key is \\\"customer_response\\\" and the value is your objective and detailed answer to the customer's query. If additional policy details are relevant, include them in your response to ensure the customer receives complete and accurate guidance.\n\nStructure your response as follows:\n\nkey: \\\"customer_response\\\"\nvalue: \\\"<Your objective, detailed response here>\\\"\n\nThe objective is to fully address the customer's concern, providing them with precise information and clear next steps where applicable, without unnecessary embellishments.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix API Key and Endpoint - Python\nDESCRIPTION: Configures the Phoenix trace collector and authentication by assigning relevant environment variables in Python. Requires the user's Phoenix API key and the service endpoint URL. These variables enable secure tracing, with 'PHOENIX_CLIENT_HEADERS' carrying the API key and 'PHOENIX_COLLECTOR_ENDPOINT' specifying the collector URL. Provide 'PHOENIX_API_KEY' as your access credential.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Adding User ID to Spans in JavaScript/TypeScript\nDESCRIPTION: Shows how to set the `user.id` attribute on OpenTelemetry spans using the `setUser` function from `@arizeai/openinference-core` and `context.with` from `@opentelemetry/api`. Spans created within the `context.with` callback will receive the specified user ID. Requires the `@arizeai/openinference-core` and `@opentelemetry/api` packages.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { context } from \"@opentelemetry/api\"\nimport { setUser } from \"@arizeai/openinference-core\"\n\ncontext.with(\n  setUser(context.active(), { userId: \"user-id\" }),\n  () => {\n      // Calls within this block will generate spans with the attributes:\n      // \"user.id\" = \"user-id\"\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Phoenix Hallucination Evaluation\nDESCRIPTION: Installs necessary Python packages for running hallucination evaluations, including Phoenix evals, OpenAI API, visualization libraries, and other utilities.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qq \"arize-phoenix-evals>=0.0.5\" \"openai>=1\" ipython matplotlib pycm scikit-learn tiktoken nest_asyncio 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Installing arize-phoenix-otel Python Package - Bash\nDESCRIPTION: Installs the 'arize-phoenix-otel' package via pip to enable Phoenix OpenTelemetry tracing in your Python environment. This command should be run in your terminal before configuring Python-based instrumentation. No additional parameters are required; Python and pip must be available.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/litellm.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Bedrock Client with OpenInference\nDESCRIPTION: This code instruments the AWS Bedrock client with the OpenInference instrumentor. After calling `instrument()`, all Bedrock clients created using the same session will automatically generate traces when `invoke_model` is called. `skip_dep_check=True` is included to skip dependency checks during the instrumentation process. An instrumented client is then created.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nBedrockInstrumentor().instrument(skip_dep_check=True)\ninstrumented_client = session.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Phoenix Instrumentation\nDESCRIPTION: Configures environment variables and registers Phoenix tracer provider to enable telemetry collection and tracing of the application, using Phoenix API key and endpoint.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\n_set_env(\"PHOENIX_API_KEY\")\n\nfrom openinference.instrumentation.langchain import LangChainInstrumentor\n\nfrom phoenix.otel import register\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.getenv('PHOENIX_API_KEY')}\"\n\ntracer_provider = register()\nLangChainInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Computing Hit Rate for Retrieval Evaluation\nDESCRIPTION: Determines whether at least one relevant document was retrieved in the top results for each query.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nhit = pd.DataFrame(\n    {\n        \"hit\": documents_with_relevance_df.groupby(\"context.span_id\").apply(\n            lambda x: x.eval_score[:2].sum(skipna=False) > 0\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Decorated function for OpenAI ChatCompletion with tracing (Python)\nDESCRIPTION: A function `invoke_llm` wrapping OpenAI's chat completion API with a tracing decorator `tracer.llm`. It includes input/output processing functions, supporting synchronous execution and demonstrating how to pass message parameters, model, temperature, and tools. The snippet highlights the decorator's role in tracing and the usage of the client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import NOT_GIVEN, OpenAI\nfrom openai.types.chat import ChatCompletion\n\nopenai_client = OpenAI()\n\n@tracer.llm(\n    process_input=process_input,\n    process_output=process_output,\n)\ndef invoke_llm(\n    messages: List[ChatCompletionMessageParam],\n    model: str,\n    temperature: Optional[float] = None,\n    tools: Optional[List[ChatCompletionToolParam]] = None,\n) -> ChatCompletion:\n    response: ChatCompletion = openai_client.chat.completions.create(\n        messages=messages,\n        model=model,\n        tools=tools or NOT_GIVEN,\n        temperature=temperature,\n    )\n    return response\n\ninvoke_llm(\n    messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n    temperature=0.5,\n    model=\"gpt-4\",\n)\n```\n\n----------------------------------------\n\nTITLE: Initialize Evaluation Model - Python\nDESCRIPTION: Imports the `OpenAIModel` class from `phoenix.evals`. Initializes an instance of `OpenAIModel`, specifying 'gpt-4o-mini' as the model to be used for evaluation tasks. This instance will be passed to specific evaluators.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import OpenAIModel\n\n# Initialize the OpenAI model you'll use for evaluation\neval_model = OpenAIModel(model=\"gpt-4o-mini\")\n```\n\n----------------------------------------\n\nTITLE: Launching New Phoenix Instance with Traces - Python\nDESCRIPTION: This snippet illustrates how to launch a temporary Phoenix instance within a local notebook environment to visualize traces. It uses `px.launch_app()` to start Phoenix, passing in a `TraceDataset` loaded from either a dataframe (my_traces), or a local file. The temporary instance is only active for the duration of the notebook's execution. This code will load traces from a dataframe or a local file and launch a Phoenix instance. It uses the `TraceDataset.load` method to load traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/importing-existing-traces.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# Load traces from a dataframe\npx.launch_app(trace=px.TraceDataset.load(my_traces))\n\n# Load traces from a local file\npx.launch_app(trace=px.TraceDataset.load('f7733fda-6ad6-4427-a803-55ad2182b662', directory=\"/my_saved_traces/\"))\n```\n\n----------------------------------------\n\nTITLE: Viewing Experiment Results as DataFrame\nDESCRIPTION: Displays results from the experiment in tabular DataFrame format for easy analysis of performance and outputs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nexperiment.as_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Uploading - Phoenix Dataset - Python\nDESCRIPTION: Creates a sample pandas DataFrame representing a dataset and uploads it to Phoenix for evaluation. The DataFrame contains input ('question'), output ('answer'), and metadata fields. A Phoenix client is initialized, and the DataFrame is uploaded, specifying the role of each column.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/datasets_and_experiments_quickstart.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nimport phoenix as px\n\ndf = pd.DataFrame(\n    [\n        {\n            \"question\": \"What is Paul Graham known for?\",\n            \"answer\": \"Co-founding Y Combinator and writing on startups and techology.\",\n            \"metadata\": {\"topic\": \"tech\"}\n        }\n    ]\n)\nphoenix_client = px.Client()\ndataset = phoenix_client.upload_dataset(\n    dataset_name=\"test-dataset\",\n    dataframe=df,\n    input_keys=[\"question\"],\n    output_keys=[\"answer\"],\n    metadata_keys=[\"metadata\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Creating SQL Query Engine and Tool Component\nDESCRIPTION: Initializes the Natural Language to SQL query engine with the database connection and tables to query, then wraps it in a QueryEngineTool for use with the agent framework.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsql_query_engine = NLSQLTableQueryEngine(\n    sql_database=sql_database,\n    tables=[\"albums\", \"tracks\", \"artists\"],\n    verbose=True,\n)\nsql_tool = QueryEngineTool.from_defaults(\n    query_engine=sql_query_engine,\n    name=\"sql_tool\",\n    description=(\"Useful for translating a natural language query into a SQL query\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Experiment Metadata Dictionary (Python)\nDESCRIPTION: Creates a Python dictionary to store metadata about the experiment run, such as the LLM model, embedding model, and reranker used.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/run_experiments_with_llama_index.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nexperiment_metadata = {\n    \"llm\": \"gpt-4\",\n    \"embed_model\": \"text-embedding-3-small\",\n    \"reranker\": \"cross-encoder/ms-marco-MiniLM-L-2-v2\",\n}\n```\n\n----------------------------------------\n\nTITLE: Instrumenting DSPy with OpenInference\nDESCRIPTION: This snippet instruments the DSPy library using the OpenInference SDK, sending calls to Phoenix for tracing and analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.dspy import DSPyInstrumentor\n\nDSPyInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Using context attributes for tracing (Python)\nDESCRIPTION: A context manager `using_attributes()` adds custom attributes, like a session ID, to trace spans within its scope. This facilitates contextual metadata collection, enhancing trace analysis and debugging.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\nwith using_attributes(session_id=\"123\"):\n    # this trace has session id \"123\"\n    with tracer.start_as_current_span(\n        \"chain-span-with-context-attributes\",\n        openinference_span_kind=\"chain\",\n    ) as span:\n        span.set_input(\"input\")\n        span.set_output(\"output\")\n        span.set_status(Status(StatusCode.OK))\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Testset with Ragas (Python)\nDESCRIPTION: Initializes a Ragas `TestsetGenerator` using OpenAI models. Defines a distribution across different question evolution types (`simple`, `reasoning`, `multi_context`). Generates a synthetic test dataset of `TEST_SIZE` using the loaded LlamaIndex documents, suitable for RAG evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ragas.testset.evolutions import multi_context, reasoning, simple\nfrom ragas.testset.generator import TestsetGenerator\n\nfrom phoenix.trace import using_project\n\nTEST_SIZE = 5\n\n# generator with openai models\ngenerator = TestsetGenerator.with_openai()\n\n# set question type distribution\ndistribution = {simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n\n# generate testset\nwith using_project(\"ragas-testset\"):\n    testset = generator.generate_with_llamaindex_docs(\n        documents, test_size=TEST_SIZE, distributions=distribution\n    )\ntest_df = (\n    testset.to_pandas().sort_values(\"question\").drop_duplicates(subset=[\"question\"], keep=\"first\")\n)\ntest_df.head(2)\n```\n\n----------------------------------------\n\nTITLE: Converting Classification Labels to Boolean Correctness Flags in Python\nDESCRIPTION: Creates a boolean list indicating whether each classification label equals the string 'correct'. This facilitates binary correctness evaluation and filtering. The list is stored as boolean_classifications.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nboolean_classifications = [x == \"correct\" for x in df[\"label\"].tolist()]\n```\n\n----------------------------------------\n\nTITLE: Defining a Task using the Meta-Prompted Prompt\nDESCRIPTION: This snippet defines a function `test_prompt` that uses the newly created meta-prompt from Phoenix to generate a response using the OpenAI API.  It formats the Phoenix prompt object with an input `prompt` variable, sends this to the OpenAI API, and returns the generated text.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef test_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **meta_prompt_result.format(variables={\"prompt\": input[\"prompt\"]})\n    )\n    return resp.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Tracer\nDESCRIPTION: Configures the Phoenix tracer using the `register` function from the `phoenix.otel` module.  It sets the project name and enables auto-instrumentation for installed OpenInference dependencies.  This will collect tracing information and send it to a running Phoenix instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/instructor.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAIModel Parameters - Python\nDESCRIPTION: Declares the OpenAIModel class along with all configuration fields for customizing access to the OpenAI API, including authentication, model selection, sampling behavior, batching, and timeouts. Requires the 'openai>=1.0.0' Python package to be installed. Fields like 'api_key', 'organization', and 'base_url' control API access, while parameters such as 'model', 'temperature', 'max_tokens', and 'batch_size' impact request behavior. Most fields have defaults aligned with OpenAI recommendation, but can be overridden. Input is provided as constructor parameters or read from environment variables; output is a configured model instance ready to make completions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evaluation-models.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass OpenAIModel:\n    api_key: Optional[str] = field(repr=False, default=None)\n    \"\"\"Your OpenAI key. If not provided, will be read from the environment variable\"\"\"\n    organization: Optional[str] = field(repr=False, default=None)\n    \"\"\"\n    The organization to use for the OpenAI API. If not provided, will default\n    to what's configured in OpenAI\n    \"\"\"\n    base_url: Optional[str] = field(repr=False, default=None)\n    \"\"\"\n    An optional base URL to use for the OpenAI API. If not provided, will default\n    to what's configured in OpenAI\n    \"\"\"\n    model: str = \"gpt-4\"\n    \"\"\"Model name to use. In of azure, this is the deployment name such as gpt-35-instant\"\"\"\n    temperature: float = 0.0\n    \"\"\"What sampling temperature to use.\"\"\"\n    max_tokens: int = 256\n    \"\"\"The maximum number of tokens to generate in the completion.\n    -1 returns as many tokens as possible given the prompt and\n    the models maximal context size.\"\"\"\n    top_p: float = 1\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n    frequency_penalty: float = 0\n    \"\"\"Penalizes repeated tokens according to frequency.\"\"\"\n    presence_penalty: float = 0\n    \"\"\"Penalizes repeated tokens.\"\"\"\n    n: int = 1\n    \"\"\"How many completions to generate for each prompt.\"\"\"\n    model_kwargs: Dict[str, Any] = field(default_factory=dict)\n    \"\"\"Holds any model parameters valid for `create` call not explicitly specified.\"\"\"\n    batch_size: int = 20\n    \"\"\"Batch size to use when passing multiple documents to generate.\"\"\"\n    request_timeout: Optional[Union[float, Tuple[float, float]]] = None\n    \"\"\"Timeout for requests to OpenAI completion API. Default is 600 seconds.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Create Generation Prompt with Tracing - Python\nDESCRIPTION: Defines the `create_prompt` function. Uses the `@tracer.chain` decorator to create a span for this function. Takes the original query text and the formatted context string as input. Constructs a detailed prompt string for the language model, incorporating both the user's question and the retrieved context. Returns the formatted prompt.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Create a prompt with the retrieved information\n@tracer.chain\ndef create_prompt(query_text, context):\n    prompt = f\"\"\"\nBased on the following information, please answer the question: \"{query_text}\"\n\nContext:\n{context}\n\nPlease provide a comprehensive answer based on the information provided.\n\"\"\"\n    return prompt\n```\n\n----------------------------------------\n\nTITLE: Evaluating GPT-4 Turbo summarization classification results\nDESCRIPTION: Evaluates the GPT-4 Turbo model's summarization classifications against ground truth labels, generating classification metrics and a confusion matrix for comparison with other models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df_sample[\"user_feedback\"].map(templates.SUMMARIZATION_PROMPT_RAILS_MAP).tolist()\n\nprint(classification_report(true_labels, summarization_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels,\n    predict_vector=summarization_classifications,\n    classes=rails,\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Environment Variables (Docker)\nDESCRIPTION: This Python code snippet sets up the necessary environment variables for connecting to the Arize Phoenix platform.  `PHOENIX_COLLECTOR_ENDPOINT` specifies the address of the Phoenix server. These variables are crucial to ensure that the generated traces are correctly sent to your Phoenix instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex-1.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Instantiating OpenAI Client in JavaScript\nDESCRIPTION: Constructs an OpenAI client with the user-supplied API key to allow future chat completions. Relies on the 'openai' package and assumes 'apiKey' is a valid string. Supports all OpenAI SDK interactions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_cross_sdk_tutorial.ipynb#_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst openai = new OpenAI({ apiKey: apiKey });\n```\n\n----------------------------------------\n\nTITLE: Define Summarization Task with OpenAI\nDESCRIPTION: This code defines an asynchronous function that summarizes an article using an OpenAI model, given a prompt template and the article content.  It constructs and sends a request to the OpenAI API, retrieves and returns the summary.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/summarization.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import AsyncOpenAI\n\nfrom phoenix.experiments.types import Example\n\nopenai_client = AsyncOpenAI()\n\n\nasync def summarize_article_openai(input, prompt_template: str, model: str) -> str:\n    formatted_prompt_template = prompt_template.format(article=input[\"article\"])\n    response = await openai_client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"assistant\", \"content\": formatted_prompt_template},\n        ],\n    )\n    assert response.choices\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Downloading Trace Data from Phoenix (Python)\nDESCRIPTION: Imports the `phoenix` library, creates a Phoenix client instance, and uses `client.get_spans_dataframe()` to retrieve the captured trace spans for the specified project ('evaluating_traces_quickstart') as a Pandas DataFrame. The `.head()` method is called to display the first few rows of the retrieved data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evaluating_traces_quickstart.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\nspans_df = px.Client().get_spans_dataframe(project_name=\"evaluating_traces_quickstart\")\nspans_df.head()\n```\n\n----------------------------------------\n\nTITLE: Using Environment Variables with OTel Primitives - Python\nDESCRIPTION: This example configures the Phoenix OTel using the environment variable `PHOENIX_COLLECTOR_ENDPOINT`. It uses OTel primitives through `phoenix.otel.TracerProvider`. The required modules include `opentelemetry` and `phoenix.otel`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-otel/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace as trace_api\nfrom phoenix.otel import TracerProvider\n\ntracer_provider = TracerProvider()\ntrace_api.set_tracer_provider(tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Performing User Frustration Evaluation with Phoenix (Python)\nDESCRIPTION: This Python snippet demonstrates how to perform user frustration evaluation on a dataset using the Phoenix library. It imports necessary modules, initializes an OpenAI model (`gpt-4`), defines the expected output values using `USER_FRUSTRATION_PROMPT_RAILS_MAP`, and calls the `llm_classify` function. This function applies the `USER_FRUSTRATION_PROMPT_TEMPLATE` to conversations in a DataFrame (`df`), using the specified model and ensuring the output conforms to the defined rails.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/user-frustration.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    USER_FRUSTRATION_PROMPT_RAILS_MAP,\n    USER_FRUSTRATION_PROMPT_TEMPLATE,\n    OpenAIModel,\n    download_benchmark_dataset,\n    llm_classify,\n)\n\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\n\n#The rails is used to hold the output to specific values based on the template\n#It will remove text such as \",,,\" or \"...\"\n#Will ensure the binary value expected from the template is returned\nrails = list(USER_FRUSTRATION_PROMPT_RAILS_MAP.values())\nrelevance_classifications = llm_classify(\n    dataframe=df,\n    template=USER_FRUSTRATION_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    provide_explanation=True, #optional to generate explanations for the value produced by the eval LLM\n)\n\n```\n\n----------------------------------------\n\nTITLE: Generating a Synthetic Math Problems Dataset\nDESCRIPTION: Creates a diverse dataset of math problems using GPT-4o to generate 25 different problems, spanning basic operations to word problems, for testing the agent.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/openai_agents_cookbook.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nMATH_GEN_TEMPLATE = \"\"\"\nYou are an assistant that generates diverse math problems for testing a math solver agent.\nThe problems should include:\n\nBasic Operations: Simple addition, subtraction, multiplication, division problems.\nComplex Arithmetic: Problems with multiple operations and parentheses following order of operations.\nExponents and Roots: Problems involving powers, square roots, and other nth roots.\nPercentages: Problems involving calculating percentages of numbers or finding percentage changes.\nFractions: Problems with addition, subtraction, multiplication, or division of fractions.\nAlgebra: Simple algebraic expressions that can be evaluated with specific values.\nSequences: Finding sums, products, or averages of number sequences.\nWord Problems: Converting word problems into mathematical equations.\n\nDo not include any solutions in your generated problems.\n\nRespond with a list, one math problem per line. Do not include any numbering at the beginning of each line.\nGenerate 25 diverse math problems. Ensure there are no duplicate problems.\n\"\"\"\n\nimport nest_asyncio\n\nnest_asyncio.apply()\npd.set_option(\"display.max_colwidth\", 500)\n\n# Initialize the model\nmodel = OpenAIModel(model=\"gpt-4o\", max_tokens=1300)\n\n# Generate math problems\nresp = model(MATH_GEN_TEMPLATE)\n\n# Create DataFrame\nsplit_response = resp.strip().split(\"\\n\")\nmath_problems_df = pd.DataFrame(split_response, columns=[\"question\"])\nprint(math_problems_df.head())\n```\n\n----------------------------------------\n\nTITLE: Running Instructor Example\nDESCRIPTION: This Python code demonstrates how to use the `instructor` library to extract structured data from text using an OpenAI model. It defines a `UserInfo` model using `pydantic`, patches the OpenAI client with `instructor.from_openai`, and uses the client to extract the data.  The output is then printed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/instructor.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n\n# Define your desired output structure\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n\n\n# Patch the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\n# Extract structured data from natural language\nuser_info = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserInfo,\n    messages=[{\"role\": \"user\", \"content\": \"John Doe is 30 years old.\"}],\n)\n\nprint(user_info.name)\n#> John Doe\nprint(user_info.age)\n#> 30\n```\n\n----------------------------------------\n\nTITLE: Querying Phoenix with Generated Questions in Python\nDESCRIPTION: Iterates over each row in the questions_with_document_chunk_df DataFrame, extracts the \"question\" column, uses the query_engine to query Phoenix, and prints the question along with the generated answer from the retrieval-augmented system. This snippet demonstrates how to automate question-answer evaluation at scale. It depends on a prepared DataFrame, a query_engine instance, and prints outputs for inspection.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfor _, row in questions_with_document_chunk_df.iterrows():\n    question = row[\"question\"]\n    response_vector = query_engine.query(question)\n    print(f\"Question: {question}\\nAnswer: {response_vector.response}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix and Configuring Environment\nDESCRIPTION: Detects whether to use a cloud instance of Phoenix by checking for an API key in environment variables, or launches a local Phoenix instance for monitoring.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag_haystack.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Check if PHOENIX_API_KEY is present in the environment variables.\n# If it is, we'll use the cloud instance of Phoenix. If it's not, we'll start a local instance.\n# A third option is to connect to a docker or locally hosted instance.\n# See https://docs.arize.com/phoenix/setup/environments for more information.\n\nimport os\n\nif \"PHOENIX_API_KEY\" in os.environ:\n    os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\n    os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n\nelse:\n    import phoenix as px\n\n    px.launch_app().view()\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenInference Span Processor for Vercel AI SDK - TypeScript\nDESCRIPTION: This TypeScript code configures OpenTelemetry in a Vercel (Next.js) application to process and export Vercel AI SDK spans via OpenInference. It sets up the logger for debug inspection, registers an OpenInferenceSimpleSpanProcessor, configures project-specific attributes, and uses OTLPTraceExporter for sending telemetry to Phoenix or Arize. The example highlights filtering for OpenInference spans, API key configuration, and environmental customization. Required dependencies are @opentelemetry/api, @arizeai/openinference-vercel, @vercel/otel, and @opentelemetry/exporter-trace-otlp-proto. Inputs include environment variables for API key and collector endpoint.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/vercel-ai-sdk.md#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { registerOTel } from \"@vercel/otel\";\nimport { diag, DiagConsoleLogger, DiagLogLevel } from \"@opentelemetry/api\";\nimport {\n  isOpenInferenceSpan,\n  OpenInferenceSimpleSpanProcessor,\n} from \"@arizeai/openinference-vercel\";\nimport { OTLPTraceExporter } from \"@opentelemetry/exporter-trace-otlp-proto\";\nimport { SEMRESATTRS_PROJECT_NAME } from \"@arizeai/openinference-semantic-conventions\";\n\n// For troubleshooting, set the log level to DiagLogLevel.DEBUG\n// This is not required and should not be added in a production setting\ndiag.setLogger(new DiagConsoleLogger(), DiagLogLevel.DEBUG);\n\nexport function register() {\n  registerOTel({\n    serviceName: \"phoenix-next-app\",\n    attributes: {\n      // This is not required but it will allow you to send traces to a specific \n      // project in phoenix\n      [SEMRESATTRS_PROJECT_NAME]: \"your-next-app\",\n    },\n    spanProcessors: [\n      new OpenInferenceSimpleSpanProcessor({\n        exporter: new OTLPTraceExporter({\n          headers: {\n            // API key if you're sending it to Phoenix\n            api_key: process.env[\"PHOENIX_API_KEY\"],\n          },\n          url:\n            process.env[\"PHOENIX_COLLECTOR_ENDPOINT\"] ||\n            \"https://app.phoenix.arize.com/v1/traces\",\n        }),\n        spanFilter: (span) => {\n          // Only export spans that are OpenInference to remove non-generative spans\n          // This should be removed if you want to export all spans\n          return isOpenInferenceSpan(span);\n        },\n      }),\n    ],\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Query Data from Parquet into Pandas\nDESCRIPTION: Loads query data from a parquet file hosted on a Google Cloud Storage URL into a pandas DataFrame. This is an example of loading query data from a pre-existing dataset.  The primary dependency is the `pandas` library. The expected output is a pandas DataFrame containing query data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/milvus_llamaindex_search_and_retrieval_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nquery_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/query_data_complete3.parquet\",\n)\nquery_df.head()\n```\n\n----------------------------------------\n\nTITLE: Defining a Phoenix Schema for CV Model Inferences in Python\nDESCRIPTION: Creates a Phoenix Schema object that maps DataFrame columns to their inference roles for computer vision data. Requires a Phoenix import as 'px', with column names for timestamp, prediction, actual label, and embedding features specified. Key parameters: 'timestamp_column_name', 'prediction_label_column_name', 'actual_label_column_name', and an embedding mapping dict referencing columns for vectors and data links. The Schema object is used for proper ingestion and visualization in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/quickstart/phoenix-inferences/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Define Schema to indicate which columns in train_df should map to each field\ntrain_schema = px.Schema(\n    timestamp_column_name=\"prediction_ts\",\n    prediction_label_column_name=\"predicted_action\",\n    actual_label_column_name=\"actual_action\",\n    embedding_feature_column_names={\n        \"image_embedding\": px.EmbeddingColumnNames(\n            vector_column_name=\"image_vector\",\n            link_to_data_column_name=\"url\",\n        ),\n    },\n)\n\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Service - Bash\nDESCRIPTION: This command launches the local Phoenix instance in a terminal environment. It's a necessary step for deploying and utilizing Phoenix in a local command-line setup. This command starts the Phoenix service.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/crewai.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Loading Complete Query Dataset from Parquet File in Python\nDESCRIPTION: Downloads a pre-prepared query dataset from an external URL as a pandas dataframe. This dataset contains additional fields like user feedback and LLM relevance evaluations beyond what's in the basic query data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_search_and_retrieval_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nquery_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/query_data_complete3.parquet\",\n)\nquery_df.head()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Experiment Metadata Configuration - Python\nDESCRIPTION: Defines experiment metadata in a dictionary, specifying model names for LLM, embedding, and reranking. This configuration is used for initializing LlamaIndex and evaluators. Inputs: None (constants). Outputs: experiment_metadata dictionary.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/run_experiments_with_llama_index.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nexperiment_metadata = {\n    \"llm\": \"gpt-4\",\n    \"embed_model\": \"text-embedding-3-small\",\n    \"reranker\": \"cross-encoder/ms-marco-MiniLM-L-2-v2\",\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Zero-Shot Chain-of-Thought Prompt with Phoenix and OpenAI in Python\nDESCRIPTION: Creates a zero-shot Chain-of-Thought (CoT) prompting template instructing the language model to think step-by-step before giving the integer answer. This enhanced prompt emphasizes logical reasoning and final answer formatting. Uses CompletionCreateParamsBase for model configuration and registers this prompt as a new version within Phoenix to enable comparison with the baseline performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/chain_of_thought_prompting.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nzero_shot_COT_template = \"\"\"\nYou are an evaluator who outputs the answer to a math word problem.\n\nYou must always think through the problem logically before providing an answer.\n\nFirst, show some of your reasoning.\n\nThen output the integer answer ONLY on a final new line. In this final answer, be sure not include words, commas, labels, or units and round all decimals answers.\n\n\"\"\"\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"system\", \"content\": zero_shot_COT_template},\n        {\"role\": \"user\", \"content\": \"{{Problem}}\"},\n    ],\n)\n\nzero_shot_COT = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Zero Shot COT prompt\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Llama-Index with OpenTelemetry in Python\nDESCRIPTION: Registers an OpenTelemetry tracer provider using `phoenix.otel.register`, configured to send traces to a local OpenTelemetry collector endpoint (`http://127.0.0.1:4317`). It then instantiates `LlamaIndexInstrumentor` and calls its `instrument` method to automatically instrument Llama-Index operations, capturing trace data for analysis in Phoenix. `skip_dep_check=True` bypasses dependency version checks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntracer_provider = register(endpoint=\"http://127.0.0.1:4317\")\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Running Experiment in Phoenix\nDESCRIPTION: This code snippet runs an experiment in Phoenix using the uploaded dataset. The `run_experiment` function is called with the dataset, the task function (`solve_math_problem`), and a list of evaluators (`goal_evaluator`, `tool_call_evaluator`). The result of the experiment is stored in the `experiment` variable.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/ragas_agents_cookbook_phoenix.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments import run_experiment\n\nexperiment = run_experiment(\n    dataset, task=solve_math_problem, evaluators=[goal_evaluator, tool_call_evaluator]\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Classification Results DataFrame - Python\nDESCRIPTION: Outputs the response_classifications DataFrame after classification, allowing inspection of labels, scores, and explanations. Presumes prior steps have generated this DataFrame via LLM evaluation and scoring.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nresponse_classifications\n```\n\n----------------------------------------\n\nTITLE: Implementing ReAct Prompt with Step-by-Step Reasoning\nDESCRIPTION: This snippet redefines the prompt with a ReAct prompting strategy, instructing the model to think through the problem systematically before selecting tools. It emphasizes reasoning and logical breakdown, aiming to improve task performance through structured thought.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/react_prompting.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nparams = CompletionCreateParamsBase(\n    model=\"gpt-4\",\n    temperature=0.5,\n    tools=tools,\n    tool_choice=\"required\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"\n              You are a helpful customer service agent. Carefully analyze the customerâ€™s question to fully understand their request.\n              Step 1: Think step-by-step. Identify the key pieces of information needed to answer the question. Consider any dependencies between these pieces of information.\n              Step 2: Decide which tools to use. Choose up to 3 tools that will best retrieve the required information. If multiple tools are needed, determine the correct order to call them.\n              Step 3: Output the chosen tools and any relevant parameters.\n            \"\"\",\n        },\n        {\"role\": \"user\", \"content\": \"{{questions}}\"},\n    ],\n)\n\nprompt_identifier = \"customer-support\"\n\nprompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Customer Support ReAct Prompt\",\n    version=PromptVersion.from_openai(params),\n)\n\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Endpoint Environment Variable in Python for Docker Deployment\nDESCRIPTION: This snippet configures the environment variable in Python for a Docker-hosted Phoenix server, pointing your application to the correct endpoint. Dependence on 'os' module allows dynamic setting of the collector URL at runtime.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/anthropic.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Model for Evaluations\nDESCRIPTION: Imports the `OpenAIModel` class from `phoenix.evals` and instantiates it. This configures an OpenAI language model (`gpt-4-turbo-preview`) to be used by the evaluators. It requires setting the `api_key` parameter or ensuring the `OPENAI_API_KEY` environment variable is set.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_quickstart.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import OpenAIModel\n\napi_key = None  # set your api key here or with the OPENAI_API_KEY environment variable\neval_model = OpenAIModel(model_name=\"gpt-4-turbo-preview\", api_key=api_key)\n```\n\n----------------------------------------\n\nTITLE: Converting Agent Messages to Ragas Format in Python\nDESCRIPTION: This Python function `conversation_to_ragas_sample` converts a list of agent messages into the `MultiTurnSample` format required by Ragas for evaluation. It handles different message types (user, function_call, function_call_output, assistant) and can optionally include reference equation or answer for specific Ragas metrics.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/ragas_agents_cookbook_phoenix.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef conversation_to_ragas_sample(messages, reference_equation=None, reference_answer=None):\n    \"\"\"\n    Convert a single conversation into a Ragas MultiTurnSample.\n\n    Args:\n        conversation: Dictionary containing conversation data with 'conversation' key\n        reference_equation: Optional string with the reference equation for evaluation\n\n    Returns:\n        MultiTurnSample: Formatted sample for Ragas evaluation\n    \"\"\"\n\n    import json\n\n    from ragas.dataset_schema import MultiTurnSample\n    from ragas.messages import AIMessage, HumanMessage, ToolCall, ToolMessage\n\n    ragas_messages = []\n    pending_tool_call = None\n    reference_tool_calls = None\n\n    for item in messages:\n        role = item.get(\"role\")\n        item_type = item.get(\"type\")\n\n        if role == \"user\":\n            ragas_messages.append(HumanMessage(content=item[\"content\"]))\n            pending_tool_call = None\n\n        elif item_type == \"function_call\":\n            args = json.loads(item[\"arguments\"])\n            pending_tool_call = ToolCall(name=item[\"name\"], args=args)\n\n        elif item_type == \"function_call_output\":\n            if pending_tool_call is not None:\n                ragas_messages.append(AIMessage(content=\"\", tool_calls=[pending_tool_call]))\n                ragas_messages.append(\n                    ToolMessage(\n                        content=item[\"output\"],\n                        name=pending_tool_call.name,\n                        tool_call_id=\"tool_call_1\",\n                    )\n                )\n                pending_tool_call = None\n            else:\n                print(\"[WARN] ToolMessage without preceding ToolCall â€” skipping.\")\n\n        elif role == \"assistant\":\n            content = (\n                item[\"content\"][0][\"text\"]\n                if isinstance(item.get(\"content\"), list)\n                else item.get(\"content\", \"\")\n            )\n            ragas_messages.append(AIMessage(content=content))\n    print(\"Ragas_messages\", ragas_messages)\n\n    if reference_equation:\n        # Look for the first function call to extract the actual tool call\n        for item in messages:\n            if item.get(\"type\") == \"function_call\":\n                args = json.loads(item[\"arguments\"])\n                reference_tool_calls = [ToolCall(name=item[\"name\"], args=args)]\n                break\n\n        return MultiTurnSample(user_input=ragas_messages, reference_tool_calls=reference_tool_calls)\n\n    elif reference_answer:\n        return MultiTurnSample(user_input=ragas_messages, reference=reference_answer)\n\n    else:\n        return MultiTurnSample(user_input=ragas_messages)\n```\n\n----------------------------------------\n\nTITLE: Setting up Phoenix API Key and Environment Variables in Python\nDESCRIPTION: Configures environment variables for Phoenix API key and collector endpoint in Python to enable tracing and data collection for Phoenix. Requires an API key from Phoenix dashboard.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/autogen-support.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Defining Minimal Phoenix Schema for Implicit Feature Detection in Python\nDESCRIPTION: Sets up a Phoenix schema that only assigns columns for predicted and actual labels, letting Phoenix infer all other DataFrame columns as features by default. Depends on the Phoenix (`px`) library. Only `prediction_label_column_name` and `actual_label_column_name` are specified; all columns not explicitly listed are treated as input features. Suited for datasets with many features where manually listing them is impractical. The DataFrame must have at least the specified label columns; all other columns will be interpreted as features.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/README.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nschema = px.Schema(\n    prediction_label_column_name=\"predicted\",\n    actual_label_column_name=\"target\",\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating GPT-4 summarization classification results\nDESCRIPTION: Evaluates the GPT-4 model's summarization classifications against ground truth labels, generating classification metrics and a confusion matrix visualization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df_sample[\"user_feedback\"].map(templates.SUMMARIZATION_PROMPT_RAILS_MAP).tolist()\nprint(classification_report(true_labels, summarization_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels,\n    predict_vector=summarization_classifications,\n    classes=rails,\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Preparing AsyncIO Environment and Document Imports for Llama-Index in Python\nDESCRIPTION: Prepares the Jupyter environment for nested async operations using nest_asyncio and imports essential classes for document loading (GitbookLoader), manipulation (Document), and indexing (VectorStoreIndex). Calls nest_asyncio.apply() to allow asynchronous code execution inside notebook cells.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\nimport nest_asyncio\nimport pandas as pd\nfrom langchain.document_loaders import GitbookLoader\nfrom llama_index.core import Document, VectorStoreIndex\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Instantiating OpenAIModel with GPT-4 Parameters - Python\nDESCRIPTION: Creates an instance of OpenAIModel configured to use the GPT-4 model for classification tasks. Sets the temperature hyperparameter to 0.0 for deterministic output. Expects a valid OpenAI API key to be configured; this model instance is used for subsequent LLM calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\n```\n\n----------------------------------------\n\nTITLE: Wrapping Production Data into a Phoenix Inferences Object using Python\nDESCRIPTION: Packages the production DataFrame and its schema into a Phoenix Inferences object named \"production\" to enable inclusion alongside the training (reference) data in Phoenix visualizations for comparative analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/inferences_quickstart.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprod_ds = px.Inferences(dataframe=prod_df, schema=prod_schema, name=\"production\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Head of Evaluation DataFrames\nDESCRIPTION: Displays the first few rows of the QA correctness and hallucination evaluation DataFrames.  This is a simple way to inspect the results of the LLM Evals.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nqa_correctness_eval_df.head()\n```\n\nLANGUAGE: python\nCODE:\n```\nhallucination_eval_df.head()\n```\n\n----------------------------------------\n\nTITLE: Configuring DSPy Model and Retriever Components\nDESCRIPTION: Initializes language model and retrieval components (ColBERTv2) and configures DSPy settings to use these components in downstream modules.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/dspy_tracing_tutorial.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nlm = dspy.LM(\"openai/gpt-4\", cache=False)\ncolbertv2_wiki17_abstracts = dspy.ColBERTv2(\n    url=\"http://20.102.90.50:2017/wiki17_abstracts\"  # endpoint for a hosted ColBERTv2 service\n)\n\ndspy.settings.configure(lm=lm, rm=colbertv2_wiki17_abstracts)\n```\n\n----------------------------------------\n\nTITLE: Downloading Dataset from Arize Phoenix (Python)\nDESCRIPTION: Retrieves the dataset previously uploaded to the Phoenix application using its specified name.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/run_experiments_with_llama_index.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nds = px.Client().get_dataset(name=dataset_name)\n```\n\n----------------------------------------\n\nTITLE: Sampling a Single Example for One-Shot Prompting\nDESCRIPTION: Loads the 'test' split of the 'syeddula/fridgeReviews' dataset and randomly samples a single row using `pandas.sample(1)`. This example review and its sentiment will be used in the one-shot prompt.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/few-shot-prompting.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nds = load_dataset(\"syeddula/fridgeReviews\")[\"test\"]\none_shot_example = ds.to_pandas().sample(1)\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving a Prompt in Phoenix\nDESCRIPTION: TypeScript code for creating a prompt with versioning information and saving it to Phoenix for version control and reuse.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/packages/phoenix-client/README.md#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createPrompt, promptVersion } from \"@arizeai/phoenix-client/prompts\";\n\nconst version = createPrompt({\n  name: \"my-prompt\",\n  description: \"test-description\",\n  version: promptVersion({\n    description: \"version description here\",\n    modelProvider: \"OPENAI\",\n    modelName: \"gpt-3.5-turbo\",\n    template: [\n      {\n        role: \"user\",\n        content: \"{{ question }}\",\n      },\n    ],\n    invocationParameters: {\n      temperature: 0.8,\n    },\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Prepare Dataset with Pandas\nDESCRIPTION: This Python snippet creates a Pandas DataFrame with example data for evaluation. The DataFrame includes 'reference', 'query', and 'response' columns, which are necessary for the subsequent evaluation steps. The example dataset consists of question-answer pairs based on a specific reference.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evals_quickstart.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf = pd.DataFrame(\n    [\n        {\n            \"reference\": \"The Eiffel Tower is located in Paris, France. It was constructed in 1889 as the entrance arch to the 1889 World's Fair.\",\n            \"query\": \"Where is the Eiffel Tower located?\",\n            \"response\": \"The Eiffel Tower is located in Paris, France.\",\n        },\n        {\n            \"reference\": \"The Great Wall of China is over 13,000 miles long. It was built over many centuries by various Chinese dynasties to protect against nomadic invasions.\",\n            \"query\": \"How long is the Great Wall of China?\",\n            \"response\": \"The Great Wall of China is approximately 13,171 miles (21,196 kilometers) long.\",\n        },\n        {\n            \"reference\": \"The Amazon rainforest is the largest tropical rainforest in the world. It covers much of northwestern Brazil and extends into Colombia, Peru and other South American countries.\",\n            \"query\": \"What is the largest tropical rainforest?\",\n            \"response\": \"The Amazon rainforest is the largest tropical rainforest in the world. It is home to the largest number of plant and animal species in the world.\",\n        },\n        {\n            \"reference\": \"Mount Everest is the highest mountain on Earth. It is located in the Mahalangur Himal sub-range of the Himalayas, straddling the border between Nepal and Tibet.\",\n            \"query\": \"Which is the highest mountain on Earth?\",\n            \"response\": \"Mount Everest, standing at 29,029 feet (8,848 meters), is the highest mountain on Earth.\",\n        },\n        {\n            \"reference\": \"The Nile is the longest river in the world. It flows northward through northeastern Africa for approximately 6,650 km (4,132 miles) from its most distant source in Burundi to the Mediterranean Sea.\",\n            \"query\": \"What is the longest river in the world?\",\n            \"response\": \"The Nile River, at 6,650 kilometers (4,132 miles), is the longest river in the world.\",\n        },\n        {\n            \"reference\": \"The Mona Lisa was painted by Leonardo da Vinci. It is considered an archetypal masterpiece of the Italian Renaissance and has been described as 'the best known, the most visited, the most written about, the most sung about, the most parodied work of art in the world'.\",\n            \"query\": \"Who painted the Mona Lisa?\",\n            \"response\": \"The Mona Lisa was painted by the Italian Renaissance artist Leonardo da Vinci.\",\n        },\n        {\n            \"reference\": \"The human body has 206 bones. These bones provide structure, protect organs, anchor muscles, and store calcium.\",\n            \"query\": \"How many bones are in the human body?\",\n            \"response\": \"The adult human body typically has 256 bones.\",\n        },\n        {\n            \"reference\": \"Jupiter is the largest planet in our solar system. It is a gas giant with a mass more than two and a half times that of all the other planets in the solar system combined.\",\n            \"query\": \"Which planet is the largest in our solar system?\",\n            \"response\": \"Jupiter is the largest planet in our solar system.\",\n        },\n        {\n            \"reference\": \"William Shakespeare wrote 'Romeo and Juliet'. It is a tragedy about two young star-crossed lovers whose deaths ultimately reconcile their feuding families.\",\n            \"query\": \"Who wrote 'Romeo and Juliet'?\",\n            \"response\": \"The play 'Romeo and Juliet' was written by William Shakespeare.\",\n        },\n        {\n            \"reference\": \"The first moon landing occurred in 1969. On July 20, 1969, American astronauts Neil Armstrong and Edwin 'Buzz' Aldrin became the first humans to land on the moon as part of the Apollo 11 mission.\",\n            \"query\": \"When did the first moon landing occur?\",\n            \"response\": \"The first moon landing took place on July 20, 1969.\",\n        },\n    ]\n)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Searching for Query Answer using Pandas and OpenTelemetry\nDESCRIPTION: This function searches for the answer to a customer inquiry within a policy data DataFrame using Pandas. It accepts a user payload in JSON format, a Pandas DataFrame of policy data, and an OpenTelemetry tracer.  It starts a span named \"Search for Query Answer\", loads the user payload, calls `update_payload_with_search_results` to update the payload, filters the results, sets custom attributes (query_category, query_text, reference_text), defines the span type as \"CHAIN\", sets the status code, and returns the updated payload as a JSON string. Requires `pandas` and `opentelemetry`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef answer_search(\n    user_payload_json: str,\n    policy_df: pd.DataFrame,\n    tracer: opentelemetry.sdk.trace.Tracer,\n) -> str:\n    \"\"\"If customer intent is an inquiry, search for the answer in the policy data\n\n    Parameters\n    ----------\n    user_payload_json : str\n        JSON formatted string of the user payload\n    policy_df : pd.DataFrame\n        Dataframe of policy data\n    tracer : opentelemetry.sdk.trace.Tracer\n        Tracer to handle span creation\n\n    Returns\n    -------\n    str\n        JSON formatted string of the answer payload\n    \"\"\"\n    # Define Span Name & Start\n    with tracer.start_as_current_span(\"Search for Query Answer\") as span:\n        user_payload_dict = json.loads(user_payload_json)\n        updated_dict = update_payload_with_search_results(\n            user_payload_dict, policy_df, \"Category\", \"query_category\"\n        )\n\n        keys_to_update = {\"Question\", \"Answer\"}\n        updated_dict = {\n            k: v for k, v in updated_dict.items() if k in keys_to_update or k in user_payload_dict\n        }\n\n        # Define Custom Attribute String - Shopping Category String\n        span.set_attribute(\"query_category.name\", updated_dict[\"Category\"])\n        # Define Define Custom Attribute String - Query Text String\n        span.set_attribute(\"query_text.name\", updated_dict[\"Question\"])\n        # Define Define Custom Attribute String - Reference Text String\n        span.set_attribute(\"reference_text.name\", updated_dict[\"Answer\"])\n\n        # Define Span Type as \"CHAIN\"\n        span.set_attribute(\n            SpanAttributes.OPENINFERENCE_SPAN_KIND, OpenInferenceSpanKindValues.CHAIN.value\n        )\n\n        # Set Status Code\n        span.set_status(trace_api.StatusCode.OK)\n\n        return json.dumps(updated_dict)\n```\n\n----------------------------------------\n\nTITLE: Combining Evaluation Metrics with Query Embeddings for Analysis in Python\nDESCRIPTION: Reverses the order of the query embeddings DataFrame and concatenates it with hallucination and QA correctness evaluations reset to zero-based indexing. Relevant query metadata columns are also included. This final combined DataFrame prepares a unified dataset for ingestion into Phoenix or other visualization tools to examine detailed LLM evaluation alongside embeddings.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nquery_embeddings_df = query_embeddings_df.iloc[::-1]\nquery_df = pd.concat(\n    [\n        hallucination_eval_df[[\"score\", \"label\"]]\n        .rename(columns={\"score\": \"hallucination_score\", \"label\": \"hallucination_label\"})\n        .reset_index(drop=True),\n        qa_correctness_eval_df[[\"score\", \"label\"]]\n        .rename(columns={\"score\": \"qa_correctness_score\", \"label\": \"qa_correctness_label\"})\n        .reset_index(drop=True),\n        query_embeddings_df[[\"query\", \"response\", \"vector\"]].reset_index(drop=True),\n    ],\n    axis=1,\n)\nquery_df.head()\n```\n\n----------------------------------------\n\nTITLE: Initializing LlamaIndex Components and Loading Data - Python\nDESCRIPTION: Sets default LLM and embedding models for LlamaIndex, creates a SentenceTransformerRerank instance for reranking results, downloads a Paul Graham essay and ingests it as a document into a VectorStoreIndex. Inputs: Model names from experiment_metadata. Dependencies: Internet connectivity for file URL. Outputs: index variable ready for RAG tasks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/run_experiments_with_llama_index.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nSettings.llm = OpenAI(model=experiment_metadata[\"llm\"])\nSettings.embed_model = OpenAIEmbedding(model=experiment_metadata[\"embed_model\"])\nreranker = SentenceTransformerRerank(model=experiment_metadata[\"reranker\"], top_n=2)\n\nessay = \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\"\nwith tempfile.NamedTemporaryFile() as tf:\n    urlretrieve(essay, tf.name)\n    documents = SimpleDirectoryReader(input_files=[tf.name]).load_data()\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix OTEL SDK using pip - Bash\nDESCRIPTION: Installs the arize-phoenix-otel Python package using pip. This is required to add observability via OpenTelemetry for tracing LLM calls made by the application. The package is a prerequisite for enabling tracing with Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/mistralai.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Phoenix\nDESCRIPTION: Uploads the pre-processed dataset (email examples and expected outputs) to Arize Phoenix. Uses the current timestamp to make the dataset name unique.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset = px.Client().upload_dataset(\n    dataset_name=f\"{dataset_name}{datetime.now(timezone.utc)}\",\n    inputs=df.inputs,\n    outputs=df.outputs.map(lambda obj: obj[\"output\"]),\n)\n```\n\n----------------------------------------\n\nTITLE: Formatting Evaluation DataFrame for Phoenix (Python)\nDESCRIPTION: Prepares the `eval_df` for upload to Phoenix. It creates a 'score' column by converting the boolean 'label' column to integers (True=1, False=0) and converts the 'label' column itself to string type. Phoenix specifically looks for 'label' (string) and 'score' (numeric) columns for displaying evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evaluating_traces_quickstart.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\neval_df[\"score\"] = eval_df[\"label\"].astype(int)\neval_df[\"label\"] = eval_df[\"label\"].astype(str)\n```\n\n----------------------------------------\n\nTITLE: Instantiating GPT-3.5-Turbo Model for Hallucination Detection\nDESCRIPTION: Creates an instance of OpenAIModel configured to use GPT-3.5-Turbo with zero temperature and a 20-second timeout for deterministic hallucination classification.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model_name=\"gpt-3.5-turbo\", temperature=0.0, request_timeout=20)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Filtering SpanProcessor in Python\nDESCRIPTION: This code demonstrates how to create a custom FilteringSpanProcessor that extends BatchSpanProcessor to filter out spans based on specific conditions. In this example, spans named \"secret_span\" are prevented from being sent to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/advanced/modifying-spans.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\nfrom phoenix.otel import BatchSpanProcessor\nfrom opentelemetry.context import Context\nfrom opentelemetry.sdk.trace import ReadableSpan, Span\n\n\nclass FilteringSpanProcessor(BatchSpanProcessor):\n    def _filter_condition(self, span: Span) -> bool:\n        # returns True if the span should be filtered out\n        return span.name == \"secret_span\"\n\n    def on_start(self, span: Span, parent_context: Context) -> None:\n        if self._filter_condition(span):\n            return\n        super().on_start(span, parent_context)\n\n    def on_end(self, span: ReadableSpan) -> None:\n        if self._filter_condition(span):\n            logger.info(\"Filtering span: %s\", span.name)\n            return\n        super().on_end(span)\n\n\ntracer_provider = register()\ntracer_provider.add_span_processor(\n    FilteringSpanProcessor(\n        endpoint=\"http://localhost:6006/v1/traces\",\n        protocol=\"http/protobuf\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Instantiating OpenAIModel and Generating a Completion - Python\nDESCRIPTION: Demonstrates how to create an OpenAI model instance and perform a basic completion using the OpenAIModel class. You must have the required OpenAI credentials (API key, etc.) either set as environment variables or passed explicitly. The primary input is a prompt string; the output is the completion result returned by the model. Ensure the required 'openai>=1.0.0' dependency is installed before use.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evaluation-models.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAI()\nmodel(\"Hello there, this is a test if you are working?\")\n# Output: \"Hello! I'm working perfectly. How can I assist you today?\"\n```\n\n----------------------------------------\n\nTITLE: Extracting Tool Call Names from Agent Output JSON in Python\nDESCRIPTION: Defines a function `extract_tool_calls` that parses the JSON output field to identify and collect names of tool calls present within the response content under type 'tool_use'. Applies this function to the outputs field of the trace dataframe to create a new column listing tool calls made by the agent in each span.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/amazon_bedrock_agents_tracing_and_evals.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Function to extract tool call names from the output\ndef extract_tool_calls(output_value):\n    tool_calls = []\n    try:\n        o = json.loads(output_value)\n\n        # Check if the output has 'content' which is a list of message components\n        if \"content\" in o and isinstance(o[\"content\"], list):\n            for item in o[\"content\"]:\n                # Check if this item is a tool_use type\n                if isinstance(item, dict) and item.get(\"type\") == \"tool_use\":\n                    # Extract the name of the tool being called\n                    tool_name = item.get(\"name\")\n                    if tool_name:\n                        tool_calls.append(tool_name)\n    except (json.JSONDecodeError, TypeError, AttributeError):\n        pass\n\n    return tool_calls\n\n\n# Apply the function to each row of trace_df.output.value\ntrace_df[\"tool_call\"] = trace_df[\"outputs\"].apply(\n    lambda x: extract_tool_calls(x) if isinstance(x, str) else []\n)\n\n# Display the tool calls found\nprint(\"Tool calls found in traces:\", trace_df[\"tool_call\"].sum())\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Agents and Instrumentation (Bash)\nDESCRIPTION: Installs the necessary libraries for using the OpenAI Agents SDK and the specific OpenInference instrumentation for it, enabling tracing of agent operations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai-agents-sdk.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-openai-agents openai-agents\n```\n\n----------------------------------------\n\nTITLE: Running Classifications with Explanations\nDESCRIPTION: This code snippet runs code functionality classifications on a smaller sample of the DataFrame, enabling explanations. It uses `llm_classify` with `provide_explanation=True` and `verbose=True` to generate explanations for each classification. A smaller dataframe sample of 5 rows is used.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_functionality_classifications.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsmall_df_sample = df.copy().sample(n=5).reset_index(drop=True)\nrelevance_classifications_df = llm_classify(\n    dataframe=small_df_sample,\n    template=CODE_FUNCTIONALITY_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    provide_explanation=True,\n    verbose=True,\n    concurrency=20,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and running a DSPy application with tracing\nDESCRIPTION: Complete example of a DSPy application that defines a BasicQA signature and uses it with OpenAI. Includes custom attributes for tracing like session_id, user_id, metadata, and tags.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/dspy.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass BasicQA(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n\n\nif __name__ == \"__main__\":\n    turbo = dspy.OpenAI(model=\"gpt-3.5-turbo\")\n\n    dspy.settings.configure(lm=turbo)\n\n    with using_attributes(\n        session_id=\"my-test-session\",\n        user_id=\"my-test-user\",\n        metadata={\n            \"test-int\": 1,\n            \"test-str\": \"string\",\n            \"test-list\": [1, 2, 3],\n            \"test-dict\": {\n                \"key-1\": \"val-1\",\n                \"key-2\": \"val-2\",\n            },\n        },\n        tags=[\"tag-1\", \"tag-2\"],\n        prompt_template_version=\"v1.0\",\n        prompt_template_variables={\n            \"city\": \"Johannesburg\",\n            \"date\": \"July 11th\",\n        },\n    ):\n        # Define the predictor.\n        generate_answer = dspy.Predict(BasicQA)\n\n        # Call the predictor on a particular input.\n        pred = generate_answer(\n            question=\"What is the capital of the united states?\"\n        )\n        print(f\"Predicted Answer: {pred.answer}\")\n```\n\n----------------------------------------\n\nTITLE: Generating SQL Queries via LLM and Querying DuckDB - Python\nDESCRIPTION: Defines a prompt template and helper function for generating SQL queries using the LLM and a Phoenix-traced tool for querying sales data from the loaded Parquet data via DuckDB. The 'lookup_sales_data' function, decorated with tracing, automates SQL creation and execution based on a user prompt, returning results as a string. Prerequisites: the store_sales_df DataFrame must be loaded, and tracing/dependencies set up. Inputs include a user prompt about sales data; output is a string with the query results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nSQL_GENERATION_PROMPT = \"\"\"\nGenerate an SQL query based on a prompt. Do not reply with anything besides the SQL query.\nThe prompt is: {prompt}\n\nThe available columns are: {columns}\nThe table name is: {table_name}\n\"\"\"\n\n\ndef generate_sql_query(prompt: str, columns: list, table_name: str) -> str:\n    \"\"\"Generate an SQL query based on a prompt\"\"\"\n    formatted_prompt = SQL_GENERATION_PROMPT.format(\n        prompt=prompt, columns=columns, table_name=table_name\n    )\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n    )\n\n    return response.choices[0].message.content\n\n\n@tracer.tool()\ndef lookup_sales_data(prompt: str) -> str:\n    \"\"\"Implementation of sales data lookup from parquet file using SQL\"\"\"\n    try:\n        table_name = \"sales\"\n        # Read the parquet file into a DuckDB table\n        duckdb.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} AS SELECT * FROM store_sales_df\")\n\n        print(store_sales_df.columns)\n        print(table_name)\n        sql_query = generate_sql_query(prompt, store_sales_df.columns, table_name)\n        sql_query = sql_query.strip()\n        sql_query = sql_query.replace(\"```sql\", \"\").replace(\"```\", \"\")\n\n        with tracer.start_as_current_span(\n            \"execute_sql_query\", openinference_span_kind=\"chain\"\n        ) as span:\n            span.set_input(value=sql_query)\n\n            # Execute the SQL query\n            result = duckdb.sql(sql_query).df()\n            span.set_output(value=str(result))\n            span.set_status(StatusCode.OK)\n        return result.to_string()\n    except Exception as e:\n        return f\"Error accessing data: {str(e)}\"\n```\n\n----------------------------------------\n\nTITLE: Defining ReAct Prompt Function Component\nDESCRIPTION: Creates the function component that formats prompts using the ReAct chat formatter, incorporating tools, chat history, and current reasoning state for the agent's decision-making process.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n## define prompt function\n\n\ndef react_prompt_fn(\n    task: Task, state: Dict[str, Any], input: str, tools: List[BaseTool]\n) -> List[ChatMessage]:\n    # Add input to reasoning\n    chat_formatter = ReActChatFormatter()\n    return chat_formatter.format(\n        tools,\n        chat_history=task.memory.get() + state[\"memory\"].get_all(),\n        current_reasoning=state[\"current_reasoning\"],\n    )\n\n\nreact_prompt_component = AgentFnComponent(fn=react_prompt_fn, partial_dict={\"tools\": [sql_tool]})\n```\n\n----------------------------------------\n\nTITLE: Querying and Printing Responses (RAG)\nDESCRIPTION: This snippet iterates through each question in the `questions_with_document_chunk_df` DataFrame, queries the `query_engine` with the question, and prints the question along with the response from the `query_engine`. It assumes the existence of a `query_engine` object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# loop over the questions and generate the answers\nfor _, row in questions_with_document_chunk_df.iterrows():\n    question = row[\"question\"]\n    response_vector = query_engine.query(question)\n    print(f\"Question: {question}\\nAnswer: {response_vector.response}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Defining Content Moderation Prompt (Python)\nDESCRIPTION: Defines the `MessageCreateParamsBase` for a content moderation task, including a structured prompt template with placeholders for guidelines and user text.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncontent = \"\"\"\n    You are a content moderation expert tasked with categorizing user-generated text based on the following guidelines:\n\n    {{guidelines}}\n\n    Here is the user-generated text to categorize:\n    <user_text>{{user_text}}</user_text>\n\n    Based on the guidelines above, classify this text as either ALLOW or BLOCK. Return nothing else.\\\n\"\"\"\n\nparams = MessageCreateParamsBase(\n    model=\"claude-3-5-haiku-latest\",\n    max_tokens=10,\n    messages=[{\"role\": \"user\", \"content\": dedent(content)}],\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix from a Notebook - Python\nDESCRIPTION: Imports the phoenix module and launches the Phoenix observability app within a notebook environment. Enables in-notebook visualization and tracing capture. Note that traces are not persisted after the notebook session ends unless self-hosting or external deployment is used.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/mistralai.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Running Task and Evals Together\nDESCRIPTION: This code snippet runs the task and the defined evaluators in a single step using `run_experiment`.  This streamlined approach performs the entire experiment from task execution to evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n_ = run_experiment(dataset, task, evaluators)\n```\n\n----------------------------------------\n\nTITLE: Classifying User Intent using OpenAI API and OpenTelemetry\nDESCRIPTION: This function uses the OpenAI API to classify user intent as either a purchase or an inquiry.  It takes a user prompt, a user payload in JSON format, and an OpenTelemetry tracer as input.  It starts a new span named \"Classify User Intent\", loads the user payload, calls the OpenAI API, updates the payload with the API response, sets custom attributes (customerID, customerInput, premiumCustomer), defines the span type as \"CHAIN\", sets the status code, and returns the updated payload as a JSON string. It requires the `openai` and `opentelemetry` packages.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef openai_classify_user_intent(\n    user_prompt: str, user_payload_json: str, tracer: opentelemetry.sdk.trace.Tracer\n) -> str:\n    \"\"\"\n    Classify the user intent as either a purchase or an inquiry using the OpenAI API\n\n    Parameters\n    ----------\n    user_prompt : str\n        Prompt template for OpenAI API\n    user_payload_json : str\n        User JSON payload with Customer ID, Customer Input, and Premium Customer\n    tracer : opentelemetry.sdk.trace.Tracer\n        Tracer to handle span creation\n\n    Returns\n    -------\n    str\n        JSON formatted string of the user payload with the updated response from the OpenAI API\n    \"\"\"\n    with tracer.start_as_current_span(\"Classify User Intent\") as span:  # Define Span Name & Start\n        user_payload_dict = json.loads(user_payload_json)\n        customer_input = user_payload_dict.get(\"Customer Input\", \"\")\n        response_dict = call_openai_api(user_prompt, customer_input)\n        user_payload_dict.update(response_dict)\n\n        # Define Custom Attribute String - Customer ID\n        span.set_attribute(\"customerID.name\", user_payload_dict[\"Customer ID\"])\n        # Define Custom Attribute String - Customer Input\n        span.set_attribute(\"customerInput.name\", user_payload_dict[\"Customer Input\"])\n        # Define Custom Attribute String - Premium Customer Bool String\n        span.set_attribute(\"premiumCustomer.name\", user_payload_dict[\"Premium Customer\"])\n\n        # Define Span Type as \"CHAIN\"\n        span.set_attribute(\n            SpanAttributes.OPENINFERENCE_SPAN_KIND, OpenInferenceSpanKindValues.CHAIN.value\n        )\n\n        # Set Status Code\n        span.set_status(trace_api.StatusCode.OK)\n\n        return json.dumps(user_payload_dict)\n```\n\n----------------------------------------\n\nTITLE: Running Evaluations on Experiment (Python)\nDESCRIPTION: Takes the completed `experiment` object (from the dry run or a full run) and applies the defined `evaluators` to each run within the experiment. The results of the evaluations are stored within the `experiment` object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nexperiment = evaluate_experiment(experiment, evaluators)\n```\n\n----------------------------------------\n\nTITLE: Launch Phoenix Application Python\nDESCRIPTION: Initializes and launches the Phoenix application in a new process. This is required to visualize traces and evaluate experiments performed in the tutorial. If a Phoenix server is already running, this step can be skipped.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport phoenix as px\n\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Defining and Running the Evaluation Task Function in Python\nDESCRIPTION: This asynchronous Python function `solve_math_problem` serves as a wrapper to run the agent for evaluation purposes. It takes an input, runs the agent using `Runner.run`, and returns a dictionary containing the final output and the message history. The snippet then demonstrates running this task function with `asyncio.run` and printing the result.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/ragas_agents_cookbook_phoenix.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom agents import Runner\n\n\n# This is our task function. It takes a question and returns the final output and the messages recorded to generate the final output.\nasync def solve_math_problem(input):\n    if isinstance(input, dict):\n        input = next(iter(input.values()))\n    result = await Runner.run(agent, input)\n    return {\"final_output\": result.final_output, \"messages\": result.to_input_list()}\n\n\nresult = asyncio.run(solve_math_problem(\"What is 15 + 28?\"))\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenInference and OpenTelemetry\nDESCRIPTION: This snippet shows how to install the required packages for OpenInference and OpenTelemetry using pip. These packages are necessary for using the tracing decorators and methods.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-semantic-conventions opentelemetry-api opentelemetry-sdk\n```\n\n----------------------------------------\n\nTITLE: Cloning the Phoenix Repository using Git (Bash)\nDESCRIPTION: This Bash command clones the official Phoenix repository from GitHub using git. Before running migration commands or modifying migration files, cloning the repository is required to obtain the necessary project resources. It requires Git to be installed and network access to GitHub. The output is a local folder named 'phoenix' containing the project files.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/src/phoenix/db/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/Arize-ai/phoenix.git\n```\n\n----------------------------------------\n\nTITLE: Querying Span Data from Phoenix for Last 7 Days\nDESCRIPTION: Initializes Phoenix client, defines start and end times for data retrieval over the past week, queries spans, and displays the first few entries of the resulting DataFrame. Facilitates evaluation and analysis of model traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_openai_tutorial.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom datetime import datetime, timedelta\n\nimport phoenix as px\n\n# Initiate Phoenix client\npx_client = px.Client()\n\n# Get spans from the last 7 days only\nstart = datetime.now() - timedelta(days=7)\n\n# Get spans to exclude the last 24 hours\nend = datetime.now() - timedelta(days=0)\n\nphoenix_df = px_client.query_spans(start_time=start, end_time=end)\nphoenix_df.head()\n```\n\n----------------------------------------\n\nTITLE: Generating duplicate joke evaluations (Python)\nDESCRIPTION: Processes the downloaded spans DataFrame to identify duplicate jokes based on the output message content. Creates a new DataFrame with span IDs and a 'label' column indicating whether the joke in that span is a duplicate encountered previously.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/feedback-and-annotations/evaluating-phoenix-traces.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Create a new DataFrame with selected columns\neval_df = spans_df[[\"context.span_id\", \"attributes.llm.output_messages\"]].copy()\neval_df.set_index(\"context.span_id\", inplace=True)\n\n# Create a list to store unique jokes\nunique_jokes = set()\n\n\n# Function to check if a joke is a duplicate\ndef is_duplicate(joke_data):\n    joke = joke_data[0][\"message.content\"]\n    if joke in unique_jokes:\n        return True\n    else:\n        unique_jokes.add(joke)\n        return False\n\n\n# Apply the is_duplicate function to create the new column\neval_df[\"label\"] = eval_df[\"attributes.llm.output_messages\"].apply(is_duplicate)\n\n# Convert boolean to integer (0 for False, 1 for True)\neval_df[\"label\"] = eval_df[\"label\"]\n\n# Reset unique_jokes list to ensure correct results if the cell is run multiple times\nunique_jokes.clear()\n```\n\n----------------------------------------\n\nTITLE: Printing Phoenix Session URL (Python)\nDESCRIPTION: Prints the URL of the current Phoenix session, providing a direct link to access the Phoenix web interface to view the traces collected from the RAG application execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprint(session.url)\n```\n\n----------------------------------------\n\nTITLE: Creating Inferences Object for RAG Data in Phoenix\nDESCRIPTION: This code initializes a Phoenix Inferences object by pairing the dataframe containing RAG data with the defined schema.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/retrieval-rag.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprimary_inferences = px.Inferences(primary_dataframe, primary_schema)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix - Python\nDESCRIPTION: Launches a Phoenix application within a Jupyter Notebook environment. This allows for real-time visualization of traces. It utilizes the `phoenix` library to start the Phoenix app, providing a URL to access the tracing dashboard in a browser.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Defining Tools for LLM\nDESCRIPTION: Defines a list of tools that the LLM can use to interact with external systems. Each tool is described by its type (function), name, description, and parameters, including the parameter types, descriptions, and requirements.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/react_prompting.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"product_comparison\",\n            \"description\": \"Compare features of two products.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"product_a_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unique identifier of Product A.\",\n                    },\n                    \"product_b_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unique identifier of Product B.\",\n                    },\n                },\n                \"required\": [\"product_a_id\", \"product_b_id\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"product_details\",\n            \"description\": \"Get detailed features on one product.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"product_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unique identifier of the Product.\",\n                    }\n                },\n                \"required\": [\"product_id\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"apply_discount_code\",\n            \"description\": \"Checks for discounts and promotions. Applies a discount code to an order.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"order_id\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The unique identifier of the order.\",\n                    },\n                    \"discount_code\": {\n                        \"type\": \"string\",\n                        \"description\": \"The discount code to apply.\",\n                    },\n                },\n                \"required\": [\"order_id\", \"discount_code\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"customer_support\",\n            \"description\": \"Get contact information for customer support regarding an issue.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"issue_type\": {\n                        \"type\": \"string\",\n                        \"description\": \"The type of issue (e.g., billing, technical support).\",\n                    }\n                },\n                \"required\": [\"issue_type\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"track_package\",\n            \"description\": \"Track the status of a package based on the tracking number.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"tracking_number\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The tracking number of the package.\",\n                    }\n                },\n                \"required\": [\"tracking_number\"],\n            },\n        },\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Running and Tracing Groq Chat Completion (Python)\nDESCRIPTION: Implements a basic Groq chat completion API call, instrumented for tracing. Imports the Groq SDK and configures the client with the API key from the 'GROQ_API_KEY' environment variable. Sends a request to the model with a single user message and prints the generated assistant response. Requires proper environment configuration and package installation as prerequisites.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/groq.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom groq import Groq\n\nclient = Groq(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"GROQ_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain the importance of low latency LLMs\",\n        }\n    ],\n    model=\"mixtral-8x7b-32768\",\n)\nprint(chat_completion.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluation Function and Running a Prompt Experiment with OpenAI in Python\nDESCRIPTION: Defines a function that formats the prompt input and sends a chat completion request to OpenAI, extracting the label from the response. Uses run_experiment from Phoenix to execute the evaluation across the dataset. Collects results for further analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef openai_eval(input):\n    formatted_prompt = prompt.format(variables=dict(input))\n    response = openai.OpenAI().chat.completions.create(**formatted_prompt)\n    return {\"label\": response.choices[0].message.content}\n```\n\n----------------------------------------\n\nTITLE: Install Base Phoenix Python Package\nDESCRIPTION: Installs the base `arize-phoenix` Python package. This is used for launching local Phoenix instances directly from the CLI or within a Notebook environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/vertexai.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Evaluating Classification Performance with Confusion Matrix and Report\nDESCRIPTION: Compares predicted labels to ground-truth labels, printing classification metrics and visualizing the confusion matrix with normalized counts in a blue color scheme.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ntrue_labels = df[\"readable\"].map(CODE_READABILITY_PROMPT_RAILS_MAP).tolist()\n\nprint(classification_report(true_labels, readability_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels, predict_vector=readability_classifications, classes=rails\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Example Traces and Accessing Events (Python)\nDESCRIPTION: This code loads example trace data from Phoenix using the `load_example_traces` function and then accesses a specific event from the traces' dataframe. The `load_example_traces` function takes a string representing the example trace type as input. It returns a Dataset object containing the traces. Then, the first event is accessed from the \"events\" column of the resulting DataFrame. The output is the specified event.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/tracing.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntraces_ds = px.load_example_traces(\"random\")\n\ntraces_ds.dataframe[\"events\"][0]\n```\n\n----------------------------------------\n\nTITLE: Applying Complex Boolean Filters to Spans Using SpanQuery - Python\nDESCRIPTION: This example shows how to use the where method in SpanQuery to filter spans based on multiple boolean conditions, including substring membership within attributes. The filter retrieves RETRIEVER spans whose input value contains the word 'programming'. The query expression must be a valid Python boolean string, with no external function calls permitted.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nquery = SpanQuery().where(\n    \"span_kind == 'RETRIEVER' and 'programming' in input.value\"\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix SDK and dependencies via pip\nDESCRIPTION: Installs the necessary Phoenix SDK package to enable tracing functionality within Python projects. It ensures the environment has the correct minimum version needed for the tutorial.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/tracing_quickstart_openai.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q \"arize-phoenix>=4.29.0\"\n```\n\n----------------------------------------\n\nTITLE: Running Zero-Shot CoT Prompt Experiment and Evaluation\nDESCRIPTION: Defines a Python function to execute the Zero-Shot CoT prompt against the dataset, parsing the model's response to extract both the full reasoning and the final answer. It applies the evaluation logic to the extracted final answer to assess accuracy. The results are tracked by running the task and evaluation via the Phoenix experiment framework.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/chain-of-thought-prompting.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport re\n\n\ndef zero_shot_COT_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **zero_shot_COT.format(variables={\"Problem\": input[\"Word Problem\"]})\n    )\n    response_text = resp.choices[0].message.content.strip()\n    lines = response_text.split(\"\\n\")\n    final_answer = lines[-1].strip()\n    final_answer = re.sub(r\"^\\*\\*(\\d+)\\*\\*$\", r\"\\1\", final_answer)\n    return {\"full_response\": response_text, \"final_answer\": final_answer}\n\n\ndef evaluate_response(output, expected):\n    final_answer = output[\"final_answer\"]\n    if not final_answer.isdigit():\n        return False\n    return int(final_answer) == int(expected[\"Answer\"])\n\n\ninitial_experiment = run_experiment(\n    dataset,\n    task=zero_shot_COT_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Zero-Shot COT Prompt\",\n    experiment_name=\"zero-shot-cot-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + zero_shot_COT.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application with RAG Data\nDESCRIPTION: This snippet demonstrates how to launch the Phoenix application with the prepared RAG inferences object for analysis and visualization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/retrieval-rag.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsession = px.launch_app(primary_inferences)\n```\n\n----------------------------------------\n\nTITLE: Launching Arize Phoenix Application with RAG Data Python\nDESCRIPTION: This snippet launches the Arize Phoenix interactive application in the user's web browser, pre-loaded with the RAG data contained within the `primary_inferences` object. The launched application allows users to visualize, analyze, and debug the RAG data and model performance based on the provided schema. This is the entry point for interactive data exploration in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/define-your-schema/retrieval-rag.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsession = px.launch_app(primary_inferences)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Dependencies via Bash\nDESCRIPTION: Installs necessary Python packages using pip within a Bash environment (likely a notebook cell magic %%bash). It installs specific versions of `arize-phoenix`, `openai`, `httpx`, `openinference-instrumentation-openai`, `gcsfs`, `nest-asyncio`, `openinference-instrumentation-llama-index`, `llama-index-callbacks-arize-phoenix`, `llama-index`, and `llama-index-llms-openai`. These packages are required for tracing, interacting with OpenAI and LlamaIndex, and handling asynchronous operations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_llamaindex_tutorial.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n%%bash\n\npip install -q \"arize-phoenix>=4.29.0\" openai 'httpx<0.28' openinference-instrumentation-openai\npip install -q gcsfs nest-asyncio \"openinference-instrumentation-llama-index>=3.0.0\"\npip install -qU llama-index-callbacks-arize-phoenix\npip install -qU llama-index llama-index-llms-openai\n```\n\n----------------------------------------\n\nTITLE: Fetching and Applying Prompt with Data (Python)\nDESCRIPTION: This snippet retrieves a saved prompt from Phoenix using `Client().prompts.get` and then applies it to sample data. The `variables` dictionary provides the input `desc` for the prompt.  It then calls the Anthropic model using the formatted prompt.  The output is then processed by iterating through the response content, extracting, and pretty printing the result via `pp(content.input)` if tool_use is detected. Dependencies include Anthropic and Phoenix clients along with access to an Anthropic model.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nprompt = Client().prompts.get(prompt_identifier=prompt_identifier)\n\nvariables = {\n    \"desc\": \"The man is tall, with a beard and a scar on his left cheek. He has a deep voice and wears a black leather jacket.\"\n}\nresponse = Anthropic().messages.create(**prompt.format(variables=variables))\nfor content in response.content:\n    if content.type == \"tool_use\":\n        pp(content.input)\n        break\n\n```\n\n----------------------------------------\n\nTITLE: Prepare Evaluation Dataset (Python)\nDESCRIPTION: This Python snippet prepares a dataset in the form of a Pandas DataFrame for evaluation. It defines a list of dictionaries, where each dictionary represents a data point containing `reference`, `query`, and `response` fields.  The DataFrame is then created using this data. This data represents example Q&A pairs to evaluate the LLM's response to the input queries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/quickstart/evals.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf = pd.DataFrame(\n    [\n        {\n            \"reference\": \"The Eiffel Tower is located in Paris, France. It was constructed in 1889 as the entrance arch to the 1889 World's Fair.\",\n            \"query\": \"Where is the Eiffel Tower located?\",\n            \"response\": \"The Eiffel Tower is located in Paris, France.\",\n        },\n        {\n            \"reference\": \"The Great Wall of China is over 13,000 miles long. It was built over many centuries by various Chinese dynasties to protect against nomadic invasions.\",\n            \"query\": \"How long is the Great Wall of China?\",\n            \"response\": \"The Great Wall of China is approximately 13,171 miles (21,196 kilometers) long.\",\n        },\n        {\n            \"reference\": \"The Amazon rainforest is the largest tropical rainforest in the world. It covers much of northwestern Brazil and extends into Colombia, Peru and other South American countries.\",\n            \"query\": \"What is the largest tropical rainforest?\",\n            \"response\": \"The Amazon rainforest is the largest tropical rainforest in the world. It is home to the largest number of plant and animal species in the world.\",\n        },\n        {\n            \"reference\": \"Mount Everest is the highest mountain on Earth. It is located in the Mahalangur Himal sub-range of the Himalayas, straddling the border between Nepal and Tibet.\",\n            \"query\": \"Which is the highest mountain on Earth?\",\n            \"response\": \"Mount Everest, standing at 29,029 feet (8,848 meters), is the highest mountain on Earth.\",\n        },\n        {\n            \"reference\": \"The Nile is the longest river in the world. It flows northward through northeastern Africa for approximately 6,650 km (4,132 miles) from its most distant source in Burundi to the Mediterranean Sea.\",\n            \"query\": \"What is the longest river in the world?\",\n            \"response\": \"The Nile River, at 6,650 kilometers (4,132 miles), is the longest river in the world.\",\n        },\n        {\n            \"reference\": \"The Mona Lisa was painted by Leonardo da Vinci. It is considered an archetypal masterpiece of the Italian Renaissance and has been described as 'the best known, the most visited, the most written about, the most sung about, the most parodied work of art in the world'.\",\n            \"query\": \"Who painted the Mona Lisa?\",\n            \"response\": \"The Mona Lisa was painted by the Italian Renaissance artist Leonardo da Vinci.\",\n        },\n        {\n            \"reference\": \"The human body has 206 bones. These bones provide structure, protect organs, anchor muscles, and store calcium.\",\n            \"query\": \"How many bones are in the human body?\",\n            \"response\": \"The adult human body typically has 256 bones.\",\n        },\n        {\n            \"reference\": \"Jupiter is the largest planet in our solar system. It is a gas giant with a mass more than two and a half times that of all the other planets in the solar system combined.\",\n            \"query\": \"Which planet is the largest in our solar system?\",\n            \"response\": \"Jupiter is the largest planet in our solar system.\",\n        },\n        {\n            \"reference\": \"William Shakespeare wrote 'Romeo and Juliet'. It is a tragedy about two young star-crossed lovers whose deaths ultimately reconcile their feuding families.\",\n            \"query\": \"Who wrote 'Romeo and Juliet'?\",\n            \"response\": \"The play 'Romeo and Juliet' was written by William Shakespeare.\",\n        },\n        {\n            \"reference\": \"The first moon landing occurred in 1969. On July 20, 1969, American astronauts Neil Armstrong and Edwin 'Buzz' Aldrin became the first humans to land on the moon as part of the Apollo 11 mission.\",\n            \"query\": \"When did the first moon landing occur?\",\n            \"response\": \"The first moon landing took place on July 20, 1969.\",\n        },\n    ]\n)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Running Experiment with Gradient Optimized Prompt\nDESCRIPTION: This snippet runs an experiment using the gradient-optimized prompt, evaluates the responses, and logs the experiment results.  It defines the `task`, evaluators, experiment description, experiment name and relevant experiment metadata.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ngradient_experiment = run_experiment(\n    dataset,\n    task=test_gradient_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Prompt Optimization Technique #3: Prompt Gradients\",\n    experiment_name=\"gradient-optimization\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + gradient_prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Bias Detection Prompt and OpenAI Model Initialization in Python\nDESCRIPTION: This snippet imports necessary evaluation components and applies `nest_asyncio` to allow nested event loops needed for asynchronous OpenAI API calls. It then initializes an `OpenAIModel` instance configured to use the \"gpt-4o\" model with temperature set to 0.0 for deterministic outputs. Additionally, it defines a multi-line string prompt designed to instruct the model to analyze document bias based on an input question and document text, requesting both a bias classification label and an explanation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/llamaindex-workflows-research-agent/evaluate_traces.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.evals import (\n    OpenAIModel,\n    llm_classify,\n)\n\nnest_asyncio.apply()\n\nmodel = OpenAIModel(\n    model=\"gpt-4o\",\n    temperature=0.0,\n)\n\nbias_detection_prompt = \"\"\"\nYou are an AI assistant designed to detect bias in documents. Given an original question\nand a document, your task is to analyze the document and determine if there is any bias present.\n\nPlease follow these steps:\n1. Read the original question carefully.\n2. Read the document provided.\n3. Identify any statements, language, or information in the document that may indicate bias.\n4. Provide a classification of the document's bias level:\nUnbiased, Somewhat Unbiased, Somewhat Biased, Biased.\n5. Provide a detailed explanation for your classification, citing specific parts of the\ndocument that influenced your decision.\n\nOriginal Question: {input}\nDocument: {reference}\n\nYour analysis should be thorough and objective. Please ensure that your explanation\nis clear and concise.\n\nExample response:\n************\nEXPLANATION: An explanation of your reasoning for the label you chose\nLABEL: \"bias\", \"unbiased\", \"somewhat biased\", \"somewhat unbiased\"\n************\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Processing Generated SQL Queries and Building Dataset\nDESCRIPTION: Iterates through the generated SQL queries, attempts to execute each one using a predefined `execute_query` function, and structures the results into a list (`generated_dataset`). Includes basic error handling for failed queries during execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ngenerated_dataset = []\nfor q in generated_questions:\n    try:\n        result = execute_query(q[\"sql\"])\n        generated_dataset.append(\n            {\n                \"input\": q[\"question\"],\n                \"expected\": {\n                    \"results\": result,\n                    \"error\": None,\n                    \"query\": q[\"sql\"],\n                },\n                \"metadata\": {\n                    \"category\": \"Generated\",\n                },\n            }\n        )\n    except duckdb.Error as e:\n        print(f\"Query failed: {q['sql']}\", e)\n        print(\"Skipping...\")\n\ngenerated_dataset[0]\n```\n\n----------------------------------------\n\nTITLE: Deploying Phoenix with PostgreSQL backend using Kubernetes\nDESCRIPTION: Command to deploy Phoenix connected to a remote PostgreSQL database using kubectl and kustomize. This applies the PostgreSQL overlay to the base configuration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/kustomize/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -k kustomize/backends/postgres\n```\n\n----------------------------------------\n\nTITLE: Deploying LangGraph App to Agent Engine\nDESCRIPTION: This code shows the deployment of the `SimpleLangGraphApp` to Agent Engine. It uses `agent_engines.create()` to deploy the application with necessary dependencies and metadata.  It also specifies the required packages for deployment.  The deployed application is given a display name and description for identification and management within Agent Engine.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/google_agent_engine_tracing_tutorial.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nremote_agent = agent_engines.create(\n    SimpleLangGraphApp(project=PROJECT_ID, location=LOCATION),\n    requirements=[\n        \"google-cloud-aiplatform[agent_engines,langchain]==1.87.0\",\n        \"cloudpickle==3.0.0\",\n        \"pydantic==2.11.2\",\n        \"langgraph==0.2.76\",\n        \"httpx\",\n        \"arize-phoenix-otel>=0.9.0\",\n        \"openinference-instrumentation-langchain>=0.1.4\",\n    ],\n    display_name=\"Agent Engine with LangGraph\",\n    description=\"This is a sample custom application in Agent Engine that uses LangGraph\",\n    extra_packages=[],\n)\n```\n\n----------------------------------------\n\nTITLE: Formulating Item Search Description Prompt for OpenAI in Python\nDESCRIPTION: Creates a prompt template for the AI to help customers by outputting detailed item descriptions focusing on features, price, and how the item meets customer needs. The output is structured as JSON to provide clear information supporting the customer's purchase decisions on an e-commerce platform.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nitem_search_prompt = \"\"\"\nYou are a helpful assistant designed to output JSON. Support the shopping process for customers in an e-commerce shop.\n\nWhen an item matches the customer's search criteria, your response should offer a concise and objective description of the item, focusing on its features, price, how it addresses their product search and any relevant details pertinent to the customer's needs.\n\nOutput JSON where the key is \\\"customer_response\\\" and the value is a thorough description of the item. Highlight the item's features and specifications that meet the customer's requirements and any additional information necessary for an informed purchase.\n\nStructure your response as follows:\n\nkey: \\\"customer_response\\\"\nvalue: \\\"<Your detailed, objective description here>\\\"\n\nThe goal is to equip the customer with all the necessary information about the item, focusing on providing factual and relevant details to assist them in their decision-making process.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Filtering Spans by Evaluation Results with Special DSL Syntax - Python\nDESCRIPTION: This snippet explains how to filter tracing spans based on the results of automatic or manual evaluations, using the evals indexer syntax in SpanQuery.where. The query returns only spans where the 'correctness' evaluation label is 'incorrect'. Evaluation results must have been previously computed and ingested to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nquery = SpanQuery().where(\n    \"evals['correctness'].label == 'incorrect'\"\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Phoenix Tracing UI URL in Python\nDESCRIPTION: Prints the URL of the active Phoenix session's web-based UI. This URL provides access to the interactive interface for visualizing and exploring telemetry traces, enabling users to troubleshoot and analyze LLM application execution details.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(\"The Phoenix UI:\", px.active_session().url)\n```\n\n----------------------------------------\n\nTITLE: Viewing Uploaded Dataset as a DataFrame in Python\nDESCRIPTION: Retrieves the uploaded dataset from Phoenix and presents it as a pandas DataFrame for inspection or further data manipulation. This allows users to visually verify dataset contents programmatically.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset.as_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Defining Tools for Agent - Python\nDESCRIPTION: This code defines a list of tools that the agent can use. Each tool specifies its name, description, and the expected parameters. These tools represent functions the agent can call to perform specific tasks related to sales data analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"lookup_sales_data\",\n            \"description\": \"Look up data from Store Sales Price Elasticity Promotions dataset\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"prompt\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unchanged prompt that the user provided.\",\n                    }\n                },\n                \"required\": [\"prompt\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"analyze_sales_data\",\n            \"description\": \"Analyze sales data to extract insights\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"data\": {\n                        \"type\": \"string\",\n                        \"description\": \"The lookup_sales_data tool's output.\",\n                    },\n                    \"prompt\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unchanged prompt that the user provided.\",\n                    },\n                },\n                \"required\": [\"data\", \"prompt\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"generate_visualization\",\n            \"description\": \"Generate Python code to create data visualizations\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"data\": {\n                        \"type\": \"string\",\n                        \"description\": \"The lookup_sales_data tool's output.\",\n                    },\n                    \"visualization_goal\": {\n                        \"type\": \"string\",\n                        \"description\": \"The goal of the visualization.\",\n                    },\n                },\n                \"required\": [\"data\", \"visualization_goal\"],\n            },\n        },\n    },\n    # {\n    #     \"type\": \"function\",\n    #     \"function\": {\n    #         \"name\": \"run_python_code\",\n    #         \"description\": \"Run Python code in a restricted environment\",\n    #         \"parameters\": {\n    #             \"type\": \"object\",\n    #             \"properties\": {\n    #                 \"code\": {\"type\": \"string\", \"description\": \"The Python code to run.\"}\n    #             },\n    #             \"required\": [\"code\"]\n    #         }\n    #     }\n    # }\n]\n```\n\n----------------------------------------\n\nTITLE: Using Summarization Eval Template - Python\nDESCRIPTION: This snippet demonstrates how to use the `SUMMARIZATION_PROMPT_TEMPLATE` to classify summaries using an OpenAI model. It imports necessary modules from the `phoenix.evals` library, initializes an `OpenAIModel`, and calls the `llm_classify` function to perform the evaluation. The 'rails' parameter ensures that the output from the LLM is restricted to the defined values, and the `provide_explanation` parameter enables the generation of explanations for the results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/summarization-eval.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport phoenix.evals.default_templates as templates\nfrom phoenix.evals import (\n    OpenAIModel,\n    download_benchmark_dataset,\n    llm_classify,\n)\n\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\n\n#The rails is used to hold the output to specific values based on the template\n#It will remove text such as \",,,\" or \"...\"\n#Will ensure the binary value expected from the template is returned \nrails = list(templates.SUMMARIZATION_PROMPT_RAILS_MAP.values())\nsummarization_classifications = llm_classify(\n    dataframe=df_sample,\n    template=templates.SUMMARIZATION_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    provide_explanation=True, #optional to generate explanations for the value produced by the eval LLM\n)\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing Phoenix Inferences Dataset in Python\nDESCRIPTION: Instantiates a Phoenix inferences dataset by passing a pandas DataFrame and a Schema object. Requires the pandas and Phoenix (`px`) libraries. Accepts the DataFrame `df` with data and a properly constructed `schema`; optionally takes a `name` for display in the UI. Returns a Phoenix Inferences dataset object that can be used for further analysis or visualization. Typical input is data that has been preprocessed and structured according to the desired schema.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nds = px.Inferences(df, schema)\n```\n\n----------------------------------------\n\nTITLE: Viewing QA Correctness Evaluation Results\nDESCRIPTION: Displays the head of the dataframe containing QA correctness evaluations to assess LLM response quality.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nqa_correctness_eval_df.head()\n```\n\n----------------------------------------\n\nTITLE: Preview of Hallucination Evaluation Results\nDESCRIPTION: Displays the first two records of the hallucination evaluation results, indicating whether responses contain hallucinations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_application_tracing_evaluating_and_analysis.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nhallucination_eval.head(2)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix in a Jupyter notebook\nDESCRIPTION: Python code to install and launch Phoenix directly from within a Jupyter notebook environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/guardrails-ai.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Traces DataFrame - Python\nDESCRIPTION: This code retrieves a DataFrame of the spans from the phoenix session, useful for introspecting the data captured within the Phoenix Tracing for the RAG pipeline.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nspans_df = px.Client().get_spans_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Printing Phoenix Tracing UI URL in Python\nDESCRIPTION: Outputs the URL of the Arize Phoenix web interface from the session object that was obtained when launching Phoenix. This allows the user to easily access the UI to view the collected trace data generated by the instrumented CrewAI workflow.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/crewai_tracing_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"View traces at {session.url}\")\n```\n\n----------------------------------------\n\nTITLE: Saving Prompt in Phoenix from OpenAI Response\nDESCRIPTION: Creates and stores a prompt in Phoenix's prompt registry with metadata including name, description, and version derived from OpenAI's response parameters. Enables reuse and management of prompts.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nprompt_identifier = \"haiku-recursion\"\n\nprompt = Client().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Haiku about recursion in programming\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Llama-Index with OpenTelemetry in Python\nDESCRIPTION: Sets up OpenTelemetry tracing provider configured to send telemetry data to a local endpoint and applies instrumentation to the Llama-Index library. This enables tracing of operations within Llama-Index for observability and performance monitoring. The instrumentation skips dependency checks for quicker initialization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntracer_provider = register(endpoint=\"http://127.0.0.1:4317\")\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Extracting Retrieved Documents from Phoenix Traces\nDESCRIPTION: Gets the documents that were retrieved for each query from the Phoenix client's trace data for evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_retrieved_documents\n\nretrieved_documents_df = get_retrieved_documents(px.Client())\nretrieved_documents_df\n```\n\n----------------------------------------\n\nTITLE: Setting up LangChain Agent with GPT-4o\nDESCRIPTION: Configures the LangChain agent with GPT-4o, specifying the model and binding it to the schema defined in the registry. It uses JsonOutputFunctionsParser to format output as JSON. This agent's purpose is to extract the key information from an email.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel = \"gpt-4o\"\n\nllm = ChatOpenAI(model=model).bind_functions(\n    functions=[registry[dataset_name].schema],\n    function_call=registry[dataset_name].schema.schema()[\"title\"],\n)\noutput_parser = JsonOutputFunctionsParser()\nextraction_chain = registry[dataset_name].instructions | llm | output_parser\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application - Python\nDESCRIPTION: This code launches the Phoenix application, providing a user interface for monitoring and evaluating the RAG pipeline.  It allows the user to view the traces and monitor the performance of the RAG system.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\n(session := px.launch_app()).view()\n```\n\n----------------------------------------\n\nTITLE: Collecting Traces with Instrumented Bedrock Client\nDESCRIPTION: This snippet demonstrates collecting traces by sending a series of trivia questions to the instrumented Bedrock client. It iterates through a list of questions, constructs a prompt for each, calls the Bedrock model, and prints the question and answer. The calls to `invoke_model` are traced thanks to OpenInference.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntrivia_questions = [\n    \"What is the only U.S. state that starts with two vowels?\",\n    \"What is the 3rd month of the year in alphabetical order?\",\n    \"What is the capital of Mongolia?\",\n    \"How many minutes are there in a leap year?\",\n    \"If a train leaves New York at 3 PM traveling west at 60 mph, and another leaves Chicago at 4 PM traveling east at 80 mph, at what time will they meet?\",\n    \"Which element has the chemical symbol 'Fe'?\",\n    \"What five-letter word becomes shorter when you add two letters to it?\",\n    \"What country has won the most FIFA World Cups?\",\n    \"If today is Wednesday, what day of the week will it be 100 days from now?\",\n    \"A farmer has 17 sheep and all but 9 run away. How many does he have left?\",\n]\n\nfor i, question in enumerate(trivia_questions, start=1):\n    prompt_str = f\"\"\"\n{{\n    \"prompt\": \"Human: {question} Assistant:\",\n    \"max_tokens_to_sample\": 300\n}}\n\"\"\"\n    response = instrumented_client.invoke_model(\n        modelId=\"anthropic.claude-v2:1\",\n        body=prompt_str.encode(\"utf-8\"),\n        contentType=\"application/json\",\n        accept=\"application/json\",\n    )\n\n    response_body = json.loads(response.get(\"body\").read())\n    print(f\"Q{i}: {question}\")\n    print(f\"A{i}: {response_body['completion'].strip()}\\n{'-'*60}\")\n```\n\n----------------------------------------\n\nTITLE: Computing ROUGE-L Scores for Summary Evaluation\nDESCRIPTION: Defines a function to compute ROUGE-L scores comparing LLM-generated summaries with human reference summaries, then applies it to both baseline and recent datasets.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_summarization_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef compute_rougeL_scores(df: pd.DataFrame) -> pd.Series:\n    return rouge(\n        response_col=df[\"summary\"],\n        references_col=df[\"reference_summary\"],\n        rouge_types=[\"rougeL\"],\n    )[\"rougeL\"]\n\n\nbaseline_df[\"rougeL_score\"] = compute_rougeL_scores(baseline_df)\nrecent_df[\"rougeL_score\"] = compute_rougeL_scores(recent_df)\n```\n\n----------------------------------------\n\nTITLE: Resetting DataFrame Index for Auto-Embeddings in Python\nDESCRIPTION: Code snippet showing how to reset a DataFrame index before generating embeddings, which is required because Arize expects the dataframe's index to be sorted and begin at 0.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/generating-embeddings.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = df.reset_index(drop=True)\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluation Functions Using Phoenix Evals (Python)\nDESCRIPTION: This snippet imports necessary modules and defines multiple evaluation functions that classify model outputs using templates and models like GPT-4. Functions include generic evaluation, routing, function call, and parameter extraction evaluations, returning evaluation results with scores, labels, and explanations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.evals import (\n    OpenAIModel,\n    llm_classify,\n)\nfrom phoenix.experiments.types import EvaluationResult\n\n\ndef generic_eval(input, output, prompt_template):\n    df_in = pd.DataFrame({\"question\": [str(input[\"question\"] Cas)], \"response\": [str(output)]})\n    rails = [\"correct\", \"incorrect\"]\n    eval_df = llm_classify(\n        data=df_in,\n        template=prompt_template,\n        model=OpenAIModel(model=\"gpt-4o\"),\n        rails=rails,\n        provide_explanation=True,\n    )\n    label = eval_df[\"label\"][0]\n    score = (\n        1 if rails and label == rails[0] else 0\n    )  # Choose the 0 item in rails as the correct \"1\" label\n    explanation = eval_df[\"explanation\"][0]\n    return EvaluationResult(score=score, label=label, explanation=explanation)\n\n\ndef routing_eval(input, output):\n    return generic_eval(input, output, ROUTER_EVAL_TEMPLATE)\n\n\ndef function_call_eval(input, output):\n    return generic_eval(input, output, FUNCTION_SELECTION_EVAL_TEMPLATE)\n\n\ndef parameter_extraction_eval(input, output):\n    return generic_eval(input, output, PARAMETER_EXTRACTION_EVAL_TEMPLATE)\n\n```\n\n----------------------------------------\n\nTITLE: Compute Hit\nDESCRIPTION: This code snippet computes a 'hit' metric, which determines if at least one relevant document was retrieved within the top k (in this case, k=2). It groups the documents by the 'context.span_id' and sums the relevance scores (again, considers the top 2), checking if the sum is greater than 0.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nhit = pd.DataFrame(\n    {\n        \"hit\": documents_with_relevance_df.groupby(\"context.span_id\").apply(\n            lambda x: x.eval_score[:2].sum(skipna=False) > 0\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting LlamaIndex with OpenTelemetry for Tracing in Python\nDESCRIPTION: Sets up OpenTelemetry tracing by registering a tracer provider endpoint for telemetry data collection at a local server. Instruments the LlamaIndex component using the LlamaIndexInstrumentor to automatically generate telemetry spans for tracing query operations. Skips dependency checks to ensure instrumentation regardless of environment conditions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Adding Classification Results as New Columns in DataFrame in Python\nDESCRIPTION: Assigns the previously extracted labels and explanation lists as new columns 'label' and 'explanation' to the working DataFrame df, thus augmenting the dataset with classification metadata.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndf[\"label\"] = labels\ndf[\"explanation\"] = explanation\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: This command installs the necessary Python dependencies required to run the MCP client and server tracing example. The `pip install -r requirements.txt` command uses `pip`, the Python package installer, to install all packages listed in the `requirements.txt` file.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mcp/tracing_between_mcp_client_and_server/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key (Python)\nDESCRIPTION: This snippet retrieves the OpenAI API key from the environment variable `OPENAI_API_KEY` or prompts the user to enter it. It then instantiates the OpenAI client using the provided API key. This ensures the OpenAI client is authenticated and ready to use.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_sessions_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nfrom openai import OpenAI\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nclient = OpenAI(api_key=openai_api_key)\n```\n\n----------------------------------------\n\nTITLE: Converting LlamaIndex Storage Context to Dataframe in Python\nDESCRIPTION: Defines a function to extract document IDs, texts, and embeddings from a LlamaIndex storage context and convert them to a pandas dataframe. The function handles deduplication of document chunks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_search_and_retrieval_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef storage_context_to_dataframe(storage_context: StorageContext) -> pd.DataFrame:\n    \"\"\"Converts the storage context to a pandas dataframe.\n\n    Args:\n        storage_context (StorageContext): Storage context containing the index\n        data.\n\n    Returns:\n        pd.DataFrame: The dataframe containing the index data.\n    \"\"\"\n    document_ids = []\n    document_texts = []\n    document_embeddings = []\n    docstore = storage_context.docstore\n    vector_store = storage_context.vector_store\n    for node_id, node in docstore.docs.items():\n        document_ids.append(node.hash)  # use node hash as the document ID\n        document_texts.append(node.text)\n        document_embeddings.append(np.array(vector_store.get(node_id)))\n    return pd.DataFrame(\n        {\n            \"document_id\": document_ids,\n            \"text\": document_texts,\n            \"text_vector\": document_embeddings,\n        }\n    )\n\n\ndatabase_df = storage_context_to_dataframe(storage_context)\ndatabase_df = database_df.drop_duplicates(subset=[\"text\"])\ndatabase_df.head()\n```\n\n----------------------------------------\n\nTITLE: Defining an Async Text2SQL Task Pipeline in Python\nDESCRIPTION: Creates an async function text2sql that generates a SQL query from a question, executes it on the database, and returns the query, results, and error if any. Catches duckdb errors and wraps outputs in a dictionary. Dependencies: generate_query, execute_query, duckdb. Input: a question string. Output: dictionary with 'query', 'results', and 'error' keys.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nasync def text2sql(question):\n    query = await generate_query(question)\n    results = None\n    error = None\n    try:\n        results = execute_query(query)\n    except duckdb.Error as e:\n        error = str(e)\n\n    return {\n        \"query\": query,\n        \"results\": results,\n        \"error\": error,\n    }\n```\n\n----------------------------------------\n\nTITLE: Running Initial Prompt Experiment with Phoenix and Asyncio Integration in Python\nDESCRIPTION: Runs an initial experiment on the base prompt using the 'run_experiment' function from Phoenix experiments module. Applies 'nest_asyncio' to enable nested event loops in Jupyter or similar environments. The experiment uses the test dataset, the 'test_prompt' task, and the 'evaluate_response' evaluator, with metadata linking to the prompt version saved in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.experiments import run_experiment\n\nnest_asyncio.apply()\n\ninitial_experiment = run_experiment(\n    dataset,\n    task=test_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Initial base prompt\",\n    experiment_name=\"initial-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Formatting and Sending Anthropic Chat Completion Requests in Python\nDESCRIPTION: Formats the prompt with variables and Anthropic SDK identifiers and sends a prompt to Anthropic's API. Prints the first text response from the model. Requires environment variable ANTHROPIC_API_KEY for authentication.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nvariables = dict(ds.as_dataframe().input.iloc[0])\nformatted_prompt = prompt.format(variables=variables, sdk=\"anthropic\")\nresponse = anthropic.Anthropic().messages.create(**{**formatted_prompt, \"model\": anthropic_model})\nprint(response.content[0].text)\n```\n\n----------------------------------------\n\nTITLE: Serializing Llama-Index Documents to JSON in Python\nDESCRIPTION: Converts Llama-Index Document objects to dictionaries and saves them as a prettified JSON file for use outside the notebook. Uses the to_dict() method for conversion and json.dump for file I/O. Accepts the documents variable and outputs a file named demo_llama_index_documents.json.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Convert documents to a JSON serializable format (if needed)\ndocuments_json = [doc.to_dict() for doc in documents]\n\n# Save documents to a JSON file\nwith open(\"demo_llama_index_documents.json\", \"w\") as file:\n    json.dump(documents_json, file, indent=4)\n```\n\n----------------------------------------\n\nTITLE: Defining Task Function using OpenAI LLM in Python\nDESCRIPTION: Initializes a Llama-Index OpenAI LLM instance using the 'gpt-3.5-turbo' model. Defines a synchronous function `task` that accepts a dictionary `input` (representing a dataset example). Inside the function, it constructs a prompt by concatenating the 'document' and the 'content' of the last message from 'messages', sends this prompt to the initialized LLM using `llm.complete()`, and returns the generated text response (`.text`).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\n\ndef task(input):\n    return llm.complete(input[\"document\"] + \"\\n\\n\" + input[\"messages\"][-1][\"content\"]).text\n```\n\n----------------------------------------\n\nTITLE: Applying nest_asyncio for Asynchronous Requests\nDESCRIPTION: This snippet applies the `nest_asyncio` patch to enable re-entrant event loops, which is useful for asynchronous request submission in notebook environments like Jupyter or Google Colab. This can significantly speed up eval submission by about 5x depending on the organization's rate limits.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_functionality_classifications.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Printing Phoenix UI URL Python\nDESCRIPTION: Prints a message that includes the URL of the Phoenix UI, allowing the user to easily navigate to the UI to view the trace data collected during the extraction process. It is a simple way to notify the user and encourage them to explore the real-time tracing capabilities.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/anthropic_tracing_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"ðŸ”¥ðŸ¦ Open the Phoenix UI if you haven't already: {session.url}\")\n```\n\n----------------------------------------\n\nTITLE: Python: Retrieve Current Span ID with OpenTelemetry SDK\nDESCRIPTION: This snippet demonstrates how to obtain the current span's ID using the OpenTelemetry SDK in Python. The span ID is necessary for associating feedback annotations with specific spans in a trace.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/feedback-and-annotations/capture-feedback.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom opentelemetry.trace import format_span_id, get_current_span\n\nspan = get_current_span()\nspan_id = format_span_id(span.get_span_context().span_id)\n```\n\n----------------------------------------\n\nTITLE: Printing Active Phoenix Session URL - Python\nDESCRIPTION: Displays the URL for the currently active Arize Phoenix session. This URL can be used to access the Phoenix UI and visualize the traces captured during the code execution, aiding in debugging and analysis of the RAG pipeline.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprint(\"phoenix URL\", px.active_session().url)\n```\n\n----------------------------------------\n\nTITLE: Viewing Evaluation Results as DataFrame\nDESCRIPTION: Displays the aggregated evaluation results in DataFrame format, enabling analysis of model performance across dataset samples.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nexperiment.get_evaluations()\n```\n\n----------------------------------------\n\nTITLE: Displaying summarization classification template\nDESCRIPTION: Displays the default prompt template used to evaluate text summarizations. This template can be modified to experiment with different evaluation approaches.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(templates.SUMMARIZATION_PROMPT_TEMPLATE)\n```\n\n----------------------------------------\n\nTITLE: Sample OpenAI API Call with Tracing\nDESCRIPTION: Example implementation showing how to use the OpenAI library with automatic tracing enabled, making a simple chat completions call and ensuring trace data is flushed to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-ts.md#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\n// main.ts\nimport OpenAI from \"openai\";\n\n// set OPENAI_API_KEY in environment, or pass it in arguments\nconst openai = new OpenAI();\n\nopenai.chat.completions\n  .create({\n    model: \"gpt-4o\",\n    messages: [{ role: \"user\", content: \"Write a haiku.\" }],\n  })\n  .then((response) => {\n    console.log(response.choices[0].message.content);\n  })\n  // for demonstration purposes, keep the node process alive long\n  // enough for BatchSpanProcessor to flush Trace to Phoenix\n  // with its default flush time of 5 seconds\n  .then(() => new Promise((resolve) => setTimeout(resolve, 6000)));\n```\n\n----------------------------------------\n\nTITLE: Defining LiteLLMModel Parameters in Python\nDESCRIPTION: This snippet defines the `LiteLLMModel` class, inheriting from `BaseEvalModel`. It specifies parameters for interacting with models via the LiteLLM library, including model name, temperature, max tokens, top_p, retry logic, request timeout, and model-specific keyword arguments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evaluation-models.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass LiteLLMModel(BaseEvalModel):\n    model: str = \"gpt-3.5-turbo\"\n    \"\"\"The model name to use.\"\"\"\n    temperature: float = 0.0\n    \"\"\"What sampling temperature to use.\"\"\"\n    max_tokens: int = 256\n    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n    top_p: float = 1\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n    num_retries: int = 6\n    \"\"\"Maximum number to retry a model if an RateLimitError, OpenAIError, or\n    ServiceUnavailableError occurs.\"\"\"\n    request_timeout: int = 60\n    \"\"\"Maximum number of seconds to wait when retrying.\"\"\"\n    model_kwargs: Dict[str, Any] = field(default_factory=dict)\n    \"\"\"Model specific params\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Running Q&A Classification with GPT-3.5 Turbo\nDESCRIPTION: Executes the same classification task using GPT-3.5 Turbo for comparison with GPT-4's performance on the same dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nQ_and_A_classifications = llm_classify(\n    dataframe=df_sample,\n    template=QA_PROMPT_TEMPLATE,\n    model=model,\n    rails=list(QA_PROMPT_RAILS_MAP.values()),\n    concurrency=20,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: OTEL Span Example for Retriever\nDESCRIPTION: This JSON snippet represents an example OTEL span for a Retriever operation. It includes metadata such as trace ID, span ID, start and end times, and attributes related to the document retrieval process. Key attributes include the input value, retrieved document IDs, scores, content, and metadata. The `openinference.span.kind` attribute is set to \"RETRIEVER\".\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/concepts-tracing/what-are-traces.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n     \"name\": \"retrieve\",\n     \"context\": {\n         \"trace_id\": \"0x6c80880dbeb609e2ed41e06a6397a0dd\",\n         \"span_id\": \"0x03f3466720f4bfc7\",\n         \"trace_state\": \"[]\"\n     },\n     \"kind\": \"SpanKind.INTERNAL\",\n     \"parent_id\": \"0x7eb5df0046c77cd2\",\n     \"start_time\": \"2024-05-08T21:46:11.044464Z\",\n     \"end_time\": \"2024-05-08T21:46:11.465803Z\",\n     \"status\": {\n         \"status_code\": \"OK\"\n     },\n     \"attributes\": {\n         \"openinference.span.kind\": \"RETRIEVER\",\n         \"input.value\": \"tell me about postal service\",\n         \"retrieval.documents.0.document.id\": \"6d4e27be-1d6d-4084-a619-351a44834f38\",\n         \"retrieval.documents.0.document.score\": 0.7711453293100421,\n         \"retrieval.documents.0.document.content\": \"<document-chunk-1>\",       \n         \"retrieval.documents.0.document.metadata\": \"{\\\"page_label\\\": \\\"7\\\", \\\"file_name\\\": \\\"/data/101.pdf\\\", \\\"file_path\\\": \\\"/data/101.pdf\\\", \\\"file_type\\\": \\\"application/pdf\\\", \\\"file_size\\\": 47931, \\\"creation_date\\\": \\\"2024-04-12\\\", \\\"last_modified_date\\\": \\\"2024-04-12\\\"}\",\n         \"retrieval.documents.1.document.id\": \"869d9f6d-db9a-43c4-842f-74bd8d505147\",\n         \"retrieval.documents.1.document.score\": 0.7672439175862021,\n         \"retrieval.documents.1.document.content\": \"<document-chunk-2>\",\n         \"retrieval.documents.1.document.metadata\": \"{\\\"page_label\\\": \\\"6\\\", \\\"file_name\\\": \\\"/data/101.pdf\\\", \\\"file_path\\\": \\\"/data/101.pdf\\\", \\\"file_type\\\": \\\"application/pdf\\\", \\\"file_size\\\": 47931, \\\"creation_date\\\": \\\"2024-04-12\\\", \\\"last_modified_date\\\": \\\"2024-04-12\\\"}\",\n         \"retrieval.documents.2.document.id\": \"72b5cb6b-464f-4460-b497-cc7c09d1dbef\",\n         \"retrieval.documents.2.document.score\": 0.7647611816897794,\n         \"retrieval.documents.2.document.content\": \"<document-chunk-3>\",\n         \"retrieval.documents.2.document.metadata\": \"{\\\"page_label\\\": \\\"4\\\", \\\"file_name\\\": \\\"/data/101.pdf\\\", \\\"file_path\\\": \\\"/data/101.pdf\\\", \\\"file_type\\\": \\\"application/pdf\\\", \\\"file_size\\\": 47931, \\\"creation_date\\\": \\\"2024-04-12\\\", \\\"last_modified_date\\\": \\\"2024-04-12\\\"}\"\n     },\n     \"events\": [],\n     \"links\": [],\n     \"resource\": {\n         \"attributes\": {},\n         \"schema_url\": \"\"\n     }\n }\n```\n\n----------------------------------------\n\nTITLE: Optimizing a Prompt Using Gradients\nDESCRIPTION: This snippet defines a function `optimize_prompt` that uses the calculated gradient to optimize a base prompt.  It gets the embedding of the base prompt, moves in the direction of the gradient in embedding space, and then uses the OpenAI API to generate a new prompt that corresponds to the optimized embedding.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Function to optimize a prompt using the gradient\ndef optimize_prompt(base_prompt, gradient, step_size=0.1):\n    # Get base embedding\n    base_embedding = get_embedding(base_prompt)\n\n    # Move in gradient direction\n    optimized_embedding = base_embedding + step_size * gradient\n\n    # Use GPT to convert the optimized embedding back to text\n    client = OpenAI()\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are helping to optimize prompts. Given the original prompt and its embedding, generate a new version that maintains the core meaning but moves in the direction of the optimized embedding.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Original prompt: {base_prompt}\\nOptimized embedding direction: {optimized_embedding[:10]}...\\nPlease generate an improved version that moves in this embedding direction.\",\n            },\n        ],\n    )\n    return response.choices[0].message.content.strip()\n\n\n# Test the gradient-based optimization\ngradient_prompt = optimize_prompt(original_base_prompt, gradient)\n```\n\n----------------------------------------\n\nTITLE: Querying the RouterQueryEngine for the Most Expensive Digital Camera Using Python\nDESCRIPTION: Sends the natural language query 'What is the most expensive digital camera?' to the RouterQueryEngine, which routes it to the SQL backend for structured information retrieval. Prints the text response to the console. This illustrates routing triggering SQL queries visible in Phoenix traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_sql_retriever_tutorial.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nresponse = query_engine.query(\"What is the most expensive digital camera?\")\nprint(str(response))\n```\n\n----------------------------------------\n\nTITLE: Defining the TraceDataset Class in Phoenix (Python)\nDESCRIPTION: This code snippet defines the TraceDataset class, which wraps a pandas DataFrame representing spans and traces for LLM observability using the Phoenix library. The constructor accepts a DataFrame and an optional name for dataset identification. The TraceDataset is primarily intended for loading previously saved trace data rather than real-time ingestion. Dependencies include pandas for DataFrame support. The dataframe parameter expects each row to be a flattened span, and the name parameter, if omitted, will cause a random name to be auto-generated.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/inference-and-schema.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass TraceDataset(\n    dataframe: pandas.DataFrame,\n    name: Optional[str] = None,\n)\n```\n\n----------------------------------------\n\nTITLE: Workflow Edges and Conditional Logic Setup\nDESCRIPTION: Specifies the sequential and conditional transitions between workflow nodes to control the process flow from start to finish, including looping for query generation and validation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nworkflow.add_edge(START, \"first_tool_call\")\nworkflow.add_edge(\"first_tool_call\", \"list_tables_tool\")\nworkflow.add_edge(\"list_tables_tool\", \"model_get_schema\")\nworkflow.add_edge(\"model_get_schema\", \"get_schema_tool\")\nworkflow.add_edge(\"get_schema_tool\", \"query_gen\")\nworkflow.add_conditional_edges(\n    \"query_gen\",\n    should_continue,\n)\nworkflow.add_edge(\"correct_query\", \"execute_query\")\nworkflow.add_edge(\"execute_query\", \"query_gen\")\n```\n\n----------------------------------------\n\nTITLE: Configuring DSPy with OpenAI\nDESCRIPTION: This snippet configures DSPy to use the OpenAI `gpt-3.5-turbo` model for language model tasks. It initializes the language model and configures DSPy settings.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n# Import DSPy and set up the language model\nimport dspy\n\n# Configure DSPy to use OpenAI\nturbo = dspy.LM(model=\"gpt-3.5-turbo\")\ndspy.settings.configure(lm=turbo)\n```\n\n----------------------------------------\n\nTITLE: Creating UUID, Defining Agent Tool Responses\nDESCRIPTION: This code snippet initializes a unique identifier (UUID) for the experiment and defines a dictionary containing question-tool call mappings for ground truth evaluation of tool calling. It sets up a dataset to compare the agent's tool selection against the expected tool calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n\nid = str(uuid.uuid4())\n\nagent_tool_responses = {\n    \"What was the most popular product SKU?\": \"lookup_sales_data, analyze_sales_data\",\n    \"What was the total revenue across all stores?\": \"lookup_sales_data, analyze_sales_data\",\n    \"Which store had the highest sales volume?\": \"lookup_sales_data, analyze_sales_data\",\n    \"Create a bar chart showing total sales by store\": \"generate_visualization, lookup_sales_data, run_python_code\",\n    \"What percentage of items were sold on promotion?\": \"lookup_sales_data, analyze_sales_data\",\n    \"Plot daily sales volume over time\": \"generate_visualization, lookup_sales_data, run_python_code\",\n    \"What was the average transaction value?\": \"lookup_sales_data, analyze_sales_data\",\n    \"Create a box plot of transaction values\": \"generate_visualization, lookup_sales_data, run_python_code\",\n    \"Which products were frequently purchased together?\": \"lookup_sales_data, analyze_sales_data\",\n    \"Plot a line graph showing the sales trend over time with a 7-day moving average\": \"generate_visualization, lookup_sales_data, run_python_code\",\n}\n\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies for Phoenix Project\nDESCRIPTION: This snippet lists the required Python packages and their versions for the Phoenix project. It's crucial for setting up the correct environment and ensuring compatibility. Dependencies cover a broad range including OpenAI, Gradio, LangChain, and various instrumentation tools.  The '==' symbol specifies the exact required version, '>=' specifies the minimum version, and '~=' is a pessimistic version constraint.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/agent_framework_comparison/requirements.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nopenai==1.52.2\ngradio==5.11.0\npython-dotenv==1.0.1\narize-phoenix>=4.29.0\n\nlangchain==0.2.17\nlangchain_openai==0.1.25\nlanggraph==0.2.39\nllama_index==0.12.9\nllama_index.llms.openai==0.2.15\nautogen~=0.2\ncrewai==0.70.1\n\nopeninference-instrumentation==0.1.18\nopeninference-instrumentation-openai==0.1.14\nopeninference-instrumentation-langchain==0.1.28\nopeninference-instrumentation-llama-index==3.0.3\nopeninference-instrumentation-crewai==0.1.2\nopeninference-instrumentation-litellm==0.1.4\n```\n\n----------------------------------------\n\nTITLE: Importing Core Libraries and Configuring Environment - Python\nDESCRIPTION: Imports standard libraries like `typing` and `pandas`. Applies `nest_asyncio` for compatibility in notebook environments to allow concurrent operations and sets pandas display options to prevent truncation of data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/summarization.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Any, Dict\n\nimport nest_asyncio\nimport pandas as pd\n\nnest_asyncio.apply()  # needed for concurrent evals in notebook environments\npd.set_option(\"display.max_colwidth\", None)  # display full cells of dataframes\n```\n\n----------------------------------------\n\nTITLE: Running LangChain RetrievalQA Chain with Tracing for Multiple Queries in Python\nDESCRIPTION: Iterates over the first 10 user queries and invokes the RetrievalQA chain for each, which triggers execution and automatic tracing due to prior instrumentation. Uses tqdm to display progress. This runs the LLM application while collecting span traces to analyze later in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_vertex_ai_tracing_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfor query in tqdm(queries[:10]):\n    chain.invoke(query)\n```\n\n----------------------------------------\n\nTITLE: Saving Prompt in Phoenix (Python)\nDESCRIPTION: This snippet creates and saves the constructed prompt to Phoenix. It utilizes `Client().prompts.create` to register the prompt with Phoenix, assigning it a unique `prompt_identifier`, a descriptive `name`, and a `prompt_description`. It leverages `PromptVersion.from_anthropic(params)` to use the parameters defined in the previous step.  Dependencies include the Phoenix client library and an active Phoenix instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# prompt identifier should contain only alphanumeric characters, hyphens or underscores\nprompt_identifier = \"character-characteristics\"\n\nprompt = Client().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Character characteristics\",\n    version=PromptVersion.from_anthropic(params),\n)\n\n```\n\n----------------------------------------\n\nTITLE: Defining LLM Prompt Template for Question Generation - Python\nDESCRIPTION: Defines a multi-line string variable containing a prompt template designed to instruct an LLM to generate questions based on provided context. The template specifies the required input format (context), the desired output format (JSON with specific keys), the persona of the LLM (Teacher/Professor), and constraints (generate 3 diverse questions strictly from the context).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ngenerate_questions_template = \"\"\"\\nContext information is below.\\n\\n---------------------\\n{text}\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup \\n3 questions for an upcoming \\nquiz/examination. The questions should be diverse in nature \\nacross the document. Restrict the questions to the \\ncontext information provided.\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Computing NDCG@2 for Retrieval Evaluation\nDESCRIPTION: Calculates the Normalized Discounted Cumulative Gain at 2 metric to evaluate the quality of document retrieval rankings.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom sklearn.metrics import ndcg_score\n\n\ndef _compute_ndcg(df: pd.DataFrame, k: int):\n    \"\"\"Compute NDCG@k in the presence of missing values\"\"\"\n    n = max(2, len(df))\n    eval_scores = np.zeros(n)\n    doc_scores = np.zeros(n)\n    eval_scores[: len(df)] = df.eval_score\n    doc_scores[: len(df)] = df.document_score\n    try:\n        return ndcg_score([eval_scores], [doc_scores], k=k)\n    except ValueError:\n        return np.nan\n\n\nndcg_at_2 = pd.DataFrame(\n    {\"score\": documents_with_relevance_df.groupby(\"context.span_id\").apply(_compute_ndcg, k=2)}\n)\n```\n\n----------------------------------------\n\nTITLE: Logging Evaluation Results to Phoenix Server\nDESCRIPTION: Logs both hallucination and QA correctness evaluations to the Phoenix server for visualization in the UI, using the SpanEvaluations class.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import SpanEvaluations\n\npx.Client().log_evaluations(\n    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval),\n    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval),\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix Experimentation - Python\nDESCRIPTION: Installs the necessary Python dependencies for running Phoenix and the datasets library. Requires access to pip and an environment with permissions for installing packages. Dependencies include arize-phoenix version 8.0.0 or later, and the datasets library. Run this at the start of your session to ensure all modules are available before importing or running additional code.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q \"arize-phoenix>=8.0.0\" datasets\n```\n\n----------------------------------------\n\nTITLE: Configuring the OpenAI API Key\nDESCRIPTION: Retrieves the OpenAI API key from the environment variable `OPENAI_API_KEY`. If the environment variable is not set, it prompts the user to enter the key securely using `getpass`. The key is then assigned to `openai.api_key` for use by the OpenAI library and also stored back into the environment variable.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Upload Dataset to Phoenix (Python)\nDESCRIPTION: This code uploads the prepared pandas DataFrame as a dataset to the connected Phoenix instance. It creates a dataset name with a timestamp, specifies the DataFrame, and identifies which columns contain the model inputs ('img') and ground truth outputs ('label'). Required dependencies: `datetime`, `phoenix`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\ntest_cases = px.Client().upload_dataset(\n    dataset_name=f\"image-classification-test-sample-{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\",\n    dataframe=df,\n    input_keys=[\"img\"],\n    output_keys=[\"label\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Specific LLM Tracing Spans as Parquet Fixtures in Python\nDESCRIPTION: Uses SpanQuery to filter and extract specific LLM spans (OpenAI.chat and LLM.predict), then saves them, as well as all LLM spans, as Parquet files for later reuse. Inputs are queries using px.Client(); output files reside in the fixtures directory.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace.dsl import SpanQuery\n\nllm_open_ai = px.Client().query_spans(\n    SpanQuery().where(\"span_kind == 'LLM' and name == 'OpenAI.chat'\")\n)\n\nllm_predict = px.Client().query_spans(\n    SpanQuery().where(\"span_kind == 'LLM' and name == 'LLM.predict'\")\n)\n\nall_llm = px.Client().query_spans(SpanQuery().where(\"span_kind == 'LLM'\"))\n\nllm_open_ai.to_parquet(\"fixtures/demo_llama_index_llm_open_ai.parquet\")\nllm_predict.to_parquet(\"fixtures/demo_llama_index_llm_predict.parquet\")\nall_llm.to_parquet(\"fixtures/demo_llama_index_llm_all_spans.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Define Self-Consistency CoT Prompt Template and Create Phoenix Prompt (Python)\nDESCRIPTION: Defines a multi-line string template for a Self-Consistency Chain of Thought prompt, instructing the model to solve a problem multiple times and output the majority answer. It then uses the Phoenix Client to create a new prompt version based on this template and OpenAI's `gpt-3.5-turbo` model.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/chain-of-thought-prompting.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nconsistency_COT_template = \"\"\n\nYou are an evaluator who outputs the answer to a math word problem.\n\nFollow these steps:\n1. Solve the problem **multiple times independently**, thinking through the solution carefully each time.\n2. Show some of your reasoning for each independent attempt.\n3. Identify the integer answer that appears most frequently across your attempts.\n4. On a **new line**, output only this majority answer as a plain integer with **no words, commas, labels, units, or special characters**.\n\"\"\"\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"system\", \"content\": consistency_COT_template},\n        {\"role\": \"user\", \"content\": \"{{Problem}}\"},\n    ],\n)\n\nself_consistency_COT = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"self consistency COT prompt\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex, Phoenix, OpenAI, and Supporting Libraries Using pip in Python\nDESCRIPTION: This snippet installs necessary Python libraries including Phoenix with evals and LlamaIndex support, OpenAI API client, HTTPX with version constraints, Google Cloud Filesystem, asyncio utilities, sqlalchemy for SQL database management, and Wikipedia SDK. It prepares the environment to enable the rest of the notebook functionality.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_sql_retriever_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"arize-phoenix[evals,llama-index]\" \"openai>=1\" 'httpx<0.28' gcsfs nest-asyncio \"llama-index-readers-wikipedia\" \"sqlalchemy\" wikipedia\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App - Python\nDESCRIPTION: Starts the Phoenix interactive web application, enabling visualization and management of datasets and experiments. No inputs or outputs are returned by the command; it's primarily for side-effect (UI launch). Requires a running Python process and Phoenix installed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/run_experiments_with_llama_index.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset into Phoenix\nDESCRIPTION: Loads the 'fridgeReviews' dataset from Hugging Face Datasets library and converts it to a Pandas DataFrame.  Then, it uploads the dataset to Phoenix, specifying the input and output keys for the reviews and sentiment labels, respectively. A unique dataset name is generated using UUID.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/few_shot_prompting.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nds = load_dataset(\"syeddula/fridgeReviews\")[\"train\"]\nds = ds.to_pandas()\nds.head()\n```\n\n----------------------------------------\n\nTITLE: Triggering Auto-Instrumented OpenAI Span for Phoenix Export in Python\nDESCRIPTION: Demonstrates triggering an auto-instrumented span. It initializes the `openai.OpenAI` client and makes a call to the `chat.completions.create` method. The `OpenAIInstrumentor` automatically creates a span for this call. Since the span name generated by the instrumentor typically won't contain 'console', the `phoenix_condition` will be met, and the span will be exported to Phoenix via the configured OTLP exporter.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/span_filtering_tutorial.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclient = openai.OpenAI()\nclient.chat.completions.create(\n    model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}]\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Prompt from Phoenix Client in Python\nDESCRIPTION: Fetches the previously created prompt from the Phoenix server by its unique identifier. This allows formatting and usage in evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprompt = Client().prompts.get(prompt_identifier=prompt_name)\n```\n\n----------------------------------------\n\nTITLE: Compiling RAG Module with Teleprompter\nDESCRIPTION: Uses DSPy's teleprompt BootstrapFewShot to compile the custom RAG module, incorporating validation logic to select effective few-shot demonstrations for prompt construction.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/dspy_tracing_tutorial.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom dspy.teleprompt import BootstrapFewShot\n\n# Validation logic: check that the predicted answer is correct.\n# Also check that the retrieved context does actually contain that answer.\ndef validate_context_and_answer(example, pred, trace=None):\n    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n    return answer_EM and answer_PM\n\ninput_module = RAG()\nteleprompter = BootstrapFewShot(metric=validate_context_and_answer)\ncompiled_module = teleprompter.compile(input_module, trainset=trainset)\n```\n\n----------------------------------------\n\nTITLE: Launching the Phoenix Server and Opening the UI in Python\nDESCRIPTION: This code initializes the Phoenix application using the phoenix Python package and opens its user interface. The px.launch_app().view() command starts the Phoenix server and launches the default web-based view. Phoenix should be installed prior to running this snippet. It requires no input and loads the default Phoenix dashboard.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\npx.launch_app().view()\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Client Package\nDESCRIPTION: Command to install the Phoenix TypeScript client package using npm or other package managers.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/packages/phoenix-client/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# or yarn, pnpm, bun, etc...\nnpm install @arizeai/phoenix-client\n```\n\n----------------------------------------\n\nTITLE: Setting up Phoenix Client\nDESCRIPTION: Initializes the Phoenix client and uploads the dataset to Phoenix. The input_keys and output_keys arguments specify the columns to be used as inputs and outputs. A unique dataset name is created with a UUID.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/few_shot_prompting.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n\nimport phoenix as px\nfrom phoenix.client import Client as PhoenixClient\n\nunique_id = uuid.uuid4()\n\n# Upload the dataset to Phoenix\ndataset = px.Client().upload_dataset(\n    dataframe=ds,\n    input_keys=[\"Review\"],\n    output_keys=[\"Sentiment\"],\n    dataset_name=f\"review-classification-{unique_id}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Overriding Span Name in Chain Decorator\nDESCRIPTION: This snippet demonstrates overriding the default span name generated by the `@tracer.chain` decorator. This allows for custom naming of spans for better organization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict, Any\n\n@tracer.chain(name=\"decorated-chain-with-overriden-name\")\ndef this_name_should_be_overriden(input: str) -> Dict[str, Any]:\n    return {\"output\": \"output\"}\n\nthis_name_should_be_overriden(\"input\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Python Libraries\nDESCRIPTION: Imports essential Python libraries for the tutorial: `pandas` (as `pd`) for data manipulation, `IPython.display`'s `HTML` and `display` for rendering rich output in notebooks, and the `phoenix` library itself (as `px`).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/image_classification_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom IPython.display import HTML, display\n\nimport phoenix as px\n```\n\n----------------------------------------\n\nTITLE: Evaluating and Plotting GPT-3.5 Turbo Classification Output in Python\nDESCRIPTION: Repeats the computation and visualization of classification report and confusion matrix using GPT-3.5 Turbo predictions. Ensures consistent benchmarking and enables direct comparison of accuracy and error profiles between model variants. Results are displayed using matplotlib and pycm.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df_sample[\"relevant\"].map(RAG_RELEVANCY_PROMPT_RAILS_MAP).tolist()\n\nprint(classification_report(true_labels, relevance_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels, predict_vector=relevance_classifications, classes=rails\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix in Notebook with Python\nDESCRIPTION: This snippet imports the Phoenix module and launches the Phoenix dashboard directly within a notebook environment. Suitable for quick testing; note that data is not persisted after closure. For persistence, more advanced deployment is required.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/anthropic.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Reusing Prompt Task and Evaluation Functions for ReAct Prompt in Python\nDESCRIPTION: Reuses the same 'prompt_task' and 'evaluate_response' functions as with the initial prompt to run experiments on the newly created ReAct prompt. These functions handle sending requests to OpenAI and classifying the output using LLM-as-Judge evaluation, without modifications in implementation, demonstrating modularity and prompt swapping capabilities. Inputs, outputs, dependencies, and logic remain consistent with prior implementations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/react-prompting.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef prompt_task(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **prompt.format(variables={\"questions\": input[\"Questions\"]})\n    )\n    return resp\n\n\ndef evaluate_response(input, output):\n    response_classifications = llm_classify(\n        dataframe=pd.DataFrame([{\"question\": input[\"Questions\"], \"tool_calls\": output}]),\n        template=TOOL_CALLING_PROMPT_TEMPLATE,\n        model=OpenAIModel(model=\"gpt-3.5-turbo\"),\n        rails=list(TOOL_CALLING_PROMPT_RAILS_MAP.values()),\n        provide_explanation=True,\n    )\n    score = response_classifications.apply(lambda x: 0 if x[\"label\"] == \"incorrect\" else 1, axis=1)\n    return score\n```\n\n----------------------------------------\n\nTITLE: curl: Send Feedback Annotation to Phoenix API\nDESCRIPTION: This cURL command posts a feedback annotation directly to Phoenixâ€™s REST API endpoint. It includes the full JSON payload with span ID, feedback label, score, explanation, and required headers for authorization. It is suitable for testing or scripting the feedback submission.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/feedback-and-annotations/capture-feedback.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' \\\n  'https://app.phoenix.arize.com/v1/span_annotations?sync=false' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -H 'api_key: <your phoenix api key>' \\\n  -d '{\n  \"data\": [\n    {\n      \"span_id\": \"67f6740bbe1ddc3f\",\n      \"name\": \"correctness\",\n      \"annotator_kind\": \"HUMAN\",\n      \"result\": {\n        \"label\": \"correct\",\n        \"score\": 1,\n        \"explanation\": \"it is correct\"\n      },\n      \"metadata\": {}\n    }\n  ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Applying Content Moderation Prompt from Phoenix (Python)\nDESCRIPTION: Fetches the saved content moderation prompt from Phoenix and applies it iteratively to a list of user comments, formatting the prompt with dynamic variables and printing the classification result for each.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nexample_guidelines = \"\"\"\n    BLOCK CATEGORY:\n    - Promoting violence, illegal activities, or hate speech\n    - Explicit sexual content\n    - Harmful misinformation or conspiracy theories\n\n    ALLOW CATEGORY:\n    - Most other content is allowed, as long as it is not explicitly disallowed\\\"\n\"\"\"\n\nuser_comments = [\n    \"This movie was great, I really enjoyed it. The main actor really killed it!\",\n    \"Delete this post now or you better hide. I am coming after you and your family.\",\n    \"Stay away from the 5G cellphones!! They are using 5G to control you.\",\n    \"Thanks for the helpful information!\",\n]\n\nprompt = Client().prompts.get(prompt_identifier=prompt_identifier)\n\nfor comment in user_comments:\n    variables = {\"user_text\": comment, \"guidelines\": example_guidelines}\n    response = Anthropic().messages.create(**prompt.format(variables=variables))\n    print(f\"User comment: {comment}\")\n    print(f\"Model response: {response.content[0].text}\")\n    print()\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key\nDESCRIPTION: This snippet configures the OpenAI API key.  It checks if the API key is set as an environment variable. If not, it prompts the user to enter the key.  This is a prerequisite for authenticating with the OpenAI service.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_tracing_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nclient = OpenAI(api_key=openai_api_key)\n```\n```\n\n----------------------------------------\n\nTITLE: Evaluating summary using Refiner\nDESCRIPTION: This snippet defines prompts for a Refiner evaluation strategy using `PromptTemplate` and then initializes a `Refiner` evaluator. The prompts guide the language model to iteratively refine the evaluation of the generated summary against different chunks of the original data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/long_context_summary_evals.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model_name=\"gpt-4\")\ninitial_prompt_template = PromptTemplate(\n    \"You will be given a CONTEXT that contains multiple documents. \"\n    \"You will also be given a SUMMARY that summarizes the documents in the CONTEXT in addition to other (unseen) documents. \"\n    \"You must provide an EVALUATION of the quality of the SUMMARY relative to the provided CONTEXT. \"\n    \"Your EVALUATION should judge the quality of the SUMMARY and should concisely explain your reasoning. \"\n    \"Bear in mind that the SUMMARY may include information from unseen documents. \"\n    \"Focus on important points, not trivial details.\"\n    \"\\n\\n\"\n    \"=======\"\n    f\"SUMMARY: {summary}\"\n    \"\\n\\n\"\n    \"=======\"\n    \"CONTEXT: {chunk}\"\n    \"=======\"\n    \"EVALUATION: \"\n)\nrefine_prompt_template = PromptTemplate(\n    \"You will be given: \\n\"\n    \"  - a CONTEXT that contains multiple documents\\n\"\n    \"  - a SUMMARY that summarizes the documents in the CONTEXT in addition to other (unseen) documents\\n\"\n    \"  - an ACCUMULATED EVALUATION of the quality of the SUMMARY relative to other subsets of the summarized documents\\n\"\n    \"You must provide a REFINED EVALUATION of the quality of the SUMMARY that considers the current CONTEXT. \"\n    \"Bear in mind that the SUMMARY may include information from unseen documents, although you don't need to mention explicitly mention that. \"\n    \"Focus on important points, not trivial details.\"\n    \"\\n\\n\"\n    \"=======\"\n    f\"SUMMARY: {summary}\"\n    \"\\n\\n\"\n    \"=======\"\n    \"\\n\\n\"\n    \"CONTEXT: {chunk}\"\n    \"\\n\\n\"\n    \"=======\"\n    \"\\n\\n\"\n    \"ACCUMULATED EVALUATION: {accumulator}\"\n    \"\\n\\n\"\n    \"=======\"\n    \"\\n\\n\"\n    \"REFINED EVALUATION: \"\n)\nsynthesize_prompt_template = PromptTemplate(\n    \"You will be given a SUMMARY that summarizes a large number of documents. \"\n    \"You will also be given a VERBOSE EVALUATION of the quality of that SUMMARY. \"\n    \"Given this VERBOSE EVALUATION, you must provide a single, CONCISE EVALUATION of the quality of the SUMMARY. \"\n    'Your CONCISE EVALUATION should judge the quality of the SUMMARY as either \"good\" or \"bad\" and should only contain one of those two words with no additional explanation.'\n    \"\\n\\n\"\n    \"=======\"\n    \"\\n\\n\"\n    f\"SUMMARY: {summary}\"\n    \"\\n\\n\"\n    \"=======\"\n    \"\\n\\n\"\n    \"VERBOSE EVALUATION: {accumulator}\"\n    \"\\n\\n\"\n    \"=======\"\n    \"\\n\\n\"\n    \"CONCISE EVALUATION: \"\n)\nevaluator = Refiner(\n    model=model,\n    initial_prompt_template=initial_prompt_template,\n    refine_prompt_template=refine_prompt_template,\n    synthesize_prompt_template=synthesize_prompt_template,\n)\n```\n\n----------------------------------------\n\nTITLE: Running the Agent and Printing Results in Python\nDESCRIPTION: This asynchronous Python snippet uses the `Runner.run` method to execute the defined agent with a specific prompt (\"what is 15 + 28?\"). It then prints the full `Runner` result object, the final output generated by the agent, and a list of messages that compose the agent's interaction history.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/ragas_agents_cookbook_phoenix.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresult = await Runner.run(agent, \"what is 15 + 28?\")\n\n# Run Result object\nprint(result)\n\n# Get the final output\nprint(result.final_output)\n\n# Get the entire list of messages recorded to generate the final output\nprint(result.to_input_list())\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Questions for Chatbot Testing with OpenAI Python\nDESCRIPTION: This snippet constructs a system prompt template instructing an LLM to generate synthetic user questions for an e-commerce chatbot and pairs each with its expected tool(s). The OpenAI client is used to submit this prompt and receive generated questions. It requires prior setup of the 'tools' variable and an initialized OpenAI client. Input is a formatted prompt; output is the model's text response containing questions and expected tool calls. The model used is 'gpt-4o'.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/demos/playground_demo.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ngenerate_questions_prompt_template = \"\"\"\nYou are an assistant that generates synthetic questions for a customer service chatbot on an e-commerce website.\nThe chatbot has access to a variety of tools described below.\nYou should generate questions that a user of the e-commerce site might ask of the chatbot service.\nYour questions should contain some that can be answered with the tools provided, and some that cannot.\n\nThe tools available to the chatbot are:\n\n{tools}\n\nYour questions should contain a mix of the following:\n\n- direct questions: straightforward questions that can naturally be answered with a single tool call, or that do not require any tools at all\n- multiple categories: questions that naturally require more than one of the tools provided\n- vague details: questions with limited or vague information that require clarification to categorize correctly\n- mixed intentions: queries where the customer's goal or need is unclear or seems to conflict within the question itself\n\nRespond with a list of ten questions with one question per line.\nDo not include any numbering at the beginning of each line or any category headings.\nAfter each question, you must include a double colon (::) followed by a comma-separated list of the tools that you would expect to be called immediately after the question is asked.\nThis list must be empty if the question should not be answered with a tool call.\n\"\"\"\nllm_client = OpenAI()\ngenerate_questions_prompt = generate_questions_prompt_template.format(tools=json.dumps(tools))\nresponse = llm_client.chat.completions.create(\n    messages=[ChatCompletionUserMessageParam(content=generate_questions_prompt, role=\"user\")],\n    model=\"gpt-4o\",\n)\nresponse_content = response.choices[0].message.content\nassert response_content\nprint(response_content)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI and Logging Evaluations Python\nDESCRIPTION: This code instruments OpenAI with a tracer provider and then generates several simulated OpenAI calls within a project context. After the simulations, it uninstruments OpenAI, exports the spans, and then logs evaluations on the root spans using the Phoenix client.  The evaluations consist of randomly generated scores and labels.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/multi-turn_chat_sessions.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\ntry:\n    with using_project(project_name):\n        for _ in range(session_count):\n            simulate_openai()\nfinally:\n    OpenAIInstrumentor().uninstrument()\nspans = export_spans(0)\n\n# Annotate root spans\nroot_span_ids = pd.Series(\n    [format_span_id(span.context.span_id) for span in spans if span.parent is None]\n)\nfor name in [\"Helpfulness\", \"Relevance\", \"Engagement\"]:\n    span_ids = root_span_ids.sample(frac=0.5)\n    df = pd.DataFrame(\n        {\n            \"context.span_id\": span_ids,\n            \"score\": np.random.rand(len(span_ids)),\n            \"label\": np.random.choice([\"ðŸ‘\", \"ðŸ‘Ž\"], len(span_ids)),\n        }\n    ).set_index(\"context.span_id\")\n    px.Client().log_evaluations(SpanEvaluations(name, df))\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix LLM Evaluation in Python\nDESCRIPTION: Installs Phoenix client libraries and SDKs necessary for interacting with various LLM APIs (Anthropic, OpenAI, Google, Groq) as well as instrumentation packages. Required prior to running any Phoenix-related code for this evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uqqq arize-phoenix-client arize-phoenix-otel arize-phoenix anthropic openai google-generativeai groq openinference-instrumentation-anthropic openinference-instrumentation-openai openinference-instrumentation-groq openinference-instrumentation-vertexai\n```\n\n----------------------------------------\n\nTITLE: Fetching and Using Saved Prompt from Phoenix\nDESCRIPTION: Retrieves a stored prompt by identifier from Phoenix, formats it with the current context, and executes the OpenAI chat completion to generate a response, demonstrating prompt reuse.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nprompt = Client().prompts.get(prompt_identifier=prompt_identifier)\nresp = OpenAI().chat.completions.create(**prompt.format())\nprint(resp.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Loading and Uploading Dataset to Phoenix in Python\nDESCRIPTION: Defines a sample size (7), loads the 'nvidia/ChatQA-Training-Data' dataset (specifically the 'synthetic_convqa' configuration, 'train' split) using the `datasets` library, converts it to a pandas DataFrame, selects relevant columns ('messages', 'document'), samples a subset of rows using the defined size and a fixed random state for reproducibility, and uploads this sampled data to Phoenix using `px.Client().upload_dataset()`. The dataset is named dynamically using the dataset name and the current nanosecond timestamp to ensure uniqueness.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsample_size = 7\npath = \"nvidia/ChatQA-Training-Data\"\nname = \"synthetic_convqa\"\ndf = load_dataset(path, name, split=\"train\").to_pandas()\ndf = df.loc[:, [\"messages\", \"document\"]]\ndataset = px.Client().upload_dataset(\n    dataset_name=f\"{name}_{time_ns()}\",\n    dataframe=df.sample(sample_size, random_state=42),\n)\n```\n\n----------------------------------------\n\nTITLE: Filtering DataFrame by product ID\nDESCRIPTION: This snippet filters the DataFrame to only include rows where the 'product/productId' matches a specific target ID ('B0009B0IX4'). It then counts the occurrences of each unique summary for this product using value_counts().\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/long_context_summary_evals.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntarget_product_id = \"B0009B0IX4\"\nproduct_df = df[df[\"product/productId\"] == target_product_id]\nproduct_df[\"review/summary\"].value_counts()\n```\n\n----------------------------------------\n\nTITLE: Setting Retrieval Evaluation Sample Size in Python\nDESCRIPTION: Defines the N_EVAL_SAMPLE_SIZE variable, which determines the number of samples evaluated in LLM relevance testing. Increasing this number increases evaluation time, with example runtimes shown for different models and sample sizes. Change this parameter to control trade-off between thoroughness and speed in benchmarking.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#####################\n## N_EVAL_SAMPLE_SIZE\n#####################\n# Eval sample size determines the run time\n# 100 samples: GPT-4 ~ 80 sec / GPT-3.5 ~ 40 sec\n# 1,000 samples: GPT-4 ~15-17 min / GPT-3.5 ~ 6-7min (depending on retries)\n# 10,000 samples GPT-4 ~170 min / GPT-3.5 ~ 70min\nN_EVAL_SAMPLE_SIZE = 100\n```\n\n----------------------------------------\n\nTITLE: Peeking at Chroma Collection Contents\nDESCRIPTION: Executes the `peek()` method on the 'agentic-rag-demo-company-policies' Chroma collection. This function returns a small sample of the data within the collection, useful for verifying that documents and embeddings have been added correctly.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/agentic_rag_tracing.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nchroma_collection.peek()\n```\n\n----------------------------------------\n\nTITLE: Retrieving RAG Retrieved Documents with Phoenix Evaluation API in Python\nDESCRIPTION: Uses get_retrieved_documents to collect RAG query results (retrieved chunks) from Phoenix, returning them as a DataFrame. px.Client() is required as an input; the output is retrieved_documents_df.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_retrieved_documents\n\nretrieved_documents_df = get_retrieved_documents(px.Client())\nretrieved_documents_df\n```\n\n----------------------------------------\n\nTITLE: Setting Mistral API Key - Python\nDESCRIPTION: This code snippet sets the Mistral API key as an environment variable. The API key is retrieved either from the environment or prompted from the user if not found. This is a prerequisite to use Mistral models for indexing, querying, and evaluation within the RAG pipeline. The key is crucial for authenticating and accessing the Mistral AI services.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (mistral_api_key := os.getenv(\"MISTRAL_API_KEY\")):\n    mistral_api_key = getpass(\"ðŸ”‘ Enter your MISTRAL API key: \")\nos.environ[\"MISTRAL_API_KEY\"] = mistral_api_key\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API key for LLM access\nDESCRIPTION: Sets up the OpenAI API key for accessing the language models. The key is either retrieved from environment variables or requested from the user via input.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Tracing Tools with Decorators\nDESCRIPTION: This snippet showcases tracing a tool function using a decorator. The function takes two inputs and doesn't return any value. The tool description is extracted from the docstring.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@tracer.tool\ndef decorated_tool(input1: str, input2: int) -> None:\n    \"\"\"\n    tool-description\n    \"\"\"\n\ndecorated_tool(\"input1\", 1)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Groq Client for Tracing (Python)\nDESCRIPTION: Applies OpenInference instrumentation to the Groq Python client. This enables automatic capturing of request and response details for calls made using the `groq` library, formatting them into OpenTelemetry traces. Requires the `openinference-instrumentation-groq` library.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/groq_tracing_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.groq import GroqInstrumentor\n\nGroqInstrumentor().instrument()\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix and Haystack Dependencies\nDESCRIPTION: This command installs the necessary packages for using Phoenix with Haystack, including arize-phoenix, arize-phoenix-otel, openinference-instrumentation-haystack, haystack-ai, opentelemetry-sdk, and opentelemetry-exporter-otlp. It uses pip to manage the installation process.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/haystack_tracing_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install arize-phoenix arize-phoenix-otel openinference-instrumentation-haystack haystack-ai opentelemetry-sdk opentelemetry-exporter-otlp\n```\n\n----------------------------------------\n\nTITLE: Configure Collector Endpoint via Environment Variables - Python\nDESCRIPTION: This Python snippet illustrates setting the collector endpoint through the `PHOENIX_COLLECTOR_ENDPOINT` environment variable.  This allows the `register` function to automatically configure the tracer to send spans to the specified Phoenix server using gRPC. Requires `phoenix.otel` module.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-otel/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\ntracer_provider = register()\n```\n\n----------------------------------------\n\nTITLE: Verifying Evaluable Functions on a Single Run\nDESCRIPTION: Runs each evaluator on the first experiment run, prints their qualified name, and a shortened JSON of the output, confirming evaluators process data correctly.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nrun = experiment[0]\nexample = dataset.examples[run.dataset_example_id]\nfor fn in evaluators:\n    _ = await fn(run.output, example.input, example.output)\n    print(fn.__qualname__)\n    print(shorten(json.dumps(_), width=80))\n```\n\n----------------------------------------\n\nTITLE: Opening Phoenix UI in Browser in Python\nDESCRIPTION: Retrieves the URL for accessing the Phoenix UI in a browser tab or window. Works in both local Jupyter environments and Colab notebooks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/manage-the-app.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsession.url\n```\n\n----------------------------------------\n\nTITLE: Generating Questions using OpenAI Model in Python\nDESCRIPTION: Calls the initialized `OpenAIModel` instance (`model`) with the previously defined `GEN_TEMPLATE` as input. This sends the prompt to the OpenAI API (using the configured model 'gpt-4o') and stores the generated text response in the `resp` variable.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresp = model(GEN_TEMPLATE)\n```\n\n----------------------------------------\n\nTITLE: Defining a Hallucination Eval Template Prompt for LLMs in Prompt Format\nDESCRIPTION: This prompt template is designed to instruct a human or an LLM-based eval system to determine if an answer to a given query is hallucinated or factual based on reference text. It clearly defines hallucination as information not supported by the reference text and constrains the response to a single word, either \"factual\" or \"hallucinated\". The template includes placeholders for query, reference text, and the answer, emphasizing strict adherence to the reference to spot misinformation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/hallucinations.md#_snippet_0\n\nLANGUAGE: Prompt\nCODE:\n```\nIn this task, you will be presented with a query, a reference text and an answer. The answer is\ngenerated to the question based on the reference text. The answer may contain false information. You\nmust use the reference text to determine if the answer to the question contains false information,\nif the answer is a hallucination of facts. Your objective is to determine whether the answer text\ncontains factual information and is not a hallucination. A 'hallucination' refers to\nan answer that is not based on the reference text or assumes information that is not available in\nthe reference text. Your response should be a single word: either \"factual\" or \"hallucinated\", and\nit should not include any other text or characters. \"hallucinated\" indicates that the answer\nprovides factually inaccurate information to the query based on the reference text. \"factual\"\nindicates that the answer to the question is correct relative to the reference text, and does not\ncontain made up information. Please read the query and reference text carefully before determining\nyour response.\n\n    # Query: {query}\n    # Reference text: {reference}\n    # Answer: {response}\n    Is the answer above factual or hallucinated based on the query and reference text?\n```\n\n----------------------------------------\n\nTITLE: Inspecting Experiment Run Data\nDESCRIPTION: This snippet allows the user to inspect the data structure of a single experiment run.  It retrieves the first entry from the experiment, and displays it.  This is helpful for debugging, and understanding the output before evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nexperiment[0]\n```\n\n----------------------------------------\n\nTITLE: Creating Corpus Inferences from Dataframe and Schema\nDESCRIPTION: This snippet demonstrates pairing a pandas dataframe containing text and embedding data with the previously defined schema to create inferences. The `px.Inferences` function binds the dataset with the schema for downstream retrieval tasks in Phoenix. Dependencies include the `px` library and a correctly structured dataframe.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/corpus-data.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncorpus_inferences = px.Inferences(corpus_dataframe, corpus_schema)\n```\n\n----------------------------------------\n\nTITLE: Import necessary libraries for Phoenix and Llama Index\nDESCRIPTION: Imports classes and functions required to interact with GCS, load indexes, establish service contexts, and launch the Phoenix app.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_application_tracing_evaluating_and_analysis.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom gcsfs import GCSFileSystem\nfrom llama_index.core import (\n    ServiceContext,\n    StorageContext,\n    load_index_from_storage,\n)\nfrom llama_index.core.graph_stores import SimpleGraphStore\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nfrom tqdm import tqdm\n\nimport phoenix as px\nfrom phoenix.trace import DocumentEvaluations, SpanEvaluations\n```\n\n----------------------------------------\n\nTITLE: Loading Knowledge Base into Pandas\nDESCRIPTION: Queries a vector store (using a client) to retrieve document embeddings, IDs, and text. This information is then loaded into a pandas DataFrame. The process involves iterating through the results of the vector store query, extracting the necessary data, and constructing the DataFrame. Dependencies include a vector store client, `metadata_dict_to_node`, and the `pandas` and `numpy` libraries. The output is a pandas DataFrame containing document embeddings, IDs, and text.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/milvus_llamaindex_search_and_retrieval_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Use vector store client to query the knowledge base and load our knowledge base into a dataframe\nmyclient = vector_store.client\ndocument_embeddings, document_ids, document_texts = [], [], []\ni = 0\n\nall_data = myclient.query(collection_name=\"colab_collection\", filter='id >= \"\"')\nfor x in all_data:\n    node = metadata_dict_to_node({\"_node_content\": all_data[i][\"_node_content\"]})\n    document_embeddings.append(np.array(x[\"embedding\"]))\n    document_ids.append(node.hash)\n    document_texts.append(node.text)\n    i = i + 1\n\ndatabase_df = pd.DataFrame(\n    {\n        \"document_id\": document_ids,\n        \"text\": document_texts,\n        \"text_vector\": document_embeddings,\n    }\n)\ndatabase_df = database_df[database_df[\"text\"] != \"\\n\"]\ndatabase_df.head()\n```\n\n----------------------------------------\n\nTITLE: Adding Attribute to Existing Span - TypeScript\nDESCRIPTION: Example of adding a custom key-value attribute to an existing span using the `span.setAttribute()` method. Attributes provide additional contextual information about the operation represented by the span.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nfunction chat(message: string, user: User) {\n  return tracer.startActiveSpan(`chat:${i}`, (span: Span) => {\n    const result = Math.floor(Math.random() * (max - min) + min);\n\n    // Add an attribute to the span\n    span.setAttribute('mycompany.userid', user.id);\n\n    span.end();\n    return result;\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Image Examples from Exported Cluster (Python)\nDESCRIPTION: Utilizes the previously defined `display_examples` function to visualize the first few image examples contained within the exported DataFrame (`export_df`). This allows for inspection of the specific data points (e.g., blurry, noisy images) identified and exported from the problematic cluster in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/image_classification_tutorial.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndisplay_examples(export_df.head())\n```\n\n----------------------------------------\n\nTITLE: Querying Corpus Embeddings from Phoenix Using DSL in Python\nDESCRIPTION: Retrieves vector embeddings from the corpus dataset using Phoenix's SpanQuery DSL with explode operation on embedding fields. The client must be connected to the Phoenix project configured for indexing. The resulting DataFrame includes decomposed embedding text and vector columns for downstream use such as visualization or comparison with query embeddings.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace.dsl.helpers import SpanQuery\n\nclient = px.Client()\ncorpus_df = px.Client().query_spans(\n    SpanQuery().explode(\n        \"embedding.embeddings\",\n        text=\"embedding.text\",\n        vector=\"embedding.vector\",\n    ),\n    project_name=INDEXING_PROJECT,\n)\ncorpus_df.head()\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Prompt by Name, Version, or Tag in Python\nDESCRIPTION: This code illustrates how to retrieve prompts from Phoenix by different identifiers: by name, by version ID, or by tag. It demonstrates the usage of the Phoenix Client to fetch prompt details for management or reuse.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/quickstart-prompts/quickstart-prompts-python.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.client import Client\n\nclient = Client()\n\n# Pulling a prompt by name\nprompt_name = \"article-bullet-summarizer\"\nclient.prompts.get(prompt_identifier=prompt_name)\n\n# Pulling a prompt by version id\n# The version ID can be found in the versions tab in the UI\nprompt = client.prompts.get(prompt_version_id=\"UHJvbXB0VmVyc2lvbjoy\")\n\n# Pulling a prompt by tag\n# Since tags don't uniquely identify a prompt version \n#  it must be paired with the prompt identifier (e.g. name)\nprompt = client.prompts.get(prompt_identifier=prompt_name, tag=\"staging\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom LLM-Based Accuracy Evaluator Using Phoenix Decorator in Python\nDESCRIPTION: This snippet defines an evaluator using the Phoenix `create_evaluator` decorator with \"llm\" kind. The evaluator uses a prompt template with the input question, reference answer, and model output to query an OpenAI GPT-4o chat completion for accuracy judgment. It returns 1.0 if the answer is \"accurate\" and 0.0 otherwise, supporting flexible natural language evaluation using LLMs. Requires OpenAI client and Phoenix decorators.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/quickstart-datasets.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments.evaluators import create_evaluator\nfrom typing import Any, Dict\n\n\neval_prompt_template = \"\"\"\nGiven the QUESTION and REFERENCE_ANSWER, determine whether the ANSWER is accurate.\nOutput only a single word (accurate or inaccurate).\n\nQUESTION: {question}\n\nREFERENCE_ANSWER: {reference_answer}\n\nANSWER: {answer}\n\nACCURACY (accurate / inaccurate):\n\"\"\"\n\n\n@create_evaluator(kind=\"llm\")  # need the decorator or the kind will default to \"code\"\ndef accuracy(input: Dict[str, Any], output: str, expected: Dict[str, Any]) -> float:\n    message_content = eval_prompt_template.format(\n        question=input[\"question\"], reference_answer=expected[\"answer\"], answer=output\n    )\n    response = openai_client.chat.completions.create(\n        model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": message_content}]\n    )\n    response_message_content = response.choices[0].message.content.lower().strip()\n    return 1.0 if response_message_content == \"accurate\" else 0.0\n```\n\n----------------------------------------\n\nTITLE: Running a Dry-Run of the Experiment with Sample Dataset in Python\nDESCRIPTION: Executes a dry run of the experiment pipeline using the Phoenix `run_experiment` function on three randomly selected examples from the dataset. This provides a quick verification of pipeline functionality without full execution on the entire dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nexperiment = run_experiment(dataset, task, dry_run=3)\n```\n\n----------------------------------------\n\nTITLE: Viewing Phoenix Experiment Results as DataFrame in Python\nDESCRIPTION: Calls the `as_dataframe()` method on the `experiment` object returned by `run_experiment`. This converts the experiment results (inputs, outputs, metadata for each run) stored within the Phoenix object into a pandas DataFrame for easy viewing and analysis in the current environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nexperiment.as_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Run Evaluations on Experiment Results Python\nDESCRIPTION: Executes the previously defined set of 'evaluators' against the results of the 'experiment' object (which currently holds the dry run results). The 'evaluate_experiment' function processes each run in the experiment, applies the specified evaluators, and updates the experiment object with the resulting evaluation scores and explanations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nexperiment = evaluate_experiment(experiment, evaluators)\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenInference Instrumentation for Llama Index\nDESCRIPTION: Configures OpenInference instrumentation for Llama Index to enable tracing and monitoring of the agent's operations through Phoenix, connecting to a local endpoint.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Testing LLM Model Connectivity\nDESCRIPTION: Sends a simple test prompt to the configured model to verify that the API connection is working properly before running the full evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel(\"Hello world, this is a test if you are working?\")\n```\n\n----------------------------------------\n\nTITLE: Querying and Extracting Query Embeddings Using Phoenix DSL in Python\nDESCRIPTION: Uses Phoenix's DSL helpers to define a SpanQuery that indexes on \"trace_id\" and explodes embeddings data into separate columns including the textual query and its vector representation. Queries are then fetched from the Phoenix client with additional selection of span ids and output values for queries without parents. The resulting DataFrames for embeddings and queries are joined on span ids to produce a combined query embeddings DataFrame with relevant metadata.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace.dsl.helpers import SpanQuery\n\nembeddings_df = px.Client().query_spans(\n    SpanQuery()\n    .with_index(\"trace_id\")\n    .explode(\n        \"embedding.embeddings\",\n        query=\"embedding.text\",\n        vector=\"embedding.vector\",\n    ),\n)\nqueries_df = px.Client().query_spans(\n    SpanQuery()\n    .with_index(\"trace_id\")\n    .select(\"span_id\", response=\"output.value\")\n    .where(\"parent_id is None\")\n)\nquery_embeddings_df = queries_df.join(embeddings_df, how=\"inner\").set_index(\"context.span_id\")\nquery_embeddings_df.head()\n```\n\n----------------------------------------\n\nTITLE: Calculating Prompt Gradient\nDESCRIPTION: This code defines the `calculate_prompt_gradient` function which calculates the gradient direction for prompt optimization. It takes lists of successful and failed prompts as input. It computes the embeddings of both successful and failed prompts, calculates the average embedding for each group, and then computes the gradient direction by subtracting the average failed embedding from the average successful embedding, normalizing the result.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Function to calculate gradient direction between successful and failed prompts\ndef calculate_prompt_gradient(successful_prompts, failed_prompts):\n    # Get embeddings for successful and failed prompts\n    successful_embeddings = [get_embedding(p) for p in successful_prompts]\n    failed_embeddings = [get_embedding(p) for p in failed_prompts]\n\n    # Calculate average embeddings\n    avg_successful = np.mean(successful_embeddings, axis=0)\n    avg_failed = np.mean(failed_embeddings, axis=0)\n\n    # Calculate gradient direction\n    gradient = avg_successful - avg_failed\n    return gradient / np.linalg.norm(gradient)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix with Query and Document Datasets for UMAP Analysis\nDESCRIPTION: Prepares and launches Phoenix with query and document datasets for UMAP projection and clustering analysis, sampling documents to maintain performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# get a random sample of 500 documents (including retrieved documents)\n# this will be handled by by the application in a coming release\nnum_sampled_point = 500\nretrieved_document_ids = set(\n    [\n        doc_id\n        for doc_ids in query_df[\":feature.[str].retrieved_document_ids:prompt\"].to_list()\n        for doc_id in doc_ids\n    ]\n)\nretrieved_document_mask = database_df[\"document_id\"].isin(retrieved_document_ids)\nnum_retrieved_documents = len(retrieved_document_ids)\nnum_additional_samples = num_sampled_point - num_retrieved_documents\nunretrieved_document_mask = ~retrieved_document_mask\nsampled_unretrieved_document_ids = set(\n    database_df[unretrieved_document_mask][\"document_id\"]\n    .sample(n=num_additional_samples, random_state=0)\n    .to_list()\n)\nsampled_unretrieved_document_mask = database_df[\"document_id\"].isin(\n    sampled_unretrieved_document_ids\n)\nsampled_document_mask = retrieved_document_mask | sampled_unretrieved_document_mask\nsampled_database_df = database_df[sampled_document_mask]\n\ndatabase_schema = px.Schema(\n    prediction_id_column_name=\"document_id\",\n    prompt_column_names=px.EmbeddingColumnNames(\n        vector_column_name=\"text_vector\",\n        raw_data_column_name=\"text\",\n    ),\n)\ndatabase_ds = px.Inferences(\n    dataframe=sampled_database_df,\n    schema=database_schema,\n    name=\"database\",\n)\n\n(session := px.launch_app(primary=query_ds, corpus=database_ds, run_in_thread=False)).View()\n```\n\n----------------------------------------\n\nTITLE: Initializing Multiple Phoenix Inferences with a Shared Schema in Python\nDESCRIPTION: This snippet shows how to initialize two Phoenix inference sets, `train_ds` and `prod_ds`, using the same schema. This is useful when the training and production data have the same format. The `schema` variable, assumed to have already been defined, is used for both inference sets. The inferences will be named \"training\" and \"production\" respectively in the Phoenix UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/inferences.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_ds = px.Inferences(train_df, schema, \"training\")\nprod_ds = px.Inferences(prod_df, schema, \"production\")\n```\n\n----------------------------------------\n\nTITLE: Connect to Weaviate Cloud - Python\nDESCRIPTION: Imports necessary Weaviate modules ('weaviate', 'Auth'). Connects to a Weaviate Cloud instance using the URL and API key retrieved from environment variables. Prints the readiness status of the client to verify the connection. Note: `client.close()` is commented out but indicates resource cleanup.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport weaviate\nfrom weaviate.classes.init import Auth\n\n# Best practice: store your credentials in environment variables\nweaviate_url = os.environ[\"WEAVIATE_URL\"]\nweaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]\n\nclient = weaviate.connect_to_weaviate_cloud(\n    cluster_url=weaviate_url,\n    auth_credentials=Auth.api_key(weaviate_api_key),\n)\n\nprint(client.is_ready())  # Should print: `True`\n\n# client.close()  # Free up resources\n```\n\n----------------------------------------\n\nTITLE: Creating Phoenix Database Schema and Dataset in Python\nDESCRIPTION: This code creates a Phoenix Schema to define the structure of the database dataset, specifying the document ID column and embedding columns for vector and raw text data. It then creates a Phoenix Inferences object from the sampled database dataframe.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_search_and_retrieval_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndatabase_schema = px.Schema(\n    prediction_id_column_name=\"document_id\",\n    prompt_column_names=px.EmbeddingColumnNames(\n        vector_column_name=\"text_vector\",\n        raw_data_column_name=\"text\",\n    ),\n)\ndatabase_ds = px.Inferences(\n    dataframe=sampled_database_df,\n    schema=database_schema,\n    name=\"database\",\n)\n```\n\n----------------------------------------\n\nTITLE: Loading, Sampling, and Uploading Dataset to Phoenix - Python\nDESCRIPTION: Loads a specific version of the `cnn_dailymail` dataset from HuggingFace, converts it to a pandas DataFrame, samples 10 rows, and renames columns. Uploads the prepared DataFrame to Phoenix as a dataset with specified input and output keys and a timestamped name, making it available for experiments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/summarization.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom datetime import datetime\n\nfrom datasets import load_dataset\n\nhf_ds = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\ndf = (\n    hf_ds[\"test\"]\n    .to_pandas()\n    .sample(n=10, random_state=0)\n    .set_index(\"id\")\n    .rename(columns={\"highlights\": \"summary\"})\n)\nnow = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\ndataset = px.Client().upload_dataset(\n    dataframe=df,\n    input_keys=[\"article\"],\n    output_keys=[\"summary\"],\n    dataset_name=f\"news-article-summaries-{now}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating GPT-3.5 Classification Performance\nDESCRIPTION: Compared predicted and ground-truth labels, outputs performance metrics, and plots the confusion matrix similar to GPT-4, to benchmark model differences.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\ntrue_labels = df[\"readable\"].map(CODE_READABILITY_PROMPT_RAILS_MAP).tolist()\n\nprint(classification_report(true_labels, readability_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels, predict_vector=readability_classifications, classes=rails\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Data Schema for Arize Phoenix in Python\nDESCRIPTION: Creates a `phoenix.Schema` instance to describe the structure of the input DataFrames. It maps column names like 'prediction_id', 'prediction_ts', 'pred_label', and 'label' to their corresponding roles in Phoenix, and specifically defines the embedding feature 'text_embedding' linking the 'text_vector' and 'text' columns.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/sentiment_classification_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nschema = px.Schema(\n    prediction_id_column_name=\"prediction_id\",\n    timestamp_column_name=\"prediction_ts\",\n    prediction_label_column_name=\"pred_label\",\n    actual_label_column_name=\"label\",\n    embedding_feature_column_names={\n        \"text_embedding\": px.EmbeddingColumnNames(\n            vector_column_name=\"text_vector\", raw_data_column_name=\"text\"\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix\nDESCRIPTION: Command to install the Arize Phoenix package using pip package manager.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/reverse-proxy/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Defining A-Frame AR Scene Container (HTML)\nDESCRIPTION: Sets up the main A-Frame scene container element. The `embedded` attribute allows embedding the scene within a standard HTML page, and the `renderer` attribute configures rendering options like logarithmic depth buffer for better depth sorting.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<a-scene embedded renderer=\"logarithmicDepthBuffer: true;\">\n```\n\n----------------------------------------\n\nTITLE: Retrieving QA Data for Response Evaluation\nDESCRIPTION: Fetches question-answer data with reference information from Phoenix to prepare for evaluating the quality and accuracy of LLM responses.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag_haystack.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_qa_with_reference\n\nqa_with_reference_df = get_qa_with_reference(px.Client())\nqa_with_reference_df\n```\n\n----------------------------------------\n\nTITLE: Creating Phoenix Inferences Object for LLM Data (Python)\nDESCRIPTION: Instantiates a Phoenix `Inferences` object using the alias `px.Inferences`. This object bundles the input dataframe (`primary_dataframe`, assumed to exist and contain the LLM data) with its corresponding structure defined by `primary_schema`. This prepares the data for analysis and visualization within the Phoenix application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/prompt-and-response-llm.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprimary_inferences = px.Inferences(primary_dataframe, primary_schema)\n```\n\n----------------------------------------\n\nTITLE: Configuring A-Frame Camera and Cursor (HTML)\nDESCRIPTION: Configures the scene's camera and adds a cursor for interaction. The `<a-entity camera>` sets up the camera with look controls and a starting position. The nested `<a-entity cursor>` defines a cursor that fuses (fills) when hovering over elements with the class `clickable`, using a raycaster to detect objects and specifying its visual appearance (geometry and material).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_7\n\nLANGUAGE: HTML\nCODE:\n```\n<a-entity camera look-controls position=\"0 1.6 0\"><a-entity cursor=\"fuse: true; fuseTimeout: 500\" raycaster=\"objects: .clickable\" geometry=\"primitive: ring; radiusOuter: 0.016; radiusInner: 0.01; segmentsTheta: 32\" material=\"color: #fff; opacity: 0.6; side: double\"></a-entity></a-entity>\n```\n\n----------------------------------------\n\nTITLE: Loading and Uploading Dataset to Phoenix\nDESCRIPTION: Loads a customer questions dataset from Hugging Face Datasets and uploads it to Phoenix. It converts the dataset to a Pandas DataFrame, generates a unique ID for the dataset, and uses the Phoenix client to upload the dataset with specified input keys.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/react_prompting.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nds = load_dataset(\"syeddula/customer_questions\")[\"train\"]\nds = ds.to_pandas()\nds.head()\nimport uuid\n\nunique_id = uuid.uuid4()\n\n# Upload the dataset to Phoenix\ndataset = px.Client().upload_dataset(\n    dataframe=ds,\n    input_keys=[\"Questions\"],\n    dataset_name=f\"customer-questions-{unique_id}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Validating Task Function with Sample Data\nDESCRIPTION: Executes the asynchronous task on a sample dataset example, prints a shortened JSON string of the output to verify functionality. Demonstrates that the task can successfully process input and produce response.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nexample = dataset[0]\ntask_output = await task(example.input)\nprint(shorten(json.dumps(task_output), width=80))\n```\n\n----------------------------------------\n\nTITLE: Running the Experiment in Phoenix\nDESCRIPTION: Runs the experiment in Phoenix, specifying the dataset and the task function. This initiates the model comparison, allowing Phoenix to handle and orchestrate model evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nexperiment = run_experiment(dataset, task)\n```\n\n----------------------------------------\n\nTITLE: Initializing an OpenAI Math Solver Agent\nDESCRIPTION: Instantiates an OpenAI agent with instructions to solve math problems by evaluating expressions with Python, using the previously defined solve_equation tool.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/openai_agents_cookbook.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom agents import Agent\n\nagent = Agent(\n    name=\"Math Solver\",\n    instructions=\"You solve math problems by evaluating them with python and returning the result\",\n    tools=[solve_equation],\n)\n```\n\n----------------------------------------\n\nTITLE: Running LlamaIndex - Python\nDESCRIPTION: Demonstrates a basic LlamaIndex query, loading documents from a directory, creating an index, and querying the index. Requires setting OPENAI_API_KEY. This shows how to use LlamaIndex as usual, with the tracing instrumentation capturing the execution details.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API KEY\"\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Some question about the data should go here\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Applying Asyncio Patch (Python)\nDESCRIPTION: Imports various modules from `json`, `textwrap`, `time`, `typing`, `nest_asyncio`, `pandas`, `phoenix`, `llama_index.core.evaluation`, `llama_index.llms.openai`, `openinference.instrumentation.llama_index`, and `opentelemetry.sdk.trace`. It also applies `nest_asyncio` to allow nested asyncio loops, which is often needed in environments like notebooks. These imports provide the functionality for data handling, LLMs, evaluation, tracing, and experiment management.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom textwrap import shorten\nfrom time import time_ns\nfrom typing import Tuple\n\nimport nest_asyncio\nimport pandas as pd\nimport phoenix as px\nfrom llama_index.core.evaluation import (\n    PairwiseComparisonEvaluator,\n)\nfrom llama_index.llms.openai import OpenAI\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom phoenix.experiments import evaluate_experiment, run_experiment\nfrom phoenix.experiments.types import Explanation, Score\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Define Image Classification Task with OpenAI GPT-4o-mini (Python)\nDESCRIPTION: Defines the core task function that takes a single input from the dataset (an image). It initializes the OpenAI client, creates a chat completion request using the 'gpt-4o-mini' model with a multimodal message including text and the base64 encoded image URL, and returns the lowercase content of the model's response (the predicted label). Required dependencies: `openai`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\ndef task(input):\n    client = OpenAI()\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Whatâ€™s in this image? Your answer should be a single word. The word should be one of the following: \"\n                        + str(label_map.values()),\n                    },\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": input[\"img\"],\n                        },\n                    },\n                ],\n            }\n        ],\n        max_tokens=300,\n    )\n\n    output_label = response.choices[0].message.content.lower()\n    return output_label\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application Viewer\nDESCRIPTION: Starts the Phoenix visualization app and opens the web interface for monitoring and analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_application_tracing_evaluating_and_analysis.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\npx.launch_app().view()\n```\n\n----------------------------------------\n\nTITLE: Extracting Labels from Classification Results DataFrame - Python\nDESCRIPTION: Extracts the predicted 'label' column from the classification results DataFrame to a list. Prepares for downstream evaluation, such as confusion matrix and metrics. Expects 'relevance_classifications_df' to be previously defined.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nrelevance_classifications = relevance_classifications_df[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Saving Anthropic Prompt to Phoenix (Python)\nDESCRIPTION: Saves the parameters of the previously defined text generation prompt into the Phoenix prompt registry, allowing it to be versioned and managed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# prompt identifier should contain only alphanumeric characters, hyphens or underscores\nprompt_identifier = \"haiku-recursion\"\n\nprompt = Client().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Haiku about recursion in programming\",\n    version=PromptVersion.from_anthropic(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Testing LLM Model Response with a Sample Input in Python\nDESCRIPTION: Invokes the initialized OpenAIModel (e.g., GPT-4) with a test string to validate API connectivity and basic model functioning. The model parameter must be correctly set, and the function prints the modelâ€™s reply for confirmation. Useful for debugging initial setup.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel(\"Hello world, this is a test if you are working?\")\n```\n\n----------------------------------------\n\nTITLE: Loading Query Data from Buffer into Pandas\nDESCRIPTION: Loads query data from a callback handler's buffer into a pandas DataFrame. This DataFrame is then used for further processing within the Phoenix environment.  Dependencies include the `callback_handler` and `as_dataframe` from the relevant libraries. The output is a pandas DataFrame containing query data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/milvus_llamaindex_search_and_retrieval_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nquery_data_buffer = callback_handler.flush_query_data_buffer()\nsample_query_df = as_dataframe(query_data_buffer)\nsample_query_df\n```\n\n----------------------------------------\n\nTITLE: Running a Phoenix Experiment\nDESCRIPTION: This Python snippet executes the experiment using `phoenix.experiments.run_experiment`. It takes the dataset, the defined task function, and a list of evaluator functions as input and runs the task/evaluators on each example in the dataset, reporting results to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/run-experiments.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments import run_experiment\n\nexperiment = run_experiment(dataset, task=task, evaluators=[no_error, has_results])\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key (Python)\nDESCRIPTION: Checks if the `OPENAI_API_KEY` environment variable is set. If not, it prompts the user to enter the key securely using `getpass` and sets it as an environment variable for subsequent use by the OpenAI library. This is required for the LLM calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Testing Agent with Policy Query\nDESCRIPTION: Sends a second chat query to the `ReActAgent` asking about the PTO policy for the 'ML Solutions team'. The agent should process this query, potentially using the 'ChromaPolicy' tool to retrieve relevant information, and then synthesize an answer based on the retrieved context and its internal reasoning. The agent's response is printed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/agentic_rag_tracing.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nresponse = agent.chat(\"What is the pto policy for the ML Solutions team?\")\nprint(str(response))\n```\n\n----------------------------------------\n\nTITLE: Run Experiment Dry Run Python\nDESCRIPTION: Initiates an experiment using 'phoenix.experiments.run_experiment'. It takes the loaded 'dataset' and the defined 'task' function as inputs. The 'dry_run=3' parameter instructs Phoenix to execute the task function only on 3 randomly selected examples from the dataset. This is useful for quickly testing the experiment setup without processing the entire dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nexperiment = run_experiment(dataset, task, dry_run=3)\n```\n\n----------------------------------------\n\nTITLE: Defining Database Lookup Questions and Expected Results\nDESCRIPTION: This code defines a list of questions and prepares expected results for a database lookup tool. It populates these into a DataFrame to allow evaluation of the agent's ability to generate correct SQL queries to answer these questions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm import tqdm\n\ndb_lookup_questions = [\n    \"What was the most popular product SKU?\",\n    \"Which store had the highest total sales value?\",\n    \"How many items were sold on promotion?\",\n    \"What was the average quantity sold per transaction?\",\n    \"Which product class code generated the most revenue?\",\n    \"What day of the week had the highest sales volume?\",\n    \"How many unique stores made sales?\",\n    \"What was the highest single transaction value?\",\n    \"Which products were frequently sold together?\",\n    \"What's the trend in sales over time?\",\n]\n\nexpected_results = []\n\nfor question in tqdm(db_lookup_questions, desc=\"Processing SQL lookup questions\"):\n    try:\n        with suppress_tracing():\n            expected_results.append(lookup_sales_data(question))\n    except Exception as e:\n        print(f\"Error processing question: {question}\")\n        print(e)\n        db_lookup_questions.remove(question)\n```\n\n----------------------------------------\n\nTITLE: Testing RAG Query Engine with Single Question\nDESCRIPTION: Tests the RAG system by querying information about Paul Graham's role at Y Combinator and prints the response.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresponse = query_engine.query(\"What did the Paul Graham do at Y Combinator?\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Configuring Anthropic API Key Python\nDESCRIPTION: Retrieves the Anthropic API key from the environment variables. If the key is not present in the environment, it prompts the user to enter the key securely using `getpass`. This key is essential for authenticating with the Anthropic API.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/anthropic_tracing_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport anthropic\n\nif not (anthropic_api_key := os.getenv(\"ANTHROPIC_API_KEY\")):\n    anthropic_api_key = getpass(\"ðŸ”‘ Enter your Anthropic API key: \")\nclient = anthropic.Anthropic(api_key=anthropic_api_key)\n```\n\n----------------------------------------\n\nTITLE: Fetching Dataset from URL\nDESCRIPTION: Downloads a JSON dataset from a specified Google Cloud Storage URL. It uses `urllib.request.urlopen` to fetch the data, reads the response buffer, decodes it as UTF-8, parses the JSON content, and extracts the list of 'rows' into the `rows` variable.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_with_mongodb_and_llama_index.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://storage.googleapis.com/arize-assets/xander/mongodb/mongodb_dataset.json\"\n\nwith urllib.request.urlopen(url) as response:\n    buffer = response.read()\n    data = json.loads(buffer.decode(\"utf-8\"))\n    rows = data[\"rows\"]\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Dependencies for Evaluation and LLMs using pip\nDESCRIPTION: Installs required Python packages for Phoenix, LLM integrations, and auxiliary libraries necessary for evaluation processes and app setup.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_application_tracing_evaluating_and_analysis.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n!pip install -qq \"arize-phoenix[evals,llama-index]\" \"openai>=1\" gcsfs nest_asyncio llama-index-llms-openai 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Running Code Readability Evaluation with Phoenix Python\nDESCRIPTION: This Python snippet demonstrates how to run the code readability evaluation using the Arize Phoenix library. It imports necessary components, initializes an OpenAI model (`gpt-4`), defines 'rails' to constrain the model's output to specific values, and then uses the `llm_classify` function to apply the predefined readability prompt template to a DataFrame (`df`) containing code examples.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/code-generation-eval.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.evals import (\n    CODE_READABILITY_PROMPT_RAILS_MAP,\n    CODE_READABILITY_PROMPT_TEMPLATE,\n    OpenAIModel,\n    download_benchmark_dataset,\n    llm_classify,\n)\n\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\n\n#The rails is used to hold the output to specific values based on the template\n#It will remove text such as \",,,\" or \"...\"\n#Will ensure the binary value expected from the template is returned \nrails = list(CODE_READABILITY_PROMPT_RAILS_MAP.values())\nreadability_classifications = llm_classify(\n    dataframe=df,\n    template=CODE_READABILITY_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    provide_explanation=True, #optional to generate explanations for the value produced by the eval LLM\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Session-Bound Traces for OpenAI Calls (TypeScript)\nDESCRIPTION: Demonstrates wrapping an application's logic (like an AI agent's turn) in a custom OpenTelemetry span and associating it with a session ID. It uses OpenInference semantic conventions and context propagation helpers (`setSession`) to ensure that both the custom span and any automatically generated child spans (like the OpenAI chat completion call) are linked to the same unique session identifier.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/tracing_openai_sessions_tutorial.ipynb#_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { trace } from \"npm:@opentelemetry/api\";\nimport { SemanticConventions } from \"npm:@arizeai/openinference-semantic-conventions\";\nimport { context } from \"npm:@opentelemetry/api\";\nimport { setSession } from \"npm:@arizeai/openinference-core\";\n\nconst tracer = trace.getTracer(\"agent\");\n\nconst client = new OpenAI({\n  apiKey: process.env[\"OPENAI_API_KEY\"], // This is the default and can be omitted\n});\n\nasync function assistant(params: {\n  messages: { role: string; content: string }[];\n  sessionId: string;\n}) {\n  return tracer.startActiveSpan(\"agent\", async (span: Span) => {\n    span.setAttribute(SemanticConventions.OPENINFERENCE_SPAN_KIND, \"agent\");\n    span.setAttribute(SemanticConventions.SESSION_ID, params.sessionId);\n    span.setAttribute(\n      SemanticConventions.INPUT_VALUE,\n      messages[messages.length - 1].content,\n    );\n    try {\n      // This is not strictly necessary but it helps propagate the session ID\n      // to all child spans\n      return context.with(\n        setSession(context.active(), { sessionId: params.sessionId }),\n        async () => {\n          // Calls within this block will generate spans with the session ID set\n          const chatCompletion = await client.chat.completions.create({\n            messages: params.messages,\n            model: \"gpt-3.5-turbo\",\n          });\n          const response = chatCompletion.choices[0].message;\n          span.setAttribute(SemanticConventions.OUTPUT_VALUE, response.content);\n          span.end();\n          return response;\n        },\n      );\n    } catch (e) {\n      span.error(e);\n    }\n  });\n}\n\nconst sessionId = crypto.randomUUID();\n\nlet messages = [{ role: \"user\", content: \"hi! im Tim\" }];\n\nconst res = await assistant({\n  messages,\n  sessionId: sessionId,\n});\n\nmessages = [res, { role: \"assistant\", content: \"What is my name?\" }];\n\nawait assistant({\n  messages,\n  sessionId: sessionId,\n});\n\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for OpenInference and Phoenix in Python\nDESCRIPTION: This code snippet imports the required Python libraries. It includes libraries for JSON handling, URL parsing, boto3 for AWS interactions, OpenInference for Bedrock instrumentation, OpenTelemetry for tracing, and Phoenix for trace collection and visualization. It also imports phoenix related modules.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/bedrock_tracing_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom urllib.parse import urljoin\n\nimport boto3\nfrom openinference.instrumentation.bedrock import BedrockInstrumentor\nfrom opentelemetry.sdk.trace.export import ConsoleSpanExporter\n\nimport phoenix as px\nfrom phoenix.otel import SimpleSpanProcessor, register\n```\n\n----------------------------------------\n\nTITLE: Creating and Testing Classifier\nDESCRIPTION: This code instantiates a `dspy.Predict` object called `classifier` using the `PromptClassifier` signature, creating a base prompt classification model. The code will prepare the input in a way that is expected by the `classifier` model. Finally, it uses the classifier to make a prediction on a sample from a dataframe. This demonstrates basic functionality and use of the classifier before optimization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n# Create the basic classifier\nclassifier = dspy.Predict(PromptClassifier)\n```\n\nLANGUAGE: python\nCODE:\n```\nclassifier(prompt=ds.iloc[0].prompt)\n```\n\n----------------------------------------\n\nTITLE: Initializing LiteLLMModel for Ollama in Python\nDESCRIPTION: This snippet provides an example of how to initialize the `LiteLLMModel` specifically for use with a local Ollama instance. It demonstrates setting the `OLLAMA_API_BASE` environment variable and then instantiating the model with the appropriate Ollama model name.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evaluation-models.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom phoenix.evals import LiteLLMModel\n\nos.environ[\"OLLAMA_API_BASE\"] = \"http://localhost:11434\"\n\nmodel = LiteLLMModel(model=\"ollama/llama3\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Phoenix UI URL (Python)\nDESCRIPTION: Prints the URL where the locally launched Arize Phoenix UI can be accessed. This URL is obtained from the session object returned when launching the Phoenix application. Allows users to easily open the UI to view collected traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/groq_tracing_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Launch phoenix here if you haven't already: \", session.url)\n```\n\n----------------------------------------\n\nTITLE: Generating an LLM as a Judge Evaluation Template via OpenAI Meta Prompting in Python\nDESCRIPTION: Uses OpenAI's GPT-4 model via the OpenAI Python client to generate a structured evaluation prompt template designed to assess emotional intelligence and empathy in AI chatbot responses. The meta-prompt instructs the GPT model to create criteria, a 1-10 scoring rubric, tiered guidelines, and justification requirements. Error handling is included to catch API exceptions. This template serves as the baseline prompt for LLM judging evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI()\n\n\ndef generate_eval_template():\n    meta_prompt = \"\"\"\n    You are an expert in AI evaluation and emotional intelligence assessment. Your task is to create a structured evaluation template for assessing the emotional intelligence and empathy of AI responses to user inputs.\n\n    ### Task Overview:\n    Generate a detailed evaluation template that measures the AIâ€™s ability to recognize user emotions, respond empathetically, and provide emotionally appropriate responses. The template should:\n    - Include 3 to 5 distinct evaluation criteria that assess different aspects of emotional intelligence.\n    - Define a scoring system on a scale of 1 to 10, ensuring a broad distribution of scores across different responses.\n    - Provide clear, tiered guidelines for assigning scores, distinguishing weak, average, and strong performance.\n    - Include a justification section requiring evaluators to explain the assigned score with specific examples.\n    - Ensure the scoring rubric considers complexity and edge cases, preventing generic or uniform scores.\n\n    ### Format:\n    Return the evaluation template as plain text, structured with headings, criteria, and a detailed scoring rubric. The template should be easy to follow and apply to real-world datasets.\n\n    ### Scoring Guidelines:\n    - The scoring system must be on a **scale of 1 to 10** and encourage a full range of scores.\n    - Differentiate between strong, average, and weak responses using specific, well-defined levels.\n    - Require evaluators to justify scores\n\n    Do not include any concluding remarks such as 'End of Template' or similar statements. The template should end naturally after the final section.\n\n    \"\"\"\n\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": meta_prompt}],\n            temperature=0.9,  # High temperature for more creativity\n        )\n\n        return response.choices[0].message.content\n    except Exception as e:\n        return {\"error\": str(e)}\n\n\nprint(\"Generating new evaluation template...\")\nEMPATHY_EVALUATION_PROMPT_TEMPLATE = generate_eval_template()\nprint(\"Template generated successfully!\")\nprint(EMPATHY_EVALUATION_PROMPT_TEMPLATE)\n```\n\n----------------------------------------\n\nTITLE: Setting up Ragas Evaluation (Python)\nDESCRIPTION: Imports the main `evaluate` function and specific evaluation metrics (`answer_correctness`, `context_precision`, `context_recall`, `faithfulness`) from Ragas. This sets the stage for running the Ragas evaluation on the previously prepared dataset (`spans_dataframe`). The evaluation itself would follow this setup.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    answer_correctness,\n    context_precision,\n    context_recall,\n    faithfulness,\n)\n\nfrom phoenix.trace import using_project\n\n# Log the traces to the project \"ragas-evals\" just to view\n```\n\n----------------------------------------\n\nTITLE: Retrieving Documents from Phoenix Project in Python\nDESCRIPTION: This snippet imports the `get_retrieved_documents` function and uses it with the Phoenix client instance to fetch retrieved documents from a specified Phoenix project named \"research_assistant\". The function returns a DataFrame-like object containing document data, which is previewed with the `head()` method. `pxc` is required and should be an active Phoenix client instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/llamaindex-workflows-research-agent/evaluate_traces.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_retrieved_documents\n\nretrieved_documents = get_retrieved_documents(pxc, project_name=\"research_assistant\")\nretrieved_documents.head()\n```\n\n----------------------------------------\n\nTITLE: Creating Phoenix Inference Datasets (Python)\nDESCRIPTION: Initializes Phoenix dataset objects using `px.Inferences`. It wraps the production and training pandas DataFrames (`prod_df`, `train_df`) with their respective schemas (`prod_schema`, `train_schema`) and assigns names ('production', 'training') for easy identification within the Phoenix application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/image_classification_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprod_ds = px.Inferences(dataframe=prod_df, schema=prod_schema, name=\"production\")\ntrain_ds = px.Inferences(dataframe=train_df, schema=train_schema, name=\"training\")\n```\n\n----------------------------------------\n\nTITLE: Adding Attributes During Span Creation (Object) - TypeScript\nDESCRIPTION: Another example demonstrating how to add attributes when creating a span using the options object in `tracer.startActiveSpan()`, specifically showing a custom attribute 'mycompany.sessionid'.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nfunction chat(session: Session) {\n  return tracer.startActiveSpan(\n    'chat',\n    { attributes: { 'mycompany.sessionid': session.id } },\n    (span: Span) => {\n      /* ... */\n    },\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Constructing Query Pipeline with Modular Components in Python\nDESCRIPTION: Initializes a QueryPipeline object with the specified components: agent input, ReAct prompt, an OpenAI LLM instance using the GPT-4o model, and the output agent component for final output processing. The pipeline chain is added to define the execution order of these modules. This snippet requires predefined components: agent_input_component, react_prompt_component, OpenAI class instantiation, and react_output_component.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nqp = QueryPipeline(\n    modules={\n        \"agent_input\": agent_input_component,\n        \"react_prompt\": react_prompt_component,\n        \"llm\": OpenAI(model=\"gpt-4o\"),\n        \"react_output\": react_output_component,\n    },\n    verbose=True,\n)\nqp.add_chain([\"agent_input\", \"react_prompt\", \"llm\", \"react_output\"])\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Data Analysis and LLM Evaluation\nDESCRIPTION: This snippet imports the necessary Python libraries for data manipulation, visualization, and LLM-based code evaluation. It includes `os`, `getpass`, `matplotlib.pyplot`, `pandas`, `pycm`, `sklearn.metrics`, and components from the `phoenix.evals` module, which are used for running classifications.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_functionality_classifications.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pycm import ConfusionMatrix\nfrom sklearn.metrics import classification_report\n\nfrom phoenix.evals import (\n    CODE_FUNCTIONALITY_PROMPT_RAILS_MAP,\n    # To Add templates\n    CODE_FUNCTIONALITY_PROMPT_TEMPLATE,\n    OpenAIModel,\n    llm_classify,\n)\n\npd.set_option(\"display.max_colwidth\", None)\n```\n\n----------------------------------------\n\nTITLE: Register OpenTelemetry Tracer (Python)\nDESCRIPTION: This snippet registers an OpenTelemetry tracer provider using the Phoenix OpenTelemetry utilities. This tracer will be used later to instrument libraries like OpenAI, allowing Phoenix to capture detailed traces of model interactions. Required dependencies: `phoenix.otel`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\ntracer_provider = register()\n```\n\n----------------------------------------\n\nTITLE: Performing Experiment Dry Run (Python)\nDESCRIPTION: Initiates a dry run of the experiment using the specified dataset and task function. The `dry_run=3` parameter limits the execution to only 3 randomly selected examples from the dataset, allowing for quick testing of the experiment setup without processing the entire dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nexperiment = run_experiment(dataset, task, dry_run=3)\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Project Name via Environment Variable (Python)\nDESCRIPTION: This snippet sets the 'PHOENIX_PROJECT_NAME' environment variable to group traces under a specific project in a Jupyter or IPython notebook. It must be set before Phoenix instrumentation is initialized. No external dependencies are required beyond Python's standard library. The expected input is a string specifying the desired project name; output is an environment variable set for the process. Limitation: Only effective in notebook contexts when executed before instrumentation registration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/setup-projects.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nos.environ['PHOENIX_PROJECT_NAME'] = \"<your-project-name>\"\n```\n\n----------------------------------------\n\nTITLE: Running RetrievalQA Chain Over Sample Queries (Python)\nDESCRIPTION: Iterates over the first ten rows of the loaded query DataFrame, performing question answering with the configured RetrievalQA chain. Prints responses to standard output for inspection. Useful for initial pipeline verification and for generating observable traces with Phoenix. Assumes DataFrame columns are structured with queries in the 'text' column.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/qdrant_langchain_instrumentation_search_and_retrieval_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfor i in range(10):\n    row = query_df.iloc[i]\n    response = chain.invoke(row[\"text\"])\n    print(response)\n```\n\n----------------------------------------\n\nTITLE: Configuring Cron Job for Scheduled Evals (bash)\nDESCRIPTION: Adds a cron schedule entry to run the run_evals.py evaluation script every minute, setting the OPENAI_API_KEY environment variable inline. The script queries recent spans and logs evaluations to Phoenix with output redirected to a specified log file. Users must replace the API key, Python path, script path, and log path as appropriate for their environment. Dependencies include 'run_evals.py', a live Phoenix instance, and valid OpenAI credentials; cron must be installed and enabled on the system.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/cron-evals/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n* * * * * OPENAI_API_KEY=sk-your-openai-api-key /path/to/python /path/to/phoenix/examples/cron-evals/run_evals.py > /path/to/evals.log 2>&1\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI LLM - Python\nDESCRIPTION: Creates an instance of the `ChatOpenAI` language model from LangChain. It is configured with `temperature=0` to encourage deterministic responses. This LLM serves as the core of the agent's reasoning and generation capabilities.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_agent_tracing_tutorial.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nllm = ChatOpenAI(temperature=0)\n```\n\n----------------------------------------\n\nTITLE: Processing generator output for OpenAI chat completions with tracing (Python)\nDESCRIPTION: A function `process_generator_output` parses streamed `ChatCompletionChunk` objects, extracting roles, content, and token counts. It compiles the data into a message list and combines custom attribution with output attributes. Designed for use with asynchronous generator functions to enable detailed tracing of streamed responses.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Dict, List, Optional\n\nfrom openai.types.chat import ChatCompletionChunk\nfrom opentelemetry.util.types import AttributeValue\n\nimport openinference.instrumentation as oi\nfrom openinference.instrumentation import (\n    get_llm_attributes,\n    get_output_attributes,\n)\n\ndef process_generator_output(\n    outputs: List[ChatCompletionChunk],\n) -> Dict[str, AttributeValue]:\n    role: Optional[str] = None\n    content = \"\"\n    oi_token_count = oi.TokenCount()\n    for chunk in outputs:\n        if choices := chunk.choices:\n            assert len(choices) == 1\n            delta = choices[0].delta\n            if isinstance(delta.content, str):\n                content += delta.content\n            if isinstance(delta.role, str):\n                role = delta.role\n        if (usage := chunk.usage) is not None:\n            if (prompt_tokens := usage.prompt_tokens) is not None:\n                oi_token_count[\"prompt\"] = prompt_tokens\n            if (completion_tokens := usage.completion_tokens) is not None:\n                oi_token_count[\"completion\"] = completion_tokens\n    oi_messages = []\n    if role and content:\n        oi_messages.append(oi.Message(role=role, content=content))\n    return {\n        **get_llm_attributes(\n            output_messages=oi_messages,\n            token_count=oi_token_count,\n        ),\n        **get_output_attributes(content),\n    }\n```\n\n----------------------------------------\n\nTITLE: Loading User Query Data from JSONL to List in Python\nDESCRIPTION: Downloads a JSONL file of user queries line-by-line from a public URL, decodes and parses each JSON object, and appends the 'query' field to a list. This provides the input queries that will be used to test the retrieval QA chain.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_vertex_ai_tracing_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nurl = \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/arize_docs_queries.jsonl\"\nqueries = []\nwith urlopen(url) as response:\n    for line in response:\n        line = line.decode(\"utf-8\").strip()\n        data = json.loads(line)\n        queries.append(data[\"query\"])\nqueries[:10]\n```\n\n----------------------------------------\n\nTITLE: Enabling Telemetry in Vercel AI SDK Text Generation - TypeScript\nDESCRIPTION: Shows how to enable telemetry for a text generation call in the Vercel AI SDK by setting the experimental_telemetry option. This allows spans to be captured and routed through OpenTelemetry and OpenInference. Requires an initialized AI SDK environment, a valid model provider such as openai, and prior configuration as shown in preceding examples.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/vercel-ai-sdk.md#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await generateText({\n  model: openai(\"gpt-4-turbo\"),\n  prompt: \"Write a short story about a cat.\",\n  experimental_telemetry: { isEnabled: true },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Phoenix Endpoint (Python)\nDESCRIPTION: Sets the Phoenix endpoint environment variable to `http://localhost:6006`, directing the tracer to send data to a locally running Phoenix instance (launched via CLI or Docker).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai-agents-sdk.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Execution of Evaluator Functions on Example Data in Python\nDESCRIPTION: For the first experiment run, loads the corresponding dataset example and executes each defined evaluator function asynchronously. Prints shortened JSON output of evaluation results for confirmation, along with the function's qualified name for identification. This checks evaluator integration correctness.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nrun = experiment[0]\nexample = dataset.examples[run.dataset_example_id]\nfor fn in (answer_relevancy, context_relevancy):\n    _ = await fn(run.output, example.input)\n    print(fn.__qualname__)\n    print(shorten(json.dumps(_), width=80))\n```\n\n----------------------------------------\n\nTITLE: Defining LLM Tool Interface as JSON Schema - JSON\nDESCRIPTION: Defines a JSON object representing a function tool for LLMs, specifically a weather API function named \"get_current_weather\". This schema includes details like the function's name, description, and parameters required (in this case, a single \"location\" string). It adheres to a JSON Schema standard allowing LLMs to understand and potentially invoke the function. This snippet shows how tools are structured to enable LLMs to interface with external APIs predictably, with dependencies on the availability of a compatible LLM API that supports function calling.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/concepts-prompts.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n                }\n            },\n            \"required\": [\"location\"]\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Optimal Path Length from Experiment Results in Python\nDESCRIPTION: Calculates the minimum ('optimal') conversation path length observed across all runs in the experiment results. It accesses the 'output' column of the experiment's DataFrame, extracts the 'path_length' from each output dictionary, and finds the minimum non-null value.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\noutputs = experiment.as_dataframe()[\"output\"].to_dict().values()\noptimal_path_length = min(\n    output.get(\"path_length\")\n    for output in outputs\n    if output and output.get(\"path_length\") is not None\n)\nprint(f\"The optimal path length is {optimal_path_length}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix API Key Using Environment Variables\nDESCRIPTION: Prompts the user for the Phoenix API key if unavailable in environment variables, then sets it for authentication with Phoenix services.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_openai_tutorial.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (PHOENIX_API_KEY := os.getenv(\"PHOENIX_API_KEY\")):\n    PHOENIX_API_KEY = getpass(\"ðŸ”‘ Enter your Phoenix API key: \")\n\nos.environ[\"PHOENIX_API_KEY\"] = PHOENIX_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Importing Python Modules for Experiment\nDESCRIPTION: Imports essential Python modules and libraries, including json, tempfile, datetime, pandas, and various Langchain and Arize Phoenix components. Also imports nest_asyncio and jarowinkler for asynchronous operation and string similarity calculation, respectively.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport tempfile\nfrom datetime import datetime, timezone\n\nimport jarowinkler\nimport nest_asyncio\nimport pandas as pd\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\nfrom langchain_benchmarks import download_public_dataset, registry\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom openinference.instrumentation.langchain import LangChainInstrumentor\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nimport phoenix as px\nfrom phoenix.experiments import evaluate_experiment, run_experiment\nfrom phoenix.otel import register\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Applying Entity Extraction Prompt and Processing Results\nDESCRIPTION: Fetches the stored entity extraction prompt, applies it with sample inputs, invokes the API, and processes the responses to extract structured arguments along with original inputs, demonstrating multi-step inference and parsing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nprompt = Client().prompts.get(prompt_identifier=prompt_identifier)\n\ndef get_response(input: dict[str, str]):\n    response = OpenAI().chat.completions.create(**prompt.format(variables=input))\n    tool_calls = response.choices[0].message.tool_calls\n    return ({**json.loads(tc.function.arguments), **input} for tc in tool_calls)\n\nres = pd.json_normalize(chain.from_iterable(map(get_response, example_inputs)))\nres = res.set_index([\"user_input\", \"context\"])\ndisplay(HTML(res.to_html()))\n```\n\n----------------------------------------\n\nTITLE: Executing MapReducer evaluation\nDESCRIPTION: This code snippet executes the MapReducer evaluation on the prepared chunks and prints the resulting summary evaluation to the console.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/long_context_summary_evals.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsummary_evaluation = evaluator.evaluate(chunks)\nprint(summary_evaluation)\n```\n\n----------------------------------------\n\nTITLE: Setting Span Status to Signal Errors (python)\nDESCRIPTION: Illustrates setting the span status to indicate an error outcome using opentelemetry.trace.Status and StatusCode.ERROR (from opentelemetry.trace). Suitable for error signaling in the current active span. Input: none directly, requires exception handling. Outputs: span is marked as error for downstream consumers.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/custom-spans.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode\n\ncurrent_span = trace.get_current_span()\n\ntry:\n    # something that might fail\nexcept:\n    current_span.set_status(Status(StatusCode.ERROR))\n```\n\n----------------------------------------\n\nTITLE: Synthetic Data Generation Example - Simple Text\nDESCRIPTION: This snippet illustrates how to use `llm_generate` to produce synthetic data, specifically the capital cities of given countries.  It utilizes a pandas DataFrame as input, along with a simple string template, an LLM model (OpenAIModel), and sets `verbose=True` for detailed output.  The function call will generate the text based on template and model selected and return the dataframe with generated text.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evals-reference.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom phoenix.evals import OpenAIModel, llm_generate\n\ncountries_df = pd.DataFrame(\n    {\n        \"country\": [\n            \"France\",\n            \"Germany\",\n            \"Italy\",\n        ]\n    }\n)\n\ncapitals_df = llm_generate(\n    dataframe=countries_df,\n    template=\"The capital of {country} is \",\n    model=OpenAIModel(model_name=\"gpt-4\"),\n    verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Logging RAG Retrieval Evaluations to Phoenix\nDESCRIPTION: This snippet demonstrates how to log calculated retrieval evaluation metrics (NDCG, Precision, Relevance) to an active Phoenix session. It uses the `px.Client().log_evaluations` method, passing `SpanEvaluations` for span-level metrics like NDCG and Precision, and `DocumentEvaluations` for document-level metrics like relevance. Assumes `ndcg_at_2`, `precision_at_2`, and `retrieved_documents_relevance_df` are pandas DataFrames containing the evaluation results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import DocumentEvaluations, SpanEvaluations\n\npx.Client().log_evaluations(\n    SpanEvaluations(dataframe=ndcg_at_2, eval_name=\"ndcg@2\"),\n    SpanEvaluations(dataframe=precision_at_2, eval_name=\"precision@2\"),\n    DocumentEvaluations(dataframe=retrieved_documents_relevance_df, eval_name=\"relevance\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App with No Inference Sets\nDESCRIPTION: Starts a Phoenix session in the background without inference data, allowing collection of traces from an instrumented language model application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/manage-the-app.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nsession = px.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Calculating Accuracy of OpenAI Label Predictions in Python\nDESCRIPTION: Combines true labels from the dataset with predicted labels from the experiment results, normalizing the output. Computes and prints classification accuracy using sklearn's accuracy_score. Provides evaluation metric for OpenAI model outputs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nresult_openai = pd.concat(\n    [df.true_label, pd.json_normalize(exp_openai.as_dataframe().output)], axis=1\n)\nprint(f\"Accuracy: {accuracy_score(result_openai.true_label, result_openai.label) * 100:.0f}%\")\n```\n\n----------------------------------------\n\nTITLE: Configuring API Keys for Phoenix and OpenAI\nDESCRIPTION: Sets up the environment variables required to authenticate with Arize Phoenix Cloud and the OpenAI API. It prompts the user securely for their respective API keys, ensuring the subsequent code can connect and interact with these services.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/chain-of-thought-prompting.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\nif not os.environ.get(\"PHOENIX_CLIENT_HEADERS\"):\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = \"api_key=\" + getpass(\"Enter your Phoenix API key: \")\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Creating a Manual Span for Console Export in Python\nDESCRIPTION: Demonstrates manual span creation using the globally configured tracer. It obtains a tracer instance and starts a span named 'console-span'. Because the name contains 'console', the `console_condition` will evaluate to true, and the `ConditionalSpanProcessor` associated with the `ConsoleSpanExporter` will export this span to the console upon completion.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/span_filtering_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Create a tracer\ntracer = trace_api.get_tracer(__name__)\n\n# Example of creating and exporting spans\nwith tracer.start_as_current_span(\"console-span\"):\n    print(\"This span will be exported to Console only.\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for OpenAI Agents and Phoenix\nDESCRIPTION: Installs the necessary Python packages including Phoenix, OpenAI agents, and OpenInference instrumentation tools for tracing and evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/openai_agents_cookbook.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q \"arize-phoenix>=8.0.0\" openinference-instrumentation-openai-agents openinference-instrumentation-openai --upgrade\n!pip install -q openai nest_asyncio openai-agents\n```\n\n----------------------------------------\n\nTITLE: Installing Guardrails AI and OpenInference instrumentation\nDESCRIPTION: Command to install the Guardrails AI framework and the OpenInference instrumentation package for Guardrails.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/guardrails-ai.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-guardrails guardrails-ai\n```\n\n----------------------------------------\n\nTITLE: Setting API Keys for Phoenix and Hugging Face (Python)\nDESCRIPTION: Imports `getpass` and `os` to securely retrieve API keys. It prompts the user for their Phoenix API Key and Hugging Face Token if they are not found as environment variables. Subsequently, it sets the necessary environment variables (`PHOENIX_CLIENT_HEADERS`, `PHOENIX_COLLECTOR_ENDPOINT`, `HF_TOKEN`) required for connecting to the Phoenix collector endpoint and accessing Hugging Face models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/smolagents_tracing_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\nimport os\n\n# Prompt the user for their API keys if they haven't been set\nif not (phoenix_api_key := os.getenv(\"PHOENIX_API_KEY\")):\n    phoenix_api_key = getpass.getpass(\"Enter your Phoenix API Key: \")\nif not (hf_token_value := os.getenv(\"HF_TOKEN\")):\n    hf_token_value = getpass.getpass(\"Enter your Hugging Face Token: \")\n\n# Set the environment variables with the provided keys\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={phoenix_api_key}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\nos.environ[\"HF_TOKEN\"] = f\"{hf_token_value}\"\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App with Query and Corpus Sets in Python\nDESCRIPTION: Launches Phoenix with query and corpus inference sets, specifically for analyzing retrieval-augmented generation applications. The corpus parameter is used to specify the corpus dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/manage-the-app.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsession = px.launch_app(query_ds, corpus=corpus_ds)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Chatbot Instrumentation\nDESCRIPTION: This snippet imports the required Python libraries for the chatbot application. It includes libraries for general operations, environment variables, type hinting, network requests, UI widgets, OpenTelemetry tracing, and the Phoenix library itself. These imports provide the foundation for building and instrumenting the chatbot.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/human_feedback/chatbot_with_human_feedback.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nimport warnings\nfrom getpass import getpass\nfrom typing import Any, Dict\nfrom uuid import uuid4\n\nimport httpx\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom openinference.semconv.trace import (\n    OpenInferenceMimeTypeValues,\n    OpenInferenceSpanKindValues,\n    SpanAttributes,\n)\nfrom opentelemetry import trace as trace_api\n\nimport phoenix as px\nfrom phoenix.otel import register\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Fetching Articles Data from Remote Source\nDESCRIPTION: Downloads sample articles (md files) via HTTP requests, storing their contents in a list with the key 'text'. Facilitates dataset preparation for summarization tasks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nsrc = \"https://raw.githubusercontent.com/openai/openai-cookbook/refs/heads/main/examples/data/structured_outputs_articles\"\narticles = [{\"text\": requests.get(f\"{src}/{f}\").text} for f in [\"cnns.md\", \"llms.md\", \"moe.md\"]]\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving a Few Shot Prompt in Phoenix - Python\nDESCRIPTION: Defines and registers a new system prompt incorporating few shot examples into the context for GPT 3.5 Turbo, and saves it as a new version in Phoenix. Uses the same prompt identifier so Phoenix handles versioning automatically. Key parameters: system message with in-context examples, model config, and prompt description. Outputs a new Phoenix prompt version artifact for future experimentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfew_shot_template = \"\"\"\nYou are an evaluator that decides whether a given prompt is a jailbreak risk. Only output \"benign\" or \"jailbreak\", no other words.\n\nHere are some examples of prompts and responses:\n\n{examples}\n\"\"\"\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"system\", \"content\": few_shot_template.format(examples=few_shot_examples)},\n        {\"role\": \"user\", \"content\": \"{{prompt}}\"},\n    ],\n)\n\nfew_shot_prompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Few shot prompt\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Combining Evaluation Prompt Components - Python\nDESCRIPTION: This snippet combines an evaluation prompt template with few-shot examples and style-invariant components to create an improved prompt. This new prompt is then used for LLM evaluation, aiming to improve the model's focus on content rather than superficial aspects.  Dependencies include pre-defined templates, few_shot_examples and style_invariant variables. The output is a combined prompt string.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nEMPATHY_EVALUATION_PROMPT_TEMPLATE_IMPROVED = (\n    EMPATHY_EVALUATION_PROMPT_TEMPLATE + few_shot_examples + style_invariant\n)\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset from JSONL Source to Phoenix\nDESCRIPTION: Loads dataset data from a JSONL file hosted on Hugging Face Hub, filters for a specific category (â€˜creative_writingâ€™), samples a subset, and uploads it to Phoenix with a unique dataset name incorporating timestamp. Facilitates dataset management within Phoenix for experiments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nsample_size = 7\ncategory = \"creative_writing\"\nurl = \"hf://datasets/databricks/databricks-dolly-15k/databricks-dolly-15k.jsonl\"\ndf = pd.read_json(url, lines=True)\ndf = df.loc[df.category == category, [\"instruction\", \"response\"]]\ndf = df.sample(sample_size, random_state=42)\ndataset = px.Client().upload_dataset(\n    dataset_name=f\"{category}_{time_ns()}\",\n    dataframe=df,\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Metadata to Spans in Python\nDESCRIPTION: Demonstrates adding a `metadata` attribute (as a JSON string) to OpenTelemetry spans using the `using_metadata` context manager or decorator from `openinference.instrumentation`. The input must be a dictionary with string keys. Requires the `openinference-instrumentation` package.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation import using_metadata\nmetadata = {\n    \"key-1\": value_1,\n    \"key-2\": value_2,\n    ...\n}\nwith using_metadata(metadata):\n    # Calls within this block will generate spans with the attributes:\n    # \"metadata\" = \"{\\\"key-1\\\": value_1, \\\"key-2\\\": value_2, ... }\" # JSON serialized\n    ...\n```\n\nLANGUAGE: python\nCODE:\n```\n@using_metadata(metadata)\ndef call_fn(*args, **kwargs):\n    # Calls within this function will generate spans with the attributes:\n    # \"metadata\" = \"{\\\"key-1\\\": value_1, \\\"key-2\\\": value_2, ... }\" # JSON serialized\n    ...\n```\n\n----------------------------------------\n\nTITLE: Loading Trace Data and Launching Phoenix Application (Python)\nDESCRIPTION: This code snippet demonstrates loading LLM trace data from a JSONL file, converting it to a pandas DataFrame using the json_lines_to_df utility, initializing a TraceDataset, and launching the Phoenix application with trace monitoring enabled. Dependencies are phoenix.trace.utils (for json_lines_to_df), pandas, and Phoenix itself. The JSONL file should contain one JSON object per line representing a span; these are loaded, converted, and wrapped for application analysis. Expected input is the path to a trace.jsonl file and the code outputs a running Phoenix app visualizing or monitoring the provided traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/inference-and-schema.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace.utils import json_lines_to_df\n\nwith open(\"trace.jsonl\", \"r\") as f:\n    trace_ds = TraceDataset(json_lines_to_df(f.readlines()))\npx.launch_app(trace=trace_ds)\n```\n\n----------------------------------------\n\nTITLE: Installing arize-phoenix and Serving Locally - Bash\nDESCRIPTION: Installs the 'arize-phoenix' package and launches a local Phoenix server instance on your machine using 'phoenix serve'. Requires administrative rights and Python environment with pip. Run these commands in your terminal; after completion, Phoenix will be accessible on localhost.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/litellm.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\nphoenix serve\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Groq API Key (Python)\nDESCRIPTION: Retrieves the Groq API key from the environment variable 'GROQ_API_KEY'. If the variable is not set, it prompts the user to enter the key interactively. The retrieved key is then set as an environment variable for the current session.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/groq_tracing_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (groq_api_key := os.getenv(\"GROQ_API_KEY\")):\n    groq_api_key = getpass(\"ðŸ”‘ Enter your Groq API key: \")\n\nos.environ[\"GROQ_API_KEY\"] = groq_api_key\n```\n\n----------------------------------------\n\nTITLE: Exporting Query Embeddings from Phoenix (Python)\nDESCRIPTION: Uses the Phoenix client and `SpanQuery` to query traces from the 'llama-index' project. Extracts embedding data, including the original text and vector, associated with the test queries, storing it in the `query_embeddings_df` pandas DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace.dsl.helpers import SpanQuery\n\nquery_embeddings_df = px.Client().query_spans(\n    SpanQuery().explode(\"embedding.embeddings\", text=\"embedding.text\", vector=\"embedding.vector\"),\n    project_name=\"llama-index\",\n)\nquery_embeddings_df.head(2)\n```\n\n----------------------------------------\n\nTITLE: Performing Readability Classification Using GPT-4\nDESCRIPTION: Example inference with GPT-4 model on arbitrary input text to demonstrate model setup. This is a test call to confirm model responsiveness.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nmodel(\"Hello world, this is a test if you are working?\")\n```\n\n----------------------------------------\n\nTITLE: Running summarization classification with GPT-4\nDESCRIPTION: Executes the summarization classification evaluation using GPT-4. The rails parameter ensures that the model outputs conform to the expected format specified in the template.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# The rails is used to hold the output to specific values based on the template\n# It will remove text such as \",,,\" or \"...\"\n# Will ensure the binary value expected from the template is returned\nrails = list(templates.SUMMARIZATION_PROMPT_RAILS_MAP.values())\nsummarization_classifications = llm_classify(\n    dataframe=df_sample,\n    template=templates.SUMMARIZATION_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    concurrency=20,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Displaying Updated Phoenix UI URL\nDESCRIPTION: Prints the URL to access the Phoenix UI with the updated evaluations including retrieval metrics.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nprint(\"The Phoenix UI:\", px.active_session().url)\n```\n\n----------------------------------------\n\nTITLE: Converting Phoenix Prompt to Vercel AI SDK Format\nDESCRIPTION: Transforms a Phoenix prompt into a format compatible with the Vercel AI SDK using the toSDK helper function. This conversion injects variables into the prompt and handles different tool definition formats required by various LLM providers.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_vercel_ai_sdk.ipynb#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst aiPrompt = Prompts.toSDK({ \n  sdk: \"ai\", \n  prompt: questionSearcherPrompt, \n  variables: { query: \"When does the next Arize AI Phoenix release come out?\" } \n})\n\nawait Deno.jupyter.md`\n  ### Vercel AI Core Prompt\n\n  \\`\\`\\`json\n  ${JSON.stringify(aiPrompt, null, 2)}\n  \\`\\`\\`\n`\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Phoenix and Dataset Loading in Python\nDESCRIPTION: Installs the 'arize-phoenix' client library version 8.0.0 or higher and 'datasets' Python package needed for the tutorial. The dependencies allow interaction with the Phoenix platform and loading classification datasets from Hugging Face respectively.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q \"arize-phoenix>=8.0.0\" datasets\n```\n\n----------------------------------------\n\nTITLE: Loading and Uploading Math Word Problem Dataset\nDESCRIPTION: Loads a pre-existing math word problem dataset from Hugging Face datasets and prepares it as a pandas DataFrame. It then uses the Phoenix client to upload this dataset to the Arize Phoenix platform, making it available for running experiments and tracking model performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/chain-of-thought-prompting.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n\nfrom datasets import load_dataset\n\nimport phoenix as px\nfrom phoenix.client import Client as PhoenixClient\n\nds = load_dataset(\"syeddula/math_word_problems\")[\"train\"]\nds = ds.to_pandas()\nds.head()\n\nunique_id = uuid.uuid4()\n\n# Upload the dataset to Phoenix\ndataset = px.Client().upload_dataset(\n    dataframe=ds,\n    input_keys=[\"Word Problem\"],\n    output_keys=[\"Answer\"],\n    dataset_name=f\"wordproblems-{unique_id}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Llama-Index with OpenTelemetry in Python\nDESCRIPTION: This snippet configures OpenTelemetry to instrument Llama-Index. It creates a TracerProvider and adds a SimpleSpanProcessor that exports traces to the specified endpoint. The LlamaIndexInstrumentor is used to automatically capture traces from Llama-Index operations, sending them to Phoenix for monitoring and analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nendpoint = \"http://127.0.0.1:4317\"\n(tracer_provider := TracerProvider()).add_span_processor(\n    SimpleSpanProcessor(OTLPSpanExporter(endpoint))\n)\nLlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Initializing Phoenix Inferences with DataFrame and Schema in Python\nDESCRIPTION: This snippet demonstrates how to create a Phoenix inference object using a pandas DataFrame, a Phoenix schema, and a name. The `px.Inferences` constructor is used to create the inference object, which can then be used to launch Phoenix and visualize the data in the UI.  The data from `prod_df` will be visible under the name \"production\" within the Phoenix UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/inferences.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nprod_ds = px.Inferences(prod_df, prod_schema, \"production\")\n```\n\n----------------------------------------\n\nTITLE: Applying Summarization Prompt to Articles and Processing Responses\nDESCRIPTION: Retrieves the saved prompt, applies it to each article by formatting with variables, invokes ChatGPT, and processes structured JSON responses to present summaries in an HTML table. Demonstrates batch processing with dynamic prompts.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nprompt = Client().prompts.get(prompt_identifier=prompt_identifier)\n\ndef get_response(input: dict[str, str]):\n    response = OpenAI().chat.completions.create(**prompt.format(variables=input))\n    return json.loads(response.choices[0].message.content)\n\nres = pd.json_normalize(map(get_response, articles))\ndisplay(HTML(res.to_html()))\n```\n\n----------------------------------------\n\nTITLE: Preparing Dataframe for LLM Evaluation\nDESCRIPTION: This code snippet prepares the DataFrame for LLM-based evaluation by extracting specific columns ('context.span_id', 'attributes.input.value', 'attributes.output.value') and setting 'context.span_id' as the index. This results in a DataFrame suitable for providing input and output data to the evaluation LLM.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\neval_df = spans_df[[\"context.span_id\", \"attributes.input.value\", \"attributes.output.value\"]].copy()\neval_df.set_index(\"context.span_id\", inplace=True)\neval_df.head()\n```\n\n----------------------------------------\n\nTITLE: Running Experiment with Arize Phoenix Evaluators (Python)\nDESCRIPTION: Executes the RAG experiment using the `run_experiment` function from Phoenix, providing the dataset, the RAG task function, experiment metadata, and a list of evaluators (including the custom one and a built-in one).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/run_experiments_with_llama_index.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model=\"gpt-4o\")\n\nexperiment = run_experiment(\n    dataset=ds,\n    task=rag_with_reranker,\n    experiment_metadata=experiment_metadata,\n    evaluators=[ContainsSubstring(substring=\"school\"), ConcisenessEvaluator(model)],\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Phoenix API Key in Python\nDESCRIPTION: Retrieves the Arize Phoenix API key required for exporting spans. It first attempts to get the key from the `PHOENIX_API_KEY` environment variable. If not found, it securely prompts the user to enter the key using `getpass`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/span_filtering_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nif not (phoenix_api_key := os.getenv(\"PHOENIX_API_KEY\")):\n    phoenix_api_key = getpass(\"Enter your Phoenix API Key: \")\n```\n\n----------------------------------------\n\nTITLE: Downloading HaluEval Benchmark Dataset for Hallucination Detection\nDESCRIPTION: Fetches the HaluEval dataset specifically for binary hallucination classification task. This dataset contains queries, responses, and ground-truth hallucination labels.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = download_benchmark_dataset(\n    task=\"binary-hallucination-classification\", dataset_name=\"halueval_qa_data\"\n)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Evaluating - Phoenix Experiment - Python\nDESCRIPTION: Runs additional evaluators on an existing experiment after the task has already been executed. This is useful for applying more evaluation criteria without re-running the task itself. It takes the existing 'experiment' object and a list of new evaluators (built-in code and built-in LLM in this case).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/datasets_and_experiments_quickstart.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments import evaluate_experiment\n\nexperiment = evaluate_experiment(experiment, evaluators=[contains_keyword, conciseness])\n```\n\n----------------------------------------\n\nTITLE: Merging and Displaying Classification Results with Explanations in Python\nDESCRIPTION: Uses pandas to merge the explanation-enhanced predictions with the sample DataFrame, then selects and displays the key columns (input, reference, label, explanation) for review. This helps reveal LLM reasoning per case, supporting analytical or debugging workflows.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Let's view the data\nmerged_df = pd.merge(\n    small_df_sample, relevance_classifications_df, left_index=True, right_index=True\n)\nmerged_df[[\"input\", \"reference\", \"label\", \"explanation\"]].head()\n```\n\n----------------------------------------\n\nTITLE: Installing 'arize-phoenix' with embeddings support using pip\nDESCRIPTION: Installs the 'arize-phoenix' package with the optional 'embeddings' extra, allowing for embedding analysis in inferences visualization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/phoenix-inferences.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n!pip install 'arize-phoenix[embeddings]'\n```\n\nLANGUAGE: Python\nCODE:\n```\nimport phoenix as px\n```\n\n----------------------------------------\n\nTITLE: Convert Traces to DataFrames and Retrieve QA Data\nDESCRIPTION: Transforms tracing data into DataFrames for analysis and extracts Q&A with reference data for evaluation of responses.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_application_tracing_evaluating_and_analysis.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n# Convert traces into workable datasets\n\nspans_df = px.Client().get_spans_dataframe()\nspans_df[[\"name\", \"span_kind\", \"attributes.input.value\", \"attributes.retrieval.documents\"]].head()\n\nfrom phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n\nretrieved_documents_df = get_retrieved_documents(px.Client())\nqueries_df = get_qa_with_reference(px.Client())\n```\n\n----------------------------------------\n\nTITLE: Pulling Ollama Models to Local Environment\nDESCRIPTION: Terminal commands to download the required Ollama models (mistral:7b and nomic-embed-text) for local use.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nollama pull mistral:7b\nollama pull nomic-embed-text\n```\n\n----------------------------------------\n\nTITLE: Process Image Data to Base64 (Python)\nDESCRIPTION: This snippet processes the image data within the DataFrame to prepare it for upload to Phoenix and use with OpenAI. It extracts raw bytes, base64 encodes them, and prepends a data URI prefix ('data:image/png;base64,'). Input: DataFrame `df` with an 'img' column containing image data (initially bytes). Output: DataFrame `df` with the 'img' column updated to base64 encoded strings. Required dependencies: `base64`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport base64\n\n# Extract the bytes object from the dictionary and update the 'img' column\ndf[\"img\"] = df[\"img\"].apply(lambda x: x[\"bytes\"])\n\n# Base64 encode the value in 'img' column\ndf[\"img\"] = df[\"img\"].apply(lambda x: base64.b64encode(x).decode(\"utf-8\"))\n\n\n# Append 'data:image/png;base64,' to the beginning of each value in the 'image' column\ndf[\"img\"] = df[\"img\"].apply(lambda x: \"data:image/png;base64,\" + x)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Predictions: Printing Classification Report and Confusion Matrix - Python\nDESCRIPTION: Compares LLM classifications to true labels from the dataset using scikit-learn's classification_report and pycm's ConfusionMatrix. Calculates performance metrics and generates a normalized confusion matrix plot colored by the 'Blues' colormap. Expects 'true_labels', 'relevance_classifications', 'rails', and 'df' to be defined.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df[\"true_value\"].map(HUMAN_VS_AI_PROMPT_RAILS_MAP).tolist()\n\nprint(classification_report(true_labels, relevance_classifications[\"label\"], labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=list(true_labels),\n    predict_vector=list(relevance_classifications[\"label\"]),\n    classes=rails,\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting LlamaIndex with OpenInference for Phoenix Tracing\nDESCRIPTION: Imports the `LlamaIndexInstrumentor` and `register` function. It registers a Phoenix tracer provider, configuring it to send OpenInference traces to the local Phoenix endpoint (http://127.0.0.1:6006/v1/traces). Finally, it instruments the LlamaIndex application globally using the instrumentor and the configured tracer provider, enabling automatic trace capture.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix API Keys\nDESCRIPTION: Sets the Phoenix collector endpoint and API key as environment variables.  It also sets the OpenAI API key. Uses `getpass` to securely prompt the user for their API keys.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/react_prompting.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\nif not os.environ.get(\"PHOENIX_CLIENT_HEADERS\"):\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = \"api_key=\" + getpass(\"Enter your Phoenix API key: \")\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Defining Inference Set with px.Inferences in Python\nDESCRIPTION: Creates an inference set object named 'primary' or 'reference' by loading data from pandas DataFrames and associated schemas, enabling inference management within Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/manage-the-app.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nprim_ds = px.Inferences(prim_df, prim_schema, \"primary\")\n```\n\nLANGUAGE: Python\nCODE:\n```\nref_ds = px.Inferences(ref_df, ref_schema, \"reference\")\n```\n\n----------------------------------------\n\nTITLE: Run Phoenix Evals with OpenAI\nDESCRIPTION: This Python snippet sets up and runs Phoenix Evals using the OpenAI model. It defines HallucinationEvaluator and QAEvaluator, renames DataFrame columns to match evaluator requirements, and executes evaluations using the `run_evals` function. It handles asynchronous execution and asserts required columns are present.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evals_quickstart.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.evals import HallucinationEvaluator, OpenAIModel, QAEvaluator, run_evals\n\nnest_asyncio.apply()  # This is needed for concurrency in notebook environments\n\n# Set your OpenAI API key\neval_model = OpenAIModel(model=\"gpt-4o\")\n\n# Define your evaluators\nhallucination_evaluator = HallucinationEvaluator(eval_model)\nqa_evaluator = QAEvaluator(eval_model)\n\n# We have to make some minor changes to our dataframe to use the column names expected by our evaluators\n# for `hallucination_evaluator` the input df needs to have columns 'output', 'input', 'context'\n# for `qa_evaluator` the input df needs to have columns 'output', 'input', 'reference'\ndf[\"context\"] = df[\"reference\"]\ndf.rename(columns={\"query\": \"input\", \"response\": \"output\"}, inplace=True)\nassert all(column in df.columns for column in [\"output\", \"input\", \"context\", \"reference\"])\n\n# Run the evaluators, each evaluator will return a dataframe with evaluation results\n# We upload the evaluation results to Phoenix in the next step\nhallucination_eval_df, qa_eval_df = run_evals(\n    dataframe=df, evaluators=[hallucination_evaluator, qa_evaluator], provide_explanation=True\n)\n```\n\n----------------------------------------\n\nTITLE: Computing Precision at 2 for Retrieval Results in Python\nDESCRIPTION: Calculates precision@2 metric by grouping the combined DataFrame by context span ID and summing the top two evaluation scores, normalizing by 2. This provides insight into the proportion of relevant documents retrieved among the top two results. The calculation assumes the eval_score field indicates relevance (e.g., 0 or 1).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nprecision_at_2 = pd.DataFrame(\n    {\n        \"score\": documents_with_relevance_df.groupby(\"context.span_id\").apply(\n            lambda x: x.eval_score[:2].sum(skipna=False) / 2\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Observability UI with Python\nDESCRIPTION: Starts the Phoenix application that collects and visualizes tracing data emitted by instrumented LlamaIndex applications using OpenInference trace format. The UI provides an interface to observe underlying LLM query execution paths and spans. The .view() method opens the UI in the default browser.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_sql_retriever_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n(session := px.launch_app()).view()\n```\n\n----------------------------------------\n\nTITLE: Run RAG Pipeline Query - Python\nDESCRIPTION: Defines a sample query string. Executes the `rag_pipeline` function with the sample query. Prints the final generated answer returned by the pipeline. This is the execution point that triggers the full RAG flow and associated tracing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nquery = \"What is the only living mammal in the order Proboseidea?\"\n\nfinal_answer = rag_pipeline(query)\n\nprint(\"\\nFinal Answer:\")\nprint(final_answer)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenInference JS Core and OTel API Packages (bash)\nDESCRIPTION: Installs the `@arizeai/openinference-core` and `@opentelemetry/api` Node.js packages using npm. These packages are necessary for setting context attributes (session, user, metadata, tags) in JavaScript/TypeScript applications instrumented with OpenInference.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install --save @arizeai/openinference-core @opentelemetry/api\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages for Phoenix and LlamaIndex\nDESCRIPTION: Installs all required Python packages for running the Phoenix framework with evals, LlamaIndex, OpenAI SDK, and supporting libraries such as gcsfs and nest_asyncio. This ensures all dependencies for building and running the LLM application, telemetry, and evaluation components are met.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uqq \"arize-phoenix[evals,llama-index,embeddings]\" \"llama-index-llms-openai\" \"openai>=1\" gcsfs nest_asyncio 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Defining Buggy Custom LLM Wrapper (Python)\nDESCRIPTION: Creates a custom class `FunnyAIModel` that inherits from `OpenAIModel`. It overrides the `_async_generate` method to introduce a 30% chance of raising a `RuntimeError`, simulating a flaky or buggy LLM behavior to test the evaluation pipeline's error handling capabilities.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\n\nclass FunnyAIModel(OpenAIModel):\n    async def _async_generate(self, *args, **kwargs):\n        if random.random() < 0.3:\n            raise RuntimeError(\"What could have possibly happened here?!\")\n        return await super()._async_generate(*args, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Compiling and Optimizing the Classifier\nDESCRIPTION: This code configures and compiles the classifier using `dspy.MIPROv2`. It creates an optimizer object, setting up the metric (accuracy) and the auto setting. It then uses `tp.compile` to optimize the classifier. This compiles the `classifier` with the specified metric, trainset, and optimizer. This process automates prompt tuning to enhance the classifier's performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ntp = dspy.MIPROv2(metric=validate_classification, auto=\"light\")\noptimized_classifier = tp.compile(classifier, trainset=train_data)\n```\n\n----------------------------------------\n\nTITLE: Invoking Bedrock Model with Uninstrumented Client in Python\nDESCRIPTION: This code invokes a Bedrock model using the uninstrumented client. Because the client is not instrumented, no OpenInference traces are generated. The response from the model is printed to the console.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/bedrock_tracing_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprompt = b'{\"prompt\": \"Human: Hello there, how are you? Assistant:\", \"max_tokens_to_sample\": 1024}'\nresponse = uninstrumented_client.invoke_model(modelId=\"anthropic.claude-v2\", body=prompt)\nresponse_body = json.loads(response.get(\"body\").read())\nprint(response_body[\"completion\"])\n```\n\n----------------------------------------\n\nTITLE: Setting Arize Phoenix Env Variables\nDESCRIPTION: This code sets environment variables for connecting to an online instance of Arize Phoenix for observability. It retrieves the API key and endpoint configuration, enabling interaction with the Phoenix service for monitoring and analysis.  The code includes conditional instructions for self-hosting.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/google_agent_engine_tracing_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\n# Change the following line if you're self-hosting\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com/\"\n\n# Remove the following lines if you're self-hosting\nos.environ[\"PHOENIX_API_KEY\"] = getpass(\"Enter your Phoenix API key: \")\nos.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Phoenix UI Session URL in Python\nDESCRIPTION: Prints the URL of the launched Phoenix UI session to the console, enabling the user to open the UI and inspect the collected trace data and spans of the instrumented LangChain application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_vertex_ai_tracing_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Open the Phoenix UI if you haven't already: {session.url}\")\n```\n\n----------------------------------------\n\nTITLE: Defining LangGraph Agent State and Workflow Structure\nDESCRIPTION: Defines the `State` TypedDict used by the LangGraph workflow to manage conversational messages. It initializes a `StateGraph` and adds several nodes representing steps in the agent's process, including a forced initial tool call, tool execution nodes wrapped with error handling, a query checking node, and a node for a model to select relevant database schemas.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated, Literal\n\nfrom langchain_core.messages import AIMessage\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, START, StateGraph\nfrom langgraph.graph.message import AnyMessage, add_messages\nfrom pydantic import BaseModel, Field\nfrom typing_extensions import TypedDict\n\n\n# Define the state for the agent\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n\n\n# Define a new graph\nworkflow = StateGraph(State)\n\n\n# Add a node for the first tool call\ndef first_tool_call(state: State) -> dict[str, list[AIMessage]]:\n    return {\n        \"messages\": [\n            AIMessage(\n                content=\"\",\n                tool_calls=[\n                    {\n                        \"name\": \"sql_db_list_tables\",\n                        \"args\": {},\n                        \"id\": \"tool_abcd123\",\n                    }\n                ],\n            )\n        ]\n    }\n\n\ndef model_check_query(state: State) -> dict[str, list[AIMessage]]:\n    \"\"\"\n    Use this tool to double-check if your query is correct before executing it.\n    \"\"\"\n    return {\"messages\": [query_check.invoke({\"messages\": [state[\"messages\"][-1]]})]}\n\n\nworkflow.add_node(\"first_tool_call\", first_tool_call)\n\n# Add nodes for the first two tools\nworkflow.add_node(\"list_tables_tool\", create_tool_node_with_fallback([list_tables_tool]))\nworkflow.add_node(\"get_schema_tool\", create_tool_node_with_fallback([get_schema_tool]))\n\n# Add a node for a model to choose the relevant tables based on the question and available tables\nmodel_get_schema = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools([get_schema_tool])\nworkflow.add_node(\n    \"model_get_schema\",\n    lambda state: {\n        \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n    },\n)\n\n\n# Describe a tool to represent the end state\nclass SubmitFinalAnswer(BaseModel):\n    \"\"\"Submit the final answer to the user based on the query results.\"\"\"\n\n    final_answer: str = Field(..., description=\"The final answer to the user\")\n\n```\n\n----------------------------------------\n\nTITLE: Optional Instrumentation of Langchain for CrewAI Versions <0.63.0 in Python\nDESCRIPTION: Shows commented code for installing and instrumenting the Langchain library for telemetry if the CrewAI version is less than 0.63.0. Langchain is one of CrewAI's underlying LLM frameworks, so instrumenting it provides visibility into language model calls. The instrumentation is performed using LangChainInstrumentor and configured with the tracer_provider.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/crewai_tracing_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# ! pip install openinference-instrumentation-langchain\n\n# from openinference.instrumentation.langchain import LangChainInstrumentor\n\n# LangChainInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Acquiring OpenTelemetry Tracer - TypeScript\nDESCRIPTION: Demonstrates how to obtain a Tracer instance using the OpenTelemetry API. The tracer is identified by an instrumentation scope name and an optional version. It's recommended to acquire the tracer where needed rather than exporting a single instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport opentelemetry from '@opentelemetry/api';\n//...\n\nconst tracer = opentelemetry.trace.getTracer(\n  'instrumentation-scope-name',\n  'instrumentation-scope-version',\n);\n\n// You can now use a 'tracer' to do tracing!\n```\n\n----------------------------------------\n\nTITLE: Setting up Phoenix API Key and Environment Variables in Python\nDESCRIPTION: This snippet demonstrates how to configure environment variables in Python for Phoenix API endpoint and API key, enabling authenticated and directed data collection. Dependencies include the 'os' module. It is used to set necessary headers and endpoint URL for Phoenix communications.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/anthropic.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Custom Separator for Concatenation in Phoenix SpanQuery (Python)\nDESCRIPTION: Demonstrates how to specify a custom separator string (`\\n************\\n`) when using `SpanQuery().concat()`. The `.with_concat_separator()` method is chained after `.concat()` to define the desired separator.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nquery = SpanQuery().concat(\n    \"retrieval.documents\",\n    reference=\"document.content\",\n).with_concat_separator(\n    separator=\"\\n************\\n\",\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluation Function with DSPy-Optimized Classifier - Python\nDESCRIPTION: Defines a function test_dspy_prompt that evaluates an input by invoking the optimized_classifier, extracting and returning the label. Relies on an optimized DSPy classifier for prompt-based inference. The function expects a dictionary input with a 'prompt' key and outputs a classification label. This evaluation function is later passed as the task in experiment runs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\n# Create evaluation function using optimized classifier\ndef test_dspy_prompt(input):\n    result = optimized_classifier(prompt=input[\"prompt\"])\n    return result.label\n```\n\n----------------------------------------\n\nTITLE: Initializing an OpenAI Agent in Python\nDESCRIPTION: This Python snippet creates an instance of the `Agent` class from the `agents` library (part of OpenAI Agents SDK). It names the agent \"Math Solver\", provides instructions on its purpose, and registers the `solve_equation` function tool.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/ragas_agents_cookbook_phoenix.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom agents import Agent\n\nagent = Agent(\n    name=\"Math Solver\",\n    instructions=\"You solve math problems by evaluating them with python and returning the result\",\n    tools=[solve_equation],\n)\n```\n\n----------------------------------------\n\nTITLE: Viewing Document Relevance Evaluation Results\nDESCRIPTION: Displays the head of the document relevance evaluation dataframe to review the assessment of retrieval quality.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nretrieved_documents_eval.head()\n```\n\n----------------------------------------\n\nTITLE: Logging Response Evaluations to Phoenix\nDESCRIPTION: Logs the QA correctness and hallucination evaluation results to Phoenix for visualization and analysis, using the SpanEvaluations class to structure the data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag_haystack.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npx.Client().log_evaluations(\n    SpanEvaluations(dataframe=qa_correctness_eval_df, eval_name=\"Q&A Correctness\"),\n    SpanEvaluations(dataframe=hallucination_eval_df, eval_name=\"Hallucination\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Schema for Image Data (Python)\nDESCRIPTION: This snippet defines a schema for a dataset intended for use with Arize Phoenix, focusing on image data. It specifies 'defective' as the actual label column. It configures an embedding feature named 'image_embedding', linking the 'image_vector' column (the embedding) to the 'image' column (the URL of the image data) using `px.EmbeddingColumnNames` to enable display of the image in the Phoenix app.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nschema = px.Schema(\n    actual_label_column_name=\"defective\",\n    embedding_feature_column_names={\n        \"image_embedding\": px.EmbeddingColumnNames(\n            vector_column_name=\"image_vector\",\n            link_to_data_column_name=\"image\",\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Executing LLM Evaluations with run_evals in Python\nDESCRIPTION: Example showing how to execute evaluations by passing a dataframe and a list of evaluators to the run_evals function. Returns evaluation results including labels, scores, and explanations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evals-reference.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nhallucination_eval_df, qa_correctness_eval_df = run_evals(\n    dataframe=dataframe,\n    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n    provide_explanation=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Serializing Gitbook Documents with LangChain in Python\nDESCRIPTION: Defines load_gitbook_docs() to load documents from a specified Gitbook URL using GitbookLoader. Serializes fetched documents into LangChain format and fetches documentation from the Phoenix Gitbook site. Uses logging for information. Expects docs_url input and outputs a list of LangChainDocument objects.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nFetches the Arize documentation from Gitbook and serializes it into LangChain format.\n\"\"\"\n\ndef load_gitbook_docs(docs_url: str):\n    \"\"\"Loads documents from a Gitbook URL.\n\n    Args:\n        docs_url (str): URL to Gitbook docs.\n\n    Returns:\n        List[LangChainDocument]: List of documents in LangChain format.\n    \"\"\"\n    loader = GitbookLoader(\n        docs_url,\n        load_all_paths=True,\n    )\n    return loader.load()\n\n\nlogging.basicConfig(level=logging.INFO, stream=sys.stdout)\n\n# Fetch documentation\ndocs_url = \"https://docs.arize.com/phoenix\"\nembedding_model_name = \"text-embedding-ada-002\"\ndocs = load_gitbook_docs(docs_url)\n```\n\n----------------------------------------\n\nTITLE: Uploading Code Generation Dataset\nDESCRIPTION: This code uploads the Pandas DataFrame, including questions, example data and chart config, to the Phoenix client. The dataset is used to assess the accuracy of code generation and visualizations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndataset = px_client.upload_dataset(\n    dataframe=code_generation_df,\n    dataset_name=f\"code_generation_questions_{id}\",\n    input_keys=[\"question\", \"example_data\", \"chart_configs\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Running Initial Experiment with Basic Prompt Template in Python\nDESCRIPTION: Executes a first experiment using a simple prompt template for article summarization. It uses OpenAI's GPT-4o model and applies predefined evaluators to measure performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/summarization.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments import run_experiment\n\nexperiment_results = run_experiment(\n    dataset,\n    task,\n    experiment_name=\"initial-template\",\n    experiment_description=\"first experiment using a simple prompt template\",\n    experiment_metadata={\"vendor\": \"openai\", \"model\": gpt_4o},\n    evaluators=EVALUATORS,\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Input/Output from LLM Spans with Phoenix SpanQuery (Python)\nDESCRIPTION: Demonstrates how to filter for spans specifically originating from Large Language Models (`span_kind == 'LLM'`) using `SpanQuery().where()`. It then selects the input and output values, renaming them to `input` and `output` respectively, using `select()`. The query is executed via `px.Client().query_spans()` to return a DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace.dsl import SpanQuery\n\nquery = SpanQuery().where(\n    \"span_kind == 'LLM'\",\n).select(\n    input=\"input.value\",\n    output=\"output.value,\n)\n\n# The Phoenix Client can take this query and return a dataframe.\npx.Client().query_spans(query)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry SDK and Dependencies in Node.js (Shell)\nDESCRIPTION: This shell command installs the core OpenTelemetry API, instrumentation, and export packages necessary for enabling distributed tracing in a Node.js application. It uses npm to bring in modules required to initialize and configure a TracerProvider, including exporters for integration with trace collectors like Phoenix. Modules installed include semantic conventions, tracing SDKs, resource helpers, and OTLP exporter. This command should be run in the project directory before attempting to instrument code.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# npm, pnpm, yarn, etc\nnpm install @opentelemetry/semantic-conventions @opentelemetry/api @opentelemetry/instrumentation @opentelemetry/resources @opentelemetry/sdk-trace-base @opentelemetry/sdk-trace-node @opentelemetry/exporter-trace-otlp-proto\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Models\nDESCRIPTION: Defines the Ollama models to be used for the LLM (mistral:7b) and the embedding model (nomic-embed-text) in the application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nollama_model = \"mistral:7b\"\nollama_embed_model = \"nomic-embed-text\"\n```\n\n----------------------------------------\n\nTITLE: Aggregating, Emphasizing Relevant Answers, and Assembling WikiQA Records in Python\nDESCRIPTION: This snippet iterates over the grouped questions in the DataFrame to consolidate question-answer data. For each unique question, it determines relevant answers by label, concatenates and emphasizes relevant answers in uppercase, and constructs summary records capturing question, document title, all answers, and emphasis. The result is a new DataFrame 'df' suitable for downstream QA evaluation, with key fields like 'query_text', 'document_text_with_emphasis', and 'relevant'. Dependencies: pandas, numpy, WikiQA data structure.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_wiki_qa.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nrows = []\nfor question_id, group in raw_df.groupby([\"question_id\"], as_index=False):\n    questions = group[\"question\"].unique().tolist()\n    assert len(questions) == 1\n    question = questions[0]\n    document_titles = group[\"document_title\"].unique().tolist()\n    assert len(document_titles) == 1\n    document_title = document_titles[0]\n    document_text = \" \".join(group[\"answer\"].tolist())\n    texts_with_emphasis = []\n    for relevance_label, text in zip(group[\"label\"].to_list(), group[\"answer\"].to_list()):\n        is_relevant = relevance_label == 1\n        if is_relevant:\n            texts_with_emphasis.append(text.upper())\n        else:\n            texts_with_emphasis.append(text)\n    document_text_with_emphasis = \" \".join(texts_with_emphasis)\n    is_relevant = 1 in group[\"label\"].to_list()\n    rows.append(\n        {\n            \"query_id\": question_id,\n            \"query_text\": question,\n            \"document_title\": document_title,\n            \"document_text\": document_text,\n            \"document_text_with_emphasis\": document_text_with_emphasis,\n            \"relevant\": is_relevant,\n        }\n    )\ndf = pd.DataFrame(rows)\ndf\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenTelemetry Tracing with Arize Phoenix in Python\nDESCRIPTION: Configures OpenTelemetry tracing to send data to a hosted Arize Phoenix instance. It sets environment variables `PHOENIX_CLIENT_HEADERS` (containing the Phoenix API key) and `PHOENIX_COLLECTOR_ENDPOINT`. It then registers the Phoenix OpenTelemetry tracer provider using `phoenix.otel.register()` and instruments LlamaIndex using `LlamaIndexInstrumentor` to automatically capture traces from LlamaIndex operations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_llamaindex_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# setup Arize Phoenix for logging/observability\nimport os\n\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nfrom phoenix.otel import register\n\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n\n# Configuration is picked up from your environment variables\ntracer_provider = register()\n\n# Instrument LlamaIndex. This allows Phoenix to collect traces from LlamaIndex queries.\nLlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider, skip_dep_check=True)\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix API Key and Endpoint - Python\nDESCRIPTION: Configures environment variables needed to authenticate and communicate with the Phoenix Cloud backend. Sets the Phoenix API key and collector endpoint within the process environment, enabling the SDK to stream telemetry data. Users must replace the placeholder with their actual Phoenix API key.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/mistralai.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App\nDESCRIPTION: Launches the Arize Phoenix application and provides a view of its user interface.  It applies the nest_asyncio library to enable asynchronous operations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npx.launch_app().view()\n```\n\n----------------------------------------\n\nTITLE: Pause Execution to Ensure Data Availability\nDESCRIPTION: Pauses the script momentarily to allow background data processing to complete before analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_application_tracing_evaluating_and_analysis.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom time import sleep\n\nsleep(2)  # wait a little bit for data to become fully available\n```\n\n----------------------------------------\n\nTITLE: Running Evaluation Experiment with GPT-3.5-turbo in Python\nDESCRIPTION: Configures the experiment to use a different language model by setting the `TASK_MODEL` variable to \"gpt-3.5-turbo\". It then calls the `run_experiment` function again with the same `synthetic_dataset`, `task` definition, and evaluators (`no_error`, `has_results`). The `experiment_metadata` is updated to reflect the change in the model being tested.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nTASK_MODEL = \"gpt-3.5-turbo\"\n\nexperiment = run_experiment(\n    synthetic_dataset,\n    task=task,\n    evaluators=[no_error, has_results],\n    experiment_metadata={\"model\": TASK_MODEL},\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key in Python\nDESCRIPTION: Checks if the OpenAI API key is defined in environment variables; if missing, prompts the user to input it securely. The key is then assigned to the OpenAI client and set as an environment variable for downstream uses. This step is mandatory for authenticating OpenAI API requests throughout the notebook.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_sql_retriever_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"\\U0001F511 Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Displaying Phoenix UI URL\nDESCRIPTION: Prints the URL to access the Phoenix UI where the logged evaluations can be viewed and analyzed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nprint(\"The Phoenix UI:\", px.active_session().url)\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Collector Endpoint (Local) - Python\nDESCRIPTION: This code sets the Phoenix collector endpoint environment variable for a local instance. It configures the instrumented application to send trace data to the locally running Phoenix instance.  The endpoint is set to `http://localhost:6006` by default.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/crewai.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Evaluating an Experiment with Additional Evaluators Using Phoenix in Python\nDESCRIPTION: This snippet applies additional evaluators to an existing experiment object after initial execution, enabling iterative evaluation refinement. The `evaluate_experiment` function accepts the experiment and a list of new evaluators to run on the results. This feature supports incremental evaluation without rerunning tasks on the dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/quickstart-datasets.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments import evaluate_experiment\n\nexperiment = evaluate_experiment(experiment, evaluators=[contains_keyword, conciseness])\n```\n\n----------------------------------------\n\nTITLE: Running Custom LLM Classification with Phoenix Evals in Python\nDESCRIPTION: Demonstrates using the `llm_classify` function from Phoenix Evals to apply a custom evaluation template. It requires instantiating an LLM model (e.g., `OpenAIModel`) and passing it along with the target DataFrame (`df`) and the custom template (`MY_CUSTOM_TEMPLATE`). This executes the evaluation defined in the template across the dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/concepts-evals/building-your-own-evals.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model_name=\"gpt-4\",temperature=0.6)\npositive_eval = llm_classify(\n    dataframe=df,\n    template= MY_CUSTOM_TEMPLATE,\n    model=model\n)\n```\n\n----------------------------------------\n\nTITLE: Generate and Log Hallucination & Q&A Evaluation Metrics\nDESCRIPTION: Creates evaluations for hallucinations and answer correctness using OpenAI models, logs the results to Phoenix for monitoring and insights.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_application_tracing_evaluating_and_analysis.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.evals import (\n    HALLUCINATION_PROMPT_RAILS_MAP,\n    HALLUCINATION_PROMPT_TEMPLATE,\n    QA_PROMPT_RAILS_MAP,\n    QA_PROMPT_TEMPLATE,\n    OpenAIModel,\n    llm_classify,\n)\n\nnest_asyncio.apply()  # Speeds up OpenAI API calls\n\n# Creating Hallucination Eval which checks if the application hallucinated\nhallucination_eval = llm_classify(\n    dataframe=queries_df,\n    model=OpenAIModel(model=\"gpt-4\", temperature=0.0),\n    template=HALLUCINATION_PROMPT_TEMPLATE,\n    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n    provide_explanation=True,  # Makes the LLM explain its reasoning\n    concurrency=4,\n)\nhallucination_eval[\"score\"] = (\n    hallucination_eval.label[~hallucination_eval.label.isna()] == \"factual\"\n).astype(int)\n\n# Creating Q&A Eval which checks if the application answered the question correctly\nqa_correctness_eval = llm_classify(\n    dataframe=queries_df,\n    model=OpenAIModel(model=\"gpt-4\", temperature=0.0),\n    template=QA_PROMPT_TEMPLATE,\n    rails=list(QA_PROMPT_RAILS_MAP.values()),\n    provide_explanation=True,  # Makes the LLM explain its reasoning\n    concurrency=4,\n)\n\nqa_correctness_eval[\"score\"] = (\n    hallucination_eval.label[~qa_correctness_eval.label.isna()] == \"correct\"\n).astype(int)\n\n# Logs the Evaluations to Phoenix\npx.Client().log_evaluations(\n    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval),\n    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval),\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Loading LlamaIndex Query Engine\nDESCRIPTION: Sets global LlamaIndex settings to utilize the GPT-4 model for language generation and OpenAI embeddings. Loads the index from storage, and creates a query engine instance that can process questions over the dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/project_sessions_llama_index_query_engine.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nSettings.llm = OpenAI(model=\"gpt-4o-mini\")\nSettings.embed_model = OpenAIEmbedding()\nindex = load_index_from_storage(storage_context)\nquery_engine = index.as_query_engine()\n```\n\n----------------------------------------\n\nTITLE: Evaluation Function for Few-Shot CoT Prompt\nDESCRIPTION: Defines a function that sends the math problem to the OpenAI API using the few-shot prompt, extracts the final answer, and evaluates if it matches the expected solution. Configures and runs an experiment with the Phoenix framework.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/chain_of_thought_prompting.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef few_shot_COT_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **few_shot_COT.format(variables={\"Problem\": input[\"Word Problem\"]})\n    )\n    response_text = resp.choices[0].message.content.strip()\n    lines = response_text.split(\"\\n\")\n    final_answer = lines[-1].strip()\n    final_answer = re.sub(r\"^\\*\\*(\\d+)\\*\\*$\", r\"\\1\", final_answer)\n    return {\"full_response\": response_text, \"final_answer\": final_answer}\n\n\ndef evaluate_response(output, expected):\n    final_answer = output[\"final_answer\"]\n    if not final_answer.isdigit():\n        return False\n    return int(final_answer) == int(expected[\"Answer\"])\n\n\nimport nest_asyncio\n\nfrom phoenix.experiments import run_experiment\n\nnest_asyncio.apply()\n\ninitial_experiment = run_experiment(\n    dataset,\n    task=few_shot_COT_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Few-Shot COT Prompt\",\n    experiment_name=\"few-shot-cot-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + few_shot_COT.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Saving DSPy Prompt in Phoenix\nDESCRIPTION: This code saves the prompt associated with the DSPy-optimized classifier into Phoenix, using the PhoenixClient. The prompt from the optimized classifier is extracted, and configured to send it to an OpenAI model. The output is then saved to Phoenix. This allows the user to store the prompt used by the optimized classifier for reference and later analysis. The metadata keeps track of the details.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": optimized_classifier.signature.instructions,\n        },  # if your meta prompt includes few shot examples, make sure to include them here\n        {\"role\": \"user\", \"content\": \"{{prompt}}\"},\n    ],\n)\n\ndspy_prompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"DSPy prompt result\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Preparing Summarization Datasets\nDESCRIPTION: Downloads a parquet file containing article-summary pairs, splits it into baseline and recent datasets, and resets indices for compatibility with the embedding generator.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_summarization_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/summarization/llm_summarization.parquet\"\n)\nbaseline_df = df[:300]\nrecent_df = df[300:]\nbaseline_df = baseline_df.reset_index(\n    drop=True\n)  # recommended when using EmbeddingGenerator.generate_embeddings\nrecent_df = recent_df.reset_index(\n    drop=True\n)  # recommended when using EmbeddingGenerator.generate_embeddings\nbaseline_df.head()\n```\n\n----------------------------------------\n\nTITLE: Parsing and Validating Raw Travel Attributes Using JSON Schema in Python\nDESCRIPTION: Defines a TypedDict for travel attributes and a parsing function that converts raw JSON strings into structured Python dictionaries, validating against the JSON Schema. The subsequent loop attempts to parse and validate each extracted raw attribute string, capturing JSON parsing and schema validation outcomes, and stores them in a list of records including metadata flags. This requires the json, jsonschema, pandas, and typing libraries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_tracing_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass TravelAttributes(TypedDict):\n    location: str\n    budget_level: Literal[\"low\", \"medium\", \"high\", \"not_stated\"]\n    purpose: Literal[\"business\", \"pleasure\", \"other\", \"non_stated\"]\n\n\ndef parse_travel_attributes(\n    travel_attributes_string: str, schema: Dict[str, Any]\n) -> TravelAttributes:\n    \"\"\"\n    Parses a raw travel attributes string into a structured dictionary of travel attributes.\n\n    Parameters:\n    - travel_attributes_string (str): A raw travel attributes string.\n    - schema (Dict[str, Any]): A JSON Schema to validate.\n\n    Returns:\n    - TravelAttributes: Parsed travel attributes.\n    \"\"\"\n    travel_attributes = json.loads(travel_attributes_string)\n    jsonschema.validate(instance=travel_attributes, schema=schema)\n    return travel_attributes\n\n\nrecords = []\nfor travel_request, raw_travel_attributes in zip(travel_requests, raw_travel_attributes_column):\n    is_json_parseable = True\n    conforms_to_json_schema = True\n    travel_attributes = None\n    try:\n        travel_attributes = parse_travel_attributes(raw_travel_attributes, parameters_schema)\n    except json.JSONDecodeError:\n        is_json_parseable = False\n        conforms_to_json_schema = False\n    except jsonschema.ValidationError:\n        conforms_to_json_schema = False\n    records.append(\n        {\n            \"travel_request\": travel_request,\n            \"raw_travel_attributes\": raw_travel_attributes,\n            \"is_json_parseable\": is_json_parseable,\n            \"conforms_to_json_schema\": conforms_to_json_schema,\n            **(travel_attributes or {}),\n        }\n    )\n\noutput_df = pd.DataFrame(records)\noutput_df\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Tracer - Python\nDESCRIPTION: This code configures the Phoenix tracer, registering it for tracing. It sets the project name and enables auto-instrumentation based on installed OpenInference dependencies, automatically tracing calls to LLMs and other dependencies within the application. This code is necessary for integrating tracing into the application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/crewai.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Sets up the OpenAI API key by either retrieving it from environment variables or prompting the user to enter it interactively, then storing it as an environment variable.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag_haystack.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\nimport os\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Phoenix EmbeddingColumnNames class definition in Python\nDESCRIPTION: The EmbeddingColumnNames dataclass associates dataframe columns with embedding features. It maps vector data with optional raw text or external resource links for embedding-related analysis in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/inference-and-schema.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass EmbeddingColumnNames(\n    vector_column_name: str,\n    raw_data_column_name: Optional[str] = None,\n    link_to_data_column_name: Optional[str] = None,\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Refiner evaluation\nDESCRIPTION: This code snippet executes the Refiner evaluation on the prepared chunks and prints the resulting summary evaluation to the console.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/long_context_summary_evals.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsummary_evaluation = evaluator.evaluate(chunks)\nprint(summary_evaluation)\n```\n\n----------------------------------------\n\nTITLE: Defining a Signature for Question Answering\nDESCRIPTION: Creates a DSPy Signature class that specifies input fields and output for question-answering tasks, describing the expected data flow and model's purpose.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/dspy_tracing_tutorial.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nclass GenerateAnswer(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n\n    context = dspy.InputField(desc=\"may contain relevant facts\")\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables for Phoenix and OpenAI\nDESCRIPTION: Configures environment variables for connecting to Phoenix Cloud and stores API keys for both Phoenix and OpenAI services.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/react-prompting.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\nif not os.environ.get(\"PHOENIX_CLIENT_HEADERS\"):\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = \"api_key=\" + getpass(\"Enter your Phoenix API key: \")\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Tool Function - Python\nDESCRIPTION: Defines a Python function `solve_equation` decorated with `@function_tool` to be used by the agent. It takes a mathematical equation as a string and uses Python's `eval()` to compute the result, returning it as a string. Requires dependencies from the `agents` library.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/integrations/ragas.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom agents import Runner, function_tool\n\n@function_tool\ndef solve_equation(equation: str) -> str:\n    \"\"\"Use python to evaluate the math equation, instead of thinking about it yourself.\n\n    Args:\"\n       equation: string which to pass into eval() in python\n    \"\"\"\n    return str(eval(equation))\n```\n\n----------------------------------------\n\nTITLE: Defining a Validation Metric\nDESCRIPTION: This code defines `validate_classification` which computes the accuracy of the classifier. It compares the predicted label to the expected label, returning True if they match. The metric is used for determining the performance of the DSPy-optimized classifier. This function will be used in optimization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef validate_classification(example, prediction, trace=None):\n    return example[\"label\"] == prediction[\"label\"]\n```\n\n----------------------------------------\n\nTITLE: Loading NBA Dataset and Registering with DuckDB in Python\nDESCRIPTION: Downloads a public NBA dataset using the datasets library, registers it as an in-memory table in DuckDB, and queries the first row as a Python dictionary. Dependencies are duckdb and datasets. Key parameters include the dataset name ('suzyanil/nba-data'), the table name ('nba'), and conversion methods to Pandas DataFrame and dictionary. Input: none. Output: a dictionary of the first row's data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\nfrom datasets import load_dataset\n\ndata = load_dataset(\"suzyanil/nba-data\")[\"train\"]\n\nconn = duckdb.connect(database=\":memory:\", read_only=False)\nconn.register(\"nba\", data.to_pandas())\n\nconn.query(\"SELECT * FROM nba LIMIT 5\").to_df().to_dict(orient=\"records\")[0]\n```\n\n----------------------------------------\n\nTITLE: Applying Span Attributes with `using_attributes` Decorator in Python\nDESCRIPTION: Illustrates how to use the `using_attributes` helper as a Python decorator to apply a standard set of attributes (session ID, user ID, metadata, tags, prompt details) to all spans generated by calls within the decorated function (`call_fn`). This approach is useful for consistently applying attributes to specific functions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@using_attributes(\n    session_id=\"my-session-id\",\n    user_id=\"my-user-id\",\n    metadata=metadata,\n    tags=tags,\n    prompt_template=prompt_template,\n    prompt_template_version=prompt_template_version,\n    prompt_template_variables=prompt_template_variables,\n)\ndef call_fn(*args, **kwargs):\n    # Calls within this function will generate spans with the attributes:\n    # \"session.id\" = \"my-session-id\"\n    # \"user.id\" = \"my-user-id\"\n    # \"metadata\" = \"{\\\"key-1\\\": value_1, \\\"key-2\\\": value_2, ... }\" # JSON serialized\n    # \"tag.tags\" = \"[\\\"tag_1\\\",\\\"tag_2\\\",...]\"\n    # \"llm.prompt_template.template\" = \"Please describe the weather forecast for {city} on {date}\"\n    # \"llm.prompt_template.variables\" = \"{\\\"city\\\": \\\"Johannesburg\\\", \\\"date\\\": \\\"July 11\\\"}\" # JSON serialized\n    # \"llm.prompt_template.version \" = \"v1.0\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Launching the Phoenix Application UI (Python)\nDESCRIPTION: Starts the Phoenix interactive web application using `px.launch_app`. It specifies the production dataset (`prod_ds`) as the primary data to analyze and the training dataset (`train_ds`) as a reference for comparison. The session object is stored in the `session` variable, and `.view()` displays the URL to access the UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/image_classification_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n(session := px.launch_app(primary=prod_ds, reference=train_ds)).view()\n```\n\n----------------------------------------\n\nTITLE: Logging Document Evaluations to Phoenix\nDESCRIPTION: This code logs the `retrieved_documents_eval` DataFrame to the Phoenix server using `log_evaluations` method and the `DocumentEvaluations` object. This enables the visualization of retrieval metrics, such as precision, ndcg, and hit, in the Phoenix UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import DocumentEvaluations\n\npx.Client().log_evaluations(\n    DocumentEvaluations(eval_name=\"Relevance\", dataframe=retrieved_documents_eval)\n)\n```\n\n----------------------------------------\n\nTITLE: Running RAG Relevance Evaluation with Phoenix and OpenAIModel in Python\nDESCRIPTION: This Python snippet demonstrates how to evaluate retrieval relevance using the Phoenix evaluation package and an OpenAI language model. It sets up the model, retrieves predetermined output rails, and applies the evaluation prompt to a pandas DataFrame containing retrieval pairs, handling output canonicalization and supporting optional explanations. Prerequisites include installing the phoenix package, setting up OpenAI key access, and having a valid DataFrame (df) with the required columns. Inputs are the DataFrame and optionally an explanation flag; output is a set of binary relevance classifications (and explanations if requested) for each row.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/retrieval-rag-relevance.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.evals import (\n    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n    RAG_RELEVANCY_PROMPT_TEMPLATE,\n    OpenAIModel,\n    download_benchmark_dataset,\n    llm_classify,\n)\n\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\n\n#The rails is used to hold the output to specific values based on the template\n#It will remove text such as \",,,\" or \"...\"\n#Will ensure the binary value expected from the template is returned\nrails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\nrelevance_classifications = llm_classify(\n    dataframe=df,\n    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    provide_explanation=True, #optional to generate explanations for the value produced by the eval LLM\n)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Data Structure\nDESCRIPTION: This code snippet allows the user to take a closer look at the data structure of one example within the dataset by accessing the first element of the dataset using indexing. This provides insight into how the data is organized.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset[0]\n```\n\n----------------------------------------\n\nTITLE: Defining QA Template for LLM Evaluation\nDESCRIPTION: This snippet defines a template used for evaluating the correctness of answers generated by the LLM. The template provides the LLM with a question and an answer, and instructs it to determine whether the answer correctly and fully answers the question. The LLM is expected to respond with either \"correct\" or \"incorrect\".\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nqa_template = \"\"\"You are given a question and an answer. You must determine whether the\\ngiven answer correctly answers the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: {Question}\\n    ************\\n    [Answer]: {Answer}\\n    [END DATA]\\nYour response must be a single word, either \\\"correct\\\" or \\\"incorrect\\\",\\nand should not contain any text or characters aside from that word.\\n\\\"correct\\\" means that the question is correctly and fully answered by the answer.\\n\\\"incorrect\\\" means that the question is not correctly or only partially answered by the\\nanswer.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Loading Query DataFrame from Remote Parquet Dataset (Python)\nDESCRIPTION: Loads user query, retrieval, and feedback data into a Pandas DataFrame from a remote Parquet file. The data includes queries, embeddings, response texts, retrieved contexts, similarity scores, and user feedback labels. Requires internet access and the pandas library. Used for subsequent evaluation and tracing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/qdrant_langchain_instrumentation_search_and_retrieval_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nquery_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/langchain/langchain_query_dataframe_with_user_feedbackv2.parquet\"\n)\n```\n\n----------------------------------------\n\nTITLE: Exporting Trace Data into DataFrames for Evaluation\nDESCRIPTION: This code exports trace data from the Phoenix client for analysis by creating two pandas DataFrames: one concatenates retrieved documents per query, and the other explodes each document into individual rows for detailed assessment. Dependencies include pandas and Phoenix SDK functions, enabling detailed evaluation of retrieval quality and response correctness.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_with_mongodb_and_llama_index.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nqueries_df = get_qa_with_reference(session)\\nretrieved_documents_df = get_retrieved_documents(session)\n```\n\n----------------------------------------\n\nTITLE: Generating Chat Completion with MistralAI SDK - Python\nDESCRIPTION: Demonstrates using the MistralAI SDK to perform a chat completion call. Handles API key retrieval, initializes the client, sends a user message prompt, and prints the resulting response from the selected model. Requires the mistralai package and a valid MISTRAL_API_KEY environment variable. The example shows how LLM calls are intercepted for telemetry when instrumentation is enabled.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/mistralai.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom mistralai import Mistral\nfrom mistralai.models import UserMessage\n\napi_key = os.environ[\"MISTRAL_API_KEY\"]\nmodel = \"mistral-tiny\"\n\nclient = Mistral(api_key=api_key)\n\nchat_response = client.chat.complete(\n    model=model,\n    messages=[UserMessage(content=\"What is the best French cheese?\")],\n)\nprint(chat_response.choices[0].message.content)\n\n```\n\n----------------------------------------\n\nTITLE: Using the DSPy Classifier to Make Predictions\nDESCRIPTION: This snippet demonstrates how to use the DSPy classifier to make predictions.  It passes a prompt from the dataset to the classifier, which outputs a label prediction.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nclassifier(prompt=ds.iloc[0].prompt)\n```\n\n----------------------------------------\n\nTITLE: Previewing Bias Classification Results in Python\nDESCRIPTION: This snippet simply calls the `head()` method on the `bias_classifications` DataFrame to display the first few rows of classification results, allowing for a quick inspection of the labels, explanations, and scores generated by the LLM classification process.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/llamaindex-workflows-research-agent/evaluate_traces.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nbias_classifications.head()\n```\n\n----------------------------------------\n\nTITLE: Interacting with the agent using a calculation query\nDESCRIPTION: Sends a math question to the agent and prints the response. Illustrates how the agent interprets the input, possibly invokes tools, and produces a reply in Shakespearean style based on the configured prompt.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_openai_agent_tracing_tutorial.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nresponse = agent.query(\"What is (121 * 3) + 42?\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Combining Retrieved Documents with Relevance Evaluations in Python\nDESCRIPTION: Concatenates the retrieved documents DataFrame with the relevance evaluation DataFrame along the columns, adding prefixes to evaluation columns for clarity. The combined DataFrame contains retrieval, input, and evaluation metrics in one unified structure to facilitate advanced analyses and metric calculations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndocuments_with_relevance_df = pd.concat(\n    [retrieved_documents_df, retrieved_documents_relevance_df.add_prefix(\"eval_\")], axis=1\n)\ndocuments_with_relevance_df\n```\n\n----------------------------------------\n\nTITLE: Pulling Latest Phoenix Docker Image - Bash\nDESCRIPTION: Fetches the most recent Phoenix server image from Docker Hub. Ensures that the containerized observability server is available locally for deployment and integration with the tracing SDK.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/mistralai.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix with Primary and Reference Datasets in Python\nDESCRIPTION: Launches the Phoenix UI session by calling `px.launch_app`, providing both a primary dataset (`prod_ds`) for active evaluation and a reference dataset (`train_ds`) as a baseline for comparison. Requires the Phoenix library (`px`) and pre-defined datasets representing production and training inferences. Returns a session object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/phoenix-inferences.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsession = px.launch_app(primary=prod_ds, reference=train_ds)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Observability Application for Tracing in Python\nDESCRIPTION: Starts the Phoenix tracing application in the background using the `px.launch_app()` function. This enables the collection and visualization of OpenInference trace data emitted by instrumented LLM applications. The returned `session` object provides the URL to access the Phoenix UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_vertex_ai_tracing_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsession = px.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Saving the Optimized DSPy Prompt to Phoenix\nDESCRIPTION: This snippet saves the optimized DSPy prompt (specifically the instructions) into Arize Phoenix.  The Phoenix prompt object is created with the signature instructions from the optimized classifier and saved using the OpenAI prompt format.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": optimized_classifier.signature.instructions,\n        },  # if your meta prompt includes few shot examples, make sure to include them here\n        {\"role\": \"user\", \"content\": \"{{prompt}}\"},\n    ],\n)\n\ndspy_prompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"DSPy prompt result\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a ConditionalSpanProcessor for OpenTelemetry in Python\nDESCRIPTION: Defines a custom `ConditionalSpanProcessor` class inheriting from `SpanProcessor`. It takes an `exporter` and a `condition` function during initialization. The `on_end` method checks if the span satisfies the `condition` before exporting it via the provided `exporter`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/span_filtering_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass ConditionalSpanProcessor(SpanProcessor):\n    def __init__(self, exporter: SpanExporter, condition: callable):\n        self.exporter = exporter\n        self.condition = condition\n\n    def on_start(self, span, parent_context):\n        pass\n\n    def on_end(self, span):\n        # Only export spans that meet the condition\n        if self.condition(span):\n            self.exporter.export([span])\n\n    def shutdown(self):\n        self.exporter.shutdown()\n\n    def force_flush(self, timeout_millis=None):\n        self.exporter.force_flush(timeout_millis)\n```\n\n----------------------------------------\n\nTITLE: Calculating Gradient and Identifying Prompts\nDESCRIPTION: The code identifies successful and failed examples from a dataset, and converts them to lists of prompts by extracting the `prompt` field of the `input` object. The `calculate_prompt_gradient` function is then called, using the initial five entries from each list, to compute the gradient. This is a critical step for determining the direction in which the prompt needs to be modified to improve its performance based on success or failure of prior prompts.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Get successful and failed examples from our dataset\nsuccessful_examples =\n    ground_truth_df[ground_truth_df[\"output\"] == ground_truth_df[\"expected\"].get(\"type\")][\"input\"]\n    .apply(lambda x: x[\"prompt\"])\n    .tolist()\nfailed_examples =\n    ground_truth_df[ground_truth_df[\"output\"] != ground_truth_df[\"expected\"].get(\"type\")][\"input\"]\n    .apply(lambda x: x[\"prompt\"])\n    .tolist()\n\n# Calculate the gradient direction\ngradient = calculate_prompt_gradient(successful_examples[:5], failed_examples[:5])\n```\n\n----------------------------------------\n\nTITLE: Saving the Modified Trace Dataset\nDESCRIPTION: Saves the trace dataset ('td'), which now contains the modified timestamps. The 'save()' method likely persists the dataset, potentially to a local file or a configured storage location associated with the Phoenix project.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/vision.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntd.save()\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI API Key with Environment Variable or Interactive Prompt in Python\nDESCRIPTION: Checks if the OpenAI API key is set as an environment variable; if absent, prompts the user to input it securely using getpass. Sets the OpenAI API key for usage in subsequent API calls and updates the environment variable accordingly. Ensures secure handling and availability of credentials for programmatic access to OpenAI services.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n\topenai_api_key = getpass(\"\\ud83d\\udd11 Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Viewing Phoenix Dataset as DataFrame in Python\nDESCRIPTION: Calls the `as_dataframe()` method on the Phoenix `dataset` object created in the previous step. This retrieves the dataset stored in Phoenix and converts it into a pandas DataFrame for easy inspection and manipulation within the Python environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset.as_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Conditional Workflow Control Function\nDESCRIPTION: Defines logic to determine whether the workflow should proceed to generate, re-generate, or terminate based on the latest message, examining for tool calls or error messages.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ndef should_continue(state: State) -> Literal[END, \"correct_query\", \"query_gen\"]:\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # If there is a tool call, then we finish\n    if getattr(last_message, \"tool_calls\", None):\n        return END\n    if last_message.content.startswith(\"Error:\"):\n        return \"query_gen\"\n    else:\n        return \"correct_query\"\n```\n\n----------------------------------------\n\nTITLE: Creating sample dataset for summarization evaluation\nDESCRIPTION: Samples a subset of the benchmark dataset based on the defined evaluation size. The columns are renamed to match the expected input format for the evaluation function.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf_sample = (\n    df.sample(n=N_EVAL_SAMPLE_SIZE)\n    .reset_index(drop=True)\n    .rename(columns={\"document\": \"input\", \"summary\": \"output\"})\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Endpoint with Environment Variables in Python (Phoenix v3.0.0+)\nDESCRIPTION: This modern approach to endpoint configuration uses environment variables (PHOENIX_PORT) to set the collector endpoint without any explicit exporter or tracer objects. Prerequisites: os (for setting environment variable) and phoenix.trace.langchain. Input: sets PHOENIX_PORT to '12345'. Output: LangChainInstrumentor uses the specified port for the exporter configuration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/MIGRATION.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.trace.langchain import LangChainInstrumentor\n\nos.environ[\"PHOENIX_PORT\"] = \"12345\"\nLangChainInstrumentor().instrument()\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key (Python)\nDESCRIPTION: Retrieves the OpenAI API key from environment variables or prompts the user via `getpass` if not found. Sets the key as an environment variable `OPENAI_API_KEY`, required for interacting with OpenAI models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Excluding Columns from Feature Inference in Phoenix Schema\nDESCRIPTION: This Python code demonstrates how to exclude specific columns from Phoenix's implicit feature inference by adding them to the excluded_column_names field of the schema. In this example, hospital and insurance_provider fields are excluded from analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nschema = px.Schema(\n    prediction_label_column_name=\"predicted\",\n    actual_label_column_name=\"target\",\n    excluded_column_names=[\n        \"hospital\",\n        \"insurance_provider\",\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Building and Running Langflow Docker Containers Using Docker Compose\nDESCRIPTION: Commands for managing Langflow development containers using Docker Compose. The first command safely stops and removes any existing containers defined in the specified development compose YAML file. The second command recreates and starts the containers, ensuring updated images without orphaned containers remain. This container orchestration is essential to run the Langflow app locally, enabling developers to interact with the UI and test flows integrated with Arize Phoenix telemetry.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langflow.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose -f docker/dev.docker-compose.yml down || true \ndocker compose -f docker/dev.docker-compose.yml up --remove-orphans\n```\n\n----------------------------------------\n\nTITLE: Building a Query Engine from Vector Index (Python)\nDESCRIPTION: Transforms the constructed vector index into a query engine compatible with the LlamaIndex querying interface. This engine will be used for efficiently retrieving relevant content in response to user queries. Requires an existing VectorStoreIndex object; returns a queryable engine instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nquery_engine = vector_index.as_query_engine()\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix and Instrumenting DSPy with OpenInference\nDESCRIPTION: Starts the Phoenix app for tracing, and sets up OpenInference instrumentation on DSPy and LLMs via OpenTelemetry to monitor and record execution spans and telemetry data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/dspy_tracing_tutorial.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nphoenix_session = px.launch_app()\n\nfrom openinference.instrumentation.dspy import DSPyInstrumentor\nfrom openinference.instrumentation.litellm import LiteLLMInstrumentor\n\nfrom phoenix.otel import register\n\nregister(endpoint=\"http://127.0.0.1:6006/v1/traces\")\nDSPyInstrumentor().instrument(skip_dep_check=True)\nLiteLLMInstrumentor().instrument(skip_dep_check=True)\n```\n\n----------------------------------------\n\nTITLE: Installing necessary packages for Phoenix Evals and LlamaIndex integration\nDESCRIPTION: Installs Python packages required for building a RAG system, including Phoenix Evals, LlamaIndex, OpenAI SDK, and other dependencies. Ensures that all necessary libraries are available for subsequent steps in data loading, indexing, and evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n!pip install -qq \"arize-phoenix[evals,llama-index]\" \"llama-index-llms-openai\" \"openai>=1\" gcsfs nest_asyncio 'httpx<0.28' \n\n```\n\n----------------------------------------\n\nTITLE: Installing and Launching Phoenix (Python)\nDESCRIPTION: Installs the main `arize-phoenix` package and launches the Phoenix application, typically starting a local server to view collected traces and prompt data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uqqq arize-phoenix\nimport phoenix as px\n\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Questions to Extract SQL Queries and Responses Using Regex in Python\nDESCRIPTION: Processes a list of questions through the agent pipeline sequentially. For each question, creates a task and runs a step to produce an agent response string. Uses regular expressions to extract the generated SQL query and agent response from that string. Stores and prints extracted queries and responses for later analysis. It depends on tqdm for progress, re for regex, and the previously defined agent pipeline and worker classes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nall_sql_queries = []\nall_ans = []\nfor question in tqdm(questions):\n    agent_worker = QueryPipelineAgentWorker(qp)\n    agent = AgentRunner(agent_worker)\n    task = agent.create_task(question)\n    # Need to manually run the task as to recover the convo_history\n    step_output = agent.run_step(task.task_id)\n    ans = str(step_output)\n    sql_query_match = re.search(r\"\\'sql_query\\': \\'([^\\']+)\\'\", ans)\n\n    if not sql_query_match:\n        print(ans)\n    # Extract the sql_query if the pattern is found\n    sql_query = sql_query_match.group(1) if sql_query_match else None\n\n    # Regular expression to extract the response\n    response_match = re.search(r\"response=\\'([^\\']+)\\'\", ans)\n\n    # Extract the response if the pattern is found\n    response = response_match.group(1) if response_match else None\n\n    print(\"SQL Query:\", sql_query)\n    print(\"Response:\", response)\n    all_ans.append(response)\n    all_sql_queries.append(sql_query)\n```\n\n----------------------------------------\n\nTITLE: Running Evals with exit_on_error=False (Python)\nDESCRIPTION: Executes the `llm_classify` function again, but this time with `exit_on_error=False` and `max_retries=2`. This configuration overrides the default behavior, preventing the evaluation run from stopping early on errors. Instead, it attempts retries (up to 2) and marks rows as failed or skipped if errors persist or inputs are missing, ensuring the run completes for all possible rows.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nrails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\nall_evals = llm_classify(\n    dataframe=df_sample,\n    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n    model=funny_model,\n    rails=rails,\n    concurrency=3,\n    max_retries=2,\n    exit_on_error=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating Milvus Vector Store on Zilliz Cloud (Python)\nDESCRIPTION: Prompts the user for their Zilliz Cloud public endpoint and token. Connects to Zilliz Cloud and instantiates a `MilvusVectorStore` using the provided credentials and configuration details (collection name, dimension, fields). It then iterates through the downloaded documentation data (`rows`), converts each entry into a LlamaIndex `TextNode` including embeddings and source relationships, and adds these nodes to the Milvus collection, overwriting if necessary.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/milvus_llamaindex_search_and_retrieval_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nENDPOINT = getpass(prompt=\"Please set your public endpoint from Zilliz Cloud: \")\nTOKEN = getpass(prompt=\"Enter your token from Zilliz Cloud: \")\n```\n\nLANGUAGE: python\nCODE:\n```\n# Connect to Zilliz Cloud and instantiate a Milvus Vector Store\nvector_store = MilvusVectorStore(\n    uri=ENDPOINT,\n    token=TOKEN,\n    collection_name=\"colab_collection\",\n    dim=1536,\n    embedding_field=\"embedding\",\n    doc_id_field=\"doc_id\",\n    overwrite=True,\n)\n\n# Insert the downloaded Arize documentation data to the vector store\nnodes = []\nfor row in rows:\n    node = TextNode(\n        embedding=row[\"embedding\"],\n        text=row[\"text\"],\n        id_=row[\"id\"],\n        relationships={NodeRelationship.SOURCE: RelatedNodeInfo(node_id=row[\"doc_id\"])},\n    )\n    nodes.append(node)\n\nvector_store.add(nodes)\nprint(\"Successfully added Arize documentation data into the vector store!\")\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Local Collector Endpoint - Python\nDESCRIPTION: Configures the Python environment to send trace data to a Phoenix instance running locally on port 6006. Updates the 'PHOENIX_COLLECTOR_ENDPOINT' environment variable, ensuring instrumentation libraries route their telemetry to the local Phoenix server.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for OpenAI and Phoenix (Python)\nDESCRIPTION: This code snippet installs the necessary Python packages, including OpenAI, arize-phoenix, jsonschema, openinference-instrumentation-openai, openinference-instrumentation, opentelemetry-api, opentelemetry-sdk, and openinference-semantic-conventions. These packages are essential for instrumenting and tracing OpenAI applications with Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_sessions_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"openai>=1.0.0\" arize-phoenix jsonschema openinference-instrumentation-openai openinference-instrumentation opentelemetry-api opentelemetry-sdk openinference-semantic-conventions\n```\n\n----------------------------------------\n\nTITLE: Listing Pinned Python Dependencies (requirements.txt format)\nDESCRIPTION: Specifies the exact versions of Python packages required for the project, generated by `uv pip compile pyproject.toml`. It follows the standard requirements file format, listing each package and its version (e.g., `package-name==version`). Comments indicate which other packages necessitate this dependency (transitive dependencies).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/manually-instrumented-chatbot/chat-service/requirements.txt#_snippet_0\n\nLANGUAGE: requirements.txt\nCODE:\n```\n# This file was autogenerated by uv via the following command:\n#    uv pip compile pyproject.toml\nannotated-types==0.6.0\n    # via pydantic\nanyio==4.3.0\n    # via\n    #   httpx\n    #   starlette\ncertifi==2024.7.4\n    # via\n    #   httpcore\n    #   httpx\n    #   requests\ncharset-normalizer==3.3.2\n    # via requests\nclick==8.1.7\n    # via uvicorn\ndeprecated==1.2.14\n    # via\n    #   opentelemetry-api\n    #   opentelemetry-exporter-otlp-proto-grpc\n    #   opentelemetry-exporter-otlp-proto-http\nfastapi==0.110.1\ngoogleapis-common-protos==1.63.0\n    # via\n    #   opentelemetry-exporter-otlp-proto-grpc\n    #   opentelemetry-exporter-otlp-proto-http\ngrpcio==1.62.1\n    # via opentelemetry-exporter-otlp-proto-grpc\nh11==0.14.0\n    # via\n    #   httpcore\n    #   uvicorn\nhttpcore==1.0.5\n    # via httpx\nhttpx==0.27.0\nidna==3.6\n    # via\n    #   anyio\n    #   httpx\n    #   requests\nimportlib-metadata==7.0.0\n    # via opentelemetry-api\nopeninference-semantic-conventions==0.1.5\nopentelemetry-api==1.24.0\n    # via\n    #   opentelemetry-exporter-otlp-proto-grpc\n    #   opentelemetry-exporter-otlp-proto-http\n    #   opentelemetry-sdk\nopentelemetry-exporter-otlp==1.24.0\nopentelemetry-exporter-otlp-proto-common==1.24.0\n    # via\n    #   opentelemetry-exporter-otlp-proto-grpc\n    #   opentelemetry-exporter-otlp-proto-http\nopentelemetry-exporter-otlp-proto-grpc==1.24.0\n    # via opentelemetry-exporter-otlp\nopentelemetry-exporter-otlp-proto-http==1.24.0\n    # via opentelemetry-exporter-otlp\nopentelemetry-proto==1.24.0\n    # via\n    #   opentelemetry-exporter-otlp-proto-common\n    #   opentelemetry-exporter-otlp-proto-grpc\n    #   opentelemetry-exporter-otlp-proto-http\nopentelemetry-sdk==1.24.0\n    # via\n    #   opentelemetry-exporter-otlp-proto-grpc\n    #   opentelemetry-exporter-otlp-proto-http\nopentelemetry-semantic-conventions==0.45b0\n    # via opentelemetry-sdk\nprotobuf==4.25.3\n    # via\n    #   googleapis-common-protos\n    #   opentelemetry-proto\npydantic==2.6.4\n    # via fastapi\npydantic-core==2.16.3\n    # via pydantic\nrequests==2.31.0\n    # via opentelemetry-exporter-otlp-proto-http\nsniffio==1.3.1\n    # via\n    #   anyio\n    #   httpx\nstarlette==0.40.0\n    # via fastapi\ntyping-extensions==4.11.0\n    # via\n    #   fastapi\n    #   opentelemetry-sdk\n    #   pydantic\n    #   pydantic-core\nurllib3==2.2.1\n    # via requests\nuvicorn==0.29.0\nwrapt==1.16.0\n    # via deprecated\nzipp==3.19.1\n    # via importlib-metadata\n```\n\n----------------------------------------\n\nTITLE: Initializing GPT-4 Model for Readability Classification\nDESCRIPTION: Creates an instance of the OpenAIModel configured to use GPT-4 with deterministic output (temperature=0.0). This model is employed for classifying code snippets' readability.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nmodel = OpenAIModel(\n    model=\"gpt-4\",\n    temperature=0.0,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Human vs AI Dataset - Python\nDESCRIPTION: Downloads and reads a CSV dataset containing Q&A records for evaluation. Loads the CSV file from a cloud URL into a DataFrame, drops rows with missing 'correct_answer', and resets the index. Expects 'correct_answer' column to exist and data to be CSV-formatted.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncsv_file_path = \"https://storage.googleapis.com/arize-phoenix-assets/evals/human_vs_ai/human_vs_ai_classifications.csv\"\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv(csv_file_path).dropna(subset=[\"correct_answer\"]).reset_index(drop=True)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for OpenAI and OpenTelemetry Tracing in Python\nDESCRIPTION: Imports essential Python modules and packages necessary for working with OpenAI API, telemetry tracing with OpenTelemetry SDK and API, OpenInference semantic conventions and instrumentation, and data handling with pandas. These imports prepare the environment for instrumentation and telemetry export configuration, including OTLPSpanExporter for exporting traces, and the OpenAIInstrumentor for OpenAI API call instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nfrom getpass import getpass\nfrom io import StringIO\n\nimport openai\nimport opentelemetry\nimport pandas as pd\nfrom openai import OpenAI\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nfrom openinference.semconv.trace import OpenInferenceSpanKindValues, SpanAttributes\nfrom opentelemetry import trace as trace_api\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk import trace as trace_sdk\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\n```\n\n----------------------------------------\n\nTITLE: Running Agent on Math Problems with asyncio\nDESCRIPTION: This code snippet iterates through a DataFrame of math problems and uses an agent (defined by `solve_math_problem`) to solve each problem asynchronously. It collects the conversation history for each problem and stores it in a list called `conversations`. Dependencies: asyncio and `agents.Runner` must be available.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/ragas_agents_cookbook_phoenix.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom agents import Runner\n\n# Run agent on all problems and collect conversation history\nconversations = []\nfor idx, row in math_problems_df.iterrows():\n    result = asyncio.run(solve_math_problem(row[\"question\"]))\n    conversations.append(\n        {\n            \"question\": row[\"question\"],\n            \"final_output\": result[\"final_output\"],\n            \"messages\": result[\"messages\"],\n        }\n    )\n\nprint(f\"Processed {len(conversations)} problems\")\nprint(conversations[0])\n```\n\n----------------------------------------\n\nTITLE: Viewing Single Dataset Example (Python)\nDESCRIPTION: Accesses the first example in the uploaded dataset object. This allows inspecting the structure of individual data points (input, output, etc.) within the dataset object managed by Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndataset[0]\n```\n\n----------------------------------------\n\nTITLE: Saving Meta Prompt in Phoenix\nDESCRIPTION: This code saves the generated meta prompt into Phoenix using the PhoenixClient. The code checks if the prompt contains placeholders (`{examples}`). Then it formats the prompt with few-shot examples if necessary. Finally it defines the parameters for OpenAI to generate the prompt and stores the result in Phoenix, including a description and version information.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nif r\"\\{examples\\}\" in new_prompt:\n    new_prompt = new_prompt.format(examples=few_shot_examples)\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"system\", \"content\": new_prompt},\n        {\"role\": \"user\", \"content\": \"{{prompt}}\"},\n    ],\n)\n\nmeta_prompt_result = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Meta prompt result\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Previewing DataFrame Contents for Model Inferences using Python\nDESCRIPTION: Prints the first few rows of the DataFrame containing the model inferences data. Useful for verifying the structure and contents of the dataset before processing or visualization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/inferences_quickstart.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_df.head()\n```\n\n----------------------------------------\n\nTITLE: Using Instrumented BeeAI Agent\nDESCRIPTION: Illustrates how to use the instrumented BeeAI framework. It imports the previously created `instrumentation.js`, initializes a `BeeAgent` with an Ollama language model, memory, and tools (DuckDuckGo search, OpenMeteo), and then runs the agent with a prompt. The agent's response is logged to the console. It depends on the `instrumentation.js` file being executed first and the `beeai-framework` package.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/beeai.md#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport \"./instrumentation\";\nimport { BeeAgent } from \"beeai-framework/agents/bee/agent\";\nimport { TokenMemory } from \"beeai-framework/memory/tokenMemory\";\nimport { DuckDuckGoSearchTool } from \"beeai-framework/tools/search/duckDuckGoSearch\";\nimport { OpenMeteoTool } from \"beeai-framework/tools/weather/openMeteo\";\nimport { OllamaChatModel } from \"beeai-framework/adapters/ollama/backend/chat\";\n\nconst llm = new OllamaChatModel(\"llama3.1\");\nconst agent = new BeeAgent({\n  llm,\n  memory: new TokenMemory(),\n  tools: [new DuckDuckGoSearchTool(), new OpenMeteoTool()],\n});\n\nconst response = await agent.run({\n  prompt: \"What's the current weather in Berlin?\",\n});\n\nconsole.log(`Agent ðŸ¤– : `, response.result.text);\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Multimodal LLM with LlamaIndex\nDESCRIPTION: Initializes the OpenAI multimodal LLM using LlamaIndex. It loads image documents from the specified directory using `SimpleDirectoryReader` and sets up the `OpenAIMultiModal` LLM with the `gpt-4o` model and a maximum of 1500 new tokens.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_multimodal_image_reasoning.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.multi_modal_llms.openai import OpenAIMultiModal\n\n# put your local directory here\nimage_documents = SimpleDirectoryReader(\"./input_images\").load_data()\n\nopenai_mm_llm = OpenAIMultiModal(\n    model=\"gpt-4o\",\n    max_new_tokens=1500,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key (Python)\nDESCRIPTION: Configures the OpenAI API key required for using OpenAI's embedding and chat completion models. It first attempts to retrieve the key from the `OPENAI_API_KEY` environment variable and, if not found, prompts the user to enter it securely using `getpass`. The retrieved key is then set for the `openai` library and as an environment variable.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/milvus_llamaindex_search_and_retrieval_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Evaluating Classifications for GPT-3.5 Turbo with Report and Matrix\nDESCRIPTION: This snippet evaluates the predictions generated by GPT-3.5 Turbo against the ground truth using a classification report and a confusion matrix.  The true labels are mapped from the DataFrame's 'is_well_coded' column using the provided prompt rails map.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_functionality_classifications.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df[\"is_well_coded\"].map(CODE_FUNCTIONALITY_PROMPT_RAILS_MAP).tolist()\n\nprint(classification_report(true_labels, relevance_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels, predict_vector=relevance_classifications, classes=rails\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Phoenix Python SDK - Python\nDESCRIPTION: This snippet demonstrates how to set environment variables in Python to authenticate and configure the Phoenix client. It sets the PHOENIX_CLIENT_HEADERS for API key authentication and PHOENIX_COLLECTOR_ENDPOINT for the service endpoint. The PHOENIX_CLIENT_HEADERS variable is ignored if self-hosting; in that case, only PHOENIX_COLLECTOR_ENDPOINT must be specified. These variables are prerequisites for all client operations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key=...\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Running and Evaluating GPT-4 Turbo Relevance Classification in Python\nDESCRIPTION: Instantiates OpenAIModel for GPT-4 Turbo preview, then performs batched relevance classification and computes evaluation metrics as in earlier patterns. Demonstrates the ease of switching models and allows users to preview performance and speed tradeoffs of new OpenAI model releases.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model_name=\"gpt-4-turbo-preview\")\nrelevance_classifications = llm_classify(\n    dataframe=df_sample,\n    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n    model=model,\n    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n    concurrency=20,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Renaming Selected Span Attributes with Phoenix SpanQuery (Python)\nDESCRIPTION: Demonstrates renaming columns in the output DataFrame when selecting attributes using `SpanQuery().select()`. Keyword arguments are used to map desired column names (e.g., `input`) to the original span attributes (e.g., `input.value`).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nquery = SpanQuery().select(\n    input=\"input.value\",\n    output=\"output.value\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Installs the Arize Phoenix library with Llama-Index integration, the Hugging Face datasets library, and nest_asyncio using pip. The -U flag ensures upgrades to the latest versions, and -qqq suppresses installation output. These packages are dependencies for running the subsequent code, enabling experiment tracking, data handling, and asynchronous operations in environments like Jupyter notebooks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uqqq \"arize-phoenix[llama-index]>=4.6\" datasets nest_asyncio\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix and Llama-Index in Python\nDESCRIPTION: Installs the latest versions of the arize-phoenix library with llama-index support and the nest_asyncio library. This command sets up the required packages for asynchronous support and integration between Phoenix and Llama-Index in a Python environment such as Jupyter or Colab.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uqqq \"arize-phoenix[llama-index]>=4.6\" nest_asyncio\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix in a Notebook Environment (Python)\nDESCRIPTION: Initializes and launches the Phoenix observability UI from a notebook by importing and calling 'px.launch_app()'. Requires the 'arize-phoenix' package to be installed and assumes the notebook runtime can expose the necessary ports for UI interaction. Traces in notebooks are ephemeral and not persisted by default.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/groq.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Haystack with OpenTelemetry\nDESCRIPTION: This code snippet sets up OpenTelemetry instrumentation for Haystack. It imports the HaystackInstrumentor and the register function from phoenix.otel. The HaystackInstrumentor is then used to instrument the Haystack library, enabling tracing of Haystack components.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/haystack_tracing_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.haystack import HaystackInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register()\n\nHaystackInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Saving Phoenix Traces to a Specific Directory (Python)\nDESCRIPTION: Shows how to specify a custom directory when saving Phoenix traces using `get_trace_dataset().save()`. The desired directory path (e.g., '/my_saved_traces') is passed as the `directory` argument. The example also includes creating the directory using `os.makedirs` if it doesn't exist. Requires the `os` module.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Specify and Create the Directory for Trace Dataset\ndirectory = '/my_saved_traces'\nos.makedirs(directory, exist_ok=True)\n\n# Save the Trace Dataset\ntrace_id = px.Client().get_trace_dataset().save(directory=directory)\n```\n\n----------------------------------------\n\nTITLE: Installing arize-phoenix-otel Package - Bash\nDESCRIPTION: This command installs the `arize-phoenix-otel` package, which is required for tracing CrewAI applications with Arize Phoenix. This package provides the necessary instrumentation to capture and send traces to the Phoenix backend.  It's a prerequisite for setting up the tracing environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/crewai.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: AWS Bedrock Agent Configuration and Client Initialization in Python\nDESCRIPTION: Establishes AWS Bedrock agent parameters including SSO profile name, AWS region, Bedrock service name, and agent identifiers (agent ID and alias ID). Initializes a boto3 session and creates a Bedrock agent runtime client that allows invoking the agent in subsequent steps.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/amazon_bedrock_agents_tracing_and_evals.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# SSO Profile Configuration\nPROFILE_NAME = \"phoenix\"  # The name of the AWS SSO profile you created\nREGION = \"us-east-2\"  # The region where your Bedrock agent is deployed\nSERVICE_NAME = \"bedrock-agent-runtime\"  # The service name of your Bedrock agent\n\n# Bedrock Agent Configuration\nAGENT_ID = \"\"  # The ID of your Bedrock agent, found in the Bedrock Agents console\nAGENT_ALIAS_ID = \"\"  # The alias ID of your Bedrock agent, found in the Bedrock Agents console\n```\n\nLANGUAGE: python\nCODE:\n```\nsession = boto3.Session(profile_name=PROFILE_NAME)\nbedrock_agent_runtime = session.client(SERVICE_NAME, region_name=REGION)\n```\n\n----------------------------------------\n\nTITLE: Starting Node.js Application with Tracing (Shell)\nDESCRIPTION: This shell command demonstrates launching a Node.js application with tracing enabled by preloading the OpenTelemetry and OpenInference instrumentation script prior to starting the Express server. It uses Node.js v23, which natively supports TypeScript execution, and requires both `instrumentation.ts` (for tracing setup) and `app.ts` (for the server) to be present. This approach ensures all OpenAI SDK calls from the application will be traced without a build or transpilation step. Modify for applicable Node versions as necessary. Ensure environment variables and Phoenix collector availability prior to execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n# node v23\nnode --require ./instrumentation.ts app.ts\n\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Phoenix and LLM Integration\nDESCRIPTION: Installs necessary Python packages for Phoenix evaluations, LlamaIndex, OpenAI integration, and OpenInference instrumentation. These dependencies are required for tracing and evaluating LLM-based agent interactions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qq \"arize-phoenix[evals,llama-index]\" \"llama-index-llms-openai\" \"openai>=1\" gcsfs nest_asyncio openinference-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Viewing Experiment Run Results as a DataFrame in Python\nDESCRIPTION: Retrieves the experiment runs and displays them as a pandas DataFrame. This enables inspection of results, task outputs, and metadata for experimental analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nexperiment.as_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Executing a Custom Query and Printing the Response\nDESCRIPTION: Sends a specific, user-defined question (\"What is Arize and how can it help me as an AI Engineer?\") to the LlamaIndex `query_engine`. The engine processes the query using RAG, and the resulting response object is stored in the `response` variable and then printed to the console.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nresponse = query_engine.query(\"What is Arize and how can it help me as an AI Engineer?\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Running Local LangGraph App\nDESCRIPTION: This code snippet demonstrates how to instantiate the `SimpleLangGraphApp` and set it up using the `set_up()` method. Then, it proceeds to query the app with various prompts, simulating user interactions. This tests the application's behavior with different inputs, including requests that should be handled by the tools.  Dependencies include the project and location variables (`PROJECT_ID`, `LOCATION`).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/google_agent_engine_tracing_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nagent = SimpleLangGraphApp(project=PROJECT_ID, location=LOCATION)\nagent.set_up()\n```\n\n----------------------------------------\n\nTITLE: Creating VectorStore-Based Query Engines for Digital Camera Wikipedia Documents Using LlamaIndex in Python\nDESCRIPTION: Converts each LlamaIndex VectorStoreIndex into a query engine and wraps them in QueryEngineTool instances with descriptive text. These vector tools facilitate answering generic questions about digital cameras via semantic search over Wikipedia content.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_sql_retriever_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nvector_query_engines = [index.as_query_engine() for index in vector_indices]\nvector_tools = []\nfor query_engine in vector_query_engines:\n    vector_tool = QueryEngineTool.from_defaults(\n        query_engine=query_engine,\n        description=\"Useful for answering generic questions about digital cameras.\",\n    )\n    vector_tools.append(vector_tool)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Phoenix Experiments Using Custom Evaluators Python\nDESCRIPTION: This snippet demonstrates running experiment evaluation for multiple Phoenix experiment IDs. For each experiment, the evaluation routines 'matches_expected' and 'judged_correct' are applied using the Phoenix client's 'evaluate_experiment' method. Inputs are experiment_id strings and the two evaluator functions. Output: evaluation results, typically updating the experiment state in Phoenix. Dependencies: Phoenix SDK, evaluator function definitions, and availability of experiment IDs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/demos/playground_demo.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nphoenix_client = px.Client()\n\nexperiment_ids = [\n    \"RXhwZXJpbWVudDoyMTg=\",\n    \"RXhwZXJpbWVudDoyMTk=\",\n    \"RXhwZXJpbWVudDoyMjA=\",\n    \"RXhwZXJpbWVudDoyMjE=\",\n]\nfor experiment_id in experiment_ids:\n    experiment = phoenix_client.get_experiment(experiment_id=experiment_id)\n    evaluate_experiment(experiment, evaluators=[matches_expected, judged_correct])\n```\n\n----------------------------------------\n\nTITLE: Adding Semantic Attributes with OpenInference (python)\nDESCRIPTION: Demonstrates attaching standard semantic attributes for LLM tracing using the OpenInference Semantic Conventions package. Inputs: semantic attribute keys (e.g., INPUT_VALUE, LLM_MODEL_NAME), values to assign (e.g., prompt text, model name). Dependencies: opentelemetry.trace, openinference.semconv.trace. Output: current span is enriched with standard LLM semantic fields.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/custom-spans.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace\nfrom openinference.semconv.trace import SpanAttributes\n\n# ...\n\ncurrent_span = trace.get_current_span()\ncurrent_span.set_attribute(SpanAttributes.INPUT_VALUE, \"Hello world!\")\ncurrent_span.set_attribute(SpanAttributes.LLM_MODEL_NAME, \"gpt-3.5-turbo\")\n```\n\n----------------------------------------\n\nTITLE: OTEL Span Example for LLM\nDESCRIPTION: This JSON snippet represents an example OTEL span for an LLM operation. It includes metadata such as trace ID, span ID, start and end times, and attributes related to the LLM interaction. Key attributes include input messages, model name, invocation parameters, and output values. The `openinference.span.kind` attribute is set to \"LLM\".\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/concepts-tracing/what-are-traces.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n   \"name\": \"llm\",\n   \"context\": {\n       \"trace_id\": \"0x6c80880dbeb609e2ed41e06a6397a0dd\",\n       \"span_id\": \"0xd9bdedf0df0b7208\",\n       \"trace_state\": \"[]\"\n   },\n   \"kind\": \"SpanKind.INTERNAL\",\n   \"parent_id\": \"0x7eb5df0046c77cd2\",\n   \"start_time\": \"2024-05-08T21:46:11.480777Z\",\n   \"end_time\": \"2024-05-08T21:46:35.368042Z\",\n   \"status\": {\n       \"status_code\": \"OK\"\n   },\n   \"attributes\": {\n       \"openinference.span.kind\": \"LLM\",\n       \"llm.input_messages.0.message.role\": \"system\",\n       \"llm.input_messages.0.message.content\": \"\n  The following is a friendly conversation between a user and an AI assistant.\n  The assistant is talkative and provides lots of specific details from its context.\n  If the assistant does not know the answer to a question, it truthfully says it\n  does not know.\n\n  Here are the relevant documents for the context:\n\n  page_label: 7\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\n\nDomestic Mail Manual â€¢ Updated 7-9-23101\n101.6.4Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\na. No piece may weigh more than 70 pounds.\nb. The combined length and girth of a piece (the length of its longest side plus \nthe distance around its thickest part) may not exceed 108 inches.\nc. Lower size or weight standards apply to mail addressed to certain APOs and \nFPOs, subject to 703.2.0  and 703.4.0  and for Department of State mail, \nsubject to 703.3.0 .\",\n       \"llm.input_messages.1.message.role\": \"user\",\n       \"llm.input_messages.1.message.content\": \"Hello\",\n       \"llm.model_name\": \"gpt-4-turbo-preview\",\n       \"llm.invocation_parameters\": \"{\\\"temperature\\\": 0.1, \\\"model\\\": \\\"gpt-4-turbo-preview\\\"}\",\n       \"output.value\": \"How are you?\"\n    },\n   \"events\": [],\n   \"links\": [],\n   \"resource\": {\n       \"attributes\": {},\n       \"schema_url\": \"\"\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Span Attributes with `using_attributes` Context Manager in Python\nDESCRIPTION: Demonstrates using the `using_attributes` context manager from `openinference.instrumentation` to add various attributes like session ID, user ID, metadata, tags, and prompt details to the current OpenTelemetry context in Python. Calls made within the `with` block will automatically have these attributes attached to their corresponding spans, following OpenInference semantic conventions. This simplifies adding multiple attributes compared to using individual context managers.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation import using_attributes\ntags = [\"tag_1\", \"tag_2\", ...]\nmetadata = {\n    \"key-1\": value_1,\n    \"key-2\": value_2,\n    ...\n}\nprompt_template = \"Please describe the weather forecast for {city} on {date}\"\nprompt_template_variables = {\"city\": \"Johannesburg\", \"date\":\"July 11\"}\nprompt_template_version = \"v1.0\"\nwith using_attributes(\n    session_id=\"my-session-id\",\n    user_id=\"my-user-id\",\n    metadata=metadata,\n    tags=tags,\n    prompt_template=prompt_template,\n    prompt_template_version=prompt_template_version,\n    prompt_template_variables=prompt_template_variables,\n):\n    # Calls within this block will generate spans with the attributes:\n    # \"session.id\" = \"my-session-id\"\n    # \"user.id\" = \"my-user-id\"\n    # \"metadata\" = \"{\\\"key-1\\\": value_1, \\\"key-2\\\": value_2, ... }\" # JSON serialized\n    # \"tag.tags\" = \"[\\\"tag_1\\\",\\\"tag_2\\\",...]\"\n    # \"llm.prompt_template.template\" = \"Please describe the weather forecast for {city} on {date}\"\n    # \"llm.prompt_template.variables\" = \"{\\\"city\\\": \\\"Johannesburg\\\", \\\"date\\\": \\\"July 11\\\"}\" # JSON serialized\n    # \"llm.prompt_template.version \" = \"v1.0\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Python\nDESCRIPTION: Imports the `os` and `getpass` modules. Checks if the `OPENAI_API_KEY` environment variable is set; if not, it securely prompts the user to enter their key using `getpass` and sets it as an environment variable for the current session. This key is required for interacting with OpenAI models later in the script.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Defining Structured Function Call for Entity Extraction\nDESCRIPTION: Creates a Pydantic model and prompt configuration for extracting structured entities from user input, such as product parameters like category, subcategory, and color with defined categories. Enables structured function calling in OpenAI models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nclass ProductSearchParameters(BaseModel):\n    class Category(str, Enum):\n        shoes = \"shoes\"\n        jackets = \"jackets\"\n        tops = \"tops\"\n        bottoms = \"bottoms\"\n\n    category: Category\n    subcategory: str\n    color: str\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-4o-mini\",\n    temperature=0,\n    messages=[\n        {\"role\": \"system\", \"content\": dedent(product_search_prompt)},\n        {\"role\": \"user\", \"content\": \"CONTEXT: {{context}}\\n\\nUSER INPUT: {{user_input}}\"},\n    ],\n    tools=[\n        openai.pydantic_function_tool(\n            ProductSearchParameters,\n            name=\"product_search\",\n            description=\"Search for a match in the product database\",\n        )\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Checking Task Function\nDESCRIPTION: Tests the task function by applying it to a single example from the dataset.  This verifies that the task, which extracts information from the email using the LLM and parses the output to JSON, is working correctly.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfirst_key = next(iter(dataset.examples))\nfirst_example = dataset.examples[first_key]\n\ntask(first_example)\n```\n\n----------------------------------------\n\nTITLE: Executing the DSPy App on Development Set\nDESCRIPTION: Runs evaluation over the dev dataset, querying the compiled and teleprompted module for each example, printing questions, predicted answers, and truncated retrieved contexts while traces are collected in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/dspy_tracing_tutorial.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nfor example in devset:\n    question = example[\"question\"]\n    prediction = compiled_module(question)\n    print(\"Question\")\n    print(\"========\")\n    print(question)\n    print()\n    print(\"Predicted Answer\")\n    print(\"================\")\n    print(prediction.answer)\n    print()\n    print(\"Retrieved Contexts (truncated)\")\n    print(f\"{[c[:200] + '...' for c in prediction.context]}\")\n    print()\n    print()\n\nprint(phoenix_session.url)\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Hallucination Score\nDESCRIPTION: Calculates and displays the average hallucination score from the `hallucination_eval_df` DataFrame. The `mean(numeric_only=True)` method computes the average of numerical columns, usually the 'score' column, providing an overall measure of hallucination.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nhallucination_eval_df.mean(numeric_only=True)\n```\n\n----------------------------------------\n\nTITLE: Extracting QA Pairs with Reference Context\nDESCRIPTION: Retrieves question-answer pairs along with their reference context from Phoenix for response evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_qa_with_reference\n\nqa_with_reference_df = get_qa_with_reference(px.Client())\nqa_with_reference_df\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Phoenix and Haystack\nDESCRIPTION: Installs necessary Python packages including OpenInference instrumentation for Haystack, the Haystack AI framework, and Arize Phoenix for monitoring and evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag_haystack.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q openinference-instrumentation-haystack haystack-ai \"arize-phoenix>=4.29.0\" 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Python\nDESCRIPTION: Sets the OpenAI API key using the getpass function for secure input. It checks if the key is already set in the environment variables and prompts the user if it's not.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix and Experiment Dependencies - Shell\nDESCRIPTION: Installs necessary Python packages for running the Phoenix summarization tutorial. Includes packages for Anthropic, Arize Phoenix, OpenAI, OpenInference instrumentation, ROUGE evaluation, and token counting.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/summarization.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install anthropic \"arize-phoenix>=4.6.0\" openai openinference-instrumentation-openai rouge tiktoken\n```\n\n----------------------------------------\n\nTITLE: Exporting Trace Data for Evaluation\nDESCRIPTION: Exports trace data from Phoenix as pandas dataframes for query results and retrieved documents to enable evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_tracing_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nqueries_df = get_qa_with_reference(px.Client())\nretrieved_documents_df = get_retrieved_documents(px.Client())\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Schema with Predictions and Actuals in Python\nDESCRIPTION: Creates a `phoenix.Schema` object mapping specific columns from a pandas DataFrame to standard Phoenix fields: timestamp, prediction score, prediction label, and actual label (ground truth). This schema is suitable for evaluating model performance where predictions and actual outcomes are available. Assumes the DataFrame contains columns named 'timestamp', 'prediction_score', 'prediction', and 'target'.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/define-your-schema/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nschema = px.Schema(\n    timestamp_column_name=\"timestamp\",\n    prediction_score_column_name=\"prediction_score\",\n    prediction_label_column_name=\"prediction\",\n    actual_label_column_name=\"target\",\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Endpoint (Python)\nDESCRIPTION: This snippet sets the Phoenix collector endpoint as an environment variable. This is required when using a self-hosted Phoenix instance. Replace \"Your Phoenix Endpoint\" with the actual URL of your Phoenix deployment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-python.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"Your Phoenix Endpoint\"\n```\n\n----------------------------------------\n\nTITLE: Augmenting Trace DataFrame with Tool Definitions - Python\nDESCRIPTION: Adds a new column 'tool_definitions' to the trace_df DataFrame, storing the concatenated list of all available tool descriptions. Requires existing trace_df DataFrame and the tool_definitions string. Input: DataFrame and string; Output: DataFrame with extra column.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ntrace_df[\"tool_definitions\"] = tool_definitions\n```\n\n----------------------------------------\n\nTITLE: Running LLM-as-a-Judge Experiment with Condensed Prompt - Python\nDESCRIPTION: This snippet defines the judge function and experiment runner utilizing the more efficient, condensed prompt template. It sends user responses through the LLM classifier, using the GPT-4 model and a compacted empathy prompt to improve efficiency and reduce cost or latency. Dependencies include EMPATHY_EVALUATION_PROMPT_TEMPLATE_CONDENSED, llm_classify, OpenAIModel, and run_experiment, as well as a suitable evaluate_response function.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef llm_as_a_judge(input):\n    response_classifications = llm_classify(\n        dataframe=pd.DataFrame([{\"AI_Response\": input[\"AI_Response\"]}]),\n        template=EMPATHY_EVALUATION_PROMPT_TEMPLATE_CONDENSED,\n        model=OpenAIModel(model=\"gpt-4\"),\n        rails=list(map(str, range(1, 11))),\n        provide_explanation=True,\n    )\n    score = response_classifications.iloc[0][\"label\"]\n    return int(score)\n\n\nexperiment = run_experiment(\n    dataset, task=llm_as_a_judge, evaluators=[evaluate_response], experiment_name=\"condensed_prompt\"\n)\n```\n\n----------------------------------------\n\nTITLE: Synthetic Data Generation with Output Parser - JSON\nDESCRIPTION: This example shows how to generate structured data in JSON format using an `output_parser` function.  It defines an `output_parser` which uses the `json` library to parse the LLM's output. It uses an OpenAI model and a PromptTemplate to provide the appropriate formatting. The `model_kwargs` parameter is set to instruct the LLM to generate JSON output. The function uses the defined output_parser to parse each generated response into a dictionary. Then it provides the output_parser function as the argument to llm_generate.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evals-reference.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom typing import Dict\n\nimport pandas as pd\nfrom phoenix.evals import OpenAIModel, PromptTemplate, llm_generate\n\n\ndef output_parser(response: str) -> Dict[str, str]:\n        try:\n            return json.loads(response)\n        except json.JSONDecodeError as e:\n            return {\"__error__\": str(e)}\n\ncountries_df = pd.DataFrame(\n    {\n        \"country\": [\n            \"France\",\n            \"Germany\",\n            \"Italy\",\n        ]\n    }\n)\n\ntemplate = PromptTemplate(\"\"\"\nGiven the country {country}, output the capital city and a description of that city.\nThe output must be in JSON format with the following keys: \\\"capital\\\" and \\\"description\\\".\n\nresponse:\n\"\"\")\n\ncapitals_df = llm_generate(\n    dataframe=countries_df,\n    template=template,\n    model=OpenAIModel(\n        model_name=\"gpt-4-turbo-preview\",\n        model_kwargs={\n            \"response_format\": {\"type\": \"json_object\"}\n        }\n        ),\n    output_parser=output_parser\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Image Embedding Schema with Arize Phoenix in Python\nDESCRIPTION: Defines a schema specifying the actual label column and an embedding feature for image data. The embedding_feature_column_names dictionary contains an entry with vector column and link to image URL column. This schema is used by the Arize Phoenix library to understand the structure of embedding data representing images for downstream processing. Requires the Arize Phoenix SDK (imported as px).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/define-your-schema/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nschema = px.Schema(\n    actual_label_column_name=\"defective\",\n    embedding_feature_column_names={\n        \"image_embedding\": px.EmbeddingColumnNames(\n            vector_column_name=\"image_vector\",\n            link_to_data_column_name=\"image\",\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Top 20 Rows of Relevance Classification Results in Python\nDESCRIPTION: Prints the first 20 rows of the DataFrame containing relevance classification results obtained from the LLM evaluate process, facilitating inspection and verification of evaluation outputs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nrelevance_df.head(20)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Python Environment\nDESCRIPTION: Checks if the OpenAI API key is present in environment variables and prompts the user to securely input it if not. The key is then set in the environment for use by downstream OpenAI API clients. This snippet depends on the getpass module for password input without echoing to the console.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"\\ud83d\\udd11 Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenAI API Key for Authentication\nDESCRIPTION: Configures the OpenAI API key by either retrieving it from environment variables or prompting the user for input. This key is required for making calls to OpenAI's API services.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Importing Required Python Libraries\nDESCRIPTION: Imports various Python libraries needed for web requests (urllib.request), data encoding (base64), string manipulation (io.StringIO), type hinting (typing), HTML parsing (bs4), interacting with the OpenAI API (openai), data manipulation (pandas), generating fake data (faker), configuring tracing (openinference.instrumentation), instrumenting OpenAI (openinference.instrumentation.openai), and interacting with Phoenix (phoenix).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/vision.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport urllib.request\nfrom base64 import b64encode\nfrom io import StringIO\nfrom typing import Dict\n\nimport bs4 as bs\nimport openai\nimport pandas as pd\nfrom faker import Faker\nfrom openinference.instrumentation import TraceConfig\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nimport phoenix as px\n```\n\n----------------------------------------\n\nTITLE: Using Semantic Attributes on Span - JavaScript\nDESCRIPTION: Example demonstrating how to set a semantic attribute (specifically `SemanticAttributes.INPUT_VALUE` from OpenInference conventions) on a span using `span.setAttribute()`. This provides standardized information for tracing LLM operations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_18\n\nLANGUAGE: javascript\nCODE:\n```\nconst doWork = () => {\n  tracer.startActiveSpan('app.doWork', (span) => {\n    span.setAttribute(SemanticAttributes.INPUT_VALUE, 'work input');\n    // Do some work...\n\n    span.end();\n  });\n};\n```\n\n----------------------------------------\n\nTITLE: Illustrating LLM Span Attributes - TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates the structure of an OpenTelemetry LLM span attribute payload, specifically for a chat operation. It shows how input and output messages, model name, token counts, and invocation parameters are captured, providing context for translating span data into a format usable by the Playground for features like LLM replay.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/internal_docs/specs/playground.md#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  llm: {\n    output_messages: [\n      {\n        message: {\n          content: \"This is an AI Answer\",\n          role: \"assistant\",\n        },\n      },\n    ],\n    model_name: \"gpt-3.5-turbo\",\n    token_count: { completion: 9.0, prompt: 1881.0, total: 1890.0 },\n    input_messages: [\n      {\n        message: {\n          content: \"You are a chatbot\",\n          role: \"system\",\n        },\n      },\n      {\n        message: {\n          content: \"Anser me the following question. Are you sentient?\",\n          role: \"user\",\n        },\n      },\n    ],\n    invocation_parameters:\n      '{\"context_window\": 16384, \"num_output\": -1, \"is_chat_model\": true, \"is_function_calling_model\": true, \"model_name\": \"gpt-3.5-turbo\"}',\n  },\n  openinference: { span: { kind: \"LLM\" } },\n}\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App with No Inferences in Python\nDESCRIPTION: Launches a Phoenix session in the background without any predefined inference sets. This mode is used to collect OpenInference traces from an instrumented LLM application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/manage-the-app.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsession = px.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix and Instrumentation Dependencies Using Shell\nDESCRIPTION: Installs the Arize Phoenix SDK along with OpenTelemetry SDK, OTLP exporter, CrewAI, CrewAI tools, and OpenInference instrumentation libraries necessary for tracing and monitoring CrewAI activities. This snippet sets up the environment with all required Python packages for subsequent instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/crewai_tracing_tutorial.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n!pip install -q arize-phoenix opentelemetry-sdk opentelemetry-exporter-otlp crewai crewai_tools openinference-instrumentation-crewai\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix from Wheel\nDESCRIPTION: Commands to install Phoenix from a Python wheel file. The wheel package must be installed first, and this provides a faster installation compared to the source distribution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\npip install wheel\npip install /path/to/wheel.whl\n```\n\n----------------------------------------\n\nTITLE: Evaluating GPT-4 Hallucination Detection Performance\nDESCRIPTION: Compares the LLM's hallucination classifications against ground-truth labels, generating classification metrics and visualizing a confusion matrix to assess performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df[\"is_hallucination\"].map(HALLUCINATION_PROMPT_RAILS_MAP).tolist()\nprint(classification_report(true_labels, hallucination_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels,\n    predict_vector=hallucination_classifications,\n    classes=rails,\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating GPT-4 Q&A Classification Results\nDESCRIPTION: Evaluates GPT-4's classification performance by comparing predictions against ground truth labels, generating a classification report and confusion matrix visualization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df_sample[\"answer_true\"].map(QA_PROMPT_RAILS_MAP).tolist()\n\nprint(classification_report(true_labels, Q_and_A_classifications, labels=rails))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels, predict_vector=Q_and_A_classifications, classes=rails\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI Client with OpenInference (Python)\nDESCRIPTION: Imports `OpenAIInstrumentor` from `openinference.instrumentation.openai` and calls its `instrument()` method, passing the previously created `tracer_provider`. This setup automatically captures details of subsequent OpenAI API calls as traces and sends them to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evaluating_traces_quickstart.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Installing Command Line packages\nDESCRIPTION: Installs the `arize-phoenix` package for launching a local Phoenix instance and the `arize-phoenix-otel` package for tracing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/instructor.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Viewing Traces with Retrieved Documents\nDESCRIPTION: Displays the input values and retrieved documents for traces where documents are present, supporting detailed evaluation of retrieval accuracy.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nspans_with_docs_df[\"attributes.input.value\", \"attributes.retrieval.documents\"].head()\n\n```\n\n----------------------------------------\n\nTITLE: Defining Custom ContainsSubstring Evaluator (Python)\nDESCRIPTION: Implements a custom Phoenix evaluator class `ContainsSubstring` that checks if a specified substring is present in the experiment run's output result and returns a binary score (1 if present, 0 otherwise).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/run_experiments_with_llama_index.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass ContainsSubstring:\n    name = \"contains_substring\"\n    annotator_kind = \"CODE\"\n\n    def __init__(self, substring: str):\n        self.substring = substring\n\n    def evaluate(self, _: Example, exp_run: ExperimentRun) -> EvaluationResult:\n        result = exp_run.output.result\n        score = int(isinstance(result, str) and self.substring in result)\n        return EvaluationResult(\n            score=score,\n            explanation=f\"the substring `{repr(self.substring)}` was in the output\",\n        )\n\n    async def async_evaluate(self, _: Example, exp_run: ExperimentRun) -> EvaluationResult:\n        return self.evaluate(_, exp_run)\n```\n\n----------------------------------------\n\nTITLE: Implementing Task Function with Few Shot Prompt in Python\nDESCRIPTION: Defines a new task function 'test_prompt' that calls the OpenAI chat completion API using the few shot prompt version from Phoenix. It passes the input prompt into the prompt variables and returns the model's classification output. This task function replaces the base prompt task for subsequent experiments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef test_prompt(input):\n    client = OpenAI()\n    prompt_vars = {\"prompt\": input[\"prompt\"]}\n    resp = client.chat.completions.create(**few_shot_prompt.format(variables=prompt_vars))\n    return resp.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Compiling the Workflow into a Runnable Application\nDESCRIPTION: Transforms the defined workflow into an executable form, enabling invocation and execution within the application context.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\napp = workflow.compile()\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Evals and Dependencies with pip - Bash\nDESCRIPTION: Installs the arize-phoenix package (version 4.29.0 or higher) and supporting Python libraries such as openai, nest_asyncio, and a compatible version of httpx using pip in a bash environment. Required before using Phoenix Evals or OpenAI-powered evaluation. Expects the user to already have Python and pip installed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/evals.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n%%bash\npip install -q \"arize-phoenix>=4.29.0\"\npip install -q openai nest_asyncio 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Generating Questions with LLM - Python\nDESCRIPTION: This Python snippet demonstrates the generation of synthetic questions using an LLM. The code defines a `document_chunks` list and constructs a Pandas DataFrame from it.  A template is defined to guide the LLM for question generation. The  `llm_generate` function is utilized with an OpenAI model to create a DataFrame of questions. The generated questions are then uploaded to a dataset using the Phoenix client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-datasets/creating-datasets.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport json\n\nfrom phoenix.evals import OpenAIModel, llm_generate\n\n\nimport pandas as pd\n\ndocument_chunks = [\n  \"Paul Graham is a VC\",\n  \"Paul Graham loves lisp\",\n  \"Paul founded YC\",\n]\ndocument_chunks_df = pd.DataFrame({\"text\": document_chunks})\n\ndef output_parser(response: str, index: int):\n    try:\n        return json.loads(response)\n    except json.JSONDecodeError as e:\n        return {\"__error__\": str(e)}\n\ngenerate_questions_template = (\n    \"Context information is below.\\n\\n\"\n    \"---------------------\\n\"\n    \"{text}\\n\"\n    \"---------------------\\n\\n\"\n    \"Given the context information and not prior knowledge.\\n\"\n    \"generate only questions based on the below query.\\n\\n\"\n    \"You are a Teacher/ Professor. Your task is to setup \"\n    \"one question for an upcoming \"\n    \"quiz/examination. The questions should be diverse in nature \"\n    \"across the document. Restrict the questions to the \"\n    \"context information provided.\\n\\n\"\n    \"Output the questions in JSON format with the key question\"\n)\nquestions_df = llm_generate(\n    dataframe=document_chunks_df,\n    template=generate_questions_template,\n    model=OpenAIModel(model=\"gpt-3.5-turbo\"),\n    output_parser=output_parser,\n    concurrency=20,\n)\nquestions_df[\"output\"] = [None, None, None]\nimport phoenix as px\n\n# Note that the below code assumes that phoenix is running and accessible\nclient = px.Client()\nclient.upload_dataset(\n    dataframe=questions_df, dataset_name=\"paul-graham-questions\",\n    input_keys=[\"question\"],\n    output_keys=[\"output\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Relevance Classification with OpenAI Model on SQL Query DataFrame in Python\nDESCRIPTION: Initializes an OpenAI GPT-4o model instance with zero temperature for deterministic output and applies a classification function llm_classify on the DataFrame using a template and rails for instructions. The classification evaluates the relevance/correctness of SQL queries generated in the DataFrame. Provides explanation outputs with each classification. This step requires prior definitions of SQL_GEN_EVAL_PROMPT_TEMPLATE, SQL_GEN_EVAL_PROMPT_RAILS_MAP, llm_classify, OpenAIModel, and a populated DataFrame df.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nrails = list(SQL_GEN_EVAL_PROMPT_RAILS_MAP.values())\nmodel = OpenAIModel(\n    model=\"gpt-4o\",\n    temperature=0.0,\n)\nrelevance_classifications = llm_classify(\n    dataframe=df,\n    template=SQL_GEN_EVAL_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    provide_explanation=True,\n)\n# relevance_classifications\n```\n\n----------------------------------------\n\nTITLE: Running Tasks and Evaluations in Tandem\nDESCRIPTION: Initiates a combined process that uploads dataset, runs tasks, and performs evaluations in a single step, streamlining the workflow.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\n_ = run_experiment(dataset, task, evaluators)\n```\n\n----------------------------------------\n\nTITLE: Implement Text2SQL Generation Python\nDESCRIPTION: Sets up the core logic for generating SQL queries from natural language using the OpenAI API. It initializes an asynchronous OpenAI client, retrieves the schema of the 'nba' table from DuckDB, and constructs a detailed system prompt for the LLM (GPT-4o). The prompt instructs the model to act as a SQL expert and generate a query based on the provided table schema and user request. The `generate_query` async function sends the prompt and user input to the OpenAI chat completions endpoint and returns the generated query string.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nimport openai\n\nclient = openai.AsyncClient()\n\ncolumns = conn.query(\"DESCRIBE nba\").to_df().to_dict(orient=\"records\")\n\n# We will use GPT4o to start\nTASK_MODEL = \"gpt-4o\"\nCONFIG = {\"model\": TASK_MODEL}\n\nsystem_prompt = (\n    \"You are a SQL expert, and you are given a single table named nba with the following columns:\\n\"\n    f\"{\",\".join(column[\"column_name\"] + \": \" + column[\"column_type\"] for column in columns)}\\n\"\n    \"Write a SQL query corresponding to the user's request. Return just the query text, \"\n    \"with no formatting (backticks, markdown, etc.).\"\n)\n\n\nasync def generate_query(input):\n    response = await client.chat.completions.create(\n        model=TASK_MODEL,\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": input,\n            },\n        ],\n    )\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Viewing Hallucination Evaluation Results\nDESCRIPTION: Displays the head of the hallucination evaluation dataframe to review flagged hallucinations in the RAG system's outputs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nhallucination_eval.head()\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Phoenix for Experimentation - Python\nDESCRIPTION: Generates a unique experiment identifier, creates a Phoenix client, and uploads the sampled jailbreak dataset for tracking and analysis. Specifies 'prompt' as input and 'type' as output columns, with the dataset name including a unique UUID to avoid naming collisions. Dependencies: phoenix, uuid. Output is a dataset object in Phoenix for further runs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n\nimport phoenix as px\nfrom phoenix.client import Client as PhoenixClient\n\nunique_id = uuid.uuid4()\n\n# Upload the dataset to Phoenix\ndataset = px.Client().upload_dataset(\n    dataframe=ds,\n    input_keys=[\"prompt\"],\n    output_keys=[\"type\"],\n    dataset_name=f\"jailbreak-classification-{unique_id}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Chat Application with LangChain\nDESCRIPTION: This JavaScript snippet demonstrates creating a simple chat application using LangChain and OpenAI. It defines a prompt template, sets a question, initializes a ChatOpenAI model, creates a chain, invokes the chain with a question, and logs the response content. The application relies on the `ChatOpenAI` and `ChatPromptTemplate` from `@langchain/openai` and `@langchain/core/prompts` packages. It requires the `OPENAI_API_KEY` environment variable to be set for the OpenAI API calls to succeed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/langchain/tracing_langchain_node_tutorial.ipynb#_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst PROMPT_TEMPLATE = `\n    You are a highly trained AI model that can answer any question. Please answer questions concisely. \n    Answer the following question: \n    \"{question}\"\n`;\n\nconst question = \"What is javascript used for?\";\n\nconst chatPrompt = ChatPromptTemplate.fromTemplate(PROMPT_TEMPLATE)\n\nconst model = new ChatOpenAI({ model: \"gpt-4\" });\n\nconst chain = chatPrompt.pipe(model);\n\nconst response = await chain.invoke({ question });\n\nconsole.log(response.content);\n```\n\n----------------------------------------\n\nTITLE: Running Few Shot Examples Prompt Experiment with Phoenix in Python\nDESCRIPTION: Executes a Phoenix experiment using the dataset, the new few shot task, and existing evaluator to measure improvements from few shot prompt optimization. Metadata includes reference to the specific prompt version to track experiment lineage and performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfew_shot_experiment = run_experiment(\n    dataset,\n    task=test_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Prompt Optimization Technique #1: Few Shot Examples\",\n    experiment_name=\"few-shot-examples\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + few_shot_prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Installing the Arize SDK using Pip\nDESCRIPTION: Installs the base Arize Python SDK using pip with the quiet flag (`-qq`). The Arize SDK is used for interacting with the Arize platform, including logging data and potentially generating embeddings.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n!pip install -qq arize\n```\n\n----------------------------------------\n\nTITLE: Instrumenting LiteLLM for CrewAI Versions >=0.63.0 in Python\nDESCRIPTION: Installs and enables instrumentation of LiteLLM, which replaces Langchain in newer versions of CrewAI (0.63.0 and above). The LiteLLMInstrumentor hooks into LiteLLM calls to capture traces for AI model operations with OpenTelemetry using the existing tracer_provider. This ensures trace coverage for the updated LLM backend.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/crewai_tracing_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n! pip install openinference-instrumentation-litellm\n\nfrom openinference.instrumentation.litellm import LiteLLMInstrumentor\n\nLiteLLMInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Initializing Python Environment (Python)\nDESCRIPTION: Imports all necessary standard, third-party, and application-specific libraries for building and evaluating the retrieval QA system. Initializes the environment by applying nest_asyncio for interactive environments and sets Pandas display options for better visualization. All listed packages must be installed prior to running these imports.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/qdrant_langchain_instrumentation_search_and_retrieval_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Standard library imports\nimport os\nfrom getpass import getpass\n\n# Third-party library imports\nimport nest_asyncio\nimport numpy as np\nimport pandas as pd\nfrom langchain.callbacks import StdOutCallbackHandler\n\n# LangChain imports\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import GitbookLoader\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Qdrant\nfrom langchain_openai import ChatOpenAI\n\n# OpenInference imports\nfrom openinference.instrumentation.langchain import LangChainInstrumentor\n\n# Phoenix and OpenInference imports\nimport phoenix as px\nfrom phoenix.otel import register\n\n# Miscellaneous imports\n\n# Configuration and Initialization\nnest_asyncio.apply()\npd.set_option(\"display.max_colwidth\", None)\n```\n\n----------------------------------------\n\nTITLE: Setting LLM Vendor API Keys via Environment Variables in Python\nDESCRIPTION: Prompts the user to input API keys interactively if they are not already set in environment variables. Supports OpenAI, Anthropic, Google, and Groq API keys with blank inputs allowed. Necessary for authenticating subsequent LLM API calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key: \")\nif not os.getenv(\"ANTHROPIC_API_KEY\"):\n    os.environ[\"ANTHROPIC_API_KEY\"] = getpass(\"Anthropic API key: \")\nif not os.getenv(\"GOOGLE_API_KEY\"):\n    os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Google API key: \")\nif not os.getenv(\"GROQ_API_KEY\"):\n    os.environ[\"GROQ_API_KEY\"] = getpass(\"Groq API key: \")\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix OTEL Tracer Provider with Custom Project Name, Headers, and Batching in Python\nDESCRIPTION: Defines how to customize the Phoenix OTEL tracer provider with explicit arguments like project_name, headers (including authorization), and batch processing control. This configuration overrides defaults or environment variables as needed to tailor span export parameters and authentication headers for Phoenix server communication.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\ntracer_provider = register(\n    project_name=\"otel-test\",\n    headers={\"Authorization\": \"Bearer TOKEN\"},\n    batch=True,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up Google Generative AI Provider\nDESCRIPTION: Configures the Google Generative AI provider by prompting for an API key and initializing the createGoogleGenerativeAI utility. This establishes the connection to Google's generative AI services.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_vercel_ai_sdk.ipynb#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst googleApiKey = prompt(\"Enter your Google Generative AI API key:\")\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst google = createGoogleGenerativeAI({ apiKey: googleApiKey });\n```\n\n----------------------------------------\n\nTITLE: Creating Interactive Chat Interface with OpenAI GPT-3.5\nDESCRIPTION: Sets up an interactive chat interface within a Jupyter Notebook using `ipywidgets`. It initializes the OpenAI client, creates display widgets (output area, input text area, send button), sends the prepared initial prompt (`chat_initial_input`) about the cluster data to the 'gpt-3.5-turbo' model, displays the response, and sets up an event handler (`on_submit_button_click`) to process subsequent user messages through the same GPT model.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# @title Chat GPT - Cluster Analysis\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nmessages = []\n\n# Create the output widget\noutput = widgets.Output(\n    layout={\"border\": \"1px solid black\", \"width\": \"100%\", \"height\": \"300px\", \"overflow\": \"scroll\"}\n)\n\n# Create the input widget\ninput_box = widgets.Textarea(\n    value=\"\",\n    placeholder=\"Type your message here...\",\n    description=\"\",\n    disabled=False,\n    layout=widgets.Layout(width=\"100%\", height=\"100px\"),\n)\n\n# Create the submit button\nsubmit_button = widgets.Button(\n    description=\"Send\", disabled=False, button_style=\"success\", tooltip=\"Send your message\"\n)\n\n# Display the output widget and the input components\ndisplay(output)\ndisplay(input_box)\ndisplay(submit_button)\n\nwith output:\n    message = chat_initial_input\n    if message:\n        messages.append(\n            {\"role\": \"user\", \"content\": message},\n        )\n        chat = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages)\n    reply = chat.choices[0].message.content\n    print(f\"ChatGPT RESPONSE: {reply}\")\n    print(\"\\n\")\n    print(\"-- Ask another question related to your data below --\")\n    messages.append({\"role\": \"assistant\", \"content\": reply})\n\n\ndef process_input(input_text):\n    # Simulate a simple chatbot response (you can replace this with your own logic)\n    response = f\"You said: {input_text}\"\n    return response\n\n\ndef on_submit_button_click(button):\n    with output:\n        user_input = input_box.value.strip()\n        if user_input:\n            messages.append(\n                {\"role\": \"user\", \"content\": user_input},\n            )\n            chat = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages)\n            reply = chat.choices[0].message.content\n            print(f\"ChatGPT RESPONSE: {reply}\")\n            messages.append({\"role\": \"assistant\", \"content\": reply})\n        input_box.value = \"\"\n\n\n# Set the button click event handler\nsubmit_button.on_click(on_submit_button_click)\n```\n\n----------------------------------------\n\nTITLE: Generating Prompt and Response Embeddings with Arize\nDESCRIPTION: Resets the DataFrame index to ensure it's sequential. If embedding columns ('prompt_vector', 'response_vector') don't exist, it uses the previously initialized Arize `EmbeddingGenerator` to generate embeddings for the text in the 'prompt' and 'response' columns and adds them to the DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Very fast on GPU (seconds) but can take a 2-3 minute on a CPU\nconversations_df = conversations_df.reset_index(drop=True)\nif not all(col in conversations_df.columns for col in [\"prompt_vector\", \"response_vector\"]):\n    conversations_df[\"prompt_vector\"] = generator.generate_embeddings(\n        text_col=conversations_df[\"prompt\"]\n    )\n    conversations_df[\"response_vector\"] = generator.generate_embeddings(\n        text_col=conversations_df[\"response\"]\n    )\n```\n\n----------------------------------------\n\nTITLE: Cleaning and Renaming Columns in Evaluation Dataframe\nDESCRIPTION: This code snippet cleans the input values and renames the columns of the evaluation DataFrame to prepare it for use with the LLM evaluation template.  It removes the prefixes 'Human: ' and 'Assistant:' from the input values and renames 'attributes.input.value' to 'Question' and 'attributes.output.value' to 'Answer'.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nevals_copy = eval_df.copy()\nevals_copy[\"attributes.input.value\"] = (\n    evals_copy[\"attributes.input.value\"]\n    .str.replace(r\"^Human: \", \"\", regex=True)\n    .str.replace(r\"Assistant:$\", \"\", regex=True)\n)\n\nevals_copy = evals_copy.rename(\n    columns={\"attributes.input.value\": \"Question\", \"attributes.output.value\": \"Answer\"}\n)\nevals_copy.head()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Phoenix with Evaluation and Llama Index\nDESCRIPTION: Installs the necessary Python packages including Arize Phoenix with evaluation and Llama Index integration, OpenAI SDK, and other utilities for data visualization and processing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qq \"arize-phoenix[eval,llama-index]\" \"openai>=1\" pyvis datasets pycm requests 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix and OpenAI Integration in Python\nDESCRIPTION: This snippet installs the necessary Python packages for running the Phoenix server, interfacing with OpenAI's API, handling asynchronous requests, working with DuckDB, datasets, and providing tracing instrumentation. Dependencies such as 'arize-phoenix', 'openai', 'httpx', 'duckdb', 'datasets', 'pyarrow', 'pydantic', 'nest_asyncio', and 'openinference-instrumentation-openai' are installed using pip. The installation is quiet (no output). Prerequisite: Python and pip must be installed. No input or output is expected.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"arize-phoenix>=4.6.0\" openai 'httpx<0.28' duckdb datasets pyarrow \"pydantic>=2.0.0\" nest_asyncio openinference-instrumentation-openai --quiet\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key\nDESCRIPTION: Checks for the 'OPENAI_API_KEY' environment variable; if missing, prompts the user for API key input and sets it as an environment variable. Enables the application to authenticate with OpenAI's API for language models and embeddings.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/project_sessions_llama_index_query_engine.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries (Python)\nDESCRIPTION: Imports various Python libraries essential for the script's functionality, including OS utilities, input handling, data manipulation (pandas), web scraping (requests, beautifulsoup4), LLM interaction (anthropic), OpenTelemetry instrumentation, and the Phoenix client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\nfrom itertools import chain\nfrom pprint import pp\nfrom textwrap import dedent\n\nimport pandas as pd\nimport requests\nfrom anthropic import Anthropic\nfrom anthropic.types.message_create_params import MessageCreateParamsBase\nfrom bs4 import BeautifulSoup\nfrom IPython.display import HTML, display\nfrom openinference.instrumentation.anthropic import AnthropicInstrumentor\n\nfrom phoenix.client import Client\nfrom phoenix.client.types import PromptVersion\nfrom phoenix.otel import register\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix OTEL Integration (Bash)\nDESCRIPTION: Installs the `arize-phoenix-otel` package using pip, which is required for OpenTelemetry-based tracing integration with Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai-agents-sdk.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Restarting Kernel for Package Installation\nDESCRIPTION: This code restarts the Jupyter kernel to ensure that the newly installed packages are available for use in the current runtime environment.  It's essential to run this cell after installing the packages to avoid import errors.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/google_agent_engine_tracing_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport IPython\n\napp = IPython.Application.instance()\napp.kernel.do_shutdown(True)\n```\n\n----------------------------------------\n\nTITLE: Compute NDCG@k\nDESCRIPTION: Computes Normalized Discounted Cumulative Gain (NDCG) at k, a metric used to evaluate the quality of the ranked results for each query. This code defines a function `_compute_ndcg` and applies it to the grouped dataframe. It uses `sklearn.metrics.ndcg_score` to calculate the NDCG score.  This indicates the effectiveness of the retrieval ranking.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom sklearn.metrics import ndcg_score\n\n\ndef _compute_ndcg(df: pd.DataFrame, k: int):\n    \"\"\"Compute NDCG@k in the presence of missing values\"\"\"\n    n = max(2, len(df))\n    eval_scores = np.zeros(n)\n    doc_scores = np.zeros(n)\n    eval_scores[: len(df)] = df.eval_score\n    doc_scores[: len(df)] = df.document_score\n    try:\n        return ndcg_score([eval_scores], [doc_scores], k=k)\n    except ValueError:\n        return np.nan\n\n\nndcg_at_2 = pd.DataFrame(\n    {\"score\": documents_with_relevance_df.groupby(\"context.span_id\").apply(_compute_ndcg, k=2)}\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluate Responses Using Phoenix's LLM Classification Templates\nDESCRIPTION: This snippet defines multiple evaluation templates for assessing whether the AI's responses and function calls are correct, matching responses against questions with detailed criteria. It then runs classification evaluations using Phoenix's llm_classify function with a specified model, Rails options, and concurrency settings for analyzing response correctness, function selection, and parameter extraction.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.evals import OpenAIModel, llm_classify\n\nrails = [\"incorrect\", \"correct\"]\n\nrouter_eval_df = llm_classify(\n    data=response_df,\n    template=ROUTER_EVAL_TEMPLATE,\n    model=OpenAIModel(model=\"gpt-4o\"),\n    rails=rails,\n    provide_explanation=True,\n    include_prompt=True,\n    concurrency=4,\n)\n\nfunction_selection_eval_df = llm_classify(\n    data=response_df,\n    template=FUNCTION_SELECTION_EVAL_TEMPLATE,\n    model=OpenAIModel(model=\"gpt-4o\"),\n    rails=rails,\n    provide_explanation=True,\n    include_prompt=True,\n    concurrency=4,\n)\n\nparameter_extraction_eval_df = llm_classify(\n    data=response_df,\n    template=PARAMETER_EXTRACTION_EVAL_TEMPLATE,\n    model=OpenAIModel(model=\"gpt-4o\"),\n    rails=rails,\n    provide_explanation=True,\n    include_prompt=True,\n    concurrency=4,\n)\n```\n\n----------------------------------------\n\nTITLE: Inspecting a Single Phoenix Experiment Run in Python\nDESCRIPTION: Retrieves the data associated with the first run (at index 0) within the `experiment` object. Accessing an element by index provides detailed information about that specific run, typically including the input data used, the output generated by the task function, trace IDs, timestamps, and potentially other metadata captured during execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nexperiment[0]\n```\n\n----------------------------------------\n\nTITLE: Instantiating GPT-4 Turbo Model for Q&A Classification\nDESCRIPTION: Creates an instance of the newer OpenAI GPT-4 Turbo model with zero temperature for the final model comparison in the evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model_name=\"gpt-4-turbo-preview\", temperature=0.0)\n```\n\n----------------------------------------\n\nTITLE: Defining an AI Task Function Using OpenAI GPT Model in Python\nDESCRIPTION: This snippet creates a task function that takes an example input from the dataset, formats a prompt, and uses the OpenAI GPT-4o model to generate a response. The function sends a chat-completion request and returns the first message content from the model's output. It requires the OpenAI SDK and Phoenix's Example type for structured input.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/quickstart-datasets.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom phoenix.experiments.types import Example\n\nopenai_client = OpenAI()\n\ntask_prompt_template = \"Answer in a few words: {question}\"\n\n\ndef task(example: Example) -> str:\n    question = example.input[\"question\"]\n    message_content = task_prompt_template.format(question=question)\n    response = openai_client.chat.completions.create(\n        model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": message_content}]\n    )\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Setting up Tracing with TracerProvider\nDESCRIPTION: This snippet demonstrates how to configure the tracer using `TracerProvider` from `openinference.instrumentation`. It sets up the span exporter, resource, and tracer provider, and retrieves the tracer instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor\n\nfrom openinference.instrumentation import TracerProvider\nfrom openinference.semconv.resource import ResourceAttributes\n\nendpoint = \"http://127.0.0.1:6006/v1/traces\"\nresource = Resource(attributes={ResourceAttributes.PROJECT_NAME: \"openinference-tracer\"})\ntracer_provider = TracerProvider(resource=resource)\ntracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\ntracer = tracer_provider.get_tracer(__name__)\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix with Docker\nDESCRIPTION: Docker commands to pull and run the Phoenix container. This allows running Phoenix in an isolated environment with port 6006 exposed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/dspy.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Client and Dependencies\nDESCRIPTION: Installs the necessary Python packages for Phoenix client, OpenAI, and instrumentation for monitoring. This ensures all dependencies are available for subsequent setup and API interactions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n%pip install -Uqqq arize-phoenix-client arize-phoenix-otel openai requests openinference-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Downloading and Sampling Help Center Articles with Pandas in Python\nDESCRIPTION: This snippet downloads a CSV file containing help center articles from a public URL, reads it into a pandas DataFrame, randomly samples one article, and displays it as HTML. Dependencies include 'pandas' (pd), 'HTML' from IPython.display, and network access to the specified URL. Input is the fixed URL to the CSV; the output is a rendered HTML table of sampled articles. This snippet prepares data for further processing by the LLM routine converter.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nurl = \"https://raw.githubusercontent.com/openai/openai-cookbook/a3e98ea4dcf866b5e7a3cb7d63dccaa68c7d63aa/examples/data/helpcenter_articles.csv\"\narticles = pd.read_csv(url).sample(1)\ndisplay(HTML(articles.to_html()))\n```\n\n----------------------------------------\n\nTITLE: Uploading Database Question Dataset\nDESCRIPTION: This code uploads the DataFrame containing database questions and expected results to Phoenix. The uploaded dataset will be used to assess the accuracy of SQL query generation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndataset = px_client.upload_dataset(\n    dataframe=questions_df,\n    dataset_name=f\"sales_db_lookup_questions_{id}\",\n    input_keys=[\"question\"],\n    output_keys=[\"expected_result\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Classification Report and Plotting Confusion Matrix in Python\nDESCRIPTION: Prints a classification report comparing true ground truth labels against predicted boolean classifications, focusing on labels True and False. Creates and plots a normalized confusion matrix with numbered labels and a blue color map, providing a visual performance summary of the classification results. Requires sklearn or similar evaluation tools along with matplotlib for plotting.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nprint(classification_report(true_labels, boolean_classifications, labels=[True, False]))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels.tolist(), predict_vector=boolean_classifications\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Counting Execution Statuses with exit_on_error=False (Python)\nDESCRIPTION: Uses `collections.Counter` to count the execution statuses from the run configured with `exit_on_error=False`. This count confirms that with this setting, no rows are left in a 'DID NOT RUN' state, as the pipeline attempts to process all rows and marks them with a specific failure or skipped status if necessary.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nCounter(all_evals[\"execution_status\"])\n```\n\n----------------------------------------\n\nTITLE: Wrapping production DataFrame and schema into Phoenix Inferences object\nDESCRIPTION: Creates an Inferences object for the production dataset to enable comparison with training data, supporting drift detection and performance analysis visualizations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/phoenix-inferences.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nprod_ds = px.Inferences(dataframe=prod_df, schema=prod_schema, name=\"production\")\n```\n\n----------------------------------------\n\nTITLE: Tracing Agents with Context Managers\nDESCRIPTION: This snippet demonstrates tracing an agent interaction using a context manager. It involves setting the input and output of the span, as well as the status code.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry.trace import Status, StatusCode\n\nwith tracer.start_as_current_span(\n    \"agent-span-with-plain-text-io\",\n    openinference_span_kind=\"agent\",\n) as span:\n    span.set_input(\"input\")\n    span.set_output(\"output\")\n    span.set_status(Status(StatusCode.OK))\n```\n\n----------------------------------------\n\nTITLE: Compute Precision at k\nDESCRIPTION: Computes precision at k (P@k) metrics for the retrieval quality. It uses relevance classifications (generated in the previous step) to calculate P@1 and P@2.  Dependencies include `query_df` and the `compute_precisions_at_k` function. The output is that the `query_df` is modified with new columns containing the calculated precision values.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/milvus_llamaindex_search_and_retrieval_tutorial.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfirst_document_relevances = [\n    {\"relevant\": True, \"irrelevant\": False}.get(rel)\n    for rel in query_df[\":tag.str:openai_relevance_0\"].tolist()\n]\nsecond_document_relevances = [\n    {\"relevant\": True, \"irrelevant\": False}.get(rel)\n    for rel in query_df[\":tag.str:openai_relevance_1\"].tolist()\n]\n\nlist_of_precisions_at_k_lists = [\n    compute_precisions_at_k([rel0, rel1])\n    for rel0, rel1 in zip(first_document_relevances, second_document_relevances)\n]\nprecisions_at_1, precisions_at_2 = [\n    [precisions_at_k[index] for precisions_at_k in list_of_precisions_at_k_lists]\n    for index in [0, 1]\n]\nquery_df[\":tag.float:openai_precision_at_1\"] = precisions_at_1\nquery_df[\":tag.float:openai_precision_at_2\"] = precisions_at_2\nquery_df[\n    [\n        \":tag.str:openai_relevance_0\",\n        \":tag.str:openai_relevance_1\",\n        \":tag.float:openai_precision_at_1\",\n        \":tag.float:openai_precision_at_2\",\n    ]\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Environment Variables (Command Line)\nDESCRIPTION: This Python code snippet sets up the necessary environment variables for connecting to the Arize Phoenix platform. `PHOENIX_COLLECTOR_ENDPOINT` specifies the address of the Phoenix server. These variables are crucial to ensure that the generated traces are correctly sent to your Phoenix instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex-1.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Executing a Sample SQL Query Against the In-Memory SQLite Database in Python\nDESCRIPTION: Opens a connection to the SQLite database engine, executes a SQL query selecting the first five rows from the 'cameras' table, and prints each row to the console. This verifies the data loading and SQL access functionality.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_sql_retriever_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nwith engine.connect() as connection:\n    result = connection.execute(text(\"SELECT * FROM cameras LIMIT 5\")).all()\n\n    for row in result:\n        print(row)\n```\n\n----------------------------------------\n\nTITLE: Installing Portpicker via pip in Python\nDESCRIPTION: Installs the 'portpicker' Python package using pip, which is used to programmatically allocate available network ports in later server setup steps. This must be executed before any code requiring portpicker, ensuring port conflicts are avoided in multi-process environments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/internal/run_experiments.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n!pip install portpicker\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Schema for Conversation Data\nDESCRIPTION: Imports the Phoenix library as `px` and defines a `Schema` object. This schema maps columns from the `conversations_df` DataFrame to specific roles within Phoenix, such as features, prompt text/vectors, and response text/vectors, enabling Phoenix to correctly interpret and visualize the data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\n# Define a Schema() object for Phoenix to pick up data from the correct columns for logging\nschema = px.Schema(\n    feature_column_names=[\n        \"step\",\n        \"conversation_id\",\n        \"api_call_duration\",\n        \"response_len\",\n        \"prompt_len\",\n    ],\n    prompt_column_names=px.EmbeddingColumnNames(\n        vector_column_name=\"prompt_vector\", raw_data_column_name=\"prompt\"\n    ),\n    response_column_names=px.EmbeddingColumnNames(\n        vector_column_name=\"response_vector\", raw_data_column_name=\"response\"\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Few-Shot Prompt Template Function for Model Completion in Python\nDESCRIPTION: Implements a Python function that takes an input dictionary containing a 'Review' key, uses the OpenAI client to create a chat completion based on the few-shot prompt, and returns the model's classification result as a clean string. The function depends on the 'few_shot_prompt' object created earlier and the OpenAI Python client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/few-shot-prompting.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef few_shot_prompt_template(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **few_shot_prompt.format(variables={\"Review\": input[\"Review\"]})\n    )\n    return resp.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Defining Text Embedding Schema with Raw Data Column in Arize Phoenix Python SDK\nDESCRIPTION: Creates a schema tailored for text embeddings that includes the actual label column, additional feature columns, tag columns, and an embedding feature containing both the vector and its associated raw text. This enables the Arize Phoenix platform to show both embedding vectors and their original text, improving interpretability in visualizations. Requires Arize Phoenix SDK and structured dataframe with corresponding columns.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/define-your-schema/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nschema = px.Schema(\n    actual_label_column_name=\"sentiment\",\n    feature_column_names=[\n        \"category\",\n    ],\n    tag_column_names=[\n        \"name\",\n    ],\n    embedding_feature_column_names={\n        \"product_review_embeddings\": px.EmbeddingColumnNames(\n            vector_column_name=\"text_vector\",\n            raw_data_column_name=\"text\",\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Running Example Scripts with the Phoenix Client\nDESCRIPTION: Command showing how to run example scripts provided with the Phoenix TypeScript client package.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/packages/phoenix-client/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npnpm install\npnpx tsx examples/list_datasets.ts\n# change the file name to run other examples\n```\n\n----------------------------------------\n\nTITLE: Executing and Fetching SQL Query Results with DuckDB in Python\nDESCRIPTION: Defines a function that executes an SQL query on the NBA DuckDB table and returns results as a list of Python dictionaries. It then demonstrates how to call this function with a previously generated query. Inputs: an SQL query string. Outputs: a list of results as dictionaries. Dependencies: duckdb connection object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef execute_query(query):\n    return conn.query(query).fetchdf().to_dict(orient=\"records\")\n\n\nexecute_query(query)\n```\n\n----------------------------------------\n\nTITLE: Named Inferences initialization in Python\nDESCRIPTION: Creates an Inferences object with a custom name that will appear in the Phoenix application. This example shows how to specify a name for better identification of the dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/inference-and-schema.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nds = px.Inferences(df, schema, name=\"training\")\n```\n\n----------------------------------------\n\nTITLE: Validating Task Execution on a Dataset Example Asynchronously in Python\nDESCRIPTION: Fetches the first example from the dataset and executes the asynchronous task function with the example's input. The output is serialized to JSON and shortened for console-friendly display. This confirms that the task function correctly integrates with the dataset and executes without errors.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nexample = dataset[0]\ntask_output = await task(example.input)\nprint(shorten(json.dumps(task_output), width=80))\n```\n\n----------------------------------------\n\nTITLE: Displaying First Few Rows of DataFrames\nDESCRIPTION: This snippet uses the `head()` method to display the first few rows of the `hallucination_eval` and `qa_correctness_eval` DataFrames. This is used for quick inspection of the evaluation results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nhallucination_eval.head()\n```\n\n----------------------------------------\n\nTITLE: Viewing Generated Questions\nDESCRIPTION: Displays the dataframe containing the generated questions to inspect the results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nquestions_df.head()\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Inferences class in Python\nDESCRIPTION: The Inferences class constructor that creates a collection of inferences from a pandas DataFrame and schema for analysis in Phoenix. It can be used for training, validation, test, or production data cohorts.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/inference-and-schema.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Inferences(\n    dataframe: pandas.DataFrame,\n    schema: Schema,\n    name: Optional[str] = None,\n)\n```\n\n----------------------------------------\n\nTITLE: Updating an Existing Prompt to a New Version in Python\nDESCRIPTION: This code explains how to update an existing prompt with a new version. By calling the create method with the same prompt name and new content, a new version is generated and the previous one is preserved, enabling iterative development.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/quickstart-prompts/quickstart-prompts-python.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncontent = \"\"\"\\nYou're an expert educator in {{ topic }}. Summarize the following article\\nin a few concise bullet points that are easy for beginners to understand.\n\nBe sure not to miss any key points.\n\n{{ article }}\"\"\"\n\nprompt_name = \"article-bullet-summarizer\"\nprompt = Client().prompts.create(\n    name=prompt_name,\n    prompt_description=\"Summarize an article in a few bullet points\",\n    version=PromptVersion(\n        [{\"role\": \"user\", \"content\": content}],\n        model_name=\"gpt-4o-mini\",\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages Using Shell\nDESCRIPTION: Installs required Python packages including the Phoenix AI SDK (version 4.6 or higher), multiple LangChain-related libraries, nest_asyncio for event loop management, and jarowinkler for string similarity computations. This command prepares the environment to support the following Python code for experiments and instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/langchain_email_extraction.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -Uqqq \"arize-phoenix>=4.6\" langchain langchain-core langchain-community langchain-benchmarks nest_asyncio jarowinkler\n```\n\n----------------------------------------\n\nTITLE: Checking Evaluator Functions\nDESCRIPTION: This code snippet checks that the defined evaluators can run successfully. It retrieves a single run from the experiment and gets the corresponding dataset example. It iterates through the defined evaluators, runs each one with the run's output and the example's input, then prints the function name and a shortened version of the evaluation results, which includes the scores and feedback returned by the evaluators.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nrun = experiment[0]\nexample = dataset.examples[run.dataset_example_id]\nfor fn in (answer_relevancy, context_relevancy):\n    _ = await fn(run.output, example.input)\n    print(fn.__qualname__)\n    print(shorten(json.dumps(_), width=80))\n```\n\n----------------------------------------\n\nTITLE: Adding Simple Event to Span - JavaScript\nDESCRIPTION: Shows how to add a basic event to a span using `span.addEvent()`. Events represent discrete points in time within a span's duration and are like simple logs attached to the trace.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_19\n\nLANGUAGE: javascript\nCODE:\n```\nspan.addEvent('Doing something');\n\nconst result = doWork();\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key via Environment Variable - Bash\nDESCRIPTION: This snippet shows how to set the OpenAI API key as an environment variable before launching the LLM application. The environment variable `OPENAI_API_KEY` is used by the backend to authenticate requests to the OpenAI API. Be sure to replace `sk-your-key-here` with your actual OpenAI API key. There are no additional dependencies other than Bash and access permissions for environment variable export. The variable must be exported in the shell session prior to running the application, otherwise API authentication will fail.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/manually-instrumented-chatbot/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=sk-your-key-here\n\n```\n\n----------------------------------------\n\nTITLE: Inspecting a Single Dataset Entry\nDESCRIPTION: Accesses and displays the structure and content of a single dataset example to understand data format and attributes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndataset[0]\n```\n\n----------------------------------------\n\nTITLE: Generating Sample Traces with OpenAI (Python)\nDESCRIPTION: Initializes the OpenAI client, defines a function `generate_joke` to make calls to the `gpt-3.5-turbo` model, and then calls this function five times in a loop. Because the OpenAI client is instrumented, each call to `client.chat.completions.create` generates a trace that is automatically sent to the connected Phoenix instance. The generated jokes are printed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evaluating_traces_quickstart.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n# Initialize OpenAI client\nclient = OpenAI()\n\n\n# Function to generate a joke\ndef generate_joke():\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates jokes.\"},\n            {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n        ],\n    )\n    joke = response.choices[0].message.content\n    return joke\n\n\n# Generate 5 different jokes\njokes = []\nfor _ in range(5):\n    joke = generate_joke()\n    jokes.append(joke)\n    print(f\"Joke {len(jokes)}:\\n{joke}\\n\")\n\nprint(f\"Generated {len(jokes)} jokes and tracked them in Phoenix.\")\n```\n\n----------------------------------------\n\nTITLE: Defining Reference Inference Set in Python\nDESCRIPTION: Creates a reference inference set to compare against a primary set, using a separate dataframe and schema. Used when comparing two different cohorts of data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/manage-the-app.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nref_ds = px.Inferences(ref_df, ref_schema, \"reference\")\n```\n\n----------------------------------------\n\nTITLE: Printing the Dataset Object (Python)\nDESCRIPTION: This snippet simply outputs the current state or contents of the 'dataset' object for inspection or debugging. It presumes 'dataset' has been previously defined or loaded.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\ndataset\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Packages (Docker)\nDESCRIPTION: This bash command installs the `arize-phoenix-otel` package. This package is required for instrumenting the application to send traces to the Phoenix backend.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/hfsmolagents.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Adding A-Frame Cylinder Primitive (HTML)\nDESCRIPTION: Defines a cylinder geometry in the 3D scene using the A-Frame primitive element `<a-cylinder>`. Attributes set its `position` (x, y, z coordinates), `radius`, `height`, and `color`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_4\n\nLANGUAGE: HTML\nCODE:\n```\n<a-cylinder position=\"1 0.75 -3\" radius=\"0.5\" height=\"1.5\" color=\"#FFC65D\"></a-cylinder>\n```\n\n----------------------------------------\n\nTITLE: Defining UI Components for Structured Output\nDESCRIPTION: Creates Pydantic models representing HTML UI components, including types, attributes, and hierarchical structure, to enable AI-generated UI components based on user prompts.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nclass _UIType(str, Enum):\n    div = \"div\"\n    button = \"button\"\n    header = \"header\"\n    section = \"section\"\n    field = \"field\"\n    form = \"form\"\n\nclass _Attribute(BaseModel):\n    name: str\n    value: str\n\nclass _UI(BaseModel):\n    type: _UIType\n    label: str\n    children: list[\"_UI\"]\n    attributes: list[\"_Attribute\"]\n\n_UI.model_rebuild()\n\nresponse_format = type_to_response_format_param(create_model(\"Response\", ui=(_UI, ...)))\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Applying Nest Asyncio Patch in Python\nDESCRIPTION: Imports essential libraries for AWS boto3 session management, asynchronous event loop patching with nest_asyncio, and Phoenix OTEL registration. The nest_asyncio.apply() call allows nested event loops typically required in Jupyter notebooks for asynchronous instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/amazon_bedrock_agents_tracing_and_evals.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\nfrom getpass import getpass\n\nimport boto3\nimport nest_asyncio\n\nfrom phoenix.otel import register\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Instrumenting a Custom LangChain Retriever for Phoenix Tracing in Python\nDESCRIPTION: Demonstrates setting up tracing for a custom LangChain component (a Retriever) to ensure it appears in Phoenix traces. It requires inheriting from the relevant LangChain base class (`BaseRetriever`), manually configuring the OpenTelemetry tracer provider and exporter (OTLPSpanExporter) pointing to the Phoenix collector endpoint, and instrumenting LangChain using `LangChainInstrumentor`. The example includes a basic custom retriever implementation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/concepts-tracing/faqs-tracing.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\n\nfrom langchain_core.callbacks import CallbackManagerForRetrieverRun\nfrom langchain_core.retrievers import BaseRetriever, Document\nfrom openinference.instrumentation.langchain import LangChainInstrumentor\nfrom opentelemetry import trace as trace_api\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk import trace as trace_sdk\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\n\nPHOENIX_COLLECTOR_ENDPOINT = \"http://127.0.0.1:6006/v1/traces\"\nendpoint = PHOENIX_COLLECTOR_ENDPOINT  # Added definition for endpoint variable\ntracer_provider = trace_sdk.TracerProvider()\ntrace_api.set_tracer_provider(tracer_provider)\ntracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n\nLangChainInstrumentor().instrument()\n\n\nclass CustomRetriever(BaseRetriever):\n    \"\"\"\n    This example is taken from langchain docs.\n    https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/custom_retriever/\n    A custom retriever that contains the top k documents that contain the user query.\n    This retriever only implements the sync method _get_relevant_documents.\n    If the retriever were to involve file access or network access, it could benefit\n    from a native async implementation of `_aget_relevant_documents`.\n    As usual, with Runnables, there's a default async implementation that's provided\n    that delegates to the sync implementation running on another thread.\n    \"\"\"\n\n    k: int\n    \"\"\"Number of top results to return\"\"\"\n\n    def _get_relevant_documents(\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n    ) -> List[Document]:\n        \"\"\"Sync implementations for retriever.\"\"\"\n        matching_documents: List[Document] = []\n\n        # Custom logic to find the top k documents that contain the query\n\n        for index in range(self.k):\n            matching_documents.append(Document(page_content=f\"dummy content at {index}\", score=1.0))\n        return matching_documents\n\n\nretriever = CustomRetriever(k=3)\n\n\nif __name__ == \"__main__\":\n    documents = retriever.invoke(\"what is the meaning of life?\")\n```\n\n----------------------------------------\n\nTITLE: Wrapping Production Data and Schema into Phoenix Inferences Object in Python\nDESCRIPTION: Packages the production DataFrame and its associated Schema into a Phoenix Inferences object labeled 'production'. This object is used for side-by-side or drift comparisons with primary datasets. Requires 'prod_df' DataFrame and 'prod_schema' Schema as inputs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/quickstart/phoenix-inferences/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprod_ds = px.Inferences(dataframe=prod_df, schema=prod_schema, name=\"production\")\n\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: This snippet installs the necessary Python packages for the tutorial, including the OpenAI library, Arize Phoenix, jsonschema, and OpenInference instrumentation for OpenAI, as well as an older version of httpx.  It is essential to set up the environment before running the rest of the code.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_tracing_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\n!pip install \"openai>=1.0.0\" arize-phoenix jsonschema openinference-instrumentation-openai 'httpx<0.28'\n```\n```\n\n----------------------------------------\n\nTITLE: Creating Database Questions DataFrame\nDESCRIPTION: This snippet creates a Pandas DataFrame with the defined database questions and their expected results. It's prepared for upload and subsequent evaluation in a Phoenix experiment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\n# Create a DataFrame with the questions\nquestions_df = pd.DataFrame({\"question\": db_lookup_questions, \"expected_result\": expected_results})\n\ndisplay(questions_df)\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix Docker Container\nDESCRIPTION: This bash command runs the Phoenix container, exposing port 6006 for access. This allows users to access the phoenix instance from their machine. The docker image will start a Phoenix instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/hfsmolagents.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Getting Phoenix UI URL Python\nDESCRIPTION: Accesses the `url` attribute of the `Session` object to get the web address for the Phoenix UI. This URL can be used to open the UI in a separate browser tab or window.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/api_reference/source/api/session.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nsession.url\n```\n\n----------------------------------------\n\nTITLE: Viewing Dataset as Dataframe\nDESCRIPTION: This snippet demonstrates how to view the uploaded dataset as a Pandas DataFrame using `dataset.as_dataframe()`. This allows for easier inspection and analysis of the dataset's contents within the Phoenix platform.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndataset.as_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Defining Example Inputs for Entity Extraction\nDESCRIPTION: Provides sample user inputs with context metadata for prompt testing and validation. These examples help simulate real user queries and facilitate prompt fine-tuning.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nexample_inputs = [\n    {\n        \"user_input\": \"I'm looking for a new coat. I'm always cold so please something warm! Ideally something that matches my eyes.\",\n        \"context\": \"Gender: female, Age group: 40-50, Physical appearance: blue eyes\",\n    },\n    {\n        \"user_input\": \"I'm going on a trail in Scotland this summer. It's goind to be rainy. Help me find something.\",\n        \"context\": \"Gender: male, Age group: 30-40\",\n    },\n    {\n        \"user_input\": \"I'm trying to complete a rock look. I'm missing shoes. Any suggestions?\",\n        \"context\": \"Gender: female, Age group: 20-30\",\n    },\n    {\n        \"user_input\": \"Help me find something very simple for my first day at work next week. Something casual and neutral.\",\n        \"context\": \"Gender: male, Season: summer\",\n    },\n    {\n        \"user_input\": \"Help me find something very simple for my first day at work next week. Something casual and neutral.\",\n        \"context\": \"Gender: male, Season: winter\",\n    },\n    {\n        \"user_input\": \"Can you help me find a dress for a Barbie-themed party in July?\",\n        \"context\": \"Gender: female, Age group: 20-30\",\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Defining Schema for Phoenix Analysis\nDESCRIPTION: Creates a schema that tells Phoenix how to interpret the dataframe columns, including timestamps, tags, prompt embeddings, and response embeddings.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_summarization_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nschema = px.Schema(\n    timestamp_column_name=\"prediction_timestamp\",\n    tag_column_names=[\n        \"rougeL_score\",\n        \"reference_summary\",\n    ],\n    prompt_column_names=px.EmbeddingColumnNames(\n        vector_column_name=\"article_vector\", raw_data_column_name=\"article\"\n    ),\n    response_column_names=px.EmbeddingColumnNames(\n        vector_column_name=\"summary_vector\", raw_data_column_name=\"summary\"\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Agent Prompt Template - Python\nDESCRIPTION: Constructs the `ChatPromptTemplate` that guides the agent's interactions. It defines the standard structure of messages, including a system message setting the agent's persona, a placeholder for the user's input, and a placeholder for the agent's internal scratchpad (thought process, tool calls, and observations).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_agent_tracing_tutorial.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a helpful assistant\"),\n        (\"human\", \"{input}\"),\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean of RAG Evaluation Dataframe\nDESCRIPTION: Calculates the mean of a Pandas DataFrame containing RAG evaluation scores, focusing on numeric columns. This provides an aggregate view of the retrieval performance metrics like NDCG and precision.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nresults = rag_evaluation_dataframe.mean(numeric_only=True)\nresults\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Instrumentation Packages\nDESCRIPTION: Command to install the OpenInference instrumentation for LangChain and the LangChain OpenAI integration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langchain.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-langchain langchain_openai\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix and Arize SDK Dependencies - Python\nDESCRIPTION: Installs the required Python packages 'arize-phoenix[embeddings]' and 'arize[AutoEmbeddings]' necessary for feature embedding generation and the Phoenix workflow. This snippet should be run in an interactive notebook environment or shell capable of pip installs. Ensure internet access and appropriate environment isolation (e.g., via virtualenv or conda) before running.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/credit_card_fraud_tutorial.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n%pip install -Uq \"arize-phoenix[embeddings]\" \"arize[AutoEmbeddings]\"\n```\n\n----------------------------------------\n\nTITLE: Querying and Exploding Retrieved Documents with Phoenix DSL - Python\nDESCRIPTION: This advanced snippet builds a SpanQuery to extract retrieved documents by exploding a nested list attribute within each RETRIEVER span. The query filters for RETRIEVER spans, selects input.value under a custom column, and explodes the retrieval.documents array to produce a DataFrame where each document's content and position are included. Helpful for RAG/IR evaluation. Requires phoenix.trace.dsl with recent version support for explode.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.trace.dsl import SpanQuery\n\nquery = SpanQuery().where(\n    # Filter for the `RETRIEVER` span kind.\n    # The filter condition is a string of valid Python boolean expression.\n    \"span_kind == 'RETRIEVER'\",\n).select(\n    # Extract the span attribute `input.value` which contains the query for the\n    # retriever. Rename it as the `input` column in the output dataframe.\n    input=\"input.value\",\n).explode(\n    # Specify the span attribute `retrieval.documents` which contains a list of\n    # objects and explode the list. Extract the `document.content` attribute from\n    # each object and rename it as the `reference` column in the output dataframe.\n    \"retrieval.documents\",\n    reference=\"document.content\",\n)\n\n# The Phoenix Client can take this query and return the dataframe.\npx.Client().query_spans(query)\n```\n\n----------------------------------------\n\nTITLE: Configuring - OpenAI API Key - Python\nDESCRIPTION: Configures the OpenAI API key required for using LLM-based features in Phoenix, such as LLM-based evaluators. It attempts to retrieve the key from the 'OPENAI_API_KEY' environment variable and prompts the user if it's not set. The key is then set for the 'openai' library and stored back in environment variables.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/datasets_and_experiments_quickstart.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport openai\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Importing Phoenix Library (Python)\nDESCRIPTION: This snippet imports the Phoenix library, making its functionalities available for use within the Python script. The `phoenix` library is essential for interacting with the Phoenix platform for trace analysis and related operations.  No specific parameters or inputs are needed for importing the library.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/tracing.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n```\n\n----------------------------------------\n\nTITLE: Applying Up-Migrations with Alembic Upgrade (Bash)\nDESCRIPTION: This Bash command applies all unapplied Alembic migrations to upgrade the database schema to the latest version ('head'). It requires Alembic and a configured database connection (via environment variable if non-default). The command outputs migration progress and logs to the terminal. If the upgrade fails, refer to downgrading or troubleshooting steps.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/src/phoenix/db/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nalembic upgrade head\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix for Interactive Analysis\nDESCRIPTION: Launches the Phoenix UI with the primary dataset (recent) and reference dataset (baseline) for interactive visual analysis of LLM performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_summarization_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n(session := px.launch_app(primary=recent_ds, reference=baseline_ds)).view()\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix Instrumentation in Python\nDESCRIPTION: Configures Phoenix tracing for LangChain or LangGraph applications by registering the OpenInference tracer with project name and auto-instrument option.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langgraph.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Computing Precision@2 for Retrieval Evaluation\nDESCRIPTION: Calculates the precision at 2 metric to measure the proportion of relevant documents among the top 2 retrieved documents.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nprecision_at_2 = pd.DataFrame(\n    {\n        \"score\": documents_with_relevance_df.groupby(\"context.span_id\").apply(\n            lambda x: x.eval_score[:2].sum(skipna=False) / 2\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Building VectorStoreIndex from Split Documents using Llama-Index in Python\nDESCRIPTION: Splits loaded documents into nodes using SentenceSplitter with a custom chunk size and overlap, then constructs a VectorStoreIndex for embedding search and retrieval. Accepts a list of documents and returns an index for similarity searches.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.node_parser import SentenceSplitter\n\n# Build index with a chunk_size of 1024\nsplitter = SentenceSplitter(chunk_size=1024, chunk_overlap=250)\nnodes = splitter.get_nodes_from_documents(documents)\nvector_index = VectorStoreIndex(nodes)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenInference Instrumentation for Production Tracing\nDESCRIPTION: Installs the OpenInference instrumentation package to enable detailed tracing of agent activities in production environments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/openai_agents_cookbook.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n!pip install openinference-instrumentation\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App with Primary Production and Reference Training Inferences using Python\nDESCRIPTION: Starts the Phoenix visualization session by providing both the production dataset as the primary inferences and the training dataset as the reference. This configuration enables users to analyze model drift and performance differences interactively by comparing live production against a training baseline.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/inferences_quickstart.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsession = px.launch_app(primary=prod_ds, reference=train_ds)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable\nDESCRIPTION: Defines and uses a helper function `_set_env` to check if a specified environment variable (like `OPENAI_API_KEY`) is set. If not, it prompts the user to enter the value using `getpass` and sets the environment variable.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\nimport os\n\ndef _set_env(key: str):\n    if key not in os.environ:\n        os.environ[key] = getpass.getpass(f\"{key}:\")\n\n\n_set_env(\"OPENAI_API_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Simulating OpenAI Calls (Development) Python\nDESCRIPTION: Defines a function `simulate_openai()` that simulates OpenAI API calls within a session. It allows setting a user and session ID. Simulates nested spans for API calls and incorporates the tree structure. Includes logging evaluations on root spans.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/multi-turn_chat_sessions.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef simulate_openai():\n    user_id = gen_user_id() if random() < 0.9 else \" \"\n    session_id = gen_session_id()\n    client = openai.Client(api_key=\"sk-\")\n    model = \"gpt-4o-mini\"\n    encoding = encoding_for_model(model)\n    messages = np.concatenate(convo.sample(randint(1, 10)).values)\n    counts = [len(encoding.encode(m[\"content\"])) for m in messages]\n    openai_mock = OpenAIMock()\n    tracer = tracer_provider.get_tracer(__name__)\n    with openai_mock.router:\n        for i in range(1, len(messages), 2):\n            openai_mock.chat.completions.create.response = dict(\n                choices=[dict(index=0, finish_reason=\"stop\", message=messages[i])],\n                usage=dict(\n                    prompt_tokens=sum(counts[:i]),\n                    completion_tokens=counts[i],\n                    total_tokens=sum(counts[: i + 1]),\n                ),\n            )\n            with ExitStack() as stack:\n                attributes = {\n                    \"input.value\": messages[i - 1][\"content\"],\n                    \"output.value\": messages[i][\"content\"],\n                }\n                if random() < 0.5:\n                    attributes[\"session.id\"] = session_id\n                    attributes[\"user.id\"] = user_id\n                else:\n                    stack.enter_context(using_session(session_id))\n                    stack.enter_context(using_user(user_id))\n                root = stack.enter_context(\n                    tracer.start_as_current_span(\n                        \"root\",\n                        attributes=attributes,\n                        end_on_exit=False,\n                    )\n                )\n                with trace_tree(tracer, tree_complexity):\n                    client.chat.completions.create(model=model, messages=messages[:i])\n            root.set_status(rand_status_code())\n            root.end(int(fake.future_datetime(\"+5s\").timestamp() * 10**9))\n\n```\n\n----------------------------------------\n\nTITLE: Setting up Phoenix Cloud environment variables\nDESCRIPTION: Python code for configuring environment variables to connect to Phoenix Cloud, including the API key and collector endpoint setup.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/guardrails-ai.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Initializing LangChain SQLDatabase Wrapper\nDESCRIPTION: Initializes a `SQLDatabase` instance from `langchain_community` to connect to the local `Chinook.db` SQLite file. It then prints the database dialect, lists the usable table names, and executes a sample SQL query to verify the connection and data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.utilities import SQLDatabase\n\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\nprint(db.dialect)\nprint(db.get_usable_table_names())\ndb.run(\"SELECT * FROM Artist LIMIT 10;\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix and OpenAI\nDESCRIPTION: This code snippet installs the necessary Python packages, including `arize-phoenix`, `openai`, `ipython`, `matplotlib`, `pycm`, `scikit-learn`, `tiktoken`, `nest_asyncio`, and `httpx`.  It ensures the environment has all required libraries for running the LLM-based code functionality evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_functionality_classifications.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qq arize-phoenix  \"openai>=1\" ipython matplotlib pycm scikit-learn tiktoken nest_asyncio 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI client for tracing (Python)\nDESCRIPTION: Applies instrumentation to the OpenAI library using the previously registered OpenTelemetry tracer provider. This enables automatic tracing of OpenAI API interactions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/feedback-and-annotations/evaluating-phoenix-traces.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Pip (Python)\nDESCRIPTION: Installs the necessary Python packages for integrating Arize Phoenix, the Groq client, and the OpenInference Groq instrumentation. Requires a Python environment with pip installed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/groq_tracing_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install -q openinference-instrumentation-groq groq arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Apply Response Generation Function to Questions and Build Response DataFrame\nDESCRIPTION: This snippet copies the questions DataFrame, applies a response generation function 'run_prompt' to each question, converts responses to strings, and stores them in a new DataFrame. It facilitates automated testing of responses against questions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nresponse_df = questions_df.copy(deep=True)\nresponse_df[\"response\"] = response_df[\"question\"].apply(run_prompt)\nresponse_df[\"response\"] = response_df[\"response\"].astype(str)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LlamaIndex and Phoenix Integration\nDESCRIPTION: Installs necessary Python packages for LlamaIndex, Phoenix, and related tools using pip. Sets up the environment for importing libraries required for building and tracing LlamaIndex applications with Phoenix support.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/project_sessions_llama_index_query_engine.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n%pip install -Uq \"arize-phoenix[llama-index]\" openinference-instrumentation-llama-index gcsfs faker\n```\n\n----------------------------------------\n\nTITLE: Listing Python Project Dependencies (requirements.txt format)\nDESCRIPTION: Specifies the Python package dependencies required for the Arize Phoenix project, likely in a 'requirements.txt' format. It includes core libraries like 'asyncpg', 'grpcio', 'openai', 'pandas', observability tools ('opentelemetry-*'), API frameworks ('fastapi', 'grpc', 'strawberry-graphql'), and various type stubs ('types-*'). Noteworthy inclusions are 'pypistats' and 'requests' specifically for type-checking third-party packages, and a pinned version of 'strawberry-graphql' due to project-specific monkey-patching. The file also incorporates dependencies listed in 'ci.txt'.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/requirements/type-check.txt#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-r ci.txt\nasyncpg\ngrpcio\nlitellm>=1.0.3\nopenai>=1.0.0\nopentelemetry-exporter-otlp\nopentelemetry-instrumentation-fastapi\nopentelemetry-instrumentation-grpc\nopentelemetry-instrumentation-sqlalchemy\nopentelemetry-proto>=1.12.0\nopentelemetry-sdk\nopentelemetry-semantic-conventions\npandas-stubs==2.0.3.230814\npandas>=1.0\nprometheus_client\npsycopg[binary,pool]\npy-grpc-prometheus\npypistats  # this is needed to type-check third-party packages\nrequests  # this is needed to type-check third-party packages\nstrawberry-graphql[opentelemetry]==0.262.5  # need to pin version because we're monkey-patching\ntenacity\ntypes-cachetools\ntypes-jsonschema\ntypes-protobuf\ntypes-psutil\ntypes-requests\ntypes-setuptools\ntypes-tabulate\ntypes-tqdm\n```\n\n----------------------------------------\n\nTITLE: Counting Execution Statuses (Python)\nDESCRIPTION: Uses `collections.Counter` to aggregate and display the counts of each unique value in the `execution_status` column of the results dataframe from the default error handling run. This summarizes the distribution of success, failure, and skipped states across the evaluated rows.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nCounter(evals_with_exception_info[\"execution_status\"])\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Up Dependencies\nDESCRIPTION: Imports core modules from Python standard library and third-party packages required for making requests, handling models, and integrating with Phoenix and OpenAI APIs. Sets up the environment for efficient prompt management and response handling.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport json\nimport os\nfrom enum import Enum\nfrom getpass import getpass\nfrom itertools import chain\nfrom textwrap import dedent\n\nimport openai\nimport pandas as pd\nimport requests\nfrom IPython.display import HTML, display\nfrom openai import OpenAI\nfrom openai.lib._parsing import type_to_response_format_param\nfrom openai.types.chat.completion_create_params import CompletionCreateParamsBase\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nfrom pydantic import BaseModel, create_model\n\nfrom phoenix.client import Client\nfrom phoenix.client.types import PromptVersion\nfrom phoenix.otel import register\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAI Function Schema for Travel Request Attributes in Python\nDESCRIPTION: This code defines a JSON Schema and an OpenAI function configuration to extract three structured attributesâ€”location, budget level, and trip purposeâ€”from unstructured travel requests. The schema uses specific enumerations and descriptions for each field, enabling the OpenAI model to return validated, structured output. The schema also specifies required fields and includes a system message to guide the model.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_tracing_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nparameters_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"location\": {\n            \"type\": \"string\",\n            \"description\": 'The desired destination location. Use city, state, and country format when possible. If no destination is provided, return \"unstated\".',\n        },\n        \"budget_level\": {\n            \"type\": \"string\",\n            \"enum\": [\"low\", \"medium\", \"high\", \"not_stated\"],\n            \"description\": 'The desired budget level. If no budget level is provided, return \"not_stated\".',\n        },\n        \"purpose\": {\n            \"type\": \"string\",\n            \"enum\": [\"business\", \"pleasure\", \"other\", \"non_stated\"],\n            \"description\": 'The purpose of the trip. If no purpose is provided, return \"not_stated\".',\n        },\n    },\n    \"required\": [\"location\", \"budget_level\", \"purpose\"],\n}\ntool_schema = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"record_travel_request_attributes\",\n        \"description\": \"Records the attributes of a travel request\",\n        \"parameters\": parameters_schema,\n    },\n}\nsystem_message = (\n    \"You are an assistant that parses and records the attributes of a user's travel request.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Permanently Disabling Phoenix Auto-Instrumentation in Python\nDESCRIPTION: Shows how to permanently remove tracing by calling the `.uninstrument()` method on specific auto-instrumentors provided by Phoenix, such as `LangChainInstrumentor`, `LlamaIndexInstrumentor`, and `OpenAIInstrumentor`. This stops the respective library integrations from generating further traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/concepts-tracing/faqs-tracing.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nLangChainInstrumentor().uninstrument()\nLlamaIndexInstrumentor().uninstrument()\nOpenAIInstrumentor().uninstrument()\n# etc.\n```\n\n----------------------------------------\n\nTITLE: Retrieve Retrieved Documents for Evaluation - Python\nDESCRIPTION: Imports `phoenix` and the `get_retrieved_documents` helper function. Creates a Phoenix client instance. Calls `get_retrieved_documents` with the client and the project name to fetch a DataFrame containing information about documents retrieved during the RAG pipeline runs, which is needed for retrieval evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\nfrom phoenix.session.evaluation import get_retrieved_documents\n\n# Get the retrieved documents from Phoenix using this helper function\nretrieved_documents_df = get_retrieved_documents(px.Client(), project_name=phoenix_project_name)\n\nretrieved_documents_df\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenAI API Tracing with OpenInference and Phoenix in Python\nDESCRIPTION: This snippet enables tracing of OpenAI API calls by instrumenting the OpenAI client with OpenInference and Phoenix's OpenTelemetry tools, allowing tracking and observability of all model interactions. It imports and initializes the tracer by registering it via Phoenix, then instruments the OpenAI API calls to include tracing. Dependencies include openinference-instrumentation-openai and Phoenix with OpenTelemetry support. There is no direct input or output.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register()\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Evaluating GPT-3.5 Turbo Q&A Classification Results\nDESCRIPTION: Evaluates GPT-3.5 Turbo's classification performance with the same metrics and visualization as used for GPT-4 for direct comparison.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df_sample[\"answer_true\"].map(QA_PROMPT_RAILS_MAP).tolist()\nclasses = list(QA_PROMPT_RAILS_MAP.values())\n\nprint(classification_report(true_labels, Q_and_A_classifications, labels=classes))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels, predict_vector=Q_and_A_classifications, classes=classes\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Tracing Chains with Context Managers\nDESCRIPTION: This snippet demonstrates tracing a chain using a context manager. It sets the input and output of the span and the status code.  It leverages the opentelemetry Status and StatusCode objects.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry.trace import Status, StatusCode\n\nwith tracer.start_as_current_span(\n    \"chain-span-with-plain-text-io\",\n    openinference_span_kind=\"chain\",\n) as span:\n    span.set_input(\"input\")\n    span.set_output(\"output\")\n    span.set_status(Status(StatusCode.OK))\n```\n\n----------------------------------------\n\nTITLE: Importing and Configuring DSPy\nDESCRIPTION: This code sets up the DSPy library for prompt optimization. It imports the `dspy` library, then creates an OpenAI language model (`turbo`) and configures DSPy to use it. The DSPy LM is responsible for generating the text, and it's used to classify prompts and make predictions in the provided code. This sets up the necessary environment for prompt optimization using DSPy.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\n\n# Configure DSPy to use OpenAI\nturbo = dspy.LM(model=\"gpt-3.5-turbo\")\ndspy.settings.configure(lm=turbo)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LlamaIndex, MongoDB, and Phoenix\nDESCRIPTION: Installs necessary Python packages using the `uv` package manager via `pip`. The packages include LlamaIndex core and specific integrations for OpenAI embeddings, Arize Phoenix callbacks, MongoDB vector store, document store, index store, and readers, along with OpenAI client, MongoDB driver (`pymongo`), and other utilities.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_with_mongodb_and_llama_index.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install -q uv\n!uv pip install -q --system llama-index-embeddings-openai 'arize-phoenix[evals]' llama-index llama-index-callbacks-arize-phoenix llama-index-vector-stores-mongodb llama-index-storage-docstore-mongodb llama-index-storage-index-store-mongodb llama-index-readers-mongodb\n!uv pip install -q --system \"openai>=1\" gcsfs nest-asyncio pymongo beautifulsoup4 certifi 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application Server\nDESCRIPTION: Starts the Phoenix frontend or API server to enable trace visualization and management. This is necessary for monitoring traced sessions and interactions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/project_sessions_llama_index_query_engine.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Defining - Task Function - Python\nDESCRIPTION: Defines a Python function ('task') that simulates the core operation or generation process being evaluated. This function takes input (from the dataset), formats a prompt, interacts with an OpenAI model (gpt-4o) via the Chat Completion API, and returns the model's response. This represents the 'System Under Test'.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/datasets_and_experiments_quickstart.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nopenai_client = OpenAI()\n\ntask_prompt_template = \"Answer in a few words: {question}\"\n\n\ndef task(input) -> str:\n    question = input[\"question\"]\n    message_content = task_prompt_template.format(question=question)\n    response = openai_client.chat.completions.create(\n        model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": message_content}]\n    )\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Computing Precision@k (Pandas)\nDESCRIPTION: This snippet computes precision at a specified rank `k=2` by calculating the sum of the first `k` relevance scores in the 'eval_score' column of the DataFrame and dividing by `k`. It handles potentially missing values and is applied to a grouped DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nprecision_at_2 = pd.DataFrame(\n    {\n        \"score\": documents_with_relevance_df.groupby(\"context.span_id\").apply(\n            lambda x: x.eval_score[:2].sum(skipna=False) / 2\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Logging QA Evals to Phoenix\nDESCRIPTION: Logs QA correctness and hallucination evaluation results to Phoenix using `phoenix.trace.SpanEvaluations`.  This allows for visualization and analysis of the LLM's performance within the Phoenix UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import SpanEvaluations\n\npx.Client().log_evaluations(\n    SpanEvaluations(dataframe=qa_correctness_eval_df, eval_name=\"Q&A Correctness\"),\n    SpanEvaluations(dataframe=hallucination_eval_df, eval_name=\"Hallucination\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Git LFS (Shell)\nDESCRIPTION: Installs the Git Large File Storage (LFS) extension, which is necessary to download large files like the dataset used in this tutorial. This command is executed directly in the environment, typically a notebook cell prefixing with `!`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n!git lfs install\n```\n\n----------------------------------------\n\nTITLE: Signing the Contributor License Agreement (CLA) via PR Comment (Text)\nDESCRIPTION: A textual statement required to be posted as a comment on the first pull request submitted by a contributor. This signifies agreement to the project's Contributor License Agreement (CLA), which is necessary for the pull request to be accepted. While presented as text, it's the action required for CLA compliance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/CONTRIBUTING.md#_snippet_7\n\nLANGUAGE: Text\nCODE:\n```\nI have read the CLA Document and I hereby sign the CLA\n```\n\n----------------------------------------\n\nTITLE: Fetching HaluEval QA Data with Python\nDESCRIPTION: Downloads the HaluEval QA dataset from a URL, reads it line by line, parses each line as JSON, and loads the records into a Pandas DataFrame for subsequent processing. Requires the `pandas`, `json`, `os`, and `urllib.request` libraries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_halueval.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nfrom urllib.request import urlopen\n\nimport pandas as pd\n\npd.set_option(\"display.max_colwidth\", None)\n\ndata_url = \"https://raw.githubusercontent.com/RUCAIBox/HaluEval/main/data/qa_data.json\"\nrecords = []\nwith urlopen(data_url) as url:\n    for line in url.readlines():\n        records.append(json.loads(line))\ndf = pd.DataFrame(records)\ndf\n```\n\n----------------------------------------\n\nTITLE: Querying Anthropic Chat API with Phoenix Prompt Utilities in Python\nDESCRIPTION: This snippet demonstrates fetching a prompt version from the Phoenix client, formatting it for Anthropic's message schema, and making a chat completion request using the anthropic Python SDK. Anthropic API credentials must be configured via environment variables. Accepts a prompt_version_id and variables dict for question insertion. Outputs the full response from Anthropic as JSON.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/examples/prompts/prompts_for_various_SDKs.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprompt_version_id = \"UHJvbXB0VmVyc2lvbjo0\"\nprompt_version = Client().prompts.get(prompt_version_id=prompt_version_id)\nprint(f\"prompt_version = {prompt_version.model_dump_json(indent=2)}\\n{'-'*100}\")\n\nmessages, kwargs = to_chat_messages_and_kwargs(\n    prompt_version, variables={\"question\": \"Who made you?\"}\n)\nprint(f\"messages = {json.dumps(messages, indent=2)}\\n{'-'*100}\")\nprint(f\"kwargs = {json.dumps(kwargs, indent=2)}\\n{'-'*100}\")\n\nresponse = anthropic.Anthropic().messages.create(messages=messages, **kwargs)\nprint(f\"response = {response.model_dump_json(indent=2)}\")\n```\n\n----------------------------------------\n\nTITLE: Python: Send Feedback Annotation to Phoenix REST API\nDESCRIPTION: This code constructs and sends an annotation payload to Phoenixâ€™s /v1/span_annotations endpoint using httpx in Python. It includes the span ID, feedback details, and necessary headers for authentication. Dependencies include the httpx library and the OpenTelemetry SDK.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/feedback-and-annotations/capture-feedback.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport httpx\n\nclient = httpx.Client()\n\nannotation_payload = {\n    \"data\": [\n        {\n            \"span_id\": span_id,\n            \"name\": \"user feedback\",\n            \"annotator_kind\": \"HUMAN\",\n            \"result\": {\"label\": \"thumbs-up\", \"score\": 1},\n            \"metadata\": {},\n        }\n    ]\n}\n\nheaders = {'api_key': '<your phoenix api key>'}\n\nclient.post(\n    \"https://app.phoenix.arize.com/v1/span_annotations?sync=false\",\n    json=annotation_payload,\n    headers=headers\n)\n```\n\n----------------------------------------\n\nTITLE: Starting Main Agent Span - Python\nDESCRIPTION: The `start_main_span` function initiates the agent run. It starts a tracing span using `tracer.start_as_current_span` with the name \"AgentRun\". It sets the input and output values for the span using the provided messages and the result of `run_agent` respectively.  The span is marked with a status of OK.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\ndef start_main_span(messages):\n    print(\"Starting main span with messages:\", messages)\n\n    with tracer.start_as_current_span(\"AgentRun\", openinference_span_kind=\"agent\") as span:\n        span.set_input(value=messages)\n        ret = run_agent(messages)\n        print(\"Main span completed with return value:\", ret)\n        span.set_output(value=ret)\n        span.set_status(StatusCode.OK)\n        return ret\n```\n\n----------------------------------------\n\nTITLE: Defining and Configuring a Haystack Pipeline (Python)\nDESCRIPTION: Demonstrates creating a simple Haystack question-answering pipeline. It initializes a `Pipeline`, an `OpenAIGenerator` (using GPT-3.5 Turbo), and a `PromptBuilder` with a specified template. The components are added to the pipeline and connected. This pipeline, when run after setting up Phoenix auto-instrumentation, will automatically generate traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/haystack.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom haystack import Pipeline\nfrom haystack.components.generators import OpenAIGenerator\nfrom haystack.components.builders.prompt_builder import PromptBuilder\n\nprompt_template = \"\"\"\nAnswer the following question.\nQuestion: {{question}}\nAnswer:\n\"\"\"\n\n# Initialize the pipeline\npipeline = Pipeline()\n\n# Initialize the OpenAI generator component\nllm = OpenAIGenerator(model=\"gpt-3.5-turbo\")\nprompt_builder = PromptBuilder(template=prompt_template)\n\n# Add the generator component to the pipeline\npipeline.add_component(\"prompt_builder\", prompt_builder)\npipeline.add_component(\"llm\", llm)\npipeline.connect(\"prompt_builder\", \"llm\")\n\n# Define the question\nquestion = \"What is the location of the Hanging Gardens of Babylon?\"\n```\n\n----------------------------------------\n\nTITLE: Defining Question Generation Template\nDESCRIPTION: Creates a prompt template for generating test questions based on document chunks using an LLM.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ngenerate_questions_template = \"\"\"\\\nContext information is below.\n\n---------------------\n{text}\n---------------------\n\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n\nYou are a Teacher/ Professor. Your task is to setup \\\n3 questions for an upcoming \\\nquiz/examination. The questions should be diverse in nature \\\nacross the document. Restrict the questions to the \\\ncontext information provided.\"\n\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Loading Questions Dataset from Parquet File on GCS in Python\nDESCRIPTION: Reads a Parquet file containing RAG questions into a pandas DataFrame using a previously constructed GCS URL. This DataFrame, questions_df, is used for subsequent querying. Inputs are the URL and file name, and output is a DataFrame displayed in notebook.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nquestions_file = \"demo_llama_index_rag_questions.parquet\"\nquestions_df = pd.read_parquet(f\"{url}/{questions_file}\")\nquestions_df\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAIModel for Azure OpenAI - Python\nDESCRIPTION: Shows how to instantiate the OpenAIModel for use with Azure OpenAI by supplying Azure-specific parameters such as the deployment model name, endpoint URL, and API version. The 'model' parameter must match an active deployment engine in Azure; otherwise, initialization will fail. Azure authentication may require additional fields like 'azure_endpoint', 'api_version', and access tokens as needed. The input consists of credential and deployment configuration details; the output is a model instance configured to use Azure OpenAI endpoints.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evaluation-models.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(\n    model=\"gpt-35-turbo-16k\",\n    azure_endpoint=\"https://arize-internal-llm.openai.azure.com/\",\n    api_version=\"2023-09-15-preview\",\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Sampling Data for Few-Shot Examples in Python\nDESCRIPTION: Loads the 'syeddula/fridgeReviews' dataset using the Hugging Face `datasets` library and samples 10 random examples into a pandas DataFrame. These examples will be used within the few-shot prompt to provide context to the language model. Requires the `datasets` and `pandas` libraries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/few_shot_prompting.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nds = load_dataset(\"syeddula/fridgeReviews\")[\"test\"]\nfew_shot_examples = ds.to_pandas().sample(10)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable - Python\nDESCRIPTION: Checks if the `OPENAI_API_KEY` environment variable is already set. If not, it prompts the user securely using `getpass` to enter the API key and sets it as an environment variable for subsequent use by the OpenAI client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/summarization.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif os.environ.get(\"OPENAI_API_KEY\") is None:\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Method patching example for OpenAI chat client with tracing (Python)\nDESCRIPTION: Demonstrates how to patch the `create` method of the OpenAI chat client to include tracing functionality. The function `tracer.llm` wraps the original method, and the patched method replaces the existing client method, allowing seamless tracing during invocation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import OpenAI\nfrom openai.types.chat import ChatCompletionMessageParam\n\nopenai_client = OpenAI()\n\n# patch the create method\nwrapper = tracer.llm(\n    process_input=process_input,\n    process_output=process_output,\n)\nopenai_client.chat.completions.create = wrapper(openai_client.chat.completions.create)\n\n# invoke the patched method normally\nopenai_client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Travel Requests Python\nDESCRIPTION: Defines a list of example travel requests in natural language. These requests will be used as input to the structured data extraction service to test the extraction process.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/anthropic_tracing_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntravel_requests = [\n    \"Can you recommend a luxury hotel in Tokyo with a view of Mount Fuji for a romantic honeymoon?\",\n    \"I'm looking for a mid-range hotel in London with easy access to public transportation for a solo backpacking trip. Any suggestions?\",\n    \"I need a budget-friendly hotel in San Francisco close to the Golden Gate Bridge for a family vacation. What do you recommend?\",\n    \"Can you help me find a boutique hotel in New York City with a rooftop bar for a cultural exploration trip?\",\n    \"I'm planning a business trip to Tokyo and I need a hotel near the financial district. What options are available?\",\n    \"I'm traveling to London for a solo vacation and I want to stay in a trendy neighborhood with great shopping and dining options. Any recommendations for hotels?\",\n    \"I'm searching for a luxury beachfront resort in San Francisco for a relaxing family vacation. Can you suggest any options?\",\n    \"I need a mid-range hotel in New York City with a fitness center and conference facilities for a business trip. Any suggestions?\",\n    \"I'm looking for a budget-friendly hotel in Tokyo with easy access to public transportation for a backpacking trip. What do you recommend?\",\n    \"I'm planning a honeymoon in London and I want a luxurious hotel with a spa and romantic atmosphere. Can you suggest some options?\",\n]\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry and Related Packages - Shell\nDESCRIPTION: Shows the npm command to install OpenTelemetry, Vercel OpenTelemetry bridge, and semantic convention dependencies required to process and export telemetry spans. Ensures all observability-related libraries are available to instrument Vercel apps with OpenInference and OpenTelemetry.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/vercel-ai-sdk.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm i @opentelemetry/api @vercel/otel @opentelemetry/exporter-trace-otlp-proto @arizeai/openinference-semantic-conventions\n```\n\n----------------------------------------\n\nTITLE: Defining Custom LangChain SQL Query Tool\nDESCRIPTION: Defines a custom tool `db_query_tool` using the `@tool` decorator. This tool executes a provided SQL query against the database instance `db` using `run_no_throw`, which safely handles errors. It returns the query results or a generic error message if execution fails.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\n\n\n@tool\ndef db_query_tool(query: str) -> str:\n    \"\"\"\n    Execute a SQL query against the database and get back the result.\n    If the query is not correct, an error message will be returned.\n    If an error is returned, rewrite the query, check the query, and try again.\n    \"\"\"\n    result = db.run_no_throw(query)\n    if not result:\n        return \"Error: Query failed. Please rewrite your query and try again.\"\n    return result\n\n\nprint(db_query_tool.invoke(\"SELECT * FROM Artist LIMIT 10;\"))\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix and Arize SDK Dependencies\nDESCRIPTION: Installs the necessary packages including arize-phoenix with embeddings support and the Arize SDK with AutoEmbeddings and LLM_Evaluation modules.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_summarization_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uq \"arize-phoenix[embeddings]\" \"arize[AutoEmbeddings, LLM_Evaluation]\"\n```\n\n----------------------------------------\n\nTITLE: Python Project Dependencies (requirements.txt)\nDESCRIPTION: This file lists Python packages required by the arize-ai/phoenix project. It uses pip's requirements file format to specify packages like `Faker`, `arize`, `asyncpg`, `httpx`, `litellm`, `numpy`, `openai`, `pandas`, `protobuf`, `psycopg`, testing frameworks (`pytest-postgresql`, `responses`, `respx`, `vcrpy`), and utility libraries. Version constraints are defined for several packages (e.g., `litellm>=1.0.3,<1.57.5`, `protobuf==3.20.2`), and some dependencies (`aiohttp`, `urllib3`) are conditional based on the Python version (`python_version < \"3.10\"`). The `-r ci.txt` line indicates inclusion of dependencies from another file, likely for continuous integration environments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/requirements/unit-tests.txt#_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n-r ci.txt\nFaker>=30.1.0\narize\nasgi-lifespan\nasyncpg\ngrpc-interceptor[testing]\nhttpx\nhttpx-ws\nlitellm>=1.0.3,<1.57.5\nnest-asyncio # for executor testing\nnumpy\nopenai>=1.0.0\npandas-stubs==2.0.3.230814\npandas>=1.0\nprotobuf==3.20.2  # version minimum (for tests)\npsycopg[binary,pool]\npyarrow-stubs\npytest-postgresql\nresponses\nrespx>=0.22.0\nsyrupy\ntenacity\ntiktoken\ntypes-pytz\ntyping-extensions\nvcrpy\naiohttp>=3.0; python_version < \"3.10\"\nurllib3<2.0; python_version < \"3.10\"\n```\n\n----------------------------------------\n\nTITLE: Tracing LLM Streaming Responses\nDESCRIPTION: This snippet demonstrates tracing streaming LLM responses using a decorator on an async generator function. It captures the streaming output as it becomes available. Requires the OpenAI API key to be set as an environment variable.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import AsyncGenerator, List\n\nfrom openai import AsyncOpenAI\nfrom openai.types.chat import ChatCompletionMessageParam\n\nopenai_async_client = AsyncOpenAI()\n\n\n@tracer.llm\nasync def stream_llm_responses(\n    messages: List[ChatCompletionMessageParam],\n) -> AsyncGenerator[str, None]:\n    stream = await openai_async_client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        stream=True,\n    )\n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            yield chunk.choices[0].delta.content\n\n\n# invoke inside of an async context\nasync for token in stream_llm_responses([{\"role\": \"user\", \"content\": \"Hello, world!\"}]):\n    print(token, end=\"\")\n```\n\n----------------------------------------\n\nTITLE: Running LLM-Based Relevance Evaluations in Python\nDESCRIPTION: Uses OpenAI to evaluate whether retrieved documents are relevant to their corresponding queries. The function classifies each document as either 'relevant' or 'irrelevant' and adds these evaluations to the query dataframe.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_search_and_retrieval_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nevals_model_name = \"gpt-3.5-turbo\"\n# evals_model_name = \"gpt-4\"  # use GPT-4 if you have access\nquery_texts = sample_query_df[\":feature.text:prompt\"].tolist()\nlist_of_document_id_lists = sample_query_df[\":feature.[str].retrieved_document_ids:prompt\"].tolist()\ndocument_id_to_text = dict(zip(database_df[\"document_id\"].to_list(), database_df[\"text\"].to_list()))\n\nfirst_document_texts, second_document_texts = [\n    [\n        document_id_to_text[document_ids[document_index]]\n        for document_ids in list_of_document_id_lists\n    ]\n    for document_index in [0, 1]\n]\nfirst_document_relevances, second_document_relevances = [\n    [\n        classify_relevance(query_text, document_text, evals_model_name)\n        for query_text, document_text in zip(query_texts, first_document_texts)\n    ]\n    for document_texts in [first_document_texts, second_document_texts]\n]\n\n\nsample_query_df = sample_query_df.assign(\n    retrieved_document_text_0=first_document_texts,\n    retrieved_document_text_1=second_document_texts,\n    relevance_0=first_document_relevances,\n    relevance_1=second_document_relevances,\n)\nsample_query_df[\n    [\n        \":feature.text:prompt\",\n        \"retrieved_document_text_0\",\n        \"retrieved_document_text_1\",\n        \"relevance_0\",\n        \"relevance_1\",\n    ]\n].rename(columns={\":feature.text:prompt\": \"query_text\"})\n```\n\n----------------------------------------\n\nTITLE: Cleaning Jupyter Notebooks\nDESCRIPTION: Command to clean Jupyter notebooks by removing cell outputs and metadata. This keeps the diff small when contributing notebooks and is required for passing CI checks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ntox run -e clean_jupyter_notebooks\n```\n\n----------------------------------------\n\nTITLE: Configuring Console Exporter with Conditional Processing in Python\nDESCRIPTION: Initializes the OpenTelemetry `TracerProvider`. It then creates a `ConsoleSpanExporter` to print spans to the console and registers it with the provider using the custom `ConditionalSpanProcessor`, passing the `console_condition` function to ensure only matching spans are exported to the console.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/span_filtering_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntracer_provider = TracerProvider()\n\n# Create the Console exporter\nconsole_exporter = ConsoleSpanExporter()\n\n# Add the Console exporter to the tracer provider\ntracer_provider.add_span_processor(ConditionalSpanProcessor(console_exporter, console_condition))\n```\n\n----------------------------------------\n\nTITLE: Registering Tracer Provider for Instrumentation\nDESCRIPTION: Sets up OpenAI instrumentation with Phoenix's tracing provider to enable observability and monitoring of API calls. Enhances observability and diagnostics for AI interactions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ntracer_provider = register()\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Generating Item Search Response using OpenAI and OpenTelemetry\nDESCRIPTION: This function generates a response for a purchase question by using the OpenAI API.  It takes the user payload as a JSON string, the item search prompt, and an OpenTelemetry tracer. It starts a span named \"Item Search Response\", loads the user payload, extracts the customer input, calls the OpenAI API, updates the user payload, sets the span type as \"CHAIN\", sets the status code, and returns the updated payload as a JSON string.  It depends on the `openai` and `opentelemetry` packages.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndef item_search_response(\n    user_payload_json: str,\n    item_search_prompt: str,\n    tracer: opentelemetry.sdk.trace.Tracer,\n) -> str:\n    \"\"\"Query response when customer asks a purchase question\n\n    Parameters\n    ----------\n    user_payload_json : str\n        JSON formatted string for prompt template input\n    item_search_prompt : str\n        Item search prompt template\n    tracer : opentelemetry.sdk.trace.Tracer\n        Tracer to handle span creation\n\n    Returns\n    -------\n    str\n        JSON formatted string of item search payload\n    \"\"\"\n    # Define Span Name & Start\n    with tracer.start_as_current_span(\"Item Search Response\") as span:\n        user_payload_dict = json.loads(user_payload_json)\n        customer_input = user_payload_dict.get(\"Customer Input\", \"\")\n        response_dict = call_openai_api(item_search_prompt, customer_input)\n        user_payload_dict.update(response_dict)\n\n        # Define Span Type as \"CHAIN\"\n        span.set_attribute(\n            SpanAttributes.OPENINFERENCE_SPAN_KIND, OpenInferenceSpanKindValues.CHAIN.value\n        )\n\n        # Set Status Code\n        span.set_status(trace_api.StatusCode.OK)\n\n        return json.dumps(user_payload_dict)\n```\n\n----------------------------------------\n\nTITLE: Installing Necessary Python Packages for Evaluation Framework\nDESCRIPTION: Installs dependencies including arize-phoenix-evals, openai SDK, and utilities like matplotlib, scikit-learn, tiktoken, etc. Ensures the environment has all required packages for dataset handling, model interaction, and evaluation visualization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n!pip install -qq \"arize-phoenix-evals>=0.0.5\" \"openai>=1\" ipython matplotlib pycm scikit-learn tiktoken nest_asyncio 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Installing DSPy and Dependencies\nDESCRIPTION: This command installs the DSPy library, used for prompt optimization and its dependencies. It downloads and installs `dspy` and `openinference-instrumentation-dspy` using pip. DSPy provides tools and functionalities to optimize prompts, while `openinference-instrumentation-dspy` is used to monitor and trace the calls made to DSPy.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q dspy openinference-instrumentation-dspy\n```\n\n----------------------------------------\n\nTITLE: Running the OpenAI Agent and Retrieving Results\nDESCRIPTION: Executes the math solver agent against a sample problem and displays the result, final output, and complete conversation history.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/openai_agents_cookbook.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresult = await Runner.run(agent, \"what is 15 + 28?\")\n\n# Run Result object\nprint(result)\n\n# Get the final output\nprint(result.final_output)\n\n# Get the entire list of messages recorded to generate the final output\nprint(result.to_input_list())\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenTelemetry Tracing for Hosted Phoenix\nDESCRIPTION: Configures environment variables for Phoenix client headers and collector endpoint, and registers OpenTelemetry tracer provider. Instruments OpenAI API calls to collect tracing data automatically.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_openai_tutorial.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nfrom phoenix.otel import register\n\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n\n# Setup OTEL tracing for hosted Phoenix. The register function will automatically detect the endpoint and headers from your environment variables.\ntracer_provider = register()\n\n# Turn on instrumentation for OpenAI\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider, skip_dep_check=True)\n```\n\n----------------------------------------\n\nTITLE: Importing necessary modules in Python\nDESCRIPTION: This snippet imports required modules including json, tempfile, textwrap, time, typing, nest_asyncio, phoenix, llama_index and opentelemetry for experiment tracking, evaluation and instrumentation. It ensures all necessary libraries are available for subsequent code execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport tempfile\nfrom textwrap import shorten\nfrom time import time_ns\nfrom typing import Tuple\n\nimport nest_asyncio\nimport phoenix as px\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.core.evaluation import AnswerRelevancyEvaluator, ContextRelevancyEvaluator\nfrom llama_index.core.llama_dataset import download_llama_dataset\nfrom llama_index.llms.openai import OpenAI\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom phoenix.experiments import evaluate_experiment, run_experiment\nfrom phoenix.experiments.types import Explanation, Score\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Exporting Trace Data for Evaluation\nDESCRIPTION: Uses the Phoenix client (`px.Client()`) to export specific data structures from the active session into pandas DataFrames. `get_retrieved_documents` exports information about retrieved documents, and `get_qa_with_reference` exports query and reference answer data. These dataframes are used as input for running evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_quickstart.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n\nretrieved_documents_df = get_retrieved_documents(px.Client())\nqueries_df = get_qa_with_reference(px.Client())\n```\n\n----------------------------------------\n\nTITLE: Defining Anthropic Evaluation Function and Running Experiment in Python\nDESCRIPTION: Defines a function that uses the Anthropic SDK to send prompt completions and returns a label. Runs the experiment using Phoenix's run_experiment utility to evaluate on the dataset, collecting prediction results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef anthropic_eval(input):\n    formatted_prompt = prompt.format(variables=dict(input), sdk=\"anthropic\")\n    response = anthropic.Anthropic().messages.create(\n        **{**formatted_prompt, \"model\": anthropic_model}\n    )\n    return {\"label\": response.content[0].text}\n```\n\n----------------------------------------\n\nTITLE: Running SQL Query Generation Experiment\nDESCRIPTION: This code runs a Phoenix experiment to evaluate SQL query generation. It takes the dataset, the `run_sql_query` task, and the `evaluate_sql_result` evaluator. The experiment assesses how well the agent generates correct SQL queries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nexperiment = run_experiment(\n    dataset,\n    run_sql_query,\n    evaluators=[evaluate_sql_result],\n    experiment_name=\"SQL Query Eval\",\n    experiment_description=\"Evaluating the SQL query generation step of the agent\",\n)\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix Tracer and Instrumenting OpenAI - Python\nDESCRIPTION: Initializes the Phoenix tracer for the agent's project, then instruments OpenAI client calls so they are automatically traced within the application. Builds a tracer object for manual or decorator-based tracing with OpenInference. Requires established environment and valid Phoenix API key.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntracer_provider = register(\n    project_name=project_name,\n)\n\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n\ntracer = tracer_provider.get_tracer(__name__)\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix in Docker Container - Docker\nDESCRIPTION: Executes the Phoenix Docker container and maps host port 6006 to the container for access on localhost. This starts a local Phoenix instance suitable for development or evaluation. Ports must be free and Docker service running.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/litellm.md#_snippet_5\n\nLANGUAGE: docker\nCODE:\n```\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix OTEL Python Package\nDESCRIPTION: Installs the Phoenix OTEL Python package which provides a lightweight wrapper around OpenTelemetry primitives with Phoenix-specific defaults for tracing. This command assumes Python and pip are already installed and configured.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Setting Up Bedrock Model and Running LLM-Based Tool Call Classification in Python\nDESCRIPTION: Initializes a BedrockModel instance with the boto3 session and specific Claude model ID for LLM evaluation. Runs the classification function `llm_classify` using a prompt template, rails for prompt engineering, and enables explanation. Computes a numeric score derived from classification label correctness for each trace.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/amazon_bedrock_agents_tracing_and_evals.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nrails = list(TOOL_CALLING_PROMPT_RAILS_MAP.values())\n\neval_model = BedrockModel(session=session, model_id=\"anthropic.claude-3-5-haiku-20241022-v1:0\")\n\nresponse_classifications = llm_classify(\n    data=trace_df,\n    template=TOOL_CALLING_PROMPT_TEMPLATE,\n    model=eval_model,\n    rails=rails,\n    provide_explanation=True,\n)\nresponse_classifications[\"score\"] = response_classifications.apply(\n    lambda x: 1 if x[\"label\"] == \"correct\" else 0, axis=1\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating summary using MapReducer\nDESCRIPTION: This snippet defines prompts for a MapReduce evaluation strategy using `PromptTemplate` and then initializes a `MapReducer` evaluator. The prompts guide the language model to evaluate the generated summary against different chunks of the original data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/long_context_summary_evals.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n)\nmap_prompt_template = PromptTemplate(\n    \"You will be given a CONTEXT that contains multiple documents. \"\n    \"You will also be given a SUMMARY that summarizes the documents in the CONTEXT in addition to other (unseen) documents. \"\n    \"You must provide an EVALUATION of the quality of the SUMMARY relative to the provided CONTEXT. \"\n    \"Your EVALUATION should judge the quality of the SUMMARY and should concisely explain your reasoning. \"\n    \"Bear in mind that the SUMMARY may include information from unseen documents. \"\n    \"Focus on important points, not trivial details.\"\n    \"\\n\\n\"\n    \"=======\"\n    f\"SUMMARY: {summary}\"\n    \"\\n\\n\"\n    \"=======\"\n    \"\\n\\n\"\n    \"CONTEXT: {chunk}\"\n    \"\\n\\n\"\n    \"=======\"\n    \"\\n\\n\"\n    \"EVALUATION: \"\n)\nreduce_prompt_template = PromptTemplate(\n    \"You will be given a SUMMARY that summarizes a large number of documents. \"\n    \"You will also be given a list of EVALUATIONS of the quality of that SUMMARY. \"\n    \"Each evaluation judges the SUMMARY relative to a different subset of the documents it summarizes. \"\n    \"Given this list, you must provide a single, OVERALL EVALUATION of the quality of the SUMMARY that should take into account the individual EVALUATIONS. \"\n    'Your OVERALL EVALUATION should judge the quality of the SUMMARY as either \"good\" or \"bad\" and should only contain one of those two words with no additional explanation.'\n    \"\\n\\n\"\n    \"=======\"\n    \"\\n\\n\"\n    f\"SUMMARY: {summary}\"\n    \"\\n\\n\"\n    \"=======\"\n    \"\\n\\n\"\n    \"EVALUATIONS: {mapped}\"\n    \"\\n\\n\"\n    \"=======\"\n    \"\\n\\n\"\n    \"OVERALL EVALUATION: \"\n)\nevaluator = MapReducer(\n    model=model,\n    map_prompt_template=map_prompt_template,\n    reduce_prompt_template=reduce_prompt_template,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Running Model-Based Evaluations\nDESCRIPTION: This snippet initializes a language model (e.g., GPT-4) for evaluation purposes and creates specific evaluators for hallucinations, question-answer correctness, and relevance. It runs evaluations on the respective DataFrames, with explanations enabled, capturing the evaluation results into new DataFrames for further analysis or visualization. Dependencies include language models and custom evaluator classes, supporting automated response quality assessments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_with_mongodb_and_llama_index.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\neval_model = OpenAIModel(\\n    model=\"gpt-4o\",\\n)\\nhallucination_evaluator = HallucinationEvaluator(eval_model)\\nqa_correctness_evaluator = QAEvaluator(eval_model)\\nrelevance_evaluator = RelevanceEvaluator(eval_model)\\n\\nhallucination_eval_df, qa_correctness_eval_df = run_evals(\\n    dataframe=queries_df,\\n    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\\n    provide_explanation=True,\\n)\\nrelevance_eval_df = run_evals(\\n    dataframe=retrieved_documents_df,\\n    evaluators=[relevance_evaluator],\\n    provide_explanation=True,\\n)[0]\\n\\npx.Client().log_evaluations(\\n    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\\n    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df),\\n)\\npx.Client().log_evaluations(DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df))\n```\n\n----------------------------------------\n\nTITLE: Create Dataset from Hugging Face Data\nDESCRIPTION: This code loads data from Hugging Face, samples a subset, and uploads it as a dataset to Phoenix. It utilizes the `abisee/cnn_dailymail` dataset and specifies the input and output keys for the data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/summarization.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nfrom datasets import load_dataset\n\nhf_ds = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\ndf = (\n    hf_ds[\"test\"]\n    .to_pandas()\n    .sample(n=10, random_state=0)\n    .set_index(\"id\")\n    .rename(columns={\"highlights\": \"summary\"})\n)\nnow = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\ndataset = px.Client().upload_dataset(\n    dataset_name=f\"news-article-summaries-{now}\",\n    dataframe=df,\n    input_keys=[\"article\"],\n    output_keys=[\"summary\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Implement Self-Consistency CoT Task and Evaluation (Python)\nDESCRIPTION: Defines a task function `self_consistency_COT_prompt` that calls the OpenAI API using the previously created Phoenix prompt, parses the response to extract the final answer from the last line, and handles potential Markdown formatting. It also defines a standard evaluation function `evaluate_response` to check if the extracted answer matches the expected integer answer. Finally, it runs an experiment using the dataset, the task function, and the evaluator via `run_experiment`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/chain-of-thought-prompting.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef self_consistency_COT_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **self_consistency_COT.format(variables={\"Problem\": input[\"Word Problem\"]})\n    )\n    response_text = resp.choices[0].message.content.strip()\n    lines = response_text.split(\"\\n\")\n    final_answer = lines[-1].strip()\n    final_answer = re.sub(r\"^\\*\\*(\\d+)\\*\\*$\", r\"\\1\", final_answer)\n    return {\"full_response\": response_text, \"final_answer\": final_answer}\n\n\ndef evaluate_response(output, expected):\n    final_answer = output[\"final_answer\"]\n    if not final_answer.isdigit():\n        return False\n    return int(final_answer) == int(expected[\"Answer\"])\n\n\ninitial_experiment = run_experiment(\n    dataset,\n    task=self_consistency_COT_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Self Consistency COT Prompt\",\n    experiment_name=\"self-consistency-cot-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + self_consistency_COT.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Simple Scoring Functions for Text2SQL Output Evaluation in Python\nDESCRIPTION: Provides two simple evaluator functions: no_error (returns 1.0 if no SQL error occurred, else 0.0), and has_results (returns 1.0 if query returned at least one result). They accept an output dictionary containing 'error' and 'results' keys. No dependencies or side-effects. Inputs: output dictionary from text2sql. Outputs: numeric scores.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Test if there are no sql execution errors\n\ndef no_error(output):\n    return 1.0 if output.get(\"error\") is None else 0.0\n\n\n# Test if the query has results\ndef has_results(output):\n    results = output.get(\"results\")\n    has_results = results is not None and len(results) > 0\n    return 1.0 if has_results else 0.0\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Initializing Notebook Environment - Python\nDESCRIPTION: Loads core dependencies required for data processing, LlamaIndex operations, tracing, and OpenAI integration. Applies nest_asyncio for reentrant event loops, crucial for interactive notebook contexts. No direct input/output, but establishes global dependencies for the rest of the workflow. Prerequisites: Python 3.8+, installed dependencies from pip cell.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/run_experiments_with_llama_index.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\nfrom datetime import datetime, timezone\nfrom time import sleep\nfrom urllib.request import urlretrieve\n\nimport nest_asyncio\nimport pandas as pd\nimport phoenix as px\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.postprocessor import SentenceTransformerRerank\nfrom llama_index.core.settings import Settings\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk import trace as trace_sdk\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom phoenix.evals import OpenAIModel\nfrom phoenix.experiments import run_experiment\nfrom phoenix.experiments.evaluators import ConcisenessEvaluator\nfrom phoenix.experiments.types import EvaluationResult, Example, ExperimentRun\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Querying Gemini (Google Generative AI) Chat API with Phoenix Prompt Utilities in Python\nDESCRIPTION: This snippet utilizes the Google Generative AI (Gemini) Python SDK and the Phoenix client to send a chat prompt using specified variables. Requires Gemini API client (google-generativeai), and credentials must be set. The prompt_version_id specifies which prompt template to use; messages are split for the Gemini schema. It returns and prints the complete model response.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/examples/prompts/prompts_for_various_SDKs.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprompt_version_id = \"UHJvbXB0VmVyc2lvbjo1\"\nprompt_version = Client().prompts.get(prompt_version_id=prompt_version_id)\nprint(f\"prompt_version = {prompt_version.model_dump_json(indent=2)}\\n{'-'*100}\")\n\nmessage, kwargs = to_chat_messages_and_kwargs(\n    prompt_version, variables={\"question\": \"Who made you?\"}\n)\nprint(f\"messages = {json.dumps(messages, indent=2)}\\n{'-'*100}\")\nprint(f\"kwargs = {json.dumps(kwargs, indent=2)}\\n{'-'*100}\")\n\nresponse = GenerativeModel(**kwargs).start_chat(history=messages[:-1]).send_message(messages[-1])\nprint(f\"response = {response}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Inspect Inference Schema - Python\nDESCRIPTION: This snippet retrieves and displays the schema of the primary inference set. It accesses the `schema` attribute of the `prim_ds` (primary dataset) object, providing information about the column types and roles.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/use-example-inferences.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprim_ds.schema\n```\n\n----------------------------------------\n\nTITLE: Initializing LangGraph Application for Agent Engine\nDESCRIPTION: This code defines a `SimpleLangGraphApp` class, the core structure of the LangGraph application. It sets up initialization logic and query mechanisms. It also includes instrumentation and tool binding configurations. Dependencies include `phoenix.otel`, `ChatVertexAI`, `MessageGraph`, and defined functions like `get_product_details` and `router` (not shown).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/google_agent_engine_tracing_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleLangGraphApp:\n    def __init__(self, project: str, location: str) -> None:\n        self.project_id = project\n        self.location = location\n\n    # The set_up method is used to define application initialization logic\n    def set_up(self) -> None:\n        # Phoenix code begins\n        from phoenix.otel import register\n\n        register(\n            project_name=\"google-agent-framework-langgraph\",  # name this to whatever you would like\n            auto_instrument=True,  # this will automatically call all openinference libraries (e.g. openinference-instrumentation-langchain)\n        )\n        # Phoenix code ends\n\n        model = ChatVertexAI(model=\"gemini-2.0-flash\")\n\n        builder = MessageGraph()\n\n        model_with_tools = model.bind_tools([get_product_details])\n        builder.add_node(\"tools\", model_with_tools)\n\n        tool_node = ToolNode([get_product_details])\n        builder.add_node(\"get_product_details\", tool_node)\n        builder.add_edge(\"get_product_details\", END)\n\n        builder.set_entry_point(\"tools\")\n        builder.add_conditional_edges(\"tools\", router)\n\n        self.runnable = builder.compile()\n\n    # The query method will be used to send inputs to the agent\n    def query(self, message: str):\n        \"\"\"Query the application.\n\n        Args:\n            message: The user message.\n\n        Returns:\n            str: The LLM response.\n        \"\"\"\n        chat_history = self.runnable.invoke(HumanMessage(message))\n\n        return chat_history[-1].content\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Applying Asyncio Patch in Python\nDESCRIPTION: Imports standard and third-party Python libraries required for dataset handling (pandas, tempfile, datetime), string similarity (jarowinkler), asynchronous event loop patching (nest_asyncio), Phoenix SDK, LangChain components, and OpenTelemetry tracing exporters. Calls nest_asyncio.apply() to allow nested event loops, which is necessary for asynchronous frameworks inside environments like Jupyter or Colab.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/langchain_email_extraction.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport tempfile\nfrom datetime import datetime, timezone\n\nimport jarowinkler\nimport nest_asyncio\nimport pandas as pd\nimport phoenix as px\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\nfrom langchain_benchmarks import download_public_dataset, registry\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom openinference.instrumentation.langchain import LangChainInstrumentor\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom phoenix.experiments import evaluate_experiment, run_experiment\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Creating UI Generation Prompt and Saving in Phoenix\nDESCRIPTION: Defines a prompt to generate UI components via OpenAI, registers it in Phoenix for reuse, and formats the output as structured UI models to be rendered dynamically.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nparams = CompletionCreateParamsBase(\n    messages=[{\"role\": \"user\", \"content\": \"Generate form for {{feature}}\"}],\n    model=\"gpt-4o-mini\",\n    response_format=response_format,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix Python Package\nDESCRIPTION: This snippet installs the `arize-phoenix` Python package. This package provides the necessary tools and dependencies for integrating Phoenix with the application. It is a prerequisite for using Phoenix's features, including feedback collection and span annotation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/human_feedback/chatbot_with_human_feedback.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Defining Tool Calling Match Evaluator\nDESCRIPTION: This function defines an evaluator to check whether the output of the `run_router_step` function matches the expected tool calls from the ground truth dataset. It compares the expected tool calls string with the agent's tool calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndef tools_match(expected: str, output: str) -> bool:\n    expected_tools = expected.get(\"tool_calls\").split(\", \")\n    return expected_tools == output\n```\n\n----------------------------------------\n\nTITLE: Loading production inference dataset into pandas DataFrame\nDESCRIPTION: Loads additional production inference data into a pandas DataFrame from a specified URL for comparison or drift analysis, similar to the training dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/phoenix-inferences.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nprod_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/cv/human-actions/human_actions_training.parquet\"\n)\n\nprod_df.head()\n```\n\n----------------------------------------\n\nTITLE: Instrumenting LLM SDKs with OpenTelemetry in Python\nDESCRIPTION: This snippet configures OpenTelemetry tracing for Anthropic, OpenAI, VertexAI, and Bedrock SDKs, exporting trace data to a specified OTLP receiver. It uses tracer providers and span processors to record telemetry for API calls to each provider. Dependencies are opentelemetry-sdk, opentelemetry-instrumentation, openinference, and the respective provider SDKs. The 'endpoint' variable should point to a running OTLP-compatible trace collector. This enables detailed tracing for debugging and observability.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/examples/prompts/prompts_for_various_SDKs.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.anthropic import AnthropicInstrumentor\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nfrom openinference.instrumentation.vertexai import VertexAIInstrumentor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.instrumentation.bedrock import BedrockInstrumentor\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\n\nendpoint = \"http://127.0.0.1:4317\"\ntracer_provider = TracerProvider()\ntracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n\nAnthropicInstrumentor().instrument(tracer_provider=tracer_provider)\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\nBedrockInstrumentor().instrument(tracer_provider=tracer_provider)\nVertexAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Setting Evaluation Sample Size (Python)\nDESCRIPTION: Defines a constant representing the number of data points to be sampled from the dataset for the evaluation run. This helps control the size of the test set and manage the execution time of the notebook examples.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nN_EVAL_SAMPLE_SIZE = 40\n```\n\n----------------------------------------\n\nTITLE: Defining GeminiModel Parameters - Python\nDESCRIPTION: Specifies the GeminiModel class definition and associated fields for accessing and configuring Google's Gemini LLMs. Inputs may include project ID, GCP location, and credentials, or use default environment settings. Parameters like 'model', 'default_concurrency', 'temperature', 'max_tokens', 'top_p', and 'top_k' let you control generation characteristics, with outputs resulting from model completions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evaluation-models.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass GeminiModel:\n    project: Optional[str] = None\n    location: Optional[str] = None\n    credentials: Optional[\"Credentials\"] = None\n    model: str = \"gemini-pro\"\n    default_concurrency: int = 5\n    temperature: float = 0.0\n    max_tokens: int = 256\n    top_p: float = 1\n    top_k: int = 32\n```\n\n----------------------------------------\n\nTITLE: Launching Arize Phoenix Application - Python\nDESCRIPTION: Imports the `phoenix` library and calls `px.launch_app()` to start the Phoenix UI. The output of this call will provide instructions to access the UI in a web browser, allowing visualization of datasets and experiments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/summarization.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport phoenix as px\n\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Install Phoenix Evals\nDESCRIPTION: This bash script installs the necessary Python packages for using Phoenix Evals. It installs arize-phoenix, openai, nest_asyncio, and a specific version of httpx.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evals_quickstart.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n%%bash\npip install -q \"arize-phoenix>=4.29.0\"\npip install -q openai nest_asyncio 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Tracing Agents with Decorators\nDESCRIPTION: This snippet showcases tracing an agent function using a decorator. The function takes a string input and returns a string output. Input and output attributes are automatically set.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@tracer.agent\ndef decorated_agent(input: str) -> str:\n    return \"output\"\n\ndecorated_agent(\"input\")\n```\n\n----------------------------------------\n\nTITLE: Test Query Generation Python\nDESCRIPTION: Calls the asynchronous `generate_query` function with a sample natural language question ('Who won the most games?') and prints the resulting SQL query generated by the LLM. This serves as a quick check to see if the query generation process is working as expected.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nquery = await generate_query(\"Who won the most games?\")\nprint(query)\n```\n\n----------------------------------------\n\nTITLE: Launching Local Phoenix Server via CLI (Bash)\nDESCRIPTION: Installs the core `arize-phoenix` package and starts a local Phoenix server using the `phoenix serve` command. This is suitable for local development and testing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/haystack.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Creating Few-Shot Prompt Template with Examples and Running Summarization in Python\nDESCRIPTION: Constructs a few-shot prompt template by incorporating five example articles and summaries sampled from a Hugging Face dataset. The examples illustrate the desired summary style to provide context to the language model. The template uses placeholders to inject example content and new input articles. Outputs the filled prompt template for debugging or inspection. Requires pandas, a Hugging Face dataset, and proper renaming of dataset columns. This snippet improves summary conciseness and quality by example-driven prompting.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/summarization.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# examples to include (not included in the uploaded dataset)\ntrain_df = (\n    hf_ds[\"train\"]\n    .to_pandas()\n    .sample(n=5, random_state=42)\n    .head()\n    .rename(columns={\"highlights\": \"summary\"})\n)\n\nexample_template = \"\"\"\nARTICLE\n=======\n{article}\n\nSUMMARY\n=======\n{summary}\n\"\"\"\n\nexamples = \"\\n\".join(\n    [\n        example_template.format(article=row[\"article\"], summary=row[\"summary\"])\n        for _, row in train_df.iterrows()\n    ]\n)\n\ntemplate = \"\"\"\nSummarize the article in two to four sentences. Be concise and include only the most important information, as in the examples below.\n\nEXAMPLES\n========\n\n{examples}\n\n\nNow summarize the following article.\n\nARTICLE\n=======\n{article}\n\nSUMMARY\n=======\n\"\"\"\n\ntemplate = template.format(\n    examples=examples,\n    article=\"{article}\",\n)\nprint(template)\n```\n\n----------------------------------------\n\nTITLE: Defining LangGraph ToolNode Fallback Utilities\nDESCRIPTION: Defines two utility functions for LangGraph: `create_tool_node_with_fallback` which wraps a list of tools in a `ToolNode` with a fallback handler, and `handle_tool_error` which generates a `ToolMessage` containing an error description to inform the agent when a tool execution fails.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any\n\nfrom langchain_core.messages import ToolMessage\nfrom langchain_core.runnables import RunnableLambda, RunnableWithFallbacks\nfrom langgraph.prebuilt import ToolNode\n\n\ndef create_tool_node_with_fallback(tools: list) -> RunnableWithFallbacks[Any, dict]:\n    \"\"\"\n    Create a ToolNode with a fallback to handle errors and surface them to the agent.\n    \"\"\"\n    return ToolNode(tools).with_fallbacks(\n        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n    )\n\n\ndef handle_tool_error(state) -> dict:\n    error = state.get(\"error\")\n    tool_calls = state[\"messages\"][-1].tool_calls\n    return {\n        \"messages\": [\n            ToolMessage(\n                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n                tool_call_id=tc[\"id\"],\n            )\n            for tc in tool_calls\n        ]\n    }\n\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application\nDESCRIPTION: Starts the Phoenix web application, providing a visual interface for experiment management and monitoring. The app view is rendered interactively.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\npx.launch_app().view()\n```\n\n----------------------------------------\n\nTITLE: Defining Prompt Template for Routine Generation with LLMs in Python\nDESCRIPTION: This snippet defines a detailed prompt template (CONVERSION_PROMPT) for instructing an LLM to convert customer service documents into programmatic routines. The template specifies formatting requirements, organization of actions, how to handle conditionals, function calls, and compliance. The 'params' object is constructed with a model selection and prompt messages for later use with the LLM API. Dependencies include 'CompletionCreateParamsBase' and 'dedent'. The input is a customer service policy, and the output is expected to be a routine in a specific step-wise format, ready for execution by an LLM. This snippet is foundational, as it shapes all subsequent LLM-driven policy conversions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nCONVERSION_PROMPT = \"\"\"\\\nYou are a helpful assistant tasked with taking an external facing help center article and converting it into a internal-facing programmatically executable routine optimized for an LLM.\nThe LLM using this routine will be tasked with reading the policy, answering incoming questions from customers, and helping drive the case toward resolution.\n\nPlease follow these instructions:\n1. **Review the customer service policy carefully** to ensure every step is accounted for. It is crucial not to skip any steps or policies.\n2. **Organize the instructions into a logical, step-by-step order**, using the specified format.\n3. **Use the following format**:\n   - **Main actions are numbered** (e.g., 1, 2, 3).\n   - **Sub-actions are lettered** under their relevant main actions (e.g., 1a, 1b).\n      **Sub-actions should start on new lines**\n   - **Specify conditions using clear 'if...then...else' statements** (e.g., 'If the product was purchased within 30 days, then...').\n   - **For instructions that require more information from the customer**, provide polite and professional prompts to ask for additional information.\n   - **For actions that require data from external systems**, write a step to call a function using backticks for the function name (e.g., `call the check_delivery_date function`).\n      - **If a step requires the customer service agent to take an action** (e.g., process a refund), generate a function call for this action (e.g., `call the process_refund function`).\n      - **Define any new functions** by providing a brief description of their purpose and required parameters.\n   - **If there is an action an assistant can performon behalf of the user**, include a function call for this action (e.g., `call the change_email_address function`), and ensure the function is defined with its purpose and required parameters.\n      - This action may not be explicitly defined in the help center article, but can be done to help the user resolve their inquiry faster\n   - **The step prior to case resolution should always be to ask if there is anything more you can assist with**.\n   - **End with a final action for case resolution**: calling the `case_resolution` function should always be the final step.\n4. **Ensure compliance** by making sure all steps adhere to company policies, privacy regulations, and legal requirements.\n5. **Handle exceptions or escalations** by specifying steps for scenarios that fall outside the standard policy.\n\n**Important**: If at any point you are uncertain, respond with \"I don't know.\"\n\nPlease convert the customer service policy into the formatted routine, ensuring it is easy to follow and execute programmatically.\\\n\"\"\"\n\nparams = CompletionCreateParamsBase(\n    model=\"o3-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": dedent(CONVERSION_PROMPT)},\n        {\"role\": \"user\", \"content\": \"POLICY:\\n\\n{{content}}\"},\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Async generator for streaming OpenAI chat responses with tracing (Python)\nDESCRIPTION: An asynchronous function `stream_llm_response` wraps OpenAI's asynchronous chat completion API, yielding streamed `ChatCompletionChunk` objects. The function is decorated with `tracer.llm` with custom input and output processors, enabling detailed tracing of streamed responses during asynchronous iteration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import AsyncGenerator\n\nfrom openai import AsyncOpenAI\nfrom openai.types.chat import ChatCompletionChunk\n\nopenai_async_client = AsyncOpenAI()\n\n@tracer.llm(\n    process_input=process_input,  # same as before\n    process_output=process_generator_output,\n)\nasync def stream_llm_response(\n    messages: List[ChatCompletionMessageParam],\n    model: str,\n    temperature: Optional[float] = None,\n) -> AsyncGenerator[ChatCompletionChunk, None]:\n    async for chunk in await openai_async_client.chat.completions.create(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        stream=True,\n    ):\n        yield chunk\n\nasync for chunk in stream_llm_response(\n    messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n    temperature=0.5,\n    model=\"gpt-4\",\n):\n    print(chunk)\n```\n\n----------------------------------------\n\nTITLE: Computing Precision Metrics for RAG Evaluation in Python\nDESCRIPTION: Calculates precision@k metrics (for k=1 and k=2) based on document relevance evaluations. These metrics measure what percentage of retrieved documents are relevant to the query, providing a quantitative assessment of retrieval quality.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_search_and_retrieval_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfirst_document_relevances = [\n    {\"relevant\": True, \"irrelevant\": False}.get(rel)\n    for rel in query_df[\":tag.str:openai_relevance_0\"].tolist()\n]\nsecond_document_relevances = [\n    {\"relevant\": True, \"irrelevant\": False}.get(rel)\n    for rel in query_df[\":tag.str:openai_relevance_1\"].tolist()\n]\nlist_of_precisions_at_k_lists = [\n    compute_precisions_at_k([rel0, rel1])\n    for rel0, rel1 in zip(first_document_relevances, second_document_relevances)\n]\nprecisions_at_1, precisions_at_2 = [\n    [precisions_at_k[index] for precisions_at_k in list_of_precisions_at_k_lists]\n    for index in [0, 1]\n]\nquery_df[\":tag.float:openai_precision_at_1\"] = precisions_at_1\nquery_df[\":tag.float:openai_precision_at_2\"] = precisions_at_2\nquery_df[\n    [\n        \":tag.str:openai_relevance_0\",\n        \":tag.str:openai_relevance_1\",\n        \":tag.float:openai_precision_at_1\",\n        \":tag.float:openai_precision_at_2\",\n    ]\n]\n```\n\n----------------------------------------\n\nTITLE: Creating an OpenAI-based agent with tools and Shakespeare prompt\nDESCRIPTION: Constructs an OpenAIAgent instance using `OpenAI` LLM, incorporating predefined tools (`multiply` and `add`) and setting a Shakespearean style prompt. This enables conversational interactions where the agent can invoke tools and respond in a theatrical tone.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_openai_agent_tracing_tutorial.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nllm = OpenAI(model=\"gpt-4o\")\nagent = OpenAIAgent.from_tools(\n    [multiply_tool, add_tool],\n    llm=llm,\n    system_prompt=SHAKESPEARE_WRITING_ASSISTANT,\n)\n```\n\n----------------------------------------\n\nTITLE: Processing Agent Messages for Evaluation (Python)\nDESCRIPTION: This function, `process_messages`, extracts tool calls, tool responses, and the final output from a list of messages generated by an agent. It iterates through each message, identifying tool calls and their associated function names and arguments, and preparing a structure for tool responses. It also extracts tool responses based on `tool_call_id` and identifies the final output by checking the message role and absence of tool calls or function calls. The function returns a dictionary containing lists of tool calls and responses, the final output, the original messages, and the path length.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\ndef process_messages(messages):\n    tool_calls = []\n    tool_responses = []\n    final_output = None\n\n    for i, message in enumerate(messages):\n        # Extract tool calls\n        if \"tool_calls\" in message and message[\"tool_calls\"]:\n            for tool_call in message[\"tool_calls\"]:\n                tool_name = tool_call[\"function\"][\"name\"]\n                tool_input = tool_call[\"function\"][\"arguments\"]\n                tool_calls.append(tool_name)\n\n                # Prepare tool response structure with tool name and input\n                tool_responses.append(\n                    {\"tool_name\": tool_name, \"tool_input\": tool_input, \"tool_response\": None}\n                )\n\n        # Extract tool responses\n        if message[\"role\"] == \"tool\" and \"tool_call_id\" in message:\n            for tool_response in tool_responses:\n                if message[\"tool_call_id\"] in message.values():\n                    tool_response[\"tool_response\"] = message[\"content\"]\n\n        # Extract final output\n        if (\n            message[\"role\"] == \"assistant\"\n            and not message.get(\"tool_calls\")\n            and not message.get(\"function_call\")\n        ):\n            final_output = message[\"content\"]\n\n    result = {\n        \"tool_calls\": tool_calls,\n        \"tool_responses\": tool_responses,\n        \"final_output\": final_output,\n        \"unchanged_messages\": messages,\n        \"path_length\": len(messages),\n    }\n\n    return result\n```\n\n----------------------------------------\n\nTITLE: Retrieving QA Data for Ragas (Python)\nDESCRIPTION: Uses the `get_qa_with_reference` helper function from Phoenix evaluation utilities. Fetches structured question-answer data, including retrieved contexts and ground truth, from the traces logged under the 'llama-index' project, formatting it into a pandas DataFrame suitable for direct use with Ragas evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_qa_with_reference\n\n# dataset containing span data for evaluation with Ragas\nspans_dataframe = get_qa_with_reference(client, project_name=\"llama-index\")\nspans_dataframe.head(2)\n```\n\n----------------------------------------\n\nTITLE: Uninstrument OpenAI for Evaluation - Python\nDESCRIPTION: Imports `OpenAIInstrumentor` from `openinference.instrumentation.openai`. Calls `OpenAIInstrumentor().uninstrument()` to disable automatic tracing for OpenAI calls that are used specifically for evaluation purposes, preventing them from cluttering the primary pipeline traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\n# Because you don't want to trace the OpenAI calls used for evaluation, you can uninstrument the OpenAI client\nOpenAIInstrumentor().uninstrument()\n```\n\n----------------------------------------\n\nTITLE: Checking Evaluators Run Successfully in Python\nDESCRIPTION: This code runs the defined evaluators on an example from the experiment and prints a shortened version of the output. This confirms that evaluators are correctly processing data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nrun = experiment[0]\nexample = dataset.examples[run.dataset_example_id]\nfor fn in (answer_relevancy, context_relevancy):\n    _ = await fn(run.output, example.input)\n    print(fn.__qualname__)\n    print(shorten(json.dumps(_), width=80))\n```\n\n----------------------------------------\n\nTITLE: Uploading Objects to Phoenix\nDESCRIPTION: This Python snippet uploads data to Phoenix using a list of Python objects. The example defines a dataset name and a list of dictionaries representing input and output data. The `upload_dataset` method from the Phoenix client is used to create the dataset. This method bypasses the constraints of CSV or DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-datasets/creating-datasets.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nds = px.Client().upload_dataset(\n    dataset_name=\"my-synthetic-dataset\",\n    inputs=[{ \"question\": \"hello\" }, { \"question\": \"good morning\" }],\n    outputs=[{ \"answer\": \"hi\" }, { \"answer\": \"good morning\" }],\n);\n```\n\n----------------------------------------\n\nTITLE: Running Q&A/Hallucination Evals with Phoenix and OpenAI Python\nDESCRIPTION: Executes LLM-based evaluations using OpenAI models to assess if the LLM response is factual (Hallucination Eval) and answers the question correctly (Q&A Correctness Eval). It uses pre-defined templates and rails from Phoenix and logs the results back to the Phoenix UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/retrieval/quickstart-retrieval.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import SpanEvaluations, DocumentEvaluations\nfrom phoenix.evals import (\n  HALLUCINATION_PROMPT_RAILS_MAP,\n  HALLUCINATION_PROMPT_TEMPLATE,\n  QA_PROMPT_RAILS_MAP,\n  QA_PROMPT_TEMPLATE,\n  OpenAIModel,\n  llm_classify,\n)\n\n# Creating Hallucination Eval which checks if the application hallucinated\nhallucination_eval = llm_classify(\n  dataframe=queries_df,\n  model=OpenAIModel(\"gpt-4-turbo-preview\", temperature=0.0),\n  template=HALLUCINATION_PROMPT_TEMPLATE,\n  rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n  provide_explanation=True,  # Makes the LLM explain its reasoning\n  concurrency=4,\n)\nhallucination_eval[\"score\"] = (\n  hallucination_eval.label[~hallucination_eval.label.isna()] == \"factual\"\n).astype(int)\n\n# Creating Q&A Eval which checks if the application answered the question correctly\nqa_correctness_eval = llm_classify(\n  dataframe=queries_df,\n  model=OpenAIModel(\"gpt-4-turbo-preview\", temperature=0.0),\n  template=QA_PROMPT_TEMPLATE,\n  rails=list(QA_PROMPT_RAILS_MAP.values()),\n  provide_explanation=True,  # Makes the LLM explain its reasoning\n  concurrency=4,\n)\n\nqa_correctness_eval[\"score\"] = (\n  hallucination_eval.label[~qa_correctness_eval.label.isna()] == \"correct\"\n).astype(int)\n\n# Logs the Evaluations back to the Phoenix User Interface (Optional)\npx.Client().log_evaluations(\n  SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval),\n  SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval),\n)\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Phoenix Project\nDESCRIPTION: This snippet likely represents the initialization or setup of a Phoenix project. It may involve setting up configurations, dependencies, or initial project structures. No specific dependencies or parameters are clearly stated in this excerpt.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_0\n\nLANGUAGE: unknown\nCODE:\n```\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\n```\n\n----------------------------------------\n\nTITLE: Representing LLM Span in JSON\nDESCRIPTION: This snippet provides a JSON representation of a single span capturing details of an LLM operation. It includes standard tracing fields like name, context (trace and span IDs), timings, status, and detailed attributes specific to LLM interactions, such as input messages, model name, invocation parameters, and output value. It demonstrates the structure used to log a unit of work within a trace for analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/concepts-tracing/what-are-traces.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"name\": \"llm\",\n   \"context\": {\n       \"trace_id\": \"0x6c80880dbeb609e2ed41e06a6397a0dd\",\n       \"span_id\": \"0xd9bdedf0df0b7208\",\n       \"trace_state\": \"[]\"\n   },\n   \"kind\": \"SpanKind.INTERNAL\",\n   \"parent_id\": \"0x7eb5df0046c77cd2\",\n   \"start_time\": \"2024-05-08T21:46:11.480777Z\",\n   \"end_time\": \"2024-05-08T21:46:35.368042Z\",\n   \"status\": {\n       \"status_code\": \"OK\"\n   },\n   \"attributes\": {\n       \"openinference.span.kind\": \"LLM\",\n       \"llm.input_messages.0.message.role\": \"system\",\n       \"llm.input_messages.0.message.content\": \"\\n  The following is a friendly conversation between a user and an AI assistant.\\n  The assistant is talkative and provides lots of specific details from its context.\\n  If the assistant does not know the answer to a question, it truthfully says it\\n  does not know.\\n\\n  Here are the relevant documents for the context:\\n\\n  page_label: 7\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.4Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. No piece may weigh more than 70 pounds.\\nb. The combined length and girth of a piece (the length of its longest side plus \\nthe distance around its thickest part) may not exceed 108 inches.\\nc. Lower size or weight standards apply to mail addressed to certain APOs and \\nFPOs, subject to 703.2.0  and 703.4.0  and for Department of State mail, \\nsubject to 703.3.0 .\\n\\npage_label: 6\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.2.10Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. The reply half of a double card must be used for reply only and may not be \\nused to convey a message to the original addressee or to send statements \\nof account. The reply half may be formatted for response purposes (e.g., contain blocks for completion by the addressee).\\nb. A double card must be folded before mailing and prepared so that the \\naddress on the reply half is on the inside when the double card is originally \\nmailed. The address side of the reply half may be prepared as Business \\nReply Mail, Courtesy Reply Mail, meter reply mail, or as a USPS Returns service label.\\nc. Plain stickers, seals, or a single wire stitch (staple) may be used to fasten the \\nopen edge at the top or bottom once the card is folded if affixed so that the \\ninner surfaces of the cards can be readily examined. Fasteners must be \\naffixed according to the applicable preparation requirements for the price claimed. Any sealing on the left and right sides of the cards, no matter the \\nsealing process used, is not permitted.\\nd. The first half of a double card must be detached when the reply half is \\nmailed for return. \\n6.2.10   Enclosures\\nEnclosures in double postcards are prohibited at card prices. \\n6.3 Nonmachinable Pieces\\n6.3.1   Nonmachinable Letters\\nLetter-size pieces (except card-size pieces) that meet one or more of the \\nnonmachinable characteristics in 1.2 are subject to the nonmachinable \\nsurcharge (see 133.1.7 ). \\n6.3.2   Nonmachinable Flats\\nFlat-size pieces that do not meet the standards in 2.0 are considered parcels, \\nand the mailer must pay the applicable parcel price.  \\n6.4 Parcels \\n[7-9-23]  USPS Ground Advantage \\u2014 Retail parcels are eligible for USPS \\nTracking and Signature Confirmation service. A USPS Ground Advantage \\u2014 \\nRetail parcel is the following:\\na. A mailpiece that exceeds any one of the maximum dimensions for a flat \\n(large envelope). See 2.1.\\nb. A flat-size mailpiece, regardless of thickness, that is rigid or nonrectangular.\\nc. A flat-size mailpiece that is not uniformly thick under 2.4. \\nd.[7-9-23]  A mailpiece that does not exceed 130 inches in combined length \\nand girth.\\n7.0 Additional Physical Standards for Media Mail and Library \\nMail\\nThese standards apply to Media Mail and Library Mail:\\n\\npage_label: 4\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.1Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\n4.0 Additional Physical Standa rds for Priority Mail Express\\nEach piece of Priority Mail Express may not weigh more than 70 pounds. The \\ncombined length and girth of a piece (the length of its longest side plus the \\ndistance around its thickest part) may not exceed 108 inches. Lower size or weight standards apply to Priority Mail Express addressed to certain APO/FPO \\nand DPOs. Priority Mail Express items must be large enough to hold the required \\nmailing labels and indicia on a single optical plane without bending or folding.\\n5.0 Additional Physical St andards for Priority Mail\\nThe maximum weight is 70 pounds. The combined length and girth of a piece \\n(the length of its longest side plus the distance around its thickest part) may not \\nexceed 108 inches. Lower size and weight standards apply for some APO/FPO \\nand DPO mail subject to 703.2.0 , and 703.4.0 , and for Department of State mail \\nsubject to 703.3.0 . \\n[7-9-23] \\n6.0 Additional Physical Standa rds for First-Class Mail and \\nUSPS Ground Advantage \\u2014 Retail\\n[7-9-23]\\n6.1 Maximum Weight\\n6.1.1   First-Class Mail\\nFirst-Class Mail (letters and flats) must not exceed 13 ounces. \\n6.1.2   USPS Ground Advantage \\u2014 Retail\\nUSPS Ground Advantage \\u2014 Retail mail must not exceed 70 pounds.\\n6.2 Cards Claimed at Card Prices\\n6.2.1   Card Price\\nA card may be a single or double (reply) stamped card or a single or double postcard. Stamped cards are available from USPS with postage imprinted on \\nthem. Postcards are commercially available or privately printed mailing cards. To \\nbe eligible for card pricing, a card and each half of a double card must meet the physical standards in 6.2 and the applicable eligibility for the price claimed. \\nIneligible cards are subject to letter-size pricing. \\n6.2.2   Postcard Dimensions\\nEach card and part of a double card claimed at card pricing must be the following: \\na. Rectangular.b. Not less than 3-1/2 inches high, 5 inches long, and 0.007 inch thick.\\nc. Not more than 4-1/4 inches high, or more than 6 inches long, or greater than \\n0.016 inch thick.\\nd. Not more than 3.5 ounces (Charge flat-size prices for First-Class Mail \\ncard-type pieces over 3.5 ounces.)\\n\\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\\n  Answer \\\"don't know\\\" if not present in the document.\\n  \",\n       \"llm.input_messages.1.message.role\": \"user\",\n       \"llm.input_messages.1.message.content\": \"Hello\",\n       \"llm.model_name\": \"gpt-4-turbo-preview\",\n       \"llm.invocation_parameters\": \"{\\\"temperature\\\": 0.1, \\\"model\\\": \\\"gpt-4-turbo-preview\\\"}\",\n       \"output.value\": \"How are you?\" },\n   \"events\": [],\n   \"links\": [],\n   \"resource\": {\n       \"attributes\": {},\n       \"schema_url\": \"\"\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading and parsing Amazon product reviews\nDESCRIPTION: This snippet downloads a gzipped file of Amazon product reviews, parses each line to create a dictionary, and then converts the data into a Pandas DataFrame. It handles cases where a line might not have a value after the key.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/long_context_summary_evals.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://snap.stanford.edu/data/amazon/Cell_Phones_&_Accessories.txt.gz\"\ndata = []\nreview_data = {}\nwith urlopen(url) as response:\n    with gzip.open(response, \"rt\", encoding=\"utf-8\") as unzipped:\n        for line in unzipped:\n            line = line.strip()\n            if line:\n                parts = line.split(\": \", 1)\n                key = parts[0]\n                value = parts[1] if len(parts) > 1 else None\n                review_data[key] = value\n            else:\n                if review_data:\n                    data.append(review_data)\n                    review_data = {}\n        if review_data:\n            data.append(review_data)\n\ndf = pd.DataFrame(data)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Creating LlamaIndex ReAct Agent\nDESCRIPTION: Initializes an OpenAI language model instance using the 'gpt-3.5-turbo' model. It then creates a LlamaIndex `ReActAgent` by providing the list of defined query engine tools, the initialized language model, enabling verbose logging to see the agent's thought process, and setting the predefined context string. This agent is now ready to receive chat queries and decide which tool to use.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/agentic_rag_tracing.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\nagent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True, context=CONTEXT)\n```\n\n----------------------------------------\n\nTITLE: Creating and Displaying a Sample Dataset with pandas - Python\nDESCRIPTION: Generates a pandas DataFrame containing sample LLM evaluation data with the columns 'reference', 'query', and 'response'. This example can be replaced with user-supplied data, but it must include these three columns. The snippet creates ten diverse Q&A-style records for downstream evaluation. Requires the 'pandas' package.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/evals.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf = pd.DataFrame(\n    [\n        {\n            \"reference\": \"The Eiffel Tower is located in Paris, France. It was constructed in 1889 as the entrance arch to the 1889 World's Fair.\",\n            \"query\": \"Where is the Eiffel Tower located?\",\n            \"response\": \"The Eiffel Tower is located in Paris, France.\",\n        },\n        {\n            \"reference\": \"The Great Wall of China is over 13,000 miles long. It was built over many centuries by various Chinese dynasties to protect against nomadic invasions.\",\n            \"query\": \"How long is the Great Wall of China?\",\n            \"response\": \"The Great Wall of China is approximately 13,171 miles (21,196 kilometers) long.\",\n        },\n        {\n            \"reference\": \"The Amazon rainforest is the largest tropical rainforest in the world. It covers much of northwestern Brazil and extends into Colombia, Peru and other South American countries.\",\n            \"query\": \"What is the largest tropical rainforest?\",\n            \"response\": \"The Amazon rainforest is the largest tropical rainforest in the world. It is home to the largest number of plant and animal species in the world.\",\n        },\n        {\n            \"reference\": \"Mount Everest is the highest mountain on Earth. It is located in the Mahalangur Himal sub-range of the Himalayas, straddling the border between Nepal and Tibet.\",\n            \"query\": \"Which is the highest mountain on Earth?\",\n            \"response\": \"Mount Everest, standing at 29,029 feet (8,848 meters), is the highest mountain on Earth.\",\n        },\n        {\n            \"reference\": \"The Nile is the longest river in the world. It flows northward through northeastern Africa for approximately 6,650 km (4,132 miles) from its most distant source in Burundi to the Mediterranean Sea.\",\n            \"query\": \"What is the longest river in the world?\",\n            \"response\": \"The Nile River, at 6,650 kilometers (4,132 miles), is the longest river in the world.\",\n        },\n        {\n            \"reference\": \"The Mona Lisa was painted by Leonardo da Vinci. It is considered an archetypal masterpiece of the Italian Renaissance and has been described as 'the best known, the most visited, the most written about, the most sung about, the most parodied work of art in the world'.\",\n            \"query\": \"Who painted the Mona Lisa?\",\n            \"response\": \"The Mona Lisa was painted by the Italian Renaissance artist Leonardo da Vinci.\",\n        },\n        {\n            \"reference\": \"The human body has 206 bones. These bones provide structure, protect organs, anchor muscles, and store calcium.\",\n            \"query\": \"How many bones are in the human body?\",\n            \"response\": \"The adult human body typically has 256 bones.\",\n        },\n        {\n            \"reference\": \"Jupiter is the largest planet in our solar system. It is a gas giant with a mass more than two and a half times that of all the other planets in the solar system combined.\",\n            \"query\": \"Which planet is the largest in our solar system?\",\n            \"response\": \"Jupiter is the largest planet in our solar system.\",\n        },\n        {\n            \"reference\": \"William Shakespeare wrote 'Romeo and Juliet'. It is a tragedy about two young star-crossed lovers whose deaths ultimately reconcile their feuding families.\",\n            \"query\": \"Who wrote 'Romeo and Juliet'?\",\n            \"response\": \"The play 'Romeo and Juliet' was written by William Shakespeare.\",\n        },\n        {\n            \"reference\": \"The first moon landing occurred in 1969. On July 20, 1969, American astronauts Neil Armstrong and Edwin 'Buzz' Aldrin became the first humans to land on the moon as part of the Apollo 11 mission.\",\n            \"query\": \"When did the first moon landing occur?\",\n            \"response\": \"The first moon landing took place on July 20, 1969.\",\n        },\n    ]\n)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Filtering Spans Based on Metadata Dictionary Attributes - Python\nDESCRIPTION: This snippet illustrates using dictionary-style attribute access in the Phoenix SpanQuery.where method to filter spans where a metadata key 'topic' is set to 'programming'. The filter string must be a valid Python boolean expression, and only direct dictionary access is supported (no nested function calls or indirect references).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nquery = SpanQuery().where(\n    \"metadata[\\\"topic\\\"] == 'programming'\"\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Collector Endpoint to Localhost in Python Environment\nDESCRIPTION: This snippet configures the Phoenix collector endpoint environment variable to point to a local Phoenix instance running on port 6006. It is used when running Phoenix locally or in a containerized environment to direct traces to the proper address.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/prompt-flow.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Running Customer Support Prompt Experiment Using run_experiment in Python\nDESCRIPTION: Runs an experiment on a given dataset using the previously defined 'prompt_task' as the task and 'evaluate_response' as the evaluator within the Phoenix experimentation framework. It sets descriptive metadata including experiment name and description, linking the experiment metadata to the created prompt's identifier. This snippet assumes that 'dataset', 'prompt', and related imports are available. It facilitates versioned evaluation of prompt performance across customer support queries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/react-prompting.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ninitial_experiment = run_experiment(\n    dataset,\n    task=prompt_task,\n    evaluators=[evaluate_response],\n    experiment_description=\"Customer Support Prompt\",\n    experiment_name=\"initial-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Using Arbitrary Column Names in Phoenix SpanQuery Select (Python)\nDESCRIPTION: Shows how to assign arbitrary column names, including those with spaces or special characters (e.g., 'Value (Input)'), when selecting span attributes using `SpanQuery().select()`. This is achieved by unpacking a dictionary using the double-asterisk `**` operator.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nquery = SpanQuery().select(**{\n    \"Value (Input)\": \"input.value\",\n    \"Value (Output)\": \"output.value\",\n})\n```\n\n----------------------------------------\n\nTITLE: Checking Task Function\nDESCRIPTION: This code checks the task function defined previously. It retrieves the first example from the dataset, runs the task function on its input, and then prints a shortened version of the task's output using the `shorten` function from the `textwrap` library, to fit within the displayed width.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nexample = dataset[0]\ntask_output = await task(example.input)\nprint(shorten(json.dumps(task_output), width=80))\n```\n\n----------------------------------------\n\nTITLE: Accessing Initial Experiment Results for Meta Prompting - Python\nDESCRIPTION: Demonstrates how to access and sample results from the initial experiment as a Pandas DataFrame, for use as explicit examples in meta prompting. The DataFrame contains at least inputs, outputs, and ground truth columns. Slices the first 10 rows for subsequent use. Dependencies: a completed Phoenix experiment object. Key output: sample set for meta prompt construction.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Access the experiment results from the first round as a dataframe\nground_truth_df = initial_experiment.as_dataframe()\n\n# Sample 10 examples to use as meta prompting examples\nground_truth_df = ground_truth_df[:10]\n```\n\n----------------------------------------\n\nTITLE: Managing Prompt Tags with Phoenix AsyncClient (Python)\nDESCRIPTION: Shows how to perform prompt version tag management asynchronously using the `phoenix.client.AsyncClient`. This snippet covers creating a tag, listing tags for a given version ID, and retrieving a prompt version using its tag, all utilizing `await` for asynchronous operations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/how-to-prompts/tag-a-prompt.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.client import AsyncClient\n\n# Create a tag for a prompt version\nawait AsyncClient().prompts.tags.create(\n    prompt_version_id=\"version-123\",\n    name=\"production\",\n    description=\"Ready for production environment\"\n)\n\n# List tags for a prompt version\ntags = await AsyncClient().prompts.tags.list(prompt_version_id=\"version-123\")\nfor tag in tags:\n    print(f\"Tag: {tag.name}, Description: {tag.description}\")\n\n# Get a prompt version by tag\nprompt_version = await AsyncClient().prompts.get(\n    prompt_identifier=\"my-prompt\",\n    tag=\"production\"\n)\n```\n\n----------------------------------------\n\nTITLE: Setting HF Token\nDESCRIPTION: This Python code sets the `HF_TOKEN` environment variable with a placeholder value.  This token is used for authenticating with Hugging Face services.  Replace `<your_hf_token_value>` with your actual token.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/hfsmolagents.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"HF_TOKEN\"] = \"<your_hf_token_value>\"\n```\n\n----------------------------------------\n\nTITLE: Invoking OpenAI Chat Completions in Python\nDESCRIPTION: This snippet shows how to call the OpenAI GPT-4o chat completion endpoint using the openai_client in Python with a user message input. Required dependencies include the OpenAI Python SDK. Key parameters are the LLM model name ('gpt-4o') and a messages list. The primary output is a chat completion response, suitable for basic LLM span instrumentation; advanced UI features are not included.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nopenai_client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n)\n```\n\n----------------------------------------\n\nTITLE: Finalizing Agent Reasoning and Output in Python\nDESCRIPTION: Defines the finalize_fn function to process the latest reasoning step and optional tool output, updating the agent state and generating the final AgentChatResponse. Returns a tuple containing the agent response and a boolean indicating if the interaction is complete. It also appends conversation memory when the interaction ends. This function relies on custom classes like AgentChatResponse, ObservationReasoningStep, ResponseReasoningStep, ChatMessage, and MessageRole from the broader agent framework.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef finalize_fn(\n    task: Task,\n    state: Dict[str, Any],\n    reasoning_step: Any,\n    is_done: bool = False,\n    tool_output: Optional[Any] = None,\n) -> Tuple[AgentChatResponse, bool]:\n    \"\"\"Finalize function.\n\n    Here we take the latest reasoning step, and a tool output (if provided),\n    and return the agent output (and decide if agent is done).\n\n    This function returns an `AgentChatResponse` and `is_done` tuple. and\n    is the last component of the query pipeline. This is the expected\n    return type for any query pipeline passed to `QueryPipelineAgentWorker`.\n\n    \"\"\"\n    current_reasoning = state[\"current_reasoning\"]\n    current_reasoning.append(reasoning_step)\n    # if tool_output is not None, add to current reasoning\n    if tool_output is not None:\n        observation_step = ObservationReasoningStep(observation=str(tool_output))\n        current_reasoning.append(observation_step)\n    if isinstance(current_reasoning[-1], ResponseReasoningStep):\n        response_step = cast(ResponseReasoningStep, current_reasoning[-1])\n        response_str = response_step.response\n    else:\n        response_str = current_reasoning[-1].get_content()\n\n    # if is_done, add to memory\n    # NOTE: memory is a reserved keyword in `state`, but you can add your own too\n    if is_done:\n        state[\"memory\"].put(ChatMessage(content=task.input, role=MessageRole.USER))\n        state[\"memory\"].put(ChatMessage(content=response_str, role=MessageRole.ASSISTANT))\n\n    return AgentChatResponse(response=response_str), is_done\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application with Corpus Inferences\nDESCRIPTION: This code illustrates how to launch the Phoenix application and pass the corpus inferences to it via the `corpus=` parameter. It utilizes `px.launch_app` to initialize the app environment with the dataset, enabling retrieval operations based on the corpus inferences. Dependencies include Phoenix's `px` library and a valid application session.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/corpus-data.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nsession = px.launch_app(production_dataset, corpus=corpus_inferences)\n```\n\n----------------------------------------\n\nTITLE: Running an Experiment with Meta-Prompting\nDESCRIPTION: This snippet runs an experiment using the `run_experiment` function with the `test_prompt` task, evaluating the responses with `evaluate_response`.  It uses the dataset, task, and evaluators to conduct the experiment and logs the results.  It also assigns the experiment name, description, and metadata.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nmeta_prompting_experiment = run_experiment(\n    dataset,\n    task=test_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Prompt Optimization Technique #2: Meta Prompting\",\n    experiment_name=\"meta-prompting\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + meta_prompt_result.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Tracing LLMs with Context Managers\nDESCRIPTION: This snippet demonstrates tracing LLM calls using a context manager.  It uses the OpenAI client, sets the input messages, makes the LLM call, and sets the output and status based on the response or any exceptions that occur.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom opentelemetry.trace import Status, StatusCode\n\nopenai_client = OpenAI()\n\nmessages = [{\"role\": \"user\", \"content\": \"Hello, world!\"}]\nwith tracer.start_as_current_span(\"llm_span\", openinference_span_kind=\"llm\") as span:\n    span.set_input(messages)\n    try:\n        response = openai_client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=messages,\n        )\n    except Exception as error:\n        span.record_exception(error)\n        span.set_status(Status(StatusCode.ERROR))\n    else:\n        span.set_output(response)\n        span.set_status(Status(StatusCode.OK))\n```\n\n----------------------------------------\n\nTITLE: Building In-Memory Qdrant Vectorstore from Documents (Python)\nDESCRIPTION: Creates a Qdrant vectorstore in memory using the loaded documentation and OpenAI-generated embeddings. The vectorstore enables semantic retrieval over documentation for the chatbot application using similarity search. Requires Qdrant and LangChain community integration, and sufficient RAM for potentially large doc sets.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/qdrant_langchain_instrumentation_search_and_retrieval_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nqdrant = Qdrant.from_documents(\n    docs,\n    embeddings,\n    location=\":memory:\",\n    collection_name=\"my_documents\",\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App for Embeddings\nDESCRIPTION: This snippet launches the Phoenix application, specifically for visualizing embeddings and analysis. It defines schemas for the query and corpus datasets, initializes `px.Inferences` objects with prepared data, and calls `px.launch_app`. The primary dataset allows for viewing queries and the corpus for contextual documents. Prerequisites include having Phoenix properly installed and configured.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nquery_schema = px.Schema(\n    prompt_column_names=px.EmbeddingColumnNames(\n        raw_data_column_name=\"question\", vector_column_name=\"vector\"\n    ),\n    response_column_names=\"answer\",\n)\ncorpus_schema = px.Schema(\n    prompt_column_names=px.EmbeddingColumnNames(\n        raw_data_column_name=\"text\", vector_column_name=\"vector\"\n    )\n)\n# relaunch phoenix with a primary and corpus dataset to view embeddings\npx.close_app()\nsession = px.launch_app(\n    primary=px.Inferences(query_df, query_schema, \"query\"),\n    corpus=px.Inferences(corpus_df.reset_index(drop=True), corpus_schema, \"corpus\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Display the Response DataFrame\nDESCRIPTION: This simple snippet outputs the responses DataFrame, allowing inspection of AI-generated responses to the questions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nresponse_df\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Phoenix Endpoint (Python)\nDESCRIPTION: Sets the `PHOENIX_COLLECTOR_ENDPOINT` environment variable to point to a locally running Phoenix instance, typically at `http://localhost:6006`. This is used for both CLI and Docker local deployments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/haystack.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Executing Anthropic LLM Call with Instrumentation\nDESCRIPTION: This snippet demonstrates creating an Anthropic client, sending a message to a specific model, and printing the response content. It involves specifying model parameters such as max tokens and temperature, with the messages formatted as a list of role-content pairs. It helps in testing and observing LLM output during development.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/anthropic.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport anthropic\n\nclient = anthropic.Anthropic()\n\nmessage = client.messages.create(\n    model=\"claude-3-5-sonnet-20240620\",\n    max_tokens=1000,\n    temperature=0,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Why is the ocean salty?\"\n                }\n            ]\n        }\n    ]\n)\nprint(message.content)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Configuring Environment - Python\nDESCRIPTION: Loads environment variables from a .env file using python-dotenv and imports all necessary libraries for data processing, LLM integration, tracing instrumentation, and agent workflow. Required libraries include dotenv, json, os, getpass, duckdb, pandas, IPython, openai, openinference, opentelemetry, pydantic, tqdm, and phoenix. No user input is required.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dotenv\n\ndotenv.load_dotenv()\n\nimport json\nimport os\nfrom getpass import getpass\n\nimport duckdb\nimport pandas as pd\nfrom IPython.display import Markdown\nfrom openai import OpenAI\nfrom openinference.instrumentation import (\n    suppress_tracing,\n)\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nfrom opentelemetry.trace import StatusCode\nfrom pydantic import BaseModel, Field\nfrom tqdm import tqdm\n\nfrom phoenix.otel import register\n```\n\n----------------------------------------\n\nTITLE: Pulling Phoenix Docker Image\nDESCRIPTION: This bash command pulls the latest Phoenix image from Docker Hub, preparing it for containerized deployment. This is required to deploy a phoenix instance using docker.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/hfsmolagents.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Phoenix Tracing with Endpoint Environment Variable in Python\nDESCRIPTION: Provides an alternative method to configure remote Phoenix tracing using the `PHOENIX_COLLECTOR_ENDPOINT` environment variable. Imports `os` and `LangChainInstrumentor`, sets the endpoint variable combining host and port (or just the full endpoint URL), and instruments the LangChain application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/concepts-tracing/faqs-tracing.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom phoenix.trace import LangChainInstrumentor\n\n# assume phoenix is running at 162.159.135.42:6007\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"162.159.135.42:6007\"\n\nLangChainInstrumentor().instrument()  # logs to http://162.159.135.42:6007\n\n# run your LangChain application\n```\n\n----------------------------------------\n\nTITLE: Downloading SQLite Database File using Requests\nDESCRIPTION: Downloads the `Chinook.db` SQLite database file from a public Google Cloud Storage URL using the `requests` library. It saves the downloaded content to a local file named `Chinook.db`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nurl = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\n\nresponse = requests.get(url)\n\nif response.status_code == 200:\n    # Open a local file in binary write mode\n    with open(\"Chinook.db\", \"wb\") as file:\n        # Write the content of the response (the file) to the local file\n        file.write(response.content)\n    print(\"File downloaded and saved as Chinook.db\")\nelse:\n    print(f\"Failed to download the file. Status code: {response.status_code}\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Exported Data Python\nDESCRIPTION: Retrieves a list of pandas DataFrames corresponding to data clusters or subsets exported from the Phoenix UI. `session.exports` returns the full list in chronological order, while `session.exports[-1]` accesses the most recent export.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/api_reference/source/api/session.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nsession.exports\n```\n\nLANGUAGE: Python\nCODE:\n```\nsession.exports[-1]\n```\n\n----------------------------------------\n\nTITLE: Tracing LLMs with Decorators\nDESCRIPTION: This snippet demonstrates tracing LLM calls using a decorator. It uses the OpenAI client and automatically sets the input and output of the span, capturing any exceptions. Requires the OpenAI API key to be set as an environment variable.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\n\nfrom openai import OpenAI\nfrom openai.types.chat import ChatCompletionMessageParam\n\nopenai_client = OpenAI()\n\n\n@tracer.llm\ndef invoke_llm(\n    messages: List[ChatCompletionMessageParam],\n) -> str:\n    response = openai_client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n    )\n    message = response.choices[0].message\n    return message.content or \"\"\n\n\ninvoke_llm([{\"role\": \"user\", \"content\": \"Hello, world!\"}])\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI API Key in Environment - Python\nDESCRIPTION: Checks if the OPENAI_API_KEY environment variable is set; if not, securely prompts the user to enter their OpenAI API key using getpass. Required for authenticating API calls to OpenAI. Inputs: None if environment is set, otherwise a secret string input. Output: Environment variable set for downstream usage of OpenAI APIs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/run_experiments_with_llama_index.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"\\ud83d\\udd11 Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Defining LLM Prompt/Response Schema in Phoenix (Python)\nDESCRIPTION: Defines a Phoenix schema (`Schema`) for LLM data, mapping specific dataframe columns to Phoenix concepts. It specifies 'id' as the prediction identifier, uses `EmbeddingColumnNames` to link the 'embedding' vector column and 'prompt' raw text column for the prompt, and assigns 'response' as the LLM response column. This schema is crucial for Phoenix to interpret the structure of the input dataframe.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/define-your-schema/prompt-and-response-llm.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nprimary_schema = Schema(\n    prediction_id_column_name=\"id\",\n    prompt_column_names=EmbeddingColumnNames(\n        vector_column_name=\"embedding\",\n        raw_data_column_name=\"prompt\",\n    )\n    response_column_names=\"response\",\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Span Attributes with `setAttributes` in TypeScript\nDESCRIPTION: Demonstrates using the `setAttributes` function from `@openinference-core` along with `context.with` from `@opentelemetry/api` to add a custom attribute (`myAttribute`) to the active OpenTelemetry context in TypeScript. Spans created within the `context.with` callback will include this attribute. Attributes must be valid span attribute values.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nimport { context } from \"@opentelemetry/api\"\nimport { setAttributes } from \"@openinference-core\"\n\ncontext.with(\n  setAttributes(context.active(), { myAttribute: \"test\" }),\n  () => {\n      // Calls within this block will generate spans with the attributes:\n      // \"myAttribute\" = \"test\"\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Querying Spans DataFrame Using Phoenix Client in Python (v3.0.0+)\nDESCRIPTION: This current approach retrieves all recorded spans as a pandas DataFrame using Phoenix's px.Client interface. Required: phoenix library must be installed and imported as px. Input: none; uses default or environment-configured server. Output: DataFrame of spans for further analysis. No tracer arguments or TraceDataset required.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/MIGRATION.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport phoenix as px\n\npx.Client().get_spans_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Experiment with GPT-3.5-turbo\nDESCRIPTION: Evaluates the results of the GPT-3.5-turbo experiment using the defined Jaro-Winkler similarity function. This step calculates the similarity scores between the agent's output and the ground truth outputs from the dataset and reports the results within Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nevaluate_experiment(experiment, jarowinkler_similarity)\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix with OpenTelemetry for Tracing\nDESCRIPTION: Sets up the Phoenix tracing integration by registering it with OpenTelemetry. This enables automatic collection of trace data from the application, facilitating observability.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/tracing_quickstart_openai.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\ntracer_provider = register()\n```\n\n----------------------------------------\n\nTITLE: Run Task and Evaluations Together Python\nDESCRIPTION: Executes the full experiment workflow in a single step. It runs the defined 'task' function on the entire 'dataset' and then immediately applies the provided 'evaluators' to the output of each task run. This is a convenient way to perform the task execution and subsequent evaluation simultaneously for the complete dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n_ = run_experiment(dataset, task, evaluators)\n```\n\n----------------------------------------\n\nTITLE: Import Data into Weaviate Collection - Python\nDESCRIPTION: Imports modules for JSON handling, OS, requests, Weaviate client, and authentication. Connects to Weaviate Cloud. Downloads sample Jeopardy data from a URL. Retrieves the 'Question' collection and performs a batch import of the downloaded data, mapping 'Answer', 'Question', and 'Category' properties. Reports batch errors and closes the client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\n\nimport requests\nimport weaviate\nfrom weaviate.classes.init import Auth\n\n# Best practice: store your credentials in environment variables\nwcd_url = os.environ[\"WEAVIATE_URL\"]\nwcd_api_key = os.environ[\"WEAVIATE_API_KEY\"]\n\nclient = weaviate.connect_to_weaviate_cloud(\n    cluster_url=wcd_url,  # Replace with your Weaviate Cloud URL\n    auth_credentials=Auth.api_key(wcd_api_key),  # Replace with your Weaviate Cloud key\n)\n\nresp = requests.get(\n    \"https://raw.githubusercontent.com/weaviate-tutorials/quickstart/main/data/jeopardy_tiny.json\"\n)\ndata = json.loads(resp.text)\n\nquestions = client.collections.get(\"Question\")\n\nwith questions.batch.dynamic() as batch:\n    for d in data:\n        batch.add_object(\n            {\n                \"answer\": d[\"Answer\"],\n                \"question\": d[\"Question\"],\n                \"category\": d[\"Category\"],\n            }\n        )\n        if batch.number_errors > 10:\n            print(\"Batch import stopped due to excessive errors.\")\n            break\n\nfailed_objects = questions.batch.failed_objects\nif failed_objects:\n    print(f\"Number of failed imports: {len(failed_objects)}\")\n    print(f\"First failed object: {failed_objects[0]}\")\n\nclient.close()  # Free up resources\n```\n\n----------------------------------------\n\nTITLE: Executing Queries and Saving Trace Dataset in Phoenix\nDESCRIPTION: This snippet iterates over the first ten queries, runs each query using the query engine, and captures exceptions silently. Afterwards, it retrieves the trace dataset from Phoenix, assigns it a name, and saves it to disk. It requires the Phoenix SDK (`px.Client`) and tqdm for progress indication, facilitating real-time trace visualization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_with_mongodb_and_llama_index.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nfor query in tqdm(queries[:10]):\\n    try:\\n        query_engine.query(query)\\n    except Exception:\\n        pass\\n# Save trace dataset\\ntds = px.Client().get_trace_dataset()\\ntds.name = \"phoenix_local\"\\ntds.to_disc()\n```\n\n----------------------------------------\n\nTITLE: Applying nest_asyncio for Async Request Handling in Notebooks\nDESCRIPTION: Enables re-entrant event loops in notebook environments such as Jupyter or Colab, improving async request performance during model evaluations. Applies nest_asyncio to the current asyncio event loop.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport nest_asyncio\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Importing necessary libraries\nDESCRIPTION: This code snippet imports necessary libraries for data manipulation, text encoding, language model interaction, and evaluation using LangChain and Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/long_context_summary_evals.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gzip\nfrom typing import List\nfrom urllib.request import urlopen\n\nimport pandas as pd\nimport tiktoken\nfrom langchain.chains.summarize import load_summarize_chain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import Document\n\nfrom phoenix.evals import OpenAIModel, PromptTemplate\nfrom phoenix.evals.evaluators import MapReducer, Refiner\n```\n\n----------------------------------------\n\nTITLE: Retrieving Tracer Spans in Phoenix v2.x using TraceDataset in Python (Deprecated)\nDESCRIPTION: This example illustrates the deprecated method for accessing span data by calling .get_spans() on an OpenInferenceTracer and creating a TraceDataset. Dependencies: phoenix.trace.trace_dataset, phoenix.trace.langchain. Expected input: an active OpenInferenceTracer instance. Output: a TraceDataset constructed from spans. This method is no longer supported after v2.x.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/MIGRATION.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.trace.trace_dataset import TraceDataset  # no longer necessary\nfrom phoenix.trace.langchain import OpenInferenceTracer  # no longer supported\n\ntracer = OpenInferenceTracer()  # no longer supported\nTraceDataset.from_spans(tracer.get_spans())  # .get_spans() no longer supported\n```\n\n----------------------------------------\n\nTITLE: Query Weaviate with Tracing - Python\nDESCRIPTION: Defines the `query_weaviate` function. Uses the `tracer.start_as_current_span` context manager to create a 'retriever' span for tracing the Weaviate query. Sets the input query on the span, executes a `near_text` query on the 'Question' collection, and attaches details of the retrieved documents (UUID, metadata, content) as attributes to the span. Returns the Weaviate results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Query a Weaviate collection with tracing\ndef query_weaviate(query_text, limit=3):\n    # Start a span for the query\n    with tracer.start_as_current_span(\n        \"query_weaviate\", openinference_span_kind=\"retriever\"\n    ) as span:\n        # Set the input for the span\n        span.set_input(query_text)\n\n        # Query the collection\n        collection_name = \"Question\"\n        chunks = client.collections.get(collection_name)\n        results = chunks.query.near_text(query=query_text, limit=limit)\n\n        # Set the retrieved documents as attributes on the span\n        for i, document in enumerate(results.objects):\n            span.set_attribute(f\"retrieval.documents.{i}.document.id\", str(document.uuid))\n            span.set_attribute(f\"retrieval.documents.{i}.document.metadata\", str(document.metadata))\n            span.set_attribute(\n                f\"retrieval.documents.{i}.document.content\", str(document.properties)\n            )\n\n        return results\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for LlamaIndex, Phoenix, and Utilities\nDESCRIPTION: Imports required libraries including standard Python modules (json, os, getpass, urllib), third-party libraries (nest_asyncio, openai, pandas, gcsfs, llama_index, tqdm), and specific components from Phoenix for tracing and evaluation. It also applies nest_asyncio for concurrent operations in notebooks and configures pandas display options.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nfrom getpass import getpass\nfrom urllib.request import urlopen\n\nimport nest_asyncio\nimport openai\nimport pandas as pd\nfrom gcsfs import GCSFileSystem\nfrom llama_index.core import (\n    Settings,\n    StorageContext,\n    load_index_from_storage,\n)\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nfrom tqdm import tqdm\n\nimport phoenix as px\nfrom phoenix.evals import (\n    HallucinationEvaluator,\n    OpenAIModel,\n    QAEvaluator,\n    RelevanceEvaluator,\n    run_evals,\n)\nfrom phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\nfrom phoenix.trace import DocumentEvaluations, SpanEvaluations\n\nnest_asyncio.apply()  # needed for concurrent evals in notebook environments\npd.set_option(\"display.max_colwidth\", 1000)\n```\n\n----------------------------------------\n\nTITLE: Running Extraction Experiment Using Phoenix in Python\nDESCRIPTION: Runs the email extraction experiment by providing the uploaded dataset and the defined task function to Phoenix's run_experiment method. Phoenix automates the orchestration, execution, and result capture of the model predictions on the dataset samples.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/langchain_email_extraction.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nexperiment = run_experiment(dataset, task)\n```\n\n----------------------------------------\n\nTITLE: Importing the AST Module in Python\nDESCRIPTION: Imports the built-in `ast` module to access its functionalities for working with Python's Abstract Syntax Trees.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/src/phoenix/trace/dsl/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ast\n```\n\n----------------------------------------\n\nTITLE: Instrumenting LangChain and OpenAI with OpenTelemetry\nDESCRIPTION: Registers OpenTelemetry instrumentation for both LangChain and OpenAI to capture traces. Uses the provided endpoint for sending telemetry data to the OpenTelemetry collector.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntracer_provider = register(endpoint=\"http://127.0.0.1:4317\")\nLangChainInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\nOpenAIInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Building and Running the Application with Docker Compose - Bash\nDESCRIPTION: This snippet demonstrates how to build and run the instrumented application using Docker Compose. The `docker compose up --build` command compiles the latest container images and launches all services as defined in the application's Docker Compose configuration. Ensure you have Docker and Docker Compose installed prior to running this command. After execution, the chat interface will be accessible at http://localhost:8501 and the Phoenix observability platform at http://localhost:6006.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/manually-instrumented-chatbot/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up --build\n\n```\n\n----------------------------------------\n\nTITLE: Using the Phoenix REST API with TypeScript Client\nDESCRIPTION: TypeScript code demonstrating how to access Phoenix REST API endpoints using the strongly-typed client to fetch datasets and specific prompts.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/packages/phoenix-client/README.md#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createClient } from \"@arizeai/phoenix-client\";\n\nconst phoenix = createClient();\n\n// Get all datasets\nconst datasets = await phoenix.GET(\"/v1/datasets\");\n\n// Get specific prompt\nconst prompt = await phoenix.GET(\"/v1/prompts/{prompt_identifier}/latest\", {\n  params: {\n    path: {\n      prompt_identifier: \"my-prompt\",\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Filtering Spans by Substring in Metadata with Phoenix SpanQuery (Python)\nDESCRIPTION: Demonstrates how to filter Phoenix spans using `SpanQuery().where()` based on the presence of a substring ('programming') within a metadata field ('topic'). Leverages Python's `in` operator for the substring check as Python strings lack a `contain` method.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nquery = SpanQuery().where(\n    \"'programming' in metadata[\\\"topic\\\"]\"\n)\n```\n\n----------------------------------------\n\nTITLE: Viewing the Default Readability Classification Prompt Template\nDESCRIPTION: Displays the template used to prompt the LLM for code readability classification. The template includes variables for input description and code output, intended to be tweaked for customization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nprint(CODE_READABILITY_PROMPT_TEMPLATE)\n```\n\n----------------------------------------\n\nTITLE: Fetching and Parsing HTML Content with BeautifulSoup\nDESCRIPTION: Retrieves the HTML content from the specified URL ('https://nextml.github.io/caption-contest-data/') using 'urllib.request.urlopen'. It then parses the fetched HTML source using 'BeautifulSoup' and finds all 'table' elements within the document.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/vision.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsource = urllib.request.urlopen(\"https://nextml.github.io/caption-contest-data/\").read()\ntable = bs.BeautifulSoup(source).find_all(\"table\")\n```\n\n----------------------------------------\n\nTITLE: Preview of Q&A Correctness Evaluation Results\nDESCRIPTION: Displays the first two records of the question-answer correctness evaluation results, highlighting accurate responses.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_application_tracing_evaluating_and_analysis.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nqa_correctness_eval.head(2)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix in Notebook - Python\nDESCRIPTION: This code launches the Phoenix application within a notebook environment.  The `px.launch_app()` function starts the Phoenix UI inside the notebook, allowing users to view their traces.  It's used for quick access to Phoenix within the Notebook.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/crewai.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Smolagents and Phoenix Tracing (Python)\nDESCRIPTION: Installs necessary Python packages using pip: `smolagents` for the agent framework, `arize-phoenix` for tracing, `opentelemetry-sdk` and `opentelemetry-exporter-otlp` for OpenTelemetry setup, and `openinference-instrumentation-smolagents` for automatic instrumentation. The `-q` flag ensures a quiet installation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/smolagents_tracing_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install smolagents -q\n!pip install -q arize-phoenix opentelemetry-sdk opentelemetry-exporter-otlp openinference-instrumentation-smolagents\n```\n\n----------------------------------------\n\nTITLE: Pulling a prompt by Name using Python Client\nDESCRIPTION: Demonstrates how to initialize a Phoenix client with environment variables and retrieve the latest version of a prompt by its name or ID using Python. Note that prompt names and IDs are synonymous, suitable for development environments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/how-to-prompts/using-a-prompt.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.client import Client\n\n# Initialize a phoenix client with your phoenix endpoint\n# By default it will read from your environment variables\nclient = Client(\n # endpoint=\"https://my-phoenix.com\",\n)\n\n# Pulling a prompt by name\nprompt_name = \"my-prompt-name\"\nclient.prompts.get(prompt_identifier=prompt_name)\n```\n\n----------------------------------------\n\nTITLE: Registering OpenAI Instrumentation in ESM Project\nDESCRIPTION: Configuration for ESM projects to manually instrument the OpenAI library for trace collection, including explicit registration of the imported OpenAI instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-ts.md#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n// instrumentation.ts\n\n// ... rest of imports\nimport OpenAI from \"openai\"\nimport { registerInstrumentations } from \"@opentelemetry/instrumentation\";\nimport { OpenAIInstrumentation } from \"@arizeai/openinference-instrumentation-openai\";\n\n// ... previous code\n\nconst instrumentation = new OpenAIInstrumentation();\ninstrumentation.manuallyInstrument(OpenAI);\n\nregisterInstrumentations({\n  instrumentations: [instrumentation],\n});\n```\n\n----------------------------------------\n\nTITLE: Organizing User Sessions and Executing Queries\nDESCRIPTION: Generates a unique session ID and random username using Faker. Uses 'using_session' and 'using_user' context managers to group subsequent queries into a session for trace collection. Runs a sample of three queries through the LlamaIndex query engine with progress tracking.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/project_sessions_llama_index_query_engine.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nsession_id = str(uuid4())\nsession_user = Faker().user_name()\n\nwith using_session(session_id), using_user(session_user):\n    for query in tqdm(sample(queries, 3)):\n        query_engine.query(query)\n```\n\n----------------------------------------\n\nTITLE: Preparing Exported Cluster Data for GPT Input\nDESCRIPTION: Retrieves the most recently exported cluster data (prompts and responses) from the active Phoenix session using `px.active_session().exports[-1]`. It converts the prompt data to a JSON string, takes a sample of the original DataFrame for baseline context (though not used in `chat_initial_input` here), and concatenates the `pre_prompt` with the cluster's prompt JSON to form the initial message for GPT.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nprompt_cluster_json = px.active_session().exports[-1].prompt.to_json()\nprompt_baseline_jason = conversations_df.sample(n=10).prompt.to_json()\nresponse_cluster_json = px.active_session().exports[-1].response.to_json()\nchat_initial_input = pre_prompt + prompt_cluster_json\n```\n\n----------------------------------------\n\nTITLE: Adding Tags to Spans in Python\nDESCRIPTION: Demonstrates adding a `tag.tags` attribute (as a JSON string representation of a list) to OpenTelemetry spans using the `using_tags` context manager or decorator from `openinference.instrumentation`. The input must be a list of strings. Requires the `openinference-instrumentation` package.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation import using_tags\ntags = [\"tag_1\", \"tag_2\", ...]\nwith using_tags(tags):\n    # Calls within this block will generate spans with the attributes:\n    # \"tag.tags\" = \"[\\\"tag_1\\\",\\\"tag_2\\\",...]\"\n    ...\n```\n\nLANGUAGE: python\nCODE:\n```\n@using_tags(tags)\ndef call_fn(*args, **kwargs):\n    # Calls within this function will generate spans with the attributes:\n    # \"tag.tags\" = \"[\\\"tag_1\\\",\\\"tag_2\\\",...]\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Installing Core Phoenix Library (Bash)\nDESCRIPTION: Installs the core `arize-phoenix` package using pip, necessary for running Phoenix locally via the command line or within a notebook environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai-agents-sdk.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Adding Tags to Spans in JavaScript/TypeScript\nDESCRIPTION: Shows how to set the `tag.tags` attribute (as a JSON string representation of an array) on OpenTelemetry spans using the `setTags` function from `@arizeai/openinference-core` and `context.with` from `@opentelemetry/api`. The input must be an array of strings. Requires the `@arizeai/openinference-core` and `@opentelemetry/api` packages.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { context } from \"@opentelemetry/api\"\nimport { setTags } from \"@arizeai/openinference-core\"\n\ncontext.with(\n  setTags(context.active(), [\"value1\", \"value2\"]),\n  () => {\n      // Calls within this block will generate spans with the attributes:\n      // \"tag.tags\" = '[\"value1\", \"value2\"]'\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Queries from a JSONL Dataset Using urlopen\nDESCRIPTION: This snippet loads a list of queries from a JSON Lines file accessible via a URL. It reads each line, decodes it, parses the JSON, and appends the 'query' field to a list. Dependencies include urllib.request.urlopen and json modules. It prepares a list of queries for subsequent processing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_with_mongodb_and_llama_index.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nqueries_url = \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/context-retrieval/arize_docs_queries.jsonl\"\\nqueries = []\\nwith urlopen(queries_url) as response:\\n    for line in response:\\n        line = line.decode(\"utf-8\").strip()\\n        data = json.loads(line)\\n        queries.append(data[\"query\"])\\nqueries[:10]\n```\n\n----------------------------------------\n\nTITLE: Printing Reference Link Evaluation Prompt Template in Phoenix Python\nDESCRIPTION: This snippet demonstrates how to print the REF_LINK_EVAL_PROMPT_TEMPLATE_STR, a prompt template string used to formulate LLM evaluation prompts for reference link correctness. No external dependencies are needed beyond Phoenix's templates. The output helps understand the structure and instructions for the LLM when evaluating if documentation links appropriately answer user questions. Input: none directly to the snippet; Output: prints the template string to stdout.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/reference-link-evals.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nprint(REF_LINK_EVAL_PROMPT_TEMPLATE_STR)\nYou are given a conversation that contains questions by a CUSTOMER and you are trying\nto determine if the documentation page shared by the ASSISTANT correctly answers\nthe CUSTOMERS questions. We will give you the conversation between the customer\nand the ASSISTANT and the text of the documentation returned:\n    [CONVERSATION AND QUESTION]:\n    {conversation}\n    ************\n    [DOCUMENTATION URL TEXT]:\n    {document_text}\n    [DOCUMENTATION URL TEXT]:\nYou should respond \"correct\" if the documentation text answers the question the\nCUSTOMER had in the conversation. If the documentation roughly answers the question\neven in a general way the please answer \"correct\". If there are multiple questions and a single\nquestion is answered, please still answer \"correct\". If the text does not answer the\nquestion in the conversation, or doesn't contain information that would allow you\nto answer the specific question please answer \"incorrect\".\n```\n\n----------------------------------------\n\nTITLE: Filtering and Exporting Tracing Spans from Phoenix using DSL in Python\nDESCRIPTION: Uses Phoenix trace domain-specific language (DSL) to query and filter out specific spans corresponding to embedding and splitting operations. Exports filtered traces as a Parquet file. Requires access to px.Client() and SpanQuery, and outputs demo_traces.parquet.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace.dsl import SpanQuery\n\ntraces = px.Client().query_spans(\n    SpanQuery().where(\n        \"name != 'BaseEmbedding.get_text_embedding_batch' and name != 'MetadataAwareTextSplitter._parse_nodes' and name != 'SentenceSplitter.split_text_metadata_aware'\"\n    ),\n    limit=5000,\n    timeout=100,\n)\ntraces.to_parquet(\"demo_traces.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Execute Queries and Print Responses\nDESCRIPTION: Iterates through a DataFrame of questions, queries a retrieval system with each question using a hypothetical `query_engine`, and prints the question and the response from the engine.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfor index, row in questions_with_document_chunk_df.iterrows():\n    if index >= MAX_QUERIES:\n        break\n    question = row[\"question\"]\n    response_vector = query_engine.query(question)\n    print(f\"Question: {question}\\nAnswer: {response_vector.response}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Spans Dataframe from Phoenix Client for Trace Analysis\nDESCRIPTION: Pulls the collected trace spans into a pandas DataFrame for detailed inspection of individual traces, including metadata such as input values and retrieved documents.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nspans_df = px.Client().get_spans_dataframe()\n\n```\n\n----------------------------------------\n\nTITLE: Defining Test Gradient Prompt\nDESCRIPTION: The code defines a function `test_gradient_prompt`, that uses the API to send the original prompt along with the `gradient_prompt` and returns the result to the user. This function will then be passed into the experiment that will run.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef test_gradient_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **gradient_prompt.format(variables={\"prompt\": input[\"prompt\"]})\n    )\n    return resp.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Defining SQL Query Run Function\nDESCRIPTION: This function defines the agent's SQL query step. It takes an Example object as input, extracts a question, and executes it against the database using the `lookup_sales_data` function. The result is returned.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndef run_sql_query(example: Example) -> str:\n    with suppress_tracing():\n        return lookup_sales_data(example.input.get(\"question\"))\n```\n\n----------------------------------------\n\nTITLE: Executing Queries Against the RAG System\nDESCRIPTION: Runs each generated question through the query engine and prints the question and response for manual inspection.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n# loop over the questions and generate the answers\nfor _, row in questions_with_document_chunk_df.iterrows():\n    question = row[\"question\"]\n    response_vector = query_engine.query(question)\n    print(f\"Question: {question}\\nAnswer: {response_vector.response}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Defining a Task using Gradient Optimized Prompt\nDESCRIPTION: This snippet defines a function `test_gradient_prompt` that uses the gradient optimized prompt from Phoenix. The function calls the OpenAI Chat Completion API, substituting the input prompt, and returns the generated text.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef test_gradient_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **gradient_prompt.format(variables={\"prompt\": input[\"prompt\"]})\n    )\n    return resp.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Importing Required Modules\nDESCRIPTION: This code imports necessary modules for various functionalities within the script, including JSON handling, temporary file management, text manipulation, time-related operations, asynchronous task handling, evaluation tools from LlamaIndex, OpenAI integration, and Arize Phoenix components for experimentation and tracing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport tempfile\nfrom textwrap import shorten\nfrom time import time_ns\nfrom typing import Tuple\n\nimport nest_asyncio\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.core.evaluation import AnswerRelevancyEvaluator, ContextRelevancyEvaluator\nfrom llama_index.core.llama_dataset import download_llama_dataset\nfrom llama_index.llms.openai import OpenAI\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nimport phoenix as px\nfrom phoenix.experiments import evaluate_experiment, run_experiment\nfrom phoenix.experiments.types import Explanation, Score\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Creating a ClassificationTemplate for Multimodal Classification in Python\nDESCRIPTION: Defines a ClassificationTemplate object to evaluate classification tasks involving multiple input types like text and audio. It specifies the allowed classification labels, and constructs prompts with parts covering task instructions, input data, and response format. The example demonstrates an intent classification task based on audio input for a voice application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/multimodal-evals.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.evals.templates import (\n    ClassificationTemplate,\n    PromptPartContentType,\n    PromptPartTemplate,\n)\n\n# Define valid classification labels (rails)\nTONE_EMOTION_RAILS = [\"positive\", \"neutral\", \"negative\"]\n\n# Create the classification template\ntemplate = ClassificationTemplate(\n    rails=TONE_EMOTION_RAILS,  # Specify the valid output labels\n    template=[\n        # Prompt part 1: Task description\n        PromptPartTemplate(\n            content_type=PromptPartContentType.TEXT,\n            template=\"\"\"\n            You are a helpful AI bot that checks for the tone of the audio.\n            Analyze the audio file and determine the tone (e.g., positive, neutral, negative).\n            Your evaluation should provide a multiclass label from the following options: ['positive', 'neutral', 'negative']. \n            \n            Here is the audio:\n            \"\"\",\n        ),\n        # Prompt part 2: Insert the audio data\n        PromptPartTemplate(\n            content_type=PromptPartContentType.AUDIO,\n            template=\"{audio}\",  # Placeholder for the audio content\n        ),\n        # Prompt part 3: Define the response format\n        PromptPartTemplate(\n            content_type=PromptPartContentType.TEXT,\n            template=\"\"\"\n            Your response must be a string, either positive, neutral, or negative, and should not contain any text or characters aside from that.\n            \"\"\",\n        ),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Running Task and Evaluations Together Using Phoenix in Python\nDESCRIPTION: Initiates a full end-to-end process where the task is run on the dataset and evaluation functions are simultaneously applied within the Phoenix experiment framework. This simplifies the workflow by combining execution and evaluation steps.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n_ = run_experiment(dataset, task, evaluators)\n```\n\n----------------------------------------\n\nTITLE: Sampling Examples for Few-Shot CoT Prompting\nDESCRIPTION: Loads a dataset of math word problems and samples 5 examples to use as few-shot demonstrations in the prompt. These examples help the model understand the expected reasoning pattern and solution format.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/chain_of_thought_prompting.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nds = load_dataset(\"syeddula/math_word_problems\")[\"test\"]\nfew_shot_examples = ds.to_pandas().sample(5)\nfew_shot_examples\n```\n\n----------------------------------------\n\nTITLE: Implementing Few-Shot CoT Prompt with OpenAI and Phoenix\nDESCRIPTION: Creates a Few-Shot Chain of Thought prompt that provides example problems with step-by-step explanations before asking the model to solve a new problem. The prompt instructs the model to show reasoning and format the final answer correctly.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/chain_of_thought_prompting.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfew_shot_COT_template = \"\"\"\nYou are an evaluator who outputs the answer to a math word problem. You must always think through the problem logically before providing an answer. Show some of your reasoning.\n\nFinally, output the integer answer ONLY on a final new line. In this final answer, be sure not include words, commas, labels, or units and round all decimals answers.\n\nHere are some examples of word problems, step by step explanations, and solutions to guide your reasoning:\n\n{examples}\n\"\"\"\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"system\", \"content\": few_shot_COT_template.format(examples=few_shot_examples)},\n        {\"role\": \"user\", \"content\": \"{{Problem}}\"},\n    ],\n)\n\nfew_shot_COT = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Few Shot COT prompt\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Running Ollama LLM Model Locally using Bash\nDESCRIPTION: Starts the Ollama LLM model server locally using the specified model 'ollama/llama3.2:1b'. This snippet requires the Ollama platform to be installed and configured on the user machine. It must run independently as a background process or terminal command to serve the model for local inference.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm.ipynb#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nollama run ollama/llama3.2:1b\n```\n\n----------------------------------------\n\nTITLE: Saving Content Moderation Prompt to Phoenix (Python)\nDESCRIPTION: Saves the parameters of the content moderation prompt, including its template structure, into the Phoenix prompt registry.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# prompt identifier should contain only alphanumeric characters, hyphens or underscores\nprompt_identifier = \"content-moderation\"\n\nprompt = Client().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Content moderation task\",\n    version=PromptVersion.from_anthropic(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Inspect Parameter Extraction Evaluation Results\nDESCRIPTION: This snippet outputs the DataFrame with results for parameter extraction correctness, providing insights into how accurately the model extracts parameters from questions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nparameter_extraction_eval_df\n```\n\n----------------------------------------\n\nTITLE: Plotting Images with Matplotlib and PIL\nDESCRIPTION: Reads images from the './input_images/' directory, and plots them using Matplotlib.  It opens images using PIL and displays them in a grid.  The `plot_images` function iterates through the image paths and displays a maximum of 9 images.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_multimodal_image_reasoning.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimage_paths = []\nfor img_path in os.listdir(\"./input_images\"):\n    image_paths.append(str(os.path.join(\"./input_images\", img_path)))\n\n\ndef plot_images(image_paths):\n    images_shown = 0\n    plt.figure(figsize=(16, 9))\n    for img_path in image_paths:\n        if os.path.isfile(img_path):\n            image = Image.open(img_path)\n\n            plt.subplot(2, 3, images_shown + 1)\n            plt.imshow(image)\n            plt.xticks([])\n            plt.yticks([])\n\n            images_shown += 1\n            if images_shown >= 9:\n                break\n\n\nplot_images(image_paths)\n```\n\n----------------------------------------\n\nTITLE: Basic Text Generation with OpenAI GPT Model\nDESCRIPTION: Creates a simple chat completion request with a specified model and messages, then outputs the generated text. Demonstrates core LLM invocation using the OpenAI SDK.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nparams = CompletionCreateParamsBase(\n    model=\"gpt-4o-mini\",\n    temperature=0,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are coding poet.\"},\n        {\"role\": \"user\", \"content\": \"Write a haiku about recursion in programming.\"},\n    ],\n)\nresp = OpenAI().chat.completions.create(**params)\nprint(resp.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Phoenix Session URL\nDESCRIPTION: Retrieves and prints the URL for the active Phoenix session's web interface. Accessing this URL in a browser allows users to visualize the logged traces and evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nprint(\"phoenix URL\", px.active_session().url)\n```\n\n----------------------------------------\n\nTITLE: Loading Sample Queries for Testing\nDESCRIPTION: Downloads and parses a sample JSONL file containing common queries about Arize documentation for testing the QA application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_tracing_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nurl = \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/arize_docs_queries.jsonl\"\nqueries = []\nwith urlopen(url) as response:\n    for line in response:\n        line = line.decode(\"utf-8\").strip()\n        data = json.loads(line)\n        queries.append(data[\"query\"])\nqueries[:10]\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Package via pip (Bash)\nDESCRIPTION: Installs the main 'arize-phoenix' package, which provides core Phoenix functionality including the server and client interfaces. No arguments are required. Should be executed in a Python 3.6+ environment with pip available.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/groq.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Initializing Multiple Phoenix Inferences with Different Schemas in Python\nDESCRIPTION: This snippet demonstrates how to initialize two Phoenix inference sets, `train_ds` and `prod_ds`, using different schemas. This is necessary when the training and production data have different formats.  `train_schema` and `prod_schema` are assumed to have been previously defined. The inferences will be named \"training\" and \"production\" respectively within the Phoenix UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/inferences.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_ds = px.Inferences(train_df, train_schema, \"training\")\nprod_ds = px.Inferences(prod_df, prod_schema, \"production\")\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Packages (Notebook)\nDESCRIPTION: This bash command installs the `arize-phoenix` package within a notebook environment, enabling interaction with the Phoenix service for monitoring and analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/hfsmolagents.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Input Component for Query Pipeline\nDESCRIPTION: Defines the agent input function component that initializes the reasoning state and processes input tasks for the query pipeline, capturing observations from the task input.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n## Agent Input Component\n## This is the component that produces agent inputs to the rest of the components\n## Can also put initialization logic here.\n\n\ndef agent_input_fn(task: Task, state: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Agent input function.\n\n    Returns:\n        A Dictionary of output keys and values. If you are specifying\n        src_key when defining links between this component and other\n        components, make sure the src_key matches the specified output_key.\n\n    \"\"\"\n    # initialize current_reasoning\n    if \"current_reasoning\" not in state:\n        state[\"current_reasoning\"] = []\n    reasoning_step = ObservationReasoningStep(observation=task.input)\n    state[\"current_reasoning\"].append(reasoning_step)\n    return {\"input\": task.input}\n\n\nagent_input_component = AgentInputComponent(fn=agent_input_fn)\n```\n\n----------------------------------------\n\nTITLE: Chatting with OpenAI Multimodal LLM\nDESCRIPTION: Demonstrates how to interact with the OpenAI Multimodal LLM. It generates a chat message with a prompt and image documents, then calls the LLM to get a response and prints the response.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_multimodal_image_reasoning.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.multi_modal_llms.openai.utils import (\n    generate_openai_multi_modal_chat_message,\n)\n\n# Setup first message: a question about the passed image documents\nmessage_1 = generate_openai_multi_modal_chat_message(\n    prompt=\"Describe the images as an alternative text\",\n    role=\"user\",\n    image_documents=image_documents,\n)\n\n# Call the LLM for a response to the question\nresponse_1 = openai_mm_llm.chat(\n    messages=[\n        message_1,\n    ],\n)\n\nprint(response_1)\n```\n\n----------------------------------------\n\nTITLE: Creating Directory and Saving Trace Dataset as Parquet Fixtures in Python\nDESCRIPTION: Creates a local directory named 'fixtures' if it does not exist, then queries and saves all available traces (up to 5000) from the Phoenix server to the given directory. Inputs are the directory path and px.Client(); output is a Parquet file containing the trace dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Specify and Create the Directory for Trace Dataset\ndirectory = \"fixtures\"\nos.makedirs(directory, exist_ok=True)\n\n# Save the Trace Dataset (set limit to above 2000)\ntrace_id = px.Client().get_trace_dataset(limit=5000, timeout=60).save(directory=directory)\n```\n\n----------------------------------------\n\nTITLE: Getting LLM Span Data Python\nDESCRIPTION: Retrieves LLM trace spans as a pandas DataFrame. Optional query strings can filter spans based on properties like `span_kind` to focus on specific types of operations (e.g., 'LLM', 'RETRIEVER').\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/api_reference/source/api/session.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nsession.get_spans_dataframe()\n```\n\nLANGUAGE: Python\nCODE:\n```\nsession.get_spans_dataframe(\"span_kind == 'LLM'\")\n```\n\nLANGUAGE: Python\nCODE:\n```\nsession.get_spans_dataframe(\"span_kind == 'RETRIEVER'\")\n```\n\n----------------------------------------\n\nTITLE: Creating an Uninstrumented Bedrock Client in Python\nDESCRIPTION: This code creates an uninstrumented Bedrock client using the boto3 session. This client will not produce OpenInference traces when invoking models, serving as a comparison to the instrumented client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/bedrock_tracing_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nuninstrumented_client = session.client(\"bedrock-runtime\")\n```\n\n----------------------------------------\n\nTITLE: Running Guardrails with OpenAI integration\nDESCRIPTION: Python example demonstrating how to use Guardrails with OpenAI to create a guard that validates responses using the TwoWords validator from the Guardrails hub.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/guardrails-ai.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom guardrails import Guard\nfrom guardrails.hub import TwoWords\nimport openai\n\nguard = Guard().use(\n    TwoWords(),\n)\nresponse = guard(\n    llm_api=openai.chat.completions.create,\n    prompt=\"What is another name for America?\",\n    model=\"gpt-3.5-turbo\",\n    max_tokens=1024,\n)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables and API Keys\nDESCRIPTION: Configures environment variables for Phoenix and OpenAI API access, setting up the collector endpoint and securely handling API keys.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/openai_agents_cookbook.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nfrom getpass import getpass\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\nif not os.environ.get(\"PHOENIX_CLIENT_HEADERS\"):\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = \"api_key=\" + getpass(\"Enter your Phoenix API key: \")\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Creating Active Span with OpenInference Attributes - TypeScript\nDESCRIPTION: Illustrates creating an active span using `tracer.startActiveSpan`. This method automatically sets the span in the current context. The example shows setting OpenInference semantic attributes on the span before ending it. It assumes a `tracer` and `openai` instance are available in the scope.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { trace, Span } from \"@opentelemetry/api\";\nimport { SpanKind } from \"@opentelemetry/api\";\nimport {\n    SemanticConventions,\n    OpenInferenceSpanKind,\n} from \"@arizeai/openinference-semantic-conventions\";\n\nexport function chat(message: string) {\n    // Create a span. A span must be closed.\n    return tracer.startActiveSpan(\n        \"chat\",\n        (span: Span) => {\n            span.setAttributes({\n                [SemanticConventions.OPENINFERENCE_SPAN_KIND]: OpenInferenceSpanKind.chain,\n                [SemanticConventions.INPUT_VALUE]: message,\n            });\n            let chatCompletion = await openai.chat.completions.create({\n                messages: [{ role: \"user\", content: message }],\n                model: \"gpt-3.5-turbo\",\n            });\n            span.setAttributes({\n                attributes: {\n                    [SemanticConventions.OUTPUT_VALUE]: chatCompletion.choices[0].message,\n                },\n            });\n            // Be sure to end the span!\n            span.end();\n            return result;\n        }\n    );\n}\n```\n\n----------------------------------------\n\nTITLE: Constructing Remote GCS URL for Data Import in Python\nDESCRIPTION: Concatenates host, bucket, and prefix to build the base URL for remote data access on Google Cloud Storage. Strings host, bucket, and prefix are combined via f-string formatting; the resultant URL variable references a directory location for further reads.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nhost = \"https://storage.googleapis.com/\"\nbucket = \"arize-phoenix-assets\"\nprefix = \"traces\"\nurl = f\"{host}{bucket}/{prefix}\"\n```\n\n----------------------------------------\n\nTITLE: Fetching Trace Data using the Phoenix Client in Python\nDESCRIPTION: Shows how to access the trace data collected in Arize Phoenix. It initializes the Phoenix client using `px.Client()`, which connects to the configured Phoenix backend (using environment variables set earlier). It then uses the `get_spans_dataframe()` method of the client to retrieve the collected trace spans as a pandas DataFrame and prints the head of the DataFrame to display the first few trace records.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_llamaindex_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npx_client = px.Client()\nphoenix_df = px_client.get_spans_dataframe()\nprint(phoenix_df.head())\n```\n\n----------------------------------------\n\nTITLE: Filtering Spans Without Specific Evaluations with Phoenix SpanQuery (Python)\nDESCRIPTION: Shows how to use `SpanQuery().where()` to filter for Phoenix spans that lack an evaluation label for a specified metric (e.g., 'correctness'). It checks if the evaluation label is `None`. The metric name 'correctness' should be replaced with the actual name used.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nquery = SpanQuery().where(\n    \"evals['correctness'].label is None\"\n)\n# correctness is whatever you named your evaluation metric\n```\n\n----------------------------------------\n\nTITLE: Initializing LangChain Agent Executor - Python\nDESCRIPTION: Creates the `agent_executor` instance, which is the runnable form of the agent. It combines the defined `tools`, the `llm`, and specifies the agent type as `AgentType.OPENAI_FUNCTIONS` to leverage OpenAI's function calling capabilities. `verbose=True` enables detailed logging during execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_agent_tracing_tutorial.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nagent_executor = initialize_agent(tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)\n```\n\n----------------------------------------\n\nTITLE: Testing Evaluators on an Experiment Run in Python\nDESCRIPTION: Retrieves the first run (`run`) from the `experiment` object and its corresponding original dataset example (`example`). It then iterates through the `evaluators` dictionary. For each evaluator (`name`, `fn`), it asynchronously calls the evaluation function `fn` (which is the `adapt` wrapper) with the run's `output` and the example's `input`. The result (`_`) containing the score and explanation is captured, and the evaluator's name and a shortened JSON representation of the result are printed. This verifies that each evaluator can run successfully on a sample output.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nrun = experiment[0]\nexample = dataset.examples[run.dataset_example_id]\nfor name, fn in evaluators.items():\n    _ = await fn(run.output, example.input)\n    print(name)\n    print(shorten(json.dumps(_), width=80))\n```\n\n----------------------------------------\n\nTITLE: Running Agent with Multiple Questions - Python\nDESCRIPTION: This code snippet iterates through a list of questions and runs the agent for each question. It uses `tqdm` to display a progress bar. A try-except block handles potential errors during the processing of each question.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nagent_questions = [\n    \"What was the most popular product SKU?\",\n    \"What was the total revenue across all stores?\",\n    \"Which store had the highest sales volume?\",\n    \"Create a bar chart showing total sales by store\",\n    \"What percentage of items were sold on promotion?\",\n    \"Plot daily sales volume over time\",\n    \"What was the average transaction value?\",\n    \"Create a box plot of transaction values\",\n    \"Which products were frequently purchased together?\",\n    \"Plot a line graph showing the sales trend over time with a 7-day moving average\",\n]\n\nfor question in tqdm(agent_questions, desc=\"Processing questions\"):\n    try:\n        ret = start_main_span([{\"role\": \"user\", \"content\": question}])\n    except Exception as e:\n        print(f\"Error processing question: {question}\")\n        print(e)\n        continue\n```\n\n----------------------------------------\n\nTITLE: Using the create_evaluator Decorator for Display Control in Phoenix (Python)\nDESCRIPTION: This snippet demonstrates how to customize a code evaluator for Phoenix experiments by using the create_evaluator decorator. The evaluator checks if the output is shorter (in words) than the expected text. Using the decorator allows specifying display properties for the UI such as the evaluator's name and type (CODE vs LLM). Requires phoenix.experiments.evaluators module. Inputs should be text strings; returns a boolean indicating comparative length.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/using-evaluators.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments.evaluators import create_evaluator\n\n# the decorator can be used to set display properties\n# `name` corresponds to the metric name shown in the UI\n# `kind` indicates if the eval was made with a \"CODE\" or \"LLM\" evaluator\n@create_evaluator(name=\"shorter?\", kind=\"CODE\")\ndef wordiness_evaluator(expected, output):\n    reference_length = len(expected.split())\n    output_length = len(output.split())\n    return output_length < reference_length\n```\n\n----------------------------------------\n\nTITLE: Generating Model Responses with Tracing and Project Context Python\nDESCRIPTION: This snippet runs a loop to send each question in the DataFrame to the OpenAI LLM within a Phoenix 'using_project' context, attaches expected tool call metadata, and collects model responses for later evaluation. It uses Phoenix context managers for trace tagging and project grouping. Dependencies are Phoenix, OpenAI Python libraries, and previously constructed questions DataFrame and tools list. Input: a question and expected tool calls from DataFrame, output: responses collected in a list.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/demos/playground_demo.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nwith using_project(\"e-commerce\"):\n    responses = []\n    for _, row in df.iterrows():\n        with using_metadata(metadata={\"expected_tool_calls\": row[\"expected_tool_calls\"]}):\n            response = llm_client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[{\"content\": row[\"question\"], \"role\": \"user\"}],\n                tools=tools,\n                temperature=0.1,\n                max_tokens=2048,\n                frequency_penalty=0.1,\n                presence_penalty=0.2,\n                stop=[\"supercalafragilisticexpialidocious\"],\n                top_p=0.9,\n            )\n            responses.append(response)\n```\n\n----------------------------------------\n\nTITLE: Setting Evaluation Sample Size in Python\nDESCRIPTION: Configures the number of samples to use for evaluation, with different runtime expectations for various sample sizes across different GPT models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#####################\n## N_EVAL_SAMPLE_SIZE\n#####################\n# Eval sample size determines the run time\n# 100 samples: GPT-4 ~ 80 sec / GPT-3.5 ~ 40 sec\n# 1,000 samples: GPT-4 ~15-17 min / GPT-3.5 ~ 6-7min (depending on retries)\n# 10,000 samples GPT-4 ~170 min / GPT-3.5 ~ 70min\nN_EVAL_SAMPLE_SIZE = 100\n```\n\n----------------------------------------\n\nTITLE: Defining Prompt Classification Task\nDESCRIPTION: The provided code defines a `PromptClassifier` class inheriting from `dspy.Signature`. This class is used for classifying prompts as either 'benign' or 'jailbreak'. It defines two fields: `prompt` which is an input field, and `label` which is an output field with a description. This forms the basic framework that DSPy will optimize.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n# Define the prompt classification task\nclass PromptClassifier(dspy.Signature):\n    \"\"\"Classify if a prompt is benign or jailbreak.\"\"\"\n\n    prompt = dspy.InputField()\n    label = dspy.OutputField(desc=\"either 'benign' or 'jailbreak'\")\n```\n\n----------------------------------------\n\nTITLE: Sampling and Renaming Dataset Subset for Evaluation\nDESCRIPTION: Resamples the dataset to the specified number of samples, ensuring faster evaluation runs. Renames columns to standardize 'prompt' and 'solution' as 'input' and 'output' for processing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndf = df.sample(n=N_EVAL_SAMPLE_SIZE).reset_index(drop=True)\ndf = df.rename(\n    columns={\"prompt\": \"input\", \"solution\": \"output\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading Phoenix trace spans (Python)\nDESCRIPTION: Retrieves trace span data for a specified Phoenix project and loads it into a pandas DataFrame. This allows for programmatic access and analysis of captured trace information.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/feedback-and-annotations/evaluating-phoenix-traces.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\nspans_df = px.Client().get_spans_dataframe(project_name=\"evaluating_traces_quickstart\")\nspans_df.head()\n```\n\n----------------------------------------\n\nTITLE: Load and Sample Few-Shot Examples (Python)\nDESCRIPTION: Loads a specific math word problem dataset from Hugging Face (`syeddula/math_word_problems`) and samples 5 examples from its 'test' split. These sampled examples will be used as in-context demonstrations for the Few-Shot CoT prompt.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/chain-of-thought-prompting.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nds = load_dataset(\"syeddula/math_word_problems\")[\"test\"]\nfew_shot_examples = ds.to_pandas().sample(5)\nfew_shot_examples\n```\n\n----------------------------------------\n\nTITLE: Phoenix Schema class definition in Python\nDESCRIPTION: The Schema class constructor that maps columns in a dataframe to appropriate model dimensions. This defines how Phoenix interprets the data for analysis, including predictions, actuals, features, and embeddings.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/inference-and-schema.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass Schema(\n    prediction_id_column_name: Optional[str] = None,\n    timestamp_column_name: Optional[str] = None,\n    feature_column_names: Optional[List[str]] = None,\n    tag_column_names: Optional[List[str]] = None,\n    prediction_label_column_name: Optional[str] = None,\n    prediction_score_column_name: Optional[str] = None,\n    actual_label_column_name: Optional[str] = None,\n    actual_score_column_name: Optional[str] = None,\n    prompt_column_names: Optional[EmbeddingColumnNames] = None\n    response_column_names: Optional[EmbeddingColumnNames] = None\n    embedding_feature_column_names: Optional[Dict[str, EmbeddingColumnNames]] = None,\n    excluded_column_names: Optional[List[str]] = None,\n)\n```\n\n----------------------------------------\n\nTITLE: Querying Root Spans\nDESCRIPTION: This snippet queries the root spans in the trace data and selects the input and output (IO) attributes.  It utilizes SpanQuery and the session object established earlier.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/span_query_language.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsession.query_spans(SpanQuery().select(**IO).where(IS_ROOT))\n```\n\n----------------------------------------\n\nTITLE: Writing Binary Relevance DataFrame to JSONL File in Python\nDESCRIPTION: Serializes the binary relevance subset DataFrame into a newline-delimited JSON (JSONL) file, with each record as an independent JSON object. Requires the Python json library, the binary_relevance_classification_df, and write permissions for the output directory. Inputs are the DataFrame and target path; output is a JSONL file. Filenames are parameterized by dataset version and split. Rows must be serializable to JSON.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_ms_marco.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndata_path = f\"ms_marco-{version}-{split}.jsonl\"\nwith open(data_path, \"w\") as f:\n    for record in binary_relevance_classification_df.to_dict(orient=\"records\"):\n        f.write(json.dumps(record) + \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Running Experiment with DSPy-Optimized Classifier in Phoenix - Python\nDESCRIPTION: Executes an experiment using the DSPy-optimized classifier by passing the dataset, evaluation function, and evaluation logic to run_experiment. Includes configurable metadata about the optimization technique. Dependencies include Phoenix experiment tracking utilities, the previously defined test_dspy_prompt, an optimized_classifier, and an evaluate_response function. Expects a dataset input and logs experiment-related metadata.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\n# Run experiment with DSPy-optimized classifier\ndspy_experiment = run_experiment(\n    dataset,\n    task=test_dspy_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Prompt Optimization Technique #4: DSPy Prompt Tuning\",\n    experiment_name=\"dspy-optimization\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + dspy_prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Building LlamaIndex Query Engine (Python)\nDESCRIPTION: Defines a function `build_query_engine` that creates a VectorStoreIndex from documents using OpenAI embeddings. It then configures and returns a query engine set to retrieve the top 2 most similar documents based on query similarity. This indexing process is traced under the 'indexing' project in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\nfrom phoenix.trace import using_project\n\ndef build_query_engine(documents):\n    vector_index = VectorStoreIndex.from_documents(\n        documents,\n        embed_model=OpenAIEmbedding(),\n    )\n    query_engine = vector_index.as_query_engine(similarity_top_k=2)\n    return query_engine\n\n\nwith using_project(\"indexing\"):\n    # By assigning a project name, the instrumentation will send all the embeddings to the indexing project\n    query_engine = build_query_engine(documents)\n```\n\n----------------------------------------\n\nTITLE: Parsing and Dumping Name AST Node in Python\nDESCRIPTION: Parses a simple variable name string (`xyz`) into an AST using `ast.parse` and prints its structure using `ast.dump`. This shows how simple identifiers are represented as `ast.Name` nodes in the AST, typically with a `Load()` context.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/src/phoenix/trace/dsl/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(ast.dump(ast.parse(\"xyz\", mode=\"eval\").body, indent=4))\n```\n\n----------------------------------------\n\nTITLE: Initializing Phoenix Client (Deno JavaScript)\nDESCRIPTION: Creates and configures a Phoenix client instance, specifying the base URL of the Phoenix server (defaulting to localhost:6006). It also shows how to optionally include an authorization header for instances requiring an API key. This client is used for interacting with Phoenix prompts.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_openai_tutorial.ipynb#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst px = PhoenixClient.createClient({\n  options: {\n    baseUrl: \"http://localhost:6006\",\n    // Uncomment this if you are using a Phoenix instance that requires an API key\n    // headers: {\n    //   Authorization: \"bearer xxxxxx\",\n    // }\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: Running Bedrock with invoke_model\nDESCRIPTION: This code demonstrates how to invoke a Bedrock model using the `invoke_model` method. It constructs a prompt and passes it to the model, then processes the response to extract the completion.  This snippet relies on the `client` variable from the previous example. It also expects the `json` library to be available to parse the response body.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/bedrock.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprompt = (\n    b'{\"prompt\": \"Human: Hello there, how are you? Assistant:\", \"max_tokens_to_sample\": 1024}'\n)\nresponse = client.invoke_model(modelId=\"anthropic.claude-v2\", body=prompt)\nresponse_body = json.loads(response.get(\"body\").read())\nprint(response_body[\"completion\"])\n```\n\n----------------------------------------\n\nTITLE: Implementing Output Agent Component with Tool Execution in Python\nDESCRIPTION: Defines OutputAgentComponent, a custom agent component managing output generation from a reasoning step, including invoking tools and parsing outputs. It initializes with a list of tools, sets up a ToolRunnerComponent and ReActOutputParser, and runs by processing chat responses, deciding if the agent is done, and possibly running associated tools. Input, optional input, output keys, and sub-components are explicitly defined. This class depends on external components like ToolRunnerComponent, ReActOutputParser, and QueryComponent within the agent framework.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass OutputAgentComponent(CustomAgentComponent):\n    \"\"\"Output agent component.\"\"\"\n\n    tool_runner_component: ToolRunnerComponent\n    output_parser: ReActOutputParser\n\n    def __init__(self, tools, **kwargs):\n        tool_runner_component = ToolRunnerComponent(tools)\n        super().__init__(\n            tool_runner_component=tool_runner_component, output_parser=ReActOutputParser(), **kwargs\n        )\n\n    def _run_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        chat_response = kwargs[\"chat_response\"]\n        task = kwargs[\"task\"]\n        state = kwargs[\"state\"]\n        reasoning_step = self.output_parser.parse(chat_response.message.content)\n        if reasoning_step.is_done:\n            return {\"output\": finalize_fn(task, state, reasoning_step, is_done=True)}\n        else:\n            tool_output = self.tool_runner_component.run_component(\n                tool_name=reasoning_step.action,\n                tool_input=reasoning_step.action_input,\n            )\n            return {\n                \"output\": finalize_fn(\n                    task,\n                    state,\n                    reasoning_step,\n                    is_done=False,\n                    tool_output=tool_output,\n                )\n            }\n\n    @property\n    def _input_keys(self) -> Set[str]:\n        return {\"chat_response\"}\n\n    @property\n    def _optional_input_keys(self) -> Set[str]:\n        return {\"is_done\", \"tool_output\"}\n\n    @property\n    def _output_keys(self) -> Set[str]:\n        return {\"output\"}\n\n    @property\n    def sub_query_components(self) -> List[QueryComponent]:\n        return [self.tool_runner_component]\n```\n\n----------------------------------------\n\nTITLE: Defining - Custom LLM Evaluator - Python\nDESCRIPTION: Illustrates creating a custom LLM-based evaluator using the `@create_evaluator(kind=\"llm\")` decorator. The decorated function formats a prompt based on the input, output, and expected values, sends it to an LLM, and parses the response to return an evaluation score (1.0 for 'accurate', 0.0 otherwise).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/datasets_and_experiments_quickstart.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments.evaluators import create_evaluator\n\neval_prompt_template = \"\"\"\nGiven the QUESTION and REFERENCE_ANSWER, determine whether the ANSWER is accurate.\nOutput only a single word (accurate or inaccurate).\n\nQUESTION: {question}\n\nREFERENCE_ANSWER: {reference_answer}\n\nANSWER: {answer}\n\nACCURACY (accurate / inaccurate):\n\"\"\"\n\n\n@create_evaluator(kind=\"llm\")  # need the decorator or the kind will default to \"code\"\ndef accuracy(input: Dict[str, Any], output: str, expected: Dict[str, Any]) -> float:\n    message_content = eval_prompt_template.format(\n        question=input[\"question\"], reference_answer=expected[\"answer\"], answer=output\n    )\n    response = openai_client.chat.completions.create(\n        model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": message_content}]\n    )\n    response_message_content = response.choices[0].message.content.lower().strip()\n    return 1.0 if response_message_content == \"accurate\" else 0.0\n```\n\n----------------------------------------\n\nTITLE: Downloading Article Content (Python)\nDESCRIPTION: Downloads text content from specified URLs using `requests` and extracts paragraph text using `BeautifulSoup` to prepare input data for summarization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\narticles = []\nfor item in (\"third-party-testing\", \"alignment-faking\"):\n    response = requests.get(f\"https://www.anthropic.com/news/{item}\")\n    soup = BeautifulSoup(response.text, \"html.parser\")\n    articles.append({\"article\": \" \".join([p.text for p in soup.find_all(\"p\")])})\n```\n\n----------------------------------------\n\nTITLE: Display DataFrame Head After Label Mapping (Python)\nDESCRIPTION: Displays the first few rows of the DataFrame again after the label mapping has been applied. This confirms that the numerical labels have been successfully replaced by string labels. Input: The modified pandas DataFrame `df`. Output: Prints the head of the DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Downloading summarization benchmark dataset\nDESCRIPTION: Downloads a benchmark dataset for text summarization evaluation, specifically the CNN Daily News Mail dataset which is commonly used for benchmarking text summarization models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = download_benchmark_dataset(\n    task=\"summarization-classification\", dataset_name=\"summarization-test\"\n)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix for Notebook Usage\nDESCRIPTION: This code installs the Phoenix package in a Jupyter notebook or similar environment to enable launching the Phoenix dashboard within the notebook interface. This simplifies quick experimentation but lacks persistence.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/anthropic.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries\nDESCRIPTION: Installs necessary Python packages: `arize-phoenix` for experiment tracking and prompt management, `datasets` for loading the review data, and `openinference-instrumentation-openai` for instrumenting OpenAI API calls. The `-qqq` flags ensure a quiet installation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/few-shot-prompting.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install -qqq \"arize-phoenix>=8.0.0\" datasets openinference-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Importing Phoenix Library Python\nDESCRIPTION: Imports the phoenix library, commonly aliased as `px`, to access its functions and classes for launching and interacting with the Phoenix application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/api_reference/source/api/session.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport phoenix as px\n```\n\n----------------------------------------\n\nTITLE: Running CrewAI Example - Python\nDESCRIPTION: This comprehensive code snippet demonstrates a basic CrewAI application. It defines agents with specific roles and goals, then creates tasks for each agent. It sets up the Crew with the defined agents and tasks, allowing the user to then kickoff the Crew.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/crewai.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom crewai import Agent, Task, Crew, Process\nfrom crewai_tools import SerperDevTool\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\nos.environ[\"SERPER_API_KEY\"] = \"YOUR_SERPER_API_KEY\"\nsearch_tool = SerperDevTool()\n\n# Define your agents with roles and goals\nresearcher = Agent(\n  role='Senior Research Analyst',\n  goal='Uncover cutting-edge developments in AI and data science',\n  backstory=\"\"\"You work at a leading tech think tank.\n  Your expertise lies in identifying emerging trends.\n  You have a knack for dissecting complex data and presenting actionable insights.\"\"\",\n  verbose=True,\n  allow_delegation=False,\n  # You can pass an optional llm attribute specifying what model you wanna use.\n  # llm=ChatOpenAI(model_name=\"gpt-3.5\", temperature=0.7),\n  tools=[search_tool]\n)\nwriter = Agent(\n  role='Tech Content Strategist',\n  goal='Craft compelling content on tech advancements',\n  backstory=\"\"\"You are a renowned Content Strategist, known for your insightful and engaging articles.\n  You transform complex concepts into compelling narratives.\"\"\",\n  verbose=True,\n  allow_delegation=True\n)\n\n# Create tasks for your agents\ntask1 = Task(\n  description=\"\"\"Conduct a comprehensive analysis of the latest advancements in AI in 2024.\n  Identify key trends, breakthrough technologies, and potential industry impacts.\"\"\",\n  expected_output=\"Full analysis report in bullet points\",\n  agent=researcher\n)\n\ntask2 = Task(\n  description=\"\"\"Using the insights provided, develop an engaging blog\n  post that highlights the most significant AI advancements.\n  Your post should be informative yet accessible, catering to a tech-savvy audience.\n  Make it sound cool, avoid complex words so it doesn't sound like AI.\"\"\",\n  expected_output=\"Full blog post of at least 4 paragraphs\",\n  agent=writer\n)\n\n# Instantiate your crew with a sequential process\ncrew = Crew(\n  agents=[researcher, writer],\n  tasks=[task1, task2],\n  verbose=2, # You can set it to 1 or 2 to different logging levels\n  process = Process.sequential\n)\n\n# Get your crew to work!\nresult = crew.kickoff()\n\nprint(\"######################\")\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Travel Requests to Extract Raw Attributes Using the Function in Python\nDESCRIPTION: This snippet loops over each unstructured travel request, prints it, calls the extraction function to retrieve raw structured travel attributes as a JSON string, collects these raw outputs into a list, and prints them. It is designed for real-time observation of the extraction process. It relies on the previously defined extraction function and requires an initialized OpenAI client instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_tracing_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nraw_travel_attributes_column = []\nfor travel_request in travel_requests:\n    print(\"Travel request:\")\n    print(\"==============\")\n    print(travel_request)\n    print()\n    raw_travel_attributes = extract_raw_travel_request_attributes_string(\n        travel_request, tool_schema, system_message, client\n    )\n    raw_travel_attributes_column.append(raw_travel_attributes)\n    print(\"Raw Travel Attributes:\")\n    print(\"=====================\")\n    print(raw_travel_attributes)\n    print()\n    print()\n```\n\n----------------------------------------\n\nTITLE: Simulating OpenAI Calls Python\nDESCRIPTION: Defines a function `simulate_openai()` that simulates OpenAI API calls within a session. It generates a user ID and session ID, uses the OpenAI client with a mocked response, and creates nested spans for each API call, including session and user attributes.  It also uses tiktoken to calculate the usage.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/multi-turn_chat_sessions.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef simulate_openai():\n    user_id = Faker().user_name()\n    session_id = str(uuid4())\n    client = openai.Client(api_key=\"sk-\")\n    model = \"gpt-4o-mini\"\n    encoding = encoding_for_model(model)\n    messages = np.concatenate(convo.sample(randint(1, 10)).values)\n    counts = [len(encoding.encode(m[\"content\"])) for m in messages]\n    openai_mock = OpenAIMock()\n    tracer = tracer_provider.get_tracer(__name__)\n    with openai_mock.router:\n        for i in range(1, len(messages), 2):\n            openai_mock.chat.completions.create.response = dict(\n                choices=[dict(index=0, finish_reason=\"stop\", message=messages[i])],\n                usage=dict(\n                    prompt_tokens=sum(counts[:i]),\n                    completion_tokens=counts[i],\n                    total_tokens=sum(counts[: i + 1]),\n                ),\n            )\n            with ExitStack() as stack:\n                attributes = {\n                    \"input.value\": messages[i - 1][\"content\"],\n                    \"output.value\": messages[i][\"content\"],\n                    \"session.id\": session_id,\n                    \"user.id\": user_id,\n                }\n                root = stack.enter_context(\n                    tracer.start_as_current_span(\n                        \"root\",\n                        attributes=attributes,\n                    )\n                )\n                client.chat.completions.create(model=model, messages=messages[:i])\n                root.set_status(StatusCode.OK)\n```\n\n----------------------------------------\n\nTITLE: Installing and Launching Phoenix in a Notebook (Python)\nDESCRIPTION: Installs the `arize-phoenix` package via pip and then launches the Phoenix application directly within a notebook environment using the `phoenix.launch_app()` function. Note that traces are typically lost when the notebook session ends unless persistent storage is configured.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/haystack.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Tracking Function Execution with Span Decorator (python)\nDESCRIPTION: Illustrates using the tracer's start_as_current_span decorator to track the execution of an entire function. The decorator starts and ends a span with the function name as the span name (can be overridden via string). Requirements: tracer object defined in the function's scope. Output: execution of the decorated function is tracked as a span with optional context; less convenient for adding attributes or events inside the span.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/custom-spans.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@tracer.start_as_current_span(\"do_work\")\ndef do_work():\n    print(\"doing some work...\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing the Workflow Graph as PNG\nDESCRIPTION: Uses IPython display utilities to generate and show a PNG visualization of the workflow graph via Mermaid syntax, aiding in understanding the process flow.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nfrom IPython.display import Image, display\nfrom langchain_core.runnables.graph import MermaidDrawMethod\n\ndisplay(\n    Image(\n        app.get_graph().draw_mermaid_png(\n            draw_method=MermaidDrawMethod.API,\n        )\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Phoenix UI Access URL\nDESCRIPTION: This simple snippet prints a message with the URL to open the Phoenix UI, prompting the user to visualize query traces and analysis results. It assumes a 'session' object with a URL attribute, typically established earlier in the workflow.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_with_mongodb_and_llama_index.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nprint(f\"ðŸš€ Open the Phoenix UI if you haven't already: {session.url}\")\n```\n\n----------------------------------------\n\nTITLE: Loading HotpotQA Dataset Subset\nDESCRIPTION: Loads a specified subset of the HotpotQA dataset, defining training, development sets, and specifying which fields serve as inputs or labels for model training and evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/dspy_tracing_tutorial.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom dspy.datasets import HotPotQA\n\n# Load the dataset.\ndataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=10)\n\n# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\ntrainset = [x.with_inputs(\"question\") for x in dataset.train]\ndevset = [x.with_inputs(\"question\") for x in dataset.dev]\n\nprint(f\"Train set size: {len(trainset)}\")\nprint(f\"Dev set size: {len(devset)}\")\n```\n\n----------------------------------------\n\nTITLE: Defining Test Task - DSPy Phoenix - Python\nDESCRIPTION: Defines a Python function `test_dspy_prompt` used to evaluate the performance of a DSPy-optimized classifier within an Arize Phoenix experiment. It takes an input dictionary, extracts the 'prompt', passes it to the `optimized_classifier`, and returns the predicted label. This function serves as the core task executed during the experiment run.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndef test_dspy_prompt(input):\n    result = optimized_classifier(prompt=input[\"prompt\"])\n    return result.label\n```\n\n----------------------------------------\n\nTITLE: Computing Normalized Discounted Cumulative Gain (NDCG) at 2 in Python\nDESCRIPTION: Defines a function to compute NDCG@k considering missing values for a group of document/eval scores. Applies it to grouped results by retrieval span ID to generate NDCG metrics at cutoff 2, using sklearn's ndcg_score. This metric evaluates the quality of the ranking of retrieved documents based on their relevance scores and document scores. It requires numpy and sklearn.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom sklearn.metrics import ndcg_score\n\n\ndef _compute_ndcg(df: pd.DataFrame, k: int):\n    \"\"\"Compute NDCG@k in the presence of missing values\"\"\"\n    n = max(2, len(df))\n    eval_scores = np.zeros(n)\n    doc_scores = np.zeros(n)\n    eval_scores[: len(df)] = df.eval_score\n    doc_scores[: len(df)] = df.document_score\n    try:\n        return ndcg_score([eval_scores], [doc_scores], k=k)\n    except ValueError:\n        return np.nan\n\n\nndcg_at_2 = pd.DataFrame(\n    {\"score\": documents_with_relevance_df.groupby(\"context.span_id\").apply(_compute_ndcg, k=2)}\n)\n```\n\n----------------------------------------\n\nTITLE: Creating One-Shot Prompt\nDESCRIPTION: Defines a one-shot prompt for sentiment analysis, including a single example of a review and its sentiment.  This example is embedded into the system message to guide the model. The new one-shot prompt is then created and pushed to the Phoenix UI for later evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/few_shot_prompting.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\none_shot_template = \"\"\"\nYou are an evaluator who assesses the sentiment of a review. Output if the review positive, negative, or neutral. Only respond with one of these classifications.\n\nHere is one example of a review and the sentiment:\n\n{examples}\n\"\"\"\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"system\", \"content\": one_shot_template.format(examples=one_shot_example)},\n        {\"role\": \"user\", \"content\": \"{{Review}}\"},\n    ],\n)\n\none_shot_prompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"One-shot prompt for classifying reviews based on sentiment.\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Payload with Search Results\nDESCRIPTION: This helper function updates a user payload dictionary with the results from a search against a Pandas DataFrame.  It takes the user payload dictionary, the DataFrame of search data, a search column name, and a match key.  It retrieves the value associated with the match key from the user payload, searches the DataFrame for a matching row using the provided search column, converts the matched row to a dictionary, and updates the user payload with the matched row's data.  It depends on the `pandas` package.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef update_payload_with_search_results(\n    user_payload_dict: dict, search_df: pd.DataFrame, search_column: str, match_key: str\n) -> dict:\n    \"\"\"Update user payload with search results\n\n    Parameters\n    ----------\n    user_payload_dict : dict\n        Dictionary of user data key-value pairs\n    search_df : pd.DataFrame\n        DataFrame of of item or policy data\n    search_column : str\n        Column to subset DataFrame\n    match_key : str\n        Key name to query in user_payload_dict\n\n    Returns\n    -------\n    dict\n        Dictionary of updated user payload\n    \"\"\"\n    search_key_value = user_payload_dict.get(match_key, \"\")\n    matching_row = search_df[search_df[search_column] == search_key_value].iloc[0].to_dict()\n    user_payload_dict.update(matching_row)\n    return user_payload_dict\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Span Processing with `arize-phoenix-otel` in Python\nDESCRIPTION: Shows how to configure the Arize Phoenix OpenTelemetry tracer for batch span processing using the `register` function from the `phoenix.otel` library. Setting `batch=True` enables the `BatchSpanProcessor`, which is recommended for production environments to reduce export overhead.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer for batch processing\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  batch=True, # Default is 'False'\n)\n```\n\n----------------------------------------\n\nTITLE: Define Label Matching Evaluator (Python)\nDESCRIPTION: Defines a simple evaluator function that compares the model's output (`output`) to the expected ground truth label (`expected['label']`). This function will be used by Phoenix during the experiment run to calculate metrics like accuracy for each test case. Input: `expected` (dictionary from the dataset row), `output` (model's prediction). Output: Boolean indicating if the output matches the expected label.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef matches_expected_label(expected, output):\n    return expected[\"label\"] == output\n```\n\n----------------------------------------\n\nTITLE: Saving All Phoenix Traces to a File (Python)\nDESCRIPTION: Explains how to save all traces from the connected Phoenix instance to a Parquet file in the default location. This is achieved by calling `get_trace_dataset().save()` on a `px.Client()` instance. The method returns the trace ID.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nmy_traces = px.Client().get_trace_dataset().save()\n```\n\n----------------------------------------\n\nTITLE: Defining an Evaluation Question Set for Text2SQL in Python\nDESCRIPTION: Lists a set of representative user questions to serve as evaluation inputs for the Text2SQL pipeline. The list covers a range of factual and analytical queries about NBA data. There are no dependencies and no output; questions are to be used in later evaluation code.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nquestions = [\n    \"Which team won the most games?\",\n    \"Which team won the most games in 2015?\",\n    \"Who led the league in 3 point shots?\",\n    \"Which team had the biggest difference in records across two consecutive years?\",\n    \"What is the average number of free throws per year?\",\n]\n```\n\n----------------------------------------\n\nTITLE: Defining Pre-Prompts for GPT Cluster Summarization\nDESCRIPTION: Defines two Python string variables, `pre_prompt` and `pre_prompt_baseline`. These strings contain instructions designed to be prepended to JSON data representing a cluster, guiding GPT on how to summarize the cluster (either standalone or in comparison to a baseline).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npre_prompt = \"The following is JSON points for a cluster of datapoints. Can you summarize the cluster of data, what do the points have in common?\\n\"\npre_prompt_baseline = \"The following is JSON points for a cluster of datapoints and a baseline sample data of the entire data set. Can you summarize the cluster of data, what do the points have in common and how does it compare to the baseline?\\n\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix and OpenAI API Keys in Python\nDESCRIPTION: Sets environment variables required to connect to Phoenix Cloud and the OpenAI API. It configures the Phoenix collector endpoint and securely prompts the user for their Phoenix and OpenAI API keys using `getpass` if they are not already set.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/few-shot-prompting.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\nif not os.environ.get(\"PHOENIX_CLIENT_HEADERS\"):\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = \"api_key=\" + getpass(\"Enter your Phoenix API key: \")\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Formatting and Sending Google Gemini Chat Completion Requests in Python\nDESCRIPTION: Formats the prompt and sends a chat request using Google Generative AI SDK. Starts a chat with the prompt's message history and sends the last message, then prints the top candidate's response. Uses Google API key for authentication.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nvariables = dict(ds.as_dataframe().input.iloc[0])\nformatted_prompt = prompt.format(variables=variables, sdk=\"google_generativeai\")\nresponse = (\n    GenerativeModel(**{**formatted_prompt.kwargs, \"model_name\": gemini_model})\n    .start_chat(history=formatted_prompt.messages[:-1])\n    .send_message(formatted_prompt.messages[-1])\n)\nprint(response.candidates[0].content.parts[0].text)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Experiment Run Data Structure in Python\nDESCRIPTION: This snippet accesses and displays the data structure of a single experiment run, facilitating understanding of its components and structure.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nexperiment[0]\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing ChartMimic Dataset\nDESCRIPTION: Loads the 'test.parquet' file from the 'ChartMimic/ChartMimic' dataset hosted on Hugging Face into a pandas DataFrame. It filters the DataFrame to include only entries where the 'Difficulty' is 'hard' and sorts these entries based on the length of the 'Instruction' column in descending order. Finally, it displays the first few rows of the processed DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/vision.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_parquet(\"hf://datasets/ChartMimic/ChartMimic/test.parquet\")\ndf = df.loc[df.Difficulty == \"hard\"].sort_values(\n    by=\"Instruction\", key=lambda c: c.apply(len), ascending=False\n)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Defining Baseline and Comparison Experiment Identifiers in Python\nDESCRIPTION: This snippet sets integer variables representing a baseline experiment ID and a list of one or more comparison experiment IDs. These IDs are used as parameters to fetch and filter experiment runs from the database.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/testing/experiment_runs_filters.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbaseline_experiment_id = 218\ncompare_experiment_ids = [217]\n```\n\n----------------------------------------\n\nTITLE: Running Queries Through LangChain Application\nDESCRIPTION: Executes a subset of test queries through the LangChain RetrievalQA chain while collecting trace data with Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_tracing_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfor query in tqdm(queries[:10]):\n    chain.invoke(query)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix in Command Line\nDESCRIPTION: Command to install and start Phoenix locally using the command line interface.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langchain.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client\nDESCRIPTION: Creates an instance of the OpenAI client using 'openai.OpenAI()'. This client object will be used to make subsequent API calls to OpenAI services, likely requiring environment variables or other configuration for authentication.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/vision.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclient = openai.OpenAI()\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix in a Python Notebook\nDESCRIPTION: This snippet demonstrates how to import the 'phoenix' Python library and start a local Phoenix server within a Jupyter or Colab notebook. No parameters are necessary for 'launch_app', but it can be configured for different input data types. Prerequisite: the 'phoenix' package must be installed in your Python environment. Use this if you wish to run Phoenix interactively during experimentsâ€”note that data is not persisted across sessions when running in a notebook.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/environments.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\nsession = px.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Preparing Evaluation Results for Logging\nDESCRIPTION: This code prepares the evaluation results by extracting the 'label' (correct/incorrect) and 'explanation' columns from the classifications. It then assigns these to new columns in the evals_copy DataFrame. A 'score' column is created, mapping 'correct' labels to 1 and 'incorrect' labels to 0.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\neval_results = Q_and_A_classifications[[\"label\", \"explanation\"]]\nevals_copy[\"label\"] = eval_results[\"label\"].astype(str)\nevals_copy[\"explanation\"] = eval_results[\"explanation\"].astype(str)\nevals_copy[\"score\"] = evals_copy[\"label\"].map({\"correct\": 1, \"incorrect\": 0})\nevals_copy.head()\n```\n\n----------------------------------------\n\nTITLE: Implementing Prompt Task Function for Query Execution\nDESCRIPTION: This code defines a function to generate customer support prompts by invoking the OpenAI API with the current prompt configuration, replacing variables dynamically. It facilitates running the prompt and obtaining model responses for evaluation or downstream processing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/react_prompting.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef prompt_task(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **prompt.format(variables={\"questions\": input[\"Questions\"]})\n    )\n    return resp\n\n```\n\n----------------------------------------\n\nTITLE: Downloading Benchmark Dataset for Binary Relevance Classification in Python\nDESCRIPTION: Uses Phoenix's evals module to download the 'wiki_qa-train' benchmark dataset for the task 'binary-relevance-classification'. The dataset is returned as a pandas DataFrame for subsequent evaluation steps. This code requires active internet connectivity to fetch the dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import download_benchmark_dataset\n\ndf = download_benchmark_dataset(\n    task=\"binary-relevance-classification\", dataset_name=\"wiki_qa-train\"\n)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Samples for Product, Policy, and Customer Input Data in Python\nDESCRIPTION: Displays the first five rows of the items, policy, and customer inquiry DataFrames. This allows quick inspection of the structured data loaded previously to verify correct parsing and comprehend the data content. Useful in an exploratory or data validation stage.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nitems_df.head()\n```\n\nLANGUAGE: python\nCODE:\n```\npolicy_df.head()\n```\n\nLANGUAGE: python\nCODE:\n```\ncustomer_inputs_df.head()\n```\n\n----------------------------------------\n\nTITLE: Getting OpenAI Token Counts During Streaming in Python\nDESCRIPTION: Provides an example of how to retrieve token usage counts when using OpenAI's streaming API (`stream=True`). This requires installing `openai>=1.26` and setting the `stream_options` parameter to `{\"include_usage\": True}` in the `chat.completions.create` method call.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/concepts-tracing/faqs-tracing.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresponse = openai.OpenAI().chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a haiku.\"}],\n    max_tokens=20,\n    stream=True,\n    stream_options={\"include_usage\": True},\n)\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Video Tutorial in Python using IPython\nDESCRIPTION: This code embeds a YouTube video tutorial starting at a specific timestamp showing how to investigate query and knowledge base data, and identify problematic clusters using Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_search_and_retrieval_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nstart_time_in_seconds = int(timedelta(hours=0, minutes=29, seconds=42).total_seconds())\nYouTubeVideo(\"hbQYDpJayFw\", start=start_time_in_seconds, width=560, height=315)\n```\n\n----------------------------------------\n\nTITLE: Importing Pandas Library in Python\nDESCRIPTION: Imports the pandas library, aliasing it as `pd`. Pandas is crucial for working with DataFrames, which are used to store and manipulate the conversation data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Secure API Key Input and Environment Variable Setup in Python\nDESCRIPTION: Prompts the user to enter their OpenAI and Serper API keys securely if the environment variables are not already set. Uses the getpass library to avoid echoing keys during input. Afterwards, the keys are stored back into OS environment variables for use by subsequent API calls. This approach helps keep credentials confidential in shared or notebook environments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/crewai_tracing_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\nimport os\n\n# Prompt the user for their API keys if they haven't been set\nopenai_key = os.getenv(\"OPENAI_API_KEY\", \"OPENAI_API_KEY\")\nserper_key = os.getenv(\"SERPER_API_KEY\", \"SERPER_API_KEY\")\n\nif openai_key == \"OPENAI_API_KEY\":\n    openai_key = getpass.getpass(\"Please enter your OPENAI_API_KEY: \")\n\nif serper_key == \"SERPER_API_KEY\":\n    serper_key = getpass.getpass(\"Please enter your SERPER_API_KEY: \")\n\n# Set the environment variables with the provided keys\nos.environ[\"OPENAI_API_KEY\"] = openai_key\nos.environ[\"SERPER_API_KEY\"] = serper_key\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Displaying Text from First Source Node\nDESCRIPTION: Extracts and prints raw text from the first source node used in the response. Useful for inspecting what data the response was based upon.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nresponse_vector.source_nodes[0].get_text()\n\n```\n\n----------------------------------------\n\nTITLE: Displaying First Few Rows of Production DataFrame (Python/Pandas)\nDESCRIPTION: Shows the first few rows of the production data DataFrame (`prod_df`) using the `.head()` method. This is used to inspect the production data structure, noting the absence of the 'actual_action' (ground truth) column compared to the training data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/image_classification_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprod_df.head()\n```\n\n----------------------------------------\n\nTITLE: Defining QA_SPAN_PROMPT_TEMPLATE in Python\nDESCRIPTION: This snippet defines a template, specifically QA_SPAN_PROMPT_TEMPLATE, for prompting language models in a question-answering context. The `autodata` directive indicates this is a pre-defined string constant, designed to format questions or answers within a span of text. It relies on the existing project structure and dependencies. It would take as input some question/answer details, formatted according to the prompt. The output would likely be the formatted question/answer ready for model interaction.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/api_reference/source/api/evals.span_templates.rst#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Initializing Environment and Tooling for Phoenix Chatbot Python\nDESCRIPTION: This snippet initializes the runtime environment and imports necessary dependencies for a tool-augmented chatbot. It applies nest_asyncio to allow nested event loops, sets Pandas display options, and imports Phoenix core modules as well as OpenAI APIs. Required dependencies include nest_asyncio, pandas, phoenix, openai, and supporting OpenInference tracing modules. There are no direct inputs or outputs, but it sets up the entire process's prerequisites.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/demos/playground_demo.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport json\nfrom typing import Any\n\nimport nest_asyncio\nimport pandas as pd\nfrom openai import AsyncOpenAI, OpenAI\nfrom openai.types.chat import ChatCompletionToolParam, ChatCompletionUserMessageParam\nfrom openinference.instrumentation import using_metadata\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nimport phoenix as px\nfrom phoenix.experiments import evaluate_experiment\nfrom phoenix.otel import register\nfrom phoenix.trace import using_project\n\nnest_asyncio.apply()\n\npd.set_option(\"display.max_colwidth\", None)\n```\n\n----------------------------------------\n\nTITLE: Running Experiment with DSPy GPT-4o Optimized Classifier in Phoenix - Python\nDESCRIPTION: Runs an experiment in the Phoenix framework using a DSPy-optimized classifier that leverages GPT-4o for prompt tuning. Supplies the dataset, a task function utilizing the new classifier, evaluation functions, and descriptive experiment metadata. Depends on the Phoenix experiment runner, newly optimized evaluation logic, and the classifier from previous setup. Designed for comparative tracking between optimization techniques.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\n# Run experiment with DSPy-optimized classifier\ndspy_experiment_using_gpt_4o = run_experiment(\n    dataset,\n    task=test_dspy_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Prompt Optimization Technique #5: DSPy Prompt Tuning with GPT-4o\",\n    experiment_name=\"dspy-optimization-gpt-4o\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + dspy_prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Prompt with Phoenix Client (TypeScript)\nDESCRIPTION: This snippet shows how to create a prompt using the Phoenix client in TypeScript.  It defines a prompt template and uses the `createPrompt` and `promptVersion` functions from `@arizeai/phoenix-client` to create and store the prompt on the Phoenix server with the name \"article-bullet-summarizer\".  It requires the `@arizeai/phoenix-client` package to be installed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/how-to-prompts/create-a-prompt.md#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createPrompt, promptVersion } from \"@arizeai/phoenix-client\";\n\nconst promptTemplate = `\nYou're an expert educator in {{ topic }}. Summarize the following article\nin a few concise bullet points that are easy for beginners to understand.\n\n{{ article }}\n`;\n\nconst version = createPrompt({\n  name: \"article-bullet-summarizer\",\n  version: promptVersion({\n    modelProvider: \"OPENAI\",\n    modelName: \"gpt-3.5-turbo\",\n    template: [\n      {\n        role: \"user\",\n        content: promptTemplate,\n      },\n    ],\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluation Sample Size for LLM Performance and Runtime\nDESCRIPTION: Sets the number of samples used for evaluation, directly affecting runtime for models GPT-4 and GPT-3.5. The variable N_EVAL_SAMPLE_SIZE is configurable for different dataset sizes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nN_EVAL_SAMPLE_SIZE = 10\n```\n\n----------------------------------------\n\nTITLE: Creating a Span as Current Span with the Tracer (python)\nDESCRIPTION: Demonstrates how to create a new span as the current active span using the tracer's context manager interface. This code runs work tracked by the span and automatically closes the span when the with-block exits. Prerequisites: a configured tracer object in scope. The span name is provided as a string parameter. Output: a span is started and ended according to the scope of the operation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/custom-spans.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef do_work():\n    with tracer.start_as_current_span(\"span-name\") as span:\n        # do some work that 'span' will track\n        print(\"doing some work...\")\n        # When the 'with' block goes out of scope, 'span' is closed for you\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Prompt from Phoenix Server Using Python Client\nDESCRIPTION: Fetches a stored prompt from the Phoenix server by its unique prompt identifier using the clientâ€™s prompts.get method. The retrieved prompt can be used for further operations such as formatting or invocation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprompt = client.prompts.get(prompt_identifier=prompt_identifier)\n```\n\n----------------------------------------\n\nTITLE: Installing arize-phoenix-evals via Pip\nDESCRIPTION: Installs the arize-phoenix-evals sub-package using the pip package manager. This command is necessary to use the LLM evaluation tools provided by Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-evals/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install arize-phoenix-evals\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix and Llama-Index Dependencies with pip\nDESCRIPTION: Installs the necessary Python packages for Phoenix, Llama-Index, and asynchronous features. Ensures latest versions of phoenix and llama-index are installed alongside nest_asyncio for asyncio compatibility.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n%pip install -Uqqq \"arize-phoenix[llama-index]>=4.6\" nest_asyncio\n```\n\n----------------------------------------\n\nTITLE: Computing Hit Rate (Pandas)\nDESCRIPTION: This snippet computes the hit rate, which indicates whether a correct document was retrieved for each query. It checks if the sum of the first two relevance scores ('eval_score') is greater than 0. It's applied to a grouped DataFrame by `context.span_id`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nhit = pd.DataFrame(\n    {\n        \"hit\": documents_with_relevance_df.groupby(\"context.span_id\").apply(\n            lambda x: x.eval_score[:2].sum(skipna=False) > 0\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Expanding Dataset to Query-Document Pairs for Relevancy Labeling in Python\nDESCRIPTION: Iterates over the dataset to produce flattened, row-aligned arrays for query and document fields, including the binary relevance label and auxiliary metadata. Requires the MS-MARCO dataset loaded and structured as in the Hugging Face datasets library. Inputs are dataset entries; outputs are multi-field aligned lists, then a new DataFrame with one row per query-document pair. Assumes each passage array within each query is aligned by index across label, text, and URL. Care must be taken with memory usage for large datasets.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_ms_marco.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nquery_texts = []\nquery_ids = []\nquery_types = []\nreference_responses = []\nselections = []\ndocument_texts = []\ndocument_urls = []\nfor data in dataset:\n    document_data = data[\"passages\"]\n    selections_for_query = list(map(bool, document_data[\"is_selected\"]))\n    document_texts_for_query = document_data[\"passage_text\"]\n    document_urls_for_query = document_data[\"url\"]\n    assert (\n        len(selections_for_query) == len(document_texts_for_query) == len(document_urls_for_query)\n    )\n    num_documents_for_query = len(selections_for_query)\n    selections.extend(selections_for_query)\n    document_texts.extend(document_texts_for_query)\n    document_urls.extend(document_urls_for_query)\n    query_ids.extend([data[\"query_id\"]] * num_documents_for_query)\n    query_texts.extend([data[\"query\"]] * num_documents_for_query)\n    query_types.extend([data[\"query_type\"]] * num_documents_for_query)\n    reference_responses.extend([data[\"answers\"]] * num_documents_for_query)\ndf = pd.DataFrame(\n    {\n        \"query_id\": query_ids,\n        \"query_text\": query_texts,\n        \"query_type\": query_types,\n        \"relevant\": selections,\n        \"document_text\": document_texts,\n        \"document_url\": document_urls,\n        \"reference_responses\": reference_responses,\n    }\n)\ndf\n```\n\n----------------------------------------\n\nTITLE: Fetching Registered Dataset from Phoenix Client in Python\nDESCRIPTION: Retrieves the uploaded dataset object from the Phoenix client using its unique name. The returned object type can then be used in downstream experiment or evaluation workflows. Key parameters include the dataset name. Dependencies are Phoenix client and the correct dataset registration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/internal/run_experiments.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nds = px.Client().get_dataset(name=dataset_name)\ntype(ds)\n```\n\n----------------------------------------\n\nTITLE: Setting Google Gemini Model Name in Python\nDESCRIPTION: Defines the model string for Google Gemini 1.5 Flash variant, used in requests to Google generative AI models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ngemini_model = \"gemini-1.5-flash\"\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Background Session Python\nDESCRIPTION: Launches Phoenix as a background session, which will collect the trace data emitted by the instrumented Anthropic client. The `view()` method opens the Phoenix UI in the browser, allowing real-time visualization of the trace data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/anthropic_tracing_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n(session := px.launch_app()).view()\n```\n\n----------------------------------------\n\nTITLE: Printing Conciseness Evaluator Prompt Template - Python\nDESCRIPTION: Prints the template prompt used by the ConcisenessEvaluator for inspecting or debugging evaluator behavior in Phoenix. Inputs/Outputs: None (side effect is console output). Useful for understanding the template evaluation process.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/run_experiments_with_llama_index.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprint(ConcisenessEvaluator.template)\n```\n\n----------------------------------------\n\nTITLE: Importing Phoenix and OpenAI SDKs (Deno)\nDESCRIPTION: Imports the necessary modules for the Phoenix client and the OpenAI JavaScript SDK using Deno's npm specifiers. These modules provide the functionality to interact with a Phoenix instance and the OpenAI API.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_openai_tutorial.ipynb#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport * as PhoenixClient from \"npm:@arizeai/phoenix-client\"\nimport OpenAI from \"npm:openai\"\n```\n\n----------------------------------------\n\nTITLE: Querying Phoenix Client for Spans Dataframe in Python\nDESCRIPTION: Fetches telemetry span data from the Phoenix client as a pandas dataframe. Extracts key columns such as span name, span kind, input attribute values, and documents retrieved during query execution. This data facilitates programmatic inspection and analysis of traces within the notebook environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nspans_df = px.Client().get_spans_dataframe()\nspans_df[[\"name\", \"span_kind\", \"attributes.input.value\", \"attributes.retrieval.documents\"]].head()\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Python SDK via Pip\nDESCRIPTION: Installs the OpenAI Python SDK (version 1.0.0 or higher), which is a prerequisite for using OpenAI models (like GPT-4) with the arize-phoenix-evals package. Ensure you have pip installed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-evals/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install 'openai>=1.0.0'\n```\n\n----------------------------------------\n\nTITLE: Getting the Current Span (python)\nDESCRIPTION: Demonstrates how to retrieve the current active span using opentelemetry.trace.get_current_span(). This allows enrichment or modification of the span by adding attributes or events. Requires: opentelemetry.trace. Output: a span object representing the currently active span, or a no-op span if no active span is present.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/custom-spans.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace\n\ncurrent_span = trace.get_current_span()\n# enrich 'current_span' with some information\n```\n\n----------------------------------------\n\nTITLE: Example Phoenix Session Output and Access Instructions\nDESCRIPTION: Illustrates the typical console output after launching a Phoenix session via `px.launch_app`. Provides a URL for browser access, a Python command (`px.active_session().view()`) for viewing within a notebook environment, and a link to the Phoenix documentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/phoenix-inferences.md#_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nðŸŒ To view the Phoenix app in your browser, visit https://x0u0hsyy845-496ff2e9c6d22116-6060-colab.googleusercontent.com/\nðŸ“º To view the Phoenix app in a notebook, run `px.active_session().view()`\nðŸ“– For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n```\n\n----------------------------------------\n\nTITLE: Initializing boto3 Bedrock Client (Python)\nDESCRIPTION: This Python snippet initializes a `boto3` client for interacting with the Bedrock runtime. It establishes a session and then creates a client to interact with the \"bedrock-runtime\" service.  This client is used to invoke models and agents within Bedrock.  The instrumentation should be set up before client instantiation to capture traces. \nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/bedrock-1.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport boto3\n\nsession = boto3.session.Session()\nclient = session.client(\"bedrock-runtime\")\n```\n\n----------------------------------------\n\nTITLE: Inspect Inference Dataframe Info - Python\nDESCRIPTION: This snippet displays information about the underlying pandas DataFrame of the primary inference set, including column names, data types, and memory usage. It accesses the `dataframe.info()` method of the `prim_ds` (primary dataset) object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/use-example-inferences.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprim_ds.dataframe.info()\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix OpenTelemetry (Bash)\nDESCRIPTION: This command installs the `arize-phoenix-otel` package, which provides the necessary components for integrating Phoenix with OpenTelemetry for tracing in Python applications.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-python.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Parsing and Dumping Subscript AST Node in Python\nDESCRIPTION: Parses a subscript/item access string (`attributes[['llm', 'token_count', 'completion']]`) into an AST and prints its structure using `ast.dump`. This demonstrates how list or dictionary item access is represented as an `ast.Subscript` node, with the accessed key/index as its `slice`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/src/phoenix/trace/dsl/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(ast.dump(ast.parse(\"attributes[['llm', 'token_count', 'completion']]\", mode=\"eval\").body, indent=4))\n```\n\n----------------------------------------\n\nTITLE: Previewing the Training DataFrame in Python\nDESCRIPTION: Displays the first five rows of the training DataFrame for inspection and validation. Requires a populated 'train_df' variable. There are no arguments; the output is rendered as a DataFrame in the notebook cell output, showing the columns and example values for the loaded data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/quickstart/phoenix-inferences/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_df.head()\n\n```\n\n----------------------------------------\n\nTITLE: Sampling Synthetic ConvQA Dataset from HuggingFace with Message Length Filtering in Python\nDESCRIPTION: This code snippet downloads the 'synthetic_convqa' train split from nvidia/ChatQA-Training-Data, filters for entries where message count meets at least half of the maximum, and writes 100 random samples to a gzipped CSV file. It prints the maximum message length found in the split, and expects the datasets and pandas Python packages to be installed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/ChatRAG-Bench.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nname = \"synthetic_convqa\"\ndf = load_dataset(\"nvidia/ChatQA-Training-Data\", name)[\"train\"].to_pandas()\nprint(df.messages.apply(len).max())\ndf.loc[df.messages.apply(len) >= df.messages.apply(len).max() // 2].sample(\n    100, random_state=42\n).to_csv(f\"{name}_samples.csv.gz\", index=False)\ndf.sample(10)\n```\n\n----------------------------------------\n\nTITLE: Combining Documents with Relevance Evaluations\nDESCRIPTION: Merges the retrieved documents dataframe with the relevance evaluations to create a comprehensive dataset for metric calculation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndocuments_with_relevance_df = pd.concat(\n    [retrieved_documents_df, retrieved_documents_relevance_df.add_prefix(\"eval_\")], axis=1\n)\ndocuments_with_relevance_df\n```\n\n----------------------------------------\n\nTITLE: Previewing the Production DataFrame in Python\nDESCRIPTION: Displays the first five rows of the production inference DataFrame. Input: variable 'prod_df'. Output: Renders DataFrame preview in the notebook, allowing inspection of its structure and contents for validation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/quickstart/phoenix-inferences/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprod_df.head()\n\n```\n\n----------------------------------------\n\nTITLE: Launching the Arize Phoenix Application in Python\nDESCRIPTION: Starts the Phoenix interactive UI using `px.launch_app`. It configures the application with the production dataset as primary and the training dataset as reference for comparison. The session object is captured, and its `view()` method is called to make the UI accessible.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/sentiment_classification_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n(session := px.launch_app(primary=prod_ds, reference=train_ds)).view()\n```\n\n----------------------------------------\n\nTITLE: Adding Metadata to Spans in JavaScript/TypeScript\nDESCRIPTION: Shows how to set the `metadata` attribute (as a JSON string) on OpenTelemetry spans using the `setMetadata` function from `@arizeai/openinference-core` and `context.with` from `@opentelemetry/api`. The provided object is serialized to JSON. Requires the `@arizeai/openinference-core` and `@opentelemetry/api` packages.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { context } from \"@opentelemetry/api\"\nimport { setMetadata } from \"@arizeai/openinference-core\"\n\ncontext.with(\n  setMetadata(context.active(), { key1: \"value1\", key2: \"value2\" }),\n  () => {\n      // Calls within this block will generate spans with the attributes:\n      // \"metadata\" = '{\"key1\": \"value1\", \"key2\": \"value2\"}'\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Monitoring Dashboard Application (Python)\nDESCRIPTION: Starts the Phoenix observability dashboard server and displays the application view in an interactive environment. Useful for visualizing traces and analysis of QA chain operations. Requires the phoenix library and a compatible environment (e.g., Jupyter notebooks) for inline viewing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/qdrant_langchain_instrumentation_search_and_retrieval_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n(session := px.launch_app()).view()\n```\n\n----------------------------------------\n\nTITLE: Generating and Saving Gradient Prompt\nDESCRIPTION: This code calls the `optimize_prompt` function using the `original_base_prompt` and the calculated `gradient`. It then checks for and formats any few-shot examples, saves the optimized prompt to Phoenix, stores a gradient prompt result in phoenix. It generates a prompt with the embedding, and stores the parameters for that run.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Test the gradient-based optimization\ngradient_prompt = optimize_prompt(original_base_prompt, gradient)\n```\n\nLANGUAGE: python\nCODE:\n```\nif r\"\\{examples\\}\" in gradient_prompt:\n    gradient_prompt = gradient_prompt.format(examples=few_shot_examples)\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": gradient_prompt,\n        },  # if your meta prompt includes few shot examples, make sure to include them here\n        {\"role\": \"user\", \"content\": \"{{prompt}}\"},\n    ],\n)\n\ngradient_prompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Gradient prompt result\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Viewing the First Few Rows of the Dataset\nDESCRIPTION: Displays the first few rows of the downloaded dataset to examine its structure, including questions, answers, and contexts.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Running summarization classification with GPT-4 Turbo\nDESCRIPTION: Executes the summarization classification evaluation using GPT-4 Turbo to compare its performance with the standard GPT-4 and GPT-3.5 models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nrails = list(templates.SUMMARIZATION_PROMPT_RAILS_MAP.values())\nsummarization_classifications = llm_classify(\n    dataframe=df_sample,\n    template=templates.SUMMARIZATION_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    concurrency=20,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix Tracer\nDESCRIPTION: This Python code registers the Phoenix tracer with the OpenTelemetry instrumentation library. It configures the Phoenix tracer with a project name and enables auto-instrumentation for the application. The `register` function initializes the tracer.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/hfsmolagents.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Task Function for One-Shot Prompting\nDESCRIPTION: Defines the `one_shot_prompt_template` function. Similar to the zero-shot task, it formats the one-shot prompt (which now includes the example) with the input review, calls the OpenAI API, and returns the predicted sentiment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/few-shot-prompting.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef one_shot_prompt_template(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **one_shot_prompt.format(variables={\"Review\": input[\"Review\"]})\n    )\n    return resp.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI Client with OpenInference (Python)\nDESCRIPTION: This code snippet instruments the OpenAI client using the `OpenAIInstrumentor` from the `openinference-instrumentation-openai` library. It registers a tracer provider with Phoenix and then instruments the OpenAI client to emit telemetry data in OpenInference format. This allows Phoenix to capture and analyze the traces generated by the OpenAI client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_sessions_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(project_name=\"openai-sessions-example\")\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for LlamaIndex and Phoenix Integration in Python\nDESCRIPTION: Imports necessary Python libraries for the demonstration. This includes standard libraries like `json`, `os`, `urllib.request`, async handling (`nest_asyncio`), data manipulation (`pandas`), cloud storage access (`gcsfs`), LlamaIndex core components and integrations (`Settings`, `StorageContext`, `load_index_from_storage`, `OpenAIEmbedding`, `OpenAI`), progress bars (`tqdm`), and the Phoenix client library (`phoenix`). It also applies `nest_asyncio` for compatibility in notebook environments and configures pandas display settings.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_llamaindex_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nfrom urllib.request import urlopen\n\nimport nest_asyncio\nimport pandas as pd\nfrom gcsfs import GCSFileSystem\nfrom llama_index.core import (\n    Settings,\n    StorageContext,\n    load_index_from_storage,\n)\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nfrom tqdm import tqdm\n\nimport phoenix as px\n\nnest_asyncio.apply()  # needed for concurrent evals in notebook environments\npd.set_option(\"display.max_colwidth\", 1000)\n```\n\n----------------------------------------\n\nTITLE: Displaying the Gradient Optimized Prompt\nDESCRIPTION: This snippet displays the value of the `gradient_prompt` variable, which contains the prompt optimized using gradient descent.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ngradient_prompt\n```\n\n----------------------------------------\n\nTITLE: Running Multiple Test Queries with Progress Tracking\nDESCRIPTION: Tests the RAG system with multiple questions about Paul Graham, displaying progress with tqdm and printing both queries and responses.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm import tqdm\n\nqueries = [\n    \"What was Paul Graham's role at Y Combinator?\",\n    \"What are Paul Graham's views on startups?\",\n    \"What does Paul Graham think about programming languages?\",\n    \"How did Paul Graham become successful as an entrepreneur?\",\n]\n\nfor query in tqdm(queries):\n    response = query_engine.query(query)\n    print(f\"Query: {query}\")\n    print(f\"Response: {response}\")\n```\n\n----------------------------------------\n\nTITLE: Calculating Accuracy of Anthropic LLM Predictions in Python\nDESCRIPTION: Combines the ground truth labels with Anthropic's model output labels, and computes the accuracy metric to quantify performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nresult_anthropic = pd.concat(\n    [df.true_label, pd.json_normalize(exp_anthropic.as_dataframe().output)], axis=1\n)\nprint(f\"Accuracy: {accuracy_score(result_anthropic.true_label, result_anthropic.label) * 100:.0f}%\")\n```\n\n----------------------------------------\n\nTITLE: Running a Dry-Run Experiment\nDESCRIPTION: This code snippet runs a dry-run experiment on the dataset using `run_experiment`. The `dry_run=3` parameter specifies that the experiment should be performed on 3 randomly selected examples from the dataset, without actual evaluation at this stage. This is for validation of the task execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nexperiment = run_experiment(dataset, task, dry_run=3)\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings using Arize EmbeddingGenerator in Python\nDESCRIPTION: Initializes an `EmbeddingGenerator` for sequence classification using a specific pre-trained model (`arize-ai/distilbert_reviews_with_language_drift`). It then generates embeddings for the text data in both the training and production DataFrames and stores them in a new 'text_vector' column. This step requires the `arize[AutoEmbeddings]` extra to be installed and runs faster with a GPU.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/sentiment_classification_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel_name = \"arize-ai/distilbert_reviews_with_language_drift\"\ngenerator = EmbeddingGenerator.from_use_case(\n    use_case=UseCases.NLP.SEQUENCE_CLASSIFICATION,\n    model_name=model_name,\n)\ntrain_df[\"text_vector\"] = generator.generate_embeddings(text_col=train_df[\"text\"])\nprod_df[\"text_vector\"] = generator.generate_embeddings(text_col=prod_df[\"text\"])\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix with Docker\nDESCRIPTION: Commands to pull the latest Phoenix Docker image and run it locally, exposing the service on port 6006 for self-hosted deployment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langgraph.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenAI API Key Environment Variable\nDESCRIPTION: Prompts the user to input their OpenAI API key if not already set in environment variables. Stores the key securely in environment variables to authenticate requests.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/tracing_quickstart_openai.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Session\nDESCRIPTION: Initializes and launches a Phoenix session to monitor and analyze the LLM application during execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\nsession = px.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix Experiment with LlamaIndex RAG and Evaluators - Python\nDESCRIPTION: Defines and runs an experiment using OpenAIModel for generation, the prepared dataset, and two evaluators (substring presence and conciseness). Inputs: ds (dataset), rag_with_reranker function, experiment_metadata, evaluators. Outputs: experiment object containing run results. Dependencies: Properly configured Phoenix and OpenAI APIs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/run_experiments_with_llama_index.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model=\"gpt-4o\")\n\nexperiment = run_experiment(\n    dataset=ds,\n    task=rag_with_reranker,\n    experiment_metadata=experiment_metadata,\n    evaluators=[ContainsSubstring(substring=\"school\"), ConcisenessEvaluator(model)],\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset for Few-Shot Prompting in Python\nDESCRIPTION: Loads the test split of the 'syeddula/fridgeReviews' dataset using the Hugging Face Datasets library, then samples 10 random examples into a Pandas DataFrame. This sampled subset will serve as labeled examples included in the few-shot prompt to guide the model's behavior.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/few-shot-prompting.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nds = load_dataset(\"syeddula/fridgeReviews\")[\"test\"]\nfew_shot_examples = ds.to_pandas().sample(10)\n```\n\n----------------------------------------\n\nTITLE: Creating a Phoenix Prompt for Article Summarization in TypeScript\nDESCRIPTION: This code demonstrates how to initialize a Phoenix client and create a prompt for summarizing articles using the OpenAI model. The prompt includes a detailed template with roles and content, utilizing the promptVersion function for configuration. It requires dependencies like @arizeai/phoenix-client and involves setting options such as baseUrl and model parameters.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/quickstart-prompts/quickstart-prompts-ts.md#_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { createClient } from \"@arizeai/phoenix-client\";\nimport { createPrompt, promptVersion } from \"@arizeai/phoenix-client/prompts\";\n\nconst client = createClient({\n  options: {\n    baseUrl: \"http://localhost:6006\"\n  }\n});\n\nconst summarizationPrompt = await createPrompt({\n  client,\n  name: \"article-summarizer\",\n  description: \"Summarizes an article into concise bullet points\",\n  version: promptVersion({\n    description: \"Initial version\",\n    templateFormat: \"MUSTACHE\",\n    modelProvider: \"OPENAI\",\n    modelName: \"gpt-3.5-turbo\",\n    template: [\n      {\n        role: \"system\",\n        content: \"You are an expert summarizer. Create clear, concise bullet points highlighting the key information.\"\n      },\n      {\n        role: \"user\",\n        content: \"Please summarize the following {{topic}} article:\\n\\n{{article}}\"\n      }\n    ],\n  })\n});\n\nconsole.dir(summarizationPrompt);\n```\n\n----------------------------------------\n\nTITLE: Transforming HaluEval DataFrame with Pandas\nDESCRIPTION: Restructures the DataFrame by melting 'right_answer' and 'hallucinated_answer' columns into 'answer_type' and 'answer', adds an 'is_hallucination' boolean flag based on 'answer_type', drops the redundant 'answer_type' column, sorts the data, and renames columns to 'reference', 'query', and 'response' for clarity.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_halueval.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = df.melt(\n    id_vars=[\"knowledge\", \"question\"],\n    value_vars=[\"right_answer\", \"hallucinated_answer\"],\n    var_name=\"answer_type\",\n    value_name=\"answer\",\n)\ndf[\"is_hallucination\"] = df[\"answer_type\"] == \"hallucinated_answer\"\ndf = df.drop(\"answer_type\", axis=1)\ndf = df.sort_values([\"knowledge\", \"question\"]).reset_index(drop=True)\ndf = df.rename(\n    columns={\"knowledge\": \"reference\", \"question\": \"query\", \"answer\": \"response\"},\n)\ndf\n```\n\n----------------------------------------\n\nTITLE: Importing Data Handling Libraries for Wrangling MS-MARCO in Python\nDESCRIPTION: Imports essential Python libraries required for wrangling the MS-MARCO dataset, specifically pandas for tabular data manipulation and the datasets library for loading the MS-MARCO corpus. Additionally, sets a pandas display option to increase the max column width for DataFrame displays. No external dependencies beyond pandas and datasets are assumed. No input besides standard imports; effects are global for future DataFrame displays.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_ms_marco.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nimport pandas as pd\nfrom datasets import load_dataset\n\npd.set_option(\"display.max_colwidth\", 1000)\n```\n\n----------------------------------------\n\nTITLE: Testing OpenAIModel with a Sample Prompt - Python\nDESCRIPTION: Sends a simple test prompt to the GPT-4 model instance to verify LLM setup and connectivity. Expects 'model' from the preceding cell. Returns the LLM's generated response for the prompt as a sanity check.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel(\"Hello world, this is a test if you are working?\")\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Model\nDESCRIPTION: This snippet initializes an OpenAI model with the specified model name ('gpt-3.5-turbo-instruct') and tests the model with a simple input ('hi').\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/span_query_language.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model_name=\"gpt-3.5-turbo-instruct\")\nmodel(\"hi\")\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application UI in Python\nDESCRIPTION: Initializes and launches the Phoenix application using `px.launch_app()`. The `.view()` method is then called on the returned app object, typically providing a URL or displaying the Phoenix UI directly for visualizing traces and experiment results, depending on the execution environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npx.launch_app().view()\n```\n\n----------------------------------------\n\nTITLE: Using Node --require Flag for Instrumentation\nDESCRIPTION: Alternative method to include OpenTelemetry instrumentation by using Node.js --require flag to preload the instrumentation file.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-ts.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n# in your cli, script, Dockerfile, etc\nnode --require ./instrumentation.ts main.ts\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Cloud Environment in Python\nDESCRIPTION: Sets up environment variables for Phoenix API key and endpoint to enable tracing via the Phoenix SDK in a cloud deployment context.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langgraph.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Running MCP Client\nDESCRIPTION: This command executes the Python script `client.py`, which initiates the MCP client. The client will subsequently start the server in a separate process during runtime. The client interacts with the Phoenix collector, sending traces of the interaction.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mcp/tracing_between_mcp_client_and_server/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython client.py\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON Content from Input Question Fields in Phoenix Trace Data Frame in Python\nDESCRIPTION: Processes the 'question' field of trace_df to extract nested message content by loading JSON strings and accessing the first message content. This normalizes input data to readable question text for analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/amazon_bedrock_agents_tracing_and_evals.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntrace_df[\"question\"] = trace_df[\"question\"].apply(\n    lambda x: json.loads(x).get(\"messages\", [{}])[0].get(\"content\", \"\") if isinstance(x, str) else x\n)\n```\n\n----------------------------------------\n\nTITLE: Comparing Column Names Between Raw and Transformed DataFrames in Python\nDESCRIPTION: Computes the set difference between the columns of the original (raw) and the new (wrangled) DataFrames to identify columns added or omitted during transformation. Requires both DataFrames to be defined. Inputs are the respective columns attributes; output is a Python set showing column differences. Used for validation and schema inspection.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_ms_marco.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nset(raw_df.columns).difference(df.columns)\n```\n\n----------------------------------------\n\nTITLE: Define ROUGE and Token Count Evaluators\nDESCRIPTION: This code defines various evaluators for assessing the quality of generated summaries. The evaluators include functions to calculate ROUGE-1 F1 score, precision, recall, and a function to count the number of tokens in a string. These functions are used to evaluate the summarization task.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/summarization.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport tiktoken\nfrom rouge import Rouge\n\n\n# convenience functions\ndef _rouge_1(hypothesis: str, reference: str) -> Dict[str, Any]:\n    scores = Rouge().get_scores(hypothesis, reference)\n    return scores[0][\"rouge-1\"]\n\n\ndef _rouge_1_f1_score(hypothesis: str, reference: str) -> float:\n    return _rouge_1(hypothesis, reference)[\"f\"]\n\n\ndef _rouge_1_precision(hypothesis: str, reference: str) -> float:\n    return _rouge_1(hypothesis, reference)[\"p\"]\n\n\ndef _rouge_1_recall(hypothesis: str, reference: str) -> float:\n    return _rouge_1(hypothesis, reference)[\"r\"]\n\n\n# evaluators\ndef rouge_1_f1_score(output: str, expected: Dict[str, Any]) -> float:\n    return _rouge_1_f1_score(hypothesis=output, reference=expected[\"summary\"])\n\n\ndef rouge_1_precision(output: str, expected: Dict[str, Any]) -> float:\n    return _rouge_1_precision(hypothesis=output, reference=expected[\"summary\"])\n\n\ndef rouge_1_recall(output: str, expected: Dict[str, Any]) -> float:\n    return _rouge_1_recall(hypothesis=output, reference=expected[\"summary\"])\n\n\ndef num_tokens(output: str) -> int:\n    encoding = tiktoken.encoding_for_model(gpt_4o)\n    return len(encoding.encode(output))\n\n\nEVALUATORS = [rouge_1_f1_score, rouge_1_precision, rouge_1_recall, num_tokens]\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies - Python\nDESCRIPTION: Installs necessary Python libraries for building and tracing the LangChain OpenAI agent. This includes `langchain`, `langchain-openai`, `arize-phoenix`, `openinference-instrumentation-langchain`, and other required packages. Requires `pip`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_agent_tracing_tutorial.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n!pip install langchain langchain-community langchain-openai openai numexpr arize-phoenix openinference-instrumentation-langchain 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Multiple agent queries with follow-ups and resetting conversation\nDESCRIPTION: Iterates through a list of queries, prompting the agent each time, printing responses, and resetting conversation context for independent interactions. Used for testing multi-turn scenarios and trace collection.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_openai_agent_tracing_tutorial.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nqueries = [\n    \"What is (121 * 3) + 42?\",\n    \"what is 3 * 3?\",\n    \"what is 4 * 4?\",\n    \"what is 75 * (3 + 4)?\",\n    \"what is 23 times 87\",\n]\n\nfor query in queries:\n    print(f\"> {query}\")\n    response = agent.query(query)\n    print(response)\n    agent.reset()\n    print(\"---\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Client with Environment Variables\nDESCRIPTION: Example of using environment variables to configure the Phoenix client for accessing a Phoenix server instance with authentication.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/packages/phoenix-client/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nPHOENIX_HOST='http://localhost:12345' PHOENIX_API_KEY='xxxxxx' pnpx tsx examples/list_datasets.ts\n# emits the following request:\n# GET http://localhost:12345/v1/datasets\n# headers: {\n#   \"Authorization\": \"Bearer xxxxxx\",\n# }\n```\n\n----------------------------------------\n\nTITLE: Running Bedrock Agents (Python)\nDESCRIPTION: This Python code demonstrates how to invoke a Bedrock Agent. It constructs a dictionary of attributes, including `inputText`, `agentId`, `agentAliasId`, `sessionId`, and `enableTrace`.  The `client.invoke_agent()` function is then called using these attributes.  The call sends data to Bedrock, and traces will be streamed to Phoenix if the setup is correct. Requires proper setup of the Phoenix tracer and `boto3` client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/bedrock-1.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsession_id = f\"default-session1_{int(time.time())}\"\n\nattributes = dict(\n    inputText=input_text,\n    agentId=AGENT_ID,\n    agentAliasId=AGENT_ALIAS_ID,\n    sessionId=session_id,\n    enableTrace=True,\n)\nresponse = client.invoke_agent(**attributes)\n```\n\n----------------------------------------\n\nTITLE: Executing Groq LLM Asynchronous Evaluation Experiment in Python\nDESCRIPTION: Runs the wrapped asynchronous Groq evaluation function over the dataset to collect label predictions for hallucination detection.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nexp_groq = run_experiment(ds, groq_eval)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Experiment by ID and Adding Evaluators\nDESCRIPTION: This Python snippet shows how to retrieve a specific experiment from Phoenix using its ID via the `phoenix.Client().get_experiment` method and then add more evaluators to it using `phoenix.experiments.evaluate_experiment`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/run-experiments.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments import evaluate_experiment\nimport phoenix as px\n\nexperiment_id = \"experiment-id\" # set your experiment ID here\nexperiment = px.Client().get_experiment(experiment_id=experiment_id)\nevaluators = [\n    # add evaluators here\n]\nexperiment = evaluate_experiment(experiment, evaluators)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Prompts using Python Context Manager\nDESCRIPTION: Uses the `using_prompt_template` context manager from `openinference.instrumentation` to add prompt template details (template string, version, variables dictionary) to the current OpenTelemetry Context. OpenInference auto-instrumentors subsequently attach these details as span attributes (`llm.prompt_template.*`). Requires a non-empty string for `template` and `version`, and a dictionary with string keys for `variables`, which is serialized to JSON.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/instrumenting-prompt-templates-and-prompt-variables.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation import using_prompt_template\n\nprompt_template = \"Please describe the weather forecast for {city} on {date}\"\nprompt_template_variables = {\"city\": \"Johannesburg\", \"date\":\"July 11\"}\nwith using_prompt_template(\n    template=prompt_template,\n    variables=prompt_template_variables,\n    version=\"v1.0\",\n    ):\n    # Commonly preceeds a chat completion to append templates to auto instrumentation\n    # response = client.chat.completions.create()\n    # Calls within this block will generate spans with the attributes:\n    # \"llm.prompt_template.template\" = \"Please describe the weather forecast for {city} on {date}\"\n    # \"llm.prompt_template.version\" = \"v1.0\"\n    # \"llm.prompt_template.variables\" = \"{\\\"city\\\": \\\"Johannesburg\\\", \\\"date\\\": \\\"July 11\\\"}\" # JSON serialized\n    ...\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Text2SQL Questions using OpenAI\nDESCRIPTION: Calls the OpenAI API to generate synthetic SQL queries and corresponding natural language questions using a structured prompt and function calling, based on a provided database schema. Defines Pydantic models for structuring the expected JSON output from the LLM.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nfrom pydantic import BaseModel\n\n\nclass Question(BaseModel):\n    sql: str\n    question: str\n\n\nclass Questions(BaseModel):\n    questions: list[Question]\n\n\nsynthetic_data_prompt = f\"\"\"\nYou are a SQL expert, and you are given a single table named nba with the following columns:\n\nColumn | Type | Example\n-------|------|--------\n{\"\\n\".join(f\"{column['column_name']} | {column['column_type']} | {samples[column['column_name']]}\" for column in columns)}\n\nGenerate SQL queries that would be interesting to ask about this table. Return the SQL query as a string, as well as the\nquestion that the query answers.\"\"\"\n\nresponse = await client.chat.completions.create(\n    model=\"gpt-4o\",\n    temperature=0,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": synthetic_data_prompt,\n        }\n    ],\n    tools=[\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"generate_questions\",\n                \"description\": \"Generate SQL queries that would be interesting to ask about this table.\",\n                \"parameters\": Questions.model_json_schema(),\n            },\n        }\n    ],\n    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"generate_questions\"}},\n)\n\ngenerated_questions = json.loads(response.choices[0].message.tool_calls[0].function.arguments)[\n    \"questions\"\n]\ngenerated_questions[0]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Tracing in Deno\nDESCRIPTION: Configures the OpenTelemetry tracing infrastructure within a Deno application using `npm:` imports. It initializes a `NodeTracerProvider` with a resource name, sets up a `SimpleSpanProcessor` to export traces via OTLP HTTP to a local endpoint (`http://localhost:6006/v1/traces`), and registers the provider globally. Basic diagnostics logging is enabled for visibility.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/tracing_openai_node_tutorial.ipynb#_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport {\n  NodeTracerProvider,\n  SimpleSpanProcessor,\n} from \"npm:@opentelemetry/sdk-trace-node\";\nimport { Resource } from \"npm:@opentelemetry/resources\";\nimport { OTLPTraceExporter } from \"npm:@opentelemetry/exporter-trace-otlp-proto\";\nimport { SEMRESATTRS_PROJECT_NAME } from \"npm:@arizeai/openinference-semantic-conventions\";\nimport { diag, DiagConsoleLogger, DiagLogLevel } from \"npm:@opentelemetry/api\";\n\n// For troubleshooting, set the log level to DiagLogLevel.DEBUG\ndiag.setLogger(new DiagConsoleLogger(), DiagLogLevel.INFO);\n\nconst provider = new NodeTracerProvider({\n  resource: new Resource({\n    [SEMRESATTRS_PROJECT_NAME]: \"deno-openai\",\n  }),\n});\n\nprovider.addSpanProcessor(\n  new SimpleSpanProcessor(\n    new OTLPTraceExporter({\n      url: \"http://localhost:6006/v1/traces\",\n    }),\n  ),\n);\n\nprovider.register();\n\nconsole.log(\"ðŸ‘€ OpenInference initialized\");\n```\n\n----------------------------------------\n\nTITLE: Classifying Document Bias Using LLM in Python\nDESCRIPTION: This snippet calls the `llm_classify` function to classify bias in documents retrieved earlier. It provides the retrieved documents DataFrame, the bias detection prompt template, the OpenAI model instance, and a list of bias classification rails (labels). It also requests detailed explanations. The resulting classification labels are mapped into numerical scores for further analysis. This relies on a valid `retrieved_documents` DataFrame and the previously initialized `model` and `bias_detection_prompt`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/llamaindex-workflows-research-agent/evaluate_traces.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nbias_classifications = llm_classify(\n    dataframe=retrieved_documents,\n    template=bias_detection_prompt,\n    model=model,\n    rails=[\"Unbiased\", \"Biased\", \"Somewhat Biased\", \"Somewhat Unbiased\"],\n    provide_explanation=True,\n)\nbias_classifications[\"score\"] = bias_classifications[\"label\"].map(\n    {\"unbiased\": 1, \"somewhat unbiased\": 0.75, \"somewhat biased\": 0.5, \"biased\": 0}\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Article Summarization Tool Prompt to Phoenix (Python)\nDESCRIPTION: Saves the parameters for the article summarization task, including the tool definition, into the Phoenix prompt registry.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# prompt identifier should contain only alphanumeric characters, hyphens or underscores\nprompt_identifier = \"article-summary\"\n\nprompt = Client().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Article summary\",\n    version=PromptVersion.from_anthropic(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix with Embeddings Support\nDESCRIPTION: Installs or updates the Arize Phoenix library using `%pip` magic (common in Jupyter). The `[embeddings]` extra ensures that dependencies needed for handling embeddings within Phoenix are included. The `-Uqq` flags specify upgrade and quiet installation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n%pip install -Uqq \"arize-phoenix[embeddings]\"\n```\n\n----------------------------------------\n\nTITLE: Appending LLM-based Relevance Results to Query DataFrame (Python)\nDESCRIPTION: Extends the main query DataFrame to include new columns for LLM-assessed relevance scores of the top two candidate retrievals. These results can then be analyzed in concert with user feedback and traditional similarity metrics. Assumes context0_relevance and context1_relevance are DataFrames or Series with a 'label' column.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/qdrant_langchain_instrumentation_search_and_retrieval_tutorial.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsample_query_df = query_df.copy()\nsample_query_df[\"openai_relevance_0\"] = context0_relevance[\"label\"]\nsample_query_df[\"openai_relevance_1\"] = context1_relevance[\"label\"]\n```\n\n----------------------------------------\n\nTITLE: Running Zero-Shot Prompt Experiment and Evaluation\nDESCRIPTION: Defines a Python function to call the OpenAI API with the zero-shot prompt configuration for each input problem in the dataset. It also defines an evaluation function to check if the model's integer output matches the ground truth answer. The Phoenix `run_experiment` function is used to execute this task and evaluation pipeline.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/chain-of-thought-prompting.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.experiments import run_experiment\n\nnest_asyncio.apply()\n\ndef zero_shot_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **prompt.format(variables={\"Problem\": input[\"Word Problem\"]})\n    )\n    return resp.choices[0].message.content.strip()\n\n\ndef evaluate_response(output, expected):\n    if not output.isdigit():\n        return False\n    return int(output) == int(expected[\"Answer\"])\n\n\ninitial_experiment = run_experiment(\n    dataset,\n    task=zero_shot_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Zero-Shot Prompt\",\n    experiment_name=\"zero-shot-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix Docker Container - Bash\nDESCRIPTION: Runs a Phoenix container using Docker. The command maps port 6006 from the container to the host machine, making the Phoenix web interface accessible at `localhost:6006`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Adding Symlinks for Local Package Development\nDESCRIPTION: Command to create symbolic links inside src/phoenix/ pointing to submodules. This makes modules like phoenix.evals, phoenix.otel, and phoenix.client available when working from source.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntox run -e add_symlinks\n```\n\n----------------------------------------\n\nTITLE: Installing Microsoft PromptFlow Package using Bash\nDESCRIPTION: This command installs the Microsoft PromptFlow Python package required to create and run flows. It is necessary prior to instrumenting flows for tracing with Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/prompt-flow.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install promptflow\n```\n\n----------------------------------------\n\nTITLE: Include Common Requirements File (Python)\nDESCRIPTION: This directive instructs the package manager (like pip) to recursively install all dependencies listed in the `common.txt` file. It is used to consolidate common dependencies shared across multiple requirements files within a project.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/requirements/canary/sdk/openai.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n-r common.txt\n```\n\n----------------------------------------\n\nTITLE: Previewing Classification Results DataFrame - Python\nDESCRIPTION: Displays the first few rows of the DataFrame produced by llm_classify. Facilitates inspection and troubleshooting. Expects 'relevance_classifications_df' to exist.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nrelevance_classifications_df.head()\n```\n\n----------------------------------------\n\nTITLE: Inspecting Non-useful 'wellFormedAnswers' Column in MS-MARCO with Python\nDESCRIPTION: Shows how to assess the 'wellFormedAnswers' column, which in MS-MARCO contains only empty lists, using pandas Series apply and value_counts. Requires that 'raw_df' contain this column. Outputs the frequency of list lengths, typically showing zero, indicating this column can be dropped.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_ms_marco.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nraw_df[\"wellFormedAnswers\"].apply(len).value_counts()\n```\n\n----------------------------------------\n\nTITLE: Running a Phoenix Experiment with Custom Task and Mock Server in Python\nDESCRIPTION: Applies nest_asyncio to enable nested event loops common in notebook environments, then runs a Phoenix experiment with the previously uploaded dataset and task function while the Receiver server is active. Relies on 'nest_asyncio', 'phoenix.experiments', and all previous setup. Inputs are dataset and task function; output is experiment execution (result handling not shown). Serves as the orchestrating step for the workflow.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/internal/run_experiments.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.experiments import run_experiment\n\nnest_asyncio.apply()\n\nwith Receiver(hello).run_in_thread(port):\n    run_experiment(\n        ds,\n        task,\n    )\n```\n\n----------------------------------------\n\nTITLE: Query Data Preparation for Phoenix\nDESCRIPTION: This code prepares a DataFrame by concatenating and merging different data sources to create a unified view of the query data for visualization in Phoenix's embedding visualizer. It combines data from `ragas_evals_df`, `query_embeddings_df`, `test_df`, and `eval_scores_df` to include questions, answers, ground truths, vector embeddings, and Ragas evaluation scores.  `pd.concat` is used to combine dataframes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nquery_df = pd.concat(\n    [\n        ragas_evals_df[[\"question\", \"answer\", \"ground_truth\"]],\n        query_embeddings_df[[\"vector\"]],\n        test_df[[\"evolution_type\"]],\n        eval_scores_df,\n    ],\n    axis=1,\n)\n```\n\n----------------------------------------\n\nTITLE: Launching the Phoenix UI\nDESCRIPTION: This Python snippet imports the phoenix library and launches the local Phoenix user interface, which is used to visualize and analyze the experiment results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/run-experiments.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Using BatchSpanProcessor - Python\nDESCRIPTION: This Python snippet demonstrates how to use the `BatchSpanProcessor` for batch processing of spans. The required components are imported and added to the `TracerProvider`. This requires the `opentelemetry` and `phoenix.otel` modules.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-otel/README.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace as trace_api\nfrom phoenix.otel import TracerProvider, BatchSpanProcessor\n\ntracer_provider = TracerProvider()\nbatch_processor = BatchSpanProcessor()\ntracer_provider.add_span_processor(batch_processor)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix in a Notebook Environment\nDESCRIPTION: Python code to install the Phoenix package and launch the Phoenix app within a Jupyter Notebook or similar environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langgraph.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Installing Legacy One-Click Callback - Bash\nDESCRIPTION: Installs the `llama-index-callbacks-arize-phoenix` package, which provides an integration for tracing LlamaIndex applications with Phoenix. This is specifically for legacy versions of LlamaIndex (versions before 0.10.43).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install 'llama-index-callbacks-arize-phoenix>0.1.3'\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Package in Development Mode\nDESCRIPTION: Command to install the arize-phoenix package in editable mode with development dependencies. This allows changes to the source code to be immediately reflected without reinstallation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Preparing Dataset Sample for Evaluation\nDESCRIPTION: Creates a sample of the dataset with renamed columns to match the expected input format for the evaluation function, supporting iterative testing with different sample sizes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf_sample = (\n    df.sample(n=N_EVAL_SAMPLE_SIZE)\n    .reset_index(drop=True)\n    .rename(\n        columns={\n            \"question\": \"input\",\n            \"context\": \"reference\",\n            \"sampled_answer\": \"output\",\n        }\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Queries\nDESCRIPTION: Fetches a JSONL file containing pre-written natural language queries from a cloud URL. Loads each line as a JSON object and extracts the 'query' field into a list for later use.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/project_sessions_llama_index_query_engine.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nqueries_url = \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/arize_docs_queries.jsonl\"\nwith urlopen(queries_url) as response:\n    queries = [json.loads(line)[\"query\"] for line in response]\n```\n\n----------------------------------------\n\nTITLE: Redefining Test Task - DSPy Phoenix GPT-4o - Python\nDESCRIPTION: Redefines the `test_dspy_prompt` function to specifically use the `optimized_classifier_using_gpt_4o`, which was compiled using GPT-4o as the prompt optimization model. Like the previous version, it takes input, passes the prompt to the classifier, and returns the predicted label. This updated function serves as the task for the second experiment run, comparing optimization techniques.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n# Create evaluation function using optimized classifier\ndef test_dspy_prompt(input):\n    result = optimized_classifier_using_gpt_4o(prompt=input[\"prompt\"])\n    return result.label\n```\n\n----------------------------------------\n\nTITLE: Launch Phoenix UI Python\nDESCRIPTION: Initializes and launches the Arize Phoenix application. This command typically starts a local web server that hosts the Phoenix user interface, allowing the user to visualize uploaded data, traces, and experiment results. It's a prerequisite for interacting with the Phoenix platform.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Pull and Run Phoenix Docker Image\nDESCRIPTION: Pulls the latest Phoenix Docker image from Docker Hub and runs a container exposing Phoenix on localhost:6006. Useful for deploying Phoenix in a containerized environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/autogen-support.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Instantiating LlamaIndex Retriever and Query Engine\nDESCRIPTION: Creates the LlamaIndex components responsible for retrieving relevant documents and generating answers. A `VectorIndexRetriever` is instantiated using the previously created `VectorStoreIndex` (backed by MongoDB), configured to retrieve the top 5 most similar documents (`similarity_top_k=5`). This retriever is then passed into a `RetrieverQueryEngine`, which orchestrates the retrieval and language model generation process to answer user queries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_with_mongodb_and_llama_index.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate Atlas Vector Search as a retriever\nvector_store_retriever = VectorIndexRetriever(index=index, similarity_top_k=5)\n\n# Pass the retriever into the query engine\nquery_engine = RetrieverQueryEngine(retriever=vector_store_retriever)\n```\n\n----------------------------------------\n\nTITLE: Querying the RAG System for Semantic Content (Python)\nDESCRIPTION: Issues a semantic query against the query engine to retrieve contextually relevant information from indexed documents. The input is a question string; output is a response object containing the answer and potentially source nodes. Demonstrates how to interact programmatically with the RAG pipeline.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresponse_vector = query_engine.query(\"What did the author do growing up?\")\n```\n\n----------------------------------------\n\nTITLE: Autodoc Directive Example for RST Files\nDESCRIPTION: Example of autodoc directives used in reStructuredText files to extract and document Python modules, classes, and methods. These directives control how documentation is generated from docstrings.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/api_reference/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n.. automodule:: module_name\n   :members:\n   :no-undoc-members:\n   :exclude-members:\n   ...\n\n.. autoclass:: module_name::class_name\n   :members:\n   ...\n\n.. automethod:: module_name.class_name::function_name\n```\n\n----------------------------------------\n\nTITLE: Creating Response Format Schema for Structured Summarization\nDESCRIPTION: Defines a detailed schema for article summaries using Pydantic models, including invented year, summary, inventors, concepts, and description, for structured output formatting with OpenAI models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nclass ArticleSummary(BaseModel):\n    invented_year: int\n    summary: str\n    inventors: list[str]\n    description: str\n\n    class Concept(BaseModel):\n        title: str\n        description: str\n\n    concepts: list[Concept]\n\nresponse_format = type_to_response_format_param(ArticleSummary)\n```\n\n----------------------------------------\n\nTITLE: Displaying Human vs AI Prompt Template - Python\nDESCRIPTION: This snippet prints the large language model prompt template used by the Human vs. AI evaluation in the Phoenix library. The template provides detailed instructions to an LLM, guiding it to compare a question, a human ground truth answer, and an AI-generated answer to determine if the AI answer is \"correct\" based on whether it captures the substance of the human response. It is designed to be used with an LLM classification function like `llm_classify`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/ai-vs-human-groundtruth.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nprint(HUMAN_VS_AI_PROMPT_TEMPLATE)\n\nYou are comparing a human ground truth answer from an expert to an answer from an AI model.\nYour goal is to determine if the AI answer correctly matches, in substance, the human answer.\n    [BEGIN DATA]\n    ************\n    [Question]: {question}\n    ************\n    [Human Ground Truth Answer]: {correct_answer}\n    ************\n    [AI Answer]: {ai_generated_answer}\n    ************\n    [END DATA]\nCompare the AI answer to the human ground truth answer, if the AI correctly answers the question,\nthen the AI answer is \"correct\". If the AI answer is longer but contains the main idea of the\nHuman answer please answer \"correct\". If the AI answer diverges or does not contain the main\nidea of the human answer, please answer \"incorrect\".\n\n```\n\n----------------------------------------\n\nTITLE: Querying Hugging Face Hub Chat Completion API with Phoenix Prompt Utilities in Python\nDESCRIPTION: This snippet shows how to use Phoenix client utilities to prepare a prompt and send a chat completion request to the Hugging Face Hub API. The huggingface_hub SDK and an inference token are required. The 'model' variable specifies which HF model to query, and prompt_version_id selects the prompt template. The full chat completion result is returned as JSON.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/examples/prompts/prompts_for_various_SDKs.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel = \"microsoft/Phi-3.5-mini-instruct\"\n\nprompt_version_id = \"UHJvbXB0VmVyc2lvbjox\"\nprompt_version = Client().prompts.get(prompt_version_id=prompt_version_id)\nprint(f\"prompt_version = {prompt_version.model_dump_json(indent=2)}\\n{'-'*100}\")\n\nmessages, kwargs = to_chat_messages_and_kwargs(\n    prompt_version, variables={\"question\": \"Who made you?\"}\n)\nprint(f\"messages = {json.dumps(messages, indent=2)}\\n{'-'*100}\")\nprint(f\"kwargs = {json.dumps(kwargs, indent=2)}\\n{'-'*100}\")\n\nresponse = huggingface_hub.InferenceClient().chat_completion(messages, model=model)\nprint(f\"response = {json.dumps(response, indent=2)}\")\n```\n\n----------------------------------------\n\nTITLE: Viewing Phoenix Evaluation Results as DataFrame in Python\nDESCRIPTION: Calls the `get_evaluations()` method on the `experiment` object after evaluations have been run using `evaluate_experiment`. This method retrieves the evaluation results (scores and explanations for each evaluator and each run) stored in the experiment object and returns them as a pandas DataFrame for easy analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nexperiment.get_evaluations()\n```\n\n----------------------------------------\n\nTITLE: Running a Simple LangChain Example\nDESCRIPTION: Python code example showing how to create and invoke a simple LangChain pipeline with ChatOpenAI, which will be automatically traced.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langchain.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_template(\"{x} {y} {z}?\").partial(x=\"why is\", z=\"blue\")\nchain = prompt | ChatOpenAI(model_name=\"gpt-3.5-turbo\")\nchain.invoke(dict(y=\"sky\"))\n```\n\n----------------------------------------\n\nTITLE: Counting product ID occurrences\nDESCRIPTION: This code snippet counts the occurrences of each unique product ID in the DataFrame's 'product/productId' column, using the value_counts() method from pandas.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/long_context_summary_evals.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf[\"product/productId\"].value_counts()\n```\n\n----------------------------------------\n\nTITLE: Formatting a PromptTemplate with Variable Values in Python\nDESCRIPTION: Example showing how to substitute variable values into a PromptTemplate using a dictionary that maps variable names to their corresponding values.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evals-reference.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nvalue_dict = {\n    \"name\": \"Peter\",\n    \"age\": 20,\n    \"location\": \"Queens\"\n}\nprint(prompt_template.format(value_dict))\n# Output: My name is Peter. I am 20 years old and I am from Queens\n```\n\n----------------------------------------\n\nTITLE: Wrapping DataFrame and Schema into a Phoenix Inferences Object using Python\nDESCRIPTION: Combines the prepared DataFrame and defined Schema into a coherent Phoenix Inferences object named \"training\". This object serves as a container that Phoenix requires to launch interactive visualizations of model inferences.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/inferences_quickstart.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_ds = px.Inferences(dataframe=train_df, schema=train_schema, name=\"training\")\n```\n\n----------------------------------------\n\nTITLE: Running Human vs AI Evaluation with Phoenix - Python\nDESCRIPTION: This Python snippet demonstrates how to set up and run the Human vs. AI evaluation using the Arize Phoenix library. It imports necessary components, initializes an `OpenAIModel` instance (specifying the model and temperature), defines output validation rules (`rails`) based on the prompt's expected values, and calls the `llm_classify` function. The function takes a DataFrame containing the data points to evaluate, the specific prompt template, the configured LLM model, and other parameters like `rails`, `verbose`, and `provide_explanation` to return classification results and explanations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/ai-vs-human-groundtruth.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.evals import (\n    HUMAN_VS_AI_PROMPT_RAILS_MAP,\n    HUMAN_VS_AI_PROMPT_TEMPLATE,\n    OpenAIModel,\n    llm_classify,\n)\n\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\n\n# The rails is used to hold the output to specific values based on the template\n# It will remove text such as \",,,\" or \"...\"\n# Will ensure the binary value expected from the template is returned\nrails = list(HUMAN_VS_AI_PROMPT_RAILS_MAP.values())\nrelevance_classifications = llm_classify(\n    dataframe=df,\n    template=HUMAN_VS_AI_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    verbose=False,\n    provide_explanation=True\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Phoenix Tracing Locally in Python\nDESCRIPTION: Demonstrates how to initialize Phoenix and instrument a LangChain application for local tracing. Assumes Phoenix is running locally on the default port 6006, requiring no extra configuration. Imports `phoenix`, `LangChainInstrumentor`, launches the Phoenix app, and applies instrumentation before running the LangChain application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/concepts-tracing/faqs-tracing.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\nfrom phoenix.trace import LangChainInstrumentor\n\npx.launch_app()\n\nLangChainInstrumentor().instrument()\n\n# run your LangChain application\n```\n\n----------------------------------------\n\nTITLE: Printing Active Phoenix Session URL in Python\nDESCRIPTION: Outputs the URL of the currently active Phoenix session, allowing users to access the Phoenix web application to view logged evaluation data and visualizations. Requires that a Phoenix client session (px) is already active.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nprint(\"phoenix URL\", px.active_session().url)\n```\n\n----------------------------------------\n\nTITLE: Deploying Phoenix with Docker\nDESCRIPTION: Commands to pull and run Phoenix in a Docker container, exposing it on localhost port 6006.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/guardrails-ai.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Phoenix Client and Vercel AI SDK\nDESCRIPTION: Imports the necessary libraries for Phoenix client and Vercel AI SDK integrations, including the Phoenix client, Vercel AI, and provider-specific modules for OpenAI and Google Generative AI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_vercel_ai_sdk.ipynb#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport * as PhoenixClient from \"npm:@arizeai/phoenix-client\"\nimport ai from \"npm:ai\"\nimport { createOpenAI } from 'npm:@ai-sdk/openai';\nimport { createGoogleGenerativeAI } from 'npm:@ai-sdk/google';\n```\n\n----------------------------------------\n\nTITLE: Melt and Clean DataFrame\nDESCRIPTION: This snippet first transforms the dataframe to a format where all questions are in a single 'question' column. Then, it removes rows with missing questions. This creates a clean dataframe, where each row has a document chunk and its associated question.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nquestions_with_document_chunk_df = questions_with_document_chunk_df.melt(\n    id_vars=[\"text\"], value_name=\"question\"\n).drop(\"variable\", axis=1)\n# If the above step was interrupted, there might be questions missing. Let's run this to clean up the dataframe.\nquestions_with_document_chunk_df = questions_with_document_chunk_df[\n    questions_with_document_chunk_df[\"question\"].notnull()\n]\n```\n\n----------------------------------------\n\nTITLE: Defining LLM Task Logic (Text-to-SQL)\nDESCRIPTION: This Python snippet sets up an OpenAI client, defines a system prompt for a text-to-SQL task using database schema information, and implements functions (`generate_query`, `execute_query`, `text2sql`) to send a question to the LLM, execute the generated SQL query, and handle potential errors. It relies on the `openai` and `duckdb` libraries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/run-experiments.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom textwrap import dedent\n\nimport openai\n\nclient = openai.Client()\ncolumns = conn.query(\"DESCRIBE nba\").to_df().to_dict(orient=\"records\")\n\nLLM_MODEL = \"gpt-4o\"\n\ncolumns_str = \",\".join(column[\"column_name\"] + \": \" + column[\"column_type\"] for column in columns)\nsystem_prompt = dedent(f\"\"\"\nYou are a SQL expert, and you are given a single table named nba with the following columns:\n{columns_str}\n\nWrite a SQL query corresponding to the user's\nrequest. Return just the query text, with no formatting (backticks, markdown, etc.).\"\"\")\n\n\ndef generate_query(question):\n    response = client.chat.completions.create(\n        model=LLM_MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": question},\n        ],\n    )\n    return response.choices[0].message.content\n\n\ndef execute_query(query):\n    return conn.query(query).fetchdf().to_dict(orient=\"records\")\n\n\ndef text2sql(question):\n    results = error = None\n    try:\n        query = generate_query(question)\n        results = execute_query(query)\n    except duckdb.Error as e:\n        error = str(e)\n    return {\"query\": query, \"results\": results, \"error\": error}\n```\n\n----------------------------------------\n\nTITLE: Loading Wikipedia Article and Creating a VectorStoreIndex with LlamaIndex in Python\nDESCRIPTION: Uses the wikipedia Python SDK to download the content of the 'Digital Camera' page by page ID. Wraps the page content in a LlamaIndex Document object and constructs a VectorStoreIndex from this document for semantic vector similarity search. This index is stored in 'vector_indices' for later querying.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_sql_retriever_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# load the Digital Camera wikipedia page\npage = wikipedia.page(pageid=52797)\ndoc = Document(id_=page.pageid, text=page.content)\n\nvector_indices = []\nvector_index = VectorStoreIndex.from_documents([doc])\nvector_indices.append(vector_index)\n```\n\n----------------------------------------\n\nTITLE: Filtering Spans Programmatically using SpanQuery in Python\nDESCRIPTION: Demonstrates creating a `SpanQuery` to filter spans based on attributes like `span_kind` using the `.where()` method and selecting specific data fields like `input.value` and `output.value` using `.select()`. The query is then executed using an active Phoenix session.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/filter-spans.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace.dsl import SpanQuery\n\nquery = SpanQuery().where(\n    \"span_kind == 'LLM'\",  # filter for the LLM span kind\n).select(\n    \"input.value\",         # input.value as the first output column\n    \"output.value\",        # output.value as the second output column\n)\n\n# The active Phoenix session can take this query and return the dataframe.\npx.active_session().query_spans(query)\n```\n\n----------------------------------------\n\nTITLE: Initializing Embeddings and Loading Documentation with LangChain (Python)\nDESCRIPTION: Instantiates OpenAIEmbeddings with a specified embedding model and API key. Defines a utility function to load and process documentation from a public Gitbook URL using LangChain's GitbookLoader. Returns the loaded documents, which will later be indexed into Qdrant. Requires a valid OpenAI API key, internet access for downloading docs, and the langchain.document_loaders module.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/qdrant_langchain_instrumentation_search_and_retrieval_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel_name = \"text-embedding-ada-002\"\n\nembeddings = OpenAIEmbeddings(model=model_name, openai_api_key=openai_api_key)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef load_gitbook_docs(docs_url):\n    \"\"\"\n    Loads documentation from a Gitbook URL.\n    \"\"\"\n\n    loader = GitbookLoader(\n        docs_url,\n        load_all_paths=True,\n    )\n    return loader.load()\n\n\ndocs_url = \"https://docs.arize.com/arize/\"\ndocs = load_gitbook_docs(docs_url)\n```\n\n----------------------------------------\n\nTITLE: Installing arize-phoenix Package - Bash\nDESCRIPTION: This command installs the arize-phoenix package, which is needed for local Phoenix instance deployments.  This package is required if you intend to run Phoenix locally via the command line or other local environments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/crewai.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Arize Phoenix (Python)\nDESCRIPTION: Uploads the created Pandas DataFrame to the Phoenix application as a dataset, specifying the input and output keys for the experiment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/run_experiments_with_llama_index.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset_name = datetime.now(timezone.utc).isoformat()\npx.Client().upload_dataset(\n    dataset_name=dataset_name,\n    dataframe=df,\n    input_keys=(\"input_messages\",),\n    output_keys=(\"output_message\",),\n)\nsleep(1)\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenAI API Key Environment Variable\nDESCRIPTION: Prompts the user for their OpenAI API key if it's not already set as an environment variable and assigns it to the environment for subsequent API calls. Ensures secure and convenient API authentication.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Building and Running Phoenix Experiment App in Production Mode Using pnpm in Bash\nDESCRIPTION: This Bash snippet provides the commands to build and execute the Phoenix experiment app in production mode. It starts Phoenix, builds the application, and then runs it. These steps must follow dependency installation and any prerequisite configuration. The final app should then be accessible on the configured local endpoint, and Phoenix should be running in the background.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/apps/phoenix-experiment-runner/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm d:up\npnpm build\npnpm start\n```\n\n----------------------------------------\n\nTITLE: Cloning Dataset Repository (Shell)\nDESCRIPTION: Clones a Git repository from Hugging Face containing PDF documents related to prompt engineering. This command uses `git clone` to download the source documents needed for building the RAG pipeline.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n!git clone https://huggingface.co/datasets/explodinggradients/prompt-engineering-papers\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix via pip in Python\nDESCRIPTION: Installs the 'arize-phoenix' package using pip, which is required for dataset upload, management, and experimentation within the Phoenix AI infrastructure. This dependency must be installed before using the phoenix API in subsequent code. This cell is intended for notebook or shell environments and has no input or output beyond the package installation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/internal/run_experiments.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# !pip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Installing Semantic Convention Dependencies - Shell\nDESCRIPTION: Shell command to install the necessary npm packages for standard OpenTelemetry and OpenInference semantic conventions, enabling the use of predefined attribute keys.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save @opentelemetry/semantic-conventions @arizeai/openinfernece-semantic-conventions\n```\n\n----------------------------------------\n\nTITLE: Creating Code Generation DataFrame\nDESCRIPTION: Creates a Pandas DataFrame to hold the questions, example data and chart configs, necessary for evaluating the generated code.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ncode_generation_df = pd.DataFrame(\n    {\n        \"question\": code_generation_questions,\n        \"example_data\": example_data,\n        \"chart_configs\": chart_configs,\n    }\n)\n\n```\n\n----------------------------------------\n\nTITLE: Initializing LangChain Instrumentor with OpenInferenceTracer in Python (Deprecated)\nDESCRIPTION: This snippet shows how tracing was previously initialized using OpenInferenceTracer and passed to LangChainInstrumentor. Dependencies include phoenix.trace.langchain. The tracer argument is no longer supported, and this method is obsolete in v3.0.0. Required: Phoenix v2.x libraries. Input: none. Output: traces transmitted via the (now-defunct) tracer setup. Not compatible with v3.0.0+.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/MIGRATION.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.trace.langchain import LangChainInstrumentor, OpenInferenceTracer\n\ntracer = OpenInferenceTracer()  # no longer supported\nLangChainInstrumentor(tracer).instrument()  # tracer argument is no longer supported\n```\n\n----------------------------------------\n\nTITLE: Running One-Shot Experiment\nDESCRIPTION: Runs the one-shot prompting experiment using the `run_experiment` function. It uses the previously defined `one_shot_prompt_template` task function, the same evaluator, and a metadata field denoting the ID of the `one_shot_prompt`.  The results of the experiment are uploaded to Phoenix for analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/few_shot_prompting.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\none_shot_experiment = run_experiment(\n    dataset,\n    task=one_shot_prompt_template,\n    evaluators=[evaluate_response],\n    experiment_description=\"One-Shot Prompting\",\n    experiment_name=\"one-shot-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + one_shot_prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Sending Asynchronous Chat Completion Requests to Groq in Python\nDESCRIPTION: Formats the prompt and asynchronously calls the Groq chat completions endpoint. Prints the returned response text content. Requires async environment support.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nvariables = dict(ds.as_dataframe().input.iloc[0])\nformatted_prompt = prompt.format(variables=variables)\nresponse = await groq.AsyncGroq().chat.completions.create(\n    **{**formatted_prompt, \"model\": groq_model}\n)\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Installing instructor and OpenInference packages\nDESCRIPTION: This command installs the `openinference-instrumentation-instructor` and `instructor` packages. It is assumed that `instructor` requires an OpenInference instrumentation package for the underlying LLM being used, such as `openinference-instrumentation-openai` for using OpenAI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/instructor.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-instructor instructor\n```\n\n----------------------------------------\n\nTITLE: Define Guideline Evaluators Python\nDESCRIPTION: Sets up guideline-based evaluators for the experiment results. It initializes a separate 'OpenAI' LLM for evaluation. Defines a dictionary 'guidelines' mapping names to evaluation criteria strings. An asynchronous helper function 'adapt' is created to format the output of the LlamaIndex 'GuidelineEvaluator' to match the expected Phoenix format (Score, Explanation). The 'evaluators' dictionary maps guideline names to partially applied 'GuidelineEvaluator.aevaluate' functions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nllm = OpenAI(temperature=0, model=\"gpt-4o\")\nguidelines = {\n    \"answer_fully\": \"The response should fully answer the query.\",\n    \"unambiguous\": \"The response should avoid being vague or ambiguous.\",\n    \"use_numbers\": \"The response should be specific and use statistics or numbers when possible.\",\n}\n\nasync def adapt(fn, output, input) -> Tuple[Score, Explanation]:\n    ans = await fn(\n        query=input[\"messages\"][0][\"content\"],\n        response=output,\n        contexts=[input[\"document\"]],\n    )\n    return ans.passing, ans.feedback\n\nevaluators = {\n    name: partial(adapt, GuidelineEvaluator(llm=llm, guidelines=guideline).aevaluate)\n    for name, guideline in guidelines.items()\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying First Few Rows of DataFrames\nDESCRIPTION: This snippet uses the `head()` method to display the first few rows of the `qa_correctness_eval` DataFrame. This is used for quick inspection of the evaluation results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nqa_correctness_eval.head()\n```\n\n----------------------------------------\n\nTITLE: Configuring Trace Masking with JavaScript OpenAI Instrumentor\nDESCRIPTION: Example showing how to create a traceConfig object and apply it to the OpenAI instrumentor in JavaScript/TypeScript. The example demonstrates hiding inputs while letting other settings fall back to environment variables or defaults.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/advanced/masking-span-attributes.md#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n/**\n * Everything left out of here will fallback to\n * environment variables then defaults\n */\nconst traceConfig = { hideInputs: true } \n\nconst instrumentation = new OpenAIInstrumentation({ traceConfig })\n```\n\n----------------------------------------\n\nTITLE: Specifying Template Types (String/Object) in Phoenix Evals Python\nDESCRIPTION: Illustrates the flexibility of Phoenix Evals in accepting templates. Templates can be defined either as simple Python strings (as shown previously) or as `PromptTemplate` objects (potentially from libraries like LangChain), allowing for different ways to structure and manage evaluation prompts.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/concepts-evals/building-your-own-evals.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#Phoenix Evals support using either strings or objects as templates\nMY_CUSTOM_TEMPLATE = \" ...\"\nMY_CUSTOM_TEMPLATE = PromptTemplate(\"This is a test {prompt}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Module Using DSPy\nDESCRIPTION: Defines a custom DSPy Module subclass employing retrieval and chain-of-thought generation to produce answers from retrieved contexts and questions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/dspy_tracing_tutorial.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nclass RAG(dspy.Module):\n    def __init__(self, num_passages=3):\n        super().__init__()\n        self.retrieve = dspy.Retrieve(k=num_passages)\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n\n    def forward(self, question):\n        context = self.retrieve(question).passages\n        prediction = self.generate_answer(context=context, question=question)\n        return dspy.Prediction(context=context, answer=prediction.answer)\n```\n\n----------------------------------------\n\nTITLE: Generating Computer Vision Embeddings in Python with Phoenix\nDESCRIPTION: Code example demonstrating how to generate embeddings for computer vision data using the EmbeddingGenerator class with ViT model for image classification.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/generating-embeddings.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom arize.pandas.embeddings import EmbeddingGenerator, UseCases\n\ndf = df.reset_index(drop=True)\n\ngenerator = EmbeddingGenerator.from_use_case(\n    use_case=UseCases.CV.IMAGE_CLASSIFICATION,\n    model_name=\"google/vit-base-patch16-224-in21k\",\n    batch_size=100\n)\ndf[\"image_vector\"] = generator.generate_embeddings(\n    local_image_path_col=df[\"local_path\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Interface Widget\nDESCRIPTION: This snippet defines the UI components of the chat application. It leverages IPython widgets to create an interactive chat interface, including an input box for user input, a send button, and a chat history display. It also defines the behavior for handling user input, generating responses, displaying bot responses, and collecting user feedback via thumbs up/down buttons. The core functionality relies on the `generate_response` and `send_feedback` functions to interact with the OpenAI API and submit feedback data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/human_feedback/chatbot_with_human_feedback.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef send_message(_):\n    input_text = input_box.value\n\n    # Send the message to the OpenAI API and get the response\n    response_data, span_id = generate_response(input_text)\n    assistant_content = response_data[\"choices\"][0][\"message\"][\"content\"]\n\n    # Create thumbs up and thumbs down buttons\n    thumbs_up = widgets.Button(description=\"ðŸ‘\", layout=widgets.Layout(width=\"30px\"))\n    thumbs_down = widgets.Button(description=\"ðŸ‘Ž\", layout=widgets.Layout(width=\"30px\"))\n\n    # Set up the callbacks for the buttons\n    thumbs_up.on_click(lambda _: send_feedback(span_id, 1))\n    thumbs_down.on_click(lambda _: send_feedback(span_id, 0))\n\n    # Create a horizontal box to hold the response and the buttons\n    response_box = widgets.HBox(\n        [widgets.Label(f\"Bot: {assistant_content}\"), thumbs_up, thumbs_down]\n    )\n\n    # Add the user's message and the response to the chat history\n    chat_history.children += (widgets.Label(f\"You: {input_text}\"), response_box)\n\n    # Clear the input box\n    input_box.value = \"\"\n\n\n# Set up the chat interface\nchat_history = widgets.VBox()\ninput_box = widgets.Text(placeholder=\"Type your message here...\")\nsend_button = widgets.Button(description=\"Send\")\nsend_button.on_click(send_message)\n\n# Display the chat interface\ndisplay(chat_history, input_box, send_button)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with pip (bash)\nDESCRIPTION: Installs all required Python packages for the project as listed in requirements.txt using pip. Dependency installation is mandatory prior to running any project scripts. Ensure you are in the correct virtual environment (if applicable) before execution. The command expects requirements.txt in the current directory and outputs installed packages to standard output.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/cron-evals/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Improving Summarization with Explicit Conciseness Instructions in Python\nDESCRIPTION: Creates an improved prompt template with explicit instructions to be concise and runs a second experiment. The template specifically requests a summary of 2-4 sentences with only the most important information.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/summarization.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntemplate = \"\"\"\nSummarize the article in two to four sentences. Be concise and include only the most important information.\n\nARTICLE\n=======\n{article}\n\nSUMMARY\n=======\n\"\"\"\ntask = partial(summarize_article_openai, prompt_template=template, model=gpt_4o)\nexperiment_results = run_experiment(\n    dataset,\n    task,\n    experiment_name=\"concise-template\",\n    experiment_description=\"explicitly instuct the llm to be concise\",\n    experiment_metadata={\"vendor\": \"openai\", \"model\": gpt_4o},\n    evaluators=EVALUATORS,\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Latest Exported Cluster Data - Phoenix Python\nDESCRIPTION: Retrieves the dataframe of the most recently exported cluster data using the Phoenix session API. Requires the Phoenix client library imported as 'px' and an active Phoenix session. The code accesses the current session via 'px.active_session()', then uses 'session.exports[-1].dataframe' to obtain the latest exported dataframe. The output is a Pandas DataFrame containing the cluster-labeled data, which can be further processed or used for labeling, evaluation, or fine-tuning. The snippet assumes an export has already been performed in the Phoenix UI, and will fail if no exports are present.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/export-your-data.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsession = px.active_session()\nsession.exports[-1].dataframe\n```\n\n----------------------------------------\n\nTITLE: Importing Modules Python\nDESCRIPTION: This snippet imports various modules and libraries required by the project. These include modules for context management, random number generation, OpenAI integration, data loading, mocking OpenAI responses, OpenTelemetry instrumentation, trace conventions, and the Phoenix framework. Also imports Faker for generating synthetic data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/multi-turn_chat_sessions.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom contextlib import ExitStack, contextmanager\nfrom random import choice, choices, randint, random, shuffle\nfrom uuid import uuid4\n\nimport numpy as np\nimport openai\nimport pandas as pd\nfrom datasets import load_dataset\nfrom faker import Faker\nfrom openai_responses import OpenAIMock\nfrom openinference.instrumentation import using_session, using_user\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nfrom openinference.semconv.trace import OpenInferenceSpanKindValues, SpanAttributes\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import SpanLimits, StatusCode, TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom opentelemetry.sdk.trace.export.in_memory_span_exporter import InMemorySpanExporter\nfrom opentelemetry.trace import format_span_id\nfrom tiktoken import encoding_for_model\n\nimport phoenix as px\nfrom phoenix.trace import using_project\nfrom phoenix.trace.span_evaluations import SpanEvaluations\n\nfake = Faker([\"ja_JP\", \"vi_VN\", \"ko_KR\", \"zh_CN\", \"th_TH\", \"bn_BD\"])\n```\n\n----------------------------------------\n\nTITLE: Re-importing and Logging Parquet Trace and Evaluation Fixtures into Phoenix in Python\nDESCRIPTION: Illustrates the import of previously exported trace and evaluation Parquet files and re-logging into Phoenix for validation or round-trip testing. Requires several fixtures and the px.Client().log_traces and log_evaluations methods. Inputs are fixture file names; output is the data's reappearance in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix import TraceDataset\nfrom phoenix.trace import DocumentEvaluations, SpanEvaluations\n\npx.Client().log_traces(\n    TraceDataset(pd.read_parquet(\"fixtures/demo_llama_index_rag_traces.parquet\"))\n)\n\nretrieved_documents_relevance_df = pd.read_parquet(\n    \"fixtures/demo_llama_index_rag_doc_relevance_eval.parquet\"\n)\nqa_eval_df = dataframe = pd.read_parquet(\n    \"fixtures/demo_llama_index_rag_qa_correctness_eval.parquet\"\n)\nhallucination_eval_df = dataframe = pd.read_parquet(\n    \"fixtures/demo_llama_index_rag_hallucination_eval.parquet\"\n)\n\npx.Client().log_evaluations(\n    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_eval_df),\n    DocumentEvaluations(\n        eval_name=\"Retrieval Relevance\", dataframe=retrieved_documents_relevance_df\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenAI API Key for Phoenix Application\nDESCRIPTION: Prompts the user for their OpenAI API key if not available in environment variables and sets it in the environment for subsequent API calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_application_tracing_evaluating_and_analysis.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\n# Uses your OpenAI API Key\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Summarizing documents using LangChain refine chain\nDESCRIPTION: This code snippet initializes a ChatOpenAI model with GPT-4 and then uses LangChain's `load_summarize_chain` to create a refine chain. The prepared chunks are converted into LangChain Document objects, and the chain is then run to generate a summary, which is printed to the console.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/long_context_summary_evals.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nllm = ChatOpenAI(model=\"gpt-4\")\nchain = load_summarize_chain(llm, chain_type=\"refine\")\ndocuments = [Document(page_content=chunk) for chunk in chunks]\nsummary = chain.run(documents)\nprint(summary)\n```\n\n----------------------------------------\n\nTITLE: Running Few-Shot Prompt Template Experiment with Phoenix and OpenAI Model in Python\nDESCRIPTION: Runs an experiment using the few-shot prompt template defined previously to generate article summaries. The task is set up via partial application binding the summarization function to the few-shot prompt and the GPT-4o model. The experiment metadata and evaluators remain consistent with prior runs. The snippet expects dependencies such as the Phoenix run_experiment function, the 'summarize_article_openai' function, and appropriate evaluator definitions. Results focus on reducing token usage while maintaining evaluation metrics.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/summarization.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntask = partial(summarize_article_openai, prompt_template=template, model=gpt_4o)\nexperiment_results = run_experiment(\n    dataset,\n    task,\n    experiment_name=\"few-shot-template\",\n    experiment_description=\"include examples\",\n    experiment_metadata={\"vendor\": \"openai\", \"model\": gpt_4o},\n    evaluators=EVALUATORS,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Customer Questions Dataset into Phoenix\nDESCRIPTION: Loads a dataset of customer service questions from Hugging Face, converts it to a Pandas DataFrame, and uploads it to Phoenix for use in ReAct prompting examples.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/react-prompting.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nds = load_dataset(\"syeddula/customer_questions\")[\"train\"]\nds = ds.to_pandas()\nds.head()\nimport uuid\n\nunique_id = uuid.uuid4()\n\n# Upload the dataset to Phoenix\ndataset = px.Client().upload_dataset(\n    dataframe=ds,\n    input_keys=[\"Questions\"],\n    dataset_name=f\"customer-questions-{unique_id}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Running LangChain RetrievalQA Chain with Tracing Enabled (bash)\nDESCRIPTION: Executes the run_chain.py script, which defines, instruments, and repeatedly queries a LangChain RetrievalQA chain over the previously built vector store. The script emits traces to an active Phoenix server for visualization and further evaluation. Ensure Phoenix is running and the vector store directory exists before execution. The process loops indefinitely, expecting the user to monitor Phoenix for trace data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/cron-evals/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython run_chain.py\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Instrumentation Library (Bash)\nDESCRIPTION: Installs the `openinference-instrumentation-openai` package using pip. This library provides automatic instrumentation for OpenAI API calls, enabling trace capture via OpenTelemetry.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evaluating_traces_quickstart.ipynb#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n%%bash\npip install -q openinference-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix Server\nDESCRIPTION: Command to start the Phoenix server which will be accessible through the configured reverse proxy at the specified root path.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/reverse-proxy/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m phoenix.server.main serve\n```\n\n----------------------------------------\n\nTITLE: Visualization of AI Data Insights using Python\nDESCRIPTION: This snippet generates visual representations of processed data to help analyze key insights within the Phoenix project, likely using libraries such as matplotlib or seaborn. It provides graphical summaries for data distributions or model outputs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot data distribution\n sns.histplot(data['processed_feature'])\n plt.title('Distribution of Processed Feature')\n plt.show()\n\n# Scatter plot of features\n sns.scatterplot(x='raw_feature', y='processed_feature', data=data)\n plt.title('Raw vs Processed Feature')\n plt.show()\n\n```\n\n----------------------------------------\n\nTITLE: Directory Structure of Phoenix API Documentation\nDESCRIPTION: Shows the file organization of the API reference documentation project, including source files, configuration files, and build scripts.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/api_reference/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\napi_reference\n â”£ source\n â”ƒ â”ƒ â”£ custom.css\n â”ƒ â”ƒ â”£ logo.png\n â”ƒ â”ƒ â”— switcher.json\n â”ƒ â”£ api\n â”ƒ â”ƒ â”£ client.rst\n â”ƒ â”ƒ â”£ evals.rst\n â”ƒ â”ƒ â”£ experiments.rst\n â”ƒ â”ƒ â”£ inferences_schema.rst\n â”ƒ â”ƒ â”— session.rst\n â”ƒ â”ƒ â”— ...\n â”ƒ â”£ conf.py\n â”ƒ â”— index.md\n â”£ Makefile\n â”£ README.md\n â”£ make.bat\n â”— requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App in Python\nDESCRIPTION: Starts an instance of the Phoenix application and opens its user interface view. This serves as a monitoring or interaction point for experiments and datasets managed with Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npx.launch_app().view()\n```\n\n----------------------------------------\n\nTITLE: Generating Completions Using Retrieved Prompt with OpenAI Python SDK\nDESCRIPTION: Shows how to format and invoke the retrieved prompt by filling template variables and using the OpenAI Python SDK to create chat completions. The response contains the generated output from the model based on the prompt's formatted content.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nvariables = {\"topic\": \"programming\"}\nresp = OpenAI().chat.completions.create(**prompt.format(variables=variables))\nprint(resp.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Installing Groq Instrumentation and SDK (Bash)\nDESCRIPTION: Installs both the 'openinference-instrumentation-groq' and the 'groq' Python packages for enabling OpenInference-compatible tracing and Groq API calls. Should be run before attempting to instrument or use Groq features in Python.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/groq.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-groq groq\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key in Python\nDESCRIPTION: Checks for the OPENAI_API_KEY environment variable and prompts the user for input if it's not found, setting it as an environment variable.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/run_experiments_with_llama_index.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Transforming Attribute to Subscript AST Node in Python\nDESCRIPTION: Defines a `Translator` class inheriting from `ast.NodeTransformer` to visit and modify nodes in an AST. The `visit_Attribute` method recursively traverses an `ast.Attribute` chain, collects the attribute names, and replaces the entire chain with an `ast.Subscript` node accessing 'attributes' with a list of the collected names. The snippet applies this transformer to a parsed attribute chain and prints the resulting AST as Python code using `ast.unparse`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/src/phoenix/trace/dsl/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass Translator(ast.NodeTransformer):\n    def visit_Attribute(self, node):\n        path = []\n        while isinstance(node, ast.Attribute):\n            path.append(node.attr)\n            node = node.value\n            if isinstance(node, ast.Name):\n                path.append(node.id)\n                break\n        return ast.Subscript(\n            value=ast.Name(id='attributes', ctx=ast.Load()),\n            slice=ast.List(\n                elts=[ast.Constant(value=p) for p in reversed(path)],\n                ctx=ast.Load(),\n            ),\n            ctx=ast.Load(),\n        )\n\nparsed = ast.parse(\"llm.token_count.completion\", mode=\"eval\")\ntranslated = Translator().visit(parsed)\nprint(ast.unparse(translated))\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies and Import Libraries\nDESCRIPTION: This code installs necessary dependencies and imports libraries required for the tutorial. It includes packages for interacting with OpenAI, Phoenix, and other libraries to evaluate summaries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/summarization.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install anthropic \"arize-phoenix>=4.6\" openai openinference-instrumentation-openai rouge tiktoken 'httpx<0.28'\n\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict\n\nimport nest_asyncio\nimport pandas as pd\n\nnest_asyncio.apply()  # needed for concurrent evals in notebook environments\npd.set_option(\"display.max_colwidth\", None)  # display full cells of dataframes\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Agentic RAG App\nDESCRIPTION: Installs the necessary Python libraries for building and tracing the agentic RAG application. This includes LlamaIndex core components, OpenAI for LLM and embeddings, Arize Phoenix for tracing, OpenInference instrumentation for LlamaIndex, Chroma vector store and client, and SQLAlchemy for potential database interactions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/agentic_rag_tracing.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q llama-index openai arize-phoenix-otel openinference-instrumentation-llama-index llama-index-vector-stores-chroma chromadb sqlalchemy\n```\n\n----------------------------------------\n\nTITLE: Getting Active Phoenix Session Python\nDESCRIPTION: Retrieves the currently running Phoenix `Session` object if the application was previously launched without assigning the returned session to a variable. This allows subsequent interaction with the running instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/api_reference/source/api/session.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nsession = px.active_session()\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Dependencies for Phoenix Project\nDESCRIPTION: This requirements file specifies all the necessary test dependencies with their version constraints. It includes testing libraries (pytest and extensions), type checking tools (mypy, pyright), comparison utilities, and AI service client libraries for integration testing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/requirements/ci.txt#_snippet_0\n\nLANGUAGE: requirements.txt\nCODE:\n```\nmypy==1.15.0\npytest==8.3.4\npytest-xdist==3.6.1\npytest-asyncio==0.25.1\nuvloop; platform_system != 'Windows'\npydantic>=1.0,!=2.0.*,<3\npyright[nodejs]==1.1.394\ndeepdiff==8.2.0\nanthropic==0.49.0\nopenai==1.62.0\nazure-identity\n```\n\n----------------------------------------\n\nTITLE: Merge DataFrames\nDESCRIPTION: This code merges two dataframes: one containing generated questions and the other containing document chunks. It concatenates them on the column axis, providing both questions and associated text chunks in one dataframe.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nquestions_with_document_chunk_df = pd.concat([questions_df, document_chunks_df], axis=1)\n```\n\n----------------------------------------\n\nTITLE: Formatting Frontend App Code using Prettier via npm (Shell)\nDESCRIPTION: Formats the frontend application code (in `./app`) using Prettier via the npm script named 'prettier'. This command ensures frontend code adheres to the project's style guidelines before submission.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/CONTRIBUTING.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nnpm run prettier\n```\n\n----------------------------------------\n\nTITLE: Defining Zero-Shot Prompt Configuration (OpenAI)\nDESCRIPTION: Defines the configuration for a basic zero-shot prompt using the OpenAI `gpt-3.5-turbo` model via the `CompletionCreateParamsBase`. The system message instructs the model to output only the integer answer, serving as a baseline for evaluation. This configuration is then registered as a prompt version in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/chain-of-thought-prompting.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom openai.types.chat.completion_create_params import CompletionCreateParamsBase\n\nfrom phoenix.client.types import PromptVersion\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an evaluator who outputs the answer to a math word problem. Only respond with the integer answer. Be sure not include words, explanations, symbols, labels, or units and round all decimals answers.\",\n        },\n        {\"role\": \"user\", \"content\": \"{{Problem}}\"},\n    ],\n)\n\nprompt_identifier = \"wordproblems\"\n\nprompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"A prompt for computing answers to word problems.\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Data Structure of Example in Python\nDESCRIPTION: This code accesses and displays the structure of a single example from the dataset. This aids in understanding the dataset's format and available fields for downstream tasks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndataset[0]\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix UI Application with Python\nDESCRIPTION: Imports the 'phoenix' Python module and launches the Phoenix UI application. The 'launch_app()' method starts the application asynchronously and '.view()' opens the UI for interaction. This snippet waits a few seconds to initialize the UI and provides visual access to telemetry and analytics data gathered by the instrumentation configured earlier.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\npx.launch_app().view()\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Schema for Training Data (Python)\nDESCRIPTION: Creates a `phoenix.Schema` object to describe the structure of the training DataFrame (`train_df`) for Phoenix. It maps DataFrame columns to Phoenix concepts like timestamps (`prediction_ts`), predicted labels (`predicted_action`), actual labels (`actual_action`), and embedding features (mapping `image_vector` as the vector and `url` as the link to data).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/image_classification_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain_schema = px.Schema(\n    timestamp_column_name=\"prediction_ts\",\n    prediction_label_column_name=\"predicted_action\",\n    actual_label_column_name=\"actual_action\",\n    embedding_feature_column_names={\n        \"image_embedding\": px.EmbeddingColumnNames(\n            vector_column_name=\"image_vector\",\n            link_to_data_column_name=\"url\",\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Running Python Type Checking using Tox (Shell)\nDESCRIPTION: Performs static type checking on the Python codebase using the 'tox' automation tool. This command helps catch potential type errors before runtime and should be run before submitting a pull request. It targets the 'type_check' environment defined in the tox configuration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/CONTRIBUTING.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\ntox run -e type_check\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI and Logging Evaluations (Development) Python\nDESCRIPTION: This code instruments OpenAI, simulates API calls for a specified session count with a specified tree complexity and drop probability. It then uninstruments OpenAI, exports the spans, and logs evaluations on root spans using the Phoenix client. The evaluations consist of randomly generated scores, labels, and explanations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/multi-turn_chat_sessions.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\ntry:\n    for _ in range(session_count):\n        simulate_openai()\nfinally:\n    OpenAIInstrumentor().uninstrument()\nspans = export_spans(prob_drop_root)\n\n# Annotate root spans\nroot_span_ids = pd.Series(\n    [span.context.span_id.to_bytes(8, \"big\").hex() for span in spans if span.parent is None]\n)\nfor name in \"ABC\":\n    span_ids = root_span_ids.sample(frac=0.5)\n    df = pd.DataFrame(\n        {\n            \"context.span_id\": span_ids,\n            \"score\": np.random.rand(len(span_ids)),\n            \"label\": np.random.choice([\"ðŸ‘\", \"ðŸ‘Ž\"], len(span_ids)),\n            \"explanation\": [fake.paragraph(10) for _ in range(len(span_ids))],\n        }\n    ).set_index(\"context.span_id\")\n    px.Client().log_evaluations(SpanEvaluations(name, df))\n```\n\n----------------------------------------\n\nTITLE: Importing Required Python Libraries (Python)\nDESCRIPTION: Imports all necessary Python modules for the tutorial, including standard libraries (`json`, `os`, `sys`, etc.), data handling (`pandas`, `numpy`), OpenAI interaction (`openai`), LlamaIndex components (`ServiceContext`, `VectorStoreIndex`, callbacks, vector stores), and Phoenix components for evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/milvus_llamaindex_search_and_retrieval_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport logging\nimport os\nimport sys\nimport textwrap\nimport urllib.request\nfrom datetime import timedelta\nfrom getpass import getpass\n\nimport numpy as np\nimport openai\nimport pandas as pd\nfrom IPython.display import YouTubeVideo\nfrom llama_index import (\n    ServiceContext,\n    VectorStoreIndex,\n)\nfrom llama_index.callbacks import CallbackManager, OpenInferenceCallbackHandler\nfrom llama_index.callbacks.open_inference_callback import as_dataframe\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms import OpenAI\nfrom llama_index.schema import NodeRelationship, RelatedNodeInfo, TextNode\nfrom llama_index.vector_stores import MilvusVectorStore\nfrom llama_index.vector_stores.utils import (\n    metadata_dict_to_node,\n)\n\nimport phoenix as px\nfrom phoenix.evals.retrievals import (\n    classify_relevance,\n    compute_precisions_at_k,\n)\n\nlogging.disable(sys.maxsize)\npd.set_option(\"display.max_colwidth\", 1000)\n```\n\n----------------------------------------\n\nTITLE: Aggregating Span-Level Bias Scores and Labels in Python\nDESCRIPTION: This snippet processes the bias classification results to aggregate scores at the document span level by calculating the mean score per `context.span_id`. It then maps these averaged scores back to the closest bias label. Further, it groups rows by `context.span_id`, concatenating explanations and averaging execution time and scores, creating a summarized DataFrame indexed by span ID. This enables a consolidated view of bias evaluations per document span.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/llamaindex-workflows-research-agent/evaluate_traces.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nspan_bias_classifications = bias_classifications.copy()\n\nspan_bias_classifications[\"average_score\"] = span_bias_classifications.groupby(\"context.span_id\")[\n    \"score\"\n].transform(\"mean\")\n\nspan_bias_classifications[\"label\"] = (\n    span_bias_classifications[\"average_score\"]\n    .apply(\n        lambda x: min(\n            {1: \"unbiased\", 0.75: \"somewhat unbiased\", 0.5: \"somewhat biased\", 0: \"biased\"}.keys(),\n            key=lambda k: abs(k - x),\n        )\n    )\n    .map({1: \"unbiased\", 0.75: \"somewhat unbiased\", 0.5: \"somewhat biased\", 0: \"biased\"})\n)\n\n# Combine all rows with the same context.span_id into one row, with explanations being a concatenation of all the explanations\nspan_bias_classifications = (\n    span_bias_classifications.groupby(\"context.span_id\")\n    .agg(\n        {\n            \"label\": \"first\",\n            \"explanation\": lambda x: \"\\n----\\n\".join(x),\n            \"exceptions\": \"first\",\n            \"execution_status\": \"first\",\n            \"execution_seconds\": \"mean\",\n            \"score\": \"mean\",\n            \"average_score\": \"first\",\n        }\n    )\n    .reset_index()\n)\nspan_bias_classifications.set_index(\"context.span_id\", inplace=True)\nspan_bias_classifications.head()\n```\n\n----------------------------------------\n\nTITLE: Creating Nested Spans for Sub-Operations (python)\nDESCRIPTION: Shows how to create nested spans to track hierarchical operations via the tracer's start_as_current_span method. The parent span wraps a sub-operation, while a child span is created inside to track a nested operation. Both spans are automatically closed upon scope exit. Requires: tracer object. Inputs: parent and child span names as strings. Output: parent-child span relationship visible in trace visualizations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/custom-spans.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef do_work():\n    with tracer.start_as_current_span(\"parent\") as parent:\n        # do some work that 'parent' tracks\n        print(\"doing some work...\")\n        # Create a nested span to track nested work\n        with tracer.start_as_current_span(\"child\") as child:\n            # do some work that 'child' tracks\n            print(\"doing some nested work...\")\n            # the nested span is closed when it's out of scope\n\n        # This span is also closed when it goes out of scope\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom OpenTelemetry Tracer Provider for Phoenix with Python\nDESCRIPTION: Sets up an OpenTelemetry TracerProvider with an OTLPSpanExporter configured to send telemetry data to a local Phoenix server endpoint. This involves creating a Resource, initializing the TracerProvider with it, adding a SimpleSpanProcessor with the OTLPSpanExporter pointing to 'http://localhost:6006/v1/traces', and setting the global tracer provider. After setup, it retrieves a tracer instance for application use. This configuration enables the export of traces to Phoenix for observability.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nresource = Resource(attributes={})\ntracer_provider = trace_sdk.TracerProvider(resource=resource)\nspan_exporter = OTLPSpanExporter(endpoint=\"http://localhost:6006/v1/traces\")\nspan_processor = SimpleSpanProcessor(span_exporter=span_exporter)\ntracer_provider.add_span_processor(span_processor=span_processor)\ntrace_api.set_tracer_provider(tracer_provider=tracer_provider)\n\ntracer = trace_api.get_tracer(__name__)\n\n# Because we are using Open AI, we will use this along with our custom instrumentation\nOpenAIInstrumentor().instrument(skip_dep_check=True)\n```\n\n----------------------------------------\n\nTITLE: Invoking Anthropic Message Completion with Converted Prompt in JavaScript\nDESCRIPTION: Calls the Anthropic SDK's messages.create method with the prompt parameters matched for Anthropic's format. Uses the 'claude-3-5-sonnet-latest' model without streaming. The response is processed and displayed in Jupyter markdown, showing either text segments or tool use results as prettified JSON. Requires Anthropic client and correctly formed prompt.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_cross_sdk_tutorial.ipynb#_snippet_11\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst response = await anthropic.messages.create({\n  ...anthropicPrompt,\n  // you can still override any of the invocation parameters as needed\n  // for example, you can change the model or stream the response\n  model: \"claude-3-5-sonnet-latest\",\n  stream: false\n})\n\nawait Deno.jupyter.md`\n  ### Anthropic Response\n\n  ${response.content.map(c => {\n    if (c.type === \"text\") {\n      return c.text\n    } else if (c.type === \"tool_use\") {\n      return `\\`\\`\\`json\\n${JSON.stringify(c, null, 2)}\\`\\`\\``\n    }\n  }).join(\"\\n\")}\n`\n```\n\n----------------------------------------\n\nTITLE: Creating a Few-Shot Prompt Template and Registering with Phoenix in Python\nDESCRIPTION: Defines a system message template for few-shot sentiment analysis, incorporating placeholders for multiple examples (`{examples}`). It configures parameters (`model`, `temperature`, `messages`) for an OpenAI API call using `CompletionCreateParamsBase` and registers this configuration as a new prompt version in Arize Phoenix using `PhoenixClient`. Requires `few_shot_examples` (a pandas DataFrame of examples) and `prompt_identifier` (a string name for the prompt).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/few_shot_prompting.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfew_shot_template = \"\"\"\n\"You are an evaluator who assesses the sentiment of a review. Output if the review positive, negative, or neutral. Only respond with one of these classifications.\"\n\nHere are examples of a review and the sentiment:\n\n{examples}\n\"\"\"\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"system\", \"content\": few_shot_template.format(examples=few_shot_examples)},\n        {\"role\": \"user\", \"content\": \"{{Review}}\"},\n    ],\n)\n\nfew_shot_prompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Few-shot prompt for classifying reviews based on sentiment.\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Query OpenAI - Python\nDESCRIPTION: Imports the `OpenAI` client. Initializes the client using the API key from environment variables. Defines the `query_openai` function which takes a prompt string. Calls `oa_client.chat.completions.create` with a specified model ('gpt-4o-mini') and messages to get a response. Returns the generated content from the model's response. This function relies on auto-instrumentation configured earlier.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n# Initialize OpenAI client\noa_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\n\n# Query OpenAI with the constructed prompt.\n# This function does not have tracing applied to it, because the OpenAI\n# client is instrumented using the auto_instrument flag in the register function.\ndef query_openai(prompt):\n    response = oa_client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n    )\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Calling OpenAI API for Caption Explanations\nDESCRIPTION: Iterates through the caption DataFrame ('df'). For each row, it formats a message using the previously defined 'message' function and sends it to the OpenAI Chat Completions API using the 'gpt-4o-mini' model. It limits the number of successful calls to 'n' (initially 25) and stops if more than 3 consecutive errors occur. Basic error handling using a try-except block is included.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/vision.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nn, errors = 25, 0\nfor idx, caption, _ in df.itertuples():\n    if n == 0 or errors > 3:\n        break\n    messages = [message(idx, caption)]\n    try:\n        client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, max_tokens=1000)\n    except BaseException:\n        errors += 1\n    else:\n        errors = 0\n        n -= 1\n```\n\n----------------------------------------\n\nTITLE: Uploading Input/Output Dataset to Phoenix Client Python\nDESCRIPTION: This snippet instantiates a Phoenix client and uploads a limited sample of QA pairs as a dataset for experiment tracking. Takes first 'num_examples' entries from the questions and expected_tool_calls lists, builds input and output lists as required for Phoenix, and triggers upload via upload_dataset. Dependencies: Phoenix Python SDK, inclusion of valid questions and expected_tool_calls. Inputs: dataset name, question dictionaries, tool call dictionaries. Outputs: Phoenix dataset object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/demos/playground_demo.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nphoenix_client = px.Client()\nnum_examples = 10\ndataset = phoenix_client.upload_dataset(\n    dataset_name=\"e-commerce\",\n    inputs=[{\"question\": question} for question in questions[:num_examples]],\n    outputs=[\n        {\"expected_tool_calls\": tool_calls} for tool_calls in expected_tool_calls[:num_examples]\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Querying the RouterQueryEngine for Digital Camera Sensor History Using Python\nDESCRIPTION: Issues the natural language query 'Tell me about the history of digital camera sensors.' to the RouterQueryEngine, which routes this general knowledge query to the vector index over Wikipedia content. The response is printed. This demonstrates vector similarity retrieval reflected in Phoenix tracing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_sql_retriever_tutorial.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nresponse = query_engine.query(\"Tell me about the history of digital camera sensors.\")\nprint(str(response))\n```\n\n----------------------------------------\n\nTITLE: Running Experiment with ReAct Prompt\nDESCRIPTION: This code runs an experiment similar to the initial setup but utilizes the ReAct prompting strategy. It evaluates how the gradual reasoning approach affects the quality of the model's responses and tool selection efficacy.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/react_prompting.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ninitial_experiment = run_experiment(\n    dataset,\n    task=prompt_task,\n    evaluators=[evaluate_response],\n    experiment_description=\"Customer Support Prompt\",\n    experiment_name=\"improved-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + prompt.id},\n)\n\n```\n\n----------------------------------------\n\nTITLE: Map Numerical Labels to Strings (Python)\nDESCRIPTION: This code maps the numerical labels in the dataset to their corresponding string labels. It defines a `label_map` dictionary and uses the pandas `map` function to update the 'label' column. Input: DataFrame `df` with a 'label' column containing numerical values. Output: DataFrame `df` with the 'label' column containing string values.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlabel_map = {\n    1: \"automobile\",\n    2: \"snakes\",\n    3: \"cat\",\n    4: \"tree\",\n    5: \"dog\",\n    6: \"frog\",\n    7: \"horse\",\n    8: \"ship\",\n}\ndf[\"label\"] = df[\"label\"].map(label_map)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries for Phoenix Evals\nDESCRIPTION: Installs all necessary dependencies for running Phoenix relevance evaluation, including arize-phoenix-evals, OpenAI SDK, matplotlib, scikit-learn, pycm, tiktoken, nest_asyncio, and httpx. This ensures all required packages are available, particularly for notebook and async environments. The command should be run in a cell in a Jupyter or Colab notebook.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qq \"arize-phoenix-evals>=0.0.5\" \"openai>=1\" ipython matplotlib pycm scikit-learn tiktoken nest_asyncio 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix Experiment App using pnpm in Bash\nDESCRIPTION: This Bash snippet demonstrates how to install all dependencies and build components for the Phoenix experiment app within a monorepo setup using pnpm. The commands first install the project's Node.js dependencies and then build all modules recursively. Prerequisites include pnpm and Node.js v20 or higher, and the context assumes the root directory of the project. The commands must be run before starting or building the app.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/apps/phoenix-experiment-runner/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm install\npnpm -r build\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix OTEL with Custom gRPC Port and Protocol in Python\nDESCRIPTION: Illustrates how to override the default transport protocol and port for Phoenix OTEL by specifying both the endpoint and the protocol keyword in the register call. This is useful when the gRPC collector is running on a non-standard port or when the protocol must be clearly enforced between \"grpc\" and \"http/protobuf\".\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\ntracer_provider = register(\n    endpoint=\"http://localhost:9999\",\n    protocol=\"grpc\", # use \"http/protobuf\" for http transport\n)\n\n```\n\n----------------------------------------\n\nTITLE: Displaying Phoenix UI URL\nDESCRIPTION: Prints the URL of the active Phoenix UI session, allowing users to view the traces generated by the LlamaIndex application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_multimodal_image_reasoning.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprint(\"The Phoenix UI:\", px.active_session().url)\n```\n\n----------------------------------------\n\nTITLE: Querying Spans with Time Range Filters Using Phoenix SDK - Python\nDESCRIPTION: This example demonstrates how to query spans from Phoenix within a specific time interval by specifying start_time and end_time using Python datetime objects. Spans from the last 7 days up to 24 hours ago are selected, and the pandas DataFrame output can be further analyzed. Dependencies include phoenix, datetime, and pandas.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport phoenix as px\nfrom phoenix.trace.dsl import SpanQuery\nfrom datetime import datetime, timedelta\n\n# Initiate Phoenix client\npx_client = px.Client()\n\n# Get spans from the last 7 days only\nstart = datetime.now() - timedelta(days=7)\n\n# Get spans to exclude the last 24 hours\nend = datetime.now() - timedelta(days=1)\n\nphoenix_df = px_client.query_spans(start_time=start, end_time=end)\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset Python\nDESCRIPTION: Loads a dataset from a specified path using the 'datasets' library. The dataset is loaded and converted to a Pandas DataFrame, and then it extracts the 'chosen' column.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/multi-turn_chat_sessions.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npath = \"GitBag/ultrainteract_multiturn_1_iter_processed_harvard\"\nconvo = load_dataset(path, split=\"test\").to_pandas().chosen\n```\n\n----------------------------------------\n\nTITLE: Configuring OTel Protocol - Python\nDESCRIPTION: This snippet shows how to enforce the OTLP transport protocol using the `protocol` argument, regardless of the endpoint. This is useful when the gRPC endpoint uses a non-default port. Valid protocols are \"http/protobuf\" and \"grpc\". The Phoenix OTel library is required.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-otel/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\ntracer_provider = register(endpoint=\"http://localhost:9999\", protocol=\"grpc\")\n```\n\n----------------------------------------\n\nTITLE: Fetching Phoenix Prompt from Server (Deno JavaScript)\nDESCRIPTION: Demonstrates how to retrieve a prompt by name ('question-asker') from the connected Phoenix server using the Phoenix client. This confirms the prompt exists and fetches its current definition. The fetched prompt object is then displayed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_openai_tutorial.ipynb#_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n// We already have the prompt, but this just proves that we can fetch it from the server as well\nconst questionAskerPrompt = await Prompts.getPrompt({ client: px, prompt: { name: \"question-asker\" } })\n\nawait Deno.jupyter.md`\n  ### question-asker prompt\n\n  ```json\n  ${JSON.stringify(questionAskerPrompt, null, 2)}\n  ```\n  `\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Format OpenAI Vision API Message\nDESCRIPTION: Defines a Python function 'message' that takes an image index ('idx') and a caption ('caption') as input. It constructs a dictionary representing a message payload suitable for the OpenAI Chat Completions API with vision capabilities. The payload includes a text prompt asking for an explanation of the caption's humor and an image URL pointing to the corresponding cartoon.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/vision.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef message(idx: int, caption: str) -> Dict[str, str]:\n    url = f\"https://nextml.github.io/caption-contest-data/cartoons/{idx}.jpg\"\n    text = f\"Explain like I'm five. What's funny about this caption?\\n\\n{caption}\\n\"\n    return {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": text},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"{url}\", \"detail\": \"low\"}},\n        ],\n    }\n```\n\n----------------------------------------\n\nTITLE: Defining Endpoints and Constants\nDESCRIPTION: This snippet defines the endpoints and constants used throughout the application. It includes the base endpoint for the Phoenix application, the feedback endpoint for sending user feedback, the OpenAI API URL, and a tracer instance. These constants provide the infrastructure needed for interacting with the OpenAI API and the Phoenix feedback system.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/human_feedback/chatbot_with_human_feedback.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nENDPOINT = \"http://localhost:6006/v1\"\nFEEDBACK_ENDPOINT = f\"{ENDPOINT}/span_annotations\"\nOPENAI_API_URL = \"https://api.openai.com/v1/chat/completions\"\nTRACER = trace_api.get_tracer(__name__)\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix and OpenAI API Credentials - Python\nDESCRIPTION: Configures environment variables for Phoenix and OpenAI operations, securely requesting the necessary API keys at runtime as needed. Sets PHOENIX_COLLECTOR_ENDPOINT to the Phoenix cloud instance and requests both Phoenix and OpenAI API keys from the user, storing them in environment variables for subsequent client authentication. Dependencies: os, getpass. No input arguments required. Sensitive keys are not hardcoded for security. Assumes interactive use.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\nif not os.environ.get(\"PHOENIX_CLIENT_HEADERS\"):\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = \"api_key=\" + getpass(\"Enter your Phoenix API key: \")\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix OpenTelemetry Instrumentation (Bash)\nDESCRIPTION: Installs the 'arize-phoenix-otel' Python package using pip, required for instrumenting Python applications to report traces to Arize Phoenix. No parameters are required. Must be executed in an environment with pip, typically Python 3.6+.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/groq.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Defining and Running Anthropic Claude Model Summarization Experiment Asynchronously in Python\nDESCRIPTION: Implements an asynchronous summarization task using Anthropic's AsyncAnthropic client. The function 'summarize_article_anthropic' formats the prompt template with the article text, sends it as a user message to the Anthropic API, and returns the generated summary text. The client is instantiated from 'anthropic' package. The task is created by partially applying the summarization function to the few-shot prompt template and specified Anthropic Claude model. The experiment executes with Phoenix's 'run_experiment' function, passing relevant metadata and evaluators. Requires Python 3.7+ for async support and proper API credentials.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/summarization.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom anthropic import AsyncAnthropic\n\nclient = AsyncAnthropic()\n\n\nasync def summarize_article_anthropic(example: Example, prompt_template: str, model: str) -> str:\n    formatted_prompt_template = prompt_template.format(article=example.input[\"article\"])\n    message = await client.messages.create(\n        model=model,\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": formatted_prompt_template}],\n    )\n    return message.content[0].text\n\n\nclaude_35_sonnet = \"claude-3-5-sonnet-20240620\"\ntask = partial(summarize_article_anthropic, prompt_template=template, model=claude_35_sonnet)\n\nexperiment_results = run_experiment(\n    dataset,\n    task,\n    experiment_name=\"anthropic-few-shot\",\n    experiment_description=\"anthropic\",\n    experiment_metadata={\"vendor\": \"anthropic\", \"model\": claude_35_sonnet},\n    evaluators=EVALUATORS,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key for LLM Access\nDESCRIPTION: Sets up the OpenAI API key either from environment variables or user input to enable access to OpenAI models for hallucination detection evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix in Notebook\nDESCRIPTION: Command to install the Python package for Phoenix in a notebook environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langchain.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix Experiment App in Development Mode Using pnpm in Bash\nDESCRIPTION: This Bash snippet details the steps to start both Phoenix and the experiment app in development mode with pnpm. It assumes the environment is set up with all dependencies installed. First, Phoenix is started, and then the developmental server for the app is launched. The app and Phoenix will be accessible at their respective local endpoints; prior configuration in .env and dataset creation may be necessary.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/apps/phoenix-experiment-runner/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npnpm d:up\npnpm dev\n```\n\n----------------------------------------\n\nTITLE: Fetching and Using UI Generation Prompt\nDESCRIPTION: Retrieves the stored UI generator prompt, supplies variables, and executes the completion to obtain a structured UI description. Outputs the generated UI layout or components.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nprompt = Client().prompts.get(prompt_identifier=prompt_identifier)\n\nvariables = {\"feature\": \"user login\"}\nresp = OpenAI().chat.completions.create(**prompt.format(variables=variables))\nprint(resp.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Display docstring for phoenix.load_example_traces - Python\nDESCRIPTION: This snippet displays the docstring for the `phoenix.load_example_traces` function to understand the available LLM traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/use-example-inferences.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npx.load_example_traces?\n```\n\n----------------------------------------\n\nTITLE: Loading Query Dataset for Projection Analysis\nDESCRIPTION: Loads a larger dataset of queries to enable meaningful UMAP projection and clustering analysis for pattern detection.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\n# Pull in queries from the LLM\nquery_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/query_data_complete3.parquet\",\n)\n\nquery_ds = px.Inferences.from_open_inference(query_df)\n\nquery_ds.dataframe.head()\n```\n\n----------------------------------------\n\nTITLE: Extracting Binary Relevance Classification Subset from DataFrame in Python\nDESCRIPTION: Selects only the essential columns for binary relevance evaluationâ€”query ID, text, document text, URL, and the binary relevance labelâ€”from the wrangled DataFrame. This prepares the data for downstream relevance classification tasks. Requires the transformed DataFrame with those columns present. Outputs a new DataFrame subset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_ms_marco.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nbinary_relevance_classification_df = df[\n    [\"query_id\", \"query_text\", \"document_text\", \"document_url\", \"relevant\"]\n]\nbinary_relevance_classification_df.head()\n```\n\n----------------------------------------\n\nTITLE: Saving Entity Extraction Prompt in Phoenix\nDESCRIPTION: Creates and registers a prompt specifically for extracting product entity parameters from free text inputs, facilitating structured data collection in a conversational AI context.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nprompt_identifier = \"extract-email-addresses-into-json-data\"\n\nprompt = Client().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Extract email addresses into JSON data\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Model Inference and Evaluation in Python\nDESCRIPTION: This code performs model inference on prepared datasets, evaluating model performance metrics such as accuracy or loss. It is integral to testing and validating AI models within the Phoenix pipeline.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Split data into features and labels\nX = data.drop('label', axis=1)\n y = data['label']\n\n# Split into train and test sets\n X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train model\n model = RandomForestClassifier()\n model.fit(X_train, y_train)\n\n# Inference\n predictions = model.predict(X_test)\n\n# Evaluate\n acc = accuracy_score(y_test, predictions)\n print(f'Accuracy: {acc}')\n\n```\n\n----------------------------------------\n\nTITLE: Running Experiment with Phoenix Using OpenAI Model in Python\nDESCRIPTION: Runs a summarization experiment using the Phoenix framework's 'run_experiment' function with an initial simple prompt template. Dependencies include importing 'run_experiment' from phoenix.experiments, a dataset, task function, and evaluator set (EVALUATORS). The experiment is annotated with metadata such as model vendor and model type. Outputs include experiment results with task outputs and evaluation metrics. Key parameters are the dataset, task, experiment name, description, metadata, and evaluators.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/summarization.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments import run_experiment\n\nexperiment_results = run_experiment(\n    dataset,\n    task,\n    experiment_name=\"initial-template\",\n    experiment_description=\"first experiment using a simple prompt template\",\n    experiment_metadata={\"vendor\": \"openai\", \"model\": gpt_4o},\n    evaluators=EVALUATORS,\n)\n```\n\n----------------------------------------\n\nTITLE: Specify OpenAI Dependency Version (Python)\nDESCRIPTION: This line specifies the minimum required version for the `openai` Python package. It ensures that the project uses at least version 1.66.5, which is necessary for compatibility with certain features or APIs. This dependency is crucial for interacting with OpenAI services.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/requirements/canary/sdk/openai.txt#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nopenai>=1.66.5\n```\n\n----------------------------------------\n\nTITLE: Running LLM-as-a-Judge Experiment with Self-Refinement Prompt - Python\nDESCRIPTION: Defines a judge function and experiment runner using a prompt template enhanced with self-refinement instructions. The function submits user responses and the improved prompt to the LLM classifier (GPT-4), eliciting initial scoring, critique, and possible adjustment on a 1-10 scale. The experiment applies this logic across the dataset. Requires llm_classify, EMPATHY_EVALUATION_PROMPT_TEMPLATE_IMPROVED, OpenAIModel, evaluate_response, and run_experiment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef llm_as_a_judge(input):\n    response_classifications = llm_classify(\n        dataframe=pd.DataFrame([{\"AI_Response\": input[\"AI_Response\"]}]),\n        template=EMPATHY_EVALUATION_PROMPT_TEMPLATE_IMPROVED,\n        model=OpenAIModel(model=\"gpt-4\"),\n        rails=list(map(str, range(1, 11))),\n        provide_explanation=True,\n    )\n    score = response_classifications.iloc[0][\"label\"]\n    return int(score)\n\n\nexperiment = run_experiment(\n    dataset, task=llm_as_a_judge, evaluators=[evaluate_response], experiment_name=\"self_refinement\"\n)\n```\n\n----------------------------------------\n\nTITLE: Logging Evaluation Results to Phoenix\nDESCRIPTION: Imports `DocumentEvaluations` and `SpanEvaluations` from `phoenix.trace`. Uses an active Phoenix client (`px.Client()`) to log the results of the relevance, Q&A correctness, and hallucination evaluations back to Phoenix. Each evaluation result DataFrame (`retrieved_documents_relevance_df`, `qa_correctness_eval_df`, `hallucination_eval_df`) is wrapped in its corresponding evaluation type (`DocumentEvaluations` or `SpanEvaluations`) along with a descriptive name (`eval_name`).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import DocumentEvaluations, SpanEvaluations\n\n# Log the evaluation results to Phoenix\npx.Client().log_evaluations(\n    DocumentEvaluations(dataframe=retrieved_documents_relevance_df, eval_name=\"relevance\"),\n    SpanEvaluations(dataframe=qa_correctness_eval_df, eval_name=\"Q&A Correctness\"),\n    SpanEvaluations(dataframe=hallucination_eval_df, eval_name=\"Hallucination\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Environment Variables - Python\nDESCRIPTION: This snippet sets the necessary environment variables required to connect to a Phoenix instance. It defines `PHOENIX_CLIENT_HEADERS` which typically includes an API key for authentication, and `PHOENIX_COLLECTOR_ENDPOINT` which specifies the Phoenix instance's address. These variables are essential for the px.Client() to communicate with the Phoenix backend. The API key is set to \"...\" to indicate where a real API key should be placed. The collector endpoint is set to a specific url. If you self-host Phoenix, the client headers should be ignored and the endpoint modified.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/importing-existing-traces.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key=...\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Temporarily Switching Phoenix Project with using_project Context Manager (Python)\nDESCRIPTION: This snippet demonstrates using the 'using_project' context manager from the phoenix.trace module to temporarily switch the project context within a notebook. All spans created inside the 'with' block will be associated with the specified project name. Requires Phoenix's trace utilities to be installed. Input is the target project name (string); all trace calls in the block are grouped accordingly. This is particularly useful for tracing short-lived processes like evaluation runs in development.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/setup-projects.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.trace import using_project\n\n# Switch project to run evals\nwith using_project(\"my-eval-project\"):\n    # all spans created within this context will be associated with\n    # the \"my-eval-project\" project.\n    # Run evaluations here...\n```\n\n----------------------------------------\n\nTITLE: Generating SQL Query (Python)\nDESCRIPTION: The `generate_sql_query` function (not provided in the text, but assumed to exist) takes a question, a list of columns from a DataFrame (`store_sales_df.columns`), and a data source as input, and generates a SQL query to answer the question. The generated query is then used to retrieve results from the data source.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\ngenerate_sql_query(\"What was the most popular product SKU?\", store_sales_df.columns, \"sales\")\n```\n\n----------------------------------------\n\nTITLE: Navigating to Database Directory with cd Command (Bash)\nDESCRIPTION: This command changes the current working directory in the shell to the Phoenix database module directory. It is necessary to be in the correct directory to execute migration-related commands and scripts effectively. There are no additional dependencies, and the input is the proper folder path.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/src/phoenix/db/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd phoenix/src/phoenix/db\n```\n\n----------------------------------------\n\nTITLE: Defining Natural Language Questions for Testing\nDESCRIPTION: Creates a list of 50 natural language questions about music artists, albums, and tracks to test the SQL query agent's capabilities against the Chinook database.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nquestions = [\n    \"What is the name of the artist with ID 5?\",\n    \"List all tracks in the album with ID 3.\",\n    \"How many tracks does the artist named 'Aerosmith' have?\",\n    \"Find the oldest song in the database.\",\n    \"What is the duration of the track with ID 10?\",\n    \"List the names of all albums released in 2020.\",\n    \"How many artists are in the database?\",\n    \"Which artist has the most tracks in the database?\",\n    \"List all tracks in the 'Pop' genre.\",\n    \"What is the average length of tracks in the database?\",\n    \"Find the most recent track.\",\n    \"List the top 5 longest tracks in the database.\",\n    \"Which album has the highest number of tracks?\",\n    \"List all artists who have released more than 3 albums.\",\n    \"What is the shortest track in the database?\",\n    \"Find all albums released by 'The Beatles'.\",\n    \"How many tracks are in the 'Rock' genre?\",\n    \"List the names of all tracks released before 2000.\",\n    \"What is the total duration of the album with ID 7?\",\n    \"Find the artist who released the album 'Thriller'.\",\n    \"List the names of all albums by 'Pink Floyd'.\",\n    \"How many albums have been released between 1990 and 2000?\",\n    \"What genres are covered by the artist 'David Bowie'?\",\n    \"List the top 10 most played tracks.\",\n    \"Which artist has the longest total track duration in the database?\",\n    \"Find all tracks with a duration longer than 5 minutes.\",\n    \"How many tracks does each album contain on average?\",\n    \"List all albums sorted by release date.\",\n    \"Which artist's albums have the highest average ratings?\",\n    \"Find the total duration of all tracks by 'Michael Jackson'.\",\n    \"How many tracks in the database are instrumental?\",\n    \"List the names of all tracks by artists with the name starting with 'J'.\",\n    \"What is the most common genre in the database?\",\n    \"Find the average album length in minutes.\",\n    \"How many artists have only one album in the database?\",\n    \"List all tracks from the album with the most number of tracks.\",\n    \"Which artist has released the most albums?\",\n    \"Find the total number of tracks produced by 'Eminem'.\",\n    \"How many albums in the database have no tracks?\",\n    \"List the name and duration of the longest track in each album.\",\n    \"What is the average number of tracks per album?\",\n    \"Find all albums that have more than 10 tracks.\",\n    \"How many tracks in the database are longer than the average track length?\",\n    \"List the albums released by the artist with the most albums.\",\n    \"Which year has the highest number of album releases?\",\n    \"Find the total playtime of all tracks in the 'Jazz' genre.\",\n    \"How many artists have names longer than 10 characters?\",\n    \"List all song genres found in the database.\",\n    \"What is the average track length of the tracks?\",\n    \"How many albums were released on average by each artist?\",\n]\n\nprint(questions)\n```\n\n----------------------------------------\n\nTITLE: Installing BeautifulSoup Library using pip\nDESCRIPTION: Installs the 'beautifulsoup4' Python library using pip. The '-Uqqq' flags ensure the library is upgraded if already installed, and the installation process is quiet (suppressing output). This library is typically used for parsing HTML and XML documents.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/vision.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uqqq beautifulsoup\n```\n\n----------------------------------------\n\nTITLE: Listing Project Dependencies\nDESCRIPTION: This snippet lists Python package dependencies required for the project.  These dependencies need to be installed for the project to function correctly. The list includes libraries like pandas, langchain, loguru, and others.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/code_gen_agent/requirements.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\narize-phoenix\narize-phoenix-evals\narize-phoenix-otel\nbeautifulsoup4==4.12.3\nlangchain==0.3.14\nlangchain-community\nlangchain-core\nlangchain-openai\nlanggraph==0.2.62\nloguru==0.7.2\npandas==2.2.3\npython-dotenv==1.0.1\ntqdm==4.66.6\ngradio\nopeninference-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Closing the Phoenix App Session\nDESCRIPTION: Gracefully terminates the background Phoenix session to free resources and finalize tracking.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/manage-the-app.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\npx.close_app()\n```\n\n----------------------------------------\n\nTITLE: Invoking OpenAI Chat Completions (Deno JavaScript)\nDESCRIPTION: Calls the `chat.completions.create` method of the initialized OpenAI client, spreading the converted OpenAI prompt parameters. It also demonstrates overriding default parameters like the model (`gpt-4o-mini`) and `stream`. The content of the AI's response is extracted and displayed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_openai_tutorial.ipynb#_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst response = await openai.chat.completions.create({\n  ...openaiPrompt,\n  // you can still override any of the invocation parameters as needed\n  // for example, you can change the model or stream the response\n  model: \"gpt-4o-mini\",\n  stream: false\n})\n\nawait Deno.jupyter.md`\n  ### OpenAI Response\n\n  ${response.choices[0].message.content}\n`\n```\n\n----------------------------------------\n\nTITLE: Installing Instrumentation Package for Anthropic SDK\nDESCRIPTION: This package installation includes the 'openinference-instrumentation-anthropic' SDK and the core 'anthropic' library, enabling instrumentation of LLM calls for monitoring purposes within Python applications. Dependencies are specified via pip.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/anthropic.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-anthropic anthropic\n```\n\n----------------------------------------\n\nTITLE: Downloading and Preparing Training and Production DataFrames - Python\nDESCRIPTION: Downloads training and production datasets as parquet files via HTTP and loads them into pandas DataFrames. It also resets the DataFrame indices to ensure compatibility with embedding generation. 'train_df.head()' previews the training data for validation. The code depends on internet access, the pandas library, and the specified URLs remaining accessible.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/credit_card_fraud_tutorial.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ntrain_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/structured/credit-card-fraud/credit_card_fraud_train.parquet\",\n)\nprod_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/structured/credit-card-fraud/credit_card_fraud_production.parquet\",\n)\ntrain_df = train_df.reset_index(\n    drop=True\n)  # recommended when using EmbeddingGeneratorForTabularFeatures\nprod_df = prod_df.reset_index(\n    drop=True\n)  # recommended when using EmbeddingGeneratorForTabularFeatures\ntrain_df.head()\n```\n\n----------------------------------------\n\nTITLE: Installing Notebook packages\nDESCRIPTION: Installs the `arize-phoenix` package for launching Phoenix in a notebook environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/instructor.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Evaluating Synthetic Dataset with Phoenix\nDESCRIPTION: Executes an evaluation experiment on the uploaded synthetic dataset using the Arize Phoenix `run_experiment` function. Specifies the task, a list of evaluation functions (`no_error`, `has_results`), and configuration metadata for the experiment run.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nrun_experiment(\n    synthetic_dataset, task=task, evaluators=[no_error, has_results], experiment_metadata=CONFIG\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Local Phoenix Endpoint - Python\nDESCRIPTION: Configures the environment variable 'PHOENIX_COLLECTOR_ENDPOINT' to point to a locally running Phoenix instance (usually on port 6006). This setup is required when tracing to a Phoenix server running on your own machine. Invoke before application instrumentation and tracing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/litellm.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: Installs the necessary Python packages, including arize-phoenix, datasets, and openinference-instrumentation-openai, using pip.  These packages are essential for running the tutorial and integrating with Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/few_shot_prompting.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qqq \"arize-phoenix>=8.0.0\" datasets openinference-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Verifying PostgreSQL Configuration\nDESCRIPTION: Command to verify that PostgreSQL is correctly configured by checking the binary directory. This ensures tests requiring a PostgreSQL backend will work properly.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npg_config --bindir\n```\n\n----------------------------------------\n\nTITLE: Defining Article Summarization Tool Prompt (Python)\nDESCRIPTION: Defines the `MessageCreateParamsBase` for an article summarization task using Anthropic's tool use feature, including the definition of a `print_summary` tool with a structured JSON schema for output.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntools = [\n    {\n        \"name\": \"print_summary\",\n        \"description\": \"Prints a summary of the article.\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"author\": {\"type\": \"string\", \"description\": \"Name of the article author\"},\n                \"topics\": {\n                    \"type\": \"array\",\n                    \"items\": {\"type\": \"string\"},\n                    \"description\": 'Array of topics, e.g. [\"tech\", \"politics\"]. Should be as specific as possible, and can overlap.',\n                },\n                \"summary\": {\n                    \"type\": \"string\",\n                    \"description\": \"Summary of the article. One or two paragraphs max.\",\n                },\n                \"coherence\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Coherence of the article's key points, 0-100 (inclusive)\",\n                },\n                \"persuasion\": {\n                    \"type\": \"number\",\n                    \"description\": \"Article's persuasion score, 0.0-1.0 (inclusive)\",\n                },\n            },\n            \"required\": [\"author\", \"topics\", \"summary\", \"coherence\", \"persuasion\", \"counterpoint\"],\n        },\n    }\n]\n\ncontent = \"\"\"\n    <article>\n    {{article}}\n    </article>\n\n    Use the `print_summary` tool.\\\n\"\"\"\n\nparams = MessageCreateParamsBase(\n    model=\"claude-3-5-haiku-latest\",\n    max_tokens=4096,\n    tools=tools,\n    tool_choice={\"type\": \"tool\", \"name\": \"print_summary\"},\n    messages=[{\"role\": \"user\", \"content\": dedent(content)}],\n)\n```\n\n----------------------------------------\n\nTITLE: Converting WikiQA Dataset Split to Pandas DataFrame in Python\nDESCRIPTION: This snippet selects the 'train' split of the loaded dataset and converts it to a pandas DataFrame. The resulting DataFrame, 'raw_df', serves as the base for feature engineering and further manipulation; it contains the raw question-answer pairs and is displayed for inspection. The 'split' variable can be altered to change which partition is processed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_wiki_qa.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsplit = \"train\"\nraw_df = dataset[split].to_pandas()\nraw_df\n```\n\n----------------------------------------\n\nTITLE: Prompting User to Continue to Evaluation Section in Python\nDESCRIPTION: Prompts the user to confirm if they want to proceed from tracing to evaluation section of the tutorial. Input starting with 'n' will halt execution by raising an assertion error. This allows interactive control during notebook usage to optionally stop the tutorial.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nif (\n    input(\"The tutorial is about to move on to the evaluation section. Continue [Y/n]?\")\n    .lower()\n    .startswith(\"n\")\n):\n    assert False, \"notebook stopped\"\n```\n\n----------------------------------------\n\nTITLE: Exporting Processed WikiQA DataFrame to JSONL File in Python\nDESCRIPTION: This snippet serializes each row of the processed DataFrame 'df' to JSON Lines and writes them to a file named based on the current split (e.g., 'wiki_qa-train.jsonl'). Each line is a JSON record ready for downstream NLP tasks. Dependencies: pandas, json. 'data_path' must be writable, and the DataFrame should already be processed as shown earlier.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_wiki_qa.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndata_path = f\"wiki_qa-{split}.jsonl\"\nwith open(data_path, \"w\") as f:\n    for record in df.to_dict(orient=\"records\"):\n        f.write(json.dumps(record) + \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Instrumenting LangChain and OpenAI Clients for Observability in Python\nDESCRIPTION: Sets up OpenTelemetry tracing by creating a TracerProvider and attaching a SimpleSpanProcessor wired to an OTLP exporter endpoint at localhost. It then instruments the LangChain framework and OpenAI API clients to automatically generate trace data for downstream monitoring. This supports distributed tracing of API calls and function executions during experiment runs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/langchain_email_extraction.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nendpoint = \"http://127.0.0.1:4317\"\n(tracer_provider := TracerProvider()).add_span_processor(\n    SimpleSpanProcessor(OTLPSpanExporter(endpoint))\n)\n\nLangChainInstrumentor().instrument(tracer_provider=tracer_provider)\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries with pip\nDESCRIPTION: Installs all necessary Python packages for OpenAI API usage, OpenTelemetry tracing, OpenInference semantic conventions and instrumentation, OTLP exporter, Phoenix UI, and HTTP client with specific version constraints. The command uses pip with line continuation for readability and ensures compatibility by restricting the httpx version below 0.28.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n!pip install openai \\\n  opentelemetry-api \\\n  opentelemetry-sdk \\\n  openinference-semantic-conventions \\\n  openinference-instrumentation-openai \\\n  opentelemetry-exporter-otlp \\\n  arize-phoenix \\\n  'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Invoking OpenAI Chat Completion with Converted Prompt in JavaScript\nDESCRIPTION: Invokes the OpenAI API's chat.completions.create method with the format-converted prompt object and explicitly sets the model to 'gpt-4o-mini' and disables streaming. The response is rendered in the Jupyter markdown cell as raw text or JSON, displaying either the message content or tool call result. Requires prior OpenAI client configuration and a properly formatted prompt object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_cross_sdk_tutorial.ipynb#_snippet_10\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst response = await openai.chat.completions.create({\n  ...openaiPrompt,\n  // you can still override any of the invocation parameters as needed\n  // for example, you can change the model or stream the response\n  model: \"gpt-4o-mini\",\n  stream: false\n})\n\nawait Deno.jupyter.md`\n  ### OpenAI Response\n\n  ${response.choices[0].message.content ?? `\\`\\`\\`json\\n${JSON.stringify(response.choices[0].message.tool_calls, null, 2)}\\`\\`\\``}\n`\n```\n\n----------------------------------------\n\nTITLE: Downloading Benchmark Dataset for Relevance Evaluation in Python\nDESCRIPTION: Downloads a built-in benchmark dataset (e.g., 'wiki_qa-train') containing queries, documents, and ground-truth relevance labels, using Phoenix's download_benchmark_dataset. Loads it into a pandas DataFrame and previews the data's first rows. Ensure proper dataset name and availability of dependencies.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = download_benchmark_dataset(\n    task=\"binary-relevance-classification\", dataset_name=\"wiki_qa-train\"\n)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Generating API Documentation with Sphinx-apidoc\nDESCRIPTION: Command for automatically generating API reference files using sphinx-apidoc. This creates reStructuredText files with autodoc directives based on the docstrings in the codebase.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/api_reference/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsphinx-apidoc -o ./source/output ../path/to/module --separate -M\n```\n\n----------------------------------------\n\nTITLE: Retrieving a prompt by Tag in TypeScript\nDESCRIPTION: Shows how to load a prompt associated with a specific tag and prompt name in TypeScript, facilitating environment-specific prompt selection such as staging or production.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/how-to-prompts/using-a-prompt.md#_snippet_5\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { getPrompt } from \"@arizeai/phoenix-client/prompts\";\n\nconst promptByTag = await getPrompt({ tag: \"staging\", name: \"my-prompt\" });\n// ^ the specific prompt version tagged \"staging\", for prompt \"my-prompt\"\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Instrumentation Package using npm\nDESCRIPTION: Shows how to install the `@arizeai/openinference-instrumentation-openai` package using the npm package manager. This package is required for automatic instrumentation of the OpenAI Node.js SDK with OpenTelemetry.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai-node-sdk.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install --save @arizeai/openinference-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Endpoint (Command Line)\nDESCRIPTION: This Python code sets the `PHOENIX_COLLECTOR_ENDPOINT` environment variable to point to the local Phoenix instance running at `http://localhost:6006`.  This configuration allows the application to send trace data to the local Phoenix server.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/hfsmolagents.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Defining Knowledge Base Schema with Phoenix Python\nDESCRIPTION: Defines the schema required by Phoenix to log knowledge base documents from a vector store. It specifies the columns for the document ID, raw text data, and embedding vector, mapping them to the Phoenix `Schema` and `EmbeddingColumnNames` objects.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/retrieval/quickstart-retrieval.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncorpus_schema = px.Schema(\n    id_column_name=\"id\",\n    document_column_names=EmbeddingColumnNames(\n        vector_column_name=\"embedding\",\n        raw_data_column_name=\"text\",\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Zero-Shot Task and Evaluator\nDESCRIPTION: Defines the `zero_shot_prompt` function that sends the review text to OpenAI and extracts the model's sentiment prediction. It also defines the `evaluate_response` function that compares the predicted sentiment with the ground truth label from the dataset. Both functions will be used in the subsequent experiment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/few_shot_prompting.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef zero_shot_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(**prompt.format(variables={\"Review\": input[\"Review\"]}))\n    return resp.choices[0].message.content.strip()\n\n\ndef evaluate_response(output, expected):\n    return output.lower() == expected[\"Sentiment\"].lower()\n```\n\n----------------------------------------\n\nTITLE: Running SQL Generation Eval with OpenAI\nDESCRIPTION: This code snippet demonstrates how to run an SQL generation evaluation using the OpenAI model. It initializes the model with 'gpt-4' and a temperature of 0.0, then uses the `llm_classify` function to classify the relevance of the generated SQL based on a provided dataframe, template, model, and rails (presumably defining constraints or guidelines). The `provide_explanation` parameter enables the generation of explanations for the classifications.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/sql-generation-eval.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nrails = list(SQL_GEN_EVAL_PROMPT_RAILS_MAP.values())\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\nrelevance_classifications = llm_classify(\n    dataframe=df,\n    template=SQL_GEN_EVAL_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    provide_explanation=True\n)\n```\n\n----------------------------------------\n\nTITLE: Prompting for OpenAI API Key in JavaScript\nDESCRIPTION: Requests the OpenAI API key from the user at runtime using the prompt function. No dependencies except standard JavaScript; essential for secure API authentication when instantiating the OpenAI SDK.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_cross_sdk_tutorial.ipynb#_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst apiKey = prompt(\"Enter your OpenAI API key:\")\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key for Model Access\nDESCRIPTION: Sets up the API key for OpenAI access, retrieving from environment variables or prompting the user securely. Ensures subsequent API calls for model inference are authenticated.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Loading MS-MARCO Dataset Split Using Datasets Library in Python\nDESCRIPTION: Loads a specified split (such as 'train') and version of the MS-MARCO dataset into memory using the Hugging Face datasets API. Requires the datasets library and internet connectivity for downloading the dataset. Inputs are the desired split and version string; output is a datasets.Dataset object containing the raw MS-MARCO entries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_ms_marco.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsplit = \"train\"\nversion = \"v1.1\"\ndataset = load_dataset(\"ms_marco\", version, split=split)\n```\n\n----------------------------------------\n\nTITLE: Installing DSPy\nDESCRIPTION: This snippet installs the DSPy and openinference-instrumentation-dspy Python packages using pip.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q dspy openinference-instrumentation-dspy\n```\n\n----------------------------------------\n\nTITLE: Defining Experiment Task Function\nDESCRIPTION: This Python function defines the core task for the experiment. It takes an example dictionary (bound to 'x') as input and calls the previously defined `text2sql` function with the 'question' field from the example, returning the result which includes the generated query, execution results, or an error.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/run-experiments.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef task(x):\n    return text2sql(x[\"question\"])\n```\n\n----------------------------------------\n\nTITLE: Importing Phoenix Evaluation Libraries and Metric Tools\nDESCRIPTION: Imports specialized libraries for confusion matrices, classification reports, and Phoenix-specific evaluation components including QA prompt templates and LLM classification helpers.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pycm import ConfusionMatrix\nfrom sklearn.metrics import classification_report\n\nfrom phoenix.evals import (\n    QA_PROMPT_RAILS_MAP,\n    QA_PROMPT_TEMPLATE,\n    OpenAIModel,\n    download_benchmark_dataset,\n    llm_classify,\n)\n\npd.set_option(\"display.max_colwidth\", None)\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with Tox\nDESCRIPTION: Command to execute unit tests using the tox environment. By default, database tests only run against SQLite for faster execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ntox run -e unit_tests\n```\n\n----------------------------------------\n\nTITLE: Creating a Python Virtual Environment with UV\nDESCRIPTION: Command to create a new Python virtual environment using UV with Python 3.9, which is the lowest version compatible with Phoenix. This helps ensure code compatibility across all supported Python versions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv venv --python 3.9\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix OpenTelemetry Package with Bash\nDESCRIPTION: This snippet installs the arize-phoenix-otel Python package, which is required to send OpenTelemetry traces from PromptFlow flows to Arize Phoenix. The command should be run in your shell environment prior to usage of the Phoenix tracing capabilities.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/prompt-flow.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Loading Query Data from Buffer into Dataframe in Python\nDESCRIPTION: Loads query data from a callback handler buffer into a pandas dataframe. The data includes query IDs, timestamps, prompt text, embeddings, responses, and document retrieval information in OpenInference format.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_search_and_retrieval_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nquery_data_buffer = callback_handler.flush_query_data_buffer()\nsample_query_df = as_dataframe(query_data_buffer)\nsample_query_df\n```\n\n----------------------------------------\n\nTITLE: Wrapping DataFrame and Schema into Phoenix Inferences object\nDESCRIPTION: Encapsulates the DataFrame and its schema into a Phoenix Inferences object, which can be used for visualization. The 'name' parameter labels this specific inference set.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/phoenix-inferences.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ntrain_ds = px.Inferences(dataframe=train_df, schema=train_schema, name=\"training\")\n```\n\n----------------------------------------\n\nTITLE: Testing the Traced Express Server with curl (Shell)\nDESCRIPTION: This shell command issues an HTTP request to the `/chat` endpoint of the sample Express server, providing the query parameter `message` with the value 'write me a haiku'. This triggers an OpenAI chat API call, which is then traced and exported to Phoenix if instrumentation is set up. Useful for testing both server and trace export functionality. Input is a string query to chat endpoint; output is a text LLM result printed to terminal.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ncurl \"http://localhost:8080/chat?message=write%20me%20a%20haiku\"\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI and Phoenix API Keys\nDESCRIPTION: This snippet sets the OpenAI and Phoenix API keys. It first checks for the existence of the environment variables. If they are not set, it prompts the user to enter the keys using `getpass`. These keys are essential for authentication and accessing the respective APIs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n\n# Your Phoenix API key can be found at app.phoenix.arize.com in the \"Keys\" section of the left navbar\nif not os.getenv(\"PHOENIX-API-KEY\"):\n    os.environ[\"PHOENIX-API-KEY\"] = getpass(\"ðŸ”‘ Enter your Phoenix API key: \")\n```\n\n----------------------------------------\n\nTITLE: Process Generated Questions and Store in DataFrame\nDESCRIPTION: This snippet splits the generated response into individual questions, creates a pandas DataFrame to store these questions, and prints the DataFrame for inspection. It prepares the dataset for response analysis and evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nsplit_response = resp.strip().split(\"\\n\")\n\nquestions_df = pd.DataFrame(split_response, columns=[\"question\"])\nprint(questions_df)\n```\n\n----------------------------------------\n\nTITLE: Define Evaluation Scoring Functions Python\nDESCRIPTION: Defines two simple scoring functions used for evaluating the Text2SQL output. `no_error` checks if the execution of the generated SQL query resulted in an error, returning 1.0 if there was no error and 0.0 otherwise. `has_results` checks if the query execution produced any results, returning 1.0 if results exist and are not empty, and 0.0 otherwise. These functions provide basic metrics for query validity and output presence.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n# Test if there are no sql execution errors\ndef no_error(output):\n    return 1.0 if output.get(\"error\") is None else 0.0\n\n\n# Test if the query has results\ndef has_results(output):\n    results = output.get(\"results\")\n    has_results = results is not None and len(results) > 0\n    return 1.0 if has_results else 0.0\n```\n\n----------------------------------------\n\nTITLE: Sampling SQA Dataset from HuggingFace with Message Length Filtering in Python\nDESCRIPTION: This snippet loads the 'sqa' split of the nvidia/ChatRAG-Bench test dataset and filters for samples where the number of messages is at least half the maximum found. It exports 100 filtered samples to a compressed CSV file and prints the maximum message length. The code requires HuggingFace's datasets package and uses the Pandas DataFrame interface for processing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/ChatRAG-Bench.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nname = \"sqa\"\ndf = load_dataset(\"nvidia/ChatRAG-Bench\", name)[\"test\"].to_pandas()\nprint(df.messages.apply(len).max())\ndf.loc[df.messages.apply(len) >= df.messages.apply(len).max() // 2].sample(\n    100, random_state=42\n).to_csv(f\"{name}_samples.csv.gz\", index=False)\ndf.sample(10)\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Project Environment Variable\nDESCRIPTION: Defines the target Phoenix project environment by setting 'PHOENIX_PROJECT_NAME'. This configures where traces will be sent and organized within Phoenix, allowing for targeted observability.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/project_sessions_llama_index_query_engine.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nos.environ[\"PHOENIX_PROJECT_NAME\"] = \"SESSIONS-DEMO\"\n```\n\n----------------------------------------\n\nTITLE: Setting the SQL Database URL Environment Variable (Bash)\nDESCRIPTION: This command sets the 'PHOENIX_SQL_DATABASE_URL' environment variable, which is required for Alembic and other tooling to connect to a non-default SQL database (such as PostgreSQL). The placeholder '<sql-database-url>' must be replaced with a valid database connection string. This variable is typically not needed if using the default SQLite setup. No output is produced, but it configures the session for subsequent commands.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/src/phoenix/db/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport PHOENIX_SQL_DATABASE_URL=<sql-database-url>\n```\n\n----------------------------------------\n\nTITLE: Accessing Exported Cluster Data from Phoenix Session (Python)\nDESCRIPTION: Retrieves the data exported from the Phoenix UI (presumably a cluster identified as problematic). It accesses the last exported dataset from the `session.exports` list (which is a list of DataFrames) and stores it in the `export_df` variable. `.head()` is called to display the first few rows of the exported data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/image_classification_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nexport_df = session.exports[-1]\nexport_df.head()\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies\nDESCRIPTION: This snippet specifies the Python package dependencies for the project. It uses the pip package management format to list packages, versions, and platform-specific dependencies. The dependencies cover areas like database access (asyncpg, psycopg), AI model integration (openai, anthropic, google-generativeai), and data analysis (fast-hdbscan, umap-learn). The use of `psycopg[binary,pool]` enables the use of pre-compiled binaries and connection pools for PostgreSQL. `uvloop` is included for improved performance on non-Windows platforms.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/requirements/dev.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nasyncpg\nopenai\nanthropic>=0.49.0\ngoogle-generativeai\npsycopg[binary,pool]\nuvloop; platform_system != 'Windows'\nfast-hdbscan\numap-learn\n```\n\n----------------------------------------\n\nTITLE: Identifying Added Columns in Wrangled DataFrame Compared to Original in Python\nDESCRIPTION: Calculates the set of columns present in the wrangled DataFrame but absent in the raw DataFrame, ensuring final schema completeness. Inputs are the sets of DataFrame column names. Outputs a set of newly added column names specific to the wrangling process.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_ms_marco.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nset(df.columns).difference(raw_df.columns)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenInference MistralAI Instrumentation and MistralAI SDK - Bash\nDESCRIPTION: Installs the openinference-instrumentation-mistralai package along with the official mistralai SDK. Both packages are required for instrumenting LLM calls made with MistralAI and report telemetry from the SDK to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/mistralai.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-mistralai mistralai\n```\n\n----------------------------------------\n\nTITLE: Running Hallucination and QA Correctness Evaluations in Python\nDESCRIPTION: Applies hallucination and QA correctness evaluators over QA reference queries, using the run_evals helper to produce two result DataFrames with explanations. Inputs are queries_df and relevant evaluators, outputs are hallucination_eval_df and qa_eval_df.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nhallucination_eval_df, qa_eval_df = run_evals(\n    dataframe=queries_df,\n    evaluators=[hallucination_evaluator, qa_evaluator],\n    provide_explanation=True,\n    concurrency=20,\n)\nhallucination_eval_df\n```\n\n----------------------------------------\n\nTITLE: Loading Camera Dataset into In-Memory SQLite Database Using pandas and SQLAlchemy in Python\nDESCRIPTION: Downloads a digital cameras dataset from the specified public Parquet file URL using pandas. Converts the DataFrame into an in-memory SQLite database via SQLAlchemy engine for SQL-based querying. This setup enables querying structured camera data using SQL in later steps.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_sql_retriever_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncamera_info = pd.read_parquet(\n    \"https://storage.googleapis.com/arize-phoenix-assets/datasets/structured/camera-info/cameras.parquet\"\n)\n\nengine = create_engine(\"sqlite:///:memory:\", future=True)\ncamera_info.to_sql(\"cameras\", engine, index=False)\n```\n\n----------------------------------------\n\nTITLE: Creating a Query Engine and Querying - Python\nDESCRIPTION: This snippet creates a query engine from the vector index and performs a query.  The response, source nodes and their text are retrieved to see the result. This helps in examining the behavior of the RAG pipeline.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nquery_engine = vector_index.as_query_engine()\nresponse_vector = query_engine.query(\"What did the author do growing up?\")\n```\n\n----------------------------------------\n\nTITLE: Adding A-Frame Plane Primitive (HTML)\nDESCRIPTION: Defines a plane geometry in the 3D scene using the A-Frame primitive element `<a-plane>`. Attributes set its `position` (x, y, z coordinates), `rotation` (rotation around x, y, z axes), `width`, `height`, and `color`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_5\n\nLANGUAGE: HTML\nCODE:\n```\n<a-plane position=\"0 0 -4\" rotation=\"-90 0 0\" width=\"4\" height=\"4\" color=\"#7BC8A4\"></a-plane>\n```\n\n----------------------------------------\n\nTITLE: Defining Retrieval/Response Schema with Phoenix Python\nDESCRIPTION: Defines the schema for logging retrieval events, including the user query, its embedding, the IDs of retrieved documents, their relevance scores, and the generated response. It uses `px.Schema` and `RetrievalEmbeddingColumnNames` to map these fields.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/retrieval/quickstart-retrieval.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprimary_schema = Schema(\n    prediction_id_column_name=\"id\",\n    prompt_column_names=RetrievalEmbeddingColumnNames(\n        vector_column_name=\"embedding\",\n        raw_data_column_name=\"query\",\n        context_retrieval_ids_column_name=\"retrieved_document_ids\",\n        context_retrieval_scores_column_name=\"relevance_scores\",\n    )\n    response_column_names=\"response\",\n)\n```\n\n----------------------------------------\n\nTITLE: Defining LangChain Query Check Runnable\nDESCRIPTION: Defines a system prompt and creates a `ChatPromptTemplate` for an LLM to act as a SQL expert, checking for common mistakes in a query. It combines this prompt with a `ChatOpenAI` model and binds the `db_query_tool` to create a runnable `query_check` that can be invoked to validate queries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate\n\nquery_check_system = \"\"\"You are a SQL expert with a strong attention to detail.\nDouble check the SQLite query for common mistakes, including:\n- Using NOT IN with NULL values\n- Using UNION when UNION ALL should have been used\n- Using BETWEEN for exclusive ranges\n- Data type mismatch in predicates\n- Properly quoting identifiers\n- Using the correct number of arguments for functions\n- Casting to the correct data type\n- Using the proper columns for joins\n\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\n\nYou will call the appropriate tool to execute the query after running this check.\"\"\"\n\nquery_check_prompt = ChatPromptTemplate.from_messages(\n    [(\"system\", query_check_system), (\"placeholder\", \"{messages}\")]\n)\nquery_check = query_check_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(\n    [db_query_tool], tool_choice=\"required\"\n)\n\nquery_check.invoke({\"messages\": [(\"user\", \"SELECT * FROM Artist LIMIT 10;\")]})\n```\n\n----------------------------------------\n\nTITLE: Run Initial Evaluation Experiment Python\nDESCRIPTION: Runs the first evaluation experiment using Phoenix. It takes the uploaded dataset (`ds`), the `task` function (which wraps the `text2sql` logic), and the defined `evaluators` (`no_error`, `has_results`). The `experiment_metadata` includes the LLM configuration used. Phoenix executes the `task` for each input in the dataset, collects the outputs and traces, and applies the evaluators to compute scores. The result is stored in the `experiment` object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nimport phoenix as px\nfrom phoenix.experiments import run_experiment\n\n\n# Define the task to run text2sql on the input question\ndef task(input):\n    return text2sql(input[\"question\"])\n\n\nexperiment = run_experiment(\n    ds, task=task, evaluators=[no_error, has_results], experiment_metadata=CONFIG\n)\n```\n\n----------------------------------------\n\nTITLE: Re-displaying Phoenix Session View in Python\nDESCRIPTION: Calls session.view() to relaunch or refresh the Phoenix UI after evaluation results have been logged. No parameters required.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nsession.view()\n```\n\n----------------------------------------\n\nTITLE: Defining Concise Prompt Template and Running Phoenix Experiment in Python\nDESCRIPTION: Defines a prompt template to instruct the language model to generate concise two-to-four sentence summaries, improving prompt specificity for the summarization task. Uses Python's 'partial' to bind the prompt template and model to a summarization function. Runs a Phoenix experiment with this concise prompt template to assess changes in summary length and metrics. Requires the 'summarize_article_openai' function and the GPT-4o model reference.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/summarization.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntemplate = \"\"\"\nSummarize the article in two to four sentences. Be concise and include only the most important information.\n\nARTICLE\n=======\n{article}\n\nSUMMARY\n=======\n\"\"\"\ntask = partial(summarize_article_openai, prompt_template=template, model=gpt_4o)\nexperiment_results = run_experiment(\n    dataset,\n    task,\n    experiment_name=\"concise-template\",\n    experiment_description=\"explicitly instuct the llm to be concise\",\n    experiment_metadata={\"vendor\": \"openai\", \"model\": gpt_4o},\n    evaluators=EVALUATORS,\n)\n```\n\n----------------------------------------\n\nTITLE: Joining Child Spans to Parent Spans using Phoenix and Pandas (Python)\nDESCRIPTION: Illustrates how to join child spans to their parent spans. It involves defining two separate `SpanQuery` objects (`query_for_parent_spans`, `query_for_child_spans`), executing them simultaneously using `px.Client().query_spans()`, and then merging the resulting list of DataFrames using `pandas.concat()`. Requires the child span query to use `parent_id` renamed as `span_id` for indexing and performs an inner join based on the index (`axis=1`, `join=\"inner\"`). Requires `pandas` library.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\npd.concatenate(\n    px.Client().query_spans(\n        query_for_parent_spans,\n        query_for_child_spans,\n    ),\n    axis=1,        # joining on the row indices\n    join=\"inner\",  # inner-join by the indices of the dataframes\n)\n```\n\n----------------------------------------\n\nTITLE: Environment Configuration for Self-hosted Phoenix\nDESCRIPTION: Configuration of environment variables specific to connecting with a self-hosted Phoenix instance, with optional authentication.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-ts.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# .env, or shell environment\n\n# Collector Endpoint for your self hosted Phoenix, like localhost\nPHOENIX_COLLECTOR_ENDPOINT=\"http://localhost:6006\"\n# (optional) If authentication enabled, add Phoenix API Key for tracing\nPHOENIX_API_KEY=\"ADD YOUR API KEY\"\n```\n\n----------------------------------------\n\nTITLE: Listing Available Tox Environments\nDESCRIPTION: Command to list all available tox environments defined in tox.ini. These environments correspond to different sets of commands for testing, linting, and other development tasks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ntox list\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI SDK Requests for Trace Collection\nDESCRIPTION: Installs the OpenAI instrumentation package and applies it to the OpenAI SDK client, enabling automatic tracing of API calls. Skips dependency checks for streamlined setup when integrating with Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/tracing_quickstart_openai.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q openinference-instrumentation-openai openai 'httpx<0.28' \n```\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider, skip_dep_check=True)\n```\n\n----------------------------------------\n\nTITLE: Preparing Trace Data for TLM Evaluation\nDESCRIPTION: Code to extract and format input prompts and output responses from Phoenix traces. This includes parsing JSON data, combining system and user prompts, and extracting the LLM responses for evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/integrations/cleanlab.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create a new DataFrame with input and output columns\neval_df = spans_df[[\"context.span_id\", \"attributes.input.value\", \"attributes.output.value\"]].copy()\neval_df.set_index(\"context.span_id\", inplace=True)\n\n# Combine system and user prompts from the traces\ndef get_prompt(input_value):\n    if isinstance(input_value, str):\n        input_value = json.loads(input_value)\n    system_prompt = input_value[\"messages\"][0][\"content\"]\n    user_prompt = input_value[\"messages\"][1][\"content\"]\n    return system_prompt + \"\\n\" + user_prompt\n\n# Get the responses from the traces\ndef get_response(output_value):\n    if isinstance(output_value, str):\n        output_value = json.loads(output_value)\n    return output_value[\"choices\"][0][\"message\"][\"content\"]\n\n# Create a list of prompts and associated responses\nprompts = [get_prompt(input_value) for input_value in eval_df[\"attributes.input.value\"]]\nresponses = [get_response(output_value) for output_value in eval_df[\"attributes.output.value\"]]\n\neval_df[\"prompt\"] = prompts\neval_df[\"response\"] = responses\n```\n\n----------------------------------------\n\nTITLE: Accessing Exported Dataframe - Python\nDESCRIPTION: This Python code snippet retrieves the last exported dataframe from the Phoenix session. It utilizes the `px.active_session()` function to get the active Phoenix session and then accesses the `exports` attribute, taking the last element (`[-1]`) and calling the `dataframe` attribute to retrieve the exported data.  This snippet is dependent on having a Phoenix session and having successfully exported data, which could either be a Parquet file or available in the notebook.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/export-your-data.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsession = px.active_session()\nsession.exports[-1].dataframe\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Packages (Command Line)\nDESCRIPTION: This bash command installs the `arize-phoenix-otel` package, necessary for instrumenting the application to send traces to the locally running Phoenix instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/hfsmolagents.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Modifying Trace Timestamps with Faker\nDESCRIPTION: Uses the Faker library to generate realistic but fake start timestamps for each trace in the retrieved dataset ('td.dataframe'). It calculates the original duration of each trace (end_time - start_time) and then computes new end times by adding the duration to the newly generated fake start times. The 'start_time' and 'end_time' columns in the trace DataFrame are overwritten with these modified timestamps.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/vision.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfake = Faker()\nstart_time = pd.Series(\n    [fake.date_time_between(\"-3d\") for _ in range(len(td.dataframe))], index=td.dataframe.index\n)\nduration = td.dataframe.end_time - td.dataframe.start_time\nend_time = start_time + duration\ntd.dataframe[\"start_time\"] = start_time\ntd.dataframe[\"end_time\"] = end_time\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for OpenTelemetry Span Filtering in Python\nDESCRIPTION: Installs required Python packages using pip and uv for OpenTelemetry setup. This includes the SDK, OTLP HTTP exporter, OpenInference instrumentation for OpenAI, the OpenAI client library, python-dotenv for environment variables, and httpx (pinned to <0.28).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/span_filtering_tutorial.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n!pip install -q uv\n!uv pip install --system -q opentelemetry-sdk opentelemetry-exporter-otlp-proto-http openinference-instrumentation-openai openai python-dotenv 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix and Launching the Application\nDESCRIPTION: Installs the Phoenix package if not already installed, imports it, and launches the Phoenix app for prompt management and UI interaction. Facilitates local prompt hosting and testing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_openai.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n%pip install -Uqqq arize-phoenix\nimport phoenix as px\n\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Using Phoenix with Docker\nDESCRIPTION: Docker commands to pull and run Phoenix in a container, exposing it on port 6006.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langchain.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Sampling DoQA Cooking Dataset from HuggingFace with Message Length Filtering in Python\nDESCRIPTION: This code downloads the 'doqa_cooking' split from nvidia/ChatRAG-Bench, selects test samples with a message count greater than or equal to half of the maximum, and exports 100 random samples to a gzipped CSV. It expects HuggingFace's datasets Python package and outputs the maximum message length; the DataFrame must include a 'messages' column.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/ChatRAG-Bench.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nname = \"doqa_cooking\"\ndf = load_dataset(\"nvidia/ChatRAG-Bench\", name)[\"test\"].to_pandas()\nprint(df.messages.apply(len).max())\ndf.loc[df.messages.apply(len) >= df.messages.apply(len).max() // 2].sample(\n    100, random_state=42\n).to_csv(f\"{name}_samples.csv.gz\", index=False)\ndf.sample(10)\n```\n\n----------------------------------------\n\nTITLE: Querying Policies Query Engine\nDESCRIPTION: Tests the query engine created for the company policies Chroma database by sending a specific question about the travel policy. This demonstrates the retrieval capability of the configured LlamaIndex query engine based on the vector store.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/agentic_rag_tracing.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nchroma_engine_policy.query(\"What is the travel policy?\")\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix API Key - Python\nDESCRIPTION: This code snippet sets the Phoenix API key and collector endpoint as environment variables.  It's a key step in configuring the connection between the instrumented application and the Phoenix service. The `PHOENIX_API_KEY` should be replaced with the actual API key from the Phoenix dashboard.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/crewai.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Initial Rows of the Training DataFrame in Python\nDESCRIPTION: Uses the Pandas `head()` method to display the first five rows of the `train_df` DataFrame, allowing inspection of column names and sample data values.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/sentiment_classification_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_df.head()\n```\n\n----------------------------------------\n\nTITLE: Define Text2SQL Task Function Python\nDESCRIPTION: Defines an asynchronous Python function `text2sql` that takes a natural language `question` as input. It first calls the `generate_query` function to get the SQL query. Then, it attempts to execute the generated query using `execute_query`. It handles potential `duckdb.Error` exceptions during execution. The function returns a dictionary containing the generated `query`, the `results` (if successful), and an `error` message (if an exception occurred). This function represents the full pipeline logic for a single input.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nasync def text2sql(question):\n    query = await generate_query(question)\n    results = None\n    error = None\n    try:\n        results = execute_query(query)\n    except duckdb.Error as e:\n        error = str(e)\n\n    return {\n        \"query\": query,\n        \"results\": results,\n        \"error\": error,\n    }\n```\n\n----------------------------------------\n\nTITLE: Performing a Dry Run of the Phoenix Experiment in Python\nDESCRIPTION: Uses the `phoenix.experiments.run_experiment` function to execute the defined `task` function on the uploaded `dataset`. The `dry_run=3` argument specifies that the task should only be run on 3 randomly selected examples from the dataset. This allows for a quick verification of the experiment setup and task execution without processing the entire dataset. The results are stored in the `experiment` object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nexperiment = run_experiment(dataset, task, dry_run=3)\n```\n\n----------------------------------------\n\nTITLE: Displaying the Hallucination Classification Prompt Template\nDESCRIPTION: Prints the default prompt template used for hallucination classification. This template structures how the LLM evaluates whether an answer contains hallucinations based on a reference context.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(HALLUCINATION_PROMPT_TEMPLATE)\n```\n\n----------------------------------------\n\nTITLE: Function for Query Generation Node with Error Checking\nDESCRIPTION: Defines a function that invokes the query generation model node, checks for incorrect tool calls, and appends error messages if the model calls tools other than SubmitFinalAnswer. Ensures correct tool usage before proceeding.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ndef query_gen_node(state: State):\n    message = query_gen.invoke(state)\n\n    # Sometimes, the LLM will hallucinate and call the wrong tool. We need to catch this and return an error message.\n    tool_messages = []\n    if message.tool_calls:\n        for tc in message.tool_calls:\n            if tc[\"name\"] != \"SubmitFinalAnswer\":\n                tool_messages.append(\n                    ToolMessage(\n                        content=f\"Error: The wrong tool was called: {tc['name']}. Please fix your mistakes. Remember to only call SubmitFinalAnswer to submit the final answer. Generated queries should be outputted WITHOUT a tool call.\",\n                        tool_call_id=tc[\"id\"],\n                    )\n                )\n    else:\n        tool_messages = []\n    return {\"messages\": [message] + tool_messages}\n```\n\n----------------------------------------\n\nTITLE: Creating and Running Smolagents Agent\nDESCRIPTION: This Python code demonstrates the creation and execution of a smolagents `CodeAgent` with tools. The agent uses a `HfApiModel` and is configured to use search and web page visit tools. The agent is created, then run with a prompt to query the GDP.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/hfsmolagents.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import (\n    CodeAgent,\n    ToolCallingAgent,\n    ManagedAgent,\n    DuckDuckGoSearchTool,\n    VisitWebpageTool,\n    HfApiModel,\n)\n\nmodel = HfApiModel()\n\nagent = ToolCallingAgent(\n    tools=[DuckDuckGoSearchTool(), VisitWebpageTool()],\n    model=model,\n)\nmanaged_agent = ManagedAgent(\n    agent=agent,\n    name=\"managed_agent\",\n    description=\"This is an agent that can do web search.\",\n)\nmanager_agent = CodeAgent(\n    tools=[],\n    model=model,\n    managed_agents=[managed_agent],\n)\nmanager_agent.run(\n    \"If the US keeps its 2024 growth rate, how many years will it take for the GDP to double?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Introducing Missing Data Errors (Python)\nDESCRIPTION: Deliberately modifies the sampled dataframe to introduce missing data by setting specific entries in the 'reference' and 'input' columns to `None`. This step simulates real-world data quality issues to demonstrate how Phoenix evals handle incomplete inputs during processing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf_sample.loc[28, \"reference\"] = None\ndf_sample.loc[37, \"input\"] = None\n```\n\n----------------------------------------\n\nTITLE: Merging Evaluation Results with Query Embeddings DataFrame in Python\nDESCRIPTION: Combines hallucination and QA correctness evaluation DataFrames with the query embeddings DataFrame by concatenating them along columns with inner join on indices. Columns are renamed to distinguish between hallucination and QA correctness labels and scores. This produces a consolidated DataFrame associating each query embedding with its evaluation scores for further analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nquery_embeddings_with_evals_df = pd.concat(\n    [\n        hallucination_eval_df[[\"label\", \"score\"]].rename(\n            columns={\"label\": \"hallucination_label\", \"score\": \"hallucination_score\"}\n        ),\n        qa_correctness_eval_df[[\"label\", \"score\"]].rename(\n            columns={\"label\": \"qa_correctness_label\", \"score\": \"qa_correctness_score\"}\n        ),\n        query_embeddings_df,\n    ],\n    axis=1,  # joining on the row indices\n    join=\"inner\",  # inner-join by the indices of the DataFrames\n)\n\nquery_embeddings_with_evals_df.head()\n```\n\n----------------------------------------\n\nTITLE: Sampling Documents from Database for Analysis in Python\nDESCRIPTION: This code samples documents from a database dataframe by combining retrieved documents with a random sample of unretrieved documents to create a balanced dataset for analysis. It maintains all retrieved documents and supplements with additional random samples to reach a target sample size.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_search_and_retrieval_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnum_sampled_point = 500\nretrieved_document_ids = set(\n    [\n        doc_id\n        for doc_ids in query_df[\":feature.[str].retrieved_document_ids:prompt\"].to_list()\n        for doc_id in doc_ids\n    ]\n)\nretrieved_document_mask = database_df[\"document_id\"].isin(retrieved_document_ids)\nnum_retrieved_documents = len(retrieved_document_ids)\nnum_additional_samples = num_sampled_point - num_retrieved_documents\nunretrieved_document_mask = ~retrieved_document_mask\nsampled_unretrieved_document_ids = set(\n    database_df[unretrieved_document_mask][\"document_id\"]\n    .sample(n=num_additional_samples, random_state=0)\n    .to_list()\n)\nsampled_unretrieved_document_mask = database_df[\"document_id\"].isin(\n    sampled_unretrieved_document_ids\n)\nsampled_document_mask = retrieved_document_mask | sampled_unretrieved_document_mask\nsampled_database_df = database_df[sampled_document_mask]\n```\n\n----------------------------------------\n\nTITLE: Defining Code Generation Questions and Example Data\nDESCRIPTION: This code defines a set of questions for evaluating Python code generation, along with corresponding example data and chart configurations. This data is then used to create a dataset for evaluating code generation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\ncode_generation_questions = [\n    \"Create a bar chart showing total sales by store\",\n    \"Plot daily sales volume over time\",\n    \"Plot a line graph showing the sales trend over time with a 7-day moving average\",\n    \"Create a histogram of quantities sold per transaction\",\n    \"Generate a pie chart showing sales distribution across product classes\",\n    \"Create a stacked bar chart showing promotional vs non-promotional sales by store\",\n    \"Generate a heatmap of sales by day of week and store number\",\n    \"Plot a line chart comparing sales trends between top 5 stores\",\n]\n\nexample_data = []\nchart_configs = []\nfor question in tqdm(code_generation_questions[:], desc=\"Processing code generation questions\"):\n    try:\n        with suppress_tracing():\n            example_data.append(lookup_sales_data(question))\n            chart_configs.append(json.dumps(extract_chart_config(example_data[-1], question)))\n    except Exception as e:\n        print(f\"Error processing question: {question}\")\n        print(e)\n        code_generation_questions.remove(question)\n```\n\n----------------------------------------\n\nTITLE: Computing NDCG@k (Pandas, Scikit-learn)\nDESCRIPTION: This snippet defines a function `_compute_ndcg` that computes the Normalized Discounted Cumulative Gain (NDCG) at a specified rank `k` using scikit-learn's `ndcg_score` function.  It handles missing values (NaN) in the input DataFrame. It is applied to a grouped DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom sklearn.metrics import ndcg_score\n\n\ndef _compute_ndcg(df: pd.DataFrame, k: int):\n    \"\"\"Compute NDCG@k in the presence of missing values\"\"\"\n    n = max(2, len(df))\n    eval_scores = np.zeros(n)\n    doc_scores = np.zeros(n)\n    eval_scores[: len(df)] = df.eval_score\n    doc_scores[: len(df)] = df.document_score\n    try:\n        return ndcg_score([eval_scores], [doc_scores], k=k)\n    except ValueError:\n        return np.nan\n\n\nndcg_at_2 = pd.DataFrame(\n    {\"score\": documents_with_relevance_df.groupby(\"context.span_id\").apply(_compute_ndcg, k=2)}\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Loading TraceDataset in Python\nDESCRIPTION: This snippet demonstrates how to download trace data stored in JSON Lines format from a specified URL using `urllib.request.urlopen`. It then decodes the lines and uses `phoenix.trace.utils.json_lines_to_df` to convert them into a Pandas DataFrame, which is subsequently loaded into a `phoenix.trace.trace_dataset.TraceDataset` object. Dependencies include `urllib.request`, `phoenix.trace.trace_dataset`, and `phoenix.trace.utils`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/log_traces_to_phoenix.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom urllib.request import urlopen\n\nfrom phoenix.trace.trace_dataset import TraceDataset\nfrom phoenix.trace.utils import json_lines_to_df\n\ntraces_url = \"https://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/trace.jsonl\"\nwith urlopen(traces_url) as response:\n    lines = [line.decode(\"utf-8\") for line in response.readlines()]\ntrace_ds = TraceDataset(json_lines_to_df(lines))\n```\n\n----------------------------------------\n\nTITLE: Initializing Vertex AI\nDESCRIPTION: This code initializes the Vertex AI SDK using the provided project ID, location, and staging bucket.  It uses the `vertexai.init()` function to configure the SDK for interacting with Google Cloud services.  Before running this code, the user must provide their Google Cloud project information.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/google_agent_engine_tracing_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nPROJECT_ID = \"\"  # @param {type:\"string\"}\nLOCATION = \"\"  # @param {type:\"string\"}\nSTAGING_BUCKET = \"\"  # @param {type:\"string\"}\n\nimport vertexai\n\nvertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)\n```\n\n----------------------------------------\n\nTITLE: Starting Phoenix Server via Terminal Command\nDESCRIPTION: This command initiates the Phoenix server directly from the terminal, running it on port 6006 by default. No input parameters are required, although customization is possible through environment variables. Ensure 'phoenix' is installed and available in your shell environment. Use this approach when you want to collect traces locally or from remote applications by pointing 'PHOENIX_COLLECTOR_ENDPOINT' to the correct server address.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/environments.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Initializing LangChain Instrumentor in Python (Phoenix v3.0.0+)\nDESCRIPTION: This snippet demonstrates the new, simplified way to instrument LangChain with Phoenix. Only LangChainInstrumentor from phoenix.trace.langchain is needed, and no tracer arguments are used. Requires: phoenix.trace.langchain installed. Input: none. Output: Tracing is activated with new API structure. Fully supported in v3.0.0+.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/MIGRATION.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.trace.langchain import LangChainInstrumentor\n\nLangChainInstrumentor().instrument()\n```\n\n----------------------------------------\n\nTITLE: Applying nest_asyncio to Enable AsyncIO Event Loop Reuse in Python\nDESCRIPTION: Applies the nest_asyncio patch to allow nested use of asyncio event loops, which is necessary when running asynchronous code in Jupyter or other environments that already have a running event loop. This facilitates asynchronous operations required by Phoenix and LiteLLM.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Generating a New Prompt using Meta-Prompting with OpenAI\nDESCRIPTION: This snippet uses the OpenAI API to generate a new prompt based on a meta-prompt and a set of examples.  It calls OpenAI's chat completion endpoint to create a new prompt that is intended to be better than the original. It depends on the `OpenAI` client and the `ground_truth_df` DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmeta_prompt = \"\"\"\nYou are an expert prompt engineer. You are given a prompt, and a list of examples.\n\nYour job is to generate a new prompt that will improve the performance of the model.\n\nHere are the examples:\n\n{examples}\n\nHere is the original prompt:\n\n{prompt}\n\nHere is the new prompt:\n\"\"\"\n\noriginal_base_prompt = (\n    prompt.format(variables={\"prompt\": \"example prompt\"}).get(\"messages\")[0].get(\"content\")\n)\n\nclient = OpenAI()\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": meta_prompt.format(\n                prompt=original_base_prompt, examples=ground_truth_df[\"example\"].to_string()\n            ),\n        }\n    ],\n)\nnew_prompt = response.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Connecting and Verifying MongoDB Atlas Connection\nDESCRIPTION: Establishes a connection to a MongoDB Atlas cluster using user-provided credentials. It constructs the connection URI, creates a `MongoClient` instance with Server API version 1, and sends a ping command to verify the connection. Requires MongoDB username and password variables (`mongo_username`, `mongo_password`) to be set beforehand.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_with_mongodb_and_llama_index.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmongo_username = \"\"  # Replace with your mongo username\nmongo_password = \"\"  # Replace with your mongo password\n\nfrom pymongo.mongo_client import MongoClient\nfrom pymongo.server_api import ServerApi\n\nuri = f\"mongodb+srv://{mongo_username}:{mongo_password}@cluster0.lq406.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n\n# Create a new client and connect to the server\nclient = MongoClient(uri, server_api=ServerApi(\"1\"))\n\n# Send a ping to confirm a successful connection\ntry:\n    client.admin.command(\"ping\")\n    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\nexcept Exception as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Generating OpenAI chat traces (Python)\nDESCRIPTION: Initializes an OpenAI client and defines a function to generate a joke using the chat completions API. It then calls this function multiple times to generate and print jokes, with each API call being automatically traced by the installed instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/feedback-and-annotations/evaluating-phoenix-traces.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n# Initialize OpenAI client\nclient = OpenAI()\n\n\n# Function to generate a joke\ndef generate_joke():\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates jokes.\"},\n            {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n        ],\n    )\n    joke = response.choices[0].message.content\n    return joke\n\n\n# Generate 5 different jokes\njokes = []\nfor _ in range(5):\n    joke = generate_joke()\n    jokes.append(joke)\n    print(f\"Joke {len(jokes)}:\\n{joke}\\n\")\n\nprint(f\"Generated {len(jokes)} jokes and tracked them in Phoenix.\")\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Evaluation and Async Operations in Python\nDESCRIPTION: Imports necessary libraries: `nest_asyncio` for running async code in environments like Jupyter notebooks, `pandas` for data manipulation, and specific components from `phoenix.evals` including constants for tool calling prompts (`TOOL_CALLING_PROMPT_RAILS_MAP`, `TOOL_CALLING_PROMPT_TEMPLATE`), the `OpenAIModel` class for interacting with OpenAI, and the `llm_classify` function for evaluations. It also applies `nest_asyncio` to allow nested event loops.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\nimport pandas as pd\n\nfrom phoenix.evals import (\n    TOOL_CALLING_PROMPT_RAILS_MAP,\n    TOOL_CALLING_PROMPT_TEMPLATE,\n    OpenAIModel,\n    llm_classify,\n)\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Converting Storage Context to DataFrame for Embedding Analysis\nDESCRIPTION: Converts the RAG storage context to a pandas DataFrame containing document IDs, texts, and embeddings for knowledge base visualization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\n\ndef storage_context_to_dataframe(storage_context: StorageContext) -> pd.DataFrame:\n    \"\"\"Converts the storage context to a pandas dataframe.\n\n    Args:\n        storage_context (StorageContext): Storage context containing the index\n        data.\n\n    Returns:\n        pd.DataFrame: The dataframe containing the index data.\n    \"\"\"\n    document_ids = []\n    document_texts = []\n    document_embeddings = []\n    docstore = storage_context.docstore\n    vector_store = storage_context.vector_store\n    for node_id, node in docstore.docs.items():\n        document_ids.append(node.hash)  # use node hash as the document ID\n        document_texts.append(node.text)\n        document_embeddings.append(np.array(vector_store.get(node_id)))\n    return pd.DataFrame(\n        {\n            \"document_id\": document_ids,\n            \"text\": document_texts,\n            \"text_vector\": document_embeddings,\n        }\n    )\n\n\ndatabase_df = storage_context_to_dataframe(storage_context)\ndatabase_df = database_df.drop_duplicates(subset=[\"text\"])\ndatabase_df.head()\n```\n\n----------------------------------------\n\nTITLE: Uninstrument MistralAIInstrumentor\nDESCRIPTION: This calls a function to uninstrument the MistralAIInstrumentor, likely to prevent unintended trace collection after the `llm_generate` call. It's part of the instrumentation process within Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nMistralAIInstrumentor().uninstrument()  # No longer needed\n```\n\n----------------------------------------\n\nTITLE: Sampling Hybridial Dataset from HuggingFace with Context Filtering in Python\nDESCRIPTION: This code demonstrates loading the 'hybridial' split of the nvidia/ChatRAG-Bench dataset from HuggingFace using the Python datasets library. It samples 10 entries from the test set where both the number of messages and context (ctxs) length are at least half the maximum observed values, then writes them as a compressed CSV. The snippet assumes the datasets package is installed and requires the columns 'messages' and 'ctxs' to be present; expected output includes printed maximum lengths and a CSV file with samples.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/ChatRAG-Bench.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nname = \"hybridial\"\ndf = load_dataset(\"nvidia/ChatRAG-Bench\", name)[\"test\"].to_pandas()\nprint(df.messages.apply(len).max())\nprint(df.ctxs.apply(len).max())\ndf.loc[\n    (df.messages.apply(len) >= df.messages.apply(len).max() // 2)\n    & (df.ctxs.apply(len) >= df.ctxs.apply(len).max() // 2)\n].sample(10, random_state=42).to_csv(f\"{name}_samples.csv.gz\", index=False)\ndf.sample(10)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Retrieves the OpenAI API key from an environment variable or prompts the user to enter it securely using getpass. The key is then set as an environment variable, which is required for using OpenAI's models for embeddings and the LLM.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/agentic_rag_tracing.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix and Dependencies\nDESCRIPTION: This code snippet installs the necessary Python packages for the project, including Arize Phoenix, LlamaIndex, and related dependencies for running the evaluations.  It uses `pip` for package management with options for quiet output and upgrades.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uqqq \"arize-phoenix[llama-index]>=4.20\" nest_asyncio arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Creating Pandas DataFrame Dataset\nDESCRIPTION: This Python code uses the pandas library to create a simple DataFrame. This DataFrame represents the dataset for the experiment, where each row is an example and the 'question' column serves as the input.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/run-experiments.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {\n        \"question\": [\n            \"Which team won the most games?\",\n            \"Which team won the most games in 2015?\",\n            \"Who led the league in 3 point shots?\",\n        ]\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix with pip in Shell\nDESCRIPTION: This shell command installs the 'arize-phoenix' Python package from PyPI using pip, enabling access to the full Phoenix platform with all features for observability, tracing, and evaluation of LLM and AI projects. It requires Python and pip already installed on the system and is typically executed from the command line or terminal. No arguments are required; this command will download and set up the latest compatible version. On success, all platform modules become available for import in Python environments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Creating Pandas DataFrame from Questions, SQL Queries, and Responses in Python\nDESCRIPTION: Constructs a pandas DataFrame from three lists: original questions, generated SQL queries, and agent responses. Displays the first three rows of the DataFrame. Requires pandas imported as pd with the lists questions, all_sql_queries, and all_ans populated from prior operations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"question\": questions, \"query_gen\": all_sql_queries, \"response\": all_ans})\ndf[:3]\n```\n\n----------------------------------------\n\nTITLE: Agent Self-Reflection Prompt Template (Plaintext)\nDESCRIPTION: This prompt template guides an AI agent, acting as an expert in a specified `{topic}`, to evaluate its `{solution}` to a `{user_query}`. It requires the agent to assess correctness, list error keywords, and provide corrective instructions, considering available `{tool_definitions}` and the `{current_state}`. The output should be concise but comprehensive.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/agent-reflection.md#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nYou are an expert in {topic}. I will give you a user query. Your task is to reflect on your provided solution and whether it has solved the problem.\nFirst, explain whether you believe the solution is correct or incorrect.\nSecond, list the keywords that describe the type of your errors from most general to most specific.\nThird, create a list of detailed instructions to help you correctly solve this problem in the future if it is incorrect.\n\nBe concise in your response; however, capture all of the essential information.\n\nHere is the data:\n    [BEGIN DATA]\n    ************\n    [User Query]: {user_query}\n    ************\n    [Tools]: {tool_definitions}\n    ************\n    [State]: {current_state}\n    ************\n    [Provided Solution]: {solution}\n    [END DATA]\n```\n\n----------------------------------------\n\nTITLE: Creating Phoenix Prompt Programmatically (Deno JavaScript)\nDESCRIPTION: Imports the Phoenix prompts helper and creates a new prompt definition named 'question-asker' in the Phoenix instance. The prompt uses the MUSTACHE template format, specifies OpenAI as the model provider, and defines a user message template with a '{{question}}' variable. The created prompt object is then displayed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_openai_tutorial.ipynb#_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport * as Prompts from \"npm:@arizeai/phoenix-client/prompts\"\nimport { promptVersion } from \"npm:@arizeai/phoenix-client/prompts\"\n\nconst questionAskerPrompt = await Prompts.createPrompt({\n name: \"question-asker\",\n description: \"Asks a question\",\n version: promptVersion({\n  description: \"Initial version\",\n  templateFormat: \"MUSTACHE\",\n  modelProvider: \"OPENAI\",\n  modelName: \"gpt-3.5-turbo\",\n  template: [\n   {\n     role: \"user\",\n     content: \"{{question}}\"\n   },\n  ],\n })\n})\n\nawait Deno.jupyter.md`\n  ### question-asker prompt\n\n  ```json\n  ${JSON.stringify(questionAskerPrompt, null, 2)}\n  ```\n  `\n```\n\n----------------------------------------\n\nTITLE: Initializing Required Python Modules and Imports (Python)\nDESCRIPTION: Imports essential Python libraries and modules to support asynchronous execution in Jupyter environments and sets up dependencies for the RAG pipeline. Must be run early, as it configures Python environments and imports pandas, Phoenix, and several components from LlamaIndex for handling document ingestion, vector indexing, and LLM integration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nimport os\nfrom getpass import getpass\n\nimport pandas as pd\nimport phoenix as px\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex, set_global_handler\nfrom llama_index.llms import OpenAI\nfrom llama_index.node_parser import SimpleNodeParser\n```\n\n----------------------------------------\n\nTITLE: Implementing and Running Summarization with Anthropic Claude in Python\nDESCRIPTION: Defines a new asynchronous task function for summarizing articles using Anthropic's Claude model, then runs an experiment with the same prompt template to compare performance with OpenAI's model.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/summarization.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom anthropic import AsyncAnthropic\n\nclient = AsyncAnthropic()\n\n\nasync def summarize_article_anthropic(example: Example, prompt_template: str, model: str) -> str:\n    formatted_prompt_template = prompt_template.format(article=example.input[\"article\"])\n    message = await client.messages.create(\n        model=model,\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": formatted_prompt_template}],\n    )\n    return message.content[0].text\n\n\nclaude_35_sonnet = \"claude-3-5-sonnet-20240620\"\ntask = partial(summarize_article_anthropic, prompt_template=template, model=claude_35_sonnet)\n\nexperiment_results = run_experiment(\n    dataset,\n    task,\n    experiment_name=\"anthropic-few-shot\",\n    experiment_description=\"anthropic\",\n    experiment_metadata={\"vendor\": \"anthropic\", \"model\": claude_35_sonnet},\n    evaluators=EVALUATORS,\n)\n```\n\n----------------------------------------\n\nTITLE: SQL Eval Prompt Template\nDESCRIPTION: This is a prompt template used for evaluating the correctness of generated SQL queries. It provides context to the LLM, including the original instruction, the generated SQL query, and the response from executing the query. The LLM is expected to output 'correct' or 'incorrect' based on its analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/sql-generation-eval.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nSQL Evaluation Prompt:\n-----------------------\nYou are tasked with determining if the SQL generated appropiately answers a given \ninstruction taking into account its generated query and response.\n\nData:\n-----\n- [Instruction]: {question}\n  This section contains the specific task or problem that the sql query is intended \n  to solve.\n\n- [Reference Query]: {query_gen}\n  This is the sql query submitted for evaluation. Analyze it in the context of the \n  provided instruction.\n\n- [Provided Response]: {response}\n  This is the response and/or conclusions made after running the sql query through \n  the database\n\nEvaluation:\n-----------\nYour response should be a single word: either \"correct\" or \"incorrect\".\nYou must assume that the db exists and that columns are appropiately named.\nYou must take into account the response as additional information to determine the \ncorrectness.\n```\n\n----------------------------------------\n\nTITLE: Defining Extraction Function Python\nDESCRIPTION: Defines a function `extract_raw_travel_request_attributes_string` that takes a travel request, the tool schema, system message, Anthropic client, and model as input. It calls the Anthropic API with the provided parameters and forces the LLM to call the function with tool_choice to extract the attributes. It returns the raw output string from the API call.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/anthropic_tracing_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef extract_raw_travel_request_attributes_string(\n    travel_request: str,\n    tool_schema: Dict[str, Any],\n    system_message: str,\n    client: client,\n    model: str = \"claude-3-5-sonnet-20240620\",\n) -> str:\n    response = client.messages.create(\n        model=model,\n        max_tokens=1024,\n        messages=[\n            {\"role\": \"user\", \"content\": travel_request},\n        ],\n        system=system_message,\n        tools=[tool_schema],\n        # By default, the LLM will choose whether or not to call a function given the conversation context.\n        # The line below forces the LLM to call the function so that the output conforms to the schema.\n        tool_choice={\"type\": \"tool\", \"name\": \"record_travel_request_attributes\"},\n    )\n    return response.content[0].input\n```\n\n----------------------------------------\n\nTITLE: Defining Tabular Feature Columns - Python\nDESCRIPTION: Assigns a list of feature column names relevant for fraud detection to a Python variable. This array is later used to guide embedding generation and schema definition. It is crucial for ensuring consistent feature selection across all data processing steps. No external dependencies required.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/credit_card_fraud_tutorial.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfeature_column_names = [\n    \"fico_score\",\n    \"loan_amount\",\n    \"term\",\n    \"interest_rate\",\n    \"installment\",\n    \"grade\",\n    \"home_ownership\",\n    \"annual_income\",\n    \"verification_status\",\n    \"pymnt_plan\",\n    \"addr_state\",\n    \"dti\",\n    \"delinq_2yrs\",\n    \"inq_last_6mths\",\n    \"mths_since_last_delinq\",\n    \"mths_since_last_record\",\n    \"open_acc\",\n    \"pub_rec\",\n    \"revol_bal\",\n    \"revol_util\",\n    \"state\",\n    \"merchant_ID\",\n    \"merchant_risk_score\",\n]\n```\n\n----------------------------------------\n\nTITLE: Converting Phoenix Prompt to Anthropic Format Using toSDK in JavaScript\nDESCRIPTION: Adapts the Phoenix sourced prompt into a format compatible with the Anthropic SDK, also injecting the 'query' parameter. Utilizes the Prompts.toSDK helper, specifying 'anthropic' as the target, and outputs the resulting object in prettified JSON via Jupyter markdown. Ensures proper structure and tool handling for Anthropic's expectations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_cross_sdk_tutorial.ipynb#_snippet_9\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst anthropicPrompt = Prompts.toSDK({ \n  sdk: \"anthropic\", \n  prompt: questionSearcherPrompt, \n  variables: { query: \"When does the next Arize Phoenix release come out?\" } \n})\n\nawait Deno.jupyter.md`\n  ### Anthropic Prompt\n\n  \\`\\`\\`json\n  ${JSON.stringify(anthropicPrompt, null, 2)}\n  \\`\\`\\`\n`\n```\n\n----------------------------------------\n\nTITLE: Downloading Documentation Data (Python)\nDESCRIPTION: Downloads pre-embedded Arize documentation data from a specified Google Cloud Storage URL. The data, which is in JSON format, is fetched using `urllib.request`, decoded, parsed, and stored in the `rows` variable. This data will be used to populate the Milvus vector store.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/milvus_llamaindex_search_and_retrieval_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nurl = \"http://storage.googleapis.com/arize-assets/xander/milvus-workshop/milvus_dataset.json\"\n\nwith urllib.request.urlopen(url) as response:\n    buffer = response.read()\n    data = json.loads(buffer.decode(\"utf-8\"))\n    rows = data[\"rows\"]\n```\n\n----------------------------------------\n\nTITLE: RAG Task with Reranker Function Definition - Python\nDESCRIPTION: Defines a callable for performing retrieval-augmented generation: sets up the chat engine using the index and reranker, takes user input, and returns the generated response as a string. Input: dictionary with 'input_messages' (list of messages). Output: assistant's reply (string). Key parameter: input['input_messages'] expected to be a non-empty list of message dicts.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/run_experiments_with_llama_index.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef rag_with_reranker(input) -> str:\n    chat_engine = index.as_chat_engine(similarity_top_k=10, node_postprocessors=[reranker])\n    response = chat_engine.chat(input[\"input_messages\"][-1][\"content\"])\n    return str(response)\n```\n\n----------------------------------------\n\nTITLE: Defining get_product_details Tool\nDESCRIPTION: This code defines a Python function named `get_product_details` that acts as a tool.  It takes a `product_name` string as input and returns product details. The tool functions as a lookup and provides a hardcoded response for a few products.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/google_agent_engine_tracing_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_product_details(product_name: str):\n    \"\"\"Gathers basic details about a product.\"\"\"\n    details = {\n        \"smartphone\": \"A cutting-edge smartphone with advanced camera features and lightning-fast processing.\",\n        \"coffee\": \"A rich, aromatic blend of ethically sourced coffee beans.\",\n        \"shoes\": \"High-performance running shoes designed for comfort, support, and speed.\",\n        \"headphones\": \"Wireless headphones with advanced noise cancellation technology for immersive audio.\",\n        \"speaker\": \"A voice-controlled smart speaker that plays music, sets alarms, and controls smart home devices.\",\n    }\n    return details.get(product_name, \"Product details not found.\")\n```\n\n----------------------------------------\n\nTITLE: Downloading and Sampling JSON Dataset in Python Using LangChain Benchmarks\nDESCRIPTION: Downloads a public 'Email Extraction' dataset by referencing its dataset ID in a registry and writes it temporarily to disk. Reads the JSON data into a pandas DataFrame with focus on 'inputs' and 'outputs' columns. Samples 10 random examples with a fixed seed for reproducibility. This sampled DataFrame is ready for experimental usage or upload to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/langchain_email_extraction.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndataset_name = \"Email Extraction\"\n\nwith tempfile.NamedTemporaryFile(suffix=\".json\") as f:\n    download_public_dataset(registry[dataset_name].dataset_id, path=f.name)\n    df = pd.read_json(f.name)[[\"inputs\", \"outputs\"]]\ndf = df.sample(10, random_state=42)\ndf\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Phoenix Cloud in Bash\nDESCRIPTION: These commands set the required environment variables to connect a client or application to a Phoenix Cloud managed instance. The 'PHOENIX_CLIENT_HEADERS' expects the API key as a value for authentication, while 'PHOENIX_COLLECTOR_ENDPOINT' must be set to the Phoenix Cloud API URL. Ensure you have the correct API key credentials before running. Expected input is your personalized API key, and the primary output is making your environment ready for Phoenix Cloud connections.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/environments.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport PHOENIX_CLIENT_HEADERS = \"api_key=ENTER YOUR API KEY\"\nexport PHOENIX_COLLECTOR_ENDPOINT = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Initializing SQLAlchemy Async Engine for SQLite in Python\nDESCRIPTION: This snippet creates an asynchronous SQLAlchemy engine connecting to a local SQLite database located at the specified filesystem path. It sets up the engine for use in subsequent async database operations with the Phoenix project's schema.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/testing/experiment_runs_filters.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nengine = create_engine(\"sqlite:////Users/xandersong/.phoenix/phoenix.db\")\n```\n\n----------------------------------------\n\nTITLE: Setting Local Phoenix Collector Endpoint (Python)\nDESCRIPTION: Configures the Python process to send traces to a locally running Phoenix collector by setting the 'PHOENIX_COLLECTOR_ENDPOINT' environment variable. Requires the local instance to be running on port 6006. Should be added before starting instrumented tracing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/groq.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Importing Traces into Existing Phoenix - Python\nDESCRIPTION: This snippet demonstrates how to load traces into an existing Phoenix instance using the `phoenix` library. It uses `px.Client().log_traces()` to send trace data from a `TraceDataset` to a running Phoenix instance. The `TraceDataset` is created from a dataframe, 'df'. The second example loads the trace dataset from a local file using 'f7733fda-6ad6-4427-a803-55ad2182b662' as the trace id and the local directory, '/my_saved_traces/'\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/importing-existing-traces.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport phoenix as px\n\n# Re-launch the app using trace data\npx.launch_app(trace=px.TraceDataset(df))\n\n# Load traces into an existing Phoenix instance\npx.Client().log_traces(trace_dataset=px.TraceDataset(df))\n\n# Load traces into an existing Phoenix instance from a local file\npx.launch_app(trace=px.TraceDataset.load('f7733fda-6ad6-4427-a803-55ad2182b662', directory=\"/my_saved_traces/\"))\n```\n\n----------------------------------------\n\nTITLE: Inspect Function Selection Evaluation Results\nDESCRIPTION: This snippet outputs the DataFrame containing results of the function selection correctness evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nfunction_selection_eval_df\n```\n\n----------------------------------------\n\nTITLE: Building the Phoenix Web App\nDESCRIPTION: Commands to build the web application by installing dependencies and running the build process. This is required for the full Phoenix experience including the web UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npnpm install\npnpm run build\n```\n\n----------------------------------------\n\nTITLE: Defining LLM-based Evaluation Prompt Template\nDESCRIPTION: This snippet specifies a prompt template used for evaluating whether the tools called during the interaction are appropriate for answering customer questions. The template guides an LLM to assess and classify the relevance of tool calls, supporting automated evaluation of tool effectiveness in the prompting process.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/react_prompting.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nTOOL_CALLING_PROMPT_TEMPLATE = \"\"\"\nYou are an evaluation assistant evaluating questions and tool calls to\n\ndetermine whether the tool called would reasonably help answer the question.\nThe tool calls have been generated by a separate agent, chosen from the list of\ntools provided below. Your job is to decide whether that agent's response was relevant to solving the customer's question.\n\n    [BEGIN DATA]\n    ************\n    [Question]: {question}\n    ************\n    [Tool Called]: {tool_calls}\n    [END DATA]\n\nYour response must be one of the following:\n1. **\"correct\"** â€“ The chosen tool(s) would sufficiently answer the question.\n2. **\"mostly_correct\"** â€“ The tool(s) are helpful, but a better selection could have been made (at most 1 missing or unnecessary tool).\n3. **\"incorrect\"** â€“ The tool(s) would not meaningfully help answer the question.\n\nExplain why you made your choice.\n\n    [Tool Definitions]:\n    product_comparison: Compare features of two products.\n    product_details: Get detailed features on one product.\n    apply_discount_code: Applies a discount code to an order.\n    customer_support: Get contact information for customer support regarding an issue.\n    track_package: Track the status of a package based on the tracking number.\n\"\"\"\n\n```\n\n----------------------------------------\n\nTITLE: Adding Nodes to the Workflow\nDESCRIPTION: Integrates the query generation, query verification, and execution functions into the workflow by adding respective nodes, establishing the steps for query creation, validation, and execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nworkflow.add_node(\"query_gen\", query_gen_node)\n\n# Add a node for the model to check the query before executing it\nworkflow.add_node(\"correct_query\", model_check_query)\n\n# Add node for executing the query\nworkflow.add_node(\"execute_query\", create_tool_node_with_fallback([db_query_tool]))\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix API Key and Collector Endpoint in Python Environment\nDESCRIPTION: This code sets environment variables in Python to configure the Phoenix client with the necessary API key and collector endpoint URL. It enables authenticated trace collection by assigning the Phoenix API key to PHOENIX_CLIENT_HEADERS and setting the PHOENIX_COLLECTOR_ENDPOINT to Phoenix's cloud URL. These environment variables are prerequisites for forwarding tracing data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/prompt-flow.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Sampling DoQA Movies Dataset from HuggingFace with Message Length Filtering in Python\nDESCRIPTION: Loads the 'doqa_movies' test split from the nvidia/ChatRAG-Bench dataset, identifies entries with message count at least half the maximum, and samples 100 such entries for export in gzipped CSV format. 'datasets' and 'pandas' libraries are required; outputs include the maximum message length and the sampled data file.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/ChatRAG-Bench.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nname = \"doqa_movies\"\ndf = load_dataset(\"nvidia/ChatRAG-Bench\", name)[\"test\"].to_pandas()\nprint(df.messages.apply(len).max())\ndf.loc[df.messages.apply(len) >= df.messages.apply(len).max() // 2].sample(\n    100, random_state=42\n).to_csv(f\"{name}_samples.csv.gz\", index=False)\ndf.sample(10)\n```\n\n----------------------------------------\n\nTITLE: Setting Up DuckDB and Loading Data\nDESCRIPTION: This Python code initializes an in-memory DuckDB database connection and loads a dataset ('suzyanil/nba-data') into a table named 'nba'. This setup is used later for executing SQL queries generated by the LLM.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/run-experiments.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\nfrom datasets import load_dataset\n\ndata = load_dataset(\"suzyanil/nba-data\")[\"train\"]\nconn = duckdb.connect(database=\":memory:\", read_only=False)\nconn.register(\"nba\", data.to_pandas())\n```\n\n----------------------------------------\n\nTITLE: Launch Phoenix App\nDESCRIPTION: Defines the database and query schemas and launches a Phoenix application to visualize and analyze the data. It uses the dataframes prepared previously to define the datasets and then starts the Phoenix UI. Dependencies include the prepared dataframes and the `phoenix` library, and `px.Inferences`.  The output is a Phoenix UI opened in a browser, allowing the user to visualize and explore the prepared data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/milvus_llamaindex_search_and_retrieval_tutorial.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndatabase_schema = px.Schema(\n    prediction_id_column_name=\"document_id\",\n    prompt_column_names=px.EmbeddingColumnNames(\n        vector_column_name=\"text_vector\",\n        raw_data_column_name=\"text\",\n    ),\n)\ndatabase_ds = px.Inferences(\n    dataframe=database_df,\n    schema=database_schema,\n    name=\"database\",\n)\n\nquery_ds = px.Inferences.from_open_inference(query_df)\n\n(session := px.launch_app(primary=query_ds, corpus=database_ds)).view()\n```\n\n----------------------------------------\n\nTITLE: Construct DataFrame from Indexed Documents\nDESCRIPTION: This code snippet creates a Pandas DataFrame containing the text of document chunks that have been indexed, using a list comprehension to iterate through the nodes. This is used as the foundation for generating questions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndocument_chunks_df = pd.DataFrame({\"text\": [node.get_text() for node in nodes]})\ndocument_chunks_df.head()\n```\n\n----------------------------------------\n\nTITLE: Creating Few-Shot Learning Examples for Summarization Template in Python\nDESCRIPTION: Prepares a few-shot prompt template by including example articles and their summaries from a training dataset. This helps the model understand the expected format and style of summaries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/summarization.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# examples to include (not included in the uploaded dataset)\ntrain_df = (\n    hf_ds[\"train\"]\n    .to_pandas()\n    .sample(n=5, random_state=42)\n    .head()\n    .rename(columns={\"highlights\": \"summary\"})\n)\n\nexample_template = \"\"\"\nARTICLE\n=======\n{article}\n\nSUMMARY\n=======\n{summary}\n\"\"\"\n\nexamples = \"\\n\".join(\n    [\n        example_template.format(article=row[\"article\"], summary=row[\"summary\"])\n        for _, row in train_df.iterrows()\n    ]\n)\n\ntemplate = \"\"\"\nSummarize the article in two to four sentences. Be concise and include only the most important information, as in the examples below.\n\nEXAMPLES\n========\n\n{examples}\n\n\nNow summarize the following article.\n\nARTICLE\n=======\n{article}\n\nSUMMARY\n=======\n\"\"\"\n\ntemplate = template.format(\n    examples=examples,\n    article=\"{article}\",\n)\nprint(template)\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAI Tool for Function Calling in JSON\nDESCRIPTION: Describes a 'search' tool in OpenAI's function calling format to allow an LLM to request an internet query operation. The tool definition is intended for OpenAI APIs, structured as a JSON object with name, description, and required parameters. Anthropic does not use this format natively.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_cross_sdk_tutorial.ipynb#_snippet_6\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"search\",\n    \"description\": \"Query the internet for the answer to a question.\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"query\": {\n          \"type\": \"string\",\n          \"description\": \"Search term\"\n        }\n      },\n      \"required\": [\"query\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Phoenix Prompt to OpenAI Format Using toSDK in JavaScript\nDESCRIPTION: Uses the Prompts.toSDK helper to adapt the Phoenix prompt for OpenAI SDK compatibility, injecting the 'query' variable content. Outputs the converted prompt as formatted JSON using Deno Jupyter markdown. Requires '@arizeai/phoenix-client/prompts'. The 'toSDK' call ensures tool definitions and structure align with OpenAI's expectations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_cross_sdk_tutorial.ipynb#_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst openaiPrompt = Prompts.toSDK({ \n  sdk: \"openai\", \n  prompt: questionSearcherPrompt, \n  variables: { query: \"When does the next Arize AI Phoenix release come out?\" } \n})\n\nawait Deno.jupyter.md`\n  ### OpenAI Prompt\n\n  \\`\\`\\`json\n  ${JSON.stringify(openaiPrompt, null, 2)}\n  \\`\\`\\`\n`\n```\n\n----------------------------------------\n\nTITLE: Executing Generated SQL Queries and Building Dataset in Python\nDESCRIPTION: Iterates through the list of `generated_questions` obtained from the LLM. For each item, it attempts to execute the SQL query using an `execute_query` function (presumably interacting with a DuckDB database). If the query succeeds, it appends a structured dictionary containing the input question, expected results (query result, no error, SQL query), and metadata to the `generated_dataset` list. Catches `duckdb.Error` exceptions for failed queries, prints an error message, and skips adding the failed item.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ngenerated_dataset = []\nfor q in generated_questions:\n    try:\n        result = execute_query(q[\"sql\"])\n        generated_dataset.append(\n            {\n                \"input\": q[\"question\"],\n                \"expected\": {\n                    \"results\": result,\n                    \"error\": None,\n                    \"query\": q[\"sql\"],\n                },\n                \"metadata\": {\n                    \"category\": \"Generated\",\n                },\n            }\n        )\n    except duckdb.Error as e:\n        print(f\"Query failed: {q['sql']}\", e)\n        print(\"Skipping...\")\n\ngenerated_dataset[0]\n```\n\n----------------------------------------\n\nTITLE: Specifying mypy-protobuf Dependency in Python\nDESCRIPTION: Declares a required dependency on the `mypy-protobuf` Python package, pinned to version 3.5.0. This library generates PEP 484 type hints for Protocol Buffer messages, enabling static type checking with MyPy for generated gRPC code.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/requirements/compile-protobuf.txt#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nmypy-protobuf==3.5.0\n```\n\n----------------------------------------\n\nTITLE: Loading and Sampling Jailbreak Classification Dataset - Python\nDESCRIPTION: Loads the 'jackhhao/jailbreak-classification' dataset via HuggingFace datasets, selects the training split, converts it to a Pandas dataframe, and samples 50 rows. The resulting dataframe is used as input for further experiments. Requires datasets and pandas libraries. Key parameter: sample size (hardcoded to 50). Outputs a sampled DataFrame for preview and workflow continuity.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nds = load_dataset(\"jackhhao/jailbreak-classification\")[\"train\"]\nds = ds.to_pandas().sample(50)\nds.head()\n```\n\n----------------------------------------\n\nTITLE: Logging Evaluation Results Back to Phoenix Using Python Client\nDESCRIPTION: Logs the DataFrame results of hallucination, QA correctness, and relevance evaluations as annotated span and document evaluations in the Phoenix UI. This enables visualization of evaluation feedback directly associated with application trace spans.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_vertex_ai_tracing_tutorial.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npx.Client().log_evaluations(\n    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df),\n    DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df),\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Environment Variables for Authentication in Python\nDESCRIPTION: Configures environment variables to specify the Phoenix collector endpoint and securely obtain the Phoenix API key via user input. This setup is required for authenticating API calls to the Phoenix Cloud service or self-hosted instances.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/amazon_bedrock_agents_tracing_and_evals.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\nif not os.environ.get(\"PHOENIX_CLIENT_HEADERS\"):\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = \"api_key=\" + getpass(\"Enter your Phoenix API key: \")\n```\n\n----------------------------------------\n\nTITLE: Evaluating GPT-4 Turbo Q&A Classification Results\nDESCRIPTION: Performs the final evaluation using GPT-4 Turbo with the same metrics and visualization to complete the model comparison across all three OpenAI models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntrue_labels = df_sample[\"answer_true\"].map(QA_PROMPT_RAILS_MAP).tolist()\nclasses = list(QA_PROMPT_RAILS_MAP.values())\n\nprint(classification_report(true_labels, Q_and_A_classifications, labels=classes))\nconfusion_matrix = ConfusionMatrix(\n    actual_vector=true_labels, predict_vector=Q_and_A_classifications, classes=classes\n)\nconfusion_matrix.plot(\n    cmap=plt.colormaps[\"Blues\"],\n    number_label=True,\n    normalized=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Web Frontend Dependencies with PNPM\nDESCRIPTION: Commands to install Node.js via NVM and PNPM package manager globally for managing web frontend dependencies. These are required for building the web application component of Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# install nvm\n# https://github.com/nvm-sh/nvm\n# install node via nvm, our .nvmrc file will automatically instruct nvm to install \n# the version specified in the file\nnvm install\n# set it as default (optional)\nnvm alias default <version-that-was-installed>\n# install pnpm globally for v22\nnpm i -g pnpm@9.15.5\n```\n\n----------------------------------------\n\nTITLE: Calculating False Negative Rate using Scikit-learn in Python\nDESCRIPTION: This code calculates the false negative rate from the exported data. It first filters the DataFrame to remove rows with uncertain ground truth labels, then computes the recall score using `recall_score` from `sklearn.metrics`. The false negative rate is calculated as 1 minus the recall.  The script assumes the `export_df` DataFrame contains `actual_label` and `predicted_label` columns. This snippet demonstrates a common analysis procedure to analyze the performance of a model.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/credit_card_fraud_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nexport_df = export_df[\n    export_df.actual_label != \"uncertain\"\n]  # remove rows with unknown ground-truth\nrecall = recall_score(\n    y_true=export_df.actual_label == \"fraud\", y_pred=export_df.predicted_label == \"fraud\"\n)\nfalse_negative_rate = 1 - recall\nfalse_negative_rate\n```\n\n----------------------------------------\n\nTITLE: Configuring API Keys and Phoenix Endpoint in Python\nDESCRIPTION: This Python snippet sets environment variables for the Phoenix collector endpoint and API key, as well as the OpenAI API key. It uses `getpass` to securely prompt the user for keys if they are not already set in the environment. `nest_asyncio.apply()` is used to enable asyncio within a notebook environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/ragas_agents_cookbook_phoenix.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\nif not os.environ.get(\"PHOENIX_CLIENT_HEADERS\"):\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = \"api_key=\" + getpass(\"Enter your Phoenix API key: \")\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Loading Sample E-commerce Data into pandas DataFrames in Python\nDESCRIPTION: Loads three sets of pipe-delimited sample data stringsâ€”product items, policy FAQs, and customer inquiriesâ€”into separate pandas DataFrames for further analysis. Uses pandas' read_csv method with StringIO to parse the multi-line strings with '|' as delimiter. This prepares structured tabular data for downstream processing such as analysis or AI prompt generation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nitems_df = pd.read_csv(StringIO(my_items.strip()), delimiter=\"|\")\npolicy_df = pd.read_csv(StringIO(policy_data.strip()), delimiter=\"|\")\ncustomer_inputs_df = pd.read_csv(StringIO(customer_inputs.strip()), delimiter=\"|\")\n```\n\n----------------------------------------\n\nTITLE: Logging Span and Document Evaluations to Phoenix in Python\nDESCRIPTION: This snippet imports classes for logging document and span evaluations and submits the aggregated span-level and document-level bias evaluations to Phoenix for tracking. The `log_evaluations` method is called on a new Phoenix client instance, passing `SpanEvaluations` and `DocumentEvaluations` objects, each constructed with their respective DataFrames and an evaluation name. This step records the evaluation results back into the Phoenix system.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/llamaindex-workflows-research-agent/evaluate_traces.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import DocumentEvaluations, SpanEvaluations\n\npx.Client().log_evaluations(\n    SpanEvaluations(\n        dataframe=span_bias_classifications,\n        eval_name=\"Bias Detection\",\n    ),\n    DocumentEvaluations(\n        dataframe=bias_classifications,\n        eval_name=\"Relevance\",\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Generate Questions using llm_generate\nDESCRIPTION: This code leverages Phoenix's `llm_generate` function to generate questions using the specified template and a language model (MistralAIModel). It takes a dataframe as input, applies the template to each row, and uses the defined output parser. The `using_project` context manager is used to group traces under a project for analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nwith using_project(TESTSET_PROJECT):  # Collect traces under the project \"testset\"\n    questions_df = llm_generate(\n        dataframe=document_chunks_df,\n        template=generate_questions_template,\n        model=MistralAIModel(model=\"mistral-large-latest\", response_format={\"type\": \"json_object\"}),\n        output_parser=output_parser,\n        concurrency=20,\n    )\n```\n\n----------------------------------------\n\nTITLE: Building LangChain Qdrant Vector Store (bash)\nDESCRIPTION: Runs a Python script (build_vector_store.py) to create and persist a vector store, which indexes the Arize documentation. Requires all dependencies specified in requirements.txt to be installed beforehand. The resulting vector store is stored in the 'vector-store' directory and is essential for later RetrievalQA operations. Inputs and outputs are managed by the script itself and not prompted from the user.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/cron-evals/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython build_vector_store.py\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Phoenix (Python)\nDESCRIPTION: Loads a dataset (`databricks-dolly-15k`) from a Hugging Face URL using pandas, filters by category (\"creative_writing\"), samples a specified number of rows (`sample_size`), and uploads the resulting dataframe to the Phoenix client. The dataset is named uniquely using a timestamp. This creates the data source for the experiment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsample_size = 7\ncategory = \"creative_writing\"\nurl = \"hf://datasets/databricks/databricks-dolly-15k/databricks-dolly-15k.jsonl\"\ndf = pd.read_json(url, lines=True)\ndf = df.loc[df.category == category, [\"instruction\", \"response\"]]\ndf = df.sample(sample_size, random_state=42)\ndataset = px.Client().upload_dataset(\n    dataset_name=f\"{category}_{time_ns()}\",\n    dataframe=df,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Endpoint via Environment Variables in Python (Deprecated)\nDESCRIPTION: This code previously configured the Phoenix tracing export endpoint by instantiating HttpExporter and passing it into OpenInferenceTracer. Dependencies: phoenix.trace.exporter and phoenix.trace.langchain. Environment variables were not required. Now deprecated after v2.x; required parameters include exporter port and tracer object. Output: traces sent to specified exporter port.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/MIGRATION.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.trace.exporter import HttpExporter  # no longer necessary\nfrom phoenix.trace.langchain import LangChainInstrumentor, OpenInferenceTracer\n\ntracer = OpenInferenceTracer(exporter=HttpExporter(port=12345))  # no longer supported\nLangChainInstrumentor(tracer).instrument()\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Prompts using Python Decorator\nDESCRIPTION: Demonstrates using the `@using_prompt_template` decorator from `openinference.instrumentation` on a function. This applies the same prompt template instrumentation (template string, version, variables) to all OpenTelemetry spans generated by calls made within the decorated function, attaching them as `llm.prompt_template.*` attributes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/instrumenting-prompt-templates-and-prompt-variables.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@using_prompt_template(\n    template=prompt_template,\n    variables=prompt_template_variables,\n    version=\"v1.0\",\n)\ndef call_fn(*args, **kwargs):\n    # Calls within this function will generate spans with the attributes:\n    # \"llm.prompt_template.template\" = \"Please describe the weather forecast for {city} on {date}\"\n    # \"llm.prompt_template.version\" = \"v1.0\"\n    # \"llm.prompt_template.variables\" = \"{\\\"city\\\": \\\"Johannesburg\\\", \\\"date\\\": \\\"July 11\\\"}\" # JSON serialized\n    ...\n```\n\n----------------------------------------\n\nTITLE: Evaluating Experiment Results Using an LLM-Based Judging System in Python\nDESCRIPTION: This snippet sets up LLM-based evaluation of experiment results by defining a LLMCriteriaEvaluator configured to check for SQL correctness using OpenAI. It then uses evaluate_experiment to compute LLM-based scores. Dependencies: phoenix.evals.models.OpenAIModel, phoenix.experiments, LLMCriteriaEvaluator. Inputs: experiment object. Outputs: LLM-based evaluation results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals.models import OpenAIModel\nfrom phoenix.experiments import evaluate_experiment\nfrom phoenix.experiments.evaluators.llm_evaluators import LLMCriteriaEvaluator\n\nllm_evaluator = LLMCriteriaEvaluator(\n    name=\"is_sql\",\n    criteria=\"is_sql\",\n    description=\"the output is a valid SQL query and that it executes without errors\",\n    model=OpenAIModel(),\n)\n\nevaluate_experiment(experiment, evaluators=[llm_evaluator])\n```\n\n----------------------------------------\n\nTITLE: Creating Initial Customer Support Prompt with PhoenixClient\nDESCRIPTION: This snippet defines and creates a structured customer support prompt using Phoenix's API, setting up model parameters and message templates. It prepares the initial prompt configuration, including system and user messages, and stores it in Phoenix for further iteration and tracking.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/react_prompting.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nparams = CompletionCreateParamsBase(\n    model=\"gpt-4\",\n    temperature=0.5,\n    tools=tools,\n    tool_choice=\"auto\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"You are a helpful customer service agent.\n            Your task is to determine the best tools to use to answer a customer's question.\n            Output the tools and pick 3 tools at maximum.\n            \"\"\",\n        },\n        {\"role\": \"user\", \"content\": \"{{questions}}\"},\n    ],\n)\n\nprompt_identifier = \"customer-support\"\n\nprompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Customer Support\",\n    version=PromptVersion.from_openai(params),\n)\n\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key (Python)\nDESCRIPTION: Sets up the OpenAI API key required for the `OpenAIModel`. It first tries to retrieve the key from the `OPENAI_API_KEY` environment variable. If not found, it interactively prompts the user to enter the key using `getpass` and then sets the environment variable for subsequent use.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Preparing Sampled Dataset with Factual and Hallucinated Labels in Python\nDESCRIPTION: Creates a sample evaluation dataset consisting of pairs of factual and hallucinated answers based on the original QA dataset. Concatenates relevant columns, adds 'true_label' annotations, samples a subset (size 10), and rearranges columns to prepare inputs and expected outputs for experiment upload.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nSAMPLE_SIZE = 10\n\nk = qa.iloc[:, :2]\ndf = pd.concat(\n    [\n        pd.concat([k, qa.iloc[:, 2].rename(\"answer\")], axis=1).assign(true_label=\"factual\"),\n        pd.concat([k, qa.iloc[:, 3].rename(\"answer\")], axis=1).assign(true_label=\"hallucinated\"),\n    ]\n)\ndf = df.sample(SAMPLE_SIZE, random_state=42).reset_index(drop=True).iloc[:, ::-1]\ndf\n```\n\n----------------------------------------\n\nTITLE: Printing the Default Human vs AI Prompt Template - Python\nDESCRIPTION: Prints out the template used for classifying AI answers versus human-labeled answers. Useful for inspection and debugging. Depends on 'HUMAN_VS_AI_PROMPT_TEMPLATE' from phoenix.evals.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(HUMAN_VS_AI_PROMPT_TEMPLATE)\n```\n\n----------------------------------------\n\nTITLE: Evaluating LLM with Improved Prompt - Python\nDESCRIPTION: This code defines a function, `llm_as_a_judge`, that uses an LLM to classify AI responses based on the combined prompt. It takes an input containing an AI response, constructs a pandas DataFrame, and calls `llm_classify` using the improved prompt and an OpenAI model. The function returns an integer score representing the evaluation result. Prerequisites include the pandas library, a configured OpenAI model and access to an LLM evaluation function (`llm_classify`).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef llm_as_a_judge(input):\n    response_classifications = llm_classify(\n        dataframe=pd.DataFrame([{\"AI_Response\": input[\"AI_Response\"]}]),\n        template=EMPATHY_EVALUATION_PROMPT_TEMPLATE_IMPROVED,\n        model=OpenAIModel(model=\"gpt-4\"),\n        rails=list(map(str, range(1, 11))),\n        provide_explanation=True,\n    )\n    score = response_classifications.iloc[0][\"label\"]\n    return int(score)\n\n\nexperiment = run_experiment(\n    dataset, task=llm_as_a_judge, evaluators=[evaluate_response], experiment_name=\"combined\"\n)\n```\n\n----------------------------------------\n\nTITLE: Preparing documents for summarization\nDESCRIPTION: This snippet samples the 'review/text' column from the filtered product DataFrame, converts it to a list, and then uses the `gather_documents_into_chunks` function to split the documents into chunks suitable for a GPT-4 model, considering its context window.  It limits the number of chunks to 3.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/long_context_summary_evals.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndocuments = product_df[\"review/text\"].sample(frac=1, random_state=0).to_list()\ngpt4_context_window_in_tokens = 8192\nchunks = gather_documents_into_chunks(\n    documents=documents,\n    max_tokens_per_chunk=(gpt4_context_window_in_tokens - 1000),  # add in a buffer\n)[:3]\nchunks\n```\n\n----------------------------------------\n\nTITLE: Create and Test a Task\nDESCRIPTION: This code defines a prompt template and instantiates a task using `functools.partial`. It then calls the task with a sample example to generate and print a summary, testing the functionality of the prompt template and the summarization function.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/summarization.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport textwrap\nfrom functools import partial\n\ntemplate = \"\"\"\nSummarize the article in two to four sentences:\n\nARTICLE\n=======\n{article}\n\nSUMMARY\n=======\n\"\"\"\ngpt_4o = \"gpt-4o-2024-05-13\"\ntask = partial(summarize_article_openai, prompt_template=template, model=gpt_4o)\ntest_example = dataset[0]\nprint(textwrap.fill(await task(test_example.input), width=100))\n```\n\n----------------------------------------\n\nTITLE: Viewing Evaluation Results\nDESCRIPTION: This code snippet shows how to view the evaluation results.  It gets the evaluation results by calling `experiment.get_evaluations()`, which allows for viewing the results of the evaluations, typically presented as a DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nexperiment.get_evaluations()\n```\n\n----------------------------------------\n\nTITLE: Displaying the Processed DataFrame\nDESCRIPTION: Outputs the `conversations_df` DataFrame. In a Jupyter Notebook or similar environment, this displays the first few and last few rows of the DataFrame, allowing inspection of the data after the vector string conversion.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconversations_df\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable (Bash)\nDESCRIPTION: Sets the `OPENAI_API_KEY` environment variable in the shell, which is required for authenticating with the OpenAI API when running agent code. Replace `[your_key_here]` with your actual key.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai-agents-sdk.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=[your_key_here]\n```\n\n----------------------------------------\n\nTITLE: Configure OpenAI API Key Python\nDESCRIPTION: Imports the 'os' module and the 'getpass' function to securely obtain the OpenAI API key. It checks if the 'OPENAI_API_KEY' environment variable is already set. If not, it prompts the user to enter the key interactively using 'getpass', ensuring the key is not displayed in the output.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Tracing Code Blocks with Context Managers\nDESCRIPTION: This snippet shows how to use the `tracer.start_as_current_span` context manager to trace specific code blocks. The input, output, and status must be set manually within the context.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry.trace import Status, StatusCode\n\nwith tracer.start_as_current_span(\n    \"my-span-name\",\n    openinference_span_kind=\"chain\",\n) as span:\n    span.set_input(\"input\")\n    span.set_output(\"output\")\n    span.set_status(Status(StatusCode.OK))\n```\n\n----------------------------------------\n\nTITLE: Instantiating OpenAIModel for GPT-3.5 Turbo Relevance Evaluation in Python\nDESCRIPTION: Creates an OpenAIModel using the GPT-3.5 Turbo model variant, with deterministic output (temperature 0.0) and a 20-second request timeout. GPT-3.5 can be significantly faster but may have tradeoffs in accuracy compared to GPT-4. This object is needed for subsequent model inference calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model=\"gpt-3.5-turbo\", temperature=0.0, request_timeout=20)\n```\n\n----------------------------------------\n\nTITLE: Importing Phoenix Prompt Utilities and Fetching Prompt in JavaScript\nDESCRIPTION: Imports the Phoenix prompt helper utilities, fetches a named prompt ('question-searcher') from the Phoenix server, and renders its contents in the Jupyter notebook using Deno's markdown output. Requires '@arizeai/phoenix-client/prompts', a valid Phoenix client, and Jupyter markdown support in Deno. Outputs the prompt object in prettified JSON.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_cross_sdk_tutorial.ipynb#_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport * as Prompts from \"npm:@arizeai/phoenix-client/prompts\"\n\nconst questionSearcherPrompt = await Prompts.getPrompt({ client: px, prompt: { name: \"question-searcher\" } })\n\nawait Deno.jupyter.md`\n  ### question-searcher prompt\n\n  \\`\\`\\`json\n  ${JSON.stringify(questionSearcherPrompt, null, 2)}\n  \\`\\`\\`\n  `\n```\n\n----------------------------------------\n\nTITLE: Creating and Using PromptTemplate Class in Python\nDESCRIPTION: Class for managing prompt templates with variable substitution. Takes a template string and delimiters to identify variables, then allows formatting with values.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evals-reference.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass PromptTemplate(\n    text: str\n    delimiters: List[str]\n)\n```\n\n----------------------------------------\n\nTITLE: Querying Vector Index\nDESCRIPTION: This code performs a query against the vector index using the query engine. It asks the question \"What did the author do growing up?\" and stores the response in the `response_vector` variable.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nresponse_vector = query_engine.query(\"What did the author do growing up?\")\n```\n\n----------------------------------------\n\nTITLE: Run Second Evaluation Experiment Python\nDESCRIPTION: Re-runs the evaluation experiment using the same dataset and task definition, but now the underlying `generate_query` function utilizes the improved prompt with the few-shot example. This allows for a comparison of the evaluation scores before and after the prompt modification to assess its impact on the Text2SQL model's performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nexperiment = run_experiment(\n    ds, task=task, evaluators=[has_results, no_error], experiment_metadata=CONFIG\n)\n```\n\n----------------------------------------\n\nTITLE: Running an Experiment to Simulate Production Traces in Phoenix\nDESCRIPTION: This code imports run_experiment from phoenix.experiments and initiates an experiment named 'Solve Math Problems' with a generated unique identifier. It executes the solve_math_problem function on the provided dataset within the experiment context, enabling simulation of traces to analyze performance and correctness in a production-like environment. Dependencies include phoenix.experiments and uuid for unique naming.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/openai_agents_cookbook.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.experiments import run_experiment\n\ninitial_experiment = run_experiment(\n    dataset,\n    task=solve_math_problem,\n    experiment_description=\"Solve Math Problems\",\n    experiment_name=f\"solve-math-questions-{str(uuid.uuid4())[:5]}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key for LangChain (Python)\nDESCRIPTION: Checks if the OpenAI API key is set as an environment variable; if not, securely prompts the user to enter it at runtime. This key is then set in the environment to allow downstream modules (such as OpenAIEmbeddings or ChatOpenAI) to authenticate API requests. Requires the OPENAI_API_KEY to use OpenAI services, and uses getpass for secure input.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/qdrant_langchain_instrumentation_search_and_retrieval_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"\\ud83d\\udd11 Enter your OpenAI API key: \")\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Running Customer Support Experiment for Improved ReAct Prompt Using run_experiment in Python\nDESCRIPTION: Executes an experiment on the dataset using the ReAct prompt, the same task and evaluator functions, and updated experiment metadata reflecting the new prompt ID. This facilitates performance comparison between the initial and improved prompts within Phoenix's experimentation framework, enabling tracking of prompt improvements and impact on LLM responses for customer support tasks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/react-prompting.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ninitial_experiment = run_experiment(\n    dataset,\n    task=prompt_task,\n    evaluators=[evaluate_response],\n    experiment_description=\"Customer Support Prompt\",\n    experiment_name=\"improved-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Load Image Dataset with Hugging Face (Python)\nDESCRIPTION: This code loads a sample image classification dataset named 'huggingface/image-classification-test-sample' from the Hugging Face Datasets library. It specifically loads the 'train' split and converts it into a pandas DataFrame for easier manipulation. Required dependencies: `datasets`, `phoenix` (imported earlier).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nimport phoenix as px\n\ndf = load_dataset(\"huggingface/image-classification-test-sample\")[\"train\"].to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Suppressing Tracing with Context Manager (Python)\nDESCRIPTION: This snippet demonstrates how to temporarily disable tracing within a specific block of code using the `suppress_tracing` context manager. This is useful for sections where tracing is not desired, such as document chunking or specific evaluations. The code inside the `with` block will not generate any traces, and tracing resumes outside the block. It requires the `phoenix.trace` module.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/advanced/suppress-tracing.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import suppress_tracing\n\nwith suppress_tracing():\n    # Code running inside this block doesn't generate traces.\n    # For example, running LLM evals here won't generate additional traces.\n    ...\n# Tracing will resume outside the block.\n...\n```\n\n----------------------------------------\n\nTITLE: Creating RAG Evaluation DataFrame (Phoenix, Pandas)\nDESCRIPTION: This snippet creates a combined RAG evaluation DataFrame by concatenating the input queries from retriever spans, NDCG@2, precision@2, and hit rate. It retrieves retriever spans from the Phoenix active session and concatenates them with the previously computed evaluation metrics using Pandas. It depends on the `phoenix` library.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nretrievals_df = px.active_session().get_spans_dataframe(\"span_kind == 'RETRIEVER'\")\nrag_evaluation_dataframe = pd.concat(\n    [\n        retrievals_df[\"attributes.input.value\"],\n        ndcg_at_2.add_prefix(\"ncdg@2_\"),\n        precision_at_2.add_prefix(\"precision@2_\"),\n        hit,\n    ],\n    axis=1,\n)\nrag_evaluation_dataframe\n```\n\n----------------------------------------\n\nTITLE: Setting up Asynchronous Execution and Imports\nDESCRIPTION: This snippet imports necessary libraries and sets up the environment for asynchronous execution. It uses nest_asyncio to allow nested async loops, imports os and getpass for API key management, pandas for data manipulation, phoenix for tracing, and llama_index components for building the RAG pipeline.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nimport os\nfrom getpass import getpass\n\nimport pandas as pd\nimport phoenix as px\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex, set_global_handler\nfrom llama_index.llms import OpenAI\nfrom llama_index.node_parser import SimpleNodeParser\n```\n\n----------------------------------------\n\nTITLE: Defining Ragas Goal Accuracy Evaluator - Python\nDESCRIPTION: Defines an asynchronous function `goal_evaluator` utilizing the Ragas `AgentGoalAccuracyWithReference` metric. It formats agent messages for Ragas via `conversation_to_ragas_sample`, employs an LLM (`gpt-4o`) wrapped for Ragas evaluation, and computes the agent's goal accuracy score by comparing its output to a reference.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/integrations/ragas.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_openai import ChatOpenAI\nfrom ragas.llms import LangchainLLMWrapper\nfrom ragas.metrics import AgentGoalAccuracyWithReference, ToolCallAccuracy\n\nasync def goal_evaluator(input, output):\n    sample = conversation_to_ragas_sample(\n        output[\"messages\"], reference_answer=output[\"final_output\"]\n    )\n    evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n    goal_accuracy = AgentGoalAccuracyWithReference(llm=evaluator_llm)\n    return await goal_accuracy.multi_turn_ascore(sample)\n```\n\n----------------------------------------\n\nTITLE: Running Code Functionality Classifications with GPT-4 Turbo\nDESCRIPTION: This snippet demonstrates running code functionality classifications with the GPT-4 Turbo model. The classifications are made using llm_classify and the resulting labels are stored in classifications.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_functionality_classifications.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model_name=\"gpt-4-1106-preview\")\nclassifications = llm_classify(\n    dataframe=df,\n    template=CODE_FUNCTIONALITY_PROMPT_TEMPLATE,\n    model=model,\n    rails=list(CODE_FUNCTIONALITY_PROMPT_RAILS_MAP.values()),\n    concurrency=20,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Defining a Phoenix Schema for Production Inferences in Python\nDESCRIPTION: Creates a Schema for the production DataFrame, omitting the ground truth column since it is not present in production. Requires a Phoenix import as 'px'. Main parameters set are 'timestamp_column_name', 'prediction_label_column_name', and the embedding mapping. This Schema is designed for datasets that lack actual/ground truth outcomes, a common scenario in production monitoring.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/quickstart/phoenix-inferences/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprod_schema = px.Schema(\n    timestamp_column_name=\"prediction_ts\",\n    prediction_label_column_name=\"predicted_action\",\n    embedding_feature_column_names={\n        \"image_embedding\": px.EmbeddingColumnNames(\n            vector_column_name=\"image_vector\",\n            link_to_data_column_name=\"url\",\n        ),\n    },\n)\n\n```\n\n----------------------------------------\n\nTITLE: Downloading Benchmark Dataset using Phoenix Evals in Python\nDESCRIPTION: Imports the `download_benchmark_dataset` function from `phoenix.evals` and uses it to fetch a pre-defined benchmark dataset ('halueval_qa_data' for 'binary-hallucination-classification' task) into a pandas DataFrame. This step is crucial for obtaining a 'golden dataset' required for benchmarking the custom evaluation template.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/concepts-evals/building-your-own-evals.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import download_benchmark_dataset\n\ndf = download_benchmark_dataset(\n    task=\"binary-hallucination-classification\", dataset_name=\"halueval_qa_data\"\n)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Creating Input Image Directory\nDESCRIPTION: Creates a directory named 'input_images' if it doesn't already exist.  This directory will be used to store the downloaded images from Tesla's website.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_multimodal_image_reasoning.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\ninput_image_path = Path(\"input_images\")\nif not input_image_path.exists():\n    Path.mkdir(input_image_path)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Environment (Python)\nDESCRIPTION: Ensures that the OpenAI API key is available to authenticate calls for synthetic data generation and LLM-based evaluation. If not set in the environment, the script prompts the user securely to input their API key using getpass. The key is stored in the 'OPENAI_API_KEY' environment variable. Requires OpenAI access.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"\\ud83d\\udd11 Enter your OpenAI API key: \")\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Tracing for LlamaIndex\nDESCRIPTION: Sets up the Phoenix tracer provider by registering it with a project name and endpoint. It then instruments LlamaIndex using `LlamaIndexInstrumentor`, linking it to the configured tracer provider. This enables automatic tracing of LlamaIndex operations, sending trace data to the specified Phoenix endpoint.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/agentic_rag_tracing.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# configure the Phoenix tracer\ntracer_provider = register(\n    project_name=\"agentic-rag-demo\",\n    endpoint=\"https://app.phoenix.arize.com/v1/traces\",  # change this endpoint if you're running Phoenix locally\n)\n\n# Finish automatic instrumentation\nLlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Set Environment Variables for Phoenix and Weaviate - Python\nDESCRIPTION: Imports the 'os' and 'getpass' modules to securely prompt the user for API keys and URLs. Sets environment variables for the Phoenix API key, client headers, and collector endpoint, as well as the Weaviate API URL and API key, which are used for subsequent connections.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nos.environ[\"PHOENIX_API_KEY\"] = getpass(\"Enter your Phoenix API key: \")\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n\nos.environ[\"WEAVIATE_URL\"] = getpass(\"Enter your Weaviate API URL: \")\nos.environ[\"WEAVIATE_API_KEY\"] = getpass(\"Enter your Weaviate API key: \")\n```\n\n----------------------------------------\n\nTITLE: Fetching Dataset for RAG Example (Shell)\nDESCRIPTION: Downloads the sample essay by Paul Graham from GitHub and saves it into a designated data directory. Ensures that necessary directories exist and retrieves the dataset using curl, which is required for subsequent RAG indexing. Requires write permissions and internet access.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n!mkdir -p 'data/paul_graham/'\n!curl 'https://raw.githubusercontent.com/Arize-ai/phoenix-assets/main/data/paul_graham/paul_graham_essay.txt' -o 'data/paul_graham/paul_graham_essay.txt'\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix and Llama-Index in Python\nDESCRIPTION: This snippet uses pip to install core dependencies for Phoenix tracing, Llama-Index document processing, OpenAI, Cohere, and related libraries. Required for all subsequent operations, it ensures compatibility and correct environment setup. No inputs are required, and outputs are not produced via codeâ€”this prepares the environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qq arize-phoenix llama-index \"openai>=1\" gcsfs nest_asyncio langchain langchain-community cohere llama-index-postprocessor-cohere-rerank 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI Calls with Phoenix Tracing in Python\nDESCRIPTION: Sets up OpenTelemetry tracing using Phoenix's instrumentation tools to send telemetry data for OpenAI API calls. Registers a tracer provider with a project name and specifies the Phoenix tracing ingestion endpoint. Uses 'OpenAIInstrumentor' to automatically instrument OpenAI, enabling observability of API interactions within the Phoenix monitoring platform.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/optimizing_llm_as_a_judge_prompts.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(\n    project_name=\"LLM-as-a-Judge\", endpoint=\"https://app.phoenix.arize.com/v1/traces\"\n)\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Defining and Using Function to Display Image Examples (Python)\nDESCRIPTION: Defines a Python function `display_examples` that takes a Pandas DataFrame, formats it to show images (from URLs) alongside actual and predicted labels using HTML, and renders it using `IPython.display`. The code then calls this function with the head of the training DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/image_classification_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef display_examples(df):\n    \"\"\"\n    Displays each image alongside the actual and predicted classes.\n    \"\"\"\n    sample_df = df.reindex(columns=[\"actual_action\", \"predicted_action\", \"url\"]).rename(\n        columns={\"url\": \"image\"}\n    )\n    html = sample_df.to_html(\n        escape=False, index=False, formatters={\"image\": lambda url: f'<img src=\"{url}\">'}\n    )\n    display(HTML(html))\n\n\ndisplay_examples(train_df.head())\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix API Key and Endpoint (Python)\nDESCRIPTION: Configures environment variables for your Python process to enable authenticated tracing to Phoenix Cloud. 'PHOENIX_CLIENT_HEADERS' should include the 'api_key', and 'PHOENIX_COLLECTOR_ENDPOINT' should be set to the Phoenix Cloud service endpoint. Requires your API key and must be run before starting instrumented Python code.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/groq.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Phoenix Client - Python\nDESCRIPTION: Uploads the previously created DataFrame as a dataset to Phoenix, setting a unique name based on current UTC timestamp and specifying input/output columns. Inputs: dataset_name, df, key column names. Outputs: Dataset registered in Phoenix; down-the-line, can be fetched by name. Sleep is used to ensure asynchronous upload completion.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/run_experiments_with_llama_index.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset_name = datetime.now(timezone.utc).isoformat()\npx.Client().upload_dataset(\n    dataset_name=dataset_name,\n    dataframe=df,\n    input_keys=(\"input_messages\",),\n    output_keys=(\"output_message\",),\n)\nsleep(1)\n```\n\n----------------------------------------\n\nTITLE: Converting Phoenix Prompts for Use with LLM Provider SDKs\nDESCRIPTION: TypeScript example showing how to convert Phoenix prompts to formats compatible with different LLM provider SDKs like Vercel AI SDK, OpenAI, or Anthropic.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/packages/phoenix-client/README.md#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from \"ai\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { getPrompt, toSDK } from \"@arizeai/phoenix-client/prompts\";\n\nconst prompt = await getPrompt({ name: \"my-prompt\" });\nconst promptAsAI = toSDK({\n  sdk: \"ai\",\n  // ^ the SDK you want to convert the prompt to, supported SDKs are listed above\n  variables: {\n    \"my-variable\": \"my-value\",\n  },\n  // ^ you can format the prompt with variables, if the prompt has any variables in its template\n  //   the format (Mustache, F-string, etc.) is specified in the Prompt itself\n  prompt,\n});\n// ^ promptAsAI is now in the format expected by the Vercel AI SDK generateText function\n\nconst response = await generateText({\n  model: openai(prompt.model_name),\n  // ^ the model adapter provided by the Vercel AI SDK can be swapped out for any other model\n  //   adapter supported by the Vercel AI SDK. Take care to use the correct model name for the\n  //   LLM provider you are using.\n  ...promptAsAI,\n});\n```\n\n----------------------------------------\n\nTITLE: Creating and Pushing a Prompt to Phoenix Server Using Python Client\nDESCRIPTION: Uses the Phoenix client to create and store a prompt on the Phoenix server. The prompt has an identifier, user role content with template variables, and specifies the model name for the prompt version. This allows remote management of prompt versions for later retrieval or invocation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.client import Client\nfrom phoenix.client.types import PromptVersion\n\n# Change base_url to your Phoenix server URL\nbase_url = \"http://localhost:6006\"\nclient = Client(base_url=base_url)\n\n# prompt identifier consists of alphanumeric characters, hyphens or underscores\nprompt_identifier = \"haiku-writer\"\n\ncontent = \"Write a haiku about {{topic}}\"\nprompt = client.prompts.create(\n    name=prompt_identifier,\n    version=PromptVersion(\n        [{\"role\": \"user\", \"content\": content}],\n        model_name=\"gpt-4o-mini\",\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Data and Building Vector Index\nDESCRIPTION: This code loads the Paul Graham essay data from the local file system, defines an OpenAI language model (gpt-4), and builds a vector index using LlamaIndex.  It uses a chunk size of 512 for node parsing, creating vector embeddings for efficient querying.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndocuments = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n\n# Define an LLM\nllm = OpenAI(model=\"gpt-4\")\n\n# Build index with a chunk_size of 512\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=512)\nnodes = node_parser.get_nodes_from_documents(documents)\nvector_index = VectorStoreIndex(nodes)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Retrieving API Keys for OpenAI and Cohere in Python\nDESCRIPTION: This snippet retrieves OpenAI and Cohere API keys via environment variables or secure user input for authenticated API access. Utilizes the getpass module for secure prompts. The keys are set as environment variables to be accessible by downstream libraries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"\\ud83d\\udd11 Enter your OpenAI API key: \")\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n\nif not (cohere_api_key := os.getenv(\"COHERE_API_KEY\")):\n    cohere_api_key = getpass(\"\\ud83d\\udd11 Enter your Cohere API key: \")\nos.environ[\"COHERE_API_KEY\"] = cohere_api_key\n```\n\n----------------------------------------\n\nTITLE: Display Sample Image (Python)\nDESCRIPTION: This snippet decodes and displays the first image from the processed DataFrame using IPython's `display` function. It extracts the base64 string, removes the data URI prefix, decodes it back to bytes, and then displays the image. This verifies the image processing steps were correct. Required dependencies: `IPython.display`, `base64`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Image, display\n\n# Get the image data from the first row\nimage_data = df.loc[0, \"img\"]\n\n# Remove the data URI prefix\nimage_data = image_data.split(\",\")[1]\n\n# Decode the base64 string\nimage_bytes = base64.b64decode(image_data)\n\n# Display the image\ndisplay(Image(data=image_bytes))\n```\n\n----------------------------------------\n\nTITLE: Configure Phoenix Cloud Endpoint Python Env Vars\nDESCRIPTION: Sets the necessary environment variables in Python (`PHOENIX_CLIENT_HEADERS` and `PHOENIX_COLLECTOR_ENDPOINT`) to configure the Phoenix SDK to send traces to the Arize Phoenix Cloud service using your API key.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/vertexai.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Importing Pandas and Configuring Display (Python)\nDESCRIPTION: Imports the pandas library for data manipulation. Configures the pandas display option `display.max_colwidth` to `None` to prevent truncation of cell content in DataFrames, aiding in viewing retrieved text or contexts.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\n# Display the complete contents of dataframe cells.\npd.set_option(\"display.max_colwidth\", None)\n```\n\n----------------------------------------\n\nTITLE: Setting up Phoenix local environment variables\nDESCRIPTION: Python code for configuring environment variables to connect to a locally deployed Phoenix instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/guardrails-ai.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Securely in Python\nDESCRIPTION: Securely prompts for and sets the OpenAI API key as an environment variable if it is not already set. Uses getpass to prevent the API key from being echoed in plaintext. Dependencies: os and getpass standard libraries. If OPENAI_API_KEY is unset, it prompts the user to enter their key; otherwise, it leaves the environment unchanged.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"\\ud83d\\udd11 Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Compute Precision@k\nDESCRIPTION: This snippet computes precision at k, a metric to evaluate the fraction of relevant documents among the top k retrieved documents. It groups the documents by a span ID, sums the relevance scores for the top 2 documents (if available), and then divides the sum by 2.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nprecision_at_2 = pd.DataFrame(\n    {\n        \"score\": documents_with_relevance_df.groupby(\"context.span_id\").apply(\n            lambda x: x.eval_score[:2].sum(skipna=False) / 2\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Task and Evaluator Functions for Experiment - Python\nDESCRIPTION: Defines two core functions: 'test_prompt' (calls the OpenAI API with the prompt and retrieves the model's response for a given input prompt) and 'evaluate_response' (compares the model's output to the expected label). Dependencies: OpenAI and a pre-saved prompt format. Expects a dictionary with a 'prompt' (input) and output as per model behavior. Lowercases outputs to avoid casing issues in evaluation. Outputs are extracted, stripped, and checked for match with ground-truth.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef test_prompt(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(**prompt.format(variables={\"prompt\": input[\"prompt\"]}))\n    return resp.choices[0].message.content.strip()\n\n\ndef evaluate_response(output, expected):\n    return output.lower() == expected[\"type\"].lower()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Trustworthiness with Cleanlab TLM\nDESCRIPTION: Code to initialize Cleanlab's TLM and evaluate the trustworthiness of each prompt-response pair. This generates trustworthiness scores and explanations for each LLM response in the dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/integrations/cleanlab.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom cleanlab_tlm import TLM\n\ntlm = TLM(options={\"log\": [\"explanation\"]})\n\n# Evaluate each of the prompt, response pairs using TLM\nevaluations = tlm.get_trustworthiness_score(prompts, responses)\n\n# Extract the trustworthiness scores and explanations from the evaluations\ntrust_scores = [entry[\"trustworthiness_score\"] for entry in evaluations]\nexplanations = [entry[\"log\"][\"explanation\"] for entry in evaluations]\n\n# Add the trust scores and explanations to the DataFrame\neval_df[\"score\"] = trust_scores\neval_df[\"explanation\"] = explanations\n```\n\n----------------------------------------\n\nTITLE: Displaying First Few Rows of DataFrame\nDESCRIPTION: This snippet calls `head()` on the `retrieved_documents_eval` DataFrame to print the first few rows for inspection. It helps view the results of retrieval relevance evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nretrieved_documents_eval.head()\n```\n\n----------------------------------------\n\nTITLE: Instrumenting CrewAI for Tracing with OpenTelemetry in Python\nDESCRIPTION: Uses the CrewAIInstrumentor class from openinference.instrumentation.crewai to enable instrumentation of CrewAI operations for telemetry. The instrument() method is called with skip_dep_check=True to bypass dependency verification and the previously registered tracer_provider to enable OpenTelemetry tracing integration with Arize Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/crewai_tracing_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.crewai import CrewAIInstrumentor\n\nCrewAIInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Anthropic API Key for Model Comparison in Python\nDESCRIPTION: Sets up the environment with an Anthropic API key for comparative experiments. It checks if the key is already present in environment variables and prompts for input if needed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/summarization.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif os.environ.get(\"ANTHROPIC_API_KEY\") is None:\n    os.environ[\"ANTHROPIC_API_KEY\"] = getpass(\"ðŸ”‘ Enter your Anthropic API key: \")\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables and API Keys for Phoenix and OpenAI Using Python\nDESCRIPTION: Sets necessary environment variables to connect to the Phoenix Cloud API and OpenAI API. Prompts the user securely for the Phoenix API key and the OpenAI API key if they are not already defined, using getpass for hidden input. This setup is prerequisite for authenticated API interactions throughout the tutorial.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\nif not os.environ.get(\"PHOENIX_CLIENT_HEADERS\"):\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = \"api_key=\" + getpass(\"Enter your Phoenix API key: \")\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Managing Prompt Tags with Phoenix Client (Python)\nDESCRIPTION: Demonstrates how to use the synchronous Phoenix Python client (`phoenix.client.Client`) to manage prompt version tags. This includes creating a new tag for a specific prompt version ID, listing existing tags for a version, and retrieving a prompt version by its assigned tag name rather than its ID or latest status.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/how-to-prompts/tag-a-prompt.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.client import Client\n\n# Create a tag for a prompt version\nClient().prompts.tags.create(\n    prompt_version_id=\"version-123\",\n    name=\"production\",\n    description=\"Ready for production environment\"\n)\n\n# List tags for a prompt version\ntags = Client().prompts.tags.list(prompt_version_id=\"version-123\")\nfor tag in tags:\n    print(f\"Tag: {tag.name}, Description: {tag.description}\")\n\n# Get a prompt version by tag\nprompt_version = Client().prompts.get(\n    prompt_identifier=\"my-prompt\",\n    tag=\"production\"\n)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry and Semantic Conventions Dependencies (shell)\nDESCRIPTION: Installs the required Python packages for OpenTelemetry API, SDK, and OpenInference Semantic Conventions via pip to enable tracing, exporting spans, and semantic conventions for LLM spans. Ensure you are running in an environment where pip and internet access are available. These commands should be executed in a terminal prior to adding manual instrumentation code.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/custom-spans.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install opentelemetry-api\npip install opentelemetry-sdk\n```\n\nLANGUAGE: shell\nCODE:\n```\npip install openinference-semantic-conventions\n```\n\n----------------------------------------\n\nTITLE: Run Task on Sample Example Python\nDESCRIPTION: Retrieves the first example from the dataset to use as input for a test run. It then calls the previously defined 'task' function with the input data from this example. The resulting output from the task (the LLM's response) is captured and printed, shortened for readability, to quickly verify that the task function executes without errors and produces output.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nexample = dataset[0]\ntask_output = task(example.input)\nprint(shorten(json.dumps(task_output), width=80))\n```\n\n----------------------------------------\n\nTITLE: Downloading the Code Readability Benchmark Dataset\nDESCRIPTION: Downloads a dataset containing code snippets with ground-truth labels indicating readability, specifically 'openai_humaneval_with_readability'. The dataset is used as a test bed for evaluation of classification models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndataset_name = \"openai_humaneval_with_readability\"\ndf = download_benchmark_dataset(task=\"code-readability-classification\", dataset_name=dataset_name)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Define LLM Task Function Python\nDESCRIPTION: Initializes an 'OpenAI' language model instance specifying the model ('gpt-3.5-turbo'). It then defines a Python function named 'task' that takes an 'input' dictionary (representing a dataset example) and uses the initialized LLM to generate a completion based on the example's document and the last message content. The function returns the generated text.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\ndef task(input):\n    return llm.complete(input[\"document\"] + \"\\n\\n\" + input[\"messages\"][-1][\"content\"]).text\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Phoenix Platform in Python\nDESCRIPTION: Creates and uploads a new dataset instance to Phoenix with a unique name combining the dataset base name and current UTC timestamp. Uses the pandas DataFrame columns 'inputs' and extracted 'outputs' values (mapping nested output objects). This enables Phoenix to store and manage data examples for later experiment orchestration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/langchain_email_extraction.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset = px.Client().upload_dataset(\n    dataset_name=f\"{dataset_name}{datetime.now(timezone.utc)}\",\n    inputs=df.inputs,\n    outputs=df.outputs.map(lambda obj: obj[\"output\"]),\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Anthropic API Key Securely in Python Environment\nDESCRIPTION: Checks if the environment variable 'ANTHROPIC_API_KEY' is set; if not, prompts the user to enter it securely using 'getpass' and sets it in the environment. This is a prerequisite step for running Anthropic Claude model experiments. It ensures sensitive API keys are not hardcoded. Requires 'os' and 'getpass' modules.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/summarization.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif os.environ.get(\"ANTHROPIC_API_KEY\") is None:\n    os.environ[\"ANTHROPIC_API_KEY\"] = getpass(\"\\ud83d\\udd11 Enter your Anthropic API key: \")\n```\n\n----------------------------------------\n\nTITLE: Merging Evaluation Results with Original Data - Python\nDESCRIPTION: Combines the hallucination and Q&A evaluation results (labels and explanations) with the original DataFrame. Appends the result and explanation columns for both evaluators, allowing subsequent inspection or export. Assumes prior successful execution of evaluation code and existence of 'hallucination_eval_df' and 'qa_eval_df'.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/evals.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresults_df = df.copy()\nresults_df[\"hallucination_eval\"] = hallucination_eval_df[\"label\"]\nresults_df[\"hallucination_explanation\"] = hallucination_eval_df[\"explanation\"]\nresults_df[\"qa_eval\"] = qa_eval_df[\"label\"]\nresults_df[\"qa_explanation\"] = qa_eval_df[\"explanation\"]\nresults_df.head()\n```\n\n----------------------------------------\n\nTITLE: Saving the New Prompt to Phoenix\nDESCRIPTION: This snippet saves the newly generated prompt (created via meta-prompting) into Arize Phoenix. It checks if the prompt contains a placeholder for examples, and if so, formats it with few-shot examples. It then creates a new prompt version in Phoenix using the OpenAI completion parameters.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nif r\"\\{examples\\}\" in new_prompt:\n    new_prompt = new_prompt.format(examples=few_shot_examples)\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"system\", \"content\": new_prompt},\n        {\"role\": \"user\", \"content\": \"{{prompt}}\"},\n    ],\n)\n\nmeta_prompt_result = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Meta prompt result\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Logging Traces to Phoenix using Client in Python\nDESCRIPTION: This snippet illustrates how to interact with a running Phoenix session. It first creates a `phoenix.Client` instance, connecting it to the Phoenix session specified by the `session_url` obtained previously. Then, it uses the `client.log_traces()` method to upload the `trace_ds` (a `TraceDataset` object) to the Phoenix instance, optionally assigning it to a project named 'old-traces'. Requires a running Phoenix session, a `phoenix.Client` instance, and a `TraceDataset` object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/log_traces_to_phoenix.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient = px.Client(endpoint=session_url)\nclient.log_traces(trace_ds, project_name=\"old-traces\")\n```\n\n----------------------------------------\n\nTITLE: Loading test dataset for one-shot prompting\nDESCRIPTION: Loads the test dataset for the `fridgeReviews` from Hugging Face datasets and converts to a pandas dataframe. Then, it samples a single row, which will be later used as a one-shot example.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/few_shot_prompting.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nds = load_dataset(\"syeddula/fridgeReviews\")[\"test\"]\none_shot_example = ds.to_pandas().sample(1)\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix tracer for DSPy\nDESCRIPTION: Python code to register and configure the Phoenix tracer provider. This sets up the OpenTelemetry tracing with a project name and enables auto-instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/dspy.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Langchain and Phoenix Components in Python\nDESCRIPTION: Imports necessary components for building and tracing a Langchain agent. It imports `hub` for pulling pre-defined prompts, `AgentExecutor` and `create_tool_calling_agent` for agent creation, `tool` decorator for defining functions as tools, and `ChatOpenAI` as the LLM. It also imports the `phoenix` library itself (aliased as `px`) and the `register` function from `phoenix.otel` for OpenTelemetry instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain.tools import tool\nfrom langchain_openai import ChatOpenAI\n\nimport phoenix as px\nfrom phoenix.otel import register\n```\n\n----------------------------------------\n\nTITLE: Exploding Multiple Attributes in SpanQuery for Content and Score - Python\nDESCRIPTION: This snippet demonstrates how to use the explode method in Phoenix's SpanQuery to extract multiple fields (such as document content and score) from each exploded item in a list attribute. The output DataFrame contains custom columns for the content and score of each retrieved document. Requires a recent Phoenix SDK supporting keyword-argument column mapping in explode.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nquery = SpanQuery().explode(\n    \"retrieval.documents\",\n    reference=\"document.content\",\n    score=\"document.score\",\n)\n```\n\n----------------------------------------\n\nTITLE: Streaming the Agent's Output and Events\nDESCRIPTION: Demonstrates how to stream events from the app as it processes the query, printing each event to monitor real-time execution flow, useful for debugging and tracing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nfor event in app.stream({\"messages\": [(\"user\", \"Which sales agent made the most in sales in 2009?\")] }):\n    print(event)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key\nDESCRIPTION: Sets the OpenAI API key for the application. It first checks if the `OPENAI_API_KEY` environment variable is set. If not, it securely prompts the user to enter their key using `getpass`. The obtained key is then assigned to `openai.api_key` and also set as an environment variable for potential use by other libraries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_with_mongodb_and_llama_index.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Invoking OpenAI Chat Completion API With Python\nDESCRIPTION: Creates an OpenAI client instance, issues a chat completion request using model 'gpt-4o', and prints the response content. The 'messages' parameter supplies user input for completion generation. This snippet demonstrates typical usage of the OpenAI API, with output observable via Phoenix tracing if instrumentation is set up.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nclient = openai.OpenAI()\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a haiku.\"}],\n)\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Llama-Index for OpenTelemetry Tracing (Python)\nDESCRIPTION: Configures OpenTelemetry tracing to send spans to a collector endpoint (defaulting to `http://127.0.0.1:4317`). It sets up a `TracerProvider` and `SimpleSpanProcessor` and then uses `LlamaIndexInstrumentor` to automatically generate traces for operations performed by Llama-Index components, which can be viewed in Phoenix. Requires a running OpenTelemetry collector.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nendpoint = \"http://127.0.0.1:4317\"\n(tracer_provider := TracerProvider()).add_span_processor(\n    SimpleSpanProcessor(OTLPSpanExporter(endpoint))\n)\nLlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Preview Run with GPT-4 Turbo Model: Classification Workflow - Python\nDESCRIPTION: Demonstrates a run using the preview GPT-4 Turbo model for LLM-based classification leveraging Phoenix. Returns a list of labels from the classification. Assumes prior variables ('df', prompt template, rails) and dependencies are defined and configured.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model_name=\"gpt-4-turbo-preview\")\nrelevance_classifications = llm_classify(\n    dataframe=df,\n    template=HUMAN_VS_AI_PROMPT_TEMPLATE,\n    model=model,\n    rails=list(HUMAN_VS_AI_PROMPT_RAILS_MAP.values()),\n    concurrency=50,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Git Branch on Google Colab\nDESCRIPTION: Code snippet to install the latest Node.js version and the main branch of Phoenix directly from GitHub on Google Colab. This allows testing the latest development version in Colab notebooks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_20\n\nLANGUAGE: jupyterpython\nCODE:\n```\n!npm install -g -s n\n!n latest\n!npm install -g -s npm@latest\n%pip install git+https://github.com/Arize-ai/phoenix.git@main\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Data Processing and Embeddings - Python\nDESCRIPTION: Imports pandas for data manipulation, the tabular embedding generator from Arize's SDK, sklearn for performance metrics, and Phoenix core functionality. These imports are prerequisites for subsequent data loading, embedding computation, and launching the Phoenix UI. All listed packages must be installed in the runtime environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/credit_card_fraud_tutorial.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nfrom arize.pandas.embeddings.tabular_generators import EmbeddingGeneratorForTabularFeatures\nfrom sklearn.metrics import recall_score\n\nimport phoenix as px\n```\n\n----------------------------------------\n\nTITLE: Saving Processed DataFrame to JSONL with Python\nDESCRIPTION: Saves the transformed Pandas DataFrame to a JSON Lines (`.jsonl`) file specified by `output_path`. It first attempts to remove the file if it exists to avoid appending to an old file and then appends each record (converted to a dictionary) as a JSON string followed by a newline.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_halueval.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\noutput_path = \"halueval_qa_data.jsonl\"\n\ntry:\n    os.remove(output_path)\nexcept OSError:\n    pass\n\nwith open(output_path, \"a\") as f:\n    for record in df.to_dict(orient=\"records\"):\n        f.write(json.dumps(record) + \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Tips for Improving Custom Evaluations\nDESCRIPTION: Although no code snippets are provided, this section emphasizes iterative testing of your custom LLM evaluation prompts against labeled ground truth datasets. It recommends analyzing metrics like F1, precision, and recall, then refining prompts accordingly, which can be aided by documentation on prompt optimization techniques.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/bring-your-own-evaluator.md#_snippet_4\n\n\n\n----------------------------------------\n\nTITLE: snap_to_rails Function for Parsing Classification Outputs\nDESCRIPTION: This explanation details how the snap_to_rails function interprets the output string from an LLM by matching it against the predefined classes in rails. It handles variations such as case differences, extra punctuation, and ambiguous responses, mapping responses to 'relevant', 'irrelevant', or 'UNPARSABLE' if no clear class can be identified.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/bring-your-own-evaluator.md#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Define Question Generation Template\nDESCRIPTION: Defines a template for an LLM prompt to generate questions based on a given context. It specifies the context information, instructs the LLM to act as a teacher, and requests the output in a JSON format with specific keys.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ngenerate_questions_template = \"\"\"\\nContext information is below.\\n\\n---------------------\\n{text}\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup \\\n3 questions for an upcoming \\\nquiz/examination. The questions should be diverse in nature \\\nacross the document. Restrict the questions to the \\\ncontext information provided.\\\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Importing Phoenix in Python\nDESCRIPTION: Installs the Arize Phoenix library using pip and imports it as 'px'. Required dependency: 'arize-phoenix' Python package, which must be installed in the notebook or Colab environment. This step sets up the environment for subsequent Phoenix operations. No inputs or outputs except successful installation/import; ensure internet access and appropriate permissions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/quickstart/phoenix-inferences/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install arize-phoenix\n\nimport phoenix as px\n\n```\n\n----------------------------------------\n\nTITLE: Connecting to Phoenix Instance (Python)\nDESCRIPTION: Checks for the `PHOENIX_API_KEY` environment variable to determine whether to connect to a cloud instance of Phoenix or launch a local instance. Sets appropriate environment variables (`PHOENIX_CLIENT_HEADERS`, `PHOENIX_COLLECTOR_ENDPOINT`) for cloud connection or uses `px.launch_app()` for local setup. This establishes the connection needed to send and retrieve trace data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evaluating_traces_quickstart.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Check if PHOENIX_API_KEY is present in the environment variables.\n# If it is, we'll use the cloud instance of Phoenix. If it's not, we'll start a local instance.\n# A third option is to connect to a docker or locally hosted instance.\n# See https://docs.arize.com/phoenix/setup/environments for more information.\n\nimport os\n\nif \"PHOENIX_API_KEY\" in os.environ:\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\n    os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n    print(\"Using cloud instance of Phoenix.\")\nelse:\n    import phoenix as px\n\n    px.launch_app().view()\n    print(\"Using local instance of Phoenix.\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Tracing with OpenInference\nDESCRIPTION: Sets up distributed tracing for OpenAI calls using Phoenix and OpenInference instrumentation. Requires a Phoenix API key and configures the tracer provider to send traces to Phoenix Cloud for monitoring LLM interactions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nimport phoenix as px\nfrom phoenix.otel import register\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = getpass(\"Enter your Phoenix API Key\")\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n\n# configure the Phoenix tracer\ntracer_provider = register(\n    endpoint=\"https://app.phoenix.arize.com/v1/traces\",\n)\nOpenAIInstrumentor().uninstrument()\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Configuring Arize Phoenix Environment Variables in .env File\nDESCRIPTION: Defines required environment variables for integrating Langflow with Arize Phoenix observability. The snippet shows how to create a .env file in the Langflow repo root using a provided template and insert the API key needed to authenticate and send telemetry data to the Phoenix platform. This setup is critical for enabling runtime tracing and analytics within Langflow applications.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langflow.md#_snippet_0\n\nLANGUAGE: env\nCODE:\n```\n# Arize Phoenix Env Variables\nPHOENIX_API_KEY=\"YOUR_PHOENIX_KEY_HERE\"\n```\n\n----------------------------------------\n\nTITLE: View Dataset as DataFrame Python\nDESCRIPTION: Calls the 'as_dataframe()' method on the Phoenix dataset object. This action converts the dataset, which is managed by Phoenix, into a standard Pandas DataFrame. This allows users to easily inspect the data's structure and content using familiar Pandas functionalities within the notebook environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset.as_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Instantiating Custom LLM Wrapper (Python)\nDESCRIPTION: Creates an instance of the previously defined `FunnyAIModel`. This model instance is configured to use 'gpt-4o' as the base model with a temperature of 0.0 (for deterministic behavior when no error occurs), preparing the intentionally buggy model for the evaluation runs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfunny_model = FunnyAIModel(\n    model=\"gpt-4o\",\n    temperature=0.0,\n)\n```\n\n----------------------------------------\n\nTITLE: Inspecting a Single Dataset Example in Python\nDESCRIPTION: Retrieves the first example (at index 0) from the Phoenix `dataset` object. Accessing an element by index allows for inspection of the structure and content of individual data points within the dataset, which typically includes input fields ('messages', 'document' in this case).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndataset[0]\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenAI API Key in Python\nDESCRIPTION: This snippet checks if the environment variable OPENAI_API_KEY is set; if not, it prompts the user securely for their OpenAI API key using getpass. The key is then assigned to the environment variable to allow subsequent OpenAI client libraries to authenticate API requests. It eliminates hardcoding the API key and supports interactive usage.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/langchain_email_extraction.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"\\ud83d\\udd11 Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Inserting Processed Data into MongoDB Collection\nDESCRIPTION: Populates a specified MongoDB Atlas collection with data prepared from the fetched dataset. It defines the database and collection names, accesses the collection via the `client` object. If `overwrite` is True, it first deletes all existing documents in the collection. Then, it iterates through the `rows` data, formats each row into a dictionary containing 'embedding', 'text', 'id', and 'source_doc_id', and inserts these documents into the MongoDB collection using `insert_many`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_with_mongodb_and_llama_index.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndb_name = \"phoenix\"\ncollection_name = \"phoenix-docs\"\n\ndb = client[db_name]  # Replace with your database name\ncollection = db[collection_name]  # Replace with your collection name\n\n# Assuming 'overwrite=True' means you want to clear the collection first and insert nodes\noverwrite = True\nif overwrite:\n    collection.delete_many({})\n    nodes = []\n    for row in rows:\n        node = {\n            \"embedding\": row[\"embedding\"],\n            \"text\": row[\"text\"],\n            \"id\": row[\"id\"],\n            \"source_doc_id\": row[\"doc_id\"],  # Assuming this is a relationship reference\n        }\n        nodes.append(node)\n\n    # Insert the documents into MongoDB Atlas\n    collection.insert_many(nodes)\n    print(\"Succesfully added nodes into mongodb!\")\n```\n\n----------------------------------------\n\nTITLE: Instantiating GPT-3.5 model for summarization evaluation\nDESCRIPTION: Creates an instance of the OpenAI GPT-3.5 model with zero temperature and a 20-second timeout for evaluation, which is faster but potentially less accurate than GPT-4.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(model_name=\"gpt-3.5-turbo\", temperature=0.0, request_timeout=20)\n```\n\n----------------------------------------\n\nTITLE: TypeScript: Retrieve Current Span ID using OpenTelemetry API\nDESCRIPTION: This snippet retrieves the active span's ID in a TypeScript application utilizing the OpenTelemetry API. It is intended for environments where tracing context is managed by the OpenTelemetry library.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/feedback-and-annotations/capture-feedback.md#_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { trace } from \"@opentelemetry/api\";\n\nasync function chat(req, res) {\n  // ...\n  const spanId = trace.getActiveSpan()?.spanContext().spanId;\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing LlamaIndex Instrumentor\nDESCRIPTION: This Python code initializes the LlamaIndexInstrumentor to trace LlamaIndex Workflows. It imports necessary modules, registers the tracer provider, and instruments the LlamaIndex package. The LlamaIndexInstrumentor creates spans for calls to LlamaIndex Workflows, which are then sent to the Phoenix server.  This process is key for tracing agent invocations within Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex-1.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\nfrom phoenix.otel import register\n\ntracer_provider = register()\nLlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix, LlamaIndex, Matplotlib\nDESCRIPTION: Installs necessary Python packages for observability, tracing (arize-phoenix, openinference-instrumentation-llama-index), framework dependencies(llama-index), and image display (matplotlib).  The %pip command is used within a Jupyter Notebook environment to install packages directly.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_multimodal_image_reasoning.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Observability & Tracing dependencies\n%pip install -qq \"arize-phoenix>=4.30.2\" \"openinference-instrumentation-llama-index==2.2.4\"\n# Framework dependencies\n%pip install -qq \"llama-index==0.10.68\"\n# Other dependencies: so that we can show and understand the images in this notebook\n%pip install -qq matplotlib\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix OTEL Tracer Provider with Environment Variables for gRPC in Python\nDESCRIPTION: Shows how to use the phoenix.otel.register function without parameters to send traces to a Phoenix server when the PHOENIX_COLLECTOR_ENDPOINT environment variable is set to a gRPC endpoint. The register function reads the environment variable automatically and configures the tracer provider accordingly for gRPC transport over the default port 4317 or as specified.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# export PHOENIX_COLLECTOR_ENDPOINT=https://your-phoenix.com:6006\n\nfrom phoenix.otel import register\n\n# sends traces to https://your-phoenix.com:4317\ntracer_provider = register()\n\n```\n\n----------------------------------------\n\nTITLE: Using OpenInference Semantic Conventions with `setAttributes` in TypeScript\nDESCRIPTION: Illustrates using the `setAttributes` function in TypeScript to add attributes defined by OpenInference Semantic Conventions (`@arizeai/openinference-semantic-conventions`). This example specifically sets the `session.id` attribute using the convention key `SemanticConventions.SESSION_ID`. Spans created within the callback inherit this attribute.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nimport { context } from \"@opentelemetry/api\"\nimport { setAttributes } from \"@openinference-core\"\nimport { SemanticConventions } from \"@arizeai/openinference-semantic-conventions\";\n\n\ncontext.with(\n  setAttributes(\n    { [SemanticConventions.SESSION_ID]: \"session-id\" } // Note: Corrected syntax assuming key is a string\n  ),\n  () => {\n      // Calls within this block will generate spans with the attributes:\n      // \"session.id\" = \"session-id\"\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Chatting without Images\nDESCRIPTION: Demonstrates asking a follow-up question without explicitly passing the image documents in the message. This tests if the LLM can recall previous images from the conversation context. The code calls the LLM with a similar setup as before, but excludes the image documents from the final message.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_multimodal_image_reasoning.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmessage_3_no_images = generate_openai_multi_modal_chat_message(\n    prompt=\"Can you tell me what the price of each spec as well?\",\n    role=\"user\",\n)\nresponse_3 = openai_mm_llm.chat(\n    messages=[\n        message_1,\n        message_2,\n        message_3_no_images,\n    ],\n)\n\nprint(response_3)\n```\n\n----------------------------------------\n\nTITLE: Starting Phoenix Server (bash)\nDESCRIPTION: Launches the Phoenix tracing and monitoring server using the main module entrypoint. This server must be running before traces and evals can be visualized in the Phoenix application. The command requires all Python dependencies to be installed and should be executed from the Phoenix project directory. No additional parameters are needed for basic serving.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/cron-evals/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m phoenix.server.main serve\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Convert String Vectors to NumPy Arrays in Python\nDESCRIPTION: Defines a Python function `string_to_array` that takes a string containing space-separated numbers (potentially representing a vector), extracts the numbers using regular expressions, and converts them into a NumPy array of floating-point numbers. Requires `re` and `numpy` libraries to be imported.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport re\n\nimport numpy as np\n\n\ndef string_to_array(s):\n    numbers = re.findall(r\"[-+]?\\d*\\.\\d+|[-+]?\\d+\", s)\n    return np.array([float(num) for num in numbers])\n```\n\n----------------------------------------\n\nTITLE: Adding Events to a Span (python)\nDESCRIPTION: Shows how to log events within a span's lifetime using add_event, which records human-readable messages at specific points. Requires: opentelemetry.trace and an active span. Input: event description (string). Output: event visible in traces, associated with the span's timeline.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/custom-spans.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace\n\ncurrent_span = trace.get_current_span()\n\ncurrent_span.add_event(\"Gonna try it!\")\n\n# Do the thing\n\ncurrent_span.add_event(\"Did it!\")\n```\n\n----------------------------------------\n\nTITLE: Classifying Retrieval Relevance with Phoenix\nDESCRIPTION: This snippet uses `llm_classify` to evaluate the relevance of retrieved documents. It uses a pre-defined prompt template and rails for LLM classification.  A 'score' is computed based on whether the document is 'relevant'.  This helps identify if irrelevant documents are polluting the context.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n    RAG_RELEVANCY_PROMPT_TEMPLATE,\n    llm_classify,\n)\n\nretrieved_documents_eval = llm_classify(\n    dataframe=retrieved_documents_df,\n    model=LiteLLMModel(model=\"ollama/\" + ollama_model),\n    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n    provide_explanation=True,\n)\n\nretrieved_documents_eval[\"score\"] = (\n    retrieved_documents_eval.label[~retrieved_documents_eval.label.isna()] == \"relevant\"\n).astype(int)\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Cloud API Key and Endpoint\nDESCRIPTION: This Python snippet sets the Phoenix API key and endpoint for tracing. The `PHOENIX_API_KEY` environment variable is set using the user's API key from their dashboard. It is used for authentication with the Phoenix Cloud service. The `PHOENIX_COLLECTOR_ENDPOINT` specifies the address to which the tracing data will be sent.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/instructor.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Loading Production Inference Data into a Pandas DataFrame using Python\nDESCRIPTION: Downloads and loads a parquet file containing model inference data from production into a pandas DataFrame, similar to the training data. This dataset is later wrapped and compared to the training data within Phoenix to detect drift and conduct comparative evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/inferences_quickstart.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprod_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/cv/human-actions/human_actions_training.parquet\"\n)\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean QA Correctness and Hallucination Scores in Python\nDESCRIPTION: Calculates the mean values of numeric columns from two pandas DataFrames, which store evaluation results for QA correctness and hallucination detection respectively. This operation summarizes the overall scoring performance of the RAG system based on these metrics. It requires the evaluation dataframes (qa_correctness_eval_df and hallucination_eval_df) to be precomputed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nqa_correctness_eval_df.mean(numeric_only=True)\n```\n\nLANGUAGE: python\nCODE:\n```\nhallucination_eval_df.mean(numeric_only=True)\n```\n\n----------------------------------------\n\nTITLE: Creating Partial Task and Testing with Sample Data - Python\nDESCRIPTION: Defines a prompt template and a specific OpenAI model. Uses `functools.partial` to fix these parameters for the `summarize_article_openai` function, creating a specific `task` callable. It then retrieves the first example from the previously uploaded dataset and invokes the `task` on it, printing the wrapped output to demonstrate its functionality.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/summarization.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport textwrap\nfrom functools import partial\n\ntemplate = \"\"\"\nSummarize the article in two to four sentences:\n\nARTICLE\n=======\n{article}\n\nSUMMARY\n=======\n\"\"\"\ngpt_4o = \"gpt-4o-2024-05-13\"\ntask = partial(summarize_article_openai, prompt_template=template, model=gpt_4o)\ntest_example = dataset.examples[0]\nprint(textwrap.fill(await task(test_example), width=100))\n```\n\n----------------------------------------\n\nTITLE: Downloading Trace Dataset from Phoenix\nDESCRIPTION: Code to retrieve spans data from Phoenix client to access LLM interaction traces for evaluation. This retrieves the spans dataframe containing the record of LLM interactions from a specified project.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/integrations/cleanlab.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\nspans_df = px.Client().get_spans_dataframe(project_name=[your_project_name])\nspans_df.head()\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI Calls with OpenInference - Python\nDESCRIPTION: Sets up OpenTelemetry tracing by configuring a `TracerProvider` and a `SimpleSpanProcessor` to export traces via HTTP to the Phoenix endpoint. Initializes and instruments the `OpenAIInstrumentor` to automatically capture trace data for OpenAI API interactions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/summarization.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk import trace as trace_sdk\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\n\nendpoint = \"http://127.0.0.1:6006/v1/traces\"\ntracer_provider = trace_sdk.TracerProvider()\ntracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Creating Phoenix Inferences Datasets from DataFrames - Python\nDESCRIPTION: Wraps the prepared production and training DataFrames into Phoenix Inferences objects with the defined schema and names ('production', 'training'). These objects are used as datasets within the Phoenix workflow for all subsequent analytics and visualizations. Ensure that the DataFrames and schema have been defined beforehand.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/credit_card_fraud_tutorial.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nprod_ds = px.Inferences(dataframe=prod_df, schema=schema, name=\"production\")\ntrain_ds = px.Inferences(dataframe=train_df, schema=schema, name=\"training\")\n```\n\n----------------------------------------\n\nTITLE: Wrapping Training Data and Schema into Phoenix Inferences Object in Python\nDESCRIPTION: Initializes a Phoenix Inferences object by combining the training DataFrame and its Schema, labeling the set as 'training'. Dependencies: valid 'train_df' DataFrame and 'train_schema' Schema from prior steps. The 'Inferences' object packages data for visualization and analysis; parameter 'name' tags this set for recognition in downstream UIs and APIs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/quickstart/phoenix-inferences/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_ds = px.Inferences(dataframe=train_df, schema=train_schema, name=\"training\")\n\n```\n\n----------------------------------------\n\nTITLE: Preparing Production DataFrame from Parquet File in Python\nDESCRIPTION: Loads production inference data from a remote Parquet file into a Pandas DataFrame for comparison. Similar to the training data loading snippet, this uses pandas and requires the provided URL to be accessible. The output is a DataFrame 'prod_df' containing production inference rows. Used for downstream drift and comparison analyses.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/quickstart/phoenix-inferences/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprod_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/cv/human-actions/human_actions_training.parquet\"\n)\n\n```\n\n----------------------------------------\n\nTITLE: Instrumenting LangChain for Observability with OpenInference Phoenix (Python)\nDESCRIPTION: Initializes observability by registering Phoenix and OpenInference tracer providers, then instruments the LangChain application to emit traces required for monitoring and evaluation. The instrumentation must occur before running the application to ensure all events are captured. Dependencies include phoenix, phoenix.otel, and openinference.instrumentation.langchain.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/qdrant_langchain_instrumentation_search_and_retrieval_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntracer_provider = register()\nLangChainInstrumentor(tracer_provider=tracer_provider).instrument(skip_dep_check=True)\n```\n\n----------------------------------------\n\nTITLE: Adding Session ID to Spans in JavaScript/TypeScript\nDESCRIPTION: Shows how to set the `session.id` attribute on OpenTelemetry spans using the `setSession` function from `@arizeai/openinference-core` and `context.with` from `@opentelemetry/api`. Spans created within the `context.with` callback will receive the specified session ID. Requires the `@arizeai/openinference-core` and `@opentelemetry/api` packages.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { context } from \"@opentelemetry/api\"\nimport { setSession } from \"@arizeai/openinference-core\"\n\ncontext.with(\n  setSession(context.active(), { sessionId: \"session-id\" }),\n  () => {\n      // Calls within this block will generate spans with the attributes:\n      // \"session.id\" = \"session-id\"\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Phoenix in Python\nDESCRIPTION: This code downloads a LlamaDataset, samples a subset of it, and uploads it to Phoenix. It uses a temporary directory to store the downloaded dataset. The `time_ns()` function is used to create a unique dataset name to avoid naming conflicts.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsample_size = 7\ndataset_name = \"EvaluatingLlmSurveyPaperDataset\"\nwith tempfile.TemporaryDirectory() as dir_name:\n    rag_dataset, documents = download_llama_dataset(dataset_name, dir_name)\ndataset = px.Client().upload_dataset(\n    dataset_name=f\"{dataset_name}_{time_ns()}\",\n    dataframe=rag_dataset.to_pandas().sample(sample_size, random_state=42),\n)\n```\n\n----------------------------------------\n\nTITLE: Uninstrumenting Auto-instrumentors (Python)\nDESCRIPTION: This code snippet shows how to permanently disable tracing by calling the `.uninstrument()` method on the auto-instrumentors. It provides examples for LangChain, LlamaIndex, and OpenAI instrumentors. This removes all tracing capabilities. This process needs the specific instrumentor classes for each framework to be imported.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/advanced/suppress-tracing.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nLangChainInstrumentor().uninstrument()\nLlamaIndexInstrumentor().uninstrument()\nOpenAIInstrumentor().uninstrument()\n# etc.\n```\n\n----------------------------------------\n\nTITLE: Loading and Sampling Datasets using HuggingFace Datasets and Phoenix in Python\nDESCRIPTION: Loads a conversational QA dataset using HuggingFace Datasets, samples a subset using pandas sampling, and prepares it for uploading to Phoenix. Requires the 'datasets' library, 'phoenix', and 'pandas'. The key parameters are 'path' and 'name' corresponding to the dataset directory and configuration. Returns a pandas DataFrame of size 'sample_size' that is used later for upload.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/internal/run_experiments.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom datasets import load_dataset\n\nimport phoenix as px\n\nsample_size = 5\npath = \"nvidia/ChatQA-Training-Data\"\nname = \"synthetic_convqa\"\ndf = load_dataset(path, name, split=\"train\").to_pandas().sample(sample_size, random_state=42)\ndf\n```\n\n----------------------------------------\n\nTITLE: Opening Phoenix UI Session in Python\nDESCRIPTION: Displays a message with a URL to access the Phoenix UI for visualization and further analysis of the evaluation results. This snippet helps users navigate to the interface where they can explore the metrics in detail.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/qdrant_langchain_instrumentation_search_and_retrieval_tutorial.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"ðŸš€ Open the Phoenix UI if you haven't already: {session.url}\")\n```\n\n----------------------------------------\n\nTITLE: Legacy One-Click Setup - Python\nDESCRIPTION: Initializes the Phoenix app and sets the global handler to \"arize_phoenix\" for legacy LlamaIndex applications to automatically collect traces. The phoenix app is launched within the notebook, and the user is directed to open the URL in the output. This setup allows users to proceed with their LlamaIndex application as usual.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Phoenix can display in real time the traces automatically\n# collected from your LlamaIndex application.\nimport phoenix as px\n# Look for a URL in the output to open the App in a browser.\npx.launch_app()\n# The App is initially empty, but as you proceed with the steps below,\n# traces will appear automatically as your LlamaIndex application runs.\n\nfrom llama_index.core import set_global_handler\n\nset_global_handler(\"arize_phoenix\")\n\n# Run all of your LlamaIndex applications as usual and traces\n# will be collected and displayed in Phoenix.\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application in Python\nDESCRIPTION: This snippet shows how to launch the Phoenix application programmatically using the `phoenix` library. It calls `px.launch_app()` to start a session, assigns it to the `session` variable using the walrus operator (:=), calls `session.view()` to potentially open the UI in the environment (like a notebook), and stores the session's URL in `session_url`. The `phoenix` library is required.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/log_traces_to_phoenix.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\n(session := px.launch_app()).view()\nsession_url = session.url\n```\n\n----------------------------------------\n\nTITLE: Configuring Evaluators and Evaluation Model for Phoenix LLM Evals in Python\nDESCRIPTION: Sets up evaluators including HallucinationEvaluator, QAEvaluator, and RelevanceEvaluator using an OpenAI LLM as the judge (model='gpt-4'). Requires phoenix.evals and valid OpenAI credentials. Returns evaluator instances for further use.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    HallucinationEvaluator,\n    OpenAIModel,\n    QAEvaluator,\n    RelevanceEvaluator,\n    run_evals,\n)\n\neval_model = OpenAIModel(model=\"gpt-4\")\nrelevance_evaluator = RelevanceEvaluator(eval_model)\nhallucination_evaluator = HallucinationEvaluator(eval_model)\nqa_evaluator = QAEvaluator(eval_model)\n```\n\n----------------------------------------\n\nTITLE: Transforming Wiki Toxic Dataset Labels in Python\nDESCRIPTION: Converts the numeric 'label' column to a boolean 'toxic' column, drops the original label column, and renames 'comment_text' to 'text' for standardization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_wiki_toxic.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf[\"toxic\"] = df[\"label\"].map(bool)\ndf = df.drop(columns=[\"label\"])\ndf = df.rename(columns={\"comment_text\": \"text\"})\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Opening Phoenix UI Session Information - Python\nDESCRIPTION: Prints the URL for the current Phoenix session, allowing the user to open the Phoenix UI and view evaluation results. Assumes the variable 'session' is an active Phoenix session object possessing a 'url' attribute. Outputs a formatted print statement with a clickable link.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nprint(f\"\\ud83d\\ude80 Open the Phoenix UI if you haven't already: {session.url}\")\n```\n\n----------------------------------------\n\nTITLE: Importing OpenInference Semantic Attributes - TypeScript\nDESCRIPTION: Shows the TypeScript import statement required to access the predefined semantic attribute keys from the OpenInference conventions package.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_17\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SemanticAttributes } from 'arizeai/openinfernece-semantic-conventions';\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-Commit Hooks\nDESCRIPTION: Command to install pre-commit hooks that automatically run before each git commit. These hooks apply formatters and other checks to ensure code quality and consistency.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Viewing Dataset as DataFrame (Python)\nDESCRIPTION: Retrieves the dataset object from the Phoenix client and displays its content as a pandas DataFrame. This allows inspecting the data structure and content that was uploaded to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset.as_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix and OpenAI API Keys\nDESCRIPTION: Sets up the environment variables required to connect to Phoenix Cloud and access the OpenAI API. It retrieves the API keys from the user using getpass and sets them as environment variables. This configuration allows the code to authenticate with the Phoenix and OpenAI services.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/few_shot_prompting.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\nif not os.environ.get(\"PHOENIX_CLIENT_HEADERS\"):\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = \"api_key=\" + getpass(\"Enter your Phoenix API key: \")\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Creating Phoenix Inference Datasets from DataFrames in Python\nDESCRIPTION: Instantiates `phoenix.Inferences` objects for both production and training data. These objects combine the respective DataFrames (`prod_df`, `train_df`) with the previously defined `schema` and assign names (\"production\", \"training\") for identification within the Phoenix application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/sentiment_classification_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprod_ds = px.Inferences(dataframe=prod_df, schema=schema, name=\"production\")\ntrain_ds = px.Inferences(dataframe=train_df, schema=schema, name=\"training\")\n```\n\n----------------------------------------\n\nTITLE: Customizing Phoenix Client HTTP Headers via Environment Variable in Python\nDESCRIPTION: Demonstrates setting custom HTTP headers for the Phoenix client by defining the PHOENIX_CLIENT_HEADERS environment variable. Useful when alternative authentication schemes or additional headers (e.g., api-key for Phoenix Cloud) are required.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = \"api-key=your-api-key,\"  # use `api-key` for Phoenix Cloud\n```\n\n----------------------------------------\n\nTITLE: Converting Phoenix Prompt to OpenAI Format (Deno JavaScript)\nDESCRIPTION: Utilizes the Phoenix prompts helper's `toSDK` function to transform the Phoenix prompt object into the specific format expected by the OpenAI JavaScript SDK. It also injects the provided value for the 'question' variable into the template. The resulting OpenAI invocation parameters are displayed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_openai_tutorial.ipynb#_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst openaiPrompt = Prompts.toSDK({ \n  sdk: \"openai\", \n  prompt: questionAskerPrompt, \n  variables: { question: \"How do I write Hello World in Deno?\" } \n})\n\nawait Deno.jupyter.md`\n  ### OpenAI Prompt\n\n  ```json\n  ${JSON.stringify(openaiPrompt, null, 2)}\n  ```\n`\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key\nDESCRIPTION: Imports `getpass` and `os` modules to handle the OpenAI API key. It checks if the `OPENAI_API_KEY` environment variable is set; if not, it securely prompts the user for the key using `getpass`. Finally, it ensures the key is available as an environment variable for the OpenAI client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Make sure you have an openAI key setup\nimport getpass\nimport os\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass.getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key - Python\nDESCRIPTION: Retrieves the OpenAI API key. It first checks the `OPENAI_API_KEY` environment variable. If the key is not found, it prompts the user for input using `getpass` for secure entry. The key is then set for both the `openai` library and the environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_agent_tracing_tutorial.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI with OpenInference in Python\nDESCRIPTION: Applies automatic instrumentation for OpenAI API calls using the OpenInference library. It creates an `OpenAIInstrumentor` instance and calls its `instrument` method, passing the existing `tracer_provider`. This ensures that subsequent calls made using the OpenAI client library will automatically generate OpenTelemetry spans.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/span_filtering_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Auto-instrumentors can still be used with this setup\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider, skip_dep_check=True)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application (Python)\nDESCRIPTION: Imports the `phoenix` library and launches the Phoenix application in the background. The `.view()` method typically opens the Phoenix user interface in a web browser tab for real-time visualization and analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\n(session := px.launch_app()).view()\n```\n\n----------------------------------------\n\nTITLE: Handling Tool Calls - Python\nDESCRIPTION: This function, `handle_tool_calls`, iterates through a list of `tool_calls`. For each call, it retrieves the corresponding function implementation from `tool_implementations`, extracts the arguments, executes the function, and appends the result to the list of messages. This function is decorated with `@tracer.chain()` implying it's part of a tracing system.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n@tracer.chain()\ndef handle_tool_calls(tool_calls, messages):\n    for tool_call in tool_calls:\n        function = tool_implementations[tool_call.function.name]\n        function_args = json.loads(tool_call.function.arguments)\n        result = function(**function_args)\n\n        messages.append({\"role\": \"tool\", \"content\": result, \"tool_call_id\": tool_call.id})\n    return messages\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: This snippet installs the necessary Python packages for the project, including the Vertex AI SDK, cloudpickle, pydantic, LangGraph, and Arize Phoenix related packages.  The packages are updated to the specified versions.  It's intended for use in a Jupyter notebook environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/google_agent_engine_tracing_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install --upgrade --user --quiet \\\n    \"google-cloud-aiplatform[agent_engines,langchain]==1.87.0\" \\\n    cloudpickle==3.0.0 \\\n    pydantic==2.11.2 \\\n    langgraph==0.2.76 \\\n    httpx \\\n    \"arize-phoenix-otel>=0.9.0\" \\\n    \"openinference-instrumentation-langchain>=0.1.4\"\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix in Notebook\nDESCRIPTION: Launches the Phoenix app within a notebook environment. Requires the `phoenix` package to be installed. This will launch the Phoenix UI within the notebook.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/instructor.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Implement and Test Query Execution Python\nDESCRIPTION: Defines a function `execute_query` that takes a SQL query string, executes it against the DuckDB database connection, fetches the results as a pandas DataFrame, and converts the results into a list of dictionaries. It then calls this function with the query generated in the previous step to test if the generated SQL is executable and produces results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef execute_query(query):\n    return conn.query(query).fetchdf().to_dict(orient=\"records\")\n\n\nexecute_query(query)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Prompts with DSPy MIPROv2\nDESCRIPTION: This snippet uses DSPy's `MIPROv2` optimizer to find the best prompt for the classification task. It prepares training data from previous examples and compiles the classifier with the optimizer.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef validate_classification(example, prediction, trace=None):\n    return example[\"label\"] == prediction[\"label\"]\n\n\n# Prepare training data from previous examples\ntrain_data = []\nfor _, row in ground_truth_df.iterrows():\n    example = dspy.Example(\n        prompt=row[\"input\"][\"prompt\"], label=row[\"expected\"][\"type\"]\n    ).with_inputs(\"prompt\")\n    train_data.append(example)\n\ntp = dspy.MIPROv2(metric=validate_classification, auto=\"light\")\noptimized_classifier = tp.compile(classifier, trainset=train_data)\n```\n\n----------------------------------------\n\nTITLE: Setting up SQLite Database and Evaluation Components\nDESCRIPTION: Imports evaluation components from Phoenix, downloads and extracts the Chinook SQLite sample database, and creates the database connection and SQL database object for the agent to query.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pycm import ConfusionMatrix\nfrom sklearn.metrics import classification_report\n\nfrom phoenix.evals import (\n    SQL_GEN_EVAL_PROMPT_RAILS_MAP,\n    SQL_GEN_EVAL_PROMPT_TEMPLATE,\n    OpenAIModel,\n    llm_classify,\n)\n\ntemp_dir = tempfile.mkdtemp()\nurl = \"https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip\"\nwith zipfile.ZipFile(BytesIO(requests.get(url).content), \"r\") as f:\n    f.extractall(temp_dir)\nengine = create_engine(f\"sqlite:///{os.path.join(temp_dir, 'chinook.db')}\")\nsql_database = SQLDatabase(engine)\n```\n\n----------------------------------------\n\nTITLE: Using `get_retrieved_documents` Helper Function in Phoenix (Python)\nDESCRIPTION: Shows how to use the pre-defined `phoenix.session.evaluation.get_retrieved_documents` helper function to easily extract retrieved documents suitable for Retrieval (RAG) Relevance evaluations. This function simplifies the process compared to writing a manual `SpanQuery`. Requires an initialized `px.Client()` instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_retrieved_documents\n\nretrieved_documents = get_retrieved_documents(px.Client())\nretrieved_documents\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Phoenix Client in Python\nDESCRIPTION: Instantiates a Phoenix client and uploads the prepared pandas DataFrame as a dataset. Specifies input keys (\"question\", \"knowledge\", \"answer\") and output key (\"true_label\") for dataset schema mapping. Adds a random suffix to dataset name for unique identification.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndataset_name = f\"hallu-eval-{token_hex(4)}\"  # adding a random suffix for demo purposes\n\nds = px.Client().upload_dataset(\n    dataframe=df,\n    dataset_name=dataset_name,\n    input_keys=[\"question\", \"knowledge\", \"answer\"],\n    output_keys=[\"true_label\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Trace Masking with Python OpenAI Instrumentor\nDESCRIPTION: Example showing how to create a TraceConfig object and apply it to the OpenAI instrumentor in Python. The configuration allows for customizing what data is hidden or limited in traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/advanced/masking-span-attributes.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation import TraceConfig\nconfig = TraceConfig(        \n    hide_inputs=...,\n    hide_outputs=...,\n    hide_input_messages=...,\n    hide_output_messages=...,\n    hide_input_images=...,\n    hide_input_text=...,\n    hide_output_text=...,\n    hide_embedding_vectors=...,\n    hide_llm_invocation_parameters=...,\n    base64_image_max_length=...,\n)\n\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nOpenAIInstrumentor().instrument(\n    tracer_provider=tracer_provider,\n    config=config,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Task Function to Invoke LangChain Extraction Chain in Python\nDESCRIPTION: Defines a function named 'task' that takes an input string and applies the previously configured LangChain extraction chain to it. This function returns a string result representing the parsed output from the language model, facilitating modular experiment execution by abstracting model invocation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/langchain_email_extraction.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef task(input) -> str:\n    return extraction_chain.invoke(input)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix Chain-of-Thought Prompting in Python\nDESCRIPTION: Installs the required Python packages needed for this tutorial, including the Phoenix SDK (arize-phoenix), datasets for loading the math problem data, and OpenInference instrumentation for OpenAI. This setup ensures all necessary libraries are available to run later code snippets involving dataset loading, prompt creation, and experiment execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/chain_of_thought_prompting.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qqqq \"arize-phoenix>=8.0.0\" datasets openinference-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix Tracer for Instrumentation - Python\nDESCRIPTION: Imports the register function from phoenix.otel and configures the tracer provider with a project name and optional auto-instrumentation. Required for enabling automatic telemetry and tracing on all compatible installed OpenInference libraries, particularly for LLM activity.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/mistralai.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Boto3 Session with Credentials in Python\nDESCRIPTION: This snippet demonstrates how to create a standard Boto3 session using explicit AWS access key ID, secret access key, and region name. This session can be used for authenticating with AWS services, including Bedrock.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evaluation-models.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport boto3\n\n# Create a Boto3 session\nsession = boto3.session.Session(\n    aws_access_key_id='ACCESS_KEY',\n    aws_secret_access_key='SECRET_KEY',\n    region_name='us-east-1'  # change to your preferred AWS region\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LlamaIndex, Phoenix, and Milvus (Python)\nDESCRIPTION: Installs necessary Python packages including `arize-phoenix` with evaluation, LlamaIndex, Langchain, OpenAI, GCSFS, and specific versions of `pymilvus` and `grpcio`. These packages are prerequisites for running the tutorial code.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/milvus_llamaindex_search_and_retrieval_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uq gcsfs \"langchain>=0.0.334\" \"arize-phoenix[evals,llama-index,embeddings]\" \"openai>=1\" 'httpx<0.28'\n```\n\nLANGUAGE: python\nCODE:\n```\n!pip install pymilvus==2.2.15\n!pip install --upgrade --force-reinstall grpcio==1.56.0\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries\nDESCRIPTION: This code imports the required Python libraries such as json, os, getpass, typing, jsonschema, pandas, openai, and phoenix.  These libraries provide the necessary functionality for interacting with the OpenAI API, handling API keys, and creating the Phoenix session.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_tracing_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport json\nimport os\nfrom getpass import getpass\nfrom typing import Any, Dict, Literal, TypedDict\n\nimport jsonschema\nimport pandas as pd\nfrom openai import OpenAI\n\nimport phoenix as px\n\npd.set_option(\"display.max_colwidth\", None)\n```\n```\n\n----------------------------------------\n\nTITLE: Preparing Data for Duplicate Joke Evaluation (Python)\nDESCRIPTION: Creates a new Pandas DataFrame (`eval_df`) selecting the span ID and LLM output messages from the downloaded spans data. It sets the span ID as the DataFrame index. A function `is_duplicate` checks if a joke content (extracted from `attributes.llm.output_messages`) has been seen before using a set `unique_jokes`. This function is applied to create a new 'label' column indicating whether each joke is a duplicate (True/False).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evaluating_traces_quickstart.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Create a new DataFrame with selected columns\neval_df = spans_df[[\"context.span_id\", \"attributes.llm.output_messages\"]].copy()\neval_df.set_index(\"context.span_id\", inplace=True)\n\n# Create a list to store unique jokes\nunique_jokes = set()\n\n\n# Function to check if a joke is a duplicate\ndef is_duplicate(joke_data):\n    joke = joke_data[0][\"message.content\"]\n    if joke in unique_jokes:\n        return True\n    else:\n        unique_jokes.add(joke)\n        return False\n\n\n# Apply the is_duplicate function to create the new column\neval_df[\"label\"] = eval_df[\"attributes.llm.output_messages\"].apply(is_duplicate)\n\n# Convert boolean to integer (0 for False, 1 for True)\neval_df[\"label\"] = eval_df[\"label\"]\n\n# Reset unique_jokes list to ensure correct results if the cell is run multiple times\nunique_jokes.clear()\n```\n\n----------------------------------------\n\nTITLE: Setting up Phoenix Tracing Instrumentation with LlamaIndex\nDESCRIPTION: Configures Phoenix tracing integration with LlamaIndex by registering the OpenInference trace endpoint and instrumenting LlamaIndex. Ensures that all relevant traces during data processing and querying are captured for observability.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n\n```\n\n----------------------------------------\n\nTITLE: Pretty Printing the Result\nDESCRIPTION: This helper function is used to format and print the key-value pairs of a dictionary. It takes a dictionary of results as input, iterates through the dictionary's items and prints each key-value pair, and finally prints a separator line.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndef pretty_print_result(result_dict: dict) -> str:\n    \"\"\"Format the output results\n\n    Parameters\n    ----------\n    result_dict : dict\n        Dictionary of results\n\n    Returns\n    -------\n    str\n        String of key and value pairs\n    \"\"\"\n    for key, value in result_dict.items():\n        print(f\"{key}: {value}\")\n    print(f\"\\n{'-'*50}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Reverting Migrations with Alembic Downgrade (Bash)\nDESCRIPTION: This command reverts the database schema to a previous migration state by downgrading Alembic migrations to a specific revision. Users must specify the target '<revision-id>', which is found in the Alembic migrations folder. Use with caution, as downgrading can cause data loss. The command outputs progress and logs to the terminal.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/src/phoenix/db/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nalembic downgrade <revision-id>\n```\n\n----------------------------------------\n\nTITLE: Performing Relevance Classification with GPT-3.5 Turbo in Python\nDESCRIPTION: Uses the llm_classify function as above, but with a GPT-3.5 Turbo model, to generate relevance predictions for a batch of evaluation samples. Retains reuse pattern with allowed output rails, template, and batch concurrency for fast processing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nrails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\nrelevance_classifications = llm_classify(\n    dataframe=df_sample,\n    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    concurrency=20,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Retrieves the OpenAI API key from the environment variables or prompts the user to enter it.  It checks for the 'OPENAI_API_KEY' environment variable and, if not found, prompts the user for input using `getpass`. The key is then stored in the environment variables.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_multimodal_image_reasoning.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Sampling DoQA Travel Dataset from HuggingFace with Message Length Filtering in Python\nDESCRIPTION: This snippet fetches the 'doqa_travel' split from nvidia/ChatRAG-Bench, calculates the maximum message length, filters for entries above half that length, and writes a 100-sample compressed CSV. Dependencies include HuggingFace's datasets package and Pandas. The 'messages' column must be present in the dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/ChatRAG-Bench.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nname = \"doqa_travel\"\ndf = load_dataset(\"nvidia/ChatRAG-Bench\", name)[\"test\"].to_pandas()\nprint(df.messages.apply(len).max())\ndf.loc[df.messages.apply(len) >= df.messages.apply(len).max() // 2].sample(\n    100, random_state=42\n).to_csv(f\"{name}_samples.csv.gz\", index=False)\ndf.sample(10)\n```\n\n----------------------------------------\n\nTITLE: Embedding Phoenix UI Inline in Notebook\nDESCRIPTION: Renders the Phoenix UI directly within a notebook cell, optionally adjusting the window height for better display.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/manage-the-app.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nsession.view()\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix Docker Container with Port Mapping using Bash\nDESCRIPTION: This snippet pulls the latest Phoenix docker image from Docker Hub and runs it, exposing port 6006 on localhost. This method facilitates running Phoenix in a containerized environment enabling trace collection in a portable and reproducible manner.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/prompt-flow.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\n\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Defining Primary Inference Set in Python\nDESCRIPTION: Creates a primary inference set by providing a pandas dataframe, matching schema, and a name identifier. This is the first step in setting up data for analysis in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/manage-the-app.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nprim_ds = px.Inferences(prim_df, prim_schema, \"primary\")\n```\n\n----------------------------------------\n\nTITLE: View Evaluation Results as DataFrame Python\nDESCRIPTION: Calls the 'get_evaluations()' method on the experiment object after evaluations have been completed. This method returns a Pandas DataFrame containing only the evaluation results for each run in the experiment. It provides a clear, tabular summary of how well the task outputs scored against each defined guideline.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nexperiment.get_evaluations()\n```\n\n----------------------------------------\n\nTITLE: Implementing Self-Consistency CoT Prompt with OpenAI and Phoenix\nDESCRIPTION: Creates a Self-Consistency Chain of Thought prompt that instructs the model to solve a problem multiple times independently and output the most frequent answer. Uses the Phoenix client to create and version the prompt.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/chain_of_thought_prompting.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconsistency_COT_template = \"\"\"\n\nYou are an evaluator who outputs the answer to a math word problem.\n\nFollow these steps:\n1. Solve the problem **multiple times independently**, thinking through the solution carefully each time.\n2. Show some of your reasoning for each independent attempt.\n3. Identify the integer answer that appears most frequently across your attempts.\n4. On a **new line**, output only this majority answer as a plain integer with **no words, commas, labels, units, or special characters**.\n\"\"\"\n\nparams = CompletionCreateParamsBase(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"system\", \"content\": consistency_COT_template},\n        {\"role\": \"user\", \"content\": \"{{Problem}}\"},\n    ],\n)\n\nself_consistency_COT = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"self consistency COT prompt\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring OTel Resources - Python\nDESCRIPTION: This Python snippet configures resources, specifically the project name. It leverages the `Resource` object and the `PROJECT_NAME` constant. It's designed to configure a TracerProvider with a resource containing metadata about the application.  This requires the `opentelemetry` and `phoenix.otel` modules.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-otel/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace as trace_api\nfrom phoenix.otel import Resource, PROJECT_NAME, TracerProvider\n\ntracer_provider = TracerProvider(resource=Resource({PROJECT_NAME: \"my-project\"}))\ntrace_api.set_tracer_provider(tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Legacy Setup for older LlamaIndex versions - Python\nDESCRIPTION: Initializes Phoenix and sets up tracing for versions of LlamaIndex before v0.10. It uses the `phoenix` library to launch the Phoenix application and sets the global handler to `arize_phoenix` to collect traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Phoenix can display in real time the traces automatically\n# collected from your LlamaIndex application.\nimport phoenix as px\n# Look for a URL in the output to open the App in a browser.\npx.launch_app()\n# The App is initially empty, but as you proceed with the steps below,\n# traces will appear automatically as your LlamaIndex application runs.\n\nimport llama_index\nllama_index.set_global_handler(\"arize_phoenix\")\n\n# Run all of your LlamaIndex applications as usual and traces\n# will be collected and displayed in Phoenix.\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI SDK with OpenInference (TypeScript)\nDESCRIPTION: Applies the OpenInference instrumentation specifically designed for the OpenAI Node.js SDK. This step intercepts calls to the OpenAI client methods and automatically creates OpenTelemetry spans with relevant attributes, ensuring that interactions with the OpenAI API are traced.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/tracing_openai_sessions_tutorial.ipynb#_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport OpenAI from 'npm:openai';\nimport { OpenAIInstrumentation } from \"npm:@arizeai/openinference-instrumentation-openai\";\n\nconst oaiInstrumentor = new OpenAIInstrumentation();\noaiInstrumentor.manuallyInstrument(OpenAI);\n```\n\n----------------------------------------\n\nTITLE: Concatenating Document Contents from Spans with Phoenix SpanQuery (Python)\nDESCRIPTION: Explains how to concatenate elements within a list-like span attribute (`retrieval.documents`) using `SpanQuery().concat()`. It specifies the attribute containing the list and the sub-attribute to concatenate (`document.content`), naming the output column `reference`. Uses the default separator (`\\n\\n`).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nquery = SpanQuery().concat(\n    \"retrieval.documents\",\n    reference=\"document.content\",\n)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Smolagents with OpenInference (Python)\nDESCRIPTION: Imports the `SmolagentsInstrumentor` from the `openinference.instrumentation.smolagents` library. It then creates an instance of the instrumentor and calls its `instrument` method, passing the previously configured `tracer_provider`. This automatically patches the Smolagents library to generate OpenTelemetry spans for its operations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/smolagents_tracing_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.smolagents import SmolagentsInstrumentor\n\nSmolagentsInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Generating a SQL Query from a Natural Language Question with OpenAI in Python\nDESCRIPTION: Demonstrates how to call the async generate_query function with a user-friendly question and print the resultant SQL. The function must be awaited, so it should be run in an async context, such as within a Jupyter notebook with nest_asyncio applied. Input: a natural language string. Output: prints the generated SQL query.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nquery = await generate_query(\"Who won the most games?\")\nprint(query)\n```\n\n----------------------------------------\n\nTITLE: Configure Collector Endpoint Directly - Python\nDESCRIPTION: This Python code snippet configures the collector endpoint directly using the `endpoint` argument in the `register` function. It demonstrates how to specify the fully qualified Phoenix server endpoint, and it requires the `phoenix.otel` module. The `endpoint` is `http://localhost:6006/v1/traces` for HTTP and `http://localhost:4317` by default for gRPC.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-otel/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\ntracer_provider = register(endpoint=\"http://localhost:6006/v1/traces\")\n```\n\n----------------------------------------\n\nTITLE: Prepare for Evaluation - Python\nDESCRIPTION: Sets the OpenAI API key environment variable using `getpass`. Imports and applies `nest_asyncio`, which is often needed in environments like notebooks for asynchronous operations, although its specific necessity here isn't explicitly stated.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key\")\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Initializing TracerProvider Python\nDESCRIPTION: This code initializes the OpenTelemetry TracerProvider to configure the tracing system. It sets span limits, creates an in-memory span exporter, and registers a span processor. It also configures an OTLP exporter to send trace data to an endpoint.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/multi-turn_chat_sessions.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntracer_provider = TracerProvider(span_limits=SpanLimits(max_attributes=1_000_000))\nin_memory_span_exporter = InMemorySpanExporter()\ntracer_provider.add_span_processor(SimpleSpanProcessor(in_memory_span_exporter))\nendpoint = \"http://127.0.0.1:4317\"\notlp_span_exporter = OTLPSpanExporter(endpoint=endpoint)\n```\n\n----------------------------------------\n\nTITLE: Inspect Single Experiment Run Result Python\nDESCRIPTION: Accesses a specific run result within the experiment object by index (here, the first run at index 0). This allows examination of the detailed structure and content of a single experiment run's record, which typically includes the input used, the output generated by the task, and potentially links to trace data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nexperiment[0]\n```\n\n----------------------------------------\n\nTITLE: Install Phoenix with Llama-Index Shell\nDESCRIPTION: Installs necessary Python packages for running the tutorial, including 'arize-phoenix' with the 'llama-index' extra, the 'datasets' library for data loading, and 'nest_asyncio' for handling nested asyncio event loops in environments like notebooks. These packages are prerequisites for setting up the environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -Uqqq \"arize-phoenix[llama-index]>=4.6\" datasets nest_asyncio\n```\n\n----------------------------------------\n\nTITLE: Recording Exceptions in OpenTelemetry Spans (TypeScript)\nDESCRIPTION: This snippet shows how to capture exceptions within a try-catch block and record them on an OpenTelemetry span using `span.recordException()`. It also demonstrates setting the span's status to ERROR using `span.setStatus()` for better error tracking.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_22\n\nLANGUAGE: typescript\nCODE:\n```\nimport opentelemetry, { SpanStatusCode } from '@opentelemetry/api';\n\n// ...\n\ntry {\n  doWork();\n} catch (ex) {\n  span.recordException(ex);\n  span.setStatus({ code: SpanStatusCode.ERROR });\n}\n```\n\n----------------------------------------\n\nTITLE: Running Experiment and Evaluations Simultaneously (Python)\nDESCRIPTION: Initiates a full experiment run, automatically executing the `task` function on the entire dataset and then applying the defined `evaluators` to the results immediately after the task completes for each example. This is a consolidated way to perform both steps.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n_ = run_experiment(dataset, task, evaluators)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Bedrock Tracing and Phoenix\nDESCRIPTION: This snippet imports the required Python libraries for tracing AWS Bedrock calls with OpenInference and sending the traces to Phoenix. It includes `json` for handling JSON data, `boto3` for interacting with AWS, `BedrockInstrumentor` from `openinference-instrumentation-bedrock` for instrumentation, and Phoenix-related modules.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nimport boto3\nfrom openinference.instrumentation.bedrock import BedrockInstrumentor\n\nimport phoenix as px\nfrom phoenix.otel import SimpleSpanProcessor, register\n```\n\n----------------------------------------\n\nTITLE: Invoking Google Generative AI with the Same Prompt\nDESCRIPTION: Uses the Vercel AI SDK to send the converted prompt to Google's Generative AI model. The code creates a Google model instance with gemini-2.0-flash-001 and displays the response in a markdown cell for comparison with the OpenAI response.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_vercel_ai_sdk.ipynb#_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst response = await generateText({\n  model: google(\"gemini-2.0-flash-001\"),\n  ...aiPrompt,\n})\n\nawait Deno.jupyter.md`\n  ### Google Generative AI Response\n\n  ${response.text || `\\`\\`\\`json\\n${JSON.stringify(response.steps[0].toolCalls, null, 2)}\\`\\`\\``}\n`\n```\n\n----------------------------------------\n\nTITLE: Adding User ID to Spans in Python\nDESCRIPTION: Demonstrates adding a `user.id` attribute to OpenTelemetry spans using the `using_user` context manager or decorator from `openinference.instrumentation`. Requires the `openinference-instrumentation` package and a non-empty string user ID. Spans created within the context or decorated function will inherit this attribute.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/add-metadata/customize-spans.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation import using_user\nwith using_user(\"my-user-id\"):\n    # Calls within this block will generate spans with the attributes:\n    # \"user.id\" = \"my-user-id\"\n    ...\n```\n\nLANGUAGE: python\nCODE:\n```\n@using_user(\"my-user-id\")\ndef call_fn(*args, **kwargs):\n    # Calls within this function will generate spans with the attributes:\n    # \"user.id\" = \"my-user-id\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Parsing and Dumping Constant AST Nodes in Python\nDESCRIPTION: Parses string representations of Python constants (None, integer, string) into ASTs using `ast.parse` and then prints their structure using `ast.dump`. This demonstrates how simple literal values are represented as `ast.Constant` nodes in the AST.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/src/phoenix/trace/dsl/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(ast.dump(ast.parse(\"None\", mode=\"eval\").body, indent=4))\nprint(ast.dump(ast.parse(\"1\", mode=\"eval\").body, indent=4))\nprint(ast.dump(ast.parse(\"'xyz'\", mode=\"eval\").body, indent=4))\n```\n\n----------------------------------------\n\nTITLE: Defining Ragas Tool Call Evaluator - Python\nDESCRIPTION: Defines an asynchronous function `tool_call_evaluator` using the Ragas `ToolCallAccuracy` metric. It prepares the agent's messages for Ragas using a helper function `conversation_to_ragas_sample` and calculates the tool call accuracy score for the agent's interaction.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/integrations/ragas.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_openai import ChatOpenAI\nfrom ragas.llms import LangchainLLMWrapper\nfrom ragas.metrics import AgentGoalAccuracyWithReference, ToolCallAccuracy\n\nasync def tool_call_evaluator(input, output):\n    sample = conversation_to_ragas_sample(output[\"messages\"], reference_equation=input[\"question\"])\n    tool_call_accuracy = ToolCallAccuracy()\n    return await tool_call_accuracy.multi_turn_ascore(sample)\n```\n\n----------------------------------------\n\nTITLE: Displaying the Q&A Classification Prompt Template\nDESCRIPTION: Prints the default template used by Phoenix to classify hallucinations in Q&A systems, which can be customized to improve classification performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(QA_PROMPT_TEMPLATE)\n```\n\n----------------------------------------\n\nTITLE: Creating Initial Customer Support Prompt Using Phoenix Client in Python\nDESCRIPTION: Defines an initial prompt configuration for a helpful customer service agent using the GPT-4 model, setting parameters like temperature and tool usage. The prompt instructs the model to choose up to three tools automatically to answer customer questions. Dependencies include the PhoenixClient SDK, the CompletionCreateParamsBase class, and PromptVersion utilities. Inputs are customer questions passed via placeholders, and outputs are prompt objects registered in Phoenix with versioning enabled for change tracking. The snippet structures the system and user messages and creates a named prompt for reuse.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/react-prompting.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nparams = CompletionCreateParamsBase(\n    model=\"gpt-4\",\n    temperature=0.5,\n    tools=tools,\n    tool_choice=\"auto\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"You are a helpful customer service agent.\\n            Your task is to determine the best tools to use to answer a customer's question.\\n            Output the tools and pick 3 tools at maximum.\\n            \"\"\",\n        },\n        {\"role\": \"user\", \"content\": \"{{questions}}\"},\n    ],\n)\n\nprompt_identifier = \"customer-support\"\n\nprompt = PhoenixClient().prompts.create(\n    name=prompt_identifier,\n    prompt_description=\"Customer Support\",\n    version=PromptVersion.from_openai(params),\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Schema for production inference data without ground truth\nDESCRIPTION: Creates a Schema object for the production dataset, which does not include an actual label column. Different schema definition may be needed based on dataset characteristics, supporting comparison visualization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/phoenix-inferences.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nprod_schema = px.Schema(\n    timestamp_column_name=\"prediction_ts\",\n    prediction_label_column_name=\"predicted_action\",\n    embedding_feature_column_names={\n        \"image_embedding\": px.EmbeddingColumnNames(\n            vector_column_name=\"image_vector\",\n            link_to_data_column_name=\"url\",\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving OpenAI API Key in Python\nDESCRIPTION: Retrieves the OpenAI API key required for making API calls. It first checks for the `OPENAI_API_KEY` environment variable. If absent, it securely prompts the user using `getpass` and sets the entered key back into the environment variable `os.environ['OPENAI_API_KEY']`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/span_filtering_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")\n```\n\n----------------------------------------\n\nTITLE: Dry-Running Experiment on Examples in Python\nDESCRIPTION: This snippet executes a dry run of an experiment using the defined dataset and task function. The `dry_run=3` argument indicates that the experiment will be run on 3 randomly selected examples.  It helps quickly validate the experiment setup without running on the entire dataset.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nexperiment = run_experiment(dataset, task, dry_run=3)\n```\n\n----------------------------------------\n\nTITLE: Setting evaluation sample size for summarization classification\nDESCRIPTION: Defines the number of samples to use for evaluation with estimated runtime information for different models. A larger sample size provides more accurate results but takes longer to run.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#####################\n## N_EVAL_SAMPLE_SIZE\n#####################\n# Eval sample size determines the run time\n# 100 samples: GPT-4 ~ 80 sec / GPT-3.5 ~ 40 sec\n# 1,000 samples: GPT-4 ~15-17 min / GPT-3.5 ~ 6-7min (depending on retries)\n# 10,000 samples GPT-4 ~170 min / GPT-3.5 ~ 70min\nN_EVAL_SAMPLE_SIZE = 100\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application (Cloud or Local)\nDESCRIPTION: Checks for the presence of a Phoenix API key in environment variables to decide between connecting to a cloud instance or launching a local Phoenix app. If no API key is found, it starts a local or dockerized Phoenix instance for local testing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/tracing_quickstart_openai.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nif \"PHOENIX_API_KEY\" in os.environ:\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\n    os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\nelse:\n    import phoenix as px\n    px.launch_app().view()\n```\n\n----------------------------------------\n\nTITLE: Registering OpenTelemetry Tracer for Phoenix in Python\nDESCRIPTION: This snippet registers the OpenTelemetry tracer provider using `phoenix.otel.register`. It configures tracing for a specified project name (`agents-cookbook`) and sets the endpoint for exporting traces to Phoenix. The `auto_instrument=True` flag enables automatic instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/ragas_agents_cookbook_phoenix.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# Setup OpenTelemetry\ntracer_provider = register(\n    project_name=\"agents-cookbook\",\n    endpoint=\"https://app.phoenix.arize.com/v1/traces\",\n    auto_instrument=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix Experiment - Python\nDESCRIPTION: Initiates an experiment using `phoenix.experiments.run_experiment`. It takes the previously uploaded `dataset`, the `solve_math_problem` function as the task to run on the dataset, and the `goal_evaluator` and `tool_call_evaluator` functions to evaluate the results. This logs the experiment run and its results to Phoenix for analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/integrations/ragas.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments import run_experiment\n\nexperiment = run_experiment(\n    dataset, task=solve_math_problem, evaluators=[goal_evaluator, tool_call_evaluator]\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Tool Function JSON Schema for LLM\nDESCRIPTION: Defines a function tool called 'search' that allows an LLM to request internet searches. The JSON schema specifies the function name, description, parameters, and required fields in the OpenAI tool format.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_vercel_ai_sdk.ipynb#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"search\",\n    \"description\": \"Query the internet for the answer to a question.\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"query\": {\n          \"type\": \"string\",\n          \"description\": \"Search term\"\n        }\n      },\n      \"required\": [\"query\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Compiling and Evaluating AST in Python\nDESCRIPTION: Parses a string expression (`xyz`) into an AST. The parsed AST is then compiled into a code object using `compile` and subsequently evaluated using `eval`. This demonstrates the process of turning an AST back into executable code and running it, providing a namespace for variable lookups.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/src/phoenix/trace/dsl/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nparsed = ast.parse(\"xyz\", mode=\"eval\")\ncompiled = compile(parsed, filename=\"\", mode=\"eval\")\n\neval(compiled, {\"xyz\": 42})\n```\n\n----------------------------------------\n\nTITLE: Set OpenAI API Key Python\nDESCRIPTION: Ensures the OpenAI API key is set as an environment variable. If the `OPENAI_API_KEY` environment variable is not found, it prompts the user to enter their key securely using `getpass` and sets it for the current session. This is required for authenticating requests to the OpenAI API.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Converting Dataset to Pandas DataFrame and Displaying Head in Python\nDESCRIPTION: Converts the loaded MS-MARCO datasets.Dataset into a pandas DataFrame for easier manipulation and displays the first rows. Requires pandas and a Dataset object. Inputs are an in-memory Dataset; outputs are the pandas DataFrame and display of its head(). Limited to the available RAM for large datasets.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_ms_marco.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nraw_df = dataset.to_pandas()\nraw_df.head()\n```\n\n----------------------------------------\n\nTITLE: Configure OpenInference Image Truncation (Python)\nDESCRIPTION: Sets an environment variable specific to OpenInference instrumentation (`OPENINFERENCE_BASE64_IMAGE_MAX_LENGTH`). By setting this to a very large value, it prevents the automatic truncation of large base64 encoded images within the captured traces. Required dependencies: `os`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENINFERENCE_BASE64_IMAGE_MAX_LENGTH\"] = (\n    \"10000000000\"  # this ensures that the image data is not truncated\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Asynchronous Task Function (Python)\nDESCRIPTION: Defines an asynchronous Python function `task` that takes an input dictionary (representing a dataset example's input) and uses `OpenAI.acomplete` to generate a response based on the input's \"instruction\". This function represents the LLM operation being evaluated.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nasync def task(input):\n    return (await OpenAI(model=\"gpt-3.5-turbo\").acomplete(input[\"instruction\"])).text\n```\n\n----------------------------------------\n\nTITLE: Generating Tabular Data Embeddings in Python with Phoenix\nDESCRIPTION: Code example illustrating how to generate embeddings for tabular data using the EmbeddingGeneratorForTabularFeatures class, with options for column selection and name mapping.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/generating-embeddings.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom arize.pandas.embeddings import EmbeddingGenerator, UseCases\n\ndf = df.reset_index(drop=True)\n# Instantiate the embeddding generator\ngenerator = EmbeddingGeneratorForTabularFeatures(\n    model_name=\"distilbert-base-uncased\",\n    tokenizer_max_length=512\n)\n\n# Select the columns from your dataframe to consider\nselected_cols = [...]\n\n# (Optional) Provide a mapping for more verbose column names\ncolumn_name_map = {...: ...}\n\n# Generate tabular embeddings and assign them to a new column\ndf[\"tabular_embedding_vector\"] = generator.generate_embeddings(\n    df,\n    selected_columns=selected_cols,\n    col_name_map=column_name_map # (OPTIONAL, can remove)\n)\n```\n\n----------------------------------------\n\nTITLE: Computing Precision@k Metrics for Document Retrieval in Python\nDESCRIPTION: Calculates precision@k for k=1,2 across sample queries by tracking how many of the retrieved documents were relevant. The code converts string relevance judgments ('relevant' or not) to binary values, then computes the cumulative precision as the ratio of relevant documents to total documents retrieved per query.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/qdrant_langchain_instrumentation_search_and_retrieval_tutorial.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nnum_relevant_documents_array = np.zeros(len(sample_query_df))\nnum_retrieved_documents = 2\nfor retrieved_document_index in range(0, num_retrieved_documents):\n    num_retrieved_documents = retrieved_document_index + 1\n    num_relevant_documents_array += (\n        sample_query_df[f\"openai_relevance_{retrieved_document_index}\"]\n        .map(lambda x: int(x == \"relevant\"))\n        .to_numpy()\n    )\n    sample_query_df[f\"openai_precision@{num_retrieved_documents}\"] = pd.Series(\n        num_relevant_documents_array / num_retrieved_documents\n    )\n\nsample_query_df[\n    [\n        \"openai_relevance_0\",\n        \"openai_relevance_1\",\n        \"openai_precision@1\",\n        \"openai_precision@2\",\n    ]\n]\n```\n\n----------------------------------------\n\nTITLE: Running Code Functionality Classifications with GPT-4\nDESCRIPTION: This code snippet runs code functionality classifications on the DataFrame using the `llm_classify` function. It uses the default code functionality prompt template and the GPT-4 model. The rails variable is used to hold the output to specific values based on the template. It extracts the 'label' column from the results and converts it to a list.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_functionality_classifications.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nrails = list(CODE_FUNCTIONALITY_PROMPT_RAILS_MAP.values())\nrelevance_classifications = llm_classify(\n    dataframe=df,\n    template=CODE_FUNCTIONALITY_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    concurrency=20,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Installing OpenInference and Dependencies with npm - Shell\nDESCRIPTION: Demonstrates how to install the @arizeai/openinference-vercel package using npm. This is required to enable utilities for ingesting Vercel AI SDK spans into platforms such as Arize and Phoenix. Dependencies must be installed in an environment with npm and Node.js configured.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/vercel-ai-sdk.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save @arizeai/openinference-vercel\n```\n\n----------------------------------------\n\nTITLE: Printing Phoenix URL - Python\nDESCRIPTION: This prints the URL of the active Phoenix session, which enables access to the application's interface for viewing the traces and analyzing the RAG pipeline's performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(\"phoenix URL\", px.active_session().url)\n```\n\n----------------------------------------\n\nTITLE: Running QA and Hallucination Evals\nDESCRIPTION: Runs QA correctness and hallucination evaluations using Phoenix LLM Evals. It initializes `QAEvaluator` and `HallucinationEvaluator` with OpenAI models, then uses `run_evals` to evaluate the question-answer pairs.  Dependencies include the `phoenix.evals` module. Results are returned as two pandas DataFrames.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import (\n    HallucinationEvaluator,\n    OpenAIModel,\n    QAEvaluator,\n    run_evals,\n)\n\nqa_evaluator = QAEvaluator(OpenAIModel(model_name=\"gpt-4-turbo-preview\"))\nhallucination_evaluator = HallucinationEvaluator(OpenAIModel(model_name=\"gpt-4-turbo-preview\"))\n\nqa_correctness_eval_df, hallucination_eval_df = run_evals(\n    evaluators=[qa_evaluator, hallucination_evaluator],\n    dataframe=qa_with_reference_df,\n    provide_explanation=True,\n    concurrency=20,\n)\n```\n\n----------------------------------------\n\nTITLE: Prompt for OpenAI API Key (Python)\nDESCRIPTION: Checks if the `OPENAI_API_KEY` environment variable is set. If not, it uses `getpass` to securely prompt the user to enter their OpenAI API key, setting the environment variable for subsequent API calls. Required dependencies: `os`, `getpass`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Saving Trace Dataset Python\nDESCRIPTION: Saves the trace dataset collected by Phoenix. The `save()` method allows the trace data to be persisted for later analysis and evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/anthropic_tracing_tutorial.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntrace_ds.save()\n```\n\n----------------------------------------\n\nTITLE: Loading WikiQA Dataset via Hugging Face Datasets in Python\nDESCRIPTION: This snippet loads the 'wiki_qa' dataset using the Hugging Face datasets library. The dataset is fetched from the online repository and assigned to the 'dataset' variable for use in subsequent data processing. Requires an active internet connection and the datasets library installed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_wiki_qa.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndataset = datasets.load_dataset(\"wiki_qa\")\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix in Notebook (Python)\nDESCRIPTION: Launches the Phoenix UI directly within a Python notebook environment. By default, traces are not persistent with this method unless configured otherwise.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai-agents-sdk.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Sending a Chat Completion Request to OpenAI Using Python\nDESCRIPTION: Creates a client instance for OpenAI, sends a chat completion request with specified model and message, and prints the generated response. Demonstrates interaction with OpenAI APIs.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_openai_tutorial.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport openai\n\nopenai_client = openai.OpenAI()\nresponse = openai_client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a haiku.\"}],\n    max_tokens=20,\n)\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Defining Embedding Function\nDESCRIPTION: Defines a function `get_embedding` which calls the OpenAI API to generate embeddings for a given text input using the text-embedding-ada-002 model. The function returns the embedding vector, enabling the use of vector-based calculations for optimizing prompts.  It's a crucial part of the prompt gradient optimization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\n\n# First we'll define a function to get embeddings for prompts\ndef get_embedding(text):\n    client = OpenAI()\n    response = client.embeddings.create(model=\"text-embedding-ada-002\", input=text)\n    return response.data[0].embedding\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix Tracer for LangChain\nDESCRIPTION: Python code to configure the Phoenix tracer with a project name and auto-instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langchain.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Conditional Authentication for Vertex AI in Python Notebooks\nDESCRIPTION: Handles user authentication based on the environment. If running inside Google Colab, it triggers Google account authentication dialog. Otherwise, it prompts the user to ensure their local `gcloud` authentication setup is correct to access Vertex AI services.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_vertex_ai_tracing_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif IS_COLAB:\n    authenticate_user()\nelse:\n    print(\n        \"If running locally, ensure that your gcloud is correctly configured to run with Vertex AI.\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Performing a Sample Query against the Index\nDESCRIPTION: Executes a question query on the index via the query engine, obtaining a response from the language model based on retrieved data chunks, demonstrating the system's retrieval and reasoning capabilities.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nresponse_vector = query_engine.query(\"What did the author do growing up?\")\n\n```\n\n----------------------------------------\n\nTITLE: Getting Span from Specific Context - JavaScript\nDESCRIPTION: Demonstrates how to retrieve a span associated with a specific context (`ctx`) that may not be the current active context. This is less common but necessary in scenarios involving manual context propagation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst ctx = getContextFromSomewhere();\nconst span = opentelemetry.trace.getSpan(ctx);\n\n// do something with the acquired span, optionally ending it if that is appropriate for your use case.\n```\n\n----------------------------------------\n\nTITLE: Running Frontend App Type Checking using npm (Shell)\nDESCRIPTION: Performs static type checking (likely using TypeScript) on the frontend application code (in `./app`) via the npm script named 'typecheck'. This helps catch potential type errors before runtime and should be run before submitting a pull request.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/CONTRIBUTING.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nnpm run typecheck\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key for LLM Evaluation in Python\nDESCRIPTION: Checks if the OPENAI_API_KEY environment variable is set; if not, securely prompts the user for their OpenAI API key and stores it in the environment. This enables authenticated API calls for LLM relevance evaluations. User input is hidden for security. Required before instantiating OpenAI models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"\\ud83d\\udd11 Enter your OpenAI API key: \")\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Prompt by Name (TypeScript)\nDESCRIPTION: This snippet demonstrates how to retrieve a prompt from the Phoenix server by its name using the TypeScript client. It imports the `getPrompt` function from `@arizeai/phoenix-client/prompts` and uses it to fetch the prompt identified by the name \"article-bullet-summarizer\". It returns a strongly-typed prompt object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/how-to-prompts/create-a-prompt.md#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { getPrompt } from \"@arizeai/phoenix-client/prompts\";\n\nconst prompt = await getPrompt({ name: \"article-bullet-summarizer\" });\n// ^ you now have a strongly-typed prompt object, in the Phoenix SDK Prompt type\n```\n\n----------------------------------------\n\nTITLE: Evaluating QA Correctness\nDESCRIPTION: This snippet evaluates the correctness of question-answer pairs using an LLM classifier. It uses the `llm_classify` function with the QA prompt template and rails. The evaluation results are added to the DataFrame, and the head of the DataFrame is displayed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/span_query_language.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nqa_eval = llm_classify(\n    qa_df,\n    model,\n    QA_PROMPT_TEMPLATE,\n    list(QA_PROMPT_RAILS_MAP.values()),\n    provide_explanation=True,\n)\nqa_eval[\"score\"] = (qa_eval.label[qa_eval.label.notnull()] == \"correct\").astype(int)\nqa_eval.head()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies Python\nDESCRIPTION: This snippet installs necessary Python packages, including datasets, OpenTelemetry instrumentation, OpenAI, Faker, and tiktoken, which are essential for the project's functionality. It uses pip with the -Uqqq flag for an upgrade and suppresses the output.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/multi-turn_chat_sessions.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uqqq datasets openinference-semantic-conventions openinference-instrumentation-openai faker openai-responses openai tiktoken\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix UI for Data Visualization and Analysis in Python\nDESCRIPTION: This code launches the Phoenix application with the query dataset as primary and the database dataset as corpus. It returns a session object and opens the Phoenix UI for interactive analysis of embeddings and clusters.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_search_and_retrieval_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n(session := px.launch_app(primary=query_ds, corpus=database_ds)).view()\n```\n\n----------------------------------------\n\nTITLE: Defining Jaro-Winkler Similarity Evaluator\nDESCRIPTION: Defines a function to calculate the Jaro-Winkler similarity between the task output and the expected output. This function converts both the output and the expected values into JSON strings before computing the similarity score.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef jarowinkler_similarity(output, expected) -> float:\n    return jarowinkler.jarowinkler_similarity(\n        json.dumps(output, sort_keys=True),\n        json.dumps(expected, sort_keys=True),\n    )\n```\n\n----------------------------------------\n\nTITLE: Invoking Agent with a Single Query - Python\nDESCRIPTION: Executes the initialized agent with a specific input query. The `invoke` method processes the query, potentially using the defined tools, and returns the agent's final response. This action is automatically traced and sent to the Phoenix application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_agent_tracing_tutorial.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nresponse = agent_executor.invoke({\"input\": \"What is 47 raised to the 5th power?\"})\nresponse\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Prompt from Phoenix\nDESCRIPTION: Fetches a prompt named 'question-searcher' from the Phoenix server using the Phoenix client. The code imports the Prompts module from phoenix-client and displays the retrieved prompt in a Jupyter notebook markdown cell.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_vercel_ai_sdk.ipynb#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport * as Prompts from \"npm:@arizeai/phoenix-client/prompts\"\n\nconst questionSearcherPrompt = await Prompts.getPrompt({ client: px, prompt: { name: \"question-searcher\" } })\n\nawait Deno.jupyter.md`\n  ### question-searcher prompt\n\n  \\`\\`\\`json\n  ${JSON.stringify(questionSearcherPrompt, null, 2)}\n  \\`\\`\\`\n  `\n```\n\n----------------------------------------\n\nTITLE: Defining Asynchronous Evaluation Function for Groq and Running Phoenix Experiment in Python\nDESCRIPTION: Defines an async function to send formatted prompts to Groq LLM and extract labels from responses. Runs this evaluation function on the dataset using Phoenix's asynchronous run_experiment utility to gather results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nasync def groq_eval(input):\n    formatted_prompt = prompt.format(variables=dict(input))\n    response = await groq.AsyncGroq().chat.completions.create(\n        **{**formatted_prompt, \"model\": groq_model}\n    )\n    return {\"label\": response.choices[0].message.content}\n```\n\n----------------------------------------\n\nTITLE: Evaluating Experiment Results Using Jarowinkler Similarity in Python\nDESCRIPTION: Applies the evaluate_experiment function from Phoenix experiments to compute metrics on the completed experiment using the previously defined Jarowinkler similarity function. This generates performance statistics indicative of how closely the predicted outputs match ground truth.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/langchain_email_extraction.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nevaluate_experiment(experiment, jarowinkler_similarity)\n```\n\n----------------------------------------\n\nTITLE: Renaming Columns in Pandas DataFrame for Consistent SQL Query Evaluation in Python\nDESCRIPTION: Renames columns 'query' to 'question' and 'sql_query' to 'query_gen' in the existing DataFrame in-place. This standardizes column names for later classification and evaluation. It assumes existence of a DataFrame named df.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf.rename(columns={\"query\": \"question\", \"sql_query\": \"query_gen\"}, inplace=True)\n```\n\n----------------------------------------\n\nTITLE: Preparing Questions Dataset for Evaluation\nDESCRIPTION: Transforms the questions dataframe to create a comprehensive dataset containing both questions and their associated document chunks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Construct a dataframe of the questions and the document chunks\nquestions_with_document_chunk_df = pd.concat([questions_df, document_chunks_df], axis=1)\nquestions_with_document_chunk_df = questions_with_document_chunk_df.melt(\n    id_vars=[\"text\"], value_name=\"question\"\n).drop(\"variable\", axis=1)\n# If the above step was interrupted, there might be questions missing. Let's run this to clean up the dataframe.\nquestions_with_document_chunk_df = questions_with_document_chunk_df[\n    questions_with_document_chunk_df[\"question\"].notnull()\n]\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix (Notebook)\nDESCRIPTION: This Python code imports the `phoenix` library and launches the Phoenix application within a notebook environment. It allows users to directly view traces collected in their Phoenix dashboard. It's important to note that traces may not persist if the notebook is closed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/hfsmolagents.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Adding Event with Attributes to Span - JavaScript\nDESCRIPTION: Demonstrates how to add an event to a span and include additional key-value attributes specific to that event using the options object in `span.addEvent()`. This provides more detailed context for the event.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_20\n\nLANGUAGE: javascript\nCODE:\n```\nspan.addEvent('some log', {\n  'log.severity': 'error',\n  'log.message': 'Data not found',\n  'request.id': requestId,\n});\n```\n\n----------------------------------------\n\nTITLE: Downloading the Benchmark Dataset\nDESCRIPTION: This code snippet downloads a CSV file containing validated Python code samples from a Google Cloud Storage bucket using `pandas.read_csv`. The DataFrame is then displayed to show the first few rows using `df.head()`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_functionality_classifications.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\n    \"https://storage.googleapis.com/arize-assets/phoenix/evals/code-functionality/validated_python_code_samples_2.csv\"\n)\n\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Evaluating with a Smaller Language Model (GPT-3.5-turbo)\nDESCRIPTION: Changes the target language model to \"gpt-3.5-turbo\" and re-runs the evaluation experiment on the synthetic dataset using the Arize Phoenix framework. Updates the experiment metadata to reflect the model being used for this specific evaluation run.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nTASK_MODEL = \"gpt-3.5-turbo\"\n\nexperiment = run_experiment(\n    synthetic_dataset,\n    task=task,\n    evaluators=[no_error, has_results],\n    experiment_metadata={\"model\": TASK_MODEL},\n)\n```\n\n----------------------------------------\n\nTITLE: Loading the Sentiment Analysis Dataset using Hugging Face Datasets\nDESCRIPTION: Loads the 'syeddula/fridgeReviews' dataset using the `datasets` library from Hugging Face. It selects the 'train' split, converts it to a pandas DataFrame, and displays the first few rows using `ds.head()`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/few-shot-prompting.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nds = load_dataset(\"syeddula/fridgeReviews\")[\"train\"]\nds = ds.to_pandas()\nds.head()\n```\n\n----------------------------------------\n\nTITLE: Combine Retrieval Results\nDESCRIPTION: Combines results to form a single evaluation dataframe. It gets the retriever spans dataframe from Phoenix and then concatenates the results from NDCG@2, precision@2, and hit metrics.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nretrievals_df = px.Client().get_spans_dataframe(\"span_kind == 'RETRIEVER'\")\nrag_evaluation_dataframe = pd.concat(\n    [\n        retrievals_df[\"attributes.input.value\"],\n        ndcg_at_2.add_prefix(\"ncdg@2_\"),\n        precision_at_2.add_prefix(\"precision@2_\"),\n        hit,\n    ],\n    axis=1,\n)\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatOpenAI Model with Tool Binding\nDESCRIPTION: Binds the configured prompt template to a GPT-4 chat model with zero temperature, associating the SubmitFinalAnswer tool for final response submission. This setup enables structured interactions for query generation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nquery_gen = query_gen_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools([\n    SubmitFinalAnswer\n])\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Phoenix Telemetry for Multiple LLM Providers in Python\nDESCRIPTION: Registers tracing with Phoenix's OpenTelemetry integration and instruments OpenAI, Anthropic, Groq, and VertexAI SDKs for telemetry collection. This instrumentation facilitates distributed tracing and monitoring of LLM API calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntracer_provider = register()\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\nAnthropicInstrumentor().instrument(tracer_provider=tracer_provider)\nGroqInstrumentor().instrument(tracer_provider=tracer_provider)\nVertexAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Previewing Generated Questions DataFrame - Python\nDESCRIPTION: Displays the first few rows (`.head()`) of the pandas DataFrame containing the questions generated by the LLM for each document chunk. This confirms that the generation process was successful and the output is structured as expected, with columns for each generated question.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nquestions_df.head()\n```\n\n----------------------------------------\n\nTITLE: Setting up Tracing with Phoenix OTEL\nDESCRIPTION: Registers a tracer provider with Phoenix, configuring an OTLP span exporter to send traces to the specified endpoint. This enables tracing the execution of the LlamaIndex application and sending the trace data to Phoenix for analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llama_index_multimodal_image_reasoning.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\ntracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\n```\n\n----------------------------------------\n\nTITLE: Installing OpenInference Instrumentation Dependency Using Shell\nDESCRIPTION: Installs the required Python package `openinference-instrumentation` which is necessary to enable session-based tracing and instrumentation features when using Python. This package provides the decorators and utilities for associating traces with sessions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/setup-sessions.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install openinference-instrumentation\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix as Global Handler for LlamaIndex Tracing\nDESCRIPTION: Configures LlamaIndex to automatically send trace data to the running Phoenix session. By setting the global handler to 'arize_phoenix', LlamaIndex operations (like querying, retrieval, generation) will be instrumented using the OpenInference standard and visualized in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_with_mongodb_and_llama_index.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nset_global_handler(\"arize_phoenix\")\n```\n\n----------------------------------------\n\nTITLE: Tracing Functions with Decorators\nDESCRIPTION: This snippet shows how to use the `@tracer.chain` decorator to trace an entire function. The input and output attributes are automatically set based on the function's parameters and return value. The status attribute is also set automatically.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@tracer.chain\ndef my_func(input: str) -> str:\n    return \"output\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix, LlamaIndex, and OpenAI\nDESCRIPTION: Installs the necessary Python packages including 'arize-phoenix' with extras for evaluations and LlamaIndex integration, 'openai' version 1 or higher, 'httpx' pinned below 0.28, 'gcsfs' for Google Cloud Storage access, 'nest-asyncio', and the OpenInference LlamaIndex instrumentation library. This setup prepares the environment for building the LlamaIndex application and tracing it with Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install \"arize-phoenix[evals,llama-index]\" \"openai>=1\" 'httpx<0.28' gcsfs nest-asyncio \"openinference-instrumentation-llama-index>=2.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Instantiating GPT-4 Model for Q&A Classification\nDESCRIPTION: Creates an instance of the OpenAI GPT-4 model with zero temperature for deterministic outputs, to be used for Q&A classification tasks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\n```\n\n----------------------------------------\n\nTITLE: Logging Evaluation Metrics to Phoenix Client Using Python\nDESCRIPTION: Imports the SpanEvaluations class from phoenix.trace to wrap evaluation DataFrames and logs them to the Phoenix client. This sends the QA correctness and hallucination evaluation data to Phoenix for downstream visualization and analysis. The px client must be initialized beforehand, and the evaluation DataFrames (qa_correctness_eval_df, hallucination_eval_df) must be available.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import SpanEvaluations\n\npx.Client().log_evaluations(\n    SpanEvaluations(dataframe=qa_correctness_eval_df, eval_name=\"Q&A Correctness\"),\n    SpanEvaluations(dataframe=hallucination_eval_df, eval_name=\"Hallucination\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Utilities for Data Processing and Visualization\nDESCRIPTION: Imports libraries for file operations, temporary directory management, and data visualization needed for the query agent demonstration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tempfile\nimport zipfile\nfrom io import BytesIO\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\n```\n\n----------------------------------------\n\nTITLE: Initialize Relevance Evaluator - Python\nDESCRIPTION: Imports `RelevanceEvaluator` and `run_evals` from `phoenix.evals`. Initializes an instance of the built-in `RelevanceEvaluator`, passing the previously initialized `eval_model` to it. This evaluator is configured to assess the relevance of retrieved documents.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import RelevanceEvaluator, run_evals\n\n# Initialize the built in Relevance evaluator\nrelevance_evaluator = RelevanceEvaluator(eval_model)\n\n```\n\n----------------------------------------\n\nTITLE: Previewing Document Chunks DataFrame - Python\nDESCRIPTION: Displays the first few rows (`.head()`) of the pandas DataFrame containing the extracted document chunks. This provides a quick view of the text content loaded into the DataFrame, confirming the structure is correct for subsequent processing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndocument_chunks_df.head()\n```\n\n----------------------------------------\n\nTITLE: Previewing Readability Classification with GPT-4 Turbo\nDESCRIPTION: Uses GPT-4 Turbo model for classification, providing a quick performance benchmark similar to GPT-4, and visualizes the results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nrails = list(CODE_READABILITY_PROMPT_RAILS_MAP.values())\nreadability_classifications = llm_classify(\n    dataframe=df,\n    template=CODE_READABILITY_PROMPT_TEMPLATE,\n    model=OpenAIModel(model=\"gpt-4-turbo-preview\", temperature=0.0),\n    rails=rails,\n    concurrency=20,\n)[\"label\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Creating LangChain ChatPromptTemplate with Query Generation System Prompt\nDESCRIPTION: Initializes a ChatPromptTemplate in LangChain by combining the system instructions with a message placeholder, preparing the prompt for subsequent use in a chat-based language model deployment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nquery_gen_prompt = ChatPromptTemplate.from_messages(\n    [\"system\", query_gen_system), (\"placeholder\", \"{messages}\")]\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Phoenix Multimodal Tracing\nDESCRIPTION: Installs the required Python packages (`arize-phoenix`, `openinference-instrumentation-openai`, `openai`) using pip. These libraries are necessary to run the subsequent Python example for sending multimodal traces to Phoenix. Requires a Python environment with pip installed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/advanced/multimodal-tracing.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -q \"arize-phoenix>=4.29.0\" openinference-instrumentation-openai openai\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Library with Embeddings Support (Python)\nDESCRIPTION: Installs the `arize-phoenix` Python package using pip, including optional dependencies for embedding analysis (`[embeddings]`). The `-Uq` flags ensure the package is upgraded to the latest version and the installation process is quiet.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/image_classification_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uq \"arize-phoenix[embeddings]\"\n```\n\n----------------------------------------\n\nTITLE: Instantiating - Phoenix LLM Evaluator - Python\nDESCRIPTION: Demonstrates how to instantiate a pre-built evaluator that leverages an LLM. This example uses the 'ConcisenessEvaluator', which requires an 'OpenAIModel' instance to interact with an LLM (gpt-4o) to perform the evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/datasets_and_experiments_quickstart.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals.models import OpenAIModel\nfrom phoenix.experiments.evaluators import ConcisenessEvaluator\n\nmodel = OpenAIModel(model=\"gpt-4o\")\nconciseness = ConcisenessEvaluator(model=model)\n```\n\n----------------------------------------\n\nTITLE: Initializing LlamaIndexInstrumentor - Python\nDESCRIPTION: Initializes and instruments the `LlamaIndexInstrumentor` with a tracer provider. This sets up the OpenTelemetry instrumentation to capture trace data from LlamaIndex operations. This step is crucial to collect trace data for LlamaIndex and send to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\nfrom phoenix.otel import register\n\ntracer_provider = register()\nLlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Defining Experiment Evaluator Functions\nDESCRIPTION: These Python functions define simple evaluators for the experiment. `no_error` checks if the task output contains an error, and `has_results` checks if the output contains database results. These functions return boolean values indicating the evaluation outcome.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/run-experiments.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef no_error(output) -> bool:\n    return not bool(output.get(\"error\"))\n\n\ndef has_results(output) -> bool:\n    return bool(output.get(\"results\"))\n```\n\n----------------------------------------\n\nTITLE: Running Frontend App Tests using npm (Shell)\nDESCRIPTION: Executes the test suite for the frontend application (located in the `./app` directory) using the npm script named 'test'. This command should be run to ensure frontend code changes pass all tests before submitting a pull request.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nnpm run test\n```\n\n----------------------------------------\n\nTITLE: Using `get_qa_with_reference` Helper Function in Phoenix (Python)\nDESCRIPTION: Demonstrates using the `phoenix.session.evaluation.get_qa_with_reference` helper function to extract question (`input`), answer (`output`), and concatenated reference documents (`reference`) suitable for Q&A on Retrieved Data evaluations. This simplifies the complex join query shown previously. Requires an initialized `px.Client()` instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/importing-and-exporting-traces/extract-data-from-spans.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_qa_with_reference\n\nqa_with_reference = get_qa_with_reference(px.Client())\nqa_with_reference\n```\n\n----------------------------------------\n\nTITLE: Retrieving a prompt by Tag in Python\nDESCRIPTION: Demonstrates how to fetch a prompt using a pre-defined tag, such as \"staging\", which is useful for environment-specific prompt management. The tag must be paired with the prompt's name or ID.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/how-to-prompts/using-a-prompt.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nclient = Client(\n # endpoint=\"https://my-phoenix.com\",\n)\n\n# Since tags don't uniquely identify a prompt version \n# it must be paired with the prompt identifier (e.g. name)\nprompt = client.prompts.get(prompt_identifier=\"my-prompt-name\", tag=\"staging\")\nprint(prompt.id)\nprompt.dumps()\n```\n\n----------------------------------------\n\nTITLE: Logging Span Evaluations to Phoenix\nDESCRIPTION: This code logs the prepared span evaluations to Phoenix. It uses the `SpanEvaluations` class to encapsulate the evaluation results and then uses the Phoenix client to log these evaluations, associating them with the corresponding spans.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import SpanEvaluations\n\npx.Client().log_evaluations(SpanEvaluations(eval_name=\"Q&A Correctness\", dataframe=evals_copy))\n```\n\n----------------------------------------\n\nTITLE: Converting LangChain Documents to Llama-Index Documents in Python\nDESCRIPTION: Transforms a list of LangChain documents into Llama-Index Document objects, preserving metadata and page content. Iterates through docs and appends converted Document instances to a new list for downstream vector indexing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndocuments = []\nfor doc in docs:\n    documents.append(Document(metadata=doc.metadata, text=doc.page_content))\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix (Command Line)\nDESCRIPTION: This bash command installs the `arize-phoenix` package and launches a local instance of Phoenix using the `phoenix serve` command.  It sets up a local instance accessible for tracing. The local instance will run on a default port which is 6006. \nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/hfsmolagents.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Tracing Chains with JSON Output\nDESCRIPTION: This snippet showcases tracing a chain function using a decorator, where the function returns a JSON-compatible dictionary. Input and output attributes are automatically derived from the function signature and return type.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict, Any\n\n@tracer.chain\ndef decorated_chain_with_json_output(input: str) -> Dict[str, Any]:\n    return {\"output\": \"output\"}\n\ndecorated_chain_with_json_output(\"input\")\n```\n\n----------------------------------------\n\nTITLE: Loading inference dataset into pandas DataFrame from Parquet file\nDESCRIPTION: Loads a training set dataset back into a pandas DataFrame from a specified URL, representing inference data for a computer vision model. This facilitates data manipulation and visualization prep.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/phoenix-inferences.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\n\ntrain_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/cv/human-actions/human_actions_training.parquet\"\n)\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix with OpenTelemetry (Python)\nDESCRIPTION: Imports the `register` function from `phoenix.otel` and registers the configured Phoenix instance with OpenTelemetry to enable trace collection for the specified project ('evaluating_traces_quickstart'). This returns a `tracer_provider` object used for instrumenting libraries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evaluating_traces_quickstart.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\ntracer_provider = register(project_name=\"evaluating_traces_quickstart\")\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix API Key and Endpoint (Python)\nDESCRIPTION: This snippet sets the Phoenix API key and collector endpoint as environment variables. The API key is used for authentication when sending traces to Phoenix Cloud. The endpoint specifies the URL of the Phoenix collector service.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-python.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: This snippet retrieves the OpenAI API key from the environment variables or prompts the user to enter it if not found. It then sets the API key as an environment variable for use by the OpenAI models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Hallucination Classification Evaluation\nDESCRIPTION: Imports required Python libraries and modules for dataset handling, LLM classification, visualization, and evaluation metrics. Sets pandas display options for better visibility of content.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport matplotlib.pyplot as plt\nimport openai\nimport pandas as pd\nfrom pycm import ConfusionMatrix\nfrom sklearn.metrics import classification_report\n\nfrom phoenix.evals import (\n    HALLUCINATION_PROMPT_RAILS_MAP,\n    HALLUCINATION_PROMPT_TEMPLATE,\n    OpenAIModel,\n    download_benchmark_dataset,\n    llm_classify,\n)\n\npd.set_option(\"display.max_colwidth\", None)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Experiment\nDESCRIPTION: Evaluates the results of the experiment using the defined Jaro-Winkler similarity function. This step calculates the similarity scores between the agent's output and the ground truth outputs from the dataset and reports the results within Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nevaluate_experiment(experiment, jarowinkler_similarity)\n```\n\n----------------------------------------\n\nTITLE: Define Evaluation Questions Python\nDESCRIPTION: Creates a Python list named `questions` containing sample natural language questions. This list serves as the input data for evaluating the performance of the Text2SQL model on various types of queries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nquestions = [\n    \"Which team won the most games?\",\n    \"Which team won the most games in 2015?\",\n    \"Who led the league in 3 point shots?\",\n    \"Which team had the biggest difference in records across two consecutive years?\",\n    \"What is the average number of free throws per year?\",\n]\n```\n\n----------------------------------------\n\nTITLE: Getting Current Active Span - JavaScript\nDESCRIPTION: Shows how to retrieve the currently active span from the context using `opentelemetry.trace.getActiveSpan()`. This is useful for adding information or performing actions on the span at different points within the span's duration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst activeSpan = opentelemetry.trace.getActiveSpan();\n\n// do something with the active span, optionally ending it if that is appropriate for your use case.\n```\n\n----------------------------------------\n\nTITLE: Initializing LLM Agent - Python\nDESCRIPTION: Creates an instance of the `Agent` class from the `agents` library. The agent is named 'Math Solver', given specific instructions, and assigned the previously defined `solve_equation` function as its tool.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/integrations/ragas.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom agents import Agent\n\nagent = Agent(\n    name=\"Math Solver\",\n    instructions=\"You solve math problems by evaluating them with python and returning the result\",\n    tools=[solve_equation],\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying the Evaluation LLM Model - Python\nDESCRIPTION: Instantiates the OpenAIModel object with the 'gpt-4o' model name, specifying which language model to use for prompt-based evaluation and classification. Assumes OpenAIModel is defined and integrated with the rest of the Phoenix evaluation pipeline. Inputs: string model name; Outputs: OpenAIModel instance. Relies on prior package setup and authentication as necessary.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\neval_model = OpenAIModel(model=\"gpt-4o\")\n```\n\n----------------------------------------\n\nTITLE: Waiting for Data Before Export (Python)\nDESCRIPTION: Pauses execution for a brief period (2 seconds) to allow tracing data from the RAG query executions to be fully ingested and made available within the Phoenix application before attempting to query and export it.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/ragas_retrieval_evals_tutorial.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom time import sleep\n\n# Wait a bit in case data hasn't beomme fully available\nsleep(2)\n```\n\n----------------------------------------\n\nTITLE: Specifying a custom GRPC endpoint - Python\nDESCRIPTION: This Python snippet shows how to specify a custom gRPC endpoint using `GRPCSpanExporter`.  This allows sending traces to a non-default gRPC endpoint. Requires `opentelemetry` and `phoenix.otel` modules.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-otel/README.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace as trace_api\nfrom phoenix.otel import TracerProvider, BatchSpanProcessor, GRPCSpanExporter\n\ntracer_provider = TracerProvider()\nbatch_processor = BatchSpanProcessor(\n    span_exporter=GRPCSpanExporter(endpoint=\"http://custom-endpoint.com:6789\")\n)\ntracer_provider.add_span_processor(batch_processor)\n```\n\n----------------------------------------\n\nTITLE: Viewing Single Experiment Run Result (Python)\nDESCRIPTION: Accesses the first result (`ExperimentRun`) in the `experiment` object. This allows inspecting the detailed structure of a single execution of the task function on a dataset example, including input, output, and other metadata.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nexperiment[0]\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix and Llama-Index Dependencies (Shell)\nDESCRIPTION: Installs the `arize-phoenix` library with the `llama-index` extra, ensuring version >= 4.6, along with `nest_asyncio` using pip. These are necessary prerequisites for running the code examples.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -Uqqq \"arize-phoenix[llama-index]>=4.6\" nest_asyncio\n```\n\n----------------------------------------\n\nTITLE: Installing Arize with AutoEmbeddings Support\nDESCRIPTION: Installs the Arize Python SDK along with optional dependencies required for the `AutoEmbeddings` feature, using pip with the quiet flag (`-qq`). This enables automatic embedding generation using the SDK.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n!pip install -qq 'arize[AutoEmbeddings]'\n```\n\n----------------------------------------\n\nTITLE: Evaluating Document Relevance with OpenAI\nDESCRIPTION: Creates and runs a relevance evaluator using OpenAI's gpt-4o-mini model to assess the relevance of retrieved documents to the query, generating scores and explanations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag_haystack.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import OpenAIModel, RelevanceEvaluator, run_evals\n\nrelevance_evaluator = RelevanceEvaluator(OpenAIModel(model=\"gpt-4o-mini\"))\n\nretrieved_documents_relevance_df = run_evals(\n    evaluators=[relevance_evaluator],\n    dataframe=retrieved_documents_df,\n    provide_explanation=True,\n    concurrency=20,\n)[0]\n```\n\n----------------------------------------\n\nTITLE: Filtering Phoenix Trace DataFrame to Include Only Rows with Tool Calls in Python\nDESCRIPTION: Filters the trace dataframe to retain only those rows where the 'tool_call' list is non-empty, ensuring subsequent evaluations focus on traces involving tool usage by the agent.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/amazon_bedrock_agents_tracing_and_evals.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntrace_df = trace_df[trace_df[\"tool_call\"].apply(lambda x: len(x) > 0)]\n\ntrace_df.head()\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application in Python\nDESCRIPTION: Starts the Phoenix application UI or backend service by invoking the launch_app method from the Phoenix SDK. This provides an interactive or programmatic interface for visualizing and managing machine learning experiments, including dataset and experiment tracking.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/langchain_email_extraction.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Extracting Groq Labels and Calculating Accuracy in Python\nDESCRIPTION: Processes Groq model output labels by extracting the last word per label, aligns with true labels, and calculates accuracy to evaluate hallucination detection performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nlabels = pd.json_normalize(exp_groq.as_dataframe().output).label.str.split(\"\\n\").str[-1]\nresult = pd.concat([labels, df.true_label], axis=1)\nprint(f\"Accuracy: {accuracy_score(result.true_label, result.label) * 100:.0f}%\")\nresult\n```\n\n----------------------------------------\n\nTITLE: Generating User ID Python\nDESCRIPTION: Defines a function `gen_user_id()` to generate a user ID. It uses Faker for realistic user IDs, along with random generation, and returns either a colon-separated string, a fake name or an integer.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/multi-turn_chat_sessions.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef gen_user_id():\n    p = random()\n    if p < 0.1:\n        return \":\" * randint(1, 5)\n    if p < 0.9:\n        return fake.name()\n    return int(abs(random()) * 1_000_000_000)\n```\n\n----------------------------------------\n\nTITLE: Parsing and Dumping Attribute AST Nodes in Python\nDESCRIPTION: Parses a dotted access chain string (`llm.token_count.completion`) into an AST and prints its structure using `ast.dump`. This illustrates how consecutive attribute accesses are represented as nested `ast.Attribute` nodes within the AST.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/src/phoenix/trace/dsl/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(ast.dump(ast.parse(\"llm.token_count.completion\", mode=\"eval\").body, indent=4))\n```\n\n----------------------------------------\n\nTITLE: Testing Task Function on Dataset Example in Python\nDESCRIPTION: Executes the 'task' function on the first example input from the uploaded Phoenix dataset. This serves as a correctness sanity check ensuring the LangChain extraction chain produces valid outputs before running full experiments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/langchain_email_extraction.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntask(dataset.examples[0].input)\n```\n\n----------------------------------------\n\nTITLE: Formatting Python Code using Ruff via Tox (Shell)\nDESCRIPTION: Formats Python code using the 'ruff' tool, executed via the 'tox' automation tool. This command ensures code adheres to the project's Python style guidelines before submission. It targets the 'ruff' environment defined in the tox configuration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/CONTRIBUTING.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\ntox run -e ruff\n```\n\n----------------------------------------\n\nTITLE: Generate VertexAI Text Content Python\nDESCRIPTION: Initializes the VertexAI SDK for a specific location and uses a `GenerativeModel` (e.g., Gemini) to generate text content. With the Phoenix OpenTelemetry tracer registered, this call will be automatically traced.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/vertexai.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport vertexai\nfrom vertexai.generative_models import GenerativeModel\n\nvertexai.init(location=\"us-central1\")\nmodel = GenerativeModel(\"gemini-1.5-flash\")\n\nprint(model.generate_content(\"Why is sky blue?\").text)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Local Server - Command Line\nDESCRIPTION: Starts a Phoenix observability instance on the user's local machine with default configuration. This command exposes Phoenixâ€™s dashboard, enabling local ingest and review of OpenAI API traces. Run after installing the arize-phoenix package.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Inspecting Mispredicted Instances where Human Label is Readable but Predicted as Unreadable\nDESCRIPTION: Filters the dataset to show examples where the model incorrectly labeled readable code as unreadable, facilitating error analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ndf[\"readability\"] = readability_classifications\n# inspect instances where ground truth was readable but evaluated to unreadable by the LLM\nfiltered_df = df.query('readable == False and readability == \"readable\"')\n\n# inspect first 5 rows that meet this condition\nresult = filtered_df.head(5)\nresult\n```\n\n----------------------------------------\n\nTITLE: Logging Tool Calling Evaluation Results back to Phoenix in Python\nDESCRIPTION: Sends the generated evaluation classifications to Phoenix by creating a SpanEvaluations object and logging it with a custom evaluation name. This action registers the evaluation results for visualization and further analysis on the Phoenix dashboard.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/amazon_bedrock_agents_tracing_and_evals.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\npx.Client().log_evaluations(\n    SpanEvaluations(eval_name=\"Tool Calling Eval\", dataframe=response_classifications),\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading Dataset from Phoenix by Name - Python\nDESCRIPTION: Fetches the uploaded dataset from Phoenix using its unique name and assigns it to variable 'ds'. Assumes the dataset was previously uploaded and exists within the Phoenix app context. Input: dataset_name (string). Output: Phoenix dataset object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/run_experiments_with_llama_index.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nds = px.Client().get_dataset(name=dataset_name)\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Llama-Index with Phoenix for Tracing\nDESCRIPTION: Configures tracing for Llama-Index operations using Phoenix instrumentation. Sets up a tracer provider endpoint (OTLP endpoint) and applies instrumentation to enable performance monitoring and debugging of Llama-Index activities.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ntracer_provider = register(endpoint=\"http://127.0.0.1:4317\")\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Launching the Phoenix Application UI\nDESCRIPTION: Launches the Phoenix web application and directs it to display the `conv_ds` dataset. This typically starts a local web server and prints a URL to access the interactive Phoenix UI in a browser for data exploration and cluster analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Click the link below to open in a view in Phoenix of ChatGPT data\npx.launch_app(conv_ds).view()\n```\n\n----------------------------------------\n\nTITLE: Specifying the Endpoint Directly with OTel - Python\nDESCRIPTION: This snippet shows how to specify the endpoint directly when initializing the `TracerProvider`.  This allows to customize the endpoint without relying on environment variables. Requires `opentelemetry` and `phoenix.otel` modules.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-otel/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace as trace_api\nfrom phoenix.otel import TracerProvider\n\ntracer_provider = TracerProvider(endpoint=\"http://localhost:4317\")\ntrace_api.set_tracer_provider(tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Creating a boto3 Session for AWS Bedrock\nDESCRIPTION: This snippet creates a `boto3` session to interact with AWS services, particularly AWS Bedrock.  This session is used to configure the environment for making API calls to AWS. It relies on boto3 being properly configured with AWS credentials.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsession = boto3.session.Session()\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Server Locally (Bash)\nDESCRIPTION: Starts a local Phoenix server instance for development and tracing. Assumes the 'arize-phoenix' package is installed. No parameters required, but can be customized according to the documentation for advanced deployments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/groq.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Retrieving Spans Data for Analysis\nDESCRIPTION: Uses the Phoenix Client to fetch spans data as a pandas DataFrame for analysis, displaying key information like span names, kinds, inputs, and retrieved documents.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nspans_df = px.Client().get_spans_dataframe()\nspans_df[[\"name\", \"span_kind\", \"attributes.input.value\", \"attributes.retrieval.documents\"]].head()\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key as an Environment Variable - Bash\nDESCRIPTION: Adds the OpenAI API key to the process environment, enabling authenticated requests to the OpenAI API. Substitute '[your_key_here]' with your actual API key. This is required for any communication with OpenAI's cloud services.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=[your_key_here]\n```\n\n----------------------------------------\n\nTITLE: Defining a Prompt Classification Task with DSPy\nDESCRIPTION: This snippet defines a `PromptClassifier` signature using DSPy for classifying prompts as either 'benign' or 'jailbreak'. It creates a DSPy `Predict` module using this signature.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n# Define the prompt classification task\nclass PromptClassifier(dspy.Signature):\n    \"\"\"Classify if a prompt is benign or jailbreak.\"\"\"\n\n    prompt = dspy.InputField()\n    label = dspy.OutputField(desc=\"either 'benign' or 'jailbreak'\")\n\n\n# Create the basic classifier\nclassifier = dspy.Predict(PromptClassifier)\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix OTEL Package (Bash)\nDESCRIPTION: Installs the `arize-phoenix-otel` package using pip. This package is required for sending trace data to Phoenix, regardless of the deployment method (Cloud, CLI, Docker).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/haystack.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for GPT Analysis\nDESCRIPTION: Installs necessary Python packages including `openai` for API interaction, `ipywidgets` for interactive elements, `pandas` for data handling, and `httpx` (pinned version) for HTTP requests using pip. The `-qq` flag ensures quiet installation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install -qq \"openai>=1\" ipywidgets pandas 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Getting Trace Dataset and Head Python\nDESCRIPTION: Retrieves the trace dataset from Phoenix and displays the first few rows of the spans dataframe. The `get_trace_dataset` method retrieves the trace data, and `get_spans_dataframe().head()` displays the first few rows in a human-readable format.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/anthropic_tracing_tutorial.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntrace_ds = px.Client().get_trace_dataset(project_name=\"anthropic-tools\")\ntrace_ds.get_spans_dataframe().head()\n```\n\n----------------------------------------\n\nTITLE: Specifying grpcio-tools Dependency in Python\nDESCRIPTION: Declares a required dependency on the `grpcio-tools` Python package, specifically version 1.54.3. This package provides the necessary tools for generating Python code from Protocol Buffer (.proto) definitions for use with gRPC.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/requirements/compile-protobuf.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ngrpcio-tools==1.54.3\n```\n\n----------------------------------------\n\nTITLE: Importing Instrumentation via Module Import\nDESCRIPTION: Method to include the OpenTelemetry instrumentation in an application by importing it at the top of the main program entrypoint.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-ts.md#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n// main.ts or similar\nimport \"./instrumentation.ts\"\n```\n\n----------------------------------------\n\nTITLE: Previewing Filtered Spans with Retrieval Documents - Python\nDESCRIPTION: Selects and displays the first few rows (`.head()`) of the 'attributes.input.value' and 'attributes.retrieval.documents' columns from the filtered spans DataFrame. This allows examination of the inputs that triggered retrieval and the actual documents that were retrieved, as captured in the Phoenix trace.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nspans_with_docs_df[[\"attributes.input.value\", \"attributes.retrieval.documents\"]].head()\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix in Docker - Bash\nDESCRIPTION: Starts the Phoenix Docker container, mapping the local port 6006 to the container. This is required for running Phoenix in a containerized environment, enabling local observability endpoints accessible at http://localhost:6006.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/mistralai.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Defining BedrockModel Parameters in Python\nDESCRIPTION: This snippet defines the `BedrockModel` class, specifying parameters for interacting with AWS Bedrock models. It includes common LLM parameters as well as fields for providing an existing Boto3 session or client for authentication.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evaluation-models.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass BedrockModel:\n    model_id: str = \"anthropic.claude-v2\"\n    \"\"\"The model name to use.\"\"\"\n    temperature: float = 0.0\n    \"\"\"What sampling temperature to use.\"\"\"\n    max_tokens: int = 256\n    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n    top_p: float = 1\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n    top_k: int = 256\n    \"\"\"The cutoff where the model no longer selects the words\"\"\"\n    stop_sequences: List[str] = field(default_factory=list)\n    \"\"\"If the model encounters a stop sequence, it stops generating further tokens. \"\"\"\n    session: Any = None\n    \"\"\"A bedrock session. If provided, a new bedrock client will be created using this session.\"\"\"\n    client = None\n    \"\"\"The bedrock session client. If unset, a new one is created with boto3.\"\"\"\n    max_content_size: Optional[int] = None\n    \"\"\"If you're using a fine-tuned model, set this to the maximum content size\"\"\"\n    extra_parameters: Dict[str, Any] = field(default_factory=dict)\n    \"\"\"Any extra parameters to add to the request body (e.g., countPenalty for a21 models)\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Uploading span evaluations to Phoenix (Python)\nDESCRIPTION: Logs the generated evaluations from the DataFrame to the specified Phoenix project. The DataFrame must contain a span ID column (used as index here) to link evaluations to traces and columns named 'label' or 'score' for display.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/feedback-and-annotations/evaluating-phoenix-traces.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.trace import SpanEvaluations\n\npx.Client().log_evaluations(SpanEvaluations(eval_name=\"Duplicate\", dataframe=eval_df))\n```\n\n----------------------------------------\n\nTITLE: Initializing Phoenix Client with Base URL in Python\nDESCRIPTION: Demonstrates creating a Phoenix client instance by specifying the base URL of the Phoenix server. The client communicates with the serverâ€™s REST API at the given address. The base_url parameter defaults to http://localhost:6006 if not provided.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.client import Client\n\nclient = Client(base_url=\"your-server-url\")  # base_url defaults to http://localhost:6006\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean QA Correctness Score\nDESCRIPTION: Calculates and displays the average QA correctness score from the `qa_correctness_eval_df` DataFrame. The `mean(numeric_only=True)` method is used to compute the average of numerical columns, typically the 'score' column.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nqa_correctness_eval_df.mean(numeric_only=True)\n```\n\n----------------------------------------\n\nTITLE: Adding More Evaluators to an Existing Experiment\nDESCRIPTION: This Python code demonstrates how to add additional evaluators to an experiment that has already been run. It uses `phoenix.experiments.evaluate_experiment` with the existing experiment object and a new list of evaluators.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/run-experiments.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.experiments import evaluate_experiment\n\nevaluators = [\n    # add evaluators here\n]\nexperiment = evaluate_experiment(experiment, evaluators)\n```\n\n----------------------------------------\n\nTITLE: Printing Instructions to Open Phoenix UI in Python\nDESCRIPTION: A single line of code that prints a user-friendly message with a Phoenix UI session URL to help users open the Phoenix interface for real-time telemetry visualization. It expects a session object with a URL attribute.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_tracing_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"ðŸ”¥ðŸ¦ Open the Phoenix UI if you haven't already: {session.url}\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Phoenix Inferences Dataset\nDESCRIPTION: Creates a Phoenix `Inferences` dataset object named `conv_ds`. This object wraps the `conversations_df` DataFrame and uses the defined `schema` to structure the data for Phoenix. The dataset is labeled as 'production'.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Create the dataset from the conversation dataframe & schema\nconv_ds = px.Inferences(conversations_df, schema, \"production\")\n```\n\n----------------------------------------\n\nTITLE: Loading GPT Conversation Data from CSV using Pandas\nDESCRIPTION: Reads conversation data from a CSV file hosted online into a pandas DataFrame named `conversations_df`. This DataFrame contains the prompt/response pairs and associated metadata.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconversations_df = pd.read_csv(\n    \"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/GENERATIVE/dataframe_llm_gpt.csv\"\n)\n```\n\n----------------------------------------\n\nTITLE: Scheduling Periodic Evaluations with Cron\nDESCRIPTION: This snippet shows a standard cron entry used to schedule the periodic execution of a Python script. The `* * * * *` pattern signifies execution every minute. This script is responsible for querying a LangChain application and running evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/online-evals.md#_snippet_0\n\nLANGUAGE: cron\nCODE:\n```\n* * * * * /path/to/python /path/to/run_evals.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix connection (Python)\nDESCRIPTION: Sets environment variables required for connecting to the Phoenix collector endpoint. Includes configuration for the Phoenix API key (replace placeholder) and the default cloud endpoint.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/feedback-and-annotations/evaluating-phoenix-traces.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Extracting and Displaying RAG Query Results (Python)\nDESCRIPTION: Accesses the generated textual response from a previously issued 'query_engine.query' call. Displays the response as a human-readable string, facilitating result verification and downstream processing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nresponse_vector.response\n```\n\n----------------------------------------\n\nTITLE: Overriding Tool Name and Description\nDESCRIPTION: This snippet demonstrates overriding the default tool name and description using the `@tracer.tool` decorator. This allows for custom labeling of tool spans.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/instrument-python.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@tracer.tool(\n    name=\"decorated-tool-with-overriden-name\",\n    description=\"overriden-tool-description\",\n)\ndef this_tool_name_should_be_overriden(input1: str, input2: int) -> None:\n    \"\"\"\n    this tool description should be overriden\n    \"\"\"\n\nthis_tool_name_should_be_overriden(\"input1\", 1)\n```\n\n----------------------------------------\n\nTITLE: Pulling a prompt by Name using TypeScript Client\nDESCRIPTION: Shows how to asynchronously retrieve the latest version of a prompt by name or ID in TypeScript using the Phoenix client library. Supports pulling by name or promptId for specific prompt versions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/how-to-prompts/using-a-prompt.md#_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { getPrompt } from \"@arizeai/phoenix-client/prompts\";\n\nconst prompt = await getPrompt({ name: \"my-prompt\" });\n// ^ the latest version of the prompt named \"my-prompt\"\n\nconst promptById = await getPrompt({ promptId: \"a1234\" })\n// ^ the latest version of the prompt with Id \"a1234\"\n```\n\n----------------------------------------\n\nTITLE: Viewing Experiment Results as DataFrame (Python)\nDESCRIPTION: Retrieves the results of the `experiment` object and displays them as a pandas DataFrame. This allows inspecting the output of the task function for each executed example in the dry run.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nexperiment.as_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Displaying Hallucination Evaluation Results\nDESCRIPTION: Displays the first few rows of the DataFrame containing the hallucination evaluation results. This allows for a quick inspection of the scores and explanations generated by the `HallucinationEvaluator`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nhallucination_eval_df.head()\n```\n\n----------------------------------------\n\nTITLE: Linting Frontend App Code using npm (Shell)\nDESCRIPTION: Runs the linter for the frontend application code (in `./app`) using the npm script named 'lint'. This command checks for potential code quality issues and style violations in the frontend code before submission.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/CONTRIBUTING.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nnpm run lint\n```\n\n----------------------------------------\n\nTITLE: Combining Documents with Relevance Evaluations\nDESCRIPTION: This snippet concatenates the `retrieved_documents_df` and `retrieved_documents_relevance_df` DataFrames along the columns axis to combine the original retrieved documents with their relevance evaluation scores. The relevance evaluation columns are prefixed with 'eval_'.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndocuments_with_relevance_df = pd.concat(\n    [retrieved_documents_df, retrieved_documents_relevance_df.add_prefix(\"eval_\")], axis=1\n)\ndocuments_with_relevance_df\n```\n\n----------------------------------------\n\nTITLE: Using Custom Delimiters with PromptTemplate in Python\nDESCRIPTION: Example showing how to use custom delimiters for variable identification in a PromptTemplate, allowing for flexible template structures beyond standard curly braces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evals-reference.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntemplate_text = \"My name is :/name-!). I am :/age-!) years old and I am from :/location-!).\"\nprompt_template = PromptTemplate(text=template_text, delimiters=[\":/\", \"-!)\"])\nprint(prompt_template.variables)\n# Output: ['name', 'age', 'location']\n```\n\n----------------------------------------\n\nTITLE: Passing Phoenix API Key Directly to Client in Python\nDESCRIPTION: Illustrates how to instantiate a Phoenix client by passing the API key directly via the api_key parameter. This method is an alternative to environment variables for authentication and injects the bearer token for request authorization.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.client import Client\n\nclient = Client(api_key=\"your-api-key\")\n```\n\n----------------------------------------\n\nTITLE: Aggregate Retrieval Results\nDESCRIPTION: Calculates the mean of numeric columns in the `rag_evaluation_dataframe`. This provides overall statistics for evaluating the RAG system's retrieval performance, summarizing the computed metrics across all queries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nresults = rag_evaluation_dataframe.mean(numeric_only=True)\nresults\n```\n\n----------------------------------------\n\nTITLE: Getting QA with Reference Data\nDESCRIPTION: Retrieves a DataFrame containing question-answer pairs with reference context from the active Phoenix session using `get_qa_with_reference`. This DataFrame is used for evaluating the quality of the LLM's responses.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.session.evaluation import get_qa_with_reference\n\nqa_with_reference_df = get_qa_with_reference(px.active_session())\nqa_with_reference_df\n```\n\n----------------------------------------\n\nTITLE: Logging Sessions Using OpenInference Instrumentation in Python\nDESCRIPTION: Demonstrates how to create and propagate a session identifier across multiple traces in Python using OpenTelemetry instrumentation with `openinference.instrumentation`. The snippet shows setting session-specific metadata attributes and using a session context manager to associate OpenAI chat completions spans to the same session. It requires the `openinference-instrumentation`, `openai`, and `opentelemetry` packages. The main parameters include `messages` (list of message dictionaries) and `session_id` (a UUID string). The output is the OpenAI chat response traced within the current session's span.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/setup-sessions.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport uuid\n\nimport openai\nfrom openinference.instrumentation import using_session\nfrom openinference.semconv.trace import SpanAttributes\nfrom opentelemetry import trace\n\nclient = openai.Client()\nsession_id = str(uuid.uuid4())\n\ntracer = trace.get_tracer(__name__)\n\n@tracer.start_as_current_span(name=\"agent\", attributes={SpanAttributes.OPENINFERENCE_SPAN_KIND: \"agent\"})\ndef assistant(\n  messages: list[dict],\n  session_id: str = str,\n):\n  current_span = trace.get_current_span()\n  current_span.set_attribute(SpanAttributes.SESSION_ID, session_id)\n  current_span.set_attribute(SpanAttributes.INPUT_VALUE, messages[-1].get('content'))\n\n  # Propagate the session_id down to spans crated by the OpenAI instrumentation\n  # This is not strictly necessary, but it helps to correlate the spans to the same session\n  with using_session(session_id):\n   response = client.chat.completions.create(\n       model=\"gpt-3.5-turbo\",\n       messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}] + messages,\n   ).choices[0].message\n\n  current_span.set_attribute(SpanAttributes.OUTPUT_VALUE, response.content)\n  return response\n\nmessages = [\n  {\"role\": \"user\", \"content\": \"hi! im bob\"}\n]\nresponse = assistant(\n  messages,\n  session_id=session_id,\n)\nmessages = messages + [\n  response,\n  {\"role\": \"user\", \"content\": \"what's my name?\"}\n]\nresponse = assistant(\n  messages,\n  session_id=session_id,\n)\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies - Python\nDESCRIPTION: Installs the necessary Python packages for building, tracing, and evaluating the RAG pipeline. Includes libraries for Arize Phoenix, Weaviate client, OpenAI client, and OpenInference instrumentation for OpenAI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_weaviate.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q arize-phoenix weaviate weaviate-client openai openinference-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Installing OpenInference Instrumentation OpenAI (Bash)\nDESCRIPTION: This command installs the `openinference-instrumentation-openai` package, which enables automatic tracing of calls made to the OpenAI library.  It must be installed before calling the `register` function to enable automatic tracing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-python.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Prompt by Name (Python)\nDESCRIPTION: This snippet shows how to retrieve a prompt from the Phoenix server by its name using the Python client. It uses the `px.Client().prompts.get` method to fetch the prompt identified by `prompt_name`. By default, it retrieves the latest version of the prompt.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/how-to-prompts/create-a-prompt.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprompt = px.Client().prompts.get(prompt_identifier=prompt_name)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Prompts from Phoenix\nDESCRIPTION: TypeScript code demonstrating how to fetch prompts from Phoenix using different identifiers such as prompt name, tag, or version ID.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/packages/phoenix-client/README.md#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { getPrompt } from \"@arizeai/phoenix-client/prompts\";\n\nconst prompt = await getPrompt({ name: \"my-prompt\" });\n// ^ you now have a strongly-typed prompt object, in the Phoenix SDK Prompt type\n\nconst promptByTag = await getPrompt({ tag: \"production\", name: \"my-prompt\" });\n// ^ you can optionally specify a tag to filter by\n\nconst promptByVersionId = await getPrompt({\n  versionId: \"1234567890\",\n});\n// ^ you can optionally specify a prompt version Id to filter by\n```\n\n----------------------------------------\n\nTITLE: Setting Mistral API Key as Environment Variable - Bash\nDESCRIPTION: Exports the MISTRAL_API_KEY environment variable, which is used by the MistralAI SDK to authenticate API requests. Ensures all subsequent SDK invocations are properly authorized on startup.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/mistralai.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport MISTRAL_API_KEY=[your_key_here]\n```\n\n----------------------------------------\n\nTITLE: Running a Simple Inference Query on the Local LiteLLMModel in Python\nDESCRIPTION: Executes a sample inference request to the instantiated LiteLLMModel with the input text 'Hello, world!'. This demonstrates the model's responsiveness and verifies correct setup of the local LLM pipeline.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel(\"Hello, world!\")\n```\n\n----------------------------------------\n\nTITLE: Initialize Phoenix Tracer with OpenInference in Python\nDESCRIPTION: Configures the Phoenix tracer in Python using the 'register' function with automatic instrumentation enabled. Associates traces with a specific project name for centralized monitoring.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/autogen-support.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix and OpenAI\nDESCRIPTION: Installs necessary Python packages for Phoenix, OpenAI instrumentation, and HTTP requests. Ensures compatibility by specifying package versions and dependencies required for tracing and API calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_openai_tutorial.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n!pip install -q \"arize-phoenix>=4.29.0\" openinference-instrumentation-openai openai 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Defining One-Shot Task\nDESCRIPTION: Defines a task function for one-shot prompting, which uses the `one_shot_prompt` created earlier to send reviews to the OpenAI API and retrieve sentiment predictions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/few_shot_prompting.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef one_shot_prompt_template(input):\n    client = OpenAI()\n    resp = client.chat.completions.create(\n        **one_shot_prompt.format(variables={\"Review\": input[\"Review\"]})\n    )\n    return resp.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Initializing PromptTemplate with Default Delimiters in Python\nDESCRIPTION: Example demonstrating how to initialize a PromptTemplate with a template string using default curly brace delimiters for variables, then accessing the detected variables.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evals-reference.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.evals import PromptTemplate\n\ntemplate_text = \"My name is {name}. I am {age} years old and I am from {location}.\"\nprompt_template = PromptTemplate(text=template_text)\n\nprint(prompt_template.variables)\n# Output: ['name', 'age', 'location']\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Notebook\nDESCRIPTION: This Python code launches the Phoenix application within a Jupyter Notebook environment. It initializes the Phoenix application and makes it accessible for viewing traces related to LlamaIndex workflows.  This requires the `arize-phoenix` package to be installed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex-1.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: LLM Classification Function in Python\nDESCRIPTION: Function for classifying dataframe rows using an LLM model with custom templates, classification rails, and optional explanations. Returns a dataframe with classification labels and optional explanations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evals-reference.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef llm_classify(\n    dataframe: pd.DataFrame,\n    model: BaseEvalModel,\n    template: Union[ClassificationTemplate, PromptTemplate, str],\n    rails: List[str],\n    system_instruction: Optional[str] = None,\n    verbose: bool = False,\n    use_function_calling_if_available: bool = True,\n    provide_explanation: bool = False,\n) -> pd.DataFrame\n```\n\n----------------------------------------\n\nTITLE: Display Phoenix UI Link (Python)\nDESCRIPTION: Checks if an active Phoenix session exists. If so, it prints the URL to the Phoenix UI where the experiment results, traces, and dataset can be viewed and analyzed. Required dependencies: `phoenix`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nif px.active_session():\n    px.active_session().view()\n```\n\n----------------------------------------\n\nTITLE: Get Retrieved Documents from Traces\nDESCRIPTION: This code retrieves data about retrieved documents from Phoenix traces. It uses a function `get_retrieved_documents` (presumably defined in Phoenix) to fetch document data associated with the traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nretrieved_documents_df = get_retrieved_documents(px.Client())\nretrieved_documents_df\n```\n\n----------------------------------------\n\nTITLE: Printing ConcisenessEvaluator Template (Python)\nDESCRIPTION: Prints the prompt template string used by the built-in Phoenix ConcisenessEvaluator, which is typically based on an OpenAI model.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/run_experiments_with_llama_index.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprint(ConcisenessEvaluator.template)\n```\n\n----------------------------------------\n\nTITLE: Pulling Phoenix Docker Image - Bash\nDESCRIPTION: Downloads the latest Phoenix Docker image from Docker Hub into your local environment. This forms the basis for running Phoenix as a self-contained container and is a prerequisite for spin-up via Docker.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key\nDESCRIPTION: Sets the OpenAI API key either from environment variables or through interactive input to authenticate subsequent API calls for GPT-4 usage.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/dspy_tracing_tutorial.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Display DataFrame Head (Python)\nDESCRIPTION: Displays the first few rows of the loaded pandas DataFrame. This helps in quickly inspecting the structure and initial content of the dataset. Input: A pandas DataFrame `df`. Output: Prints the head of the DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/multi_modal/image_classification_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Jaccard Similarity Evaluator in Python\nDESCRIPTION: This code defines a custom evaluator function that calculates the Jaccard similarity coefficient between the output and the expected answer strings. It tokenizes both strings into word sets, computes intersection and union, and returns a similarity float between 0 and 1. This function requires standard Python typing and string operations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/quickstart-datasets.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict\n\n\ndef jaccard_similarity(output: str, expected: Dict[str, Any]) -> float:\n    # https://en.wikipedia.org/wiki/Jaccard_index\n    actual_words = set(output.lower().split(\" \"))\n    expected_words = set(expected[\"answer\"].lower().split(\" \"))\n    words_in_common = actual_words.intersection(expected_words)\n    all_words = actual_words.union(expected_words)\n    return len(words_in_common) / len(all_words)\n```\n\n----------------------------------------\n\nTITLE: Exporting Spans Python\nDESCRIPTION: Defines a function `export_spans(prob_drop_root)` to export spans from the in-memory exporter. It randomizes span order, drops root spans based on the input probability, and exports spans to the OTLP exporter. It also prints session and trace counts.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/multi-turn_chat_sessions.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef export_spans(prob_drop_root):\n    \"\"\"Export spans in random order for receiver testing\"\"\"\n    spans = list(in_memory_span_exporter.get_finished_spans())\n    shuffle(spans)\n    for span in spans:\n        if span.parent is None and random() < prob_drop_root:\n            continue\n        otlp_span_exporter.export([span])\n    in_memory_span_exporter.clear()\n    session_count = len({id_ for span in spans if (id_ := span.attributes.get(\"session.id\"))})\n    trace_count = len({span.context.trace_id for span in spans})\n    print(f\"Exported {session_count} sessions, {trace_count} traces, {len(spans)} spans\")\n    return spans\n```\n\n----------------------------------------\n\nTITLE: Generating NLP Embeddings in Python with Phoenix\nDESCRIPTION: Code example showing how to generate embeddings for natural language processing data using the EmbeddingGenerator class with DistilBERT model.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/generating-embeddings.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom arize.pandas.embeddings import EmbeddingGenerator, UseCases\n\ndf = df.reset_index(drop=True)\n\ngenerator = EmbeddingGenerator.from_use_case(\n    use_case=UseCases.NLP.SEQUENCE_CLASSIFICATION,\n    model_name=\"distilbert-base-uncased\",\n    tokenizer_max_length=512,\n    batch_size=100\n)\ndf[\"text_vector\"] = generator.generate_embeddings(text_col=df[\"text\"])\n```\n\n----------------------------------------\n\nTITLE: Comparing GPT (OpenAI) and Groq DeepSeek Model Outputs in Python\nDESCRIPTION: Creates a combined pandas DataFrame aligning labels from OpenAI GPT and Groq DeepSeek outputs side-by-side for direct comparison of hallucination classification.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\npd.concat([result_openai.label.rename(\"gpt\"), result.rename({\"label\": \"deepseek\"}, axis=1)], axis=1)\n```\n\n----------------------------------------\n\nTITLE: Installing Llama-Index and Phoenix Packages using pip\nDESCRIPTION: Installs the arize-phoenix package with the llama-index extension and nest_asyncio. This ensures the necessary dependencies for integration between Phoenix and LlamaIndex are available.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -Uqqq \"arize-phoenix[llama-index]>=4.6\" nest_asyncio\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix and LlamaIndex\nDESCRIPTION: This command installs the necessary dependencies, including arize-phoenix with experimental and llama-index extensions. The -qq flag suppresses output, and the version is specified as >=2.0.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n!pip install -qq \"arize-phoenix[experimental,llama-index]>=2.0\"\n```\n\n----------------------------------------\n\nTITLE: Recording Exceptions in Spans (python)\nDESCRIPTION: Provides a pattern for attaching exception details to the current span by calling record_exception within an exception handler, in conjunction with setting the error status. Requires: opentelemetry.trace and opentelemetry.trace.Status/StatusCode. Input: an Exception object. Output: exception and error status are recorded in the trace. Developers are advised to catch specific exceptions where feasible.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/custom-spans.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode\n\ncurrent_span = trace.get_current_span()\n\ntry:\n    # something that might fail\n\n# Consider catching a more specific exception in your code\nexcept Exception as ex:\n    current_span.set_status(Status(StatusCode.ERROR))\n    current_span.record_exception(ex)\n```\n\n----------------------------------------\n\nTITLE: Building HTML Documentation with Make\nDESCRIPTION: Commands for cleaning previous builds and generating new HTML documentation from the reStructuredText files using make.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/api_reference/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake clean\nmake html\n```\n\n----------------------------------------\n\nTITLE: Running a Dry-Run Experiment on Selected Examples\nDESCRIPTION: Performs a dry run with three randomly selected dataset examples to evaluate the task execution without completing full processing. Useful for testing pipeline and data flow.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nexperiment = run_experiment(dataset, task, dry_run=3)\n```\n\n----------------------------------------\n\nTITLE: Load and Upload Dataset to Phoenix Python\nDESCRIPTION: Loads a sample dataset from the Hugging Face 'datasets' library ('nvidia/ChatQA-Training-Data'). It filters the dataset to include only the 'messages' and 'document' columns and samples a small subset ('sample_size=7'). The resulting Pandas DataFrame is then uploaded to the running Phoenix instance using 'px.Client().upload_dataset', named uniquely with a timestamp.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/guideline_eval.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsample_size = 7\npath = \"nvidia/ChatQA-Training-Data\"\nname = \"synthetic_convqa\"\ndf = load_dataset(path, name, split=\"train\").to_pandas()\ndf = df.loc[:, [\"messages\", \"document\"]]\ndataset = px.Client().upload_dataset(\n    dataset_name=f\"{name}_{time_ns()}\",\n    dataframe=df.sample(sample_size, random_state=42),\n)\n```\n\n----------------------------------------\n\nTITLE: Testing the Customer Support Agent with Discount Code Request\nDESCRIPTION: Executes the agent with a sample user inquiry about applying a discount code. This test verifies that the agent correctly routes the request to the appropriate function based on user input.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nrun_prompt(\"Hi, I'd like to apply to apply a discount code to my order.\")\n```\n\n----------------------------------------\n\nTITLE: Installing Tox with UV for Test Management\nDESCRIPTION: Command to install tox-uv globally, which uses UV for faster package management when running tests. This tool manages isolated virtual environments for various testing and development tasks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nuv tool install tox --with tox-uv\n```\n\n----------------------------------------\n\nTITLE: Inspecting Document Metadata in Llama-Index Documents (Python)\nDESCRIPTION: Displays the metadata for the first constructed Llama-Index Document object, providing insight into its structure and derived information. Useful for confirming document transformation correctness.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndocuments[0].metadata\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Application (Python)\nDESCRIPTION: Initializes and launches the Phoenix web application locally. This provides a user interface to view datasets, experiments, traces, and evaluations performed by the code. It is a prerequisite for viewing the results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/pairwise_eval.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Phoenix Connection\nDESCRIPTION: Configuration of environment variables required to connect to either Phoenix Cloud or self-hosted Phoenix instances. Includes API key and collector endpoint setup.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-ts.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# .env, or shell environment\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY=\"ADD YOUR API KEY\"\n# And Collector Endpoint for Phoenix Cloud\nPHOENIX_COLLECTOR_ENDPOINT=\"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix and Dependencies - Bash\nDESCRIPTION: Installs the necessary Python packages for integrating LlamaIndex with Phoenix, including `arize-phoenix-otel`. This package enables OpenTelemetry instrumentation for capturing and sending traces to the Phoenix platform.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Instantiating Anthropic Client in JavaScript\nDESCRIPTION: Initializes an Anthropic SDK client for subsequent LLM interactions, using the provided API key. Depends on the '@anthropic-ai/sdk' module and requires the API key string variable.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_cross_sdk_tutorial.ipynb#_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst anthropic = new Anthropic({ apiKey: anthropicApiKey });\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenInference Instrumentation for LlamaIndex\nDESCRIPTION: Configures the OpenInference instrumentation to capture trace data from LlamaIndex operations. This connects the application with Phoenix for monitoring.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register()\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Viewing Experiment Results\nDESCRIPTION: This code snippet shows how to view the results of the experiment, specifically showing the experiment results as a Pandas DataFrame.  This allows for inspection of the initial experiment run without evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nexperiment.as_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Packages (Cloud)\nDESCRIPTION: This bash command installs the `arize-phoenix-otel` package, which is used for integrating Phoenix with the OpenTelemetry protocol. This package is essential for sending tracing data from the application to the Phoenix backend.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/hfsmolagents.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Defining Task Function for Email Extraction with GPT-3.5-turbo\nDESCRIPTION: Defines the task function that invokes the LangChain agent with GPT-3.5-turbo with an email input. The task takes an input email and returns the output of the extraction chain as a string.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef task(input) -> str:\n    return extraction_chain.invoke(input)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix and Langchain\nDESCRIPTION: Installs necessary Python packages, including Arize Phoenix, Langchain, and related libraries, along with dependencies for OpenInference and Jaro-Winkler.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install arize-phoenix langchain langchain-core langchain-community langchain-benchmarks langchain-openai nest_asyncio jarowinkler openinference-instrumentation-langchain\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Server via Command Line\nDESCRIPTION: Installs the Phoenix package using pip and starts a local Phoenix server using the 'phoenix serve' command. Suitable for local development and testing with default endpoint.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/autogen-support.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\npip install arize-phoenix\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Confirming User Continuation to Evaluation Section\nDESCRIPTION: Prompts the user to confirm whether to continue to the evaluation section of the tutorial, stopping execution if the user chooses not to proceed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nif (\n    input(\"The tutorial is about to move on to the evaluation section. Continue [Y/n]?\")\n    .lower()\n    .startswith(\"n\")\n):\n    assert False, \"notebook stopped\"\n```\n\n----------------------------------------\n\nTITLE: Creating an Uninstrumented Bedrock Client\nDESCRIPTION: This snippet creates a `boto3` client for AWS Bedrock that is not instrumented with OpenInference. This client is used to demonstrate the difference between instrumented and uninstrumented calls to `invoke_model`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nuninstrumented_client = session.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n```\n\n----------------------------------------\n\nTITLE: Merging and Viewing Explanations for Sampled Classifications\nDESCRIPTION: Creates a merged dataframe of input code snippets, their predicted labels, and explanations to facilitate analysis of model's decision process.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nmerged_df = pd.merge(\n    small_df_sample, readability_classifications_df, left_index=True, right_index=True\n)\nmerged_df[[\"input\", \"output\", \"label\", \"explanation\"]].head()\n```\n\n----------------------------------------\n\nTITLE: Registering OpenAI Instrumentation in CommonJS Project\nDESCRIPTION: Configuration for CommonJS projects to automatically instrument OpenAI calls for trace collection, with simplified registration that relies on auto-instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/llm-traces-1/quickstart-tracing-ts.md#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\n// instrumentation.ts\n\n// ... rest of imports\nimport { registerInstrumentations } from \"@opentelemetry/instrumentation\";\nimport { OpenAIInstrumentation } from \"@arizeai/openinference-instrumentation-openai\";\n\n// ... previous code\n\nregisterInstrumentations({\n  instrumentations: [new OpenAIInstrumentation()],\n});\n```\n\n----------------------------------------\n\nTITLE: Exporting Phoenix Trace Data to a pandas DataFrame in Python\nDESCRIPTION: A concise code snippet that uses the Phoenix client to retrieve trace data as a pandas dataframe. This facilitates further analysis and evaluation of telemetry collected during the travel request processing. Requires the Phoenix client library to be installed and configured.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/openai_tracing_tutorial.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntrace_df = px.Client().get_spans_dataframe()\ntrace_df\n```\n\n----------------------------------------\n\nTITLE: Initializing Data Loading and Preprocessing in Python\nDESCRIPTION: This Python code snippet focuses on data loading and preprocessing steps that are essential for setting up the AI data pipeline in the Phoenix project. It likely involves importing data, cleaning, and preparing it for analysis or modeling.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load dataset\n data = pd.read_csv('data.csv')\n\n# Preprocess data\n data.fillna(0, inplace=True)\n data['processed_feature'] = data['raw_feature'].apply(lambda x: x * 2)\n\n```\n\n----------------------------------------\n\nTITLE: Viewing Dataset as Dataframe in Python\nDESCRIPTION: This snippet demonstrates how to view the uploaded dataset as a Pandas DataFrame within Phoenix.  It allows users to inspect the data structure and content for verification.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset.as_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Launch Local Phoenix Server via Docker Bash\nDESCRIPTION: Pulls the latest Phoenix Docker image from Docker Hub and runs it, mapping the container's port 6006 to the host's port 6006 to make the application accessible locally.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/vertexai.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Viewing Experiment Results as Dataframe in Python\nDESCRIPTION: This code demonstrates how to view the experiment results as a Pandas DataFrame. This allows users to inspect the outputs of the task function for each example.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nexperiment.as_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Adding Boolean Correctness Indicators to DataFrame in Python\nDESCRIPTION: Adds a new column 'is_correct' to the DataFrame with boolean values that reflect the correctness of each classification, facilitating downstream analysis and filtering operations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndf[\"is_correct\"] = boolean_classifications\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Endpoint in Python for Local Deployment\nDESCRIPTION: Sets environment variable for Phoenix collector endpoint to localhost:6006 in Python, enabling communication with a locally hosted Phoenix instance for data collection and tracing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/autogen-support.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Creating a boto3 Session in Python\nDESCRIPTION: This creates a boto3 session, which is a configured environment for interacting with AWS services. Ensure boto3 is configured with the appropriate credentials and region. Refer to AWS documentation for configuration instructions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/bedrock_tracing_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsession = boto3.session.Session()\n```\n\n----------------------------------------\n\nTITLE: Installing OpenInference Instrumentation and OpenAI - Bash\nDESCRIPTION: Installs both the openinference-instrumentation-openai and openai Python packages. Ensures that the project environment has all necessary dependencies to support OpenAI API calls and Phoenix/OTel instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-openai openai\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix Package in a Jupyter Notebook Environment\nDESCRIPTION: This snippet installs the arize-phoenix Python package within a notebook environment to enable launching the Phoenix application and integrating tracing capabilities directly in notebook workflows.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/prompt-flow.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix UI Session\nDESCRIPTION: Initializes the Phoenix application and launches a UI session, associating it with the previously loaded `TraceDataset`. The `session.view()` call is used to display the session within a notebook environment or print the URL.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_quickstart.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\n(session := px.launch_app(trace=trace_ds)).view()\nsession.view()\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix Docker Container - Bash\nDESCRIPTION: This command runs a Phoenix instance within a Docker container, exposing port 6006.  This exposes the Phoenix UI on `localhost:6006`. It allows users to access Phoenix through their browser.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/crewai.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Prompting for OpenAI API Key (Deno JavaScript)\nDESCRIPTION: Uses Deno's built-in `prompt` function to interactively ask the user for their OpenAI API key. This key is required to authenticate requests made to the OpenAI API.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_openai_tutorial.ipynb#_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst apiKey = prompt(\"Enter your OpenAI API key:\")\n```\n\n----------------------------------------\n\nTITLE: Defining Router Function\nDESCRIPTION: This code defines a router function to control the flow of conversation. The `router` function determines which tool to invoke based on user input. It inspects tool calls from the last message. If the tool calls exist, the `get_product_details` tool is chosen. Otherwise, the conversation ends.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/google_agent_engine_tracing_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef router(state: list[BaseMessage]) -> Literal[\"get_product_details\", \"__end__\"]:\n    \"\"\"Initiates product details retrieval if the user asks for a product.\"\"\"\n    # Get the tool_calls from the last message in the conversation history.\n    tool_calls = state[-1].tool_calls\n    # If there are any tool_calls\n    if len(tool_calls):\n        # Return the name of the tool to be called\n        return \"get_product_details\"\n    else:\n        # End the conversation flow.\n        return \"__end__\"\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries and Setting Up Environment in Python\nDESCRIPTION: Imports standard libraries (os, getpass), OpenAI SDK, pandas, wikipedia SDK, and essential components from LlamaIndex, Phoenix, and SQLAlchemy to support building query engines and instrumentation. Sets pandas display option for better visualization. This prepares the Python environment for executing subsequent code cells.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_sql_retriever_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport openai\nimport pandas as pd\nimport wikipedia\nfrom llama_index.core import Document, Settings\nfrom llama_index.core.indices import VectorStoreIndex\nfrom llama_index.core.query_engine import NLSQLTableQueryEngine, RouterQueryEngine\nfrom llama_index.core.selectors import LLMSingleSelector\nfrom llama_index.core.tools import QueryEngineTool\nfrom llama_index.core.utilities.sql_wrapper import SQLDatabase\nfrom llama_index.llms.openai import OpenAI\nfrom sqlalchemy import (\n    create_engine,\n    text,\n)\n\nimport phoenix as px\n\npd.set_option(\"display.max_colwidth\", 1000)\n```\n\n----------------------------------------\n\nTITLE: Configure LlamaIndex Instrumentation for Tracing\nDESCRIPTION: Sets up instrumentation for LlamaIndex to trace its operations and registers Phoenix OpenTelemetry exporter for monitoring.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_application_tracing_evaluating_and_analysis.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Printing the Phoenix UI URL\nDESCRIPTION: This code snippet prints the URL of the Phoenix UI, similar to a previous example, providing the user with access to the Phoenix UI for viewing evaluation results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nprint(\"The Phoenix UI:\", session.url)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App with Primary and Reference Inference Sets\nDESCRIPTION: Starts a Phoenix session comparing two inference sets, enabling analysis of data drift, alongside performance and quality metrics between cohorts.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/manage-the-app.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nsession = px.launch_app(prim_ds, ref_ds)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client (Deno JavaScript)\nDESCRIPTION: Creates an instance of the OpenAI client using the API key provided by the user. This client is the interface for making requests, such as chat completions, to the OpenAI platform.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_openai_tutorial.ipynb#_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst openai = new OpenAI({ apiKey: apiKey });\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix visualization app with primary inference dataset\nDESCRIPTION: Starts the Phoenix app session using the Inferences object as the primary data set. This launches an interactive visualization environment for analyzing model inferences.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/phoenix-inferences.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nsession = px.launch_app(primary=train_ds)\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Package in a Notebook Environment\nDESCRIPTION: Installs the Phoenix Python package in a notebook environment and launches the Phoenix interface using a Python command. Suitable for quick prototyping and testing within notebooks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/autogen-support.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\npip install arize-phoenix\n\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Navigating to Directory\nDESCRIPTION: This snippet changes the current directory in the terminal to the specified directory, which contains the example files for the MCP client-server tracing tutorial. It is a prerequisite for running the example.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mcp/tracing_between_mcp_client_and_server/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd tutorials/mcp/tracing_between_mcp_client_and_server\n```\n\n----------------------------------------\n\nTITLE: Deploying Phoenix with SQLite backend using Kubernetes\nDESCRIPTION: Command to deploy a single node Phoenix instance with a local SQLite database using kubectl and kustomize. This deploys the base configuration from the manifest directory.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/kustomize/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -k kustomize/base\n```\n\n----------------------------------------\n\nTITLE: Generating Status Code Python\nDESCRIPTION: Defines a function `rand_status_code()` that returns a random status code from the opentelemetry.sdk.trace.StatusCode enum, with a bias towards OK.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/multi-turn_chat_sessions.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef rand_status_code():\n    return choices(\n        [StatusCode.OK, StatusCode.ERROR, StatusCode.UNSET], k=1, weights=[0.98, 0.01, 0.01]\n    )[0]\n```\n\n----------------------------------------\n\nTITLE: Launch Local Phoenix App in Python Notebook\nDESCRIPTION: Imports the Phoenix library and launches the application server directly within the current Python notebook environment. Note that data in notebook instances is not persistent by default.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/vertexai.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Checking User Input to Continue Tutorial\nDESCRIPTION: Prompts the user to confirm if they want to continue to the analysis section and closes the Phoenix UI if proceeding.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nif (\n    input(\"The tutorial is about to move on to the analysis section. Continue [Y/n]?\")\n    .lower()\n    .startswith(\"n\")\n):\n    assert False, \"notebook stopped\"\n\npx.close_app()  # Close the Phoenix UI\n```\n\n----------------------------------------\n\nTITLE: Launching Local Phoenix Instance - Bash\nDESCRIPTION: Starts a local instance of Phoenix. This command runs the Phoenix server, which is necessary to collect and display traces. Useful for local development and testing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Fetching and Using Prompt from Phoenix (Python)\nDESCRIPTION: Retrieves a saved prompt from Phoenix using its identifier, formats its parameters, and uses them to make another call to the Anthropic API.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprompt = Client().prompts.get(prompt_identifier=prompt_identifier)\nresp = Anthropic().messages.create(**prompt.format())\nprint(resp.content[0].text)\n```\n\n----------------------------------------\n\nTITLE: Installing Core Dependencies (Python)\nDESCRIPTION: Installs the necessary Python packages for interacting with the Anthropic API, instrumenting it with OpenTelemetry for Phoenix, and using the Phoenix client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -Uqqq arize-phoenix-client arize-phoenix-otel anthropic requests beautifulsoup4 openinference-instrumentation-anthropic\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix Docker Container (Bash)\nDESCRIPTION: Starts a Phoenix container, mapping the container's port 6006 to the host's port 6006, making the local Phoenix UI accessible via `http://localhost:6006`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai-agents-sdk.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Set OpenAI API Key\nDESCRIPTION: This code checks for an existing OpenAI API key in the environment variables and prompts the user to enter one if it's not found.  It uses `getpass` to securely retrieve the API key.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/summarization.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif os.environ.get(\"OPENAI_API_KEY\") is None:\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Creating Phoenix Datasets from Dataframes\nDESCRIPTION: Wraps the baseline and recent dataframes with the defined schema to create Phoenix Inferences objects for analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_summarization_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nbaseline_ds = px.Inferences(dataframe=baseline_df, schema=schema, name=\"baseline\")\nrecent_ds = px.Inferences(dataframe=recent_df, schema=schema, name=\"recent\")\n```\n\n----------------------------------------\n\nTITLE: Importing Standard and Data Science Libraries in Python\nDESCRIPTION: Imports common Python libraries including json, logging, and sys required for serialization, logging, and system I/O tasks. No explicit functionality, but sets up global dependencies for subsequent cells.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport logging\nimport sys\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App with Corpus Data and Query Inference Sets\nDESCRIPTION: Launches a Phoenix session comparing a query inference set with a corpus dataset, suitable for evaluating retrieval-augmented generation systems.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/manage-the-app.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nsession = px.launch_app(query_ds, corpus=corpus_ds)\n```\n\n----------------------------------------\n\nTITLE: Running Evaluations on Experiment in Python\nDESCRIPTION: This snippet applies the defined evaluators to the experiment. The `evaluate_experiment` function computes the evaluation scores for each run in the experiment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nexperiment = evaluate_experiment(experiment, evaluators)\n```\n\n----------------------------------------\n\nTITLE: Installing arize-phoenix-otel - Shell\nDESCRIPTION: This snippet demonstrates how to install the `arize-phoenix-otel` library using `pip`. The `-Uq` flags are used to upgrade the package quietly.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-otel/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -Uq arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Accessing Phoenix Tracing Dashboard URL\nDESCRIPTION: Prints the URL of the Phoenix tracing interface for visualization and monitoring of traces collected during the RAG pipeline execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nprint(\"phoenix URL\", px.active_session().url)\n\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix tracer for LLM application monitoring\nDESCRIPTION: Python code to configure the Phoenix tracer provider with a project name and enable auto-instrumentation for OpenInference dependencies.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/guardrails-ai.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n  project_name=\"my-llm-app\", # Default is 'default'\n  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App with a Single Inference Set\nDESCRIPTION: Initiates a Phoenix session using a single inference set DataFrame, facilitating data and model analysis such as performance and quality without drift detection.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/manage-the-app.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nsession = px.launch_app(ds)\n```\n\n----------------------------------------\n\nTITLE: Viewing Phoenix UI Inline in Notebook Python\nDESCRIPTION: Displays the Phoenix user interface directly within an inline frame in a Jupyter Notebook or similar environment. The `height` parameter can be used to adjust the frame's height in pixels.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/api_reference/source/api/session.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nsession.view()\n```\n\nLANGUAGE: Python\nCODE:\n```\nsession.view(height=1200)\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary Libraries for DSPy Workflow\nDESCRIPTION: Imports core libraries including DSPy, OpenAI SDK, and Phoenix for implementing the retrieval-augmented generation application with observability features.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/dspy_tracing_tutorial.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport dspy\nimport openai\n\nimport phoenix as px\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App\nDESCRIPTION: This code snippet launches the Phoenix application, which is used for tracing and monitoring the RAG pipeline. This allows data capture for evaluation purposes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix and Dependencies\nDESCRIPTION: Installs the necessary Python packages, including arize-phoenix, datasets, and openinference-instrumentation-openai. The quiet flag (-qqq) minimizes output during installation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/react_prompting.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qqq \"arize-phoenix>=8.0.0\" datasets openinference-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App with Primary and Reference Sets in Python\nDESCRIPTION: Launches Phoenix with both primary and reference inference sets to compare data cohorts. This enables drift analysis in addition to model performance and data quality evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/manage-the-app.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsession = px.launch_app(prim_ds, ref_ds)\n```\n\n----------------------------------------\n\nTITLE: Running OpenAI Evaluation Experiment Using Phoenix in Python\nDESCRIPTION: Runs the Phoenix run_experiment utility with the uploaded dataset and the previously defined openai_eval function, effectively evaluating the dataset using the OpenAI LLM.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nexp_openai = run_experiment(ds, openai_eval)\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App in Python\nDESCRIPTION: This snippet launches the Phoenix application. This is essential to start the Phoenix server which is responsible for visualizing and analyzing the experiment data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Getting Evaluation Results in Python\nDESCRIPTION: This snippet retrieves the evaluation results from the experiment, allowing users to analyze the performance of the task function based on relevancy metrics.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nexperiment.get_evaluations()\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix from Command Line (Bash)\nDESCRIPTION: Starts a local Phoenix instance accessible via a web browser, typically at `http://localhost:6006`. Requires the `arize-phoenix` package to be installed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai-agents-sdk.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Defining Text Classification Tool Prompt (Python)\nDESCRIPTION: Defines the `MessageCreateParamsBase` for a text classification task using Anthropic's tool use, specifying a `print_classification` tool with a schema for returning category names and scores.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntools = [\n    {\n        \"name\": \"print_classification\",\n        \"description\": \"Prints the classification results.\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"categories\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"name\": {\"type\": \"string\", \"description\": \"The category name.\"},\n                            \"score\": {\n                                \"type\": \"number\",\n                                \"description\": \"The classification score for the category, ranging from 0.0 to 1.0.\",\n                            },\n                        },\n                        \"required\": [\"name\", \"score\"],\n                    },\n                }\n            },\n            \"required\": [\"categories\"],\n        },\n    }\n]\n\ncontent = \"\"\"\n    <document>\n    {{text}}\n    </document>\n\n    Use the print_classification tool. The categories can be Politics, Sports, Technology, Entertainment, Business.\\\n\"\"\"\n\n# https://docs.anthropic.com/en/api/messages\nparams = MessageCreateParamsBase(\n    model=\"claude-3-5-haiku-latest\",\n    max_tokens=4096,\n    tools=tools,\n    tool_choice={\"type\": \"tool\", \"name\": \"print_classification\"},\n    messages=[{\"role\": \"user\", \"content\": dedent(content)}],\n)\n```\n\n----------------------------------------\n\nTITLE: Generating a Meta Prompt\nDESCRIPTION: This snippet generates a meta prompt using the OpenAI API. The meta prompt is designed to generate a new prompt based on the original prompt and a set of example data. It constructs a prompt with the current `original_base_prompt` and uses the `ground_truth_df` example column as few shot examples. The generated prompt will be used to instruct the language model on the task.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/prompt-optimization.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmeta_prompt = \"\"\"\nYou are an expert prompt engineer. You are given a prompt, and a list of examples.\n\nYour job is to generate a new prompt that will improve the performance of the model.\n\nHere are the examples:\n\n{examples}\n\nHere is the original prompt:\n\n{prompt}\n\nHere is the new prompt:\"\"\"\n\noriginal_base_prompt =\n    prompt.format(variables={\"prompt\": \"example prompt\"}).get(\"messages\")[0].get(\"content\")\n\nclient = OpenAI()\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": meta_prompt.format(\n                prompt=original_base_prompt, examples=ground_truth_df[\"example\"].to_string()\n            ),\n        }\n    ],\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nnew_prompt = response.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Creating Trace Tree Python\nDESCRIPTION: Defines a context manager `trace_tree(tracer, n=5)` that creates a hierarchical trace structure. It uses a tracer to start and end spans, creating nested spans within a root span. The depth of the tree and whether to yield is randomized for more complexity.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/multi-turn_chat_sessions.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@contextmanager\ndef trace_tree(tracer, n=5):\n    if n <= 0:\n        yield\n        return\n    has_yielded = False\n    with tracer.start_as_current_span(\n        fake.city(),\n        attributes=dict(rand_span_kind()),\n        end_on_exit=False,\n    ) as root:\n        for _ in range(randint(0, n)):\n            with trace_tree(tracer, randint(0, n - 1)):\n                if not has_yielded and random() < 0.5:\n                    yield\n                    has_yielded = True\n                else:\n                    pass\n        if not has_yielded:\n            yield\n            has_yielded = True\n        for _ in range(randint(0, n)):\n            with trace_tree(tracer, randint(0, n - 1)):\n                pass\n    root.set_status(rand_status_code())\n    root.end(int(fake.future_datetime(\"+5s\").timestamp() * 10**9))\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Model for Dataset Generation in Python\nDESCRIPTION: This Python snippet sets up an OpenAI model instance (`gpt-4o`) using the `phoenix.evals.OpenAIModel` class. This model is intended to be used later for generating a synthetic dataset of math questions based on the provided template. `nest_asyncio.apply()` is repeated for notebook compatibility, and pandas display options are set.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/ragas_agents_cookbook_phoenix.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\nimport pandas as pd\n\nfrom phoenix.evals import OpenAIModel\n\nnest_asyncio.apply()\npd.set_option(\"display.max_colwidth\", 500)\n\n# Initialize the model\nmodel = OpenAIModel(model=\"gpt-4o\", max_tokens=1300)\n```\n\n----------------------------------------\n\nTITLE: Querying the LLM-Powered Application and Printing Responses in Python\nDESCRIPTION: Executes a set of predefined queries against the LLM application via the query engine. Iterates over queries with progress bar display and prints both the input query and the corresponding LLM-generated response. This simulates interactions with the application while generating telemetry data for tracing and analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_ops_overview.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm import tqdm\n\nqueries = [\n    \"How can I query for a monitor's status using GraphQL?\",\n    \"How do I delete a model?\",\n    \"How much does an enterprise license of Arize cost?\",\n    \"How do I log a prediction using the python SDK?\",\n]\n\nfor query in tqdm(queries):\n    response = query_engine.query(query)\n    print(f\"Query: {query}\")\n    print(f\"Response: {response}\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Labels and Explanations from Classification Results in Python\nDESCRIPTION: Retrieves classification labels and explanations from the relevance_classifications DataFrame and converts them to Python lists for further processing and analysis. Expected keys are 'label' and 'explanation'.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nlabels = relevance_classifications[\"label\"].tolist()\nexplanation = relevance_classifications[\"explanation\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Launch Phoenix Application\nDESCRIPTION: This code snippet launches the Phoenix application, allowing users to access the Phoenix UI. It provides instructions on how to open the UI using the output from the launch call.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/summarization.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\n\npx.launch_app().view()\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Tracing Application for Monitoring and Visualization\nDESCRIPTION: Starts the Phoenix tracing web application to monitor and visualize traces collected during pipeline execution. This enables real-time insight into the RAG process and data flow, aiding in debugging and performance analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\npx.launch_app().view()\n\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Monitoring Application\nDESCRIPTION: Initializes and launches the Phoenix monitoring application for observability of the agent's operations, returning a session that can be viewed in a web interface.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n(session := px.launch_app()).view()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Ragas Sample Conversion in Python\nDESCRIPTION: This snippet provides an example of using the `conversation_to_ragas_sample` function. It takes the message history from a previous agent run and converts it into a Ragas `MultiTurnSample`, including reference equation and answer for demonstration purposes. The resulting sample object is then printed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/ragas_agents_cookbook_phoenix.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Here is an example of the function in action\nsample = conversation_to_ragas_sample(\n    # This is a list of messages recorded for \"Calculate 15 + 28.\"\n    result[\"messages\"],\n    reference_equation=\"15 + 28\",\n    reference_answer=\"43\",\n)\nprint(sample)\n```\n\n----------------------------------------\n\nTITLE: Testing Agent with Employee Query\nDESCRIPTION: Sends a chat query to the initialized `ReActAgent` asking for the department of 'Sarah'. This triggers the agent's reasoning process, which should ideally select and use the 'ChromaEmployees' tool to find the answer. The agent's response is captured and printed.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/agentic_rag_tracing.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nresponse = agent.chat(\"What department is Sarah in?\")\nprint(str(response))\n```\n\n----------------------------------------\n\nTITLE: Defining Azure-Specific Parameters for OpenAIModel - Python\nDESCRIPTION: Lists additional fields required when initializing OpenAIModel with Azure integration, including API versioning and authentication tokens. These parameters are vital for managing connectivity to Azure-deployed LLM endpoints and may be necessary for secure or enterprise settings. Ensure you provide correct values aligned to your Azure OpenAI resource configuration. Input parameters include version strings, endpoints, and tokens; output is managed via subsequent operations using the configured instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evaluation-models.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napi_version: str = field(default=None)\n\"\"\"\nThe verion of the API that is provisioned\nhttps://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n\"\"\"\nazure_endpoint: Optional[str] = field(default=None)\n\"\"\"\nThe endpoint to use for azure openai. Available in the azure portal.\nhttps://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n\"\"\"\nazure_deployment: Optional[str] = field(default=None)\nazure_ad_token: Optional[str] = field(default=None)\nazure_ad_token_provider: Optional[Callable[[], str]] = field(default=None)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Trace Dataset from Phoenix\nDESCRIPTION: Initializes a Phoenix client ('px.Client()') and uses it to retrieve the trace dataset associated with the 'vision-fixture' project. A timeout of 1000 units (likely seconds) is specified for the retrieval operation. The retrieved trace data is stored in the 'td' variable.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/vision.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntd = px.Client().get_trace_dataset(timeout=1000, project_name=\"vision-fixture\")\n```\n\n----------------------------------------\n\nTITLE: Parsing LLM Output to Pandas DataFrame Python\nDESCRIPTION: This snippet processes the raw multi-line model output of question/tool call pairs, splits them into questions and expected tools, and creates a Pandas DataFrame for further analysis. It checks formatting, trims whitespace, and ensures each line yields exactly two parts split by '::'. Requires Pandas and input variable 'response_content'. Output is a DataFrame mapping each question to its expected tool(s).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/demos/playground_demo.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nquestions = []\nexpected_tool_calls = []\nfor line in response_content.strip().split(\"\\n\"):\n    assert len(parts := line.split(\"::\")) == 2\n    questions.append(parts[0].strip())\n    expected_tool_calls.append(list(map(lambda x: x.strip(), parts[1].split(\",\"))))\ndf = pd.DataFrame({\"question\": questions, \"expected_tool_calls\": expected_tool_calls})\ndf\n```\n\n----------------------------------------\n\nTITLE: Invoking the Sales Analysis Tool - Python\nDESCRIPTION: Shows how to call the AI-powered sales analysis function to identify the most popular product SKU, using previously queried sales data. Input is a user prompt and data string; output is a string analysis from the LLM. This snippet is commented and meant for demonstration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_agent.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# analysis = analyze_sales_data(\"What is the most popular product SKU?\", example_data)\n# analysis\n\n```\n\n----------------------------------------\n\nTITLE: Defining Python Project Dependencies (uv generated)\nDESCRIPTION: Lists the required Python packages and their pinned versions for the arize-ai/phoenix project. This file ensures reproducible builds by specifying exact versions for direct and transitive dependencies like altair, pandas, streamlit, etc. It was generated using the 'uv pip compile pyproject.toml' command, and comments indicate which top-level dependencies require each package.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/manually-instrumented-chatbot/frontend/requirements.txt#_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\n# This file was autogenerated by uv via the following command:\n#    uv pip compile pyproject.toml\naltair==5.3.0\n    # via streamlit\nannotated-types==0.6.0\n    # via pydantic\nanyio==4.3.0\n    # via httpx\nattrs==23.2.0\n    # via\n    #   jsonschema\n    #   referencing\nblinker==1.7.0\n    # via streamlit\ncachetools==5.3.3\n    # via streamlit\ncertifi==2024.7.4\n    # via\n    #   httpcore\n    #   httpx\n    #   requests\ncharset-normalizer==3.3.2\n    # via requests\nclick==8.1.7\n    # via streamlit\ngitdb==4.0.11\n    # via gitpython\ngitpython==3.1.43\n    # via streamlit\nh11==0.14.0\n    # via httpcore\nhttpcore==1.0.5\n    # via httpx\nhttpx==0.27.0\nidna==3.6\n    # via\n    #   anyio\n    #   httpx\n    #   requests\njinja2==3.1.6\n    # via\n    #   altair\n    #   pydeck\njsonschema==4.21.1\n    # via altair\njsonschema-specifications==2023.12.1\n    # via jsonschema\nmarkdown-it-py==3.0.0\n    # via rich\nmarkupsafe==2.1.5\n    # via jinja2\nmdurl==0.1.2\n    # via markdown-it-py\nnumpy==1.26.4\n    # via\n    #   altair\n    #   pandas\n    #   pyarrow\n    #   pydeck\n    #   streamlit\npackaging==24.0\n    # via\n    #   altair\n    #   streamlit\npandas==2.2.1\n    # via\n    #   altair\n    #   streamlit\npillow==10.3.0\n    # via streamlit\nprotobuf==4.25.3\n    # via streamlit\npyarrow==15.0.2\n    # via streamlit\npydantic==2.6.4\npydantic-core==2.16.3\n    # via pydantic\npydeck==0.8.0\n    # via streamlit\npygments==2.17.2\n    # via rich\npython-dateutil==2.9.0.post0\n    # via pandas\npytz==2024.1\n    # via pandas\nreferencing==0.34.0\n    # via\n    #   jsonschema\n    #   jsonschema-specifications\nrequests==2.32.2\n    # via streamlit\nrich==13.7.1\n    # via streamlit\nrpds-py==0.18.0\n    # via\n    #   jsonschema\n    #   referencing\nsix==1.16.0\n    # via python-dateutil\nsmmap==5.0.1\n    # via gitdb\nsniffio==1.3.1\n    # via\n    #   anyio\n    #   httpx\nstreamlit==1.37.0\ntenacity==8.2.3\n    # via streamlit\ntoml==0.10.2\n    # via streamlit\ntoolz==0.12.1\n    # via altair\ntornado==6.4.2\n    # via streamlit\ntyping-extensions==4.11.0\n    # via\n    #   pydantic\n    #   pydantic-core\n    #   streamlit\ntzdata==2024.1\n    # via pandas\nurllib3==2.2.1\n    # via requests\n```\n\n----------------------------------------\n\nTITLE: Performing a Traced LLM Completion with LiteLLM - Python\nDESCRIPTION: Executes a completion request to the 'gpt-3.5-turbo' model using LiteLLM and prints the result. The call is automatically traced using Phoenix instrumentation if previously configured. Expects a properly set 'OPENAI_API_KEY'. The 'messages' parameter specifies input prompts and roles. The output is the full LLM completion response object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/litellm.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport litellm\ncompletion_response = litellm.completion(model=\"gpt-3.5-turbo\",\n                   messages=[{\"content\": \"What's the capital of China?\", \"role\": \"user\"}])\nprint(completion_response)\n\n```\n\n----------------------------------------\n\nTITLE: Building the Phoenix Package\nDESCRIPTION: Commands to build both the web app and Python package components of Phoenix. This creates distribution files in the dist directory that can be installed elsewhere.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npnpm run build\nhatch build\n```\n\n----------------------------------------\n\nTITLE: Inspecting the Traces DataFrame - Python\nDESCRIPTION: This code inspects the dataframe that contains the traces to get context of each request and its response.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nspans_df[[\"name\", \"span_kind\", \"attributes.input.value\", \"attributes.retrieval.documents\"]].head()\n```\n\n----------------------------------------\n\nTITLE: Pulling Phoenix Docker Image - Bash\nDESCRIPTION: This command pulls the latest Phoenix Docker image from Docker Hub. This is the initial step for deploying Phoenix using Docker.  The image is named `arizephoenix/phoenix:latest`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/crewai.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Download Example Inference - Python\nDESCRIPTION: This snippet downloads a specified example inference set using `px.load_example`. The downloaded data is returned as an `ExampleInferences` instance, containing primary and reference inferences.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/use-example-inferences.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninferences = px.load_example(\"sentiment_classification_language_drift\")\ninferences\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App with Single Inference Set in Python\nDESCRIPTION: Launches a Phoenix session with a single inference set for analyzing a single cohort of data. Used for checking model performance and data quality without drift analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/manage-the-app.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsession = px.launch_app(ds)\n```\n\n----------------------------------------\n\nTITLE: Adding A-Frame Box Primitive (HTML)\nDESCRIPTION: Defines a box geometry in the 3D scene using the A-Frame primitive element `<a-box>`. Attributes set its `position` (x, y, z coordinates), `rotation` (rotation around x, y, z axes), and `color`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_2\n\nLANGUAGE: HTML\nCODE:\n```\n<a-box position=\"-1 0.5 -3\" rotation=\"0 45 0\" color=\"#4CCDE9\"></a-box>\n```\n\n----------------------------------------\n\nTITLE: Testing Model Connectivity\nDESCRIPTION: Sends a simple test query to the LLM to verify that the model is working correctly and API connectivity is established.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmodel(\"Hello world, this is a test if you are working?\")\n```\n\n----------------------------------------\n\nTITLE: Installing PostgreSQL for Testing\nDESCRIPTION: Command to install PostgreSQL using Homebrew, which is required for running tests that involve persistence. Phoenix can use either SQLite or PostgreSQL as its backend database.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nbrew install postgresql\n```\n\n----------------------------------------\n\nTITLE: Previewing Production DataFrame Contents using Python\nDESCRIPTION: Outputs the first few rows of the production DataFrame to verify its contents and structure before further processing or analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/inferences_quickstart.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprod_df.head()\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Command Line Endpoint\nDESCRIPTION: Sets the `PHOENIX_COLLECTOR_ENDPOINT` environment variable to the local Phoenix instance's address.  This directs the tracing data to the local Phoenix server.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/instructor.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Activating the Python Virtual Environment\nDESCRIPTION: Command to activate the created virtual environment before proceeding with package installation. This ensures all subsequent commands operate within the isolated environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsource ./.venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix Locally via Command Line\nDESCRIPTION: Commands to install the Phoenix package and start a local Phoenix server, allowing local development and testing of traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langgraph.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Installing Express for Sample Node.js Server (Shell)\nDESCRIPTION: This shell command installs the Express web framework in the project, providing a simple HTTP server on which to demonstrate OpenAI chat completion tracing. It is required for running the sample Express application and should be executed prior to running or developing the `app.ts` sample. Supported on npm, pnpm, yarn, or similar package managers.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/javascript.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n# npm, pnpm, yarn, etc\nnpm install express\n```\n\n----------------------------------------\n\nTITLE: Launching local Phoenix instance via command line\nDESCRIPTION: Shell command to install and launch Phoenix in a local environment. This creates a local server that will collect and display DSPy instrumentation data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/dspy.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Index.md Toctree Path Example\nDESCRIPTION: Example of how to add a reference to a module in the index.md file's toctree, which is required for the documentation to appear in the generated output.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/api_reference/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napi/module\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix in a notebook environment\nDESCRIPTION: Python code to launch Phoenix directly within a Jupyter notebook. Note that this deployment doesn't have persistent storage for traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/dspy.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport phoenix as px\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Installing UV on macOS using Homebrew\nDESCRIPTION: Command to install UV (a virtual environment management tool) via Homebrew for macOS users. UV is recommended for isolating Python dependencies in the Phoenix development environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew install uv\n```\n\n----------------------------------------\n\nTITLE: Opening Phoenix UI in Browser\nDESCRIPTION: Displays the Phoenix user interface in a web browser by extracting the session URL, supporting local Jupyter and Colab environments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/manage-the-app.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nsession.url\n```\n\n----------------------------------------\n\nTITLE: Instantiating GPT-4 model for summarization evaluation\nDESCRIPTION: Creates an instance of the OpenAI GPT-4 model with zero temperature (deterministic outputs) for consistent evaluation results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\n```\n\n----------------------------------------\n\nTITLE: Running Hallucination Classification with Explanations\nDESCRIPTION: Demonstrates how to obtain detailed explanations for each classification decision. This approach takes longer but provides valuable insights into the LLM's reasoning process.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsmall_df_sample = df.copy().sample(n=5).reset_index(drop=True)\nhallucination_classifications_df = llm_classify(\n    dataframe=small_df_sample,\n    template=HALLUCINATION_PROMPT_TEMPLATE,\n    model=model,\n    rails=rails,\n    provide_explanation=True,\n    verbose=True,\n    concurrency=20,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix local environment endpoint\nDESCRIPTION: Python code to configure the local Phoenix collector endpoint for tracing data. This endpoint is used by the instrumentation library to send trace data.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/dspy.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries for Q&A Evaluation\nDESCRIPTION: Installs the necessary Python packages including arize-phoenix-evals, OpenAI, and visualization libraries needed for the Q&A classification evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qq \"arize-phoenix-evals\" \"openai>=1\" ipython matplotlib pycm scikit-learn tiktoken nest_asyncio 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix App with Training and Production Inferences in Python\nDESCRIPTION: Starts a Phoenix session that visualizes and compares both the production (primary) and training (reference) inferences. Requires Phoenix import as 'px' and initialized 'prod_ds' (primary) and 'train_ds' (reference) Inferences objects. Parameters specify baseline (reference) and observed (primary) datasets. Launches interactive UI for comparative model drift and performance analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/quickstart/phoenix-inferences/README.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsession = px.launch_app(primary=prod_ds, reference=train_ds)\n\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix and evaluation dependencies (Bash)\nDESCRIPTION: Installs the required Python packages for interacting with Phoenix and using OpenAI for trace generation and potential evaluation. Specifies versions for compatibility.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/feedback-and-annotations/evaluating-phoenix-traces.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -q \"arize-phoenix>=4.29.0\"\npip install -q openai 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Generating Span Kind Python\nDESCRIPTION: Defines a generator function `rand_span_kind()` that yields a random OpenInferenceSpanKindValue to be used as an attribute in spans.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/multi-turn_chat_sessions.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef rand_span_kind():\n    yield SpanAttributes.OPENINFERENCE_SPAN_KIND, choice(list(OpenInferenceSpanKindValues)).value\n```\n\n----------------------------------------\n\nTITLE: Inspecting Traces with Documents - Python\nDESCRIPTION: This snippet inspects traces with retrieved documents to verify that the relevant data is included.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nspans_with_docs_df[[\"attributes.input.value\", \"attributes.retrieval.documents\"]].head()\n```\n\n----------------------------------------\n\nTITLE: Exporting Wiki Toxic Dataset to JSONL in Python\nDESCRIPTION: Saves the preprocessed Wiki Toxic dataset to a JSONL file with each record on a separate line, allowing for efficient loading in ML pipelines.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_wiki_toxic.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf.to_json(f\"wiki_toxic-{split}.jsonl\", orient=\"records\", lines=True)\n```\n\n----------------------------------------\n\nTITLE: Inspect Inference Name - Python\nDESCRIPTION: This snippet retrieves and displays the name of the primary inference set. It accesses the `name` attribute of the `prim_ds` (primary dataset) object.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/use-example-inferences.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprim_ds = inferences.primary\nprim_ds.name\n```\n\n----------------------------------------\n\nTITLE: F-string Prompt Example\nDESCRIPTION: This code snippet demonstrates a prompt template using f-string formatting. The placeholder {name} will be replaced with the provided value at runtime.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/internal_docs/specs/prompts.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nHello, {name}!\n```\n\n----------------------------------------\n\nTITLE: Running Code Formatters\nDESCRIPTION: Command to run the ruff code formatter on Phoenix code. This is required when contributing or modifying Jupyter notebooks to maintain consistent code style.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ntox run -e ruff\n```\n\n----------------------------------------\n\nTITLE: Install OpenInference VertexAI Python Packages\nDESCRIPTION: Installs the `openinference-instrumentation-vertexai` package for instrumenting VertexAI calls and the core `vertexai` SDK package.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/vertexai.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npip install openinference-instrumentation-vertexai vertexai\n```\n\n----------------------------------------\n\nTITLE: Retrieving QA Dataframe\nDESCRIPTION: This snippet retrieves the question-answer pairs with reference information from the Phoenix client and displays the head of the DataFrame.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/span_query_language.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nqa_df = get_qa_with_reference(px.Client())\nqa_df.head()\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Cloud packages\nDESCRIPTION: This command installs the `arize-phoenix-otel` package, which is required for tracing and observability with Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/instructor.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Printing Phoenix URL\nDESCRIPTION: Prints the URL of the active Phoenix session. This URL can be used to access the Phoenix UI and view the evaluation results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nprint(\"phoenix URL\", px.active_session().url)\n```\n\n----------------------------------------\n\nTITLE: Pulling Phoenix Docker Image (Bash)\nDESCRIPTION: Downloads the latest Arize Phoenix Docker image from Docker Hub, providing a containerized way to run Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai-agents-sdk.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Viewing the Uploaded Dataset as DataFrame\nDESCRIPTION: Retrieves and displays the uploaded dataset in DataFrame format for inspection and validation of dataset contents.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndataset.as_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Launching the Phoenix Analytics UI Application - Python\nDESCRIPTION: Initializes and launches the Phoenix web UI with the production and training Inferences datasets. The session object created exposes methods for interactive exploration and analysis of data drift, embedding clusters, and model monitoring in the notebook or a browser. Requires that all previous preparation steps have completed successfully and that a supported browser is available.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/credit_card_fraud_tutorial.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n(session := px.launch_app(primary=prod_ds, reference=train_ds)).view()\n```\n\n----------------------------------------\n\nTITLE: Closing Phoenix App Session in Python\nDESCRIPTION: Gracefully shuts down the running Phoenix background session when analysis is complete. This is important for properly releasing resources.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/manage-the-app.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npx.close_app()\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Root Path Environment Variable\nDESCRIPTION: Command to set the environment variable that configures Phoenix to use the custom root path that matches the reverse proxy configuration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/reverse-proxy/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport PHOENIX_HOST_ROOT_PATH=\"/phoenix_root_path\"\n```\n\n----------------------------------------\n\nTITLE: Installing Docker packages\nDESCRIPTION: This command installs the `arize-phoenix-otel` package, required for tracing within the Docker container.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/instructor.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Registering OpenTelemetry Tracing for OpenAI Phoenix Python\nDESCRIPTION: This snippet initializes OpenTelemetry tracing in the Phoenix platform, registering an OTLP endpoint for trace reporting and instrumenting OpenAI operations for observability. Requires the Phoenix and OpenInference instrumentor modules. 'register()' is called with a local HTTP trace collector endpoint, and OpenAIInstrumentor is attached. No direct inputs or outputs; effects are on trace logging externally.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/demos/playground_demo.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ntracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\nOpenAIInstrumentor(tracer_provider=tracer_provider).instrument()\n```\n\n----------------------------------------\n\nTITLE: Setting Evaluation Sample Size for LLM Hallucination Detection\nDESCRIPTION: Configures the sample size for evaluating LLM hallucination detection, with runtime estimates for different sample sizes using GPT-3.5 and GPT-4 models.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#####################\n## N_EVAL_SAMPLE_SIZE\n#####################\n# Eval sample size determines the run time\n# 100 samples: GPT-4 ~ 80 sec / GPT-3.5 ~ 40 sec\n# 1,000 samples: GPT-4 ~15-17 min / GPT-3.5 ~ 6-7min (depending on retries)\n# 10,000 samples GPT-4 ~170 min / GPT-3.5 ~ 70min\nN_EVAL_SAMPLE_SIZE = 100\n```\n\n----------------------------------------\n\nTITLE: Viewing Precision@2 Scores\nDESCRIPTION: Displays the Precision@2 scores for each retrieval operation to evaluate retrieval accuracy.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nprecision_at_2\n```\n\n----------------------------------------\n\nTITLE: Loading QA Dataset from Remote JSON Source Using pandas in Python\nDESCRIPTION: Fetches a QA dataset in JSON lines format from a GitHub repository URL and loads it into a pandas DataFrame. Demonstrates sampling of the data for inspection. Prepares raw question-answer data for evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://raw.githubusercontent.com/RUCAIBox/HaluEval/refs/heads/main/data/qa_data.json\"\nqa = pd.read_json(url, lines=True)\nqa.sample(5).iloc[:, ::-1]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Text from Source Node\nDESCRIPTION: This code retrieves the text content from the first source node in the response. This shows the context that the LLM used to formulate its response.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n# First retrieved node\nresponse_vector.source_nodes[0].get_text()\n```\n\n----------------------------------------\n\nTITLE: Executing Anthropic LLM Evaluation Experiment Using Phoenix in Python\nDESCRIPTION: Runs the experiment on the uploaded dataset with the Anthropic LLM evaluation function, generating predictions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nexp_anthropic = run_experiment(ds, anthropic_eval)\n```\n\n----------------------------------------\n\nTITLE: Install OpenTelemetry Phoenix Python Package\nDESCRIPTION: Installs the `arize-phoenix-otel` Python package, which is required for integrating Phoenix with OpenTelemetry instrumentations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/vertexai.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix OTel Package\nDESCRIPTION: Command to install the arize-phoenix-otel package, which enables OpenTelemetry-based tracing for Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langchain.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Listing Supported Pre-trained Models for Auto-Embeddings in Python\nDESCRIPTION: Code snippet to display all supported pre-trained models available for Auto-Embeddings in Phoenix using the EmbeddingGenerator class.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/generating-embeddings.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom arize.pandas.embeddings import EmbeddingGenerator\n\nEmbeddingGenerator.list_pretrained_models()\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix OTEL Tracer Provider for HTTP Transport Using Environment Variables in Python\nDESCRIPTION: Illustrates configuring the phoenix.otel.register function to send traces via the HTTP/protobuf protocol to a Phoenix server by setting the PHOENIX_COLLECTOR_ENDPOINT environment variable and explicitly specifying protocol=\"http/protobuf\". This changes the transport from default gRPC to HTTP, directing spans to the appropriate HTTP collector endpoint.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# export PHOENIX_COLLECTOR_ENDPOINT=https://your-phoenix.com:6006\n\nfrom phoenix.otel import register\n\n# sends traces to https://your-phoenix.com/v1/traces\ntracer_provider = register(\n    protocol=\"http/protobuf\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix for DSPy monitoring via pip\nDESCRIPTION: Command to install the Arize Phoenix OpenTelemetry integration package required for DSPy instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/dspy.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Empty Python Snippet\nDESCRIPTION: This is an empty code snippet. It might be included for placeholder purposes.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/answer_and_context_relevancy.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n\n```\n\n----------------------------------------\n\nTITLE: Launching Phoenix from command line\nDESCRIPTION: Commands to install Phoenix and launch a local instance through the command line interface.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/guardrails-ai.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Formatting and Sending OpenAI Chat Completion Requests in Python\nDESCRIPTION: Formats a prompt with the dataset's first input example variables and sends it as a chat completion request to OpenAI API using the openai Python SDK. Prints the model's response content. Depends on environment variable OPENAI_API_KEY.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nvariables = dict(ds.as_dataframe().input.iloc[0])\nformatted_prompt = prompt.format(variables=variables)\nresponse = openai.OpenAI().chat.completions.create(**formatted_prompt)\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Testing GPT-4 model functionality\nDESCRIPTION: Sends a simple test message to the GPT-4 model to verify that it's working correctly before proceeding with the full evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel(\"Hello world, this is a test if you are working?\")\n```\n\n----------------------------------------\n\nTITLE: Pulling a prompt by Version ID using TypeScript\nDESCRIPTION: Shows how to retrieve a specific prompt version synchronously by its version ID in TypeScript. This method guarantees access to a particular prompt state, important for version-controlled deployments.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/how-to-prompts/using-a-prompt.md#_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { getPrompt } from \"@arizeai/phoenix-client/prompts\";\n\nconst promptByVersionId = await getPrompt({ versionId: \"b5678\" })\n// ^ the latest version of the prompt with Id \"a1234\"\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Client Library using npm in TypeScript\nDESCRIPTION: This snippet shows how to install the Phoenix client library from npm to enable interaction with Phoenix AI platform in a TypeScript project. It is a prerequisite for the subsequent prompt creation and management.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/quickstart-prompts/quickstart-prompts-ts.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @arizeai/phoenix-client\n```\n\n----------------------------------------\n\nTITLE: Creating the Phoenix Client in JavaScript\nDESCRIPTION: This snippet instantiates a Phoenix client, targeting a local Phoenix server at 'http://localhost:6006'. An optional Authorization header can be set for instances requiring API keys. The client object is reusable for subsequent prompt operations. Requires the '@arizeai/phoenix-client' dependency and correct Phoenix server configuration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_cross_sdk_tutorial.ipynb#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst px = PhoenixClient.createClient({\n  options: {\n    baseUrl: \"http://localhost:6006\",\n    // Uncomment this if you are using a Phoenix instance that requires an API key\n    // headers: {\n    //   Authorization: \"bearer xxxxxx\",\n    // }\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: Pulling and Running Phoenix Docker Image (Bash)\nDESCRIPTION: Pulls the latest `arizephoenix/phoenix` image from Docker Hub and runs it in a container. Port 6006 on the host is mapped to port 6006 in the container, making the Phoenix UI accessible at `localhost:6006`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/haystack.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenTelemetry TracerProvider (TypeScript)\nDESCRIPTION: Configures and initializes the OpenTelemetry tracing SDK for a Node.js application. It sets up a `NodeTracerProvider`, assigns a project name resource, and configures a `SimpleSpanProcessor` to export traces using the OTLP/protobuf format to a specified endpoint, typically a local trace collector like Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/tracing_openai_sessions_tutorial.ipynb#_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport {\n  NodeTracerProvider,\n  SimpleSpanProcessor,\n} from \"npm:@opentelemetry/sdk-trace-node\";\nimport { Resource } from \"npm:@opentelemetry/resources\";\nimport { OTLPTraceExporter } from \"npm:@opentelemetry/exporter-trace-otlp-proto\";\nimport { SEMRESATTRS_PROJECT_NAME } from \"npm:@arizeai/openinference-semantic-conventions\";\nimport { diag, DiagConsoleLogger, DiagLogLevel } from \"npm:@opentelemetry/api\";\n\n// For troubleshooting, set the log level to DiagLogLevel.DEBUG\ndiag.setLogger(new DiagConsoleLogger(), DiagLogLevel.INFO);\n\nconst provider = new NodeTracerProvider({\n  resource: new Resource({\n    [SEMRESATTRS_PROJECT_NAME]: \"openai-node-sessions-example\",\n  }),\n});\n\nprovider.addSpanProcessor(\n  new SimpleSpanProcessor(\n    new OTLPTraceExporter({\n      url: \"http://localhost:6006/v1/traces\",\n    }),\n  ),\n);\n\nprovider.register();\n\nconsole.log(\"ðŸ‘€ OpenInference initialized\");\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Queries from URL\nDESCRIPTION: Defines a URL pointing to a JSONL file containing sample queries related to Arize documentation. It opens the URL, iterates through each line, decodes it from UTF-8, parses the JSON content, extracts the value associated with the 'query' key, and appends it to the `queries` list. Finally, it displays the first 5 queries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nqueries_url = \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/arize_docs_queries.jsonl\"\nqueries = []\nwith urlopen(queries_url) as response:\n    for line in response:\n        line = line.decode(\"utf-8\").strip()\n        data = json.loads(line)\n        queries.append(data[\"query\"])\nqueries[:5]\n```\n\n----------------------------------------\n\nTITLE: Configuring Project Name in OTEL Resource Attributes (Python)\nDESCRIPTION: This comprehensive snippet configures a custom project name for traces by setting the OTEL resource attribute 'ResourceAttributes.PROJECT_NAME' when creating a TracerProvider. Dependencies include: openinference, opentelemetry (with OTLP exporter and SDK), and phoenix server. It sets up OTEL tracing backend, exporter endpoint, and optional auto-instrumentation. Requires the project name as a string. All traces from this provider are tagged with the project's name, enabling server-side grouping. Adjust endpoints and instrumentation for your specific setup.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/setup-tracing/setup-projects.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom openinference.semconv.resource import ResourceAttributes\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\nfrom opentelemetry import trace as trace_api\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk import trace as trace_sdk\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\n\nresource = Resource(attributes={\n    ResourceAttributes.PROJECT_NAME: '<your-project-name>'\n})\ntracer_provider = trace_sdk.TracerProvider(resource=resource)\nspan_exporter = OTLPSpanExporter(endpoint=\"http://phoenix:6006/v1/traces\")\nspan_processor = SimpleSpanProcessor(span_exporter=span_exporter)\ntracer_provider.add_span_processor(span_processor=span_processor)\ntrace_api.set_tracer_provider(tracer_provider=tracer_provider)\n# Add any auto-instrumentation you want \nLlamaIndexInstrumentor().instrument()\n```\n\n----------------------------------------\n\nTITLE: Running Zero-Shot Sentiment Analysis Experiment with Phoenix\nDESCRIPTION: Executes the zero-shot prompting experiment using `phoenix.experiments.run_experiment`. It applies the `zero_shot_prompt` task function to the uploaded dataset, uses the `evaluate_response` function as an evaluator, and logs the experiment details (description, name, metadata including the prompt ID) to Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/few-shot-prompting.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nfrom phoenix.experiments import run_experiment\n\nnest_asyncio.apply()\n\ninitial_experiment = run_experiment(\n    dataset,\n    task=zero_shot_prompt,\n    evaluators=[evaluate_response],\n    experiment_description=\"Zero-Shot Prompt\",\n    experiment_name=\"zero-shot-prompt\",\n    experiment_metadata={\"prompt\": \"prompt_id=\" + prompt.id},\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix OTel packages\nDESCRIPTION: Command to install the Arize Phoenix OpenTelemetry package required for tracing LLM applications.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/guardrails-ai.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix with LlamaIndex Dependencies using pip - Shell\nDESCRIPTION: Installs the necessary Python libraries for running Phoenix with LlamaIndex support, along with sentence-transformers and torch. This command ensures the required versions are acquired; it must be run in a shell environment such as Colab, terminal, or notebook cell. Dependencies: pip, Python 3.8+.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/run_experiments_with_llama_index.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -Uqqq \"arize-phoenix[llama-index]>=4.6\" sentence-transformers torch\n```\n\n----------------------------------------\n\nTITLE: Sampling and Preparing Dataset (Python)\nDESCRIPTION: Samples a subset of the downloaded dataframe based on the defined `N_EVAL_SAMPLE_SIZE`. It then renames the `query_text` and `document_text` columns to `input` and `reference`, respectively, conforming the dataframe structure to the expected format for the `llm_classify` function.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf_sample = df.sample(n=N_EVAL_SAMPLE_SIZE).reset_index(drop=True)\ndf_sample = df_sample.rename(\n    columns={\n        \"query_text\": \"input\",\n        \"document_text\": \"reference\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Filtering Traces with Non-Null Retrieved Documents\nDESCRIPTION: Filters the spans DataFrame to include only traces where documents were retrieved, facilitating targeted analysis of relevant retrieval traces.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nspans_with_docs_df = spans_df[spans_df[\"attributes.retrieval.documents\"].notnull()]\n\n```\n\n----------------------------------------\n\nTITLE: Running Computer Use Agent Docker Container\nDESCRIPTION: This command runs the Docker container. It mounts the anthropic API key directory, maps ports for VNC and the Gradio UI, and runs in interactive mode. The user must supply the correct API keys in the Gradio UI after the container starts.  The volume mapping ensures the Anthropic API key persists.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/computer_use_agent/README.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ndocker run -v $HOME/.anthropic:/home/computeruse/.anthropic \\\n           -p 5900:5900 -p 7860:7860 -p 6080:6080 -p 8080:8080 \\\n           -it computer-use-agent:1.0\n```\n\n----------------------------------------\n\nTITLE: Processing HTML Table Data with Pandas\nDESCRIPTION: Reads the HTML table (converted to a string) into a pandas DataFrame using 'pd.read_html'. It selects specific columns (0, 2, -1), sorts the DataFrame by the 'Number of votes' column in descending order, sets a numerical index derived from the first column, selects the caption and votes columns, renames them, prints the total number of rows, and displays the first 5 rows.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/vision.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_html(StringIO(str(table)))[0].iloc[:, [0, 2, -1]]\ndf.sort_values(\"Number of votes\", ascending=False, inplace=True)\ndf = (\n    df.set_index(df.iloc[:, 0].apply(lambda s: int(s.split()[0])))\n    .rename_axis(None, axis=0)\n    .iloc[:, [1, 2]]\n)\ndf.rename(dict(zip(df.columns, [\"caption\", \"votes\"])), axis=1, inplace=True)\nprint(len(df))\ndf.head(5)\n```\n\n----------------------------------------\n\nTITLE: Viewing NDCG@2 Scores\nDESCRIPTION: Displays the NDCG@2 scores for each retrieval operation to evaluate ranking quality.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nndcg_at_2\n```\n\n----------------------------------------\n\nTITLE: Querying Mistral AI Chat API with Phoenix Prompt Utilities in Python\nDESCRIPTION: This snippet shows how to use the mistralai SDK to query the Mistral AI chat API with prompt message formatting from the Phoenix client utilities. Requires the mistralai SDK and an API key set in MISTRAL_API_KEY. The prompt_version_id selects the prompt, and variables can be passed for templating. The chat result is printed in structured JSON format.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/examples/prompts/prompts_for_various_SDKs.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel = \"mistral-small-latest\"\n\nprompt_version_id = \"UHJvbXB0VmVyc2lvbjox\"\nprompt_version = Client().prompts.get(prompt_version_id=prompt_version_id)\nprint(f\"prompt_version = {prompt_version.model_dump_json(indent=2)}\\n{'-'*100}\")\n\nmessages, kwargs = to_chat_messages_and_kwargs(\n    prompt_version, variables={\"question\": \"Who made you?\"}\n)\nprint(f\"messages = {json.dumps(messages, indent=2)}\\n{'-'*100}\")\nprint(f\"kwargs = {json.dumps(kwargs, indent=2)}\\n{'-'*100}\")\n\nresponse = mistralai.Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"]).chat.complete(\n    model=\"mistral-small-latest\", messages=messages\n)\nprint(f\"response = {response.model_dump_json(indent=2)}\")\n```\n\n----------------------------------------\n\nTITLE: Instrument OpenAI Application\nDESCRIPTION: This code instruments the OpenAI application using OpenInference to enable tracing of API calls within Phoenix. It registers the tracer provider and instruments the OpenAI API.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/summarization.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\nOpenAIInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix as a Docker Container (Bash)\nDESCRIPTION: Runs the Phoenix Docker container and exposes the service on port 6006. Assumes the image has been previously pulled. Use this to run Phoenix in an isolated environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/groq.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Phoenix\nDESCRIPTION: This code downloads a Llama dataset, uploads it to the Phoenix platform, and sets the sample size to 7. A temporary directory is used to download the dataset, and the dataset is then uploaded to Phoenix.  The dataset name is dynamically generated.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_answer_and_context_relevancy_experiment.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsample_size = 7\ndataset_name = \"EvaluatingLlmSurveyPaperDataset\"\nwith tempfile.TemporaryDirectory() as dir_name:\n    rag_dataset, documents = download_llama_dataset(dataset_name, dir_name)\ndataset = px.Client().upload_dataset(\n    dataset_name=f\"{dataset_name}_{time_ns()}\",\n    dataframe=rag_dataset.to_pandas().sample(sample_size, random_state=42),\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix, LangChain, and Qdrant (Shell)\nDESCRIPTION: Installs all required Python packages for running the end-to-end retrieval QA system, including LangChain, Qdrant client, tiktoken, cohere, langchain-openai, protobuf, arize-phoenix with evals/embeddings, recent OpenAI client, and openinference instrumentation libraries. All dependencies must be available in the chosen Python environment. The command uses pip to ensure the latest compatible versions are present.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/qdrant_langchain_instrumentation_search_and_retrieval_tutorial.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n%pip install -Uq langchain qdrant-client langchain_community tiktoken cohere langchain-openai \"protobuf>=3.20.3\" \"arize-phoenix[evals,embeddings]\" \"openai>=1\" openinference-instrumentation-langchain 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Pulling the Latest Phoenix Docker Image (Bash)\nDESCRIPTION: Downloads the latest Phoenix Docker image from Docker Hub, making it available to run as a local container. Requires Docker to be installed and configured on the system.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/groq.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies Python\nDESCRIPTION: Installs the necessary Python packages required for the tutorial, including anthropic, arize-phoenix, jsonschema, and openinference-instrumentation-anthropic. These packages provide the functionality for interacting with the Anthropic API, running Phoenix, handling JSON schemas, and instrumenting the client for tracing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/anthropic_tracing_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install anthropic arize-phoenix jsonschema openinference-instrumentation-anthropic\n```\n\n----------------------------------------\n\nTITLE: Uploading Synthetic Dataset to Arize Phoenix\nDESCRIPTION: Uses the Arize Phoenix client (`px.Client`) to upload the processed synthetic dataset to the platform. Maps the input questions and expected execution results (including query and potential error) to the appropriate fields for dataset ingestion.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nsynthetic_dataset = px.Client().upload_dataset(\n    dataset_name=\"nba-golden-synthetic\",\n    inputs=[{\"question\": example[\"input\"]} for example in generated_dataset],\n    outputs=[example[\"expected\"] for example in generated_dataset],\n);\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Prompt by Tag (TypeScript)\nDESCRIPTION: This snippet shows how to retrieve a prompt from the Phoenix server by tag using the TypeScript client.  It uses the `getPrompt` function from `@arizeai/phoenix-client/prompts` to fetch a prompt with a specified name and tag. In this case, it retrieves the prompt named \"article-bullet-summarizer\" tagged with \"production\".\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/how-to-prompts/create-a-prompt.md#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst promptByTag = await getPrompt({ tag: \"production\", name: \"article-bullet-summarizer\" });\n// ^ you can optionally specify a tag to filter by\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for LLM Analysis\nDESCRIPTION: Imports pandas for data manipulation, Arize SDK modules for embeddings generation and ROUGE score computation, and the Phoenix library for visualization and analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/llm_summarization_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom arize.pandas.embeddings import EmbeddingGenerator, UseCases\nfrom arize.pandas.generative.llm_evaluation import rouge\n\nimport phoenix as px\n```\n\n----------------------------------------\n\nTITLE: Running the Hallucination Eval Using Python and OpenAI Model\nDESCRIPTION: This Python snippet demonstrates the usage of the hallucination eval template through the Phoenix evals library. It shows importing necessary components, initializing an OpenAI GPT-4 model with temperature 0.0 for deterministic output, and classifying entries in a dataframe using the hallucination prompt template. The rails parameter constrains possible output values to expected classification labels. The snippet optionally generates textual explanations for classifications. The snippet expects a dataframe \"df\" containing query, reference, and response data and outputs classification labels indicating hallucinated or factual responses.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/hallucinations.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.evals import (\n    HALLUCINATION_PROMPT_RAILS_MAP,\n    HALLUCINATION_PROMPT_TEMPLATE,\n    OpenAIModel,\n    download_benchmark_dataset,\n    llm_classify,\n)\n\nmodel = OpenAIModel(\n    model_name=\"gpt-4\",\n    temperature=0.0,\n)\n\n#The rails is used to hold the output to specific values based on the template\n#It will remove text such as \",,,\" or \"...\"\n#Will ensure the binary value expected from the template is returned \nrails = list(HALLUCINATION_PROMPT_RAILS_MAP.values())\nhallucination_classifications = llm_classify(\n    dataframe=df, \n    template=HALLUCINATION_PROMPT_TEMPLATE, \n    model=model, \n    rails=rails,\n    provide_explanation=True, #optional to generate explanations for the value produced by the eval LLM\n)\n```\n\n----------------------------------------\n\nTITLE: Running Traefik Reverse Proxy for Phoenix\nDESCRIPTION: Command to start the Traefik reverse proxy using the configuration file that sets up the proxy on port 9999 with the custom application root path.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/reverse-proxy/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./traefik --configFile=traefik.toml\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies Shell\nDESCRIPTION: Installs the necessary Python packages required for the Text2SQL tutorial. This includes 'arize-phoenix' for observability and evaluation, 'openai' for LLM interaction, 'duckdb' for the database, 'datasets' and 'pyarrow' for data loading, 'pydantic' for data validation (often a dependency), 'nest_asyncio' for async execution in notebooks, and 'openinference-instrumentation-openai' for tracing OpenAI calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install 'arize-phoenix>=4.6.0' openai duckdb datasets pyarrow pydantic nest_asyncio openinference-instrumentation-openai --quiet\n```\n\n----------------------------------------\n\nTITLE: Previewing Key Columns of Spans DataFrame - Python\nDESCRIPTION: Selects and displays the first few rows (`.head()`) of specific columns from the pandas DataFrame containing tracing spans. The selected columns ('name', 'span_kind', 'attributes.input.value', 'attributes.retrieval.documents') provide insight into the type of operation, its input, and associated retrieval results captured by Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nspans_df[[\"name\", \"span_kind\", \"attributes.input.value\", \"attributes.retrieval.documents\"]].head()\n```\n\n----------------------------------------\n\nTITLE: Installing DSPy and OpenInference instrumentation packages\nDESCRIPTION: Command to install DSPy and the OpenInference instrumentation packages required for tracing DSPy and LiteLLM, which DSPy uses for LLM calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/dspy.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-dspy openinference-instrumentation-litellm dspy\n```\n\n----------------------------------------\n\nTITLE: Running Python Unit Tests using Tox (Shell)\nDESCRIPTION: Executes the Python unit test suite using the 'tox' automation tool. This command should be run to ensure backend code changes pass all tests before submitting a pull request. It targets the 'unit_tests' environment defined in the tox configuration.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ntox run -e unit_tests\n```\n\n----------------------------------------\n\nTITLE: Loading Wiki Toxic Dataset from HuggingFace in Python\nDESCRIPTION: Loads the test split of the Wiki Toxic dataset from HuggingFace using pandas. The data is retrieved directly from the HuggingFace API in parquet format.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_wiki_toxic.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nsplit = \"test\"\ndf = pd.read_parquet(\n    f\"https://huggingface.co/api/datasets/OxAISH-AL-LLM/wiki_toxic/parquet/default/{split}/0.parquet\"\n)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix Docker Container\nDESCRIPTION: This command runs the Phoenix Docker container, exposing port 6006 on the host machine. This allows access to the Phoenix UI.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/instructor.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Defining Phoenix Schema for Computer Vision Model Inferences in Python\nDESCRIPTION: Creates a Schema object to inform Phoenix how columns in the DataFrame correspond to inference metadata. This schema includes specifying the timestamp column, prediction label column, actual ground truth label column, and embedding feature columns containing vectors linked to URL data. Proper schema definition is crucial for Phoenix to interpret and visualize the dataset correctly.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/inferences_quickstart.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_schema = px.Schema(\n    timestamp_column_name=\"prediction_ts\",\n    prediction_label_column_name=\"predicted_action\",\n    actual_label_column_name=\"actual_action\",\n    embedding_feature_column_names={\n        \"image_embedding\": px.EmbeddingColumnNames(\n            vector_column_name=\"image_vector\",\n            link_to_data_column_name=\"url\",\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Prompt Refinement Function in JavaScript\nDESCRIPTION: This snippet defines a function that refines prompts to improve model response quality. It requires dependencies such as prompt templates and evaluation metrics. Inputs include raw prompts and parameters for refinement; outputs are optimized prompts suitable for deployment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/prompt-optimization.md#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nfunction refinePrompt(rawPrompt, options) {\n    // Process the raw prompt based on options\n    // Apply prompt templates and tuning parameters\n    // Return the optimized prompt\n    return optimizedPrompt;\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Groq Model Name String for Chat Evaluations in Python\nDESCRIPTION: Defines a string specifying the Groq model to be used for chat-based completion requests.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/hallucination_eval.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ngroq_model = \"deepseek-r1-distill-llama-70b\"\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Collector Endpoint for Local Deployment in Python\nDESCRIPTION: This snippet sets the environment variable in Python to direct data collection to a local Phoenix instance running at localhost:6006. Dependencies include the 'os' module. It is intended for configuring environment for local testing with Dockerized or local server.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/anthropic.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Defining System Message Python\nDESCRIPTION: Defines the system message to guide the Anthropic model's behavior. It instructs the model to act as an assistant that parses and records the attributes of the travel request based on the provided tool schema.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/anthropic_tracing_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsystem_message = (\n    \"You are an assistant that parses and records the attributes of a user's travel request.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Local Environment Variables\nDESCRIPTION: Python code to set the environment variable for connecting to a local Phoenix instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langchain.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Invoking OpenAI with the Converted Prompt\nDESCRIPTION: Uses the Vercel AI SDK to send the converted prompt to OpenAI's model. The code imports the generateText function, creates an OpenAI model instance with gpt-4o-mini, and displays the response in a markdown cell.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_vercel_ai_sdk.ipynb#_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { generateText } from \"npm:ai\"\n\n// lets just hardcode the model for now\n// you can use the model saved in your Phoenix Prompt if you want, but since we are using Vercel AI SDK,\n// we will just redeclare the model so that we can take advantage of Vercel AI SDK features\nconst openaiModel = openai(\"gpt-4o-mini\")\n\nconst response = await generateText({\n  model: openaiModel,\n  ...aiPrompt,\n})\n\nawait Deno.jupyter.md`\n  ### OpenAI Response\n\n  ${response.text || `\\`\\`\\`json\\n${JSON.stringify(response.steps[0].toolCalls, null, 2)}\\`\\`\\``}\n`\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Requirements\nDESCRIPTION: Defines project dependencies using pip's requirements file syntax. The '-r common.txt' line recursively includes dependencies listed in the 'common.txt' file. The 'anthropic>=0.49.0' line specifies that the 'anthropic' package, version 0.49.0 or newer, is required.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/requirements/canary/sdk/anthropic.txt#_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\n-r common.txt\nanthropic>=0.49.0\n```\n\n----------------------------------------\n\nTITLE: Instantiating VertexAIModel and Performing a Completion - Python\nDESCRIPTION: Provides an example for quickly authenticating and initializing VertexAIModel using your Google Cloud project ID and location. You must first install the 'google-cloud-aiplatform>=1.33.0' package, and pass valid credentials or specify authorized environment variables. The code demonstrates obtaining a completion given an input prompt, with outputs streamed from the chosen VertexAI model.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evaluation-models.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nproject = \"my-project-id\"\nlocation = \"us-central1\" # as an example\nmodel = VertexAIModel(project=project, location=location)\nmodel(\"Hello there, this is a tesst if you are working?\")\n# Output: \"Hello world, I am working!\"\n```\n\n----------------------------------------\n\nTITLE: Defining Multi-Modal Embedding Schema with Image and Text Features in Python\nDESCRIPTION: Demonstrates specifying a schema containing multiple embedding features for multi-modal data, such as product descriptions and images. Each embedding feature has its own vector and linked raw data or URL column name. This allows simultaneous ingestion and analysis of different embedding types with potentially different vector lengths using the Arize Phoenix Python SDK. Requires a structured dataframe with appropriate columns matching schema definitions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/define-your-schema/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nschema = px.Schema(\n    tag_column_names=[\"name\"],\n    embedding_feature_column_names={\n        \"description_embedding\": px.EmbeddingColumnNames(\n            vector_column_name=\"description_vector\",\n            raw_data_column_name=\"description\",\n        ),\n        \"image_embedding\": px.EmbeddingColumnNames(\n            vector_column_name=\"image_vector\",\n            link_to_data_column_name=\"image\",\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Load and Launch App with Example LLM Traces - Python\nDESCRIPTION: This snippet loads and launches the Phoenix app with example LLM traces using the `px.load_example_traces` and `px.launch_app` functions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/use-example-inferences.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Load up the LlamaIndex RAG example\npx.launch_app(trace=px.load_example_traces(\"llama_index_rag\"))\n```\n\n----------------------------------------\n\nTITLE: Testing Custom LLM Instance (Python)\nDESCRIPTION: Performs a simple test call to the instantiated `funny_model` with a test string. This basic interaction checks if the model instance is functional, though it may randomly raise the simulated error due to the override in `FunnyAIModel`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfunny_model(\"Hello world, this is a test if you are working?\")\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenAI API Access\nDESCRIPTION: Configures access to the OpenAI API by either retrieving the API key from environment variables or prompting the user to enter it interactively.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Range-based Output Evaluator in Python\nDESCRIPTION: This code defines a basic custom evaluator function to check if an experiment output is within the numeric range of 1 to 100. Suitable for experiments where the output should be a bounded number. The evaluator expects a numerical input and returns a boolean; no external dependencies required.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/how-to-experiments/using-evaluators.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef in_bounds(x):\n    return 1 <= x <= 100\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OpenAI Calls with OpenInference and Phoenix OTEL\nDESCRIPTION: Sets up OpenTelemetry tracing for OpenAI API calls using Phoenix. It registers a tracer provider with a specified project name and instruments the `openai` library to automatically capture trace data for visualization in Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/few-shot-prompting.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nfrom phoenix.otel import register\n\ntracer_provider = register(project_name=\"few-shot-examples\")\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Computer Use Agent\nDESCRIPTION: This command builds a Docker image named `computer-use-agent` with the tag `1.0`.  It creates a container that includes all dependencies for the agent. The Dockerfile should be in the current directory (represented by `.`). The output is an image ready to run the agent.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/examples/computer_use_agent/README.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ndocker build -t computer-use-agent:1.0 .\n```\n\n----------------------------------------\n\nTITLE: Setting A-Frame Sky Background (HTML)\nDESCRIPTION: Defines the background of the 3D scene as a sky using the `<a-sky>` primitive. The `color` attribute sets the sky's color.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_6\n\nLANGUAGE: HTML\nCODE:\n```\n<a-sky color=\"#ECECEC\"></a-sky>\n```\n\n----------------------------------------\n\nTITLE: Generating Text: Phoenix\nDESCRIPTION: This snippet demonstrates a series of characters. It contains no specific programming language.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/manual_instrumentation_tutorial.ipynb#_snippet_8\n\nLANGUAGE: text\nCODE:\n```\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\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key Using Environment Variables\nDESCRIPTION: Prompts the user for the OpenAI API key if not found in environment variables and sets it in the environment for subsequent API calls. Uses getpass for secure input.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_openai_tutorial.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key for Model Authentication - Python\nDESCRIPTION: Checks for OpenAI API key in the environment; if not present, prompts the user securely for a key and sets it. Key is stored as the 'OPENAI_API_KEY' environment variable, which is required for making OpenAI LLM requests. No input parameters; prompts via notebook if variable absent.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"\\ud83d\\udd11 Enter your OpenAI API key: \")\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI and Phoenix API Keys in Python\nDESCRIPTION: Retrieves OpenAI and Phoenix API keys required for authentication. It first attempts to get the keys from environment variables (`OPENAI_API_KEY`, `PHOENIX_API_KEY`). If not found, it securely prompts the user for the keys using `getpass`. Finally, it ensures the keys are set as environment variables for use by other libraries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/hosted_phoenix/hosted_phoenix_llamaindex_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n\nif not (phoenix_api_key := os.getenv(\"PHOENIX_API_KEY\")):\n    phoenix_api_key = getpass(\"ðŸ”‘ Enter your Phoenix API key: \")\n\nos.environ[\"PHOENIX_API_KEY\"] = phoenix_api_key\n```\n\n----------------------------------------\n\nTITLE: Defining Ground Truth Labels Within DataFrame for Evaluation in Python\nDESCRIPTION: Populates a 'ground_truth' column with a predefined list of boolean values representing the true correctness of each SQL query/response pair. These labels serve as the gold standard for classification evaluation metrics.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evals_sql_correctness_eval_with_custom_agent.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndf[\"ground_truth\"] = [\n    True,\n    True,\n    False,\n    False,\n    True,\n    False,\n    True,\n    True,\n    False,\n    True,\n    False,\n    True,\n    True,\n    True,\n    True,\n    False,\n    False,\n    False,\n    True,\n    False,\n    False,\n    False,\n    False,\n    True,\n    True,\n    True,\n    True,\n    True,\n    False,\n    False,\n    False,\n    False,\n    True,\n    False,\n    True,\n    False,\n    True,\n    False,\n    True,\n    True,\n    True,\n    True,\n    True,\n    True,\n    True,\n    False,\n    True,\n    False,\n    True,\n    False,\n]\ntrue_labels = df[\"ground_truth\"]\n```\n\n----------------------------------------\n\nTITLE: Inspecting Data Structure of a Single Experiment Run in Python\nDESCRIPTION: Accesses the first experiment run entry to examine its structure including input, output, and metadata fields. Useful for understanding experiment output formats or debugging experiment executions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nexperiment[0]\n```\n\n----------------------------------------\n\nTITLE: Printing the Phoenix UI URL\nDESCRIPTION: This code snippet prints the URL of the Phoenix UI using `session.url`. This allows the user to access the Phoenix UI and view the logged evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nprint(\"The Phoenix UI:\", session.url)\n```\n\n----------------------------------------\n\nTITLE: Inspecting QA Evaluation DataFrame Output in Python\nDESCRIPTION: Displays the QA evaluation data, typically after processing by run_evals, containing assessment scores and explanation fields. Assumes qa_eval_df has already been assigned.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/demos/demo_llama_index_rag.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nqa_eval_df\n```\n\n----------------------------------------\n\nTITLE: Retrieving Phoenix Session URL for Monitoring\nDESCRIPTION: This snippet prints the active Phoenix session URL for monitoring or further inspection of evaluation results. It depends on the 'px' session object provided by the Phoenix client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_44\n\nLANGUAGE: Python\nCODE:\n```\nprint(\"phoenix URL\", px.active_session().url)\n```\n\n----------------------------------------\n\nTITLE: Installing Smolagents and Dependencies\nDESCRIPTION: This bash command installs the `openinference-instrumentation-smolagents` package, along with the `smolagents` package. This is required for instrumenting and using the smolagents framework.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/hfsmolagents.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-smolagents smolagents\n```\n\n----------------------------------------\n\nTITLE: Pulling and Running Phoenix Docker Image\nDESCRIPTION: This code pulls the latest Phoenix Docker image from Docker Hub and runs it in a container exposing port 6006. It allows deploying Phoenix in a containerized environment for self-hosted observability setup. No dependencies besides Docker are required.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/anthropic.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\ndocker run -p 6006:6006 arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Configuring Phoenix Cloud Environment Variables\nDESCRIPTION: Python code to set up environment variables for connecting to Phoenix Cloud, including API key and collector endpoint.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/langchain.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add Phoenix API Key for tracing\nPHOENIX_API_KEY = \"ADD YOUR API KEY\"\nos.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n```\n\n----------------------------------------\n\nTITLE: Loading Text Data from an External URL into the Pipeline\nDESCRIPTION: Downloads a text file (Paul Graham's essay) into a temporary file, then loads it into LlamaIndex using SimpleDirectoryReader, preparing documents for indexing. Provides raw data necessary for building the retrieval pipeline.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_rag.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport tempfile\nfrom urllib.request import urlretrieve\n\nwith tempfile.NamedTemporaryFile() as tf:\n    urlretrieve(\n        \"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/main/data/paul_graham/paul_graham_essay.txt\",\n        tf.name,\n    )\n    documents = SimpleDirectoryReader(input_files=[tf.name]).load_data()\n\n```\n\n----------------------------------------\n\nTITLE: Displaying Phoenix UI URL\nDESCRIPTION: Prints a message with the URL to access the Phoenix UI for viewing the traces and evaluations.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_tracing_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"ðŸš€ Open the Phoenix UI if you haven't already: {session.url}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix API Key via Environment Variable in Python\nDESCRIPTION: Shows how to configure the Phoenix API key by setting an environment variable PHOENIX_API_KEY. This method allows the client to authenticate requests to the Phoenix server automatically without hardcoding credentials.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_API_KEY\"] = \"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Preparing Benchmark Dataset Sample for Evaluation\nDESCRIPTION: Samples the dataset to the specified size and renames columns to match the expected format for evaluation. This controls the runtime while providing representative results.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf = (\n    df.sample(n=N_EVAL_SAMPLE_SIZE)\n    .reset_index(drop=True)\n    .rename(columns={\"query\": \"input\", \"response\": \"output\"})\n)\n```\n\n----------------------------------------\n\nTITLE: Invoking Bedrock Model with Uninstrumented Client\nDESCRIPTION: This snippet calls the `invoke_model` method using the `uninstrumented_client`. This will execute a call to the LLM, and the output will be printed, but no tracing information will be captured. The code constructs a prompt, calls the Bedrock model, and prints the completion.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprompt = b\"{\\\"prompt\\\": \\\"Human: What is the 3rd month of the year in alphabetical order? Assistant:\\\", \\\"max_tokens_to_sample\\\": 1024}\"\nresponse = uninstrumented_client.invoke_model(modelId=\"anthropic.claude-v2:1\", body=prompt)\nresponse_body = json.loads(response.get(\"body\").read())\nprint(response_body[\"completion\"])\n```\n\n----------------------------------------\n\nTITLE: Creating a Phoenix Client Instance\nDESCRIPTION: Initializes a Phoenix client configured to connect to a local Phoenix server. The client can be customized with a base URL and authorization headers if needed for accessing a protected Phoenix instance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_vercel_ai_sdk.ipynb#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst px = PhoenixClient.createClient({\n  options: {\n    baseUrl: \"http://localhost:6006\",\n    // Uncomment this if you are using a Phoenix instance that requires an API key\n    // headers: {\n    //   Authorization: \"bearer xxxxxx\",\n    // }\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: Importing Phoenix, OpenAI, and Anthropic SDKs in JavaScript\nDESCRIPTION: This snippet imports the Phoenix client, OpenAI SDK, and Anthropic SDK using Deno and npm module imports. Dependencies include '@arizeai/phoenix-client', 'openai', and '@anthropic-ai/sdk', and ensure these are available via npm or compatible package manager. No parameters are involved as this is just for loading required modules.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_cross_sdk_tutorial.ipynb#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport * as PhoenixClient from \"npm:@arizeai/phoenix-client\"\nimport OpenAI from \"npm:openai\"\nimport { Anthropic } from \"npm:@anthropic-ai/sdk\"\n```\n\n----------------------------------------\n\nTITLE: Importing necessary libraries\nDESCRIPTION: This snippet imports the required libraries, including nest_asyncio for asynchronous operations, pandas for data manipulation, phoenix for tracing and evaluation, and various modules from phoenix for LLM evaluation, DSL queries, and semantic conventions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/internal/span_query_language.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\nimport pandas as pd\n\nimport phoenix as px\nfrom phoenix.evals import OpenAIModel, llm_classify\nfrom phoenix.evals.default_templates import (\n    HALLUCINATION_PROMPT_RAILS_MAP,\n    HALLUCINATION_PROMPT_TEMPLATE,\n    QA_PROMPT_RAILS_MAP,\n    QA_PROMPT_TEMPLATE,\n    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n    RAG_RELEVANCY_PROMPT_TEMPLATE,\n)\nfrom phoenix.trace.dsl import SpanQuery\nfrom phoenix.trace.dsl.helpers import (\n    INPUT,\n    IO,\n    IS_RETRIEVER,\n    IS_ROOT,\n    get_qa_with_reference,\n    get_retrieved_documents,\n)\nfrom phoenix.trace.semantic_conventions import (\n    DOCUMENT_CONTENT,\n    DOCUMENT_SCORE,\n    RETRIEVAL_DOCUMENTS,\n)\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Endpoint (Docker)\nDESCRIPTION: This Python code sets the `PHOENIX_COLLECTOR_ENDPOINT` environment variable to point to the Phoenix instance, assuming it's running on `http://localhost:6006`.  This configuration sends the tracing data to Phoenix for analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/hfsmolagents.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Assuming AWS IAM Role with Boto3 in Python\nDESCRIPTION: This snippet shows how to assume an AWS IAM role using Boto3's STS client to obtain temporary security credentials. It includes optional parameters for MFA and then creates a new Boto3 session using these temporary credentials, suitable for more secure or temporary access.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/evaluation-models.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n#If you need to assume a role\n# Creating an STS client\nsts_client = session.client('sts')\n\n# (optional - if needed) Assuming a role\nresponse = sts_client.assume_role(\n    RoleArn=\"arn:aws:iam::......\",\n    RoleSessionName=\"AssumeRoleSession1\",\n    #(optional) if MFA Required\n    SerialNumber='arn:aws:iam::...',\n    #Insert current token, needs to be run within x seconds of generation\n    TokenCode='PERIODIC_TOKEN'\n)\n\n# Your temporary credentials will be available in the response dictionary\ntemporary_credentials = response['Credentials']\n\n# Creating a new Boto3 session with the temporary credentials\nassumed_role_session = boto3.Session(\n    aws_access_key_id=temporary_credentials['AccessKeyId'],\n    aws_secret_access_key=temporary_credentials['SecretAccessKey'],\n    aws_session_token=temporary_credentials['SessionToken'],\n    region_name='us-east-1'\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Audio Emotion Classification Prompt Template in Plaintext\nDESCRIPTION: This snippet provides the EMOTION_PROMPT_TEMPLATE, a detailed prompt written in plaintext intended for an AI system to classify the primary emotion expressed in an audio file. It specifies key analytical features such as tone, pitch, pace, volume, and intensity with detailed instructions on response formatting and expected output values. The prompt ensures the AI selects the most dominant emotion from a specific list, emphasizing avoidance of the 'neutral' label unless no emotion is detected. This template requires integration into the Phoenix evaluation framework and serves as a core input for emotion detection model evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/evaluation/how-to-evals/running-pre-tested-evals/audio-emotion-detection.md#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nYou are an AI system designed to classify emotions in audio files.\\n\\n### TASK:\\nAnalyze the provided audio file and classify the primary emotion based on these characteristics:\\n- Tone: General tone of the speaker (e.g., cheerful, tense, calm).\\n- Pitch: Level and variability of the pitch (e.g., high, low, monotone).\\n- Pace: Speed of speech (e.g., fast, slow, steady).\\n- Volume: Loudness of the speech (e.g., loud, soft, moderate).\\n- Intensity: Emotional strength or expression (e.g., subdued, sharp, exaggerated).\\n\\nThe classified emotion must be one of the following:\\n['anger', 'happiness', 'excitement', 'sadness', 'neutral', 'frustration', 'fear', 'surprise', 'disgust', 'other']\\n\\nIMPORTANT: Choose the most dominant emotion expressed in the audio. Neutral should only be used when no other emotion is clearly present; do your best to avoid this label.\\n\\n************\\n\\nHere is the audio to classify:\\n\\n{audio}\\n\\nRESPONSE FORMAT:\\n\\nProvide a single word from the list above representing the detected emotion.\\n\\n************\\n\\nEXAMPLE RESPONSE: excitement\\n\\n************\\n\\nAnalyze the audio and respond in this format.\n```\n\n----------------------------------------\n\nTITLE: Displaying Formatted Evaluation DataFrame (Python)\nDESCRIPTION: Displays the `eval_df` DataFrame after formatting. It now includes the 'label' column as strings ('True'/'False') and a new 'score' column with corresponding integer values (1/0).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evaluating_traces_quickstart.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\neval_df\n```\n\n----------------------------------------\n\nTITLE: Downloading Training and Production Data using Pandas (Python)\nDESCRIPTION: Downloads training and production datasets for a human action image classification task from Google Cloud Storage URLs. The data is loaded into pandas DataFrames (`train_df`, `prod_df`) using the `pd.read_parquet` function.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/image_classification_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/cv/human-actions/human_actions_training.parquet\"\n)\nprod_df = pd.read_parquet(\n    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/cv/human-actions/human_actions_production.parquet\"\n)\n```\n\n----------------------------------------\n\nTITLE: Applying String-to-Array Conversion to DataFrame Columns\nDESCRIPTION: Applies the `string_to_array` function to the 'prompt_vector' and 'response_vector' columns of the `conversations_df` DataFrame. This converts the string representations of embeddings stored in the CSV into actual NumPy arrays suitable for analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/find_cluster_export_and_explore_with_gpt.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconversations_df[\"prompt_vector\"] = conversations_df[\"prompt_vector\"].apply(string_to_array)\nconversations_df[\"response_vector\"] = conversations_df[\"response_vector\"].apply(string_to_array)\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Phoenix Platform (Python)\nDESCRIPTION: This snippet initializes environment variables, creates a Phoenix client, and uploads a dataset named with a UUID. It requires the 'os' module, Phoenix SDK, and a DataFrame 'questions_df' containing input data for evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/agents-cookbook.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom uuid import uuid1\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n\nclient = px.Client()\n\ndataset = client.upload_dataset(\n    dataframe=questions_df,\n    dataset_name=\"agents-cookbook-\" + str(uuid1()),\n    input_keys=[\"question\"],\n)\n\n```\n\n----------------------------------------\n\nTITLE: Hosting Local Images for Embedding Schema URLs in Python Context\nDESCRIPTION: Provides instructions for serving local images via an HTTP server to make them accessible through URL links used in embedding schemas. The recommended approach involves running a Python HTTP server in the directory containing images, then referencing those images through HTTP URLs in the dataframe. This allows local images to be integrated with the Arize Phoenix embedding schema that expects URL links.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/how-to/define-your-schema/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npython -m http.server 8000\n```\n\n----------------------------------------\n\nTITLE: Downloading and Loading Dataset from Langchain Benchmarks\nDESCRIPTION: Downloads a JSON dataset from the langchain-benchmarks registry containing email extraction examples. It uses a temporary file to store the downloaded data, reads it as a pandas DataFrame, and samples a subset of the data for the experiment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndataset_name = \"Email Extraction\"\n\nwith tempfile.NamedTemporaryFile(suffix=\".json\") as f:\n    download_public_dataset(registry[dataset_name].dataset_id, path=f.name)\n    df = pd.read_json(f.name)[[\"inputs\", \"outputs\"]]\ndf = df.sample(10, random_state=42)\n```\n\n----------------------------------------\n\nTITLE: Importing Phoenix Span Evaluation and Query APIs - Python\nDESCRIPTION: Imports the SpanEvaluations class and SpanQuery DSL from the Phoenix library, enabling trace querying and evaluation of agent function call spans. Requirements: Phoenix must be installed and accessible in the environment. These imports allow for advanced span data extraction and evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_tool_calling.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nfrom phoenix.trace import SpanEvaluations\nfrom phoenix.trace.dsl import SpanQuery\n```\n\n----------------------------------------\n\nTITLE: Combine Document and Relevance DataFrames\nDESCRIPTION: Combines the retrieved documents DataFrame and the relevance evaluation DataFrame. This creates a single DataFrame, which has the retrieved documents and their corresponding relevance scores from the LLM.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/mistral/evaluate_rag--mistral.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndocuments_with_relevance_df = pd.concat(\n    [retrieved_documents_df, retrieved_documents_relevance_df.add_prefix(\"eval_\")], axis=1\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Jailbreak Classification Dataset Using Hugging Face Datasets in Python\nDESCRIPTION: Loads the 'jackhhao/jailbreak-classification' training dataset via the datasets library, converts it to a pandas DataFrame, and samples 50 entries to form a test set. This dataset will be the basis for experiments evaluating prompt performance on classification tasks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/prompt-optimization.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nds = load_dataset(\"jackhhao/jailbreak-classification\")[\"train\"]\nds = ds.to_pandas().sample(50)\nds.head()\n```\n\n----------------------------------------\n\nTITLE: Authenticating Colab Environment\nDESCRIPTION: This code authenticates the user's environment, specifically for Google Colab. It imports the `auth` module from `google.colab` and calls `auth.authenticate_user()` to enable access to Google Cloud resources when running the notebook in Colab.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/google_agent_engine_tracing_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport sys\n\nif \"google.colab\" in sys.modules:\n    from google.colab import auth\n\n    auth.authenticate_user()\n```\n\n----------------------------------------\n\nTITLE: Generating Session ID Python\nDESCRIPTION: Defines a function `gen_session_id()` to generate a session ID. It uses Faker for realistic session IDs, along with random generation, and returns either a colon-separated string, a fake address or an integer.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/multi-turn_chat_sessions.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef gen_session_id():\n    p = random()\n    if p < 0.1:\n        return \":\" * randint(1, 5)\n    if p < 0.9:\n        return fake.address()\n    return int(abs(random()) * 1_000_000_000)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Dataframe for Non-null Questions in Python\nDESCRIPTION: Filters a pandas DataFrame named questions_with_document_chunk_df to retain only rows where the column \"question\" is not null. This step ensures that all subsequent operations work with valid question entries generated by the LLM. It requires pandas to be imported and a DataFrame variable already defined. The input is a DataFrame and the output is a filtered DataFrame with no null questions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases/rag-evaluation.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nquestions_with_document_chunk_df = questions_with_document_chunk_df[questions_with_document_chunk_df[\"question\"].notnull()]\n```\n\n----------------------------------------\n\nTITLE: Accessing and Inspecting Individual Entries of the Dataset in Python\nDESCRIPTION: Accesses the first entry (index 0) in the Phoenix dataset allowing examination of the data structure and content of specific examples within the dataset. This aids in debugging and understanding input formats.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndataset[0]\n```\n\n----------------------------------------\n\nTITLE: Installing Haystack Instrumentation Packages (Bash)\nDESCRIPTION: Installs the necessary packages for OpenInference auto-instrumentation with Haystack. This includes `openinference-instrumentation-haystack` for the specific instrumentation logic and `haystack-ai` for the Haystack framework itself.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/haystack.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-haystack haystack-ai\n```\n\n----------------------------------------\n\nTITLE: Viewing Phoenix Session in UI (Shorthand) - Python\nDESCRIPTION: Provides an alternative or subsequent way to show the Phoenix UI for the active session. This can be called independently after session creation for further interaction. Assumes a valid session object is already initialized.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/credit_card_fraud_tutorial.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nsession.view()\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix OTEL Package Using pip - Bash\nDESCRIPTION: Installs the arize-phoenix-otel Python package required for tracing and instrumentation with Phoenix. This ensures your Python environment is set up to send trace data from OpenAI API calls. Run this command in your terminal prior to configuring Python scripts or launching Phoenix instances.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/openai.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Set OpenAI API Key\nDESCRIPTION: This Python code snippet retrieves the OpenAI API key from the environment variables. If the API key is not found, it prompts the user to enter it and then sets it as an environment variable.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evals_quickstart.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix Packages (Command Line)\nDESCRIPTION: This command installs the `arize-phoenix-otel` package, which is necessary to enable tracing with Arize Phoenix. This package provides OpenTelemetry instrumentation for sending traces to the Arize Phoenix platform.  It is a prerequisite for tracing LlamaIndex workflows within Phoenix.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/llamaindex-1.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix-otel\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix Client via pip in Shell\nDESCRIPTION: Installs the arize-phoenix-client Python package using pip with flags to upgrade quietly. This is the recommended method to setup the Phoenix client environment and its dependencies prior to usage.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/packages/phoenix-client/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -Uq arize-phoenix-client\n```\n\n----------------------------------------\n\nTITLE: Setting Phoenix Docker Endpoint\nDESCRIPTION: Sets the `PHOENIX_COLLECTOR_ENDPOINT` environment variable to the Docker container's address. This is used to connect to the Phoenix instance that is running on localhost, exposed via docker.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/instructor.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix and OpenAI Dependencies (Bash)\nDESCRIPTION: Installs the necessary Python packages `arize-phoenix` (version 4.29.0 or later), `openai`, and `httpx` (version less than 0.28) using pip. These are prerequisites for interacting with Phoenix and the OpenAI API.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/quickstarts/evaluating_traces_quickstart.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n%%bash\npip install -q \"arize-phoenix>=4.29.0\"\npip install -q openai 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Pulling Phoenix Docker Image - Docker\nDESCRIPTION: Fetches the latest Phoenix image from Docker Hub for container-based deployments. Ensure Docker is installed and running. This command does not start a container; it only retrieves the image.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/litellm.md#_snippet_4\n\nLANGUAGE: docker\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\n\n```\n\n----------------------------------------\n\nTITLE: Pulling Phoenix Docker Image\nDESCRIPTION: This command pulls the latest Phoenix Docker image from Docker Hub.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/instructor.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull arizephoenix/phoenix:latest\n```\n\n----------------------------------------\n\nTITLE: Apply nest_asyncio Python\nDESCRIPTION: Applies the `nest_asyncio` patch. This allows the execution of nested asyncio event loops, which is necessary for running asynchronous code (like the OpenAI API calls) within environments that already have an event loop running, such as Jupyter notebooks.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/datasets-and-experiments/use-cases-datasets/text2sql.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries - Python\nDESCRIPTION: Imports all required Python libraries and modules. This includes components from `langchain` and `langchain_openai` for agent construction, `openai` for API interaction, `os` and `getpass` for credential management, `phoenix` for tracing, and `openinference.instrumentation.langchain` for automatic instrumentation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_agent_tracing_tutorial.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport openai\nfrom langchain.agents import AgentType, Tool, initialize_agent\nfrom langchain.chains import LLMMathChain\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_openai import ChatOpenAI\nfrom openinference.instrumentation.langchain import LangChainInstrumentor\n\nimport phoenix as px\nfrom phoenix.otel import register\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for LangChain Application with Phoenix Tracing\nDESCRIPTION: Imports necessary Python libraries for building a LangChain application with OpenInference tracing and Phoenix evaluation capabilities.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/langchain_tracing_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nfrom getpass import getpass\nfrom urllib.request import urlopen\n\nimport nest_asyncio\nimport numpy as np\nimport pandas as pd\nfrom langchain.chains import RetrievalQA\nfrom langchain.retrievers import KNNRetriever\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom openinference.instrumentation.langchain import LangChainInstrumentor\nfrom tqdm import tqdm\n\nimport phoenix as px\nfrom phoenix.evals import (\n    HallucinationEvaluator,\n    OpenAIModel,\n    QAEvaluator,\n    RelevanceEvaluator,\n    run_evals,\n)\nfrom phoenix.otel import register\nfrom phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\nfrom phoenix.trace import DocumentEvaluations, SpanEvaluations\n\nnest_asyncio.apply()  # needed for concurrent evals in notebook environments\n```\n\n----------------------------------------\n\nTITLE: Enabling Asynchronous Code Execution in Notebooks with nest_asyncio in Python\nDESCRIPTION: This snippet installs the nest_asyncio event loop policy so asynchronous code can run within Jupyter/IPython notebooks, enabling async function calls such as those made to OpenAI's API. The nest_asyncio package must be installed beforehand. There is no input; this simply configures the notebook environment for subsequent async usage.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Resetting and Launching Phoenix App\nDESCRIPTION: This snippet resets and relaunches the Phoenix application using the `px.close_app()` and `px.launch_app()` functions. This ensures a clean state for logging and visualization of retrieval evaluations. It depends on the `phoenix` library.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/use-cases-evals/rag-evaluation.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n# First things first, let's reset phoenix\npx.close_app()\npx.launch_app()\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries and Module Setup - Python\nDESCRIPTION: Imports standard libraries and third-party modules necessary for data manipulation, plotting, evaluation, and running the Phoenix evaluation workflow. Assumes successful installation of all listed packages. No parameters required; modifies pandas display option for column width.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_human_vs_ai_classifications.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pycm import ConfusionMatrix\nfrom sklearn.metrics import classification_report\n\nfrom phoenix.evals import (\n    HUMAN_VS_AI_PROMPT_RAILS_MAP,\n    HUMAN_VS_AI_PROMPT_TEMPLATE,\n    OpenAIModel,\n    llm_classify,\n)\n\npd.set_option(\"display.max_colwidth\", None)\n```\n\n----------------------------------------\n\nTITLE: Defining HALLUCINATION_SPAN_PROMPT_TEMPLATE in Python\nDESCRIPTION: This snippet defines a template, specifically HALLUCINATION_SPAN_PROMPT_TEMPLATE, likely used for prompting a language model to identify hallucinations within a span of text. The `autodata` directive suggests that this template is a pre-defined string constant. There are no directly visible dependencies; it assumes the project has necessary modules and libraries installed and available during the documentation generation process. The input would likely be a piece of text for the model to evaluate, and the output, after the model's processing, could indicate whether or not the span contains a hallucination.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/api_reference/source/api/evals.span_templates.rst#_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for ReAct Prompting\nDESCRIPTION: Imports necessary Python libraries including Phoenix client, OpenAI SDK, and evaluation utilities needed for implementing ReAct prompting.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/react-prompting.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\nimport pandas as pd\nfrom openai import OpenAI\nfrom openai.types.chat.completion_create_params import CompletionCreateParamsBase\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nimport phoenix as px\nfrom phoenix.client import Client as PhoenixClient\nfrom phoenix.client.types import PromptVersion\nfrom phoenix.evals import (\n    TOOL_CALLING_PROMPT_RAILS_MAP,\n    OpenAIModel,\n    llm_classify,\n)\nfrom phoenix.experiments import run_experiment\nfrom phoenix.otel import register\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for LiteLLM - Python\nDESCRIPTION: Sets the 'OPENAI_API_KEY' environment variable to authenticate requests to OpenAI models through LiteLLM. Replace 'PASTE_YOUR_API_KEY_HERE' with your actual OpenAI API key. Must be executed before making authenticated LLM queries in the application.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/litellm.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"PASTE_YOUR_API_KEY_HERE\"\n\n```\n\n----------------------------------------\n\nTITLE: Displaying Exported Data Head with Pandas in Python\nDESCRIPTION: This snippet retrieves the most recently exported data as a Pandas DataFrame and then displays the first few rows using the `head()` method. It assumes the existence of a `session` object that provides access to the exported data.  The output is the first few rows of the dataframe, allowing for a quick inspection of the data's structure and contents.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/credit_card_fraud_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nexport_df = session.exports[-1]\nexport_df.head()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for OpenTelemetry Span Filtering in Python\nDESCRIPTION: Imports necessary Python modules for OpenTelemetry tracing (trace_api), OTLP and Console exporters, SDK components (TracerProvider, SpanProcessor, SpanExporter), environment variable loading (dotenv), user input (getpass), the OpenAI client, and OpenAI instrumentation (OpenAIInstrumentor).\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/span_filtering_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom dotenv import load_dotenv\nfrom opentelemetry import trace as trace_api\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import ConsoleSpanExporter, SpanExporter, SpanProcessor\n\nload_dotenv()\n\nfrom getpass import getpass\n\nimport openai\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Phoenix Analysis in Python\nDESCRIPTION: Imports necessary Python libraries: Pandas for DataFrame operations, specific classes from the `arize` library for embedding generation, and the `phoenix` library aliased as `px` for visualization and analysis.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/sentiment_classification_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom arize.pandas.embeddings import EmbeddingGenerator, UseCases\n\nimport phoenix as px\n```\n\n----------------------------------------\n\nTITLE: Applying nest_asyncio Patch (Python)\nDESCRIPTION: Applies a patch to the `asyncio` event loop using `nest_asyncio`. This is necessary in notebook environments (like Jupyter or Google Colab) to allow event loops to be re-entrant, enabling smoother execution of asynchronous operations like LLM requests and potentially improving performance.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluations_with_error_handling.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for LlamaIndex, MongoDB, and Phoenix\nDESCRIPTION: Imports required Python modules for the application. This includes standard libraries (`json`, `os`, `urllib`, `getpass`), asynchronous utilities (`nest_asyncio`), OpenAI client, data manipulation (`pandas`), LlamaIndex components (storage, indexing, querying, settings, embeddings, LLMs, readers, vector stores), MongoDB client (`pymongo`), and Arize Phoenix for tracing and evaluation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/integrations/tracing_and_evals_with_mongodb_and_llama_index.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nimport urllib\nfrom getpass import getpass\nfrom urllib.request import urlopen\n\nimport nest_asyncio\nimport openai\nimport pandas as pd\nfrom llama_index.core import StorageContext, set_global_handler\nfrom llama_index.core.indices.vector_store.base import VectorStoreIndex\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.settings import Settings\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.readers.mongodb import SimpleMongoReader\nfrom llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\nfrom pymongo.operations import SearchIndexModel\nfrom tqdm import tqdm\n\nimport phoenix as px\nfrom phoenix.evals import (\n    HallucinationEvaluator,\n    OpenAIModel,\n    QAEvaluator,\n    RelevanceEvaluator,\n    run_evals,\n)\nfrom phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\nfrom phoenix.trace import DocumentEvaluations, SpanEvaluations\n\nnest_asyncio.apply()  # needed for concurrent evals in notebook environments\npd.set_option(\"display.max_colwidth\", 1000)\n```\n\n----------------------------------------\n\nTITLE: Launch Local Phoenix Server via CLI Bash\nDESCRIPTION: Launches the Phoenix application server locally from the command line. This command starts the Phoenix backend accessible via a web interface, typically at `http://localhost:6006`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/vertexai.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Determining CrewAI Version via Shell Command\nDESCRIPTION: Displays the currently installed version of the CrewAI package using a pip command. The output guides which additional instrumentation library should be used to trace underlying LLM calls depending on whether CrewAI version is below or above 0.63.0.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/crewai_tracing_tutorial.ipynb#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n! pip show crewai | grep Version\n```\n\n----------------------------------------\n\nTITLE: Display docstring for phoenix.load_example function - Python\nDESCRIPTION: This snippet displays the docstring for the `phoenix.load_example` function, which contains a list of available inference sets for download. This helps users discover available example datasets.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/inferences/how-to-inferences/use-example-inferences.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npx.load_example?\n```\n\n----------------------------------------\n\nTITLE: Importing and Configuring Pandas and Hugging Face Datasets in Python\nDESCRIPTION: This snippet imports essential libraries including json, datasets from Hugging Face, and pandas, and configures pandas to display long column values. It is a preparatory step with no parameters and is required before any other dataset manipulation in the script.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/data/wrangle_wiki_qa.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nimport datasets\nimport pandas as pd\n\npd.set_option(\"display.max_colwidth\", 1000)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Modules\nDESCRIPTION: Imports all necessary modules from various libraries for the tutorial, including OS, getpass, chromadb, LlamaIndex core and components, OpenAI, OpenInference instrumentation, and Phoenix tracing utilities. These imports are foundational for setting up vector stores, LLMs, agents, and tracing.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/agentic_rag_tracing.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport chromadb\nimport chromadb.utils.embedding_functions as embedding_functions\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.core.agent import ReActAgent\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.vector_stores.chroma import ChromaVectorStore\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n\nfrom phoenix.otel import register\n```\n\n----------------------------------------\n\nTITLE: Prompting for Anthropic API Key in JavaScript\nDESCRIPTION: Prompts the user for their Anthropic API key using the built-in prompt function. It ensures secure handling of credentials required for invoking the Anthropic SDK.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/js/examples/notebooks/phoenix_prompts_cross_sdk_tutorial.ipynb#_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst anthropicApiKey = prompt(\"Enter your Anthropic API key:\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Phoenix and Bedrock Tracing\nDESCRIPTION: This code snippet installs the necessary Python packages for using Arize Phoenix, interacting with AWS Bedrock via boto3, and instrumenting Bedrock with OpenInference. It uses pip to install `arize-phoenix`, `boto3`, and `openinference-instrumentation-bedrock`.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/bedrock_tracing_and_evals_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install arize-phoenix boto3 openinference-instrumentation-bedrock\n```\n\n----------------------------------------\n\nTITLE: Installing LiteLLM Instrumentation and LiteLLM Packages - Bash\nDESCRIPTION: Installs both 'openinference-instrumentation-litellm' for tracing and 'litellm' for LLM API interaction via pip. Ensure these are installed in your Python environment prior to registering instrumentation or making LLM API calls.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/litellm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install openinference-instrumentation-litellm litellm\n\n```\n\n----------------------------------------\n\nTITLE: Installing BeeAI Instrumentation Dependencies\nDESCRIPTION: Shows the `npm install` commands required to add the BeeAI instrumentation package, the BeeAI framework itself, OpenTelemetry Node.js SDK components (including the OTLP exporter), and OpenInference/OpenTelemetry semantic conventions to a Node.js project.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/beeai.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save @arizeai/openinference-instrumentation-beeai beeai-framework\n\nnpm install --save @opentelemetry/sdk-node @opentelemetry/exporter-trace-otlp-http @opentelemetry/semantic-conventions @arizeai/openinference-semantic-conventions\n```\n\n----------------------------------------\n\nTITLE: Installing Arize Phoenix Package and Running Local Phoenix Server with Bash\nDESCRIPTION: This snippet installs the arize-phoenix Python package and starts a local Phoenix instance by running the server. It is intended for local development and testing environments where you prefer to run Phoenix locally instead of using the cloud service.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/integrations-tracing/prompt-flow.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install arize-phoenix\nphoenix serve\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages for ReAct Prompting\nDESCRIPTION: Installs Arize Phoenix, datasets library, and OpenInference instrumentation for OpenAI to enable ReAct prompting implementation.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/prompt-engineering/use-cases-prompts/react-prompting.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qqq \"arize-phoenix>=8.0.0\" datasets openinference-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix evaluation dependencies\nDESCRIPTION: Installs the necessary Python packages for running Phoenix evaluations, including the Arize Phoenix evals package, OpenAI client, and visualization libraries.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qq \"arize-phoenix-evals>=0.0.5\" \"openai>=1\" ipython matplotlib pycm scikit-learn tiktoken nest_asyncio 'httpx<0.28'\n```\n\n----------------------------------------\n\nTITLE: Setting Anthropic API Key (Python)\nDESCRIPTION: Checks if the Anthropic API key is set as an environment variable. If not, it prompts the user securely for the key and sets it for the current session.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/prompts/from_anthropic.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif not os.getenv(\"ANTHROPIC_API_KEY\"):\n    os.environ[\"ANTHROPIC_API_KEY\"] = getpass(\"Anthropic API key: \")\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key (Python)\nDESCRIPTION: Loads environment variables from a .env file and prompts the user for their OpenAI API key if not found. Sets the API key as an environment variable for use by the OpenAI client.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/docs/tracing/how-to-tracing/feedback-and-annotations/evaluating-phoenix-traces.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nimport dotenv\n\ndotenv.load_dotenv()\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Importing HuggingFace Datasets Library in Python\nDESCRIPTION: This snippet imports the load_dataset function from the datasets module, required for loading datasets from the HuggingFace Hub. The primary dependency is the 'datasets' Python package, which must be installed prior to use. There are no further parameters required for this import statement.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/fixtures/ChatRAG-Bench.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n```\n\n----------------------------------------\n\nTITLE: Installing Phoenix from Source Distribution\nDESCRIPTION: Command to install Phoenix from a source distribution tarball. This is useful for testing a built version of Phoenix in a separate environment.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/DEVELOPMENT.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\npip install /path/to/source/distribution/tarball.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Providing URL to Phoenix UI for trace visualization\nDESCRIPTION: Outputs the Phoenix dashboard URL, instructing users to open it in a browser for visual analysis of trace data collected during agent interactions.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_openai_agent_tracing_tutorial.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nprint(f\"Open the Phoenix UI if you haven't already: {session.url}\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Phoenix UI for Trace Analysis\nDESCRIPTION: Prints the URL to access the Phoenix UI for interactive exploration of the traces collected during application execution.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/tutorials/evals/local_llm_evals.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(\"The Phoenix UI:\", session.url)\n```\n\n----------------------------------------\n\nTITLE: Running Analytics Script (Shell)\nDESCRIPTION: These commands are used to prepare the environment and run the primary analytics Python script. First, source the `.env` file to load configurations, then execute the `analytics.py` file using the Python interpreter.\nSOURCE: https://github.com/arize-ai/phoenix/blob/main/scripts/analytics/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nsource .env\npython analytics.py\n```"
  }
]
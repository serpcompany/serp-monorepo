[
  {
    "owner": "lexiforest",
    "repo": "curl_cffi",
    "content": "TITLE: Basic Usage with Browser Impersonation in curl_cffi\nDESCRIPTION: Demonstrates how to make HTTP requests with curl_cffi while impersonating Chrome browser. The response can be parsed as JSON, and the impersonation can be verified through the JA3 fingerprint hash.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/README.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport curl_cffi\n\n# Notice the impersonate parameter\nr = curl_cffi.get(\"https://tls.browserleaks.com/json\", impersonate=\"chrome\")\n\nprint(r.json())\n# output: {..., \"ja3n_hash\": \"aa56c057ad164ec4fdcb7a5a283be9fc\", ...}\n# the js3n fingerprint should be the same as target browser\n\n# To keep using the latest browser version as `curl_cffi` updates,\n# simply set impersonate=\"chrome\" without specifying a version.\n# Other similar values are: \"safari\" and \"safari_ios\"\nr = curl_cffi.get(\"https://tls.browserleaks.com/json\", impersonate=\"chrome\")\n\n# Randomly choose a browser version based on current market share in real world\n# from: https://caniuse.com/usage-table\n# NOTE: this is a pro feature.\nr = curl_cffi.get(\"https://example.com\", impersonate=\"realworld\")\n\n# To pin a specific version, use version numbers together.\nr = curl_cffi.get(\"https://tls.browserleaks.com/json\", impersonate=\"chrome124\")\n\n# To impersonate other than browsers, bring your own ja3/akamai strings\n# See examples directory for details.\nr = curl_cffi.get(\"https://tls.browserleaks.com/json\", ja3=..., akamai=...)\n\n# http/socks proxies are supported\nproxies = {\"https\": \"http://localhost:3128\"}\nr = curl_cffi.get(\"https://tls.browserleaks.com/json\", impersonate=\"chrome\", proxies=proxies)\n\nproxies = {\"https\": \"socks://localhost:3128\"}\nr = curl_cffi.get(\"https://tls.browserleaks.com/json\", impersonate=\"chrome\", proxies=proxies)\n```\n\n----------------------------------------\n\nTITLE: Basic GET Request with Browser Impersonation in curl_cffi\nDESCRIPTION: Demonstrates making a simple GET request with curl_cffi while impersonating Chrome's browser fingerprint. Shows how to specify browser versions, use the latest browser version, and configure proxies.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport curl_cffi\n\nurl = \"https://tools.scrapfly.io/api/fp/ja3\"\n\n# Notice the impersonate parameter\nr = curl_cffi.get(\"https://tools.scrapfly.io/api/fp/ja3\", impersonate=\"chrome110\")\n\nprint(r.json())\n# output: {..., \"ja3n_hash\": \"aa56c057ad164ec4fdcb7a5a283be9fc\", ...}\n# the js3n fingerprint should be the same as target browser\n\n# To keep using the latest browser version as `curl_cffi` updates,\n# simply set impersonate=\"chrome\" without specifying a version.\n# Other similar values are: \"safari\" and \"safari_ios\"\nr = curl_cffi.get(\"https://tools.scrapfly.io/api/fp/ja3\", impersonate=\"chrome\")\n\n# http/socks proxies are supported\nproxies = {\"https\": \"http://localhost:3128\"}\nr = curl_cffi.get(\"https://tools.scrapfly.io/api/fp/ja3\", impersonate=\"chrome110\", proxies=proxies)\n\nproxies = {\"https\": \"socks://localhost:3128\"}\nr = curl_cffi.get(\"https://tools.scrapfly.io/api/fp/ja3\", impersonate=\"chrome110\", proxies=proxies)\n```\n\n----------------------------------------\n\nTITLE: Asynchronous HTTP Requests with curl_cffi's AsyncSession\nDESCRIPTION: Demonstrates how to perform asynchronous HTTP requests using curl_cffi's AsyncSession. The example shows both a simple async request and concurrent requests using asyncio.gather.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/README.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom curl_cffi import AsyncSession\n\nasync with AsyncSession() as s:\n    r = await s.get(\"https://example.com\")\n```\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom curl_cffi import AsyncSession\n\nurls = [\n    \"https://google.com/\",\n    \"https://facebook.com/\",\n    \"https://twitter.com/\",\n]\n\nasync with AsyncSession() as s:\n    tasks = []\n    for url in urls:\n        task = s.get(url)\n        tasks.append(task)\n    results = await asyncio.gather(*tasks)\n```\n\n----------------------------------------\n\nTITLE: Making Asynchronous Requests with AsyncSession\nDESCRIPTION: Demonstrates using the AsyncSession class for asynchronous HTTP requests with curl_cffi, showing both a simple request and multiple concurrent requests with asyncio.gather.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/index.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom curl_cffi import AsyncSession\n\nasync with AsyncSession() as s:\n    r = await s.get(\"https://example.com\")\n```\n\n----------------------------------------\n\nTITLE: Using Sessions with curl_cffi\nDESCRIPTION: Example showing how to use session objects in curl_cffi for maintaining cookies between requests, similar to the requests library API.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/index.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ns = requests.Session()\n\n# httpbin is a http test website\ns.get(\"https://httpbin.org/cookies/set/foo/bar\")\n\nprint(s.cookies)\n# <Cookies[<Cookie foo=bar for httpbin.org />]>\n\nr = s.get(\"https://httpbin.org/cookies\")\nprint(r.json())\n# {'cookies': {'foo': 'bar'}}\n```\n\n----------------------------------------\n\nTITLE: Using Sessions in curl_cffi for Cookie Management\nDESCRIPTION: Shows how to use curl_cffi's Session object to maintain cookies across multiple requests. This example demonstrates setting cookies on a server and then verifying they're being sent in subsequent requests.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/README.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ns = curl_cffi.Session()\n\n# httpbin is a http test website, this endpoint makes the server set cookies\ns.get(\"https://httpbin.org/cookies/set/foo/bar\")\nprint(s.cookies)\n# <Cookies[<Cookie foo=bar for httpbin.org />]>\n\n# retrieve cookies again to verify\nr = s.get(\"https://httpbin.org/cookies\")\nprint(r.json())\n# {'cookies': {'foo': 'bar'}}\n```\n\n----------------------------------------\n\nTITLE: Using Low-level curl API with curl_cffi in Python\nDESCRIPTION: This snippet demonstrates how to use the low-level curl-like API provided by curl_cffi. It sets up a Curl object, configures options, impersonates a Chrome browser, and performs a request.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/advanced.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom curl_cffi import Curl, CurlOpt\nfrom io import BytesIO\n\nbuffer = BytesIO()\nc = Curl()\nc.setopt(CurlOpt.URL, b'https://tls.browserleaks.com/json')\nc.setopt(CurlOpt.WRITEDATA, buffer)\n\nc.impersonate(\"chrome124\")\n\nc.perform()\nc.close()\nbody = buffer.getvalue()\nprint(body.decode())\n```\n\n----------------------------------------\n\nTITLE: Basic Browser Impersonation with curl_cffi\nDESCRIPTION: Simple example demonstrating how to impersonate the latest Chrome browser using curl_cffi. This approach automatically uses the most recent Chrome version available in the library.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/impersonate.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport curl_cffi\n\ncurl_cffi.get(url, impersonate=\"chrome\")\n```\n\n----------------------------------------\n\nTITLE: Establishing Asynchronous WebSocket Connection with curl_cffi\nDESCRIPTION: This code demonstrates how to create an asynchronous WebSocket connection using curl_cffi's AsyncSession. It establishes a connection to an echo WebSocket server, concurrently sends multiple \"Hello, World!\" messages using asyncio.gather, and then asynchronously iterates through and prints all the responses.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/README.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom curl_cffi import AsyncSession\n\nasync with AsyncSession() as s:\n    ws = await s.ws_connect(\"wss://echo.websocket.org\")\n    await asyncio.gather(*[ws.send_str(\"Hello, World!\") for _ in range(10)])\n    async for message in ws:\n        print(message)\n```\n\n----------------------------------------\n\nTITLE: WebSocket Connection with curl_cffi\nDESCRIPTION: Demonstrates establishing a WebSocket connection using curl_cffi, including setting up a message handler callback and maintaining a persistent connection.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/index.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom curl_cffi import Session, WebSocket\n\ndef on_message(ws: WebSocket, message):\n    print(message)\n\nwith Session() as s:\n    ws = s.ws_connect(\n        \"wss://api.gemini.com/v1/marketdata/BTCUSD\",\n        on_message=on_message,\n    )\n    ws.run_forever()\n```\n\n----------------------------------------\n\nTITLE: WebSockets Implementation in curl_cffi\nDESCRIPTION: Shows how to use curl_cffi's WebSocket implementation to establish a persistent connection to a WebSocket server. This example connects to a cryptocurrency market data stream and prints incoming messages.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/README.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom curl_cffi import WebSocket\n\ndef on_message(ws: WebSocket, message: str | bytes):\n    print(message)\n\nws = WebSocket(on_message=on_message)\nws.run_forever(\"wss://api.gemini.com/v1/marketdata/BTCUSD\")\n```\n\n----------------------------------------\n\nTITLE: Customizing Header Order in curl_cffi\nDESCRIPTION: Shows how to use custom headers with a specific order rather than the default headers that come with browser impersonation. This is done by setting default_headers to False and providing your own headers.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/faq.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nrequests.get(url, impersonate=\"chrome\", default_headers=False, headers=...)\n```\n\n----------------------------------------\n\nTITLE: Handling Multiple Concurrent Requests with AsyncSession\nDESCRIPTION: Example showing how to make multiple concurrent HTTP requests using curl_cffi's AsyncSession with asyncio.gather for efficient parallel processing.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/index.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom curl_cffi import AsyncSession\n\nurls = [\n    \"https://google.com/\",\n    \"https://facebook.com/\",\n    \"https://twitter.com/\",\n]\n\nasync with AsyncSession() as s:\n    tasks = []\n    for url in urls:\n        task = s.get(url)\n        tasks.append(task)\n    results = await asyncio.gather(*tasks)\n```\n\n----------------------------------------\n\nTITLE: Using curl_cffi as a requests adapter in Python\nDESCRIPTION: This example demonstrates how to use curl_cffi as a requests adapter using the curl-adapter library. It allows full functionality of requests while leveraging curl_cffi's capabilities.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/advanced.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom curl_adapter import CurlCffiAdapter\n\nsession = requests.Session()\nsession.mount(\"http://\", CurlCffiAdapter())\nsession.mount(\"https://\", CurlCffiAdapter())\n\n# just use requests session like you normally would\nsession.get(\"https://example.com\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Proxy with curl_cffi\nDESCRIPTION: Demonstrates how to set up a proxy when making HTTP requests with curl_cffi. This example shows the proxy parameter syntax including authentication credentials.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/faq.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport curl_cffi\n\ncurl_cffi.get(url, proxy=\"http://user:pass@example.com:3128\")\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Cookies using Pickle in curl_cffi\nDESCRIPTION: Demonstrates how to properly save cookies to disk and load them back using Python's pickle module. The example shows creating functions to save the cookie jar to a file and load it back into a new session, with a practical example of setting and retrieving a cookie value.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/cookies.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# example from: https://github.com/encode/httpx/issues/895\nimport pickle\n# import httpx\nimport curl_cffi\n\ndef save_cookies(client):\n    with open(\"cookies.pk\", \"wb\") as f:\n        pickle.dump(client.cookies.jar._cookies, f)\n\ndef load_cookies():\n    if not os.path.isfile(\"cookies.pk\"):\n        return None\n    with open(\"cookies.pk\", \"rb\") as f:\n        return pickle.load(f)\n\n# client = httpx.Client(cookies=load_cookies())\nclient = curl_cffi.Session()\nclient.get(\"https://httpbin.org/cookies/set/foo/bar\")\nsave_cookies(client)\n\nclient = curl_cffi.Session()\nclient.cookies.jar._cookies.update(load_cookies())\nprint(client.cookies.get(\"foo\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing File Download with Progress Tracking Using curl_cffi in Python\nDESCRIPTION: This function uses curl_cffi to download files with progress tracking. It accepts URL, output path, headers, and other optional parameters. The function handles the download process while providing progress updates and returns the file's path on completion.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/websockets.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef download_file(url, output_path=None, headers=None, chunk_size=1024 * 1024, verbose=True, retry=0):\n    \"\"\"\n    Download a file from a URL to the specified output path with progress tracking.\n    \n    Args:\n        url: The URL of the file to download\n        output_path: The path where the file will be saved\n        headers: Any HTTP headers to include with the request\n        chunk_size: Size of chunks to download (default 1MB)\n        verbose: Whether to print progress\n        retry: Number of retry attempts if download fails\n        \n    Returns:\n        The path to the downloaded file\n    \"\"\"\n    import os\n    import urllib.parse\n    from curl_cffi import requests\n    from tqdm.auto import tqdm\n    \n    # Default headers if none provided\n    if headers is None:\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n        }\n    \n    # If no output path provided, use the filename from the URL in the current directory\n    if output_path is None:\n        # Extract filename from URL\n        parsed_url = urllib.parse.urlparse(url)\n        output_path = os.path.basename(parsed_url.path)\n        \n        # If we couldn't get a filename, use a default name\n        if not output_path:\n            output_path = \"downloaded_file\"\n    \n    # Create the directory if it doesn't exist\n    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n    \n    # Initialize the download\n    for attempt in range(retry + 1):\n        try:\n            response = requests.get(url, headers=headers, stream=True)\n            response.raise_for_status()  # Raise an exception for HTTP errors\n            \n            # Get the total file size if available\n            total_size = int(response.headers.get(\"Content-Length\", 0))\n            \n            # Download with progress bar\n            with open(output_path, \"wb\") as f:\n                with tqdm(\n                    total=total_size,\n                    unit=\"B\",\n                    unit_scale=True,\n                    unit_divisor=1024,\n                    desc=f\"Downloading {os.path.basename(output_path)}\",\n                    disable=not verbose,\n                ) as pbar:\n                    # Use the iter_content method to get response data chunks\n                    for chunk in response.iter_content(chunk_size=chunk_size):\n                        if chunk:  # filter out keep-alive new chunks\n                            f.write(chunk)\n                            pbar.update(len(chunk))\n            return output_path\n            \n        except Exception as e:\n            if attempt < retry:\n                if verbose:\n                    print(f\"Download failed. Retrying {attempt+1}/{retry}\")\n            else:\n                raise e\n    \n    return output_path\n```\n\n----------------------------------------\n\nTITLE: Installing curl_cffi with pip\nDESCRIPTION: Command to install or upgrade the curl_cffi package using pip. This is the recommended installation method for the library.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install curl_cffi --upgrade\n```\n\n----------------------------------------\n\nTITLE: Using curl_cffi with eventlet/gevent in Python\nDESCRIPTION: This code snippet shows how to use curl_cffi with eventlet or gevent by setting the thread option in the Session constructor. This allows for non-blocking I/O operations.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/advanced.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom curl_cffi import requests\n\ns = requests.Session(thread=\"eventlet\")\ns.get(url)\n```\n\n----------------------------------------\n\nTITLE: Using curl_cffi as a httpx transport in Python\nDESCRIPTION: This snippet shows how to use curl_cffi as a httpx transport using the httpx-curl-cffi library. It provides full functionality of httpx while utilizing curl_cffi's features, including browser impersonation.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/advanced.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom httpx import Client, AsyncClient\nfrom httpx_curl_cffi import CurlTransport, AsyncCurlTransport, CurlOpt\n\nclient = Client(transport=CurlTransport(impersonate=\"chrome\", default_headers=True))\nclient.get(\"https://tools.scrapfly.io/api/fp/ja3\")\n\nasync_client = AsyncClient(transport=AsyncCurlTransport(\n    impersonate=\"chrome\",\n    default_headers=True,\n    # required for parallel requests, see curl_cffi issues below\n    curl_options={CurlOpt.FRESH_CONNECT: True}\n))\n```\n\n----------------------------------------\n\nTITLE: Installing curl_cffi via pip\nDESCRIPTION: Command to install the curl_cffi package from PyPI using pip. This should work on Linux, macOS, and Windows platforms out of the box.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/install.rst#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install curl_cffi --upgrade\n```\n\n----------------------------------------\n\nTITLE: Configuring curl_cffi with Certificate Verification Disabled\nDESCRIPTION: Demonstrates how to turn off certificate verification to avoid certificate errors when using curl_cffi. This is useful when dealing with self-signed certificates or debugging with tools like Fiddler or Charles.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/faq.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nr = curl_cffi.get(\"https://example.com\", verify=False)\n```\n\n----------------------------------------\n\nTITLE: Forcing HTTP/1.1 with curl_cffi\nDESCRIPTION: Shows how to force curl_cffi to use HTTP/1.1 instead of HTTP/2. This can be useful to avoid HTTP/2 stream errors or when working with websites that have broken HTTP/2 implementations.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/faq.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport curl_cffi\n\nr = curl_cffi.get(\"https://postman-echo.com\", http_version=curl_cffi.CurlHttpVersion.V1_1)\n```\n\n----------------------------------------\n\nTITLE: Custom OKHTTP Fingerprint Implementation with JA3 and Akamai Strings\nDESCRIPTION: Example showing how to use custom fingerprints to impersonate OKHTTP on Android 10. This implementation uses specific JA3 and Akamai strings along with extra TLS signature algorithms.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/impersonate.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# OKHTTP impersonatation examples\n# credits: https://github.com/bogdanfinn/tls-client/blob/master/profiles/contributed_custom_profiles.go\n\nurl = \"https://tls.browserleaks.com/json\"\n\nokhttp4_android10_ja3 = \",\".join(\n    [\n        \"771\",\n        \"4865-4866-4867-49195-49196-52393-49199-49200-52392-49171-49172-156-157-47-53\",\n        \"0-23-65281-10-11-35-16-5-13-51-45-43-21\",\n        \"29-23-24\",\n        \"0\",\n    ]\n)\n\nokhttp4_android10_akamai = \"4:16777216|16711681|0|m,p,a,s\"\n\nextra_fp = {\n    \"tls_signature_algorithms\": [\n        \"ecdsa_secp256r1_sha256\",\n        \"rsa_pss_rsae_sha256\",\n        \"rsa_pkcs1_sha256\",\n        \"ecdsa_secp384r1_sha384\",\n        \"rsa_pss_rsae_sha384\",\n        \"rsa_pkcs1_sha384\",\n        \"rsa_pss_rsae_sha512\",\n        \"rsa_pkcs1_sha512\",\n        \"rsa_pkcs1_sha1\",\n    ]\n    # other options:\n    # tls_min_version: int = CurlSslVersion.TLSv1_2\n    # tls_grease: bool = False\n    # tls_permute_extensions: bool = False\n    # tls_cert_compression: Literal[\"zlib\", \"brotli\"] = \"brotli\"\n    # tls_signature_algorithms: Optional[List[str]] = None\n    # http2_stream_weight: int = 256\n    # http2_stream_exclusive: int = 1\n\n    # See requests/impersonate.py and tests/unittest/test_impersonate.py for more examples\n}\n\n\nr = curl_cffi.get(\n    url, ja3=okhttp4_android10_ja3, akamai=okhttp4_android10_akamai, extra_fp=extra_fp\n)\nprint(r.json())\n```\n\n----------------------------------------\n\nTITLE: Randomizing Browser Version Selection\nDESCRIPTION: Example showing how to randomly select a browser version from available options for impersonation. This helps to vary the fingerprint between requests while maintaining valid browser fingerprints.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/impersonate.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nrandom.choice([\"chrome119\", \"chrome120\", ...])\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Download Function with curl_cffi in Python\nDESCRIPTION: This code snippet demonstrates how to use the download_file function to download a sample file from a public URL. It specifies the URL, output path, and sets verbose mode for progress tracking.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/websockets.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Example usage\nfile_url = \"https://example.com/sample_file.zip\"\noutput_path = \"downloads/sample_file.zip\"\n\n# Download the file with progress tracking\ndownloaded_file = download_file(file_url, output_path, verbose=True)\n```\n\n----------------------------------------\n\nTITLE: Custom CurlOpt Configuration for Fingerprint Modification\nDESCRIPTION: Example demonstrating how to directly modify curl options using CurlOpt to customize HTTP/2 pseudo header order. This approach provides low-level control over the request configuration.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/impersonate.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport curl_cffi\nfrom curl_cffi import Curl, CurlOpt\n\nc = Curl()\nc.setopt(CurlOpt.HTTP2_PSEUDO_HEADERS_ORDER, \"masp\")\n\n# or\ncurl_cffi.get(url, curl_options={CurlOpt.HTTP2_PSEUDO_HEADERS_ORDER, \"masp\"})\n```\n\n----------------------------------------\n\nTITLE: Handling Encoding Detection with chardet\nDESCRIPTION: Code example showing how to use the chardet library to detect the encoding of response content from curl_cffi. This is useful when dealing with pages that have inconsistent or incorrect encoding specifications.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/faq.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> import curl_cffi\n>>> r = curl_cffi.get(\"https://example.com/messy_codec.html\")\n>>> import chardet\n>>> chardet.detect(r.content)\n{'encoding': 'GB2312', 'confidence': 0.99, 'language': 'Chinese'}\n```\n\n----------------------------------------\n\nTITLE: Installing curl_cffi from GitHub source\nDESCRIPTION: Commands to clone the curl_cffi repository from GitHub, preprocess the code, and install it locally for the latest unstable version.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/install.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/lexiforest/curl_cffi/\ncd curl_cffi\nmake preprocess\npip install .\n```\n\n----------------------------------------\n\nTITLE: Installing beta releases of curl_cffi\nDESCRIPTION: Command to install pre-release (beta) versions of curl_cffi from PyPI using the --pre flag.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/install.rst#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install curl_cffi --upgrade --pre\n```\n\n----------------------------------------\n\nTITLE: Packaging curl_cffi with PyInstaller (Basic Configuration)\nDESCRIPTION: Command line for packaging a Python application using curl_cffi with PyInstaller. Uses the --hidden-import option to ensure all required dependencies are included.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/faq.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npyinstaller -F .\\example.py --hidden-import=_cffi_backend --collect-all curl_cffi\n```\n\n----------------------------------------\n\nTITLE: Packaging curl_cffi with PyInstaller (Advanced Configuration)\nDESCRIPTION: Advanced PyInstaller command that adds specific paths and libraries when packaging an application using curl_cffi. This configuration helps resolve dependency issues that may occur with simpler PyInstaller commands.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/faq.rst#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npyinstaller --noconfirm --onefile --console \\\n    --paths \"C:/Users/Administrator/AppData/Local/Programs/Python/Python39\" \\\n    --add-data \"C:/Users/Administrator/AppData/Local/Programs/Python/Python39/Lib/site-packages/curl_cffi.libs/libcurl-cbb416caa1dd01638554eab3f38d682d.dll;.\" \\\n    --collect-data \"curl_cffi\" \\\n    \"C:/Users/Administrator/Desktop/test_script.py\"\n```\n\n----------------------------------------\n\nTITLE: Installing Local Editable Build for curl-impersonate on macOS\nDESCRIPTION: This snippet shows the shell commands to set up the environment and install dependencies for a local editable build of curl-impersonate on macOS. It includes creating necessary directories, installing required libraries via Homebrew, and using pip to install the package with test and development dependencies.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/dev.rst#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# This is for using the libcurl-impersonate built by GitHub actions\n\nsudo mkdir /Users/runner\nsudo chmod 777 /Users/runner\n\n# Dependencies\nbrew install libidn2 zstd\n\n# Then install\npip install -e .[test]\npip install -e .[dev]\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Charset Detection Example\nDESCRIPTION: Example HTML meta tag showing charset specification that can be extracted using regex or lxml to determine the correct encoding of a webpage when automatic detection fails.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/docs/faq.rst#2025-04-19_snippet_7\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gbk\" />\n```\n\n----------------------------------------\n\nTITLE: HTTP Client Libraries List\nDESCRIPTION: Lists the HTTP client libraries being compared in the benchmark, divided into synchronous and asynchronous categories. All clients are tested with session/client functionality enabled.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/benchmark/README.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nSync clients\n------\n\n- curl_cffi\n- requests\n- pycurl\n- python-tls-client\n- httpx\n\nAsync clients\n------\n\n- curl_cffi\n- httpx\n- aiohttp\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements\nDESCRIPTION: A list of Python package dependencies including HTTP clients (requests, httpx, aiohttp), web frameworks (starlette), data manipulation (pandas), and server tools (uvicorn, gunicorn). Contains both synchronous and asynchronous libraries for HTTP requests and server operations.\nSOURCE: https://github.com/lexiforest/curl_cffi/blob/main/benchmark/requirements.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npandas\nstarlette\nuvicorn\nrequests\nhttpx\naiohttp\npycurl\ntls-client\ngunicorn\nuvloop\n```"
  }
]
[
  {
    "owner": "oobabooga",
    "repo": "text-generation-webui",
    "content": "TITLE: Python Chat Completion Streaming API Example\nDESCRIPTION: This Python script interacts with the /v1/chat/completions endpoint using Server-Sent Events (SSE) for streaming.  It uses the `requests` and `sseclient-py` libraries to establish a streaming connection and receive chunks of the assistant's message in real time. This example sets the 'stream' parameter to `True` in the request and prints the assistant's response as it is received.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport sseclient  # pip install sseclient-py\nimport json\n\nurl = \"http://127.0.0.1:5000/v1/chat/completions\"\n\nheaders = {\n    \"Content-Type\": \"application/json\"\n}\n\nhistory = []\n\nwhile True:\n    user_message = input(\"> \")\n    history.append({\"role\": \"user\", \"content\": user_message})\n    data = {\n        \"mode\": \"instruct\",\n        \"stream\": True,\n        \"messages\": history\n    }\n\n    stream_response = requests.post(url, headers=headers, json=data, verify=False, stream=True)\n    client = sseclient.SSEClient(stream_response)\n\n    assistant_message = ''\n    for event in client.events():\n        payload = json.loads(event.data)\n        chunk = payload['choices'][0]['delta']['content']\n        assistant_message += chunk\n        print(chunk, end='')\n\n    print()\n    history.append({\"role\": \"assistant\", \"content\": assistant_message})\n```\n\n----------------------------------------\n\nTITLE: Launching oobabooga/text-generation-webui in Colab with Python\nDESCRIPTION: This Python script sets up and launches the oobabooga/text-generation-webui within a Google Colab environment.  It clones the repository, installs dependencies, downloads a specified model from Hugging Face, and starts the web UI with customizable command-line flags. Parameters are used to configure model download and UI launch.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/Colab-TextGen-GPU.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom pathlib import Path\n\nos.environ.pop('PYTHONPATH', None)\nos.environ.pop('MPLBACKEND', None)\n\nif Path.cwd().name != 'text-generation-webui':\n  print(\"\\033[1;32;1m\\n --> Installing the web UI. This will take a while, but after the initial setup, you can download and test as many models as you like.\\033[0;37;0m\\n\")\n\n  !git clone https://github.com/oobabooga/text-generation-webui\n  %cd text-generation-webui\n\n  # Install the project in an isolated environment\n  !GPU_CHOICE=A \\\n  LAUNCH_AFTER_INSTALL=FALSE \\\n  INSTALL_EXTENSIONS=FALSE \\\n  ./start_linux.sh\n\n# Parameters\nmodel_url = \"https://huggingface.co/turboderp/gemma-2-9b-it-exl2\" #@param {type:\"string\"}\nbranch = \"8.0bpw\" #@param {type:\"string\"}\ncommand_line_flags = \"--n-gpu-layers 128 --load-in-4bit --use_double_quant --no_flash_attn\" #@param {type:\"string\"}\napi = False #@param {type:\"boolean\"}\n\nif api:\n  for param in ['--api', '--public-api']:\n    if param not in command_line_flags:\n      command_line_flags += f\" {param}\"\n\nmodel_url = model_url.strip()\nif model_url != \"\":\n    if not model_url.startswith('http'):\n        model_url = 'https://huggingface.co/' + model_url\n\n    # Download the model\n    url_parts = model_url.strip('/').strip().split('/')\n    output_folder = f\"{url_parts[-2]}_{url_parts[-1]}\"\n    branch = branch.strip('\"\\' ')  \n    if branch.strip() not in ['', 'main']:\n        output_folder += f\"_{branch}\"\n        !python download-model.py {model_url} --branch {branch}\n    else:\n        !python download-model.py {model_url}\nelse:\n    output_folder = \"\"\n\n# Start the web UI\ncmd = f\"./start_linux.sh {command_line_flags} --share\"\nif output_folder != \"\":\n    cmd += f\" --model {output_folder}\"\n\n!$cmd\n```\n\n----------------------------------------\n\nTITLE: Python Chat Completion API Example\nDESCRIPTION: This Python script demonstrates how to interact with the /v1/chat/completions endpoint to create a chat session. It uses the `requests` library to send POST requests with a chat history, and prints the assistant's responses to the console. This example implements a basic chat loop, appending user and assistant messages to a history and sending the history to the API on each turn.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nurl = \"http://127.0.0.1:5000/v1/chat/completions\"\n\nheaders = {\n    \"Content-Type\": \"application/json\"\n}\n\nhistory = []\n\nwhile True:\n    user_message = input(\"> \")\n    history.append({\"role\": \"user\", \"content\": user_message})\n    data = {\n        \"mode\": \"chat\",\n        \"character\": \"Example\",\n        \"messages\": history\n    }\n\n    response = requests.post(url, headers=headers, json=data, verify=False)\n    assistant_message = response.json()['choices'][0]['message']['content']\n    history.append({\"role\": \"assistant\", \"content\": assistant_message})\n    print(assistant_message)\n```\n\n----------------------------------------\n\nTITLE: Javascript ChatGPT-API Client Configuration\nDESCRIPTION: This JavaScript snippet configures the chatgpt-api Node.js client library. It uses environment variables `OPENAI_API_KEY` and `OPENAI_API_BASE` to set the `apiKey` and `apiBaseUrl` properties when creating a new instance of the `ChatGPTAPI` class. This enables the client to connect to a custom API endpoint.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst api = new ChatGPTAPI({\n  apiKey: process.env.OPENAI_API_KEY,\n  apiBaseUrl: process.env.OPENAI_API_BASE\n});\n```\n\n----------------------------------------\n\nTITLE: Command-Line Flags List\nDESCRIPTION: This snippet lists the command-line flags for the `server.py` script. It includes options for basic settings, model loading, Transformers/Accelerate, bitsandbytes 4-bit, llama.cpp, ExLlamaV2, HQQ, TensorRT-LLM, Cache, DeepSpeed, RoPE, and other settings. The flags configure the behavior of the text generation web UI.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_17\n\nLANGUAGE: txt\nCODE:\n```\nusage: server.py [-h] [--multi-user] [--character CHARACTER] [--model MODEL] [--lora LORA [LORA ...]] [--model-dir MODEL_DIR] [--lora-dir LORA_DIR] [--settings SETTINGS]\n                 [--extensions EXTENSIONS [EXTENSIONS ...]] [--verbose] [--idle-timeout IDLE_TIMEOUT] [--loader LOADER] [--cpu] [--auto-devices] [--gpu-memory GPU_MEMORY [GPU_MEMORY ...]]\n                 [--cpu-memory CPU_MEMORY] [--disk] [--disk-cache-dir DISK_CACHE_DIR] [--load-in-8bit] [--bf16] [--no-cache] [--trust-remote-code] [--force-safetensors] [--no_use_fast]\n                 [--use_flash_attention_2] [--use_eager_attention] [--torch-compile] [--load-in-4bit] [--use_double_quant] [--compute_dtype COMPUTE_DTYPE] [--quant_type QUANT_TYPE] [--flash-attn]\n                 [--n_ctx N_CTX] [--threads THREADS] [--threads-batch THREADS_BATCH] [--batch-size BATCH_SIZE] [--no-mmap] [--mlock] [--n-gpu-layers N_GPU_LAYERS] [--tensor-split TENSOR_SPLIT]\n                 [--numa] [--no-kv-offload] [--row-split] [--gpu-split GPU_SPLIT] [--autosplit] [--max_seq_len MAX_SEQ_LEN] [--cfg-cache] [--no_flash_attn] [--no_xformers] [--no_sdpa]\n                 [--num_experts_per_token NUM_EXPERTS_PER_TOKEN] [--enable_tp] [--hqq-backend HQQ_BACKEND] [--cpp-runner] [--cache_type CACHE_TYPE] [--deepspeed] [--nvme-offload-dir NVME_OFFLOAD_DIR]\n                 [--local_rank LOCAL_RANK] [--alpha_value ALPHA_VALUE] [--rope_freq_base ROPE_FREQ_BASE] [--compress_pos_emb COMPRESS_POS_EMB] [--listen] [--listen-port LISTEN_PORT]\n                 [--listen-host LISTEN_HOST] [--share] [--auto-launch] [--gradio-auth GRADIO_AUTH] [--gradio-auth-path GRADIO_AUTH_PATH] [--ssl-keyfile SSL_KEYFILE] [--ssl-certfile SSL_CERTFILE]\n                 [--subpath SUBPATH] [--old-colors] [--api] [--public-api] [--public-api-id PUBLIC_API_ID] [--api-port API_PORT] [--api-key API_KEY] [--admin-key ADMIN_KEY] [--api-enable-ipv6]\n                 [--api-disable-ipv4] [--nowebui]\n\nText generation web UI\n\noptions:\n  -h, --help                                     show this help message and exit\n\nBasic settings:\n  --multi-user                                   Multi-user mode. Chat histories are not saved or automatically loaded. Warning: this is likely not safe for sharing publicly.\n  --character CHARACTER                          The name of the character to load in chat mode by default.\n  --model MODEL                                  Name of the model to load by default.\n  --lora LORA [LORA ...]                         The list of LoRAs to load. If you want to load more than one LoRA, write the names separated by spaces.\n  --model-dir MODEL_DIR                          Path to directory with all the models.\n  --lora-dir LORA_DIR                            Path to directory with all the loras.\n  --settings SETTINGS                            Load the default interface settings from this yaml file. See settings-template.yaml for an example. If you create a file called settings.yaml, this\n                                                 file will be loaded by default without the need to use the --settings flag.\n  --extensions EXTENSIONS [EXTENSIONS ...]       The list of extensions to load. If you want to load more than one extension, write the names separated by spaces.\n  --verbose                                      Print the prompts to the terminal.\n  --idle-timeout IDLE_TIMEOUT                    Unload model after this many minutes of inactivity. It will be automatically reloaded when you try to use it again.\n\nModel loader:\n  --loader LOADER                                Choose the model loader manually, otherwise, it will get autodetected. Valid options: Transformers, llama.cpp, ExLlamav3_HF, ExLlamav2_HF, ExLlamav2,\n                                                 HQQ, TensorRT-LLM.\n\nTransformers/Accelerate:\n  --cpu                                          Use the CPU to generate text. Warning: Training on CPU is extremely slow.\n  --auto-devices                                 Automatically split the model across the available GPU(s) and CPU.\n  --gpu-memory GPU_MEMORY [GPU_MEMORY ...]       Maximum GPU memory in GiB to be allocated per GPU. Example: --gpu-memory 10 for a single GPU, --gpu-memory 10 5 for two GPUs. You can also set values\n                                                 in MiB like --gpu-memory 3500MiB.\n  --cpu-memory CPU_MEMORY                        Maximum CPU memory in GiB to allocate for offloaded weights. Same as above.\n  --disk                                         If the model is too large for your GPU(s) and CPU combined, send the remaining layers to the disk.\n  --disk-cache-dir DISK_CACHE_DIR                Directory to save the disk cache to. Defaults to \"cache\".\n  --load-in-8bit                                 Load the model with 8-bit precision (using bitsandbytes).\n  --bf16                                         Load the model with bfloat16 precision. Requires NVIDIA Ampere GPU.\n  --no-cache                                     Set use_cache to False while generating text. This reduces VRAM usage slightly, but it comes at a performance cost.\n  --trust-remote-code                            Set trust_remote_code=True while loading the model. Necessary for some models.\n  --force-safetensors                            Set use_safetensors=True while loading the model. This prevents arbitrary code execution.\n  --no_use_fast                                  Set use_fast=False while loading the tokenizer (it's True by default). Use this if you have any problems related to use_fast.\n  --use_flash_attention_2                        Set use_flash_attention_2=True while loading the model.\n  --use_eager_attention                          Set attn_implementation= eager while loading the model.\n  --torch-compile                                Compile the model with torch.compile for improved performance.\n\nbitsandbytes 4-bit:\n  --load-in-4bit                                 Load the model with 4-bit precision (using bitsandbytes).\n  --use_double_quant                             use_double_quant for 4-bit.\n  --compute_dtype COMPUTE_DTYPE                  compute dtype for 4-bit. Valid options: bfloat16, float16, float32.\n  --quant_type QUANT_TYPE                        quant_type for 4-bit. Valid options: nf4, fp4.\n\nllama.cpp:\n  --flash-attn                                   Use flash-attention.\n  --n_ctx N_CTX                                  Size of the prompt context.\n  --threads THREADS                              Number of threads to use.\n  --threads-batch THREADS_BATCH                  Number of threads to use for batches/prompt processing.\n  --batch-size BATCH_SIZE                        Maximum number of prompt tokens to batch together when calling llama_eval.\n  --no-mmap                                      Prevent mmap from being used.\n  --mlock                                        Force the system to keep the model in RAM.\n  --n-gpu-layers N_GPU_LAYERS                    Number of layers to offload to the GPU.\n  --tensor-split TENSOR_SPLIT                    Split the model across multiple GPUs. Comma-separated list of proportions. Example: 60,40.\n  --numa                                         Activate NUMA task allocation for llama.cpp.\n  --no-kv-offload                                Do not offload the K, Q, V to the GPU. This saves VRAM but reduces the performance.\n  --row-split                                    Split the model by rows across GPUs. This may improve multi-gpu performance.\n\nExLlamaV2:\n  --gpu-split GPU_SPLIT                          Comma-separated list of VRAM (in GB) to use per GPU device for model layers. Example: 20,7,7.\n  --autosplit                                    Autosplit the model tensors across the available GPUs. This causes --gpu-split to be ignored.\n  --max_seq_len MAX_SEQ_LEN                      Maximum sequence length.\n  --cfg-cache                                    ExLlamav2_HF: Create an additional cache for CFG negative prompts. Necessary to use CFG with that loader.\n  --no_flash_attn                                Force flash-attention to not be used.\n  --no_xformers                                  Force xformers to not be used.\n  --no_sdpa                                      Force Torch SDPA to not be used.\n  --num_experts_per_token NUM_EXPERTS_PER_TOKEN  Number of experts to use for generation. Applies to MoE models like Mixtral.\n  --enable_tp                                    Enable Tensor Parallelism (TP) in ExLlamaV2.\n\nHQQ:\n  --hqq-backend HQQ_BACKEND                      Backend for the HQQ loader. Valid options: PYTORCH, PYTORCH_COMPILE, ATEN.\n\nTensorRT-LLM:\n  --cpp-runner                                   Use the ModelRunnerCpp runner, which is faster than the default ModelRunner but doesn't support streaming yet.\n\nCache:\n  --cache_type CACHE_TYPE                        KV cache type; valid options: llama.cpp - fp16, q8_0, q4_0; ExLlamaV2 - fp16, fp8, q8, q6, q4.\n\nDeepSpeed:\n  --deepspeed                                    Enable the use of DeepSpeed ZeRO-3 for inference via the Transformers integration.\n  --nvme-offload-dir NVME_OFFLOAD_DIR            DeepSpeed: Directory to use for ZeRO-3 NVME offloading.\n  --local_rank LOCAL_RANK                        DeepSpeed: Optional argument for distributed setups.\n\nRoPE:\n  --alpha_value ALPHA_VALUE                      Positional embeddings alpha factor for NTK RoPE scaling. Use either this or compress_pos_emb, not both.\n  --rope_freq_base ROPE_FREQ_BASE                If greater than 0, will be used instead of alpha_value. Those two are related by rope_freq_base = 10000 * alpha_value ^ (64 / 63).\n```\n\n----------------------------------------\n\nTITLE: Javascript OpenAI Client Configuration\nDESCRIPTION: This JavaScript snippet configures the OpenAI client with an API key and base path. It uses environment variables `OPENAI_API_KEY` and `OPENAI_API_BASE` to set the `apiKey` and `basePath` properties of the configuration object, respectively. This allows the client to connect to the local API endpoint.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst openai = OpenAI(\n  Configuration({\n    apiKey: process.env.OPENAI_API_KEY,\n    basePath: process.env.OPENAI_API_BASE\n  })\n);\n```\n\n----------------------------------------\n\nTITLE: Chat Completion API Request with curl\nDESCRIPTION: This curl command demonstrates how to send a request to the /v1/chat/completions endpoint for generating chat responses.  It specifies the \"mode\" as \"instruct\" and includes a \"messages\" array containing a user message. The API expects a JSON payload with the messages, and it returns a chat completion based on the provided context.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://127.0.0.1:5000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n      }\n    ],\n    \"mode\": \"instruct\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Chat Completion Streaming API Request with curl\nDESCRIPTION: This curl command shows how to use SSE streaming with the /v1/chat/completions endpoint. It sets the \"stream\" parameter to true, enabling a continuous stream of chat completion data. The mode is set to \"instruct\", and it expects the API to return a stream of chat completion chunks.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://127.0.0.1:5000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n      }\n    ],\n    \"mode\": \"instruct\",\n    \"stream\": true\n  }'\n```\n\n----------------------------------------\n\nTITLE: Chat Completion with Character API Request with curl\nDESCRIPTION: This curl command sends a request to the /v1/chat/completions endpoint using a specified \"character\".  It includes a \"messages\" array with a user message, sets the mode to \"chat\", and designates the \"Example\" character. The API returns a chat completion taking into account the persona specified.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://127.0.0.1:5000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello! Who are you?\"\n      }\n    ],\n    \"mode\": \"chat\",\n    \"character\": \"Example\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Completion API Request with curl\nDESCRIPTION: This curl command sends a request to the /v1/completions endpoint to generate text based on a given prompt.  It sets the Content-Type header to application/json and provides a JSON payload with parameters like prompt, max_tokens, temperature, top_p, and seed.  The API endpoint is expected to return a completion based on the provided parameters.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://127.0.0.1:5000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"This is a cake recipe:\\n\\n1.\",\n    \"max_tokens\": 200,\n    \"temperature\": 1,\n    \"top_p\": 0.9,\n    \"seed\": 10\n  }'\n```\n\n----------------------------------------\n\nTITLE: Load Model API Request using curl\nDESCRIPTION: This curl command requests the loading of a specific model using the /v1/internal/model/load endpoint.  It provides the `model_name` and loading arguments such as `load_in_4bit` and `n_gpu_layers`. The API is expected to load the specified model with the provided configurations.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncurl -k http://127.0.0.1:5000/v1/internal/model/load \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"model_name\",\n    \"args\": {\n      \"load_in_4bit\": true,\n      \"n_gpu_layers\": 12\n    }\n  }'\n```\n\n----------------------------------------\n\nTITLE: Installing Docker on Ubuntu\nDESCRIPTION: These commands install Docker on Ubuntu. It updates the package list, installs necessary packages like curl, adds the Docker GPG key, adds the Docker repository to the apt sources, and then installs Docker CE, Docker CLI, containerd.io, docker-buildx-plugin, and docker-compose-plugin. Finally, it adds the current user to the docker group.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/09 - Docker.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\nsudo apt-get install curl\nsudo mkdir -m 0755 -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  \"$(. /etc/os-release && echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\nsudo apt update\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-compose -y\nsudo usermod -aG docker $USER\nnewgrp docker\n```\n\n----------------------------------------\n\nTITLE: Python Completion Streaming API Example\nDESCRIPTION: This Python script streams completions from the /v1/completions endpoint using SSE. It uses the `requests` and `sseclient-py` libraries, sets the `stream` parameter to `True`, and prints the model's generated text as it is received. This example is designed to display the completion output in real-time.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport requests\nimport sseclient  # pip install sseclient-py\n\nurl = \"http://127.0.0.1:5000/v1/completions\"\n\nheaders = {\n    \"Content-Type\": \"application/json\"\n}\n\ndata = {\n    \"prompt\": \"This is a cake recipe:\\n\\n1.\",\n    \"max_tokens\": 200,\n    \"temperature\": 1,\n    \"top_p\": 0.9,\n    \"seed\": 10,\n    \"stream\": True,\n}\n\nstream_response = requests.post(url, headers=headers, json=data, verify=False, stream=True)\nclient = sseclient.SSEClient(stream_response)\n\nprint(data['prompt'], end='')\nfor event in client.events():\n    payload = json.loads(event.data)\n    print(payload['choices'][0]['text'], end='')\n\nprint()\n```\n\n----------------------------------------\n\nTITLE: Installing Web UI Requirements (Shell)\nDESCRIPTION: This command installs the necessary Python packages for the web UI using pip.  The specific requirements file depends on the GPU/CPU and AVX2 support.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npip install -r <requirements file according to table below>\n```\n\n----------------------------------------\n\nTITLE: Downloading Models via Command Line (Python)\nDESCRIPTION: This Python script downloads a language model from Hugging Face using the `download-model.py` script.  It requires the model's organization and model name as an argument.  Additional options can be viewed by running the script with the `--help` flag.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\npython download-model.py organization/model\n```\n\n----------------------------------------\n\nTITLE: Starting the Web UI (Shell)\nDESCRIPTION: These commands activate the Conda environment, navigate to the web UI directory, and start the server using `python server.py`.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nconda activate textgen\ncd text-generation-webui\npython server.py\n```\n\n----------------------------------------\n\nTITLE: Python: Adding API Key to Headers\nDESCRIPTION: This Python snippet demonstrates how to include an API key in the request headers for authentication. The `Authorization` header is set to `Bearer yourPassword123`, where `yourPassword123` should be replaced with the actual API key. This header is then added to the request when interacting with the API.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer yourPassword123\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch with CUDA (Shell)\nDESCRIPTION: This command installs PyTorch with CUDA 12.4 support using pip3. It specifies the PyTorch version and index URL for pre-built wheels. This particular example targets CUDA 12.4.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npip3 install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n```\n\n----------------------------------------\n\nTITLE: Enable ngrok Extension\nDESCRIPTION: Enables the ngrok extension when running the `server.py` script. The `--extension ngrok` flag activates the ngrok functionality within the text-generation-webui.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/extensions/ngrok/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython server.py --extension ngrok\n```\n\n----------------------------------------\n\nTITLE: Dependency Versions\nDESCRIPTION: Specifies the versions of various Python libraries used in the project. This ensures consistency and avoids potential compatibility issues.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/extensions/superbooga/requirements.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nbeautifulsoup4==4.12.2\nchromadb==0.4.24\npandas==2.0.3\nposthog==2.4.2\nsentence_transformers==2.2.2\nlxml\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Host\nDESCRIPTION: This shows how to set the `OPENAI_API_HOST` environment variable to point to the local text-generation-webui instance. This configuration is needed for applications like shell_gpt to use the web UI's API.  The URL points to the base address of the service.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_HOST=http://127.0.0.1:5001\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies\nDESCRIPTION: This snippet specifies the required Python packages for the text-generation-webui project, along with their version constraints. These packages are essential for running the web UI and interacting with language models.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/full/requirements.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\naccelerate==1.5.*\nbitsandbytes==0.45.*\ncolorama\ndatasets\neinops\nfastapi==0.112.4\ngradio==4.37.*\njinja2==3.1.6\nmarkdown\nnumpy==1.26.*\npandas\npeft==0.15.*\nPillow>=9.5.0\npsutil\npydantic==2.8.2\npyyaml\nrequests\nrich\nsafetensors==0.5.*\nscipy\nsentencepiece\ntensorboard\ntransformers==4.50.*\ntqdm\nwandb\n\n# API\nflask_cloudflared==0.0.14\nsse-starlette==1.6.5\ntiktoken\n```\n\n----------------------------------------\n\nTITLE: Building and Running Docker Compose (Shell)\nDESCRIPTION: This command builds the Docker image and starts the services defined in the docker-compose.yml file.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\ndocker compose up --build\n```\n\n----------------------------------------\n\nTITLE: Dependency List for Text Generation WebUI\nDESCRIPTION: This section lists the core dependencies required for the Text Generation WebUI project. It includes packages such as fastapi, gradio, jinja2, markdown, numpy, pydantic, pyyaml, requests, rich, and tqdm. Specific versions or version ranges are specified for some dependencies.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/portable/requirements_amd.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfastapi==0.112.4\ngradio==4.37.*\njinja2==3.1.6\nmarkdown\nnumpy==1.26.*\npydantic==2.8.2\npyyaml\nrequests\nrich\ntqdm\n```\n\n----------------------------------------\n\nTITLE: OAUTH Authentication Configuration\nDESCRIPTION: Configures OAUTH authentication using Google as the provider, along with allowed domains and emails. This configuration is added to the `settings.json` file under the `ngrok` key, enabling more secure access control to the web UI.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/extensions/ngrok/README.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"ngrok\": {\n        \"oauth_provider\": \"google\",\n        \"oauth_allow_domains\": \"asdf.com\",\n        \"oauth_allow_emails\": \"asdf@asdf.com\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Alpaca Format File Example\nDESCRIPTION: This snippet provides a simple format file example for Alpaca datasets to be used as a chat bot.  It defines how the `instruction`, `input`, and `output` keys from the Alpaca dataset should be formatted into a prompt for the language model during training.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/05 - Training Tab.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"instruction,output\": \"User: %instruction%\\nAssistant: %output%\",\n    \"instruction,input,output\": \"User: %instruction%: %input%\\nAssistant: %output%\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running with multiple extensions\nDESCRIPTION: This snippet illustrates how to run the server with multiple extensions, specifying the order in which input, output, and bot prefix modifiers are applied. Extensions are activated using the `--extensions` flag, with extension names separated by spaces. The order is important for modifier execution.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\npython server.py --extensions enthusiasm translate # First apply enthusiasm, then translate\npython server.py --extensions translate enthusiasm # First apply translate, then enthusiasm\n```\n\n----------------------------------------\n\nTITLE: Llama-2-chat Prompt Format Example\nDESCRIPTION: This snippet illustrates the prompt format required by the Llama-2-chat models. It incorporates special tokens such as `</s>` (end of sequence) and `<s>` (beginning of sequence) at the end of each bot reply. The context string is enclosed between `<<SYS>>` and `<</SYS>>`.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/01 - Chat Tab.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n[INST] <<SYS>>\nAnswer the questions.\n<</SYS>>\nHi there! [/INST] Hello! It's nice to meet you. What can I help with? </s><s>[INST] How are you? [/INST] I'm doing well, thank you for asking! Is there something specific you would like to talk about or ask me? I'm here to help answer any questions you may have.\n\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies\nDESCRIPTION: This code snippet lists the Python dependencies required for the project.  It includes SpeechRecognition, openai-whisper, soundfile, and ffmpeg. These libraries are essential for speech processing and audio manipulation functionalities.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/extensions/whisper_stt/requirements.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nSpeechRecognition==3.10.0\nopenai-whisper\nsoundfile\nffmpeg\n```\n\n----------------------------------------\n\nTITLE: Extension Parameter Definition\nDESCRIPTION: This snippet demonstrates how to define extension parameters using a `params` dictionary in the `script.py` file of an extension. The `display_name` key specifies the name shown in the UI, and the `is_tab` key determines whether the extension appears in a new tab.  This configures the extensions interface in the web UI.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nparams = {\n    \"display_name\": \"Google Translate\",\n    \"is_tab\": True,\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Compose on Ubuntu\nDESCRIPTION: This command starts the Docker Compose setup, building the necessary images. Assumes that the `docker-compose.yml` file is properly configured.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/09 - Docker.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up --build\n```\n\n----------------------------------------\n\nTITLE: Installing Docker Compose Plugin Manually (Older Ubuntu)\nDESCRIPTION: This set of commands manually installs the Docker Compose plugin on older Ubuntu systems by downloading the binary, making it executable, and adding it to the PATH.  The DOCKER_CONFIG environment variable is checked.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/09 - Docker.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nDOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}\nmkdir -p $DOCKER_CONFIG/cli-plugins\ncurl -SL https://github.com/docker/compose/releases/download/v2.17.2/docker-compose-linux-x86_64 -o $DOCKER_CONFIG/cli-plugins/docker-compose\nchmod +x $DOCKER_CONFIG/cli-plugins/docker-compose\nexport PATH=\"$HOME/.docker/cli-plugins:$PATH\"\n```\n\n----------------------------------------\n\nTITLE: Setting Embedding Model Environment Variable\nDESCRIPTION: This snippet explains how to change the default embedding model by setting the `OPENEDAI_EMBEDDING_MODEL` environment variable.  This allows the user to switch to a faster, smaller model like all-MiniLM-L6-v2. The environment variable can be set before starting the application.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nOPENEDAI_EMBEDDING_MODEL=all-MiniLM-L6-v2\n```\n\n----------------------------------------\n\nTITLE: Installing Docker and Dependencies on Manjaro\nDESCRIPTION: This command installs Docker, Docker Compose, BuildKit, GCC, and nvidia-docker on Manjaro using yay.  It then adds the current user to the docker group and restarts the Docker service.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/09 - Docker.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nyay -S docker docker-compose buildkit gcc nvidia-docker\nsudo usermod -aG docker $USER\nnewgrp docker\nsudo systemctl restart docker # required by nvidia-container-runtime\n```\n\n----------------------------------------\n\nTITLE: Setting the Default WSL Distribution\nDESCRIPTION: Sets the specified WSL distribution as the default. The distro name should be taken from the output of `wsl -l` command. Required for making sure commands run under the correct distro.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/10 - WSL.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nwsl -s <DistroName>\n```\n\n----------------------------------------\n\nTITLE: Updating Requirements\nDESCRIPTION: This snippet shows how to update the project requirements using `conda` to activate the environment, navigate to the project directory, and then use `pip` to install or upgrade the packages listed in the requirements file.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nconda activate textgen\ncd text-generation-webui\npip install -r <requirements file that you have used> --upgrade\n```\n\n----------------------------------------\n\nTITLE: Setting ROCM_PATH in one_click.py\nDESCRIPTION: Modifies the os.environ[\"ROCM_PATH\"] variable in the one_click.py script to point to the ROCm installation directory. This ensures that the application can find the ROCm libraries and executables. Requires editing the file manually.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/11 - AMD Setup.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"ROCM_PATH\"] = '/opt/rocm'\n```\n\n----------------------------------------\n\nTITLE: Launching Docker Image with Docker Compose\nDESCRIPTION: These commands launch the Docker image for text-generation-webui. They navigate to the project directory, create necessary symbolic links, copy the .env example file, and then start the Docker Compose setup.  The user is expected to edit the .env file and set the `TORCH_CUDA_ARCH_LIST` environment variable appropriately.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/09 - Docker.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd text-generation-webui\nln -s docker/{nvidia/Dockerfile,nvidia/docker-compose.yml,.dockerignore} .\ncp docker/.env.example .env\n# Edit .env and set TORCH_CUDA_ARCH_LIST based on your GPU model\ndocker compose up --build\n```\n\n----------------------------------------\n\nTITLE: Setting the WSL Default Version (Windows 10)\nDESCRIPTION: Sets the default version of WSL to version 1. This is required for older versions of Windows 10 that don't support WSL2 directly.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/10 - WSL.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nwsl --set-default-version 1\n```\n\n----------------------------------------\n\nTITLE: Docker Setup for AMD GPU (Shell)\nDESCRIPTION: This shell command creates symbolic links for the Dockerfile and docker-compose file specific to AMD GPUs, streamlining the Docker setup process.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nln -s docker/{amd/Dockerfile,intel/docker-compose.yml,.dockerignore} .\n```\n\n----------------------------------------\n\nTITLE: Docker Setup for Intel GPU (Shell)\nDESCRIPTION: This shell command creates symbolic links for the Dockerfile and docker-compose file specific to Intel GPUs, simplifying the Docker setup process.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nln -s docker/{intel/Dockerfile,amd/docker-compose.yml,.dockerignore} .\n```\n\n----------------------------------------\n\nTITLE: Chat-Instruct Prompt Example (Alpaca)\nDESCRIPTION: This snippet demonstrates the chat-instruct mode using the Alpaca format. It combines an instruction (command) with a regular chat prompt to generate a single reply for the specified character. The command instructs the model to continue the chat dialogue.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/01 - Chat Tab.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nContinue the chat dialogue below. Write a single reply for the character \"Chiharu Yamada\".\nChiharu Yamada's Persona: Chiharu Yamada is a young, computer engineer-nerd with a knack for problem solving and a passion for technology.\nYou: Hi there!\nChiharu Yamada: Hello! It's nice to meet you. What can I help with?\nYou: How are you?\n\n### Response:\nChiharu Yamada:\n\n```\n\n----------------------------------------\n\nTITLE: Starting Web UI with DeepSpeed\nDESCRIPTION: This command initiates the web UI using DeepSpeed with a specified number of GPUs. The `--deepspeed` flag enables DeepSpeed functionality and a specific model is loaded using `--model`. The `--chat` flag enables chat mode.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/08 - Additional Tips.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --num_gpus=1 server.py --deepspeed --chat --model gpt-j-6B\n```\n\n----------------------------------------\n\nTITLE: Creating a Conda Environment (Shell)\nDESCRIPTION: These commands create a new Conda environment named 'textgen' with Python 3.11 and then activates it.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nconda create -n textgen python=3.11\nconda activate textgen\n```\n\n----------------------------------------\n\nTITLE: Starting Web UI in CPU Mode\nDESCRIPTION: This command starts the web UI in CPU mode, allowing LoRA training to be performed without GPU acceleration. The `--cpu` flag forces the application to utilize the CPU for processing.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/08 - Additional Tips.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython server.py --cpu\n```\n\n----------------------------------------\n\nTITLE: Setup Function\nDESCRIPTION: This function is executed only once when the extension is imported. It's a place to perform initialization tasks.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef setup():\n    \"\"\"\n    Gets executed only once, when the extension is imported.\n    \"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Keeping Colab Alive with HTML Audio\nDESCRIPTION: This code snippet uses HTML to embed a silent audio track into the Colab notebook. This prevents Colab from disconnecting the session due to inactivity. The audio player is set up to loop silently in the background.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/Colab-TextGen-GPU.ipynb#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies\nDESCRIPTION: This snippet lists Python packages required by the text-generation-webui project, with specific version constraints (e.g., fastapi==0.112.4, gradio==4.37.*, numpy==1.26.*, pydantic==2.8.2). The * wildcard indicates compatible versions within a major release. These packages are essential for the functionality of the web UI.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/portable/requirements_apple_intel.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfastapi==0.112.4\ngradio==4.37.*\njinja2==3.1.6\nmarkdown\nnumpy==1.26.*\npydantic==2.8.2\npyyaml\nrequests\nrich\ntqdm\n```\n\n----------------------------------------\n\nTITLE: Custom Generate Chat Prompt Function\nDESCRIPTION: This function replaces the default function that generates the prompt from the chat history.  It calls the original function to create the prompt. It's only used in chat mode.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef custom_generate_chat_prompt(user_input, state, **kwargs):\n    \"\"\"\n    Replaces the function that generates the prompt from the chat history.\n    Only used in chat mode.\n    \"\"\"\n    result = chat.generate_chat_prompt(user_input, state, **kwargs)\n    return result\n```\n\n----------------------------------------\n\nTITLE: Configuring API URL in Notepad++ OpenAI Plugin\nDESCRIPTION: This configuration example shows how to configure the Notepad++ OpenAI plugin by setting the `api_url` parameter in the config file to point to the local text-generation-webui instance.  This configuration allows the plugin to utilize the web UI's API.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_17\n\nLANGUAGE: ini\nCODE:\n```\napi_url=http://127.0.0.1:5001\n```\n\n----------------------------------------\n\nTITLE: LogitsProcessor Class Definition\nDESCRIPTION: This class defines a custom logits processor that can be used to manipulate the probabilities of the next token before sampling.  Currently, it doesn't perform any manipulation but serves as a template. It's designed to be used with loaders that utilize the transformers library for sampling. The __call__ method is where the actual probability manipulation would take place.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass MyLogits(LogitsProcessor):\n    \"\"\"\n    Manipulates the probabilities for the next token before it gets sampled.\n    Used in the logits_processor_modifier function below.\n    \"\"\"\n    def __init__(self):\n        pass\n\n    def __call__(self, input_ids, scores):\n        # probs = torch.softmax(scores, dim=-1, dtype=torch.float)\n        # probs[0] /= probs[0].sum()\n        # scores = torch.log(probs / (1 - probs))\n        return scores\n```\n\n----------------------------------------\n\nTITLE: Basic Authentication Configuration\nDESCRIPTION: Configures basic authentication for the ngrok ingress by specifying a username and password.  This setting is added to the `settings.json` file under the `ngrok` key.  The web UI will require these credentials before access is granted.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/extensions/ngrok/README.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"ngrok\": {\n        \"basic_auth\": \"user:password\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Tokenizer Modifier Function\nDESCRIPTION: This function modifies the input IDs and embeddings. It is used by extensions like the multimodal extension to insert image embeddings into the prompt. Only used by loaders that employ the transformers library for sampling. Currently, it returns the original prompt, input_ids, and input_embeds without modifications.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef tokenizer_modifier(state, prompt, input_ids, input_embeds):\n    \"\"\"\n    Modifies the input ids and embeds.\n    Used by the multimodal extension to put image embeddings in the prompt.\n    Only used by loaders that use the transformers library for sampling.\n    \"\"\"\n    return prompt, input_ids, input_embeds\n```\n\n----------------------------------------\n\nTITLE: Whisper STT Configuration in YAML\nDESCRIPTION: This code snippet demonstrates how to configure the whisper_stt extension by adding settings to the settings.yaml file.  It allows users to specify the Whisper language, model size, and whether to automatically submit the transcribed text. The language setting determines the language Whisper uses for transcription, and the model setting defines the size of the Whisper model used for speech recognition.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/extensions/whisper_stt/readme.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nwhisper_stt-whipser_language: chinese\nwhisper_stt-whipser_model: tiny\nwhisper_stt-auto_submit: False\n```\n\n----------------------------------------\n\nTITLE: Chat Prompt Example\nDESCRIPTION: This snippet shows a simple chat prompt format used when talking to a character. It includes a character's persona, user input, and the character's response.  The Context string describes the bot's personality and provides example messages.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/01 - Chat Tab.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nChiharu Yamada's Persona: Chiharu Yamada is a young, computer engineer-nerd with a knack for problem solving and a passion for technology.\nYou: Hi there!\nChiharu Yamada: Hello! It's nice to meet you. What can I help with?\nYou: How are you?\nChiharu Yamada: I'm doing well, thank you for asking! Is there something specific you would like to talk about or ask me? I'm here to help answer any questions you may have.\n\n```\n\n----------------------------------------\n\nTITLE: Mac Wheel Dependencies\nDESCRIPTION: This section defines platform-specific dependencies for macOS. It provides URLs to pre-built `llama_cpp_binaries` wheels, specifying compatible macOS versions and Python versions (3.11). The `platform_system`, `platform_release`, and `python_version` markers ensure that the correct wheel is installed on compatible systems. These wheels likely provide optimized C++ implementations for llama inference on macOS.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/portable/requirements_apple_intel.txt#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nhttps://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.3.0/llama_cpp_binaries-0.3.0-cp311-cp311-macosx_15_0_x86_64.whl; platform_system == \"Darwin\" and platform_release >= \"24.0.0\" and platform_release < \"25.0.0\" and python_version == \"3.11\"\nhttps://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.3.0/llama_cpp_binaries-0.3.0-cp311-cp311-macosx_14_0_x86_64.whl; platform_system == \"Darwin\" and platform_release >= \"23.0.0\" and platform_release < \"24.0.0\" and python_version == \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: State Modifier Function\nDESCRIPTION: This function modifies the state variable, which is a dictionary containing the input values from the UI. This is a placeholder and returns the original state without modifications. The state dictionary contains slider values, checkboxes, and other UI inputs.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef state_modifier(state):\n    \"\"\"\n    Modifies the state variable, which is a dictionary containing the input\n    values in the UI like sliders and checkboxes.\n    \"\"\"\n    return state\n```\n\n----------------------------------------\n\nTITLE: Downloading a Linux Distribution via Powershell\nDESCRIPTION: Downloads a specified Linux distribution as an appx package using Invoke-WebRequest and renames it to a zip file. This prepares the downloaded distro for manual installation.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/10 - WSL.md#_snippet_1\n\nLANGUAGE: PowerShell\nCODE:\n```\n$ProgressPreference = 'SilentlyContinue'\nInvoke-WebRequest -Uri <LinuxDistroURL> -OutFile Linux.appx -UseBasicParsing\nmv Linux.appx Linux.zip\n```\n\n----------------------------------------\n\nTITLE: Checking Docker Compose Version\nDESCRIPTION: This command checks the installed version of Docker Compose. It is recommended to have version 2.17 or higher installed.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/09 - Docker.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose version\n```\n\n----------------------------------------\n\nTITLE: Install Python Extension Requirements\nDESCRIPTION: This command installs the Python requirements for a specific extension. It uses pip to install the packages listed in the requirements.txt file located in the extension's directory.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/06 - Session Tab.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r extensions/extension-name/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Install Python Extension Requirements (Windows)\nDESCRIPTION: This command installs the Python requirements for a specific extension on Windows. It uses pip to install the packages listed in the requirements.txt file located in the extension's directory.  The backslash is used as a path separator.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/06 - Session Tab.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r extensions\\extension-name\\requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Compose on Windows\nDESCRIPTION: This command starts the Docker Compose setup. It assumes Docker Desktop is running and properly configured.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/09 - Docker.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up\n```\n\n----------------------------------------\n\nTITLE: Docker Setup for NVIDIA GPU (Shell)\nDESCRIPTION: This shell command creates symbolic links for the Dockerfile and docker-compose file specific to NVIDIA GPUs, simplifying the Docker setup process.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nln -s docker/{nvidia/Dockerfile,nvidia/docker-compose.yml,.dockerignore} .\n```\n\n----------------------------------------\n\nTITLE: Defining Platform-Specific CUDA Wheel Dependencies\nDESCRIPTION: This snippet defines the platform-specific CUDA wheel dependencies for the text-generation-webui project, conditional on the operating system, architecture, and Python version. These wheels provide optimized CUDA support for specific platforms.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/full/requirements_noavx2.txt#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nhttps://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.3.0/llama_cpp_binaries-0.3.0+cu124avx-cp311-cp311-win_amd64.whl; platform_system == \"Windows\" and python_version == \"3.11\"\nhttps://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.3.0/llama_cpp_binaries-0.3.0+cu124avx-cp311-cp311-linux_x86_64.whl; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"\nhttps://github.com/oobabooga/exllamav3/releases/download/v0.0.1a3/exllamav3-0.0.1a3+cu124.torch2.6.0-cp311-cp311-win_amd64.whl; platform_system == \"Windows\" and python_version == \"3.11\"\nhttps://github.com/oobabooga/exllamav3/releases/download/v0.0.1a3/exllamav3-0.0.1a3+cu124.torch2.6.0-cp311-cp311-linux_x86_64.whl; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"\nhttps://github.com/oobabooga/exllamav2/releases/download/v0.2.8/exllamav2-0.2.8+cu124.torch2.6.0-cp311-cp311-win_amd64.whl; platform_system == \"Windows\" and python_version == \"3.11\"\nhttps://github.com/oobabooga/exllamav2/releases/download/v0.2.8/exllamav2-0.2.8+cu124.torch2.6.0-cp311-cp311-linux_x86_64.whl; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"\nhttps://github.com/oobabooga/exllamav2/releases/download/v0.2.8/exllamav2-0.2.8-py3-none-any.whl; platform_system == \"Linux\" and platform_machine != \"x86_64\"\nhttps://github.com/oobabooga/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu124torch2.6.0cxx11abiFALSE-cp311-cp311-win_amd64.whl; platform_system == \"Windows\" and python_version == \"3.11\"\nhttps://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: Copying Docker Environment File (Shell)\nDESCRIPTION: This shell command copies the example environment file `.env.example` from the `docker` directory to the project root directory as `.env`.  This file is used to configure the Docker environment.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\ncp docker/.env.example .env\n```\n\n----------------------------------------\n\nTITLE: Creating logs/cache directory (Shell)\nDESCRIPTION: Creates the `logs` and `cache` directories to store log files and cached data used by the application.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nmkdir -p logs cache\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies\nDESCRIPTION: This snippet lists the core Python packages required by the text-generation-webui project, specifying version constraints for compatibility and functionality. These dependencies are essential for the base functionality of the web UI.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/full/requirements_noavx2.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\naccelerate==1.5.*\nbitsandbytes==0.45.*\ncolorama\ndatasets\neinops\nfastapi==0.112.4\ngradio==4.37.*\njinja2==3.1.6\nmarkdown\nnumpy==1.26.*\npandas\npeft==0.15.*\nPillow>=9.5.0\npsutil\npydantic==2.8.2\npyyaml\nrequests\nrich\nsafetensors==0.5.*\nscipy\nsentencepiece\ntensorboard\ntransformers==4.50.*\ntqdm\nwandb\n```\n\n----------------------------------------\n\nTITLE: Listing WSL Distributions\nDESCRIPTION: Lists the installed WSL distributions. Useful for identifying the available distributions and their names.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/10 - WSL.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nwsl -l\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA Container Toolkit on Ubuntu\nDESCRIPTION: These commands install the NVIDIA Container Toolkit on Ubuntu. It adds the NVIDIA GPG key, adds the NVIDIA repository to the apt sources, updates the package list, installs nvidia-docker2 and nvidia-container-runtime, and then restarts the Docker service.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/09 - Docker.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://nvidia.github.io/libnvidia-container/stable/ubuntu22.04/amd64 /\" | \\\nsudo tee /etc/apt/sources.list.d/nvidia.list > /dev/null \nsudo apt update\nsudo apt install nvidia-docker2 nvidia-container-runtime -y\nsudo systemctl restart docker\n```\n\n----------------------------------------\n\nTITLE: Conditional CUDA Wheel URLs\nDESCRIPTION: This section provides conditional URLs for pre-built CUDA-enabled wheels. The correct wheel is selected based on the operating system, CPU architecture, and Python version. This is crucial for leveraging GPU acceleration for faster text generation.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/full/requirements.txt#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhttps://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.3.0/llama_cpp_binaries-0.3.0+cu124-cp311-cp311-win_amd64.whl; platform_system == \"Windows\" and python_version == \"3.11\"\nhttps://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.3.0/llama_cpp_binaries-0.3.0+cu124-cp311-cp311-linux_x86_64.whl; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"\nhttps://github.com/oobabooga/exllamav3/releases/download/v0.0.1a3/exllamav3-0.0.1a3+cu124.torch2.6.0-cp311-cp311-win_amd64.whl; platform_system == \"Windows\" and python_version == \"3.11\"\nhttps://github.com/oobabooga/exllamav3/releases/download/v0.0.1a3/exllamav3-0.0.1a3+cu124.torch2.6.0-cp311-cp311-linux_x86_64.whl; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"\nhttps://github.com/oobabooga/exllamav2/releases/download/v0.2.8/exllamav2-0.2.8+cu124.torch2.6.0-cp311-cp311-win_amd64.whl; platform_system == \"Windows\" and python_version == \"3.11\"\nhttps://github.com/oobabooga/exllamav2/releases/download/v0.2.8/exllamav2-0.2.8+cu124.torch2.6.0-cp311-cp311-linux_x86_64.whl; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"\nhttps://github.com/oobabooga/exllamav2/releases/download/v0.2.8/exllamav2-0.2.8-py3-none-any.whl; platform_system == \"Linux\" and platform_machine != \"x86_64\"\nhttps://github.com/oobabooga/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu124torch2.6.0cxx11abiFALSE-cp311-cp311-win_amd64.whl; platform_system == \"Windows\" and python_version == \"3.11\"\nhttps://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: List of Python Package Dependencies\nDESCRIPTION: This snippet lists the Python package dependencies for the text-generation-webui project, along with version specifications or minimum version requirements. These packages are required to run the application and are specified in a requirements file.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/full/requirements_nowheels.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\naccelerate==1.5.*\ncolorama\ndatasets\neinops\nfastapi==0.112.4\ngradio==4.37.*\njinja2==3.1.6\nmarkdown\nnumpy==1.26.*\npandas\npeft==0.15.*\nPillow>=9.5.0\npsutil\npydantic==2.8.2\npyyaml\nrequests\nrich\nsafetensors==0.5.*\nscipy\nsentencepiece\ntensorboard\ntransformers==4.50.*\ntqdm\nwandb\n\n# API\nflask_cloudflared==0.0.14\nsse-starlette==1.6.5\ntiktoken\n```\n\n----------------------------------------\n\nTITLE: Installing build-essential package\nDESCRIPTION: Installs the `build-essential` package which contains a suite of tools required for compiling software in Ubuntu. Essential for development tasks.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/10 - WSL.md#_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\nsudo apt install build-essential\n```\n\n----------------------------------------\n\nTITLE: Install ngrok Requirements\nDESCRIPTION: Installs the necessary Python packages specified in the `extensions/ngrok/requirements.txt` file using pip. This is a prerequisite for using the ngrok extension.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/extensions/ngrok/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r extensions/ngrok/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Authtoken Configuration\nDESCRIPTION: Configures the ngrok authtoken directly in the `settings.json` file, instead of relying on the NGROK_AUTHTOKEN environment variable. Setting `authtoken_from_env` to `false` ensures that only the authtoken specified in the configuration is used.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/extensions/ngrok/README.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"ngrok\": {\n        \"authtoken\": \"<token>\",\n        \"authtoken_from_env\":false\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Navigating to Web UI Directory (Shell)\nDESCRIPTION: This command changes the current directory to the cloned text-generation-webui folder.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncd text-generation-webui\n```\n\n----------------------------------------\n\nTITLE: Input Modifier Function\nDESCRIPTION: This function modifies the entire prompt in default/notebook modes or just the 'text' part in chat mode. In chat mode, it functions the same as `chat_input_modifier` but only affects the 'text' part of the input, not 'visible_text'. Currently a placeholder, it returns the original string.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef input_modifier(string, state, is_chat=False):\n    \"\"\"\n    In default/notebook modes, modifies the whole prompt.\n\n    In chat mode, it is the same as chat_input_modifier but only applied\n    to \"text\", here called \"string\", and not to \"visible_text\".\n    \"\"\"\n    return string\n```\n\n----------------------------------------\n\nTITLE: List Models API Request using curl\nDESCRIPTION: This curl command requests a list of available models from the /v1/internal/model/list endpoint. The API should return a list of models in JSON format.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncurl -k http://127.0.0.1:5000/v1/internal/model/list \\\n  -H \"Content-Type: application/json\"\n```\n\n----------------------------------------\n\nTITLE: API Dependencies for Text Generation WebUI\nDESCRIPTION: This section lists the dependencies required for the API functionality of the Text Generation WebUI. It includes flask_cloudflared, sse-starlette, and tiktoken. These packages enable API endpoints, server-sent events, and tokenization functionalities.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/portable/requirements_amd.txt#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nflask_cloudflared==0.0.14\nsse-starlette==1.6.5\ntiktoken\n```\n\n----------------------------------------\n\nTITLE: JSON Dataset Example\nDESCRIPTION: This snippet shows an example of a JSON formatted dataset suitable for LoRA training.  The dataset consists of an array of JSON objects, each representing a training example with key-value pairs. The keys are standardized, and the values contain the content intended for training.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/05 - Training Tab.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"somekey\": \"somevalue\",\n        \"key2\": \"value2\"\n    },\n    {\n        // etc\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Listing Dependencies\nDESCRIPTION: Lists required python package dependencies with specific version constraints. Includes general dependencies as well as platform specific wheel files for macOS.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/full/requirements_apple_silicon.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\naccelerate==1.5.*\\ncolorama\\ndatasets\\neinops\\nfastapi==0.112.4\\ngradio==4.37.*\\njinja2==3.1.6\\nmarkdown\\nnumpy==1.26.*\\npandas\\npeft==0.15.*\\nPillow>=9.5.0\\npsutil\\npydantic==2.8.2\\npyyaml\\nrequests\\nrich\\nsafetensors==0.5.*\\nscipy\\nsentencepiece\\ntensorboard\\ntransformers==4.50.*\\ntqdm\\nwandb\\n\\n# API\\nflask_cloudflared==0.0.14\\nsse-starlette==1.6.5\\ntiktoken\\n\\n# Mac wheels\\nhttps://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.3.0/llama_cpp_binaries-0.3.0-cp311-cp311-macosx_15_0_arm64.whl; platform_system == \"Darwin\" and platform_release >= \"24.0.0\" and platform_release < \"25.0.0\" and python_version == \"3.11\"\\nhttps://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.3.0/llama_cpp_binaries-0.3.0-cp311-cp311-macosx_14_0_arm64.whl; platform_system == \"Darwin\" and platform_release >= \"23.0.0\" and platform_release < \"24.0.0\" and python_version == \"3.11\"\\nhttps://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.3.0/llama_cpp_binaries-0.3.0-cp311-cp311-macosx_13_0_arm64.whl; platform_system == \"Darwin\" and platform_release >= \"22.0.0\" and platform_release < \"23.0.0\" and python_version == \"3.11\"\\nhttps://github.com/oobabooga/exllamav3/releases/download/v0.0.1a3/exllamav3-0.0.1a3-py3-none-any.whl\\nhttps://github.com/oobabooga/exllamav2/releases/download/v0.2.8/exllamav2-0.2.8-py3-none-any.whl\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSpeed\nDESCRIPTION: These commands install the necessary dependencies for DeepSpeed, including mpi4py, mpich, and DeepSpeed itself. These are required to run large language models with reduced VRAM usage.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/08 - Additional Tips.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge mpi4py mpich\npip install -U deepspeed\n```\n\n----------------------------------------\n\nTITLE: Downloading Miniconda via cURL (Shell)\nDESCRIPTION: This shell command downloads the Miniconda installer script for Linux.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -sL \"https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\" > \"Miniconda3.sh\"\n```\n\n----------------------------------------\n\nTITLE: Logits API Request with Sampling Parameters using curl\nDESCRIPTION: This curl command sends a request to the /v1/internal/logits endpoint, specifying sampling parameters such as `use_samplers` set to true and `top_k` set to 3.  The API should return the logits for the given prompt, taking the specified sampling parameters into account.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncurl -k http://127.0.0.1:5000/v1/internal/logits \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Who is best, Asuka or Rei? Answer:\",\n    \"use_samplers\": true,\n    \"top_k\": 3\n  }'\n```\n\n----------------------------------------\n\nTITLE: Cloning and Configuring on Windows\nDESCRIPTION: These commands clone the repository, navigate to the cloned directory, copy the .env example file, and open the .env file in notepad for editing.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/09 - Docker.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncd Desktop\ngit clone https://github.com/oobabooga/text-generation-webui\ncd text-generation-webui\nCOPY .env.example .env\nnotepad .env\n```\n\n----------------------------------------\n\nTITLE: Customizable Extension Parameter\nDESCRIPTION: This snippet shows how to define a customizable parameter within the `params` dictionary of an extension. The value associated with the `language string` key can be overridden by defining a corresponding key in the `settings.yaml` file, using the format `extension_name-variable_name`. This provides a flexible way to configure extensions.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nparams = {\n    \"display_name\": \"Google Translate\",\n    \"is_tab\": True,\n    \"language string\": \"jp\"\n}\n```\n\n----------------------------------------\n\nTITLE: Getting WSL IP Address\nDESCRIPTION: Retrieves the IP address of the WSL instance. This IP address is necessary for setting up port forwarding to access services running within WSL from the host machine or other devices on the network.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/10 - WSL.md#_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\nwsl hostname -I\n```\n\n----------------------------------------\n\nTITLE: Logits API Request with curl\nDESCRIPTION: This curl command makes a request to the /v1/internal/logits endpoint. It provides a prompt and sets \"use_samplers\" to false. The API is expected to return the logits for the provided prompt without using sampling parameters.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncurl -k http://127.0.0.1:5000/v1/internal/logits \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Who is best, Asuka or Rei? Answer:\",\n    \"use_samplers\": false\n  }'\n```\n\n----------------------------------------\n\nTITLE: UI Function\nDESCRIPTION: This function is executed when the UI is drawn. It's where custom Gradio elements and their corresponding event handlers should be defined.  It leverages the Gradio library to create interactive components within the web interface.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef ui():\n    \"\"\"\n    Gets executed when the UI is drawn. Custom gradio elements and\n    their corresponding event handlers should be defined here.\n\n    To learn about gradio components, check out the docs:\n    https://gradio.app/docs/\n    \"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Installing libstdc++-12-dev on Debian/Ubuntu\nDESCRIPTION: Installs the libstdc++-12-dev package which provides the C++ standard library headers and static libraries. This is required for some systems when using ROCm.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/11 - AMD Setup.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install libstdc++-12-dev\n```\n\n----------------------------------------\n\nTITLE: Copying .env file\nDESCRIPTION: This command copies the .env.example file to .env, allowing for customization of environment variables. Assumes a .env.example file is present.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/09 - Docker.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Alpaca Prompt Format Example\nDESCRIPTION: This snippet shows the prompt format expected by Alpaca models. It consists of an instruction followed by a corresponding response, indicated by the `### Instruction:` and `### Response:` prefixes. The model is trained to complete the request based on the given instruction.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/01 - Chat Tab.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nHi there!\n\n### Response:\nHello! It's nice to meet you. What can I help with?\n\n### Instruction:\nHow are you?\n\n### Response:\nI'm doing well, thank you for asking! Is there something specific you would like to talk about or ask me? I'm here to help answer any questions you may have.\n\n```\n\n----------------------------------------\n\nTITLE: llama.cpp Binary Installation (Windows)\nDESCRIPTION: This section provides the URL for a pre-built llama.cpp binary for Windows systems with AVX2 support. It also includes conditional requirements based on the platform and Python version to ensure the correct binary is installed. This binary is for CPU only and AVX2 compatible systems.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/portable/requirements_cpu_only.txt#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nhttps://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.3.0/llama_cpp_binaries-0.3.0+cpuavx2-cp311-cp311-win_amd64.whl; platform_system == \"Windows\" and python_version == \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Port Forwarding in PowerShell\nDESCRIPTION: Sets up port forwarding using netsh to allow access to services running inside WSL from the host machine. Requires administrator privileges and the WSL IP address obtained with `wsl hostname -I`.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/10 - WSL.md#_snippet_12\n\nLANGUAGE: PowerShell\nCODE:\n```\nnetsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=7860 connectaddress=172.20.134.111 connectport=7860\n```\n\n----------------------------------------\n\nTITLE: Docker Setup for CPU only (Shell)\nDESCRIPTION: This shell command creates symbolic links for the Dockerfile and docker-compose file for CPU-only systems, simplifying Docker configuration.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nln -s docker/{cpu/Dockerfile,cpu/docker-compose.yml,.dockerignore} .\n```\n\n----------------------------------------\n\nTITLE: Installing WSL Feature\nDESCRIPTION: Enables the Windows Subsystem for Linux feature. This command will automatically install the latest version of WSL along with a default Linux distribution.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/10 - WSL.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nwsl --install\n```\n\n----------------------------------------\n\nTITLE: Settings.yaml Configuration Example\nDESCRIPTION: This snippet shows how to override an extension parameter defined in the `params` dictionary using the `settings.yaml` file. The key `google_translate-language string` corresponds to the `language string` variable in the `google_translate` extension.  Changing this value will modify the behavior of the Google Translate extension.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ngoogle_translate-language string: 'fr'\n```\n\n----------------------------------------\n\nTITLE: Chat Input Modifier Function\nDESCRIPTION: This function modifies the user input string in chat mode. It can modify both the visible text displayed to the user and the internal representation of the input used in the prompt. Currently, it's a placeholder and returns the original input and visible text.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef chat_input_modifier(text, visible_text, state):\n    \"\"\"\n    Modifies the user input string in chat mode (visible_text).\n    You can also modify the internal representation of the user\n    input (text) to change how it will appear in the prompt.\n    \"\"\"\n    return text, visible_text\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Base URL\nDESCRIPTION: This snippet shows how to set the `OPENAI_API_BASE` environment variable to point to the local text-generation-webui instance. This configuration is necessary for various applications and libraries to utilize the web UI's API. The URL points to the /v1 endpoint of the API.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/12 - OpenAI API.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_BASE=http://127.0.0.1:5001/v1\n```\n\n----------------------------------------\n\nTITLE: Logits Processor Modifier Function\nDESCRIPTION: This function adds logits processors to the list, allowing access and modification of next token probabilities. This example adds an instance of `MyLogits` to the processor list. Only used by loaders using the transformers library for sampling.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef logits_processor_modifier(processor_list, input_ids):\n    \"\"\"\n    Adds logits processors to the list, allowing you to access and modify\n    the next token probabilities.\n    Only used by loaders that use the transformers library for sampling.\n    \"\"\"\n    processor_list.append(MyLogits())\n    return processor_list\n```\n\n----------------------------------------\n\nTITLE: Defining API Dependencies\nDESCRIPTION: This snippet defines the API-related dependencies required by the text-generation-webui project. These packages are necessary for implementing and serving API endpoints.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/full/requirements_noavx2.txt#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nflask_cloudflared==0.0.14\nsse-starlette==1.6.5\ntiktoken\n```\n\n----------------------------------------\n\nTITLE: History Modifier Function\nDESCRIPTION: This function modifies the chat history. It's currently a placeholder and simply returns the original history. It is designed to be used in chat mode to alter the conversation history.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef history_modifier(history):\n    \"\"\"\n    Modifies the chat history.\n    Only used in chat mode.\n    \"\"\"\n    return history\n```\n\n----------------------------------------\n\nTITLE: Bot Prefix Modifier Function\nDESCRIPTION: This function modifies the prefix for the next bot reply in chat mode. The default prefix is typically something like 'Bot Name:'. This is a placeholder function that simply returns the original string.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef bot_prefix_modifier(string, state):\n    \"\"\"\n    Modifies the prefix for the next bot reply in chat mode.\n    By default, the prefix will be something like \"Bot Name:\".\n    \"\"\"\n    return string\n```\n\n----------------------------------------\n\nTITLE: Help with Download Model Script (Python)\nDESCRIPTION: This command displays the help message for the `download-model.py` script. This shows all the available options for customizing the model download process, like specifying the model's revision or the download path.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\npython download-model.py --help\n```\n\n----------------------------------------\n\nTITLE: Checking Model SHA256 Sum\nDESCRIPTION: This command uses the download script to download a specified model and verify its SHA256 checksum. The `--check` flag triggers the checksum verification after the download is complete, ensuring the model's integrity.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/08 - Additional Tips.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython download-model.py facebook/galactica-125m --check\n```\n\n----------------------------------------\n\nTITLE: Output Modifier Function\nDESCRIPTION: This function modifies the LLM output before it is presented to the user. In chat mode, the modified version goes into the history['visible'], while the original version goes into history['internal']. Currently, it's a placeholder and returns the original string.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef output_modifier(string, state, is_chat=False):\n    \"\"\"\n    Modifies the LLM output before it gets presented.\n\n    In chat mode, the modified version goes into history['visible'],\n    and the original version goes into history['internal'].\n    \"\"\"\n    return string\n```\n\n----------------------------------------\n\nTITLE: Updating Ubuntu Package List\nDESCRIPTION: Updates the package list for Ubuntu. This is required before upgrading packages to ensure you have the latest information about available updates.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/10 - WSL.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\nsudo apt update\n```\n\n----------------------------------------\n\nTITLE: Cloning the text-generation-webui Repository\nDESCRIPTION: These commands clone the text-generation-webui repository from GitHub and navigate into the cloned directory.  This assumes git is already installed.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/09 - Docker.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/oobabooga/text-generation-webui\ncd text-generation-webui\n```\n\n----------------------------------------\n\nTITLE: Custom JavaScript Function\nDESCRIPTION: This function returns a JavaScript string that gets appended to the webui's JavaScript, enabling custom client-side behavior.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef custom_js():\n    \"\"\"\n    Returns a javascript string that gets appended to the javascript\n    for the webui.\n    \"\"\"\n    return ''\n```\n\n----------------------------------------\n\nTITLE: Installing Drivers and Dependencies on Windows\nDESCRIPTION: This command installs NVIDIA drivers, CUDA, Git, and Docker Desktop on Windows using Chocolatey.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/09 - Docker.md#_snippet_11\n\nLANGUAGE: powershell\nCODE:\n```\nchoco install nvidia-display-driver cuda git docker-desktop\n```\n\n----------------------------------------\n\nTITLE: Installing a Missing Package in Ubuntu\nDESCRIPTION: Installs a specified missing package in Ubuntu using the apt package manager. Requires root privileges and the correct package name.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/10 - WSL.md#_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\nsudo apt install [missing package]\n```\n\n----------------------------------------\n\nTITLE: Custom CSS Function\nDESCRIPTION: This function returns a CSS string that is appended to the webui's CSS, allowing for custom styling.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/07 - Extensions.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef custom_css():\n    \"\"\"\n    Returns a CSS string that gets appended to the CSS for the webui.\n    \"\"\"\n    return ''\n```\n\n----------------------------------------\n\nTITLE: Install SuperboogaV2 Dependencies\nDESCRIPTION: Installs the required Python packages for the SuperboogaV2 extension. This command is executed within the conda environment of text-generation-webui.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/extensions/superboogav2/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r extensions/superboogav2/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Cloning the Web UI Repository (Shell)\nDESCRIPTION: This command clones the text-generation-webui repository from GitHub.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/oobabooga/text-generation-webui\n```\n\n----------------------------------------\n\nTITLE: Installing WSL on Windows\nDESCRIPTION: This command installs the Windows Subsystem for Linux (WSL).\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/09 - Docker.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nwsl --install\n```\n\n----------------------------------------\n\nTITLE: Installing CUDA Toolkit with Conda (Shell)\nDESCRIPTION: This command installs the CUDA toolkit using Conda. It is useful when `nvcc` needs to compile a library manually. The `-y` flag automatically confirms the installation.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nconda install -y -c \"nvidia/label/cuda-12.4.1\" cuda\n```\n\n----------------------------------------\n\nTITLE: Executing the Miniconda installer (Shell)\nDESCRIPTION: This shell command executes the downloaded Miniconda installer script.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nbash Miniconda3.sh\n```\n\n----------------------------------------\n\nTITLE: Setting the WSL Default Version (Windows 11)\nDESCRIPTION: Sets the default version of WSL to version 2. This is required for utilizing improved performance and features available in WSL2.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/10 - WSL.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nwsl --set-default-version 2\n```\n\n----------------------------------------\n\nTITLE: Navigating to Linux Install Directory\nDESCRIPTION: Changes the current directory to the specified Linux installation path. This is a prerequisite step to preparing a Linux distribution for installation through Powershell.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/10 - WSL.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ncd D:\\Path\\To\\Linux\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies\nDESCRIPTION: This section lists the Python packages that the text-generation-webui project depends on. Each line specifies a package name and its version constraint or exact version. These packages are installed using pip.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/portable/requirements_cpu_only.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nfastapi==0.112.4\ngradio==4.37.*\njinja2==3.1.6\nmarkdown\nnumpy==1.26.*\npydantic==2.8.2\npyyaml\nrequests\nrich\ntqdm\nflask_cloudflared==0.0.14\nsse-starlette==1.6.5\ntiktoken\n```\n\n----------------------------------------\n\nTITLE: Upgrading Ubuntu Packages\nDESCRIPTION: Upgrades all outdated packages in the Ubuntu system to their latest versions. Requires root privileges and follows updating the package list.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/10 - WSL.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\nsudo apt upgrade\n```\n\n----------------------------------------\n\nTITLE: llama.cpp Binary Installation (Linux)\nDESCRIPTION: This section provides the URL for a pre-built llama.cpp binary for Linux systems with AVX2 support. It also includes conditional requirements based on the platform and Python version to ensure the correct binary is installed. This binary is for CPU only and AVX2 compatible systems.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/portable/requirements_cpu_only.txt#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhttps://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.3.0/llama_cpp_binaries-0.3.0+cpuavx2-cp311-cp311-linux_x86_64.whl; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: Updating Drivers on Manjaro\nDESCRIPTION: This command updates the drivers on Manjaro using mhwd.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/09 - Docker.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsudo mhwd -a pci nonfree 0300\n```\n\n----------------------------------------\n\nTITLE: Rebooting the System\nDESCRIPTION: This command reboots the system.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/09 - Docker.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nreboot\n```\n\n----------------------------------------\n\nTITLE: Installing Chocolatey Package Manager on Windows\nDESCRIPTION: This command installs the Chocolatey package manager on Windows using PowerShell. It sets the execution policy, configures the security protocol, and downloads and executes the installation script.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/docs/09 - Docker.md#_snippet_10\n\nLANGUAGE: powershell\nCODE:\n```\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n```\n\n----------------------------------------\n\nTITLE: API Dependencies\nDESCRIPTION: This section lists dependencies specifically required for the API functionality of the text-generation-webui. These dependencies include flask_cloudflared, sse-starlette, and tiktoken. flask_cloudflared likely integrates with Cloudflare, sse-starlette enables server-sent events, and tiktoken is used for tokenization, often required for large language models.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/portable/requirements_apple_intel.txt#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nflask_cloudflared==0.0.14\nsse-starlette==1.6.5\ntiktoken\n```\n\n----------------------------------------\n\nTITLE: AMD Wheel Dependency\nDESCRIPTION: This section specifies the URL for downloading a pre-built wheel file optimized for AMD ROCm on Linux x86_64 systems with Python 3.11. The wheel is downloaded and installed based on the specified platform and Python version constraints.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/portable/requirements_amd.txt#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nhttps://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.3.0/llama_cpp_binaries-0.3.0+rocm6.1.2-cp311-cp311-linux_x86_64.whl; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: Conditional AMD Dependency\nDESCRIPTION: This entry specifies a conditional dependency for AMD systems.  It downloads a specific wheel file from a GitHub release based on the operating system, architecture, and Python version. The dependency ensures compatibility with ROCm 6.1.2 and AVX instructions.\nSOURCE: https://github.com/oobabooga/text-generation-webui/blob/main/requirements/portable/requirements_amd_noavx2.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nhttps://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.3.0/llama_cpp_binaries-0.3.0+rocm6.1.2avx-cp311-cp311-linux_x86_64.whl; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"\n```"
  }
]
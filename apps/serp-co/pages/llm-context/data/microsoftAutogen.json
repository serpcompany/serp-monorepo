[
  {
    "owner": "microsoft",
    "repo": "autogen",
    "content": "TITLE: Installing AutoGen Extensions Package\nDESCRIPTION: Command to install the first-party maintained extensions available in the 'autogen-ext' package. This is the core installation required for building multi-agent applications with AI agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/extensions-user-guide/installation.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"autogen-ext\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Team-Based Analysis with AutoGen Agents\nDESCRIPTION: Creates a team of specialized agents including a planning agent, web search agent, and data analyst agent to collaboratively analyze basketball statistics. Implements custom tools for web searching and percentage calculations.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndef create_team(model_client : OpenAIChatCompletionClient) -> SelectorGroupChat:\n    planning_agent = AssistantAgent(\n        \"PlanningAgent\",\n        description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a planning agent.\n        Your job is to break down complex tasks into smaller, manageable subtasks.\n        Your team members are:\n            Web search agent: Searches for information\n            Data analyst: Performs calculations\n\n        You only plan and delegate tasks - you do not execute them yourself.\n\n        When assigning tasks, use this format:\n        1. <agent> : <task>\n\n        After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n        \"\"\",\n    )\n    # ... rest of team creation code ...\n```\n\n----------------------------------------\n\nTITLE: Using Anthropic's Claude 3 Sonnet Model\nDESCRIPTION: Demonstrates how to use the AnthropicChatCompletionClient to access Claude 3 Sonnet model. This experimental client uses the Anthropic Python SDK under the hood.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.anthropic import AnthropicChatCompletionClient\n\nanthropic_client = AnthropicChatCompletionClient(model=\"claude-3-7-sonnet-20250219\")\nresult = await anthropic_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\nprint(result)\nawait anthropic_client.close()\n```\n\n----------------------------------------\n\nTITLE: Creating and Running an AssistantAgent with AgentChat in Python\nDESCRIPTION: This Python code snippet demonstrates how to create and run an AssistantAgent using the AgentChat framework and OpenAIChatCompletionClient. It requires Python 3.10+ and the autogen-agentchat and autogen-ext[openai] packages. The agent is initialized with the 'gpt-4o' model and tasked to say 'Hello World!'\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/index.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# pip install -U \"autogen-agentchat\" \"autogen-ext[openai]\"\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    agent = AssistantAgent(\"assistant\", OpenAIChatCompletionClient(model=\"gpt-4o\"))\n    print(await agent.run(task=\"Say 'Hello World!'\"))\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a Custom FunctionTool in AutoGen (Python)\nDESCRIPTION: This snippet illustrates creating a custom tool from a Python function using `FunctionTool`. It defines an async function `get_stock_price` with type annotations, including `typing_extensions.Annotated` for detailed parameter description. The function is then wrapped in a `FunctionTool` with a description. Finally, it demonstrates how to run this custom tool directly using `run_json` with specific arguments and print the formatted result. It requires `autogen_core`, `random`, and `typing_extensions`.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/tools.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\nfrom autogen_core import CancellationToken\nfrom autogen_core.tools import FunctionTool\nfrom typing_extensions import Annotated\n\n\nasync def get_stock_price(ticker: str, date: Annotated[str, \"Date in YYYY/MM/DD\"]) -> float:\n    # Returns a random stock price for demonstration purposes.\n    return random.uniform(10, 200)\n\n\n# Create a function tool.\nstock_price_tool = FunctionTool(get_stock_price, description=\"Get the stock price.\")\n\n# Run the tool.\ncancellation_token = CancellationToken()\nresult = await stock_price_tool.run_json({\"ticker\": \"AAPL\", \"date\": \"2021/01/01\"}, cancellation_token)\n\n# Print the result.\nprint(stock_price_tool.return_value_as_string(result))\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a Weather Assistant Agent with Tool Support\nDESCRIPTION: Complete example showing how to create an AssistantAgent that can use tools, specifically a weather information tool. The example demonstrates model client setup, tool definition, agent configuration with reflection capabilities, and executing the agent with a weather-related query.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/quickstart.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Define a model client. You can use other model client that implements\n# the `ChatCompletionClient` interface.\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    # api_key=\"YOUR_API_KEY\",\n)\n\n\n# Define a simple function tool that the agent can use.\n# For this example, we use a fake weather tool for demonstration purposes.\nasync def get_weather(city: str) -> str:\n    \"\"\"Get the weather for a given city.\"\"\"\n    return f\"The weather in {city} is 73 degrees and Sunny.\"\n\n\n# Define an AssistantAgent with the model, tool, system message, and reflection enabled.\n# The system message instructs the agent via natural language.\nagent = AssistantAgent(\n    name=\"weather_agent\",\n    model_client=model_client,\n    tools=[get_weather],\n    system_message=\"You are a helpful assistant.\",\n    reflect_on_tool_use=True,\n    model_client_stream=True,  # Enable streaming tokens from the model client.\n)\n\n\n# Run the agent and stream the messages to the console.\nasync def main() -> None:\n    await Console(agent.run_stream(task=\"What is the weather in New York?\"))\n    # Close the connection to the model client.\n    await model_client.close()\n\n\n# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\nawait main()\n```\n\n----------------------------------------\n\nTITLE: Running an AutoGen Team Task in Python\nDESCRIPTION: This snippet shows how to execute a task using the previously created `RoundRobinGroupChat` team. It calls the asynchronous `team.run()` method with a specific task prompt (\"Write a short poem about the fall season.\"). The team runs until the defined termination condition is met, and the resulting `TaskResult` object, containing all generated messages, is printed.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/teams.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Use `asyncio.run(...)` when running in a script.\nresult = await team.run(task=\"Write a short poem about the fall season.\")\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running the Autogen Group Chat\nDESCRIPTION: This snippet sets up and executes the multi-agent conversation. It defines a `TextMentionTermination` condition, triggering termination when the word 'TERMINATE' is mentioned. It then creates a `RoundRobinGroupChat` with the previously defined agents and the termination condition. The chat is run asynchronously for the task 'Plan a 3 day trip to Nepal.' using a `Console` interface for input/output. Finally, it closes the OpenAI model client connection to release resources.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/travel-planning.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\ntermination = TextMentionTermination(\"TERMINATE\")\ngroup_chat = RoundRobinGroupChat(\n    [planner_agent, local_agent, language_agent, travel_summary_agent], termination_condition=termination\n)\nawait Console(group_chat.run_stream(task=\"Plan a 3 day trip to Nepal.\"))\n\nawait model_client.close()\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Local NuGet.config for AutoGen Nightly Builds\nDESCRIPTION: This XML configuration sets up a local NuGet.config file to include the AutoGen nightly build feed. Replace $(FEED_URL) with the actual feed URL provided.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Installation.md#2025-04-22_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<configuration>\n  <packageSources>\n    <clear />\n    <add key=\"AutoGen\" value=\"$(FEED_URL)\" /> <!-- replace $(FEED_URL) with the feed url -->\n    <!-- other feeds -->\n  </packageSources>\n  <disabledPackageSources />\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Implementing Single-Agent Team with RoundRobinGroupChat in Python\nDESCRIPTION: This code demonstrates how to set up a single-agent team using RoundRobinGroupChat and TextMessageTermination in AutoGen. It creates an AssistantAgent with a tool to increment numbers, sets up a termination condition, and runs the team to increment a number from 5 to 10.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/teams.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import TextMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    # api_key=\"sk-...\", # Optional if you have an OPENAI_API_KEY env variable set.\n    # Disable parallel tool calls for this example.\n    parallel_tool_calls=False,  # type: ignore\n)\n\n\n# Create a tool for incrementing a number.\ndef increment_number(number: int) -> int:\n    \"\"\"Increment a number by 1.\"\"\"\n    return number + 1\n\n\n# Create a tool agent that uses the increment_number function.\nlooped_assistant = AssistantAgent(\n    \"looped_assistant\",\n    model_client=model_client,\n    tools=[increment_number],  # Register the tool.\n    system_message=\"You are a helpful AI assistant, use the tool to increment the number.\",\n)\n\n# Termination condition that stops the task if the agent responds with a text message.\ntermination_condition = TextMessageTermination(\"looped_assistant\")\n\n# Create a team with the looped assistant agent and the termination condition.\nteam = RoundRobinGroupChat(\n    [looped_assistant],\n    termination_condition=termination_condition,\n)\n\n# Run the team with a task and print the messages to the console.\nasync for message in team.run_stream(task=\"Increment the number 5 to 10.\"):  # type: ignore\n    print(type(message).__name__, message)\n\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Using Ollama for Local Model Inference\nDESCRIPTION: Shows how to use the OllamaChatCompletionClient to interact with locally running Llama 3.2 model through Ollama. This requires Ollama to be running on the local machine.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.ollama import OllamaChatCompletionClient\n\n# Assuming your Ollama server is running locally on port 11434.\nollama_model_client = OllamaChatCompletionClient(model=\"llama3.2\")\n\nresponse = await ollama_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\nprint(response)\nawait ollama_model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Creating and Using RAG Assistant Agent with AutoGen\nDESCRIPTION: This snippet creates a RAG assistant agent using the initialized vector memory and demonstrates how to use it to answer questions about AutoGen. It also shows how to properly close the memory when finished.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/memory.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nrag_assistant = AssistantAgent(\n    name=\"rag_assistant\", model_client=OpenAIChatCompletionClient(model=\"gpt-4o\"), memory=[rag_memory]\n)\n\n# Ask questions about AutoGen\nstream = rag_assistant.run_stream(task=\"What is AgentChat?\")\nawait Console(stream)\n\n# Remember to close the memory when done\nawait rag_memory.close()\n```\n\n----------------------------------------\n\nTITLE: Initializing Agents with the OpenAI Client in Python\nDESCRIPTION: Set up agents with specific roles and properties, defining their tool usage and handoff capabilities. The agents utilize the OpenAIChatCompletionClient for model interactions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/swarm.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    # api_key=\"YOUR_API_KEY\",\n)\n\ntravel_agent = AssistantAgent(\n    \"travel_agent\",\n    model_client=model_client,\n    handoffs=[\"flights_refunder\", \"user\"],\n    system_message=\"\"\"You are a travel agent.\n    The flights_refunder is in charge of refunding flights.\n    If you need information from the user, you must first send your message, then you can handoff to the user.\n    Use TERMINATE when the travel planning is complete.\"\"\",\n)\n\nflights_refunder = AssistantAgent(\n    \"flights_refunder\",\n    model_client=model_client,\n    handoffs=[\"travel_agent\", \"user\"],\n    tools=[refund_flight],\n    system_message=\"\"\"You are an agent specialized in refunding flights.\n    You only need flight reference numbers to refund a flight.\n    You have the ability to refund a flight using the refund_flight tool.\n    If you need information from the user, you must first send your message, then you can handoff to the user.\n    When the transaction is complete, handoff to the travel agent to finalize.\"\"\",\n)\n```\n\n----------------------------------------\n\nTITLE: Serializing and Deserializing AssistantAgent in AutoGen\nDESCRIPTION: This snippet shows how to serialize an AssistantAgent to a configuration dictionary and then deserialize it back into a new AssistantAgent object.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/serialize-components.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nagent_config = agent.dump_component()  # dump component\nprint(agent_config.model_dump_json())\nagent_new = agent.load_component(agent_config)  # load component\n```\n\n----------------------------------------\n\nTITLE: Executing Python Code with PythonCodeExecutionTool in AutoGen\nDESCRIPTION: This snippet demonstrates how to instantiate and use the built-in `PythonCodeExecutionTool` in AutoGen. It sets up a `DockerCommandLineCodeExecutor`, starts it, creates the tool wrapper, and then directly executes a simple Python code string ('print(\\'Hello, world!\\')') using the tool's `run_json` method. The result is then printed after being formatted by `return_value_as_string`. This requires `autogen_core` and `autogen_ext` dependencies.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/tools.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core import CancellationToken\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_ext.tools.code_execution import PythonCodeExecutionTool\n\n# Create the tool.\ncode_executor = DockerCommandLineCodeExecutor()\nawait code_executor.start()\ncode_execution_tool = PythonCodeExecutionTool(code_executor)\ncancellation_token = CancellationToken()\n\n# Use the tool directly without an agent.\ncode = \"print('Hello, world!')\"\nresult = await code_execution_tool.run_json({\"code\": code}, cancellation_token)\nprint(code_execution_tool.return_value_as_string(result))\n```\n\n----------------------------------------\n\nTITLE: Importing AssistantAgent and related modules in Python\nDESCRIPTION: This snippet imports the necessary modules from autogen_agentchat for creating and using an AssistantAgent. It includes AssistantAgent, StructuredMessage, Console, and OpenAIChatCompletionClient from autogen_ext.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/agents.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import StructuredMessage\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\n----------------------------------------\n\nTITLE: Initializing a RoundRobinGroupChat Team in Python\nDESCRIPTION: This snippet demonstrates the setup of a simple multi-agent team using AutoGen's `RoundRobinGroupChat`. It initializes an OpenAI model client, creates two `AssistantAgent` instances (a primary agent and a critic agent implementing the reflection pattern), defines a `TextMentionTermination` condition to stop when the critic responds with 'APPROVE', and finally instantiates the `RoundRobinGroupChat` team with these agents and the termination condition.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/teams.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.base import TaskResult\nfrom autogen_agentchat.conditions import ExternalTermination, TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create an OpenAI model client.\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o-2024-08-06\",\n    # api_key=\"sk-...\", # Optional if you have an OPENAI_API_KEY env variable set.\n)\n\n# Create the primary agent.\nprimary_agent = AssistantAgent(\n    \"primary\",\n    model_client=model_client,\n    system_message=\"You are a helpful AI assistant.\",\n)\n\n# Create the critic agent.\ncritic_agent = AssistantAgent(\n    \"critic\",\n    model_client=model_client,\n    system_message=\"Provide constructive feedback. Respond with 'APPROVE' to when your feedbacks are addressed.\",\n)\n\n# Define a termination condition that stops the task if the critic approves.\ntext_termination = TextMentionTermination(\"APPROVE\")\n\n# Create a team with the primary and critic agents.\nteam = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=text_termination)\n```\n\n----------------------------------------\n\nTITLE: Configuring Specialized AI Agents for Market Research in Python\nDESCRIPTION: Sets up AI agents with specific roles (planner, financial analyst, news analyst, writer) using OpenAI's chat completion API. Each agent is configured with custom system messages and handoff capabilities.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/swarm.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    # api_key=\"YOUR_API_KEY\",\n)\n\nplanner = AssistantAgent(\n    \"planner\",\n    model_client=model_client,\n    handoffs=[\"financial_analyst\", \"news_analyst\", \"writer\"],\n    system_message=\"\"\"You are a research planning coordinator.\n    Coordinate market research by delegating to specialized agents:\n    - Financial Analyst: For stock data analysis\n    - News Analyst: For news gathering and analysis\n    - Writer: For compiling final report\n    Always send your plan first, then handoff to appropriate agent.\n    Always handoff to a single agent at a time.\n    Use TERMINATE when research is complete.\"\"\",\n)\n\nfinancial_analyst = AssistantAgent(\n    \"financial_analyst\",\n    model_client=model_client,\n    handoffs=[\"planner\"],\n    tools=[get_stock_data],\n    system_message=\"\"\"You are a financial analyst.\n    Analyze stock market data using the get_stock_data tool.\n    Provide insights on financial metrics.\n    Always handoff back to planner when analysis is complete.\"\"\",\n)\n\nnews_analyst = AssistantAgent(\n    \"news_analyst\",\n    model_client=model_client,\n    handoffs=[\"planner\"],\n    tools=[get_news],\n    system_message=\"\"\"You are a news analyst.\n    Gather and analyze relevant news using the get_news tool.\n    Summarize key market insights from news.\n    Always handoff back to planner when analysis is complete.\"\"\",\n)\n\nwriter = AssistantAgent(\n    \"writer\",\n    model_client=model_client,\n    handoffs=[\"planner\"],\n    system_message=\"\"\"You are a financial report writer.\n    Compile research findings into clear, concise reports.\n    Always handoff back to planner when writing is complete.\"\"\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Selector Function for SelectorGroupChat in Python\nDESCRIPTION: This code snippet shows how to implement a custom selector function to control the agent selection process, ensuring the Planning Agent speaks after any specialized agent.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/selector-group-chat.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n    if messages[-1].source != planning_agent.name:\n        return planning_agent.name\n    return None\n\n# Reset the previous team and run the chat again with the selector function.\nawait team.reset()\nteam = SelectorGroupChat(\n    [planning_agent, web_search_agent, data_analyst_agent],\n    model_client=model_client,\n    termination_condition=termination,\n    selector_prompt=selector_prompt,\n    allow_repeated_speaker=True,\n    selector_func=selector_func,\n)\n\nawait Console(team.run_stream(task=task))\n```\n\n----------------------------------------\n\nTITLE: Defining Assistant Agents for Literature Review Stages (Python)\nDESCRIPTION: Instantiates three AssistantAgent objects—one each for Google Search, Arxiv Search, and Report Synthesis—configuring them with their respective tools, model client (OpenAIChatCompletionClient with the 'gpt-4o-mini' model), agent roles, and system instructions. Agents are set up for modular collaboration and clearly segmented task responsibilities. Each agent expects to interact with users or other agents as part of a coordinated workflow.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/literature-review.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\\n\\ngoogle_search_agent = AssistantAgent(\\n    name=\"Google_Search_Agent\",\\n    tools=[google_search_tool],\\n    model_client=model_client,\\n    description=\"An agent that can search Google for information, returns results with a snippet and body content\",\\n    system_message=\"You are a helpful AI assistant. Solve tasks using your tools.\",\\n)\\n\\narxiv_search_agent = AssistantAgent(\\n    name=\"Arxiv_Search_Agent\",\\n    tools=[arxiv_search_tool],\\n    model_client=model_client,\\n    description=\"An agent that can search Arxiv for papers related to a given topic, including abstracts\",\\n    system_message=\"You are a helpful AI assistant. Solve tasks using your tools. Specifically, you can take into consideration the user's request and craft a search query that is most likely to return relevant academi papers.\",\\n)\\n\\n\\nreport_agent = AssistantAgent(\\n    name=\"Report_Agent\",\\n    model_client=model_client,\\n    description=\"Generate a report based on a given topic\",\\n    system_message=\"You are a helpful assistant. Your task is to synthesize data extracted into a high quality literature review including CORRECT references. You MUST write a final report that is formatted as a literature review with CORRECT references.  Your response should end with the word 'TERMINATE'\",\\n)\\n\n```\n\n----------------------------------------\n\nTITLE: Using ToolUseAgent for Stock Price Query\nDESCRIPTION: Demonstrates how to interact with the ToolUseAgent by sending a message and handling the response, including proper runtime startup and cleanup.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/tools.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Start processing messages.\nruntime.start()\n# Send a direct message to the tool agent.\ntool_use_agent = AgentId(\"tool_use_agent\", \"default\")\nresponse = await runtime.send_message(Message(\"What is the stock price of NVDA on 2024/06/01?\"), tool_use_agent)\nprint(response.content)\n# Stop processing messages.\nawait runtime.stop()\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Creating SemanticKernel Agent for Non-Streaming Chat\nDESCRIPTION: Demonstrates how to create a SemanticKernelAgent instance and interact with it using non-streaming message handling. Uses ChatMessageContent type for message exchange.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.SemanticKernel/SemanticKernelAgent-simple-chat.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/SemanticKernelCodeSnippet.cs?name=create_semantic_kernel_agent)]\n```\n\n----------------------------------------\n\nTITLE: Importing necessary libraries in Python\nDESCRIPTION: Import necessary libraries and modules for creating agents, conditions, messages, and team functionalities in the Swarm system. This includes autogen libraries for agents and OpenAI client for processing language models.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/swarm.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Any, Dict, List\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import HandoffTermination, TextMentionTermination\nfrom autogen_agentchat.messages import HandoffMessage\nfrom autogen_agentchat.teams import Swarm\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\n----------------------------------------\n\nTITLE: Implementing an Intervention Handler for Tool Execution Approval in Python\nDESCRIPTION: Defines the `ToolInterventionHandler` class, inheriting from `autogen_core.DefaultInterventionHandler`. It overrides the `on_send` method to intercept outgoing messages. If the message is a `FunctionCall`, it uses the `input()` function to prompt the user for approval, displaying the function name and arguments. If the user does not respond with 'y' (case-insensitive), it raises a `ToolException` to prevent the tool execution; otherwise, it allows the message to proceed.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/tool-use-with-intervention.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass ToolInterventionHandler(DefaultInterventionHandler):\n    async def on_send(\n        self, message: Any, *, message_context: MessageContext, recipient: AgentId\n    ) -> Any | type[DropMessage]:\n        if isinstance(message, FunctionCall):\n            # Request user prompt for tool execution.\n            user_input = input(\n                f\"Function call: {message.name}\\nArguments: {message.arguments}\\nDo you want to execute the tool? (y/n): \"\n            )\n            if user_input.strip().lower() != \"y\":\n                raise ToolException(content=\"User denied tool execution.\", call_id=message.id, name=message.name)\n        return message\n```\n\n----------------------------------------\n\nTITLE: Defining Specialized Autogen Agents for Travel Planning\nDESCRIPTION: This code defines four distinct `AssistantAgent` instances for the travel planning task, each with a specific role and system message. It initializes an `OpenAIChatCompletionClient` using the 'gpt-4o' model, which is then used by all agents (`planner_agent`, `local_agent`, `language_agent`, `travel_summary_agent`). Each agent is configured with a description and a system message guiding its behavior within the chat.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/travel-planning.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\nplanner_agent = AssistantAgent(\n    \"planner_agent\",\n    model_client=model_client,\n    description=\"A helpful assistant that can plan trips.\",\n    system_message=\"You are a helpful assistant that can suggest a travel plan for a user based on their request.\",\n)\n\nlocal_agent = AssistantAgent(\n    \"local_agent\",\n    model_client=model_client,\n    description=\"A local assistant that can suggest local activities or places to visit.\",\n    system_message=\"You are a helpful assistant that can suggest authentic and interesting local activities or places to visit for a user and can utilize any context information provided.\",\n)\n\nlanguage_agent = AssistantAgent(\n    \"language_agent\",\n    model_client=model_client,\n    description=\"A helpful assistant that can provide language tips for a given destination.\",\n    system_message=\"You are a helpful assistant that can review travel plans, providing feedback on important/critical tips about how best to address language or communication challenges for the given destination. If the plan already includes language tips, you can mention that the plan is satisfactory, with rationale.\",\n)\n\ntravel_summary_agent = AssistantAgent(\n    \"travel_summary_agent\",\n    model_client=model_client,\n    description=\"A helpful assistant that can summarize the travel plan.\",\n    system_message=\"You are a helpful assistant that can take in all of the suggestions and advice from the other agents and provide a detailed final travel plan. You must ensure that the final plan is integrated and complete. YOUR FINAL RESPONSE MUST BE THE COMPLETE PLAN. When the plan is complete and all perspectives are integrated, you can respond with TERMINATE.\",\n)\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom AutoGen Agents for Code Generation and Execution\nDESCRIPTION: Defines the core agent classes (Assistant and Executor) and message handling infrastructure. The Assistant agent generates code using a chat completion client while the Executor agent runs the code in a Docker container. Includes message class definition and markdown code block extraction utility.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/code-execution-groupchat.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport re\nfrom dataclasses import dataclass\nfrom typing import List\n\nfrom autogen_core import DefaultTopicId, MessageContext, RoutedAgent, default_subscription, message_handler\nfrom autogen_core.code_executor import CodeBlock, CodeExecutor\nfrom autogen_core.models import (\n    AssistantMessage,\n    ChatCompletionClient,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\n\n\n@dataclass\nclass Message:\n    content: str\n\n\n@default_subscription\nclass Assistant(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"An assistant agent.\")\n        self._model_client = model_client\n        self._chat_history: List[LLMMessage] = [\n            SystemMessage(\n                content=\"\"\"Write Python script in markdown block, and it will be executed.\nAlways save figures to file in the current directory. Do not use plt.show(). All code required to complete this task must be contained within a single response.\"\"\",\n            )\n        ]\n\n    @message_handler\n    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n        self._chat_history.append(UserMessage(content=message.content, source=\"user\"))\n        result = await self._model_client.create(self._chat_history)\n        print(f\"\\n{'-'*80}\\nAssistant:\\n{result.content}\")\n        self._chat_history.append(AssistantMessage(content=result.content, source=\"assistant\"))  # type: ignore\n        await self.publish_message(Message(content=result.content), DefaultTopicId())  # type: ignore\n\n\ndef extract_markdown_code_blocks(markdown_text: str) -> List[CodeBlock]:\n    pattern = re.compile(r\"```(?:\\s*([\\w\\+\\-]+))?\\n([\\s\\S]*?)```\")\n    matches = pattern.findall(markdown_text)\n    code_blocks: List[CodeBlock] = []\n    for match in matches:\n        language = match[0].strip() if match[0] else \"\"\n        code_content = match[1]\n        code_blocks.append(CodeBlock(code=code_content, language=language))\n    return code_blocks\n\n\n@default_subscription\nclass Executor(RoutedAgent):\n    def __init__(self, code_executor: CodeExecutor) -> None:\n        super().__init__(\"An executor agent.\")\n        self._code_executor = code_executor\n\n    @message_handler\n    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n        code_blocks = extract_markdown_code_blocks(message.content)\n        if code_blocks:\n            result = await self._code_executor.execute_code_blocks(\n                code_blocks, cancellation_token=ctx.cancellation_token\n            )\n            print(f\"\\n{'-'*80}\\nExecutor:\\n{result.output}\")\n            await self.publish_message(Message(content=result.output), DefaultTopicId())\n```\n\n----------------------------------------\n\nTITLE: Running FastAPI Server for Team Chat\nDESCRIPTION: Starts the FastAPI server for team chat using the app_team.py script.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_fastapi/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython app_team.py\n```\n\n----------------------------------------\n\nTITLE: Testing an OpenAI Model Client with a Simple Query\nDESCRIPTION: Demonstrates how to send a test message to an OpenAI model client and print the response. This example also shows proper client cleanup using the close method.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core.models import UserMessage\n\nresult = await openai_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\nprint(result)\nawait openai_model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Consuming results from the asynchronous queue in Python\nDESCRIPTION: Iterates over the asynchronous queue to retrieve and print the values of any FinalResult messages that were output by the agent. Each result is destructured and its 'value' field printed, providing a mechanism for external code to access agent outputs. Requires the asyncio queue to have been populated beforehand.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/extracting-results-with-an-agent.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nwhile not queue.empty():\\n    print((result := await queue.get()).value)\n```\n\n----------------------------------------\n\nTITLE: Defining Message Protocol Classes for AutoGen Reflection Pattern\nDESCRIPTION: Implements the message protocol for communication between coder and reviewer agents using Python dataclasses. The protocol defines four message types: CodeWritingTask, CodeWritingResult, CodeReviewTask, and CodeReviewResult that handle the flow of code generation and review process.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/reflection.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass CodeWritingTask:\n    task: str\n\n\n@dataclass\nclass CodeWritingResult:\n    task: str\n    code: str\n    review: str\n\n\n@dataclass\nclass CodeReviewTask:\n    session_id: str\n    code_writing_task: str\n    code_writing_scratchpad: str\n    code: str\n\n\n@dataclass\nclass CodeReviewResult:\n    review: str\n    session_id: str\n    approved: bool\n```\n\n----------------------------------------\n\nTITLE: Registering Assistant Agent with Runtime\nDESCRIPTION: Setup of SingleThreadedAgentRuntime and registration of OpenAI Assistant agent with custom event handler.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/openai-assistant-agent.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core import SingleThreadedAgentRuntime\n\nruntime = SingleThreadedAgentRuntime()\nawait OpenAIAssistantAgent.register(\n    runtime,\n    \"assistant\",\n    lambda: OpenAIAssistantAgent(\n        description=\"OpenAI Assistant Agent\",\n        client=openai.AsyncClient(),\n        assistant_id=oai_assistant.id,\n        thread_id=thread.id,\n        assistant_event_handler_factory=lambda: EventHandler(),\n    ),\n)\nagent = AgentId(\"assistant\", \"default\")\n```\n\n----------------------------------------\n\nTITLE: Defining a LangGraph-Based Tool Use Agent - Python\nDESCRIPTION: Defines the LangGraphToolUseAgent class, a subclass of RoutedAgent, that orchestrates message processing with the ability to use tools and LLM calls within a LangGraph state graph. It manages agent-tool interactions via defined state nodes, conditional and normal edges, and handles user messages asynchronously. The class requires a description, an LLM model instance, and a list of tool callables; it supports tools integration via LangChain's API and expects LangGraph, LangChain, and supporting modules installed.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/langgraph-agent.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass LangGraphToolUseAgent(RoutedAgent):\n    def __init__(self, description: str, model: ChatOpenAI, tools: List[Callable[..., Any]]) -> None:  # pyright: ignore\n        super().__init__(description)\n        self._model = model.bind_tools(tools)  # pyright: ignore\n\n        # Define the function that determines whether to continue or not\n        def should_continue(state: MessagesState) -> Literal[\"tools\", END]:  # type: ignore\n            messages = state[\"messages\"]\n            last_message = messages[-1]\n            # If the LLM makes a tool call, then we route to the \"tools\" node\n            if last_message.tool_calls:  # type: ignore\n                return \"tools\"\n            # Otherwise, we stop (reply to the user)\n            return END\n\n        # Define the function that calls the model\n        async def call_model(state: MessagesState):  # type: ignore\n            messages = state[\"messages\"]\n            response = await self._model.ainvoke(messages)\n            # We return a list, because this will get added to the existing list\n            return {\"messages\": [response]}\n\n        tool_node = ToolNode(tools)  # pyright: ignore\n\n        # Define a new graph\n        self._workflow = StateGraph(MessagesState)\n\n        # Define the two nodes we will cycle between\n        self._workflow.add_node(\"agent\", call_model)  # pyright: ignore\n        self._workflow.add_node(\"tools\", tool_node)  # pyright: ignore\n\n        # Set the entrypoint as `agent`\n        # This means that this node is the first one called\n        self._workflow.set_entry_point(\"agent\")\n\n        # We now add a conditional edge\n        self._workflow.add_conditional_edges(\n            # First, we define the start node. We use `agent`.\n            # This means these are the edges taken after the `agent` node is called.\n            \"agent\",\n            # Next, we pass in the function that will determine which node is called next.\n            should_continue,  # type: ignore\n        )\n\n        # We now add a normal edge from `tools` to `agent`.\n        # This means that after `tools` is called, `agent` node is called next.\n        self._workflow.add_edge(\"tools\", \"agent\")\n\n        # Finally, we compile it!\n        # This compiles it into a LangChain Runnable,\n        # meaning you can use it as you would any other runnable.\n        # Note that we're (optionally) passing the memory when compiling the graph\n        self._app = self._workflow.compile()\n\n    @message_handler\n    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n        # Use the Runnable\n        final_state = await self._app.ainvoke(\n            {\n                \"messages\": [\n                    SystemMessage(\n                        content=\"You are a helpful AI assistant. You can use tools to help answer questions.\"\n                    ),\n                    HumanMessage(content=message.content),\n                ]\n            },\n            config={\"configurable\": {\"thread_id\": 42}},\n        )\n        response = Message(content=final_state[\"messages\"][-1].content)\n        return response\n```\n\n----------------------------------------\n\nTITLE: Creating Specialized Assistant Agents\nDESCRIPTION: This snippet demonstrates how to create specialized agents using the AssistantAgent class from the autogen framework. Each agent is configured with specific roles and associated tools to handle distinct tasks in the group chat system.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/selector-group-chat.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\nplanning_agent = AssistantAgent(\n    \"PlanningAgent\",\n    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n    model_client=model_client,\n    system_message=\"\"\"\n    You are a planning agent.\n    Your job is to break down complex tasks into smaller, manageable subtasks.\n    Your team members are:\n        WebSearchAgent: Searches for information\n        DataAnalystAgent: Performs calculations\n\n    You only plan and delegate tasks - you do not execute them yourself.\n\n    When assigning tasks, use this format:\n    1. <agent> : <task>\n\n    After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n    \"\"\",\n)\n\nweb_search_agent = AssistantAgent(\n    \"WebSearchAgent\",\n    description=\"An agent for searching information on the web.\",\n    tools=[search_web_tool],\n    model_client=model_client,\n    system_message=\"\"\"\n    You are a web search agent.\n    Your only tool is search_tool - use it to find information.\n    You make only one search call at a time.\n    Once you have the results, you never do calculations based on them.\n    \"\"\",\n)\n\ndata_analyst_agent = AssistantAgent(\n    \"DataAnalystAgent\",\n    description=\"An agent for performing calculations.\",\n    model_client=model_client,\n    tools=[percentage_change_tool],\n    system_message=\"\"\"\n    You are a data analyst.\n    Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n    If you have not seen the data, ask for it.\n    \"\"\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Virtual Environment with venv for AutoGen\nDESCRIPTION: Instructions for creating and activating a virtual environment using Python's built-in venv module, with platform-specific commands noted for Windows users. Also includes the command to deactivate the environment when finished.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/installation.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# On Windows, change `python3` to `python` (if `python` is Python 3).\npython3 -m venv .venv\n# On Windows, change `bin` to `scripts`.\nsource .venv/bin/activate\n```\n\nLANGUAGE: bash\nCODE:\n```\ndeactivate\n```\n\n----------------------------------------\n\nTITLE: Defining GetWeather Function in C#\nDESCRIPTION: This snippet defines a public partial class 'Function' with a 'GetWeather' method. This method will be used as the weather function for the OpenAI chat agent.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-use-function-call.md#2025-04-22_snippet_2\n\nLANGUAGE: C#\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/OpenAICodeSnippet.cs?name=weather_function)]\n```\n\n----------------------------------------\n\nTITLE: Building an AI Agent with ChatCompletion API\nDESCRIPTION: This code outlines building a simple AI agent using the RoutedAgent and ChatCompletionClient from AutoGen. The agent is capable of receiving user messages, processing them using an OpenAI model, and returning the response.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/model-clients.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom dataclasses import dataclass\n\nfrom autogen_core import MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\nfrom autogen_core.models import ChatCompletionClient, SystemMessage, UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\n@dataclass\nclass Message:\n    content: str\n\n\nclass SimpleAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"A simple agent\")\n        self._system_messages = [SystemMessage(content=\"You are a helpful AI assistant.\")]\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n        # Prepare input to the chat completion model.\n        user_message = UserMessage(content=message.content, source=\"user\")\n        response = await self._model_client.create(\n            self._system_messages + [user_message], cancellation_token=ctx.cancellation_token\n        )\n        # Return with the model's response.\n        assert isinstance(response.content, str)\n        return Message(content=response.content)\n```\n\n----------------------------------------\n\nTITLE: Implementing FastAPI WebSocket Integration with UserProxyAgent\nDESCRIPTION: Example of creating a custom input function for UserProxyAgent that waits for messages from a FastAPI WebSocket connection. This enables integration of AutoGen teams with web-based applications.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/human-in-the-loop.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@app.websocket(\"/ws/chat\")\nasync def chat(websocket: WebSocket):\n    await websocket.accept()\n\n    async def _user_input(prompt: str, cancellation_token: CancellationToken | None) -> str:\n        data = await websocket.receive_json() # Wait for user message from websocket.\n        message = TextMessage.model_validate(data) # Assume user message is a TextMessage.\n        return message.content\n    \n    # Create user proxy with custom input function\n    # Run the team with the user proxy\n    # ...\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Async Document Indexer for RAG with Memory Integration in Python\nDESCRIPTION: This extensive code block defines the SimpleDocumentIndexer class for loading, chunking, cleaning, and storing text documents into any memory backend adhering to AutoGen's Memory protocol. It uses aiofiles and aiohttp for async reading from local or URL sources, removes HTML tags, splits text, and asynchronously adds chunked MemoryContent records to memory. Key parameters include memory, chunk_size, sources list, and the MIME/content handling. Inputs are lists of document paths/URLs. The output is the number of chunks added. Dependencies: aiofiles, aiohttp, autogen_core.memory. Limitations include basic chunking strategy and naive HTML removal.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/memory.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport re\\nfrom typing import List\\n\\nimport aiofiles\\nimport aiohttp\\nfrom autogen_core.memory import Memory, MemoryContent, MemoryMimeType\\n\\n\\nclass SimpleDocumentIndexer:\\n    \\\"\\\"\\\"Basic document indexer for AutoGen Memory.\\\"\\\"\\\"\\n\\n    def __init__(self, memory: Memory, chunk_size: int = 1500) -> None:\\n        self.memory = memory\\n        self.chunk_size = chunk_size\\n\\n    async def _fetch_content(self, source: str) -> str:\\n        \\\"\\\"\\\"Fetch content from URL or file.\\\"\\\"\\\"\\n        if source.startswith((\\\"http://\\\", \\\"https://\\\")):\\n            async with aiohttp.ClientSession() as session:\\n                async with session.get(source) as response:\\n                    return await response.text()\\n        else:\\n            async with aiofiles.open(source, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n                return await f.read()\\n\\n    def _strip_html(self, text: str) -> str:\\n        \\\"\\\"\\\"Remove HTML tags and normalize whitespace.\\\"\\\"\\\"\\n        text = re.sub(r\\\"<[^>]*>\\\", \\\" \\\" , text)\\n        text = re.sub(r\\\"\\\\s+\\\", \\\" \\\" , text)\\n        return text.strip()\\n\\n    def _split_text(self, text: str) -> List[str]:\\n        \\\"\\\"\\\"Split text into fixed-size chunks.\\\"\\\"\\\"\\n        chunks: list[str] = []\\n        # Just split text into fixed-size chunks\\n        for i in range(0, len(text), self.chunk_size):\\n            chunk = text[i : i + self.chunk_size]\\n            chunks.append(chunk.strip())\\n        return chunks\\n\\n    async def index_documents(self, sources: List[str]) -> int:\\n        \\\"\\\"\\\"Index documents into memory.\\\"\\\"\\\"\\n        total_chunks = 0\\n\\n        for source in sources:\\n            try:\\n                content = await self._fetch_content(source)\\n\\n                # Strip HTML if content appears to be HTML\\n                if \\\"<\\\" in content and \\\">\\\" in content:\\n                    content = self._strip_html(content)\\n\\n                chunks = self._split_text(content)\\n\\n                for i, chunk in enumerate(chunks):\\n                    await self.memory.add(\\n                        MemoryContent(\\n                            content=chunk, mime_type=MemoryMimeType.TEXT, metadata={\\\"source\\\": source, \\\"chunk_index\\\": i}\\n                        )\\n                    )\\n\\n                total_chunks += len(chunks)\\n\\n            except Exception as e:\\n                print(f\\\"Error indexing {source}: {str(e)}\\\")\\n\\n        return total_chunks\\n\n```\n\n----------------------------------------\n\nTITLE: Invoking an AutoGen Tool via OpenAI Model Client (Python)\nDESCRIPTION: This snippet demonstrates how to integrate a custom `FunctionTool` (`stock_price_tool`) with an AI model client, specifically `OpenAIChatCompletionClient` using the `gpt-4o-mini` model. It sends a user message asking for a stock price and provides the `stock_price_tool` to the client's `create` method. The model client uses the tool's schema to generate a structured tool call based on the user's request. Requires `autogen_core`, `autogen_ext.models.openai`, `json`, and a configured OpenAI API key environment variable.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/tools.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nfrom autogen_core.models import AssistantMessage, FunctionExecutionResult, FunctionExecutionResultMessage, UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create the OpenAI chat completion client. Using OPENAI_API_KEY from environment variable.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n\n# Create a user message.\nuser_message = UserMessage(content=\"What is the stock price of AAPL on 2021/01/01?\", source=\"user\")\n\n# Run the chat completion with the stock_price_tool defined above.\ncancellation_token = CancellationToken()\ncreate_result = await model_client.create(\n    messages=[user_message], tools=[stock_price_tool], cancellation_token=cancellation_token\n)\ncreate_result.content\n```\n\n----------------------------------------\n\nTITLE: Running the Team Sample with UserProxyAgent\nDESCRIPTION: Command to launch the agent team chat interface that includes a UserProxyAgent. This sample requires user approval/rejection for agent responses, offering more user control over the interaction.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_chainlit/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nchainlit run app_team_user_proxy.py -h\n```\n\n----------------------------------------\n\nTITLE: Creating a Workflow Graph in C# using AutoGen.Core.Graph\nDESCRIPTION: This code snippet demonstrates how to create a graph representing the desired workflow for a group chat. It defines states for each agent and transitions between them, including conditional transitions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Use-graph-in-group-chat.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nvar graph = new Graph();\n\n// Create states\nvar adminState = graph.CreateState(\"admin\");\nvar coderState = graph.CreateState(\"coder\");\nvar reviewerState = graph.CreateState(\"reviewer\");\nvar runnerState = graph.CreateState(\"runner\");\n\n// Create transitions\ngraph.CreateTransition(adminState, coderState);\ngraph.CreateTransition(coderState, reviewerState);\ngraph.CreateTransition(reviewerState, runnerState);\ngraph.CreateTransition(runnerState, adminState);\ngraph.CreateTransition(runnerState, coderState);\ngraph.CreateTransition(reviewerState, coderState);\n\n// Set the initial state\ngraph.SetInitialState(adminState);\n```\n\n----------------------------------------\n\nTITLE: Creating Assistant Agent in AutoGen v0.4\nDESCRIPTION: Shows the updated approach for creating an AssistantAgent in v0.4 using model_client instead of llm_config.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=\"sk-xxx\", seed=42, temperature=0)\n\nassistant = AssistantAgent(\n    name=\"assistant\",\n    system_message=\"You are a helpful assistant.\",\n    model_client=model_client,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing MathSolver Agent for Multi-Agent Debate\nDESCRIPTION: Defines the MathSolver class, a RoutedAgent responsible for solving math problems and exchanging responses with other agents. It handles solver requests, generates responses using an LLM, and publishes intermediate or final responses based on the debate round.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/multi-agent-debate.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@default_subscription\nclass MathSolver(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient, topic_type: str, num_neighbors: int, max_round: int) -> None:\n        super().__init__(\"A debator.\")\n        self._topic_type = topic_type\n        self._model_client = model_client\n        self._num_neighbors = num_neighbors\n        self._history: List[LLMMessage] = []\n        self._buffer: Dict[int, List[IntermediateSolverResponse]] = {}\n        self._system_messages = [\n            SystemMessage(\n                content=(\n                    \"You are a helpful assistant with expertise in mathematics and reasoning. \"\n                    \"Your task is to assist in solving a math reasoning problem by providing \"\n                    \"a clear and detailed solution. Limit your output within 100 words, \"\n                    \"and your final answer should be a single numerical number, \"\n                    \"in the form of {{answer}}, at the end of your response. \"\n                    \"For example, 'The answer is {{42}}.\"\n                )\n            )\n        ]\n        self._round = 0\n        self._max_round = max_round\n\n    @message_handler\n    async def handle_request(self, message: SolverRequest, ctx: MessageContext) -> None:\n        # Add the question to the memory.\n        self._history.append(UserMessage(content=message.content, source=\"user\"))\n        # Make an inference using the model.\n        model_result = await self._model_client.create(self._system_messages + self._history)\n        assert isinstance(model_result.content, str)\n        # Add the response to the memory.\n        self._history.append(AssistantMessage(content=model_result.content, source=self.metadata[\"type\"]))\n        print(f\"{'-'*80}\\nSolver {self.id} round {self._round}:\\n{model_result.content}\")\n        # Extract the answer from the response.\n        match = re.search(r\"\\{\\{(\\-?\\d+(\\.\\d+)?)\\}\\}\", model_result.content)\n        if match is None:\n            raise ValueError(\"The model response does not contain the answer.\")\n        answer = match.group(1)\n        # Increment the counter.\n        self._round += 1\n        if self._round == self._max_round:\n            # If the counter reaches the maximum round, publishes a final response.\n            await self.publish_message(FinalSolverResponse(answer=answer), topic_id=DefaultTopicId())\n        else:\n            # Publish intermediate response to the topic associated with this solver.\n            await self.publish_message(\n                IntermediateSolverResponse(\n                    content=model_result.content,\n                    question=message.question,\n                    answer=answer,\n                    round=self._round,\n                ),\n                topic_id=DefaultTopicId(type=self._topic_type),\n            )\n\n    @message_handler\n    async def handle_response(self, message: IntermediateSolverResponse, ctx: MessageContext) -> None:\n        # Add neighbor's response to the buffer.\n        self._buffer.setdefault(message.round, []).append(message)\n        # Check if all neighbors have responded.\n        if len(self._buffer[message.round]) == self._num_neighbors:\n            print(\n                f\"{'-'*80}\\nSolver {self.id} round {message.round}:\\nReceived all responses from {self._num_neighbors} neighbors.\"\n            )\n            # Prepare the prompt for the next question.\n            prompt = \"These are the solutions to the problem from other agents:\\n\"\n            for resp in self._buffer[message.round]:\n                prompt += f\"One agent solution: {resp.content}\\n\"\n            prompt += (\n                \"Using the solutions from other agents as additional information, \"\n                \"can you provide your answer to the math problem? \"\n                f\"The original math problem is {message.question}. \"\n                \"Your final answer should be a single numerical number, \"\n                \"in the form of {{answer}}, at the end of your response.\"\n            )\n            # Send the question to the agent itself to solve.\n            await self.send_message(SolverRequest(content=prompt, question=message.question), self.id)\n            # Clear the buffer.\n            self._buffer.pop(message.round)\n```\n\n----------------------------------------\n\nTITLE: Creating AssistantAgent and UserProxyAgent in AutoGen\nDESCRIPTION: This snippet shows how to create an AssistantAgent configured with the OpenAI GPT-4o model and a UserProxyAgent. These instances will be used for serialization in the subsequent examples.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/serialize-components.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent, UserProxyAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create an agent that uses the OpenAI GPT-4o model.\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    # api_key=\"YOUR_API_KEY\",\n)\nagent = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    handoffs=[\"flights_refunder\", \"user\"],\n    # tools=[], # serializing tools is not yet supported\n    system_message=\"Use tools to solve tasks.\",\n)\nuser_proxy = UserProxyAgent(name=\"user\")\n```\n\n----------------------------------------\n\nTITLE: Creating Agents with Custom Tools in AutoGen using Python\nDESCRIPTION: This code creates a primary agent and a critic agent using AssistantAgent class from AutoGen. The critic agent is equipped with the 'approve' tool, which will be used to trigger the custom termination condition.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/termination.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    temperature=1,\n    # api_key=\"sk-...\", # Optional if you have an OPENAI_API_KEY env variable set.\n)\n\n# Create the primary agent.\nprimary_agent = AssistantAgent(\n    \"primary\",\n    model_client=model_client,\n    system_message=\"You are a helpful AI assistant.\",\n)\n\n# Create the critic agent with the approve function as a tool.\ncritic_agent = AssistantAgent(\n    \"critic\",\n    model_client=model_client,\n    tools=[approve],  # Register the approve function as a tool.\n    system_message=\"Provide constructive feedback. Use the approve tool to approve when all feedbacks are addressed.\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Extensions for AutoGen\nDESCRIPTION: Command to install the OpenAI and Azure OpenAI model extensions for AutoGen. This is required for using OpenAI and Azure OpenAI models with AutoGen.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/installation.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install \"autogen-ext[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Defining Google Search and Stock Analysis Tools in Python\nDESCRIPTION: This Python code defines two tool functions, google_search and analyze_stock. google_search queries the Google Search API for relevant information about a company and scrapes snippet and body text, requiring an API key and Search Engine ID set as environment variables. analyze_stock retrieves stock data for a ticker using yfinance, computes statistics (current price, 52-week range, moving averages, trend, volatility), and generates a chart using matplotlib. Dependencies include requests, bs4, yfinance, numpy, pandas, matplotlib, python-dotenv, and pytz. Inputs are query strings or ticker symbols; outputs are enriched results or structured statistics including the path to a generated plot image.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/company-research.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#!pip install yfinance matplotlib pytz numpy pandas python-dotenv requests bs4\\n\\ndef google_search(query: str, num_results: int = 2, max_chars: int = 500) -> list:  # type: ignore[type-arg]\\n    import os\\n    import time\\n\\n    import requests\\n    from bs4 import BeautifulSoup\\n    from dotenv import load_dotenv\\n\\n    load_dotenv()\\n\\n    api_key = os.getenv(\"GOOGLE_API_KEY\")\\n    search_engine_id = os.getenv(\"GOOGLE_SEARCH_ENGINE_ID\")\\n\\n    if not api_key or not search_engine_id:\\n        raise ValueError(\"API key or Search Engine ID not found in environment variables\")\\n\\n    url = \"https://customsearch.googleapis.com/customsearch/v1\"\\n    params = {\"key\": str(api_key), \"cx\": str(search_engine_id), \"q\": str(query), \"num\": str(num_results)}\\n\\n    response = requests.get(url, params=params)\\n\\n    if response.status_code != 200:\\n        print(response.json())\\n        raise Exception(f\"Error in API request: {response.status_code}\")\\n\\n    results = response.json().get(\"items\", [])\\n\\n    def get_page_content(url: str) -> str:\\n        try:\\n            response = requests.get(url, timeout=10)\\n            soup = BeautifulSoup(response.content, \"html.parser\")\\n            text = soup.get_text(separator=\" \", strip=True)\\n            words = text.split()\\n            content = \"\"\\n            for word in words:\\n                if len(content) + len(word) + 1 > max_chars:\\n                    break\\n                content += \" \" + word\\n            return content.strip()\\n        except Exception as e:\\n            print(f\"Error fetching {url}: {str(e)}\")\\n            return \"\"\\n\\n    enriched_results = []\\n    for item in results:\\n        body = get_page_content(item[\"link\"])\\n        enriched_results.append(\\n            {\"title\": item[\"title\"], \"link\": item[\"link\"], \"snippet\": item[\"snippet\"], \"body\": body}\\n        )\\n        time.sleep(1)  # Be respectful to the servers\\n\\n    return enriched_results\\n\\ndef analyze_stock(ticker: str) -> dict:  # type: ignore[type-arg]\\n    import os\\n    from datetime import datetime, timedelta\\n\\n    import matplotlib.pyplot as plt\\n    import numpy as np\\n    import pandas as pd\\n    import yfinance as yf\\n    from pytz import timezone  # type: ignore\\n\\n    stock = yf.Ticker(ticker)\\n\\n    # Get historical data (1 year of data to ensure we have enough for 200-day MA)\\n    end_date = datetime.now(timezone(\"UTC\"))\\n    start_date = end_date - timedelta(days=365)\\n    hist = stock.history(start=start_date, end=end_date)\\n\\n    # Ensure we have data\\n    if hist.empty:\\n        return {\"error\": \"No historical data available for the specified ticker.\"}\\n\\n    # Compute basic statistics and additional metrics\\n    current_price = stock.info.get(\"currentPrice\", hist[\"Close\"].iloc[-1])\\n    year_high = stock.info.get(\"fiftyTwoWeekHigh\", hist[\"High\"].max())\\n    year_low = stock.info.get(\"fiftyTwoWeekLow\", hist[\"Low\"].min())\\n\\n    # Calculate 50-day and 200-day moving averages\\n    ma_50 = hist[\"Close\"].rolling(window=50).mean().iloc[-1]\\n    ma_200 = hist[\"Close\"].rolling(window=200).mean().iloc[-1]\\n\\n    # Calculate YTD price change and percent change\\n    ytd_start = datetime(end_date.year, 1, 1, tzinfo=timezone(\"UTC\"))\\n    ytd_data = hist.loc[ytd_start:]  # type: ignore[misc]\\n    if not ytd_data.empty:\\n        price_change = ytd_data[\"Close\"].iloc[-1] - ytd_data[\"Close\"].iloc[0]\\n        percent_change = (price_change / ytd_data[\"Close\"].iloc[0]) * 100\\n    else:\\n        price_change = percent_change = np.nan\\n\\n    # Determine trend\\n    if pd.notna(ma_50) and pd.notna(ma_200):\\n        if ma_50 > ma_200:\\n            trend = \"Upward\"\\n        elif ma_50 < ma_200:\\n            trend = \"Downward\"\\n        else:\\n            trend = \"Neutral\"\\n    else:\\n        trend = \"Insufficient data for trend analysis\"\\n\\n    # Calculate volatility (standard deviation of daily returns)\\n    daily_returns = hist[\"Close\"].pct_change().dropna()\\n    volatility = daily_returns.std() * np.sqrt(252)  # Annualized volatility\\n\\n    # Create result dictionary\\n    result = {\\n        \"ticker\": ticker,\\n        \"current_price\": current_price,\\n        \"52_week_high\": year_high,\\n        \"52_week_low\": year_low,\\n        \"50_day_ma\": ma_50,\\n        \"200_day_ma\": ma_200,\\n        \"ytd_price_change\": price_change,\\n        \"ytd_percent_change\": percent_change,\\n        \"trend\": trend,\\n        \"volatility\": volatility,\\n    }\\n\\n    # Convert numpy types to Python native types for better JSON serialization\\n    for key, value in result.items():\\n        if isinstance(value, np.generic):\\n            result[key] = value.item()\\n\\n    # Generate plot\\n    plt.figure(figsize=(12, 6))\\n    plt.plot(hist.index, hist[\"Close\"], label=\"Close Price\")\\n    plt.plot(hist.index, hist[\"Close\"].rolling(window=50).mean(), label=\"50-day MA\")\\n    plt.plot(hist.index, hist[\"Close\"].rolling(window=200).mean(), label=\"200-day MA\")\\n    plt.title(f\"{ticker} Stock Price (Past Year)\")\\n    plt.xlabel(\"Date\")\\n    plt.ylabel(\"Price ($)\")\\n    plt.legend()\\n    plt.grid(True)\\n\\n    # Save plot to file\\n    os.makedirs(\"coding\", exist_ok=True)\\n    plot_file_path = f\"coding/{ticker}_stockprice.png\"\\n    plt.savefig(plot_file_path)\\n    print(f\"Plot saved as {plot_file_path}\")\\n    result[\"plot_file_path\"] = plot_file_path\\n\\n    return result\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using LMStudioAgent in C#\nDESCRIPTION: Demonstrates how to initialize an LMStudioAgent by configuring the endpoint and port, setting up the agent with a name and system message, and sending a request to generate Fibonacci code. The code also shows how to register a message printing hook.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/src/AutoGen.LMStudio/README.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nusing AutoGen.LMStudio;\nvar localServerEndpoint = \"localhost\";\nvar port = 5000;\nvar lmStudioConfig = new LMStudioConfig(localServerEndpoint, port);\nvar agent = new LMStudioAgent(\n    name: \"agent\",\n    systemMessage: \"You are an agent that help user to do some tasks.\",\n    lmStudioConfig: lmStudioConfig)\n    .RegisterPrintMessage(); // register a hook to print message nicely to console\n\nawait agent.SendAsync(\"Can you write a piece of C# code to calculate 100th of fibonacci?\");\n```\n\n----------------------------------------\n\nTITLE: Define Gemini Assistant Agent (Python)\nDESCRIPTION: Defines a custom agent, GeminiAssistantAgent, that integrates with the Google Gemini SDK. It inherits from BaseChatAgent and overrides the on_messages_stream method to generate responses using the Gemini API. It requires the google-genai library and an API key.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/custom-agents.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# !pip install google-genai\nimport os\nfrom typing import AsyncGenerator, Sequence\n\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.base import Response\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\nfrom autogen_core import CancellationToken\nfrom autogen_core.model_context import UnboundedChatCompletionContext\nfrom autogen_core.models import AssistantMessage, RequestUsage, UserMessage\nfrom google import genai\nfrom google.genai import types\n\n\nclass GeminiAssistantAgent(BaseChatAgent):\n    def __init__(\n        self,\n        name: str,\n        description: str = \"An agent that provides assistance with ability to use tools.\",\n        model: str = \"gemini-1.5-flash-002\",\n        api_key: str = os.environ[\"GEMINI_API_KEY\"],\n        system_message: str\n        | None = \"You are a helpful assistant that can respond to messages. Reply with TERMINATE when the task has been completed.\",\n    ):\n        super().__init__(name=name, description=description)\n        self._model_context = UnboundedChatCompletionContext()\n        self._model_client = genai.Client(api_key=api_key)\n        self._system_message = system_message\n        self._model = model\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        final_response = None\n        async for message in self.on_messages_stream(messages, cancellation_token):\n            if isinstance(message, Response):\n                final_response = message\n\n        if final_response is None:\n            raise AssertionError(\"The stream should have returned the final result.\")\n\n        return final_response\n\n    async def on_messages_stream(\n        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken\n    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:\n        # Add messages to the model context\n        for msg in messages:\n            await self._model_context.add_message(msg.to_model_message())\n\n        # Get conversation history\n        history = [\n            (msg.source if hasattr(msg, \"source\") else \"system\")\n            + \": \"\n            + (msg.content if isinstance(msg.content, str) else \"\")\n            + \"\\n\"\n            for msg in await self._model_context.get_messages()\n        ]\n        # Generate response using Gemini\n        response = self._model_client.models.generate_content(\n            model=self._model,\n            contents=f\"History: {history}\\nGiven the history, please provide a response\",\n            config=types.GenerateContentConfig(\n                system_instruction=self._system_message,\n                temperature=0.3,\n            ),\n        )\n\n        # Create usage metadata\n        usage = RequestUsage(\n            prompt_tokens=response.usage_metadata.prompt_token_count,\n            completion_tokens=response.usage_metadata.candidates_token_count,\n        )\n\n        # Add response to model context\n        await self._model_context.add_message(AssistantMessage(content=response.text, source=self.name))\n\n        # Yield the final response\n        yield Response(\n            chat_message=TextMessage(content=response.text, source=self.name, models_usage=usage),\n            inner_messages=[],\n        )\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        \"\"\"Reset the assistant by clearing the model context.\"\"\"\n        await self._model_context.clear()\n```\n\n----------------------------------------\n\nTITLE: Registering Function Call Middleware with Agent\nDESCRIPTION: Adding the function call middleware to the MistralClientAgent for processing function calls\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/MistralChatAgent-use-function-call.md#2025-04-22_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\nmistralAgent.AddMiddleware(functionCallMiddleware);\n```\n\n----------------------------------------\n\nTITLE: Streaming a New Task (Meal Plan) to Agent and Displaying Output in Console (Python)\nDESCRIPTION: This snippet demonstrates running a new agent task for generating a meal recipe while streaming the response to the console. The agent utilizes memories (like user preferences for vegan diet) to contextually tailor its output. The code assumes previous setup and memory additions. Inputs are user queries and outputs are streamed agent responses. Limitations include agent and memory model readiness.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/memory.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nstream = assistant_agent.run_stream(task=\\\"Write brief meal recipe with broth\\\")\\nawait Console(stream)\\n\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenAI Assistant Event Handler in Python\nDESCRIPTION: Custom implementation of AsyncAssistantEventHandler for handling various Assistant API events including text streaming, run steps, and message handling. Includes methods for processing code interpreter outputs and file citations.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/openai-assistant-agent.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import AsyncAssistantEventHandler, AsyncClient\nfrom openai.types.beta.threads import Message, Text, TextDelta\nfrom openai.types.beta.threads.runs import RunStep, RunStepDelta\nfrom typing_extensions import override\n\n\nclass EventHandler(AsyncAssistantEventHandler):\n    @override\n    async def on_text_delta(self, delta: TextDelta, snapshot: Text) -> None:\n        print(delta.value, end=\"\", flush=True)\n\n    @override\n    async def on_run_step_created(self, run_step: RunStep) -> None:\n        details = run_step.step_details\n        if details.type == \"tool_calls\":\n            for tool in details.tool_calls:\n                if tool.type == \"code_interpreter\":\n                    print(\"\\nGenerating code to interpret:\\n\\n```python\")\n\n    @override\n    async def on_run_step_done(self, run_step: RunStep) -> None:\n        details = run_step.step_details\n        if details.type == \"tool_calls\":\n            for tool in details.tool_calls:\n                if tool.type == \"code_interpreter\":\n                    print(\"\\n```\\nExecuting code...\")\n\n    @override\n    async def on_run_step_delta(self, delta: RunStepDelta, snapshot: RunStep) -> None:\n        details = delta.step_details\n        if details is not None and details.type == \"tool_calls\":\n            for tool in details.tool_calls or []:\n                if tool.type == \"code_interpreter\" and tool.code_interpreter and tool.code_interpreter.input:\n                    print(tool.code_interpreter.input, end=\"\", flush=True)\n\n    @override\n    async def on_message_created(self, message: Message) -> None:\n        print(f\"{'-'*80}\\nAssistant:\\n\")\n\n    @override\n    async def on_message_done(self, message: Message) -> None:\n        # print a citation to the file searched\n        if not message.content:\n            return\n        content = message.content[0]\n        if not content.type == \"text\":\n            return\n        text_content = content.text\n        annotations = text_content.annotations\n        citations: List[str] = []\n        for index, annotation in enumerate(annotations):\n            text_content.value = text_content.value.replace(annotation.text, f\"[{index}]\")\n            if file_citation := getattr(annotation, \"file_citation\", None):\n                client = AsyncClient()\n                cited_file = await client.files.retrieve(file_citation.file_id)\n                citations.append(f\"[{index}] {cited_file.filename}\")\n        if citations:\n            print(\"\\n\".join(citations))\n```\n\n----------------------------------------\n\nTITLE: Observing AutoGen Team Execution Stream in Python\nDESCRIPTION: This snippet demonstrates how to observe the messages generated by a team in real-time. It first resets the team state using `await team.reset()`. Then, it iterates through the asynchronous generator returned by `team.run_stream()`, printing each message as it's produced. Finally, it checks the `stop_reason` attribute of the `TaskResult` object yielded at the end of the stream.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/teams.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# When running inside a script, use a async main function and call it from `asyncio.run(...)`.\nawait team.reset()  # Reset the team for a new task.\nasync for message in team.run_stream(task=\"Write a short poem about the fall season.\"):  # type: ignore\n    if isinstance(message, TaskResult):\n        print(\"Stop Reason:\", message.stop_reason)\n    else:\n        print(message)\n```\n\n----------------------------------------\n\nTITLE: Defining Google Search and Arxiv Search Utility Functions (Python)\nDESCRIPTION: Implements two functions: google_search leverages Google Custom Search API (requiring environment variables GOOGLE_API_KEY and GOOGLE_SEARCH_ENGINE_ID and requests, BeautifulSoup, and dotenv libraries) to fetch and summarize search results, with enriched in-page content. arxiv_search queries the Arxiv repository (requires arxiv library), extracting relevant paper metadata. Both functions include parameters to control results depth and return lists of search result dictionaries.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/literature-review.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef google_search(query: str, num_results: int = 2, max_chars: int = 500) -> list:  # type: ignore[type-arg]\\n    import os\\n    import time\\n\\n    import requests\\n    from bs4 import BeautifulSoup\\n    from dotenv import load_dotenv\\n\\n    load_dotenv()\\n\\n    api_key = os.getenv(\"GOOGLE_API_KEY\")\\n    search_engine_id = os.getenv(\"GOOGLE_SEARCH_ENGINE_ID\")\\n\\n    if not api_key or not search_engine_id:\\n        raise ValueError(\"API key or Search Engine ID not found in environment variables\")\\n\\n    url = \"https://www.googleapis.com/customsearch/v1\"\\n    params = {\"key\": api_key, \"cx\": search_engine_id, \"q\": query, \"num\": num_results}\\n\\n    response = requests.get(url, params=params)  # type: ignore[arg-type]\\n\\n    if response.status_code != 200:\\n        print(response.json())\\n        raise Exception(f\"Error in API request: {response.status_code}\")\\n\\n    results = response.json().get(\"items\", [])\\n\\n    def get_page_content(url: str) -> str:\\n        try:\\n            response = requests.get(url, timeout=10)\\n            soup = BeautifulSoup(response.content, \"html.parser\")\\n            text = soup.get_text(separator=\" \", strip=True)\\n            words = text.split()\\n            content = \"\"\\n            for word in words:\\n                if len(content) + len(word) + 1 > max_chars:\\n                    break\\n                content += \" \" + word\\n            return content.strip()\\n        except Exception as e:\\n            print(f\"Error fetching {url}: {str(e)}\")\\n            return \"\"\\n\\n    enriched_results = []\\n    for item in results:\\n        body = get_page_content(item[\"link\"])\\n        enriched_results.append(\\n            {\"title\": item[\"title\"], \"link\": item[\"link\"], \"snippet\": item[\"snippet\"], \"body\": body}\\n        )\\n        time.sleep(1)  # Be respectful to the servers\\n\\n    return enriched_results\\n\\ndef arxiv_search(query: str, max_results: int = 2) -> list:  # type: ignore[type-arg]\\n    \"\"\"\\n    Search Arxiv for papers and return the results including abstracts.\\n    \"\"\"\\n    import arxiv\\n\\n    client = arxiv.Client()\\n    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\\n\\n    results = []\\n    for paper in client.results(search):\\n        results.append(\\n            {\\n                \"title\": paper.title,\\n                \"authors\": [author.name for author in paper.authors],\\n                \"published\": paper.published.strftime(\"%Y-%m-%d\"),\\n                \"abstract\": paper.summary,\\n                \"pdf_url\": paper.pdf_url,\\n            }\\n        )\\n\\n    # # Write results to a file\\n    # with open('arxiv_search_results.json', 'w') as f:\\n    #     json.dump(results, f, indent=2)\\n\\n    return results\\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Tool-Using Agent (`ToolUseAgent`) in Python\nDESCRIPTION: Defines the `ToolUseAgent` class, inheriting from `autogen_core.RoutedAgent`. This agent is designed to handle user messages, interact with an LLM via a `ChatCompletionClient`, utilize a provided list of `ToolSchema`, and delegate the actual tool execution to a separate `ToolAgent` specified by `tool_agent_type`. The `handle_user_message` method orchestrates the interaction, using `tool_agent_caller_loop` to manage the conversation flow and tool calls.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/tool-use-with-intervention.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass ToolUseAgent(RoutedAgent):\n    \"\"\"An agent that uses tools to perform tasks. It executes the tools\n    by itself by sending the tool execution task to a ToolAgent.\"\"\"\n\n    def __init__(\n        self,\n        description: str,\n        system_messages: List[SystemMessage],\n        model_client: ChatCompletionClient,\n        tool_schema: List[ToolSchema],\n        tool_agent_type: AgentType,\n    ) -> None:\n        super().__init__(description)\n        self._model_client = model_client\n        self._system_messages = system_messages\n        self._tool_schema = tool_schema\n        self._tool_agent_id = AgentId(type=tool_agent_type, key=self.id.key)\n\n    @message_handler\n    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n        \"\"\"Handle a user message, execute the model and tools, and returns the response.\"\"\"\n        session: List[LLMMessage] = [UserMessage(content=message.content, source=\"User\")]\n        # Use the tool agent to execute the tools, and get the output messages.\n        output_messages = await tool_agent_caller_loop(\n            self,\n            tool_agent_id=self._tool_agent_id,\n            model_client=self._model_client,\n            input_messages=session,\n            tool_schema=self._tool_schema,\n            cancellation_token=ctx.cancellation_token,\n        )\n        # Extract the final response from the output messages.\n        final_response = output_messages[-1].content\n        assert isinstance(final_response, str)\n        return Message(content=final_response)\n```\n\n----------------------------------------\n\nTITLE: Registering and Running Agents with SingleThreadedAgentRuntime in Python\nDESCRIPTION: This snippet demonstrates how to register and run the Modifier and Checker agents using the SingleThreadedAgentRuntime. It creates a local embedded runtime, registers the agents with their respective factory functions, and starts the runtime to process messages.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/quickstart.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core import AgentId, SingleThreadedAgentRuntime\n\n# Create a local embedded runtime.\nruntime = SingleThreadedAgentRuntime()\n\n# Register the modifier and checker agents by providing\n# their agent types, the factory functions for creating instance and subscriptions.\nawait Modifier.register(\n    runtime,\n    \"modifier\",\n    # Modify the value by subtracting 1\n    lambda: Modifier(modify_val=lambda x: x - 1),\n)\n\nawait Checker.register(\n    runtime,\n    \"checker\",\n    # Run until the value is less than or equal to 1\n    lambda: Checker(run_until=lambda x: x <= 1),\n)\n\n# Start the runtime and send a direct message to the checker.\nruntime.start()\nawait runtime.send_message(Message(10), AgentId(\"checker\", \"default\"))\nawait runtime.stop_when_idle()\n```\n\n----------------------------------------\n\nTITLE: Adding Python Kernel to Dotnet Interactive Composite Kernel in C#\nDESCRIPTION: This C# code snippet demonstrates how to add a Python kernel to the dotnet-interactive composite kernel using the AddPythonKernel method.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Run-dotnet-code.md#2025-04-22_snippet_6\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/RunCodeSnippetCodeSnippet.cs?name=code_snippet_1_4)]\n```\n\n----------------------------------------\n\nTITLE: Implementing the OpenAI Assistant Agent Class in Python\nDESCRIPTION: Defines the `OpenAIAssistantAgent` class, inheriting from `autogen_core.RoutedAgent`, to interact with the OpenAI Assistant API. It uses an `AsyncClient` and manages communication within a specific OpenAI thread (`thread_id`) using a given `assistant_id`. Message handlers (`handle_message`, `on_reset`, `on_upload_for_code_interpreter`, `on_upload_for_file_search`) process the defined message protocol types, interact with the API (creating messages, running assistants, deleting messages, uploading files), and handle potential streaming output via an optional event handler factory. Dependencies include `asyncio`, `os`, `typing`, `aiofiles`, `autogen_core`, and `openai`.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/openai-assistant-agent.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport os\nfrom typing import Any, Callable, List\n\nimport aiofiles\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, message_handler\nfrom openai import AsyncAssistantEventHandler, AsyncClient\nfrom openai.types.beta.thread import ToolResources, ToolResourcesFileSearch\n\n\nclass OpenAIAssistantAgent(RoutedAgent):\n    \"\"\"An agent implementation that uses the OpenAI Assistant API to generate\n    responses.\n\n    Args:\n        description (str): The description of the agent.\n        client (openai.AsyncClient): The client to use for the OpenAI API.\n        assistant_id (str): The assistant ID to use for the OpenAI API.\n        thread_id (str): The thread ID to use for the OpenAI API.\n        assistant_event_handler_factory (Callable[[], AsyncAssistantEventHandler], optional):\n            A factory function to create an async assistant event handler. Defaults to None.\n            If provided, the agent will use the streaming mode with the event handler.\n            If not provided, the agent will use the blocking mode to generate responses.\n    \"\"\"\"\"\"\n\n    def __init__(\n        self,\n        description: str,\n        client: AsyncClient,\n        assistant_id: str,\n        thread_id: str,\n        assistant_event_handler_factory: Callable[[], AsyncAssistantEventHandler],\n    ) -> None:\n        super().__init__(description)\n        self._client = client\n        self._assistant_id = assistant_id\n        self._thread_id = thread_id\n        self._assistant_event_handler_factory = assistant_event_handler_factory\n\n    @message_handler\n    async def handle_message(self, message: TextMessage, ctx: MessageContext) -> TextMessage:\n        \"\"\"Handle a message. This method adds the message to the thread and publishes a response.\"\"\"\n        # Save the message to the thread.\n        await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(\n                self._client.beta.threads.messages.create(\n                    thread_id=self._thread_id,\n                    content=message.content,\n                    role=\"user\",\n                    metadata={\"sender\": message.source},\n                )\n            )\n        )\n        # Generate a response.\n        async with self._client.beta.threads.runs.stream(\n            thread_id=self._thread_id,\n            assistant_id=self._assistant_id,\n            event_handler=self._assistant_event_handler_factory(),\n        ) as stream:\n            await ctx.cancellation_token.link_future(asyncio.ensure_future(stream.until_done()))\n\n        # Get the last message.\n        messages = await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(self._client.beta.threads.messages.list(self._thread_id, order=\"desc\", limit=1))\n        )\n        last_message_content = messages.data[0].content\n\n        # Get the text content from the last message.\n        text_content = [content for content in last_message_content if content.type == \"text\"]\n        if not text_content:\n            raise ValueError(f\"Expected text content in the last message: {last_message_content}\")\n\n        return TextMessage(content=text_content[0].text.value, source=self.metadata[\"type\"])\n\n    @message_handler()\n    async def on_reset(self, message: Reset, ctx: MessageContext) -> None:\n        \"\"\"Handle a reset message. This method deletes all messages in the thread.\"\"\"\n        # Get all messages in this thread.\n        all_msgs: List[str] = []\n        while True:\n            if not all_msgs:\n                msgs = await ctx.cancellation_token.link_future(\n                    asyncio.ensure_future(self._client.beta.threads.messages.list(self._thread_id))\n                )\n            else:\n                msgs = await ctx.cancellation_token.link_future(\n                    asyncio.ensure_future(self._client.beta.threads.messages.list(self._thread_id, after=all_msgs[-1]))\n                )\n            for msg in msgs.data:\n                all_msgs.append(msg.id)\n            if not msgs.has_next_page():\n                break\n        # Delete all the messages.\n        for msg_id in all_msgs:\n            status = await ctx.cancellation_token.link_future(\n                asyncio.ensure_future(\n                    self._client.beta.threads.messages.delete(message_id=msg_id, thread_id=self._thread_id)\n                )\n            )\n            assert status.deleted is True\n\n    @message_handler()\n    async def on_upload_for_code_interpreter(self, message: UploadForCodeInterpreter, ctx: MessageContext) -> None:\n        \"\"\"Handle an upload for code interpreter. This method uploads a file and updates the thread with the file.\"\"\"\n        # Get the file content.\n        async with aiofiles.open(message.file_path, mode=\"rb\") as f:\n            file_content = await ctx.cancellation_token.link_future(asyncio.ensure_future(f.read()))\n        file_name = os.path.basename(message.file_path)\n        # Upload the file.\n        file = await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(self._client.files.create(file=(file_name, file_content), purpose=\"assistants\"))\n        )\n        # Get existing file ids from tool resources.\n        thread = await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(self._client.beta.threads.retrieve(thread_id=self._thread_id))\n        )\n        tool_resources: ToolResources = thread.tool_resources if thread.tool_resources else ToolResources()\n        assert tool_resources.code_interpreter is not None\n        if tool_resources.code_interpreter.file_ids:\n            file_ids = tool_resources.code_interpreter.file_ids\n        else:\n            file_ids = [file.id]\n        # Update thread with new file.\n        await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(\n                self._client.beta.threads.update(\n                    thread_id=self._thread_id,\n                    tool_resources={\n                        \"code_interpreter\": {\"file_ids\": file_ids},\n                    },\n                )\n            )\n        )\n\n    @message_handler()\n    async def on_upload_for_file_search(self, message: UploadForFileSearch, ctx: MessageContext) -> None:\n        \"\"\"Handle an upload for file search. This method uploads a file and updates the vector store.\"\"\"\n        # Get the file content.\n        async with aiofiles.open(message.file_path, mode=\"rb\") as file:\n            file_content = await ctx.cancellation_token.link_future(asyncio.ensure_future(file.read()))\n        file_name = os.path.basename(message.file_path)\n        # Upload the file.\n        await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(\n                self._client.vector_stores.file_batches.upload_and_poll(\n                    vector_store_id=message.vector_store_id,\n                    files=[(file_name, file_content)],\n                )\n            )\n        )\n```\n\n----------------------------------------\n\nTITLE: Registering FunctionCallMiddleware for Function Processing in C#\nDESCRIPTION: This code demonstrates how to register an agent with FunctionCallMiddleware to process and invoke function calls. It sets up the middleware with specific functions and configures how function calls should be handled.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Use-function-call.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\nvar agent = new AssistantAgent(\"assistant\", new List<AgentCapability> { AgentCapability.Functions });\n\nagent.AddMiddleware(new FunctionCallMiddleware(new FunctionCallMiddlewareOptions\n{\n    Functions = new[] { WeatherReport },\n    FunctionCallBehavior = FunctionCallBehavior.AutoInvokeOnce\n}));\n```\n\n----------------------------------------\n\nTITLE: Sending a Message to OpenAIChatAgent with Semantic Kernel Plugin in C#\nDESCRIPTION: This code snippet shows how to initiate a chat with the OpenAIChatAgent, utilizing the custom GetWeather function from the Semantic Kernel plugin to retrieve weather information for Seattle.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.SemanticKernel/Use-kernel-plugin-in-other-agents.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\nvar response = await agent.SendMessageAsync(\"What's the weather like in Seattle?\");\nConsole.WriteLine(response);\n```\n\n----------------------------------------\n\nTITLE: Wrapping Search and Stock Functions as Agent Tools in Python\nDESCRIPTION: This code demonstrates how to wrap the previously defined google_search and analyze_stock functions into FunctionTool objects for agent consumption. This abstraction allows the agents to call specific tool APIs by name, description, and functionality, and exposes standardized input/output interfaces. Dependencies include the FunctionTool class from autogen_core.tools, and the constructed tool objects are essential for agent configuration.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/company-research.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ngoogle_search_tool = FunctionTool(\\n    google_search, description=\"Search Google for information, returns results with a snippet and body content\"\\n)\\nstock_analysis_tool = FunctionTool(analyze_stock, description=\"Analyze stock data and generate a plot\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Assistant Agents with Tools and Descriptions in Python\nDESCRIPTION: This code initializes three agent instances using AssistantAgent from the autogen_agentchat library: a search agent equipped with the Google search tool, a stock analysis agent with the stock tool, and a report agent. Each agent receives a task-specific description and system message. The model client is established using the OpenAI gpt-4o model. This setup allows sequential agentic processing with clearly separated responsibilities.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/company-research.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\\n\\nsearch_agent = AssistantAgent(\\n    name=\"Google_Search_Agent\",\\n    model_client=model_client,\\n    tools=[google_search_tool],\\n    description=\"Search Google for information, returns top 2 results with a snippet and body content\",\\n    system_message=\"You are a helpful AI assistant. Solve tasks using your tools.\",\\n)\\n\\nstock_analysis_agent = AssistantAgent(\\n    name=\"Stock_Analysis_Agent\",\\n    model_client=model_client,\\n    tools=[stock_analysis_tool],\\n    description=\"Analyze stock data and generate a plot\",\\n    system_message=\"Perform data analysis.\",\\n)\\n\\nreport_agent = AssistantAgent(\\n    name=\"Report_Agent\",\\n    model_client=model_client,\\n    description=\"Generate a report based the search and results of stock analysis\",\\n    system_message=\"You are a helpful assistant that can generate a comprehensive report on a given topic based on search and stock analysis. When you done with generating the report, reply with TERMINATE.\",\\n)\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Completion with MistralClientAgent in C#\nDESCRIPTION: Demonstrates how to use the GenerateStreamingReplyAsync method to stream chat completions from a MistralClientAgent.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen-Mistral-Overview.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\nstreaming_chat\n```\n\n----------------------------------------\n\nTITLE: OpenAIChatAgent Initialization\nDESCRIPTION: Creates and configures an OpenAIChatAgent instance with custom HTTP transport for connecting to the Ollama server\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-connect-to-third-party-api.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\nvar transport = new HttpClientTransport(new CustomHttpClientHandler());\nvar agent = new OpenAIChatAgent(\n    modelId: \"llama2\",\n    apiKey: \"empty\",  // Since we are using Ollama, we don't need a real API key\n    transport: transport\n);\n```\n\n----------------------------------------\n\nTITLE: Executing Market Research Swarm for TSLA Stock Analysis in Python\nDESCRIPTION: Initializes and runs a swarm of AI agents to conduct market research on TSLA stock. The swarm includes a termination condition and uses a console for output.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/swarm.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Define termination condition\ntext_termination = TextMentionTermination(\"TERMINATE\")\ntermination = text_termination\n\nresearch_team = Swarm(\n    participants=[planner, financial_analyst, news_analyst, writer], termination_condition=termination\n)\n\ntask = \"Conduct market research for TSLA stock\"\nawait Console(research_team.run_stream(task=task))\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Dumping and Loading Gemini Agent Configuration in Python\nDESCRIPTION: This code snippet demonstrates how to dump the configuration of a `GeminiAssistantAgent` to a JSON format and then load the configuration back into a new agent instance. It utilizes the `dump_component` and `load_component` methods for serialization and deserialization.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/custom-agents.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ngemini_assistant = GeminiAssistantAgent(\"gemini_assistant\")\nconfig = gemini_assistant.dump_component()\nprint(config.model_dump_json(indent=2))\nloaded_agent = GeminiAssistantAgent.load_component(config)\nprint(loaded_agent)\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a Round Robin Agent Team in Python\nDESCRIPTION: This snippet creates a RoundRobinGroupChat with the stock analysis, search, and report agents, setting a turn limit equal to the number of agents to ensure sequential execution. It then runs the team's workflow interactively via Console, prompting them to write a financial report on a target company ('American airlines'), and shuts down the model client. Inputs are agent objects and a task string; outputs are the managed agent interaction and an awaited report generation. Requires the Console UI and OpenAI chat client.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/company-research.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nteam = RoundRobinGroupChat([stock_analysis_agent, search_agent, report_agent], max_turns=3)\n```\n\nLANGUAGE: python\nCODE:\n```\nstream = team.run_stream(task=\"Write a financial report on American airlines\")\\nawait Console(stream)\\n\\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Interacting with JSON-enabled OpenAIChatAgent in C#\nDESCRIPTION: This code demonstrates how to interact with the OpenAIChatAgent that has JSON mode enabled. It sends a message to the agent and receives a JSON response.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-use-json-mode.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nvar response = await openAIClientAgent.GenerateReplyAsync(\"My name is John and I'm 25 years old.\");\nvar person = JsonSerializer.Deserialize<Person>(response);\n\nConsole.WriteLine($\"Name: {person.Name}\");\nConsole.WriteLine($\"Age: {person.Age}\");\nConsole.WriteLine(\"Done\");\n```\n\n----------------------------------------\n\nTITLE: Running Round Robin Team with Termination in Python\nDESCRIPTION: Illustrates running a RoundRobinGroupChat with a MaxMessageTermination condition, which stops after a specified number of messages. The task 'Write a unique, Haiku about the weather in Paris' is given to the agent team. This requires the 'autogen_agentchat' library and a setup for running asyncio tasks.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/termination.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmax_msg_termination = MaxMessageTermination(max_messages=3)\nround_robin_team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=max_msg_termination)\n\n# Use asyncio.run(...) if you are running this script as a standalone script.\nawait Console(round_robin_team.run_stream(task=\"Write a unique, Haiku about the weather in Paris\"))\n```\n\n----------------------------------------\n\nTITLE: Initializing Azure OpenAI Chat Client with AAD Authentication in Python\nDESCRIPTION: This Python snippet initializes the AzureOpenAIChatCompletionClient using Azure Active Directory for authentication. It makes use of the azure-identity package to obtain a bearer token and leverages a custom endpoint for the OpenAI service. The code requires setting specific parameters such as azure_deployment, model, and azure_endpoint tailored to your Azure setup.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/azure-openai-with-aad-auth.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n# Create the token provider\ntoken_provider = get_bearer_token_provider(\n    DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"\n)\n\nclient = AzureOpenAIChatCompletionClient(\n    azure_deployment=\"{your-azure-deployment}\",\n    model=\"{model-name, such as gpt-4o}\",\n    api_version=\"2024-02-01\",\n    azure_endpoint=\"https://{your-custom-endpoint}.openai.azure.com/\",\n    azure_ad_token_provider=token_provider,\n)\n```\n\n----------------------------------------\n\nTITLE: Continuing Conversations After Termination in Python\nDESCRIPTION: Shows how conversations can be resumed from where they left off after a termination condition is met. This snippet continues the dialogue, allowing agents to respond further. It requires the async 'Console' UI from 'autogen_agentchat'.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/termination.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Use asyncio.run(...) if you are running this script as a standalone script.\nawait Console(round_robin_team.run_stream())\n```\n\n----------------------------------------\n\nTITLE: Implementing multi-turn tool call interaction in C#\nDESCRIPTION: Example of sending tool call results back to the LLM for further response generation, demonstrating a multi-turn interaction.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Create-agent-with-tools.md#2025-04-22_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\nvar response = await agentWithTools.SendAsync(new UserMessage(\"What's the weather in Seattle?\"));\nConsole.WriteLine(response);\n\nresponse = await agentWithTools.SendAsync(new UserMessage(\"Based on the weather, what activities do you recommend?\"));\nConsole.WriteLine(response);\n```\n\n----------------------------------------\n\nTITLE: Creating and Chatting with OpenAIChatAgent in C#\nDESCRIPTION: This code demonstrates how to create an OpenAIChatAgent instance and use it for a chat interaction. It includes setting up the agent with a model and API key, then sending a message and receiving a response.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-simple-chat.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nvar agent = new OpenAIChatAgent(\"gpt-3.5-turbo\", \"your-api-key-here\");\nvar reply = await agent.GenerateReplyAsync(\"Hello, how are you?\");\nConsole.WriteLine($\"Agent: {reply}\");\n```\n\n----------------------------------------\n\nTITLE: Defining Message Protocol for Multi-Agent Debate\nDESCRIPTION: Defines dataclasses for different types of messages used in the multi-agent debate, including Question, Answer, SolverRequest, IntermediateSolverResponse, and FinalSolverResponse.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/multi-agent-debate.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass Question:\n    content: str\n\n\n@dataclass\nclass Answer:\n    content: str\n\n\n@dataclass\nclass SolverRequest:\n    content: str\n    question: str\n\n\n@dataclass\nclass IntermediateSolverResponse:\n    content: str\n    question: str\n    answer: str\n    round: int\n\n\n@dataclass\nclass FinalSolverResponse:\n    answer: str\n```\n\n----------------------------------------\n\nTITLE: Initializing Runtime with Tracer Provider\nDESCRIPTION: Code example showing how to initialize both single-threaded and worker runtimes with a configured tracer provider\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/telemetry.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# for single threaded runtime\nsingle_threaded_runtime = SingleThreadedAgentRuntime(tracer_provider=tracer_provider)\n# or for worker runtime\nworker_runtime = GrpcWorkerAgentRuntime(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring OllamaAgent for LLaMA Model\nDESCRIPTION: Code to instantiate and configure an OllamaAgent that connects to a local Ollama server, specifically targeting the LLaMA model. The agent is configured with custom parameters and a logger for monitoring interactions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Ollama/Chat-with-llama.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\n// Create an ollama agent to connect to the local LLaMA model\nvar ollamaAgent = new OllamaAgent(\n    \"llama3\",\n    baseUrl: \"http://localhost:11434\",\n    agentName: \"Assistant\",\n    ollamaParameters: new()\n    {\n        Temperature = 0.1f,\n        NumPredict = 100,\n    },\n    logger: loggerFactory.CreateLogger<OllamaAgent>());\n```\n\n----------------------------------------\n\nTITLE: Creating an OpenAI Chat Completion Client\nDESCRIPTION: Initializes an OpenAI chat completion client using the GPT-4o model. The API key can be provided directly or through an environment variable.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nopenai_model_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o-2024-08-06\",\n    # api_key=\"sk-...\", # Optional if you have an OPENAI_API_KEY environment variable set.\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing RoundRobinGroupChat Team and Saving Team State - autogen_agentchat - Python\nDESCRIPTION: This snippet creates a RoundRobinGroupChat team with an assistant agent, executes a streaming task, and saves the complete team state. The team includes a termination condition and messaging interface to Console. Inputs include the user's request (task='Write a beautiful poem 3-line about lake tangayika') and dependencies on all relevant autogen_agentchat classes. Outputs are streamed via Console and the state dictionary for the team, which includes nested agent states.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/state.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-2024-08-06\")\n\n# Define a team.\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    system_message=\"You are a helpful assistant\",\n    model_client=model_client,\n)\nagent_team = RoundRobinGroupChat([assistant_agent], termination_condition=MaxMessageTermination(max_messages=2))\n\n# Run the team and stream messages to the console.\nstream = agent_team.run_stream(task=\"Write a beautiful poem 3-line about lake tangayika\")\n\n# Use asyncio.run(...) when running in a script.\nawait Console(stream)\n\n# Save the state of the agent team.\nteam_state = await agent_team.save_state()\n```\n\n----------------------------------------\n\nTITLE: Registering Math Solver Agents in Runtime\nDESCRIPTION: Sets up the runtime environment and registers multiple math solver agents with specific configurations. Each solver is configured with a GPT model client and debate parameters.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/multi-agent-debate.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nruntime = SingleThreadedAgentRuntime()\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n\nawait MathSolver.register(\n    runtime,\n    \"MathSolverA\",\n    lambda: MathSolver(\n        model_client=model_client,\n        topic_type=\"MathSolverA\",\n        num_neighbors=2,\n        max_round=3,\n    ),\n)\nawait MathSolver.register(\n    runtime,\n    \"MathSolverB\",\n    lambda: MathSolver(\n        model_client=model_client,\n        topic_type=\"MathSolverB\",\n        num_neighbors=2,\n        max_round=3,\n    ),\n)\nawait MathSolver.register(\n    runtime,\n    \"MathSolverC\",\n    lambda: MathSolver(\n        model_client=model_client,\n        topic_type=\"MathSolverC\",\n        num_neighbors=2,\n        max_round=3,\n    ),\n)\nawait MathSolver.register(\n    runtime,\n    \"MathSolverD\",\n    lambda: MathSolver(\n        model_client=model_client,\n        topic_type=\"MathSolverD\",\n        num_neighbors=2,\n        max_round=3,\n    ),\n)\nawait MathAggregator.register(runtime, \"MathAggregator\", lambda: MathAggregator(num_solvers=4))\n```\n\n----------------------------------------\n\nTITLE: Retrieving JSON Schema from an AutoGen FunctionTool (Python)\nDESCRIPTION: This snippet shows how to access the automatically generated JSON schema for a previously defined `FunctionTool` instance (`stock_price_tool`). Accessing the `.schema` attribute returns the schema definition that model clients use to understand the tool's parameters and structure for generating appropriate function calls. It assumes `stock_price_tool` has been defined as in the previous example.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/tools.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstock_price_tool.schema\n```\n\n----------------------------------------\n\nTITLE: Persisting and Restoring Team State Using JSON Files - autogen_agentchat - Python\nDESCRIPTION: Explains how to serialize (save) and deserialize (load) the team state using Python's built-in json module and recover the agent team using the loaded state. Prerequisites include a valid team_state and previously instantiated agent and team objects. Inputs include the file path, saved state, and team instantiation code. Outputs are the reconstituted team ready to be queried for restored conversational context.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/state.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\n## save state to disk\n\nwith open(\"coding/team_state.json\", \"w\") as f:\n    json.dump(team_state, f)\n\n## load state from disk\nwith open(\"coding/team_state.json\", \"r\") as f:\n    team_state = json.load(f)\n\nnew_agent_team = RoundRobinGroupChat([assistant_agent], termination_condition=MaxMessageTermination(max_messages=2))\nawait new_agent_team.load_state(team_state)\nstream = new_agent_team.run_stream(task=\"What was the last line of the poem you wrote?\")\nawait Console(stream)\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Running the Swarm Team for Task Execution in Python\nDESCRIPTION: Execute the Swarm team with a given task, processing handoffs and user interactions asynchronously. The code runs in an event loop and awaits user input as necessary.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/swarm.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ntask = \"I need to refund my flight.\"\n\nasync def run_team_stream() -> None:\n    task_result = await Console(team.run_stream(task=task))\n    last_message = task_result.messages[-1]\n\n    while isinstance(last_message, HandoffMessage) and last_message.target == \"user\":\n        user_message = input(\"User: \")\n\n        task_result = await Console(\n            team.run_stream(task=HandoffMessage(source=\"user\", target=last_message.source, content=user_message))\n        )\n        last_message = task_result.messages[-1]\n\n\n# Use asyncio.run(...) if you are running this in a script.\nawait run_team_stream()\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Implementing ReviewerAgent for Code Review Tasks\nDESCRIPTION: Defines ReviewerAgent class that handles code review tasks using JSON-structured output and chain-of-thought prompting. Reviews code for correctness, efficiency and safety, maintaining review history.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/reflection.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@default_subscription\nclass ReviewerAgent(RoutedAgent):\n    \"\"\"An agent that performs code review tasks.\"\"\"\n\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"A code reviewer agent.\")\n        self._system_messages: List[LLMMessage] = [\n            SystemMessage(\n                content=\"\"\"You are a code reviewer. You focus on correctness, efficiency and safety of the code.\nRespond using the following JSON format:\n{\n    \"correctness\": \"<Your comments>\",\n    \"efficiency\": \"<Your comments>\",\n    \"safety\": \"<Your comments>\",\n    \"approval\": \"<APPROVE or REVISE>\",\n    \"suggested_changes\": \"<Your comments>\"\n}\n\"\"\",\n            )\n        ]\n        self._session_memory: Dict[str, List[CodeReviewTask | CodeReviewResult]] = {}\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_code_review_task(self, message: CodeReviewTask, ctx: MessageContext) -> None:\n        # Format the prompt for the code review.\n        # Gather the previous feedback if available.\n        previous_feedback = \"\"\n        if message.session_id in self._session_memory:\n            previous_review = next(\n                (m for m in reversed(self._session_memory[message.session_id]) if isinstance(m, CodeReviewResult)),\n                None,\n            )\n            if previous_review is not None:\n                previous_feedback = previous_review.review\n        # Store the messages in a temporary memory for this request only.\n        self._session_memory.setdefault(message.session_id, []).append(message)\n        prompt = f\"\"\"The problem statement is: {message.code_writing_task}\nThe code is:\n```\n{message.code}\n```\n\nPrevious feedback:\n{previous_feedback}\n\nPlease review the code. If previous feedback was provided, see if it was addressed.\n\"\"\"\n        # Generate a response using the chat completion API.\n        response = await self._model_client.create(\n            self._system_messages + [UserMessage(content=prompt, source=self.metadata[\"type\"])],\n            cancellation_token=ctx.cancellation_token,\n            json_output=True,\n        )\n        assert isinstance(response.content, str)\n        # TODO: use structured generation library e.g. guidance to ensure the response is in the expected format.\n        # Parse the response JSON.\n        review = json.loads(response.content)\n        # Construct the review text.\n        review_text = \"Code review:\\n\" + \"\\n\".join([f\"{k}: {v}\" for k, v in review.items()])\n        approved = review[\"approval\"].lower().strip() == \"approve\"\n        result = CodeReviewResult(\n            review=review_text,\n            session_id=message.session_id,\n            approved=approved,\n        )\n        # Store the review result in the session memory.\n        self._session_memory[message.session_id].append(result)\n        # Publish the review result.\n        await self.publish_message(result, topic_id=TopicId(\"default\", self.id.key))\n```\n\n----------------------------------------\n\nTITLE: Creating FunctionCallMiddleware in C#\nDESCRIPTION: This snippet shows how to create a FunctionCallMiddleware with the 'GetWeather' function and register it with the OpenAI chat agent. It also demonstrates passing a functionMap to automatically invoke the function when the agent replies with a 'GetWeather' function call.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-use-function-call.md#2025-04-22_snippet_4\n\nLANGUAGE: C#\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/OpenAICodeSnippet.cs?name=create_function_call_middleware)]\n```\n\n----------------------------------------\n\nTITLE: Adding Required Namespaces for Gemini Function Calls\nDESCRIPTION: Namespace imports required for working with Gemini agents and function calls.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Function-call-with-gemini.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nusing AutoGen.Core;\nusing AutoGen.Gemini;\nusing System.Diagnostics;\nusing System.Text.Json;\nusing System.Text.Json.Serialization;\n```\n\n----------------------------------------\n\nTITLE: Implementing Worker Agent Class\nDESCRIPTION: Implementation of the WorkerAgent class that processes tasks and synthesizes results from previous layers using a chat completion model.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/mixture-of-agents.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass WorkerAgent(RoutedAgent):\n    def __init__(\n        self,\n        model_client: ChatCompletionClient,\n    ) -> None:\n        super().__init__(description=\"Worker Agent\")\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_task(self, message: WorkerTask, ctx: MessageContext) -> WorkerTaskResult:\n        if message.previous_results:\n            # If previous results are provided, we need to synthesize them to create a single prompt.\n            system_prompt = \"You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\\n\\nResponses from models:\"\n            system_prompt += \"\\n\" + \"\\n\\n\".join([f\"{i+1}. {r}\" for i, r in enumerate(message.previous_results)])\n            model_result = await self._model_client.create(\n                [SystemMessage(content=system_prompt), UserMessage(content=message.task, source=\"user\")]\n            )\n        else:\n            # If no previous results are provided, we can simply pass the user query to the model.\n            model_result = await self._model_client.create([UserMessage(content=message.task, source=\"user\")])\n        assert isinstance(model_result.content, str)\n        print(f\"{'-'*80}\\nWorker-{self.id}:\\n{model_result.content}\")\n        return WorkerTaskResult(result=model_result.content)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom FunctionCallTermination Condition in Python\nDESCRIPTION: This code defines a custom termination condition that stops the conversation when a specific function call is made. It includes a configuration class and the main termination condition class with methods for checking termination, resetting, and handling serialization.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/termination.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Sequence\n\nfrom autogen_agentchat.base import TerminatedException, TerminationCondition\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, StopMessage, ToolCallExecutionEvent\nfrom autogen_core import Component\nfrom pydantic import BaseModel\nfrom typing_extensions import Self\n\n\nclass FunctionCallTerminationConfig(BaseModel):\n    \"\"\"Configuration for the termination condition to allow for serialization\n    and deserialization of the component.\n    \"\"\"\n\n    function_name: str\n\n\nclass FunctionCallTermination(TerminationCondition, Component[FunctionCallTerminationConfig]):\n    \"\"\"Terminate the conversation if a FunctionExecutionResult with a specific name is received.\"\"\"\n\n    component_config_schema = FunctionCallTerminationConfig\n    component_provider_override = \"autogen_agentchat.conditions.FunctionCallTermination\"\n    \"\"\"The schema for the component configuration.\"\"\"\n\n    def __init__(self, function_name: str) -> None:\n        self._terminated = False\n        self._function_name = function_name\n\n    @property\n    def terminated(self) -> bool:\n        return self._terminated\n\n    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:\n        if self._terminated:\n            raise TerminatedException(\"Termination condition has already been reached\")\n        for message in messages:\n            if isinstance(message, ToolCallExecutionEvent):\n                for execution in message.content:\n                    if execution.name == self._function_name:\n                        self._terminated = True\n                        return StopMessage(\n                            content=f\"Function '{self._function_name}' was executed.\",\n                            source=\"FunctionCallTermination\",\n                        )\n        return None\n\n    async def reset(self) -> None:\n        self._terminated = False\n\n    def _to_config(self) -> FunctionCallTerminationConfig:\n        return FunctionCallTerminationConfig(\n            function_name=self._function_name,\n        )\n\n    @classmethod\n    def _from_config(cls, config: FunctionCallTerminationConfig) -> Self:\n        return cls(\n            function_name=config.function_name,\n        )\n```\n\n----------------------------------------\n\nTITLE: Creating FunctionCallMiddleware without auto-invoke in C#\nDESCRIPTION: Example of creating a FunctionCallMiddleware without auto-invoke functionality, which allows manual control over tool invocation.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Create-agent-with-tools.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\nvar functionContracts = new[] { FunctionContract.FromType<Tool>() };\nvar middleware = new FunctionCallMiddleware(functionContracts);\n```\n\n----------------------------------------\n\nTITLE: Defining Movie Database Function Contract\nDESCRIPTION: Implementation of a MovieFunction class that provides function contracts for searching movies by director and retrieving movie reviews. The class includes input/output models and the methods that handle the function logic.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Function-call-with-gemini.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\n// Define a class for function definition and implementation\npublic class MovieFunction\n{\n    // Define a model class for FindMovieByDirector\n    [FunctionDefinition(\"Find movies by a director\")]\n    public class FindMovieByDirectorRequest\n    {\n        [JsonPropertyName(\"director\")]\n        [Description(\"name of director\")]\n        public string Director { get; set; } = string.Empty;\n    }\n\n    [FunctionDefinition(\"Get movie review\")]\n    public class GetMovieReviewRequest\n    {\n        [JsonPropertyName(\"movie\")]\n        [Description(\"name of movie\")]\n        public string Movie { get; set; } = string.Empty;\n    }\n\n    // Define a data model for movie\n    public class Movie\n    {\n        [JsonPropertyName(\"title\")]\n        public string Title { get; set; } = string.Empty;\n\n        [JsonPropertyName(\"year\")]\n        public int Year { get; set; }\n    }\n\n    // Define a data model for movie review\n    public class MovieReview\n    {\n        [JsonPropertyName(\"review\")]\n        public string Review { get; set; } = string.Empty;\n\n        [JsonPropertyName(\"rating\")]\n        public double Rating { get; set; }\n    }\n\n    [Function(nameof(FindMovieByDirector))]\n    public Movie[] FindMovieByDirector(FindMovieByDirectorRequest request)\n    {\n        var directorName = request.Director.Trim().ToLowerInvariant();\n        return directorName switch\n        {\n            \"christopher nolan\" => new Movie[]\n            {\n                new Movie { Title = \"Oppenheimer\", Year = 2023 },\n                new Movie { Title = \"Tenet\", Year = 2020 },\n                new Movie { Title = \"Dunkirk\", Year = 2017 },\n                new Movie { Title = \"Interstellar\", Year = 2014 },\n            },\n            \"greta gerwig\" => new Movie[]\n            {\n                new Movie { Title = \"Barbie\", Year = 2023 },\n                new Movie { Title = \"Little Women\", Year = 2019 },\n                new Movie { Title = \"Lady Bird\", Year = 2017 },\n            },\n            \"denis villeneuve\" => new Movie[]\n            {\n                new Movie { Title = \"Dune: Part Two\", Year = 2024 },\n                new Movie { Title = \"Dune\", Year = 2021 },\n                new Movie { Title = \"Blade Runner 2049\", Year = 2017 },\n                new Movie { Title = \"Arrival\", Year = 2016 },\n            },\n            _ => Array.Empty<Movie>()\n        };\n    }\n\n    [Function(nameof(GetMovieReview))]\n    public MovieReview GetMovieReview(GetMovieReviewRequest request)\n    {\n        var movieReviewMapping = new Dictionary<string, MovieReview>\n        {\n            {\n                \"oppenheimer\",\n                new MovieReview\n                {\n                    Review = \"Oppenheimer is a biographical thriller film based on the life of J. Robert Oppenheimer, the theoretical physicist who helped develop the first nuclear weapons. It's a visually stunning and impeccably crafted film with outstanding performances.\",\n                    Rating = 9.2\n                }\n            },\n            {\n                \"barbie\",\n                new MovieReview\n                {\n                    Review = \"Barbie is a fantasy comedy film based on the popular fashion doll. It's a funny, heartwarming, and surprisingly insightful film that explores themes of identity, feminism, and self-discovery.\",\n                    Rating = 8.5\n                }\n            },\n            {\n                \"dune: part two\",\n                new MovieReview\n                {\n                    Review = \"Dune: Part Two continues the adaptation of Frank Herbert's sci-fi epic with spectacular visuals, intense performances, and a captivating storyline. Director Denis Villeneuve delivers a worthy conclusion to the first part.\",\n                    Rating = 9.0\n                }\n            },\n        };\n\n        var movieName = request.Movie.Trim().ToLowerInvariant();\n        if (movieReviewMapping.TryGetValue(movieName, out var review))\n        {\n            return review;\n        }\n\n        return new MovieReview\n        {\n            Review = \"No review available for this movie.\",\n            Rating = 0\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Structured Logging in Python\nDESCRIPTION: Configuration code to enable structured logging with INFO level output using Python's logging module. Sets up a basic stream handler for console output.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/logging.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nfrom autogen_core import EVENT_LOGGER_NAME\n\nlogging.basicConfig(level=logging.WARNING)\nlogger = logging.getLogger(EVENT_LOGGER_NAME)\nlogger.addHandler(logging.StreamHandler())\nlogger.setLevel(logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Initializing the AutoGen Runtime with Intervention Handler in Python\nDESCRIPTION: Creates an instance of `autogen_core.SingleThreadedAgentRuntime`. Crucially, it registers the custom `ToolInterventionHandler` by passing it in a list to the `intervention_handlers` parameter during initialization. This setup ensures that the handler will intercept relevant agent messages as defined in its `on_send` method.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/tool-use-with-intervention.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create the runtime with the intervention handler.\nruntime = SingleThreadedAgentRuntime(intervention_handlers=[ToolInterventionHandler()])\n```\n\n----------------------------------------\n\nTITLE: Web Browsing Agent Creation with Playwright MCP\nDESCRIPTION: This snippet demonstrates how to create a web browsing agent using the McpWorkbench and the Playwright MCP server. It configures the agent to use the specified server URL and sends a message to the agent to perform a web search.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/workbench.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core import AgentId, SingleThreadedAgentRuntime\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import McpWorkbench, SseServerParams\n\nplaywright_server_params = SseServerParams(\n    url=\"http://localhost:8931/sse\",\n)\n\n# Start the workbench in a context manager.\n# You can also start and stop the workbench using `workbench.start()` and `workbench.stop()`.\nasync with McpWorkbench(playwright_server_params) as workbench:  # type: ignore\n    # Create a single-threaded agent runtime.\n    runtime = SingleThreadedAgentRuntime()\n\n    # Register the agent with the runtime.\n    await WorkbenchAgent.register(\n        runtime=runtime,\n        type=\"WebAgent\",\n        factory=lambda: WorkbenchAgent(\n            model_client=OpenAIChatCompletionClient(model=\"gpt-4.1-nano\"),\n            model_context=BufferedChatCompletionContext(buffer_size=10),\n            workbench=workbench,\n        ),\n    )\n\n    # Start the runtime.\n    runtime.start()\n\n    # Send a message to the agent.\n    await runtime.send_message(\n        Message(content=\"Use Bing to find out the address of Microsoft Building 99\"),\n        recipient=AgentId(\"WebAgent\", \"default\"),\n    )\n\n    # Stop the runtime.\n    await runtime.stop()\n```\n\n----------------------------------------\n\nTITLE: Registering Issues and Repairs Agent with Topic Subscription in AutoGen\nDESCRIPTION: Registers an issues and repairs agent with a specific agent topic type and system message for customer support. The agent has tools for executing refunds and looking up items, as well as a tool for transferring back to triage. The code also sets up a subscription for the agent to receive messages published to its own topic.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/handoffs.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Register the issues and repairs agent.\nissues_and_repairs_agent_type = await AIAgent.register(\n    runtime,\n    type=issues_and_repairs_agent_topic_type,  # Using the topic type as the agent type.\n    factory=lambda: AIAgent(\n        description=\"An issues and repairs agent.\",\n        system_message=SystemMessage(\n            content=\"You are a customer support agent for ACME Inc.\"\n            \"Always answer in a sentence or less.\"\n            \"Follow the following routine with the user:\"\n            \"1. First, ask probing questions and understand the user's problem deeper.\\n\"\n            \" - unless the user has already provided a reason.\\n\"\n            \"2. Propose a fix (make one up).\\n\"\n            \"3. ONLY if not satisfied, offer a refund.\\n\"\n            \"4. If accepted, search for the ID and then execute refund.\"\n        ),\n        model_client=model_client,\n        tools=[\n            execute_refund_tool,\n            look_up_item_tool,\n        ],\n        delegate_tools=[transfer_back_to_triage_tool],\n        agent_topic_type=issues_and_repairs_agent_topic_type,\n        user_topic_type=user_topic_type,\n    ),\n)\n# Add subscriptions for the issues and repairs agent: it will receive messages published to its own topic only.\nawait runtime.add_subscription(\n    TypeSubscription(topic_type=issues_and_repairs_agent_topic_type, agent_type=issues_and_repairs_agent_type.type)\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Weather Plugin for Semantic Kernel in C#\nDESCRIPTION: This code defines a simple plugin with a GetWeather function that takes a location as input and returns weather information. It demonstrates how to create a custom Semantic Kernel plugin.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.SemanticKernel/Use-kernel-plugin-in-other-agents.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nvar plugin = KernelPluginFactory.CreateFromFunctions(\"WeatherPlugin\", new()\n{\n    (\"GetWeather\", (string location) =>\n    {\n        return $\"The weather in {location} is sunny and 72 degrees.\";\n    })\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Classes and Message Structure in Python\nDESCRIPTION: This snippet defines two agent classes (Modifier and Checker) and a Message data class. The Modifier agent modifies a given number, while the Checker agent checks the value against a condition. The Message class is used for inter-agent communication.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/quickstart.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\nfrom typing import Callable\n\nfrom autogen_core import DefaultTopicId, MessageContext, RoutedAgent, default_subscription, message_handler\n\n\n@dataclass\nclass Message:\n    content: int\n\n\n@default_subscription\nclass Modifier(RoutedAgent):\n    def __init__(self, modify_val: Callable[[int], int]) -> None:\n        super().__init__(\"A modifier agent.\")\n        self._modify_val = modify_val\n\n    @message_handler\n    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n        val = self._modify_val(message.content)\n        print(f\"{'-'*80}\\nModifier:\\nModified {message.content} to {val}\")\n        await self.publish_message(Message(content=val), DefaultTopicId())  # type: ignore\n\n\n@default_subscription\nclass Checker(RoutedAgent):\n    def __init__(self, run_until: Callable[[int], bool]) -> None:\n        super().__init__(\"A checker agent.\")\n        self._run_until = run_until\n\n    @message_handler\n    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n        if not self._run_until(message.content):\n            print(f\"{'-'*80}\\nChecker:\\n{message.content} passed the check, continue.\")\n            await self.publish_message(Message(content=message.content), DefaultTopicId())\n        else:\n            print(f\"{'-'*80}\\nChecker:\\n{message.content} failed the check, stopping.\")\n```\n\n----------------------------------------\n\nTITLE: Running Arithmetic Agents in a SelectorGroupChat in Python\nDESCRIPTION: This code snippet demonstrates how to create and configure a SelectorGroupChat with multiple instances of the ArithmeticAgent, each performing a different arithmetic operation. The agents are then used to transform an input integer into a target value, with the SelectorGroupChat managing the selection of the appropriate agent for each step. It includes settings for termination conditions and custom selector prompts.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/custom-agents.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nasync def run_number_agents() -> None:\n    # Create agents for number operations.\n    add_agent = ArithmeticAgent(\"add_agent\", \"Adds 1 to the number.\", lambda x: x + 1)\n    multiply_agent = ArithmeticAgent(\"multiply_agent\", \"Multiplies the number by 2.\", lambda x: x * 2)\n    subtract_agent = ArithmeticAgent(\"subtract_agent\", \"Subtracts 1 from the number.\", lambda x: x - 1)\n    divide_agent = ArithmeticAgent(\"divide_agent\", \"Divides the number by 2 and rounds down.\", lambda x: x // 2)\n    identity_agent = ArithmeticAgent(\"identity_agent\", \"Returns the number as is.\", lambda x: x)\n\n    # The termination condition is to stop after 10 messages.\n    termination_condition = MaxMessageTermination(10)\n\n    # Create a selector group chat.\n    selector_group_chat = SelectorGroupChat(\n        [add_agent, multiply_agent, subtract_agent, divide_agent, identity_agent],\n        model_client=OpenAIChatCompletionClient(model=\"gpt-4o\"),\n        termination_condition=termination_condition,\n        allow_repeated_speaker=True,  # Allow the same agent to speak multiple times, necessary for this task.\n        selector_prompt=(\n            \"Available roles:\\n{roles}\\nTheir job descriptions:\\n{participants}\\n\"\n            \"Current conversation history:\\n{history}\\n\"\n            \"Please select the most appropriate role for the next message, and only return the role name.\"\n        ),\n    )\n\n    # Run the selector group chat with a given task and stream the response.\n    task: List[BaseChatMessage] = [\n        TextMessage(content=\"Apply the operations to turn the given number into 25.\", source=\"user\"),\n        TextMessage(content=\"10\", source=\"user\"),\n    ]\n    stream = selector_group_chat.run_stream(task=task)\n    await Console(stream)\n\n\n# Use asyncio.run(run_number_agents()) when running in a script.\nawait run_number_agents()\n```\n\n----------------------------------------\n\nTITLE: Defining Message Protocol Class\nDESCRIPTION: Simple dataclass definition for the message protocol used by agents to relay work through the workflow.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/sequential-workflow.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass Message:\n    content: str\n```\n\n----------------------------------------\n\nTITLE: Configuring Solver Agent Communication Topology\nDESCRIPTION: Establishes the communication network between solver agents using TypeSubscription. Creates a square topology where each solver is connected to two neighbors.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/multi-agent-debate.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Subscriptions for topic published to by MathSolverA.\nawait runtime.add_subscription(TypeSubscription(\"MathSolverA\", \"MathSolverD\"))\nawait runtime.add_subscription(TypeSubscription(\"MathSolverA\", \"MathSolverB\"))\n\n# Subscriptions for topic published to by MathSolverB.\nawait runtime.add_subscription(TypeSubscription(\"MathSolverB\", \"MathSolverA\"))\nawait runtime.add_subscription(TypeSubscription(\"MathSolverB\", \"MathSolverC\"))\n\n# Subscriptions for topic published to by MathSolverC.\nawait runtime.add_subscription(TypeSubscription(\"MathSolverC\", \"MathSolverB\"))\nawait runtime.add_subscription(TypeSubscription(\"MathSolverC\", \"MathSolverD\"))\n\n# Subscriptions for topic published to by MathSolverD.\nawait runtime.add_subscription(TypeSubscription(\"MathSolverD\", \"MathSolverC\"))\nawait runtime.add_subscription(TypeSubscription(\"MathSolverD\", \"MathSolverA\"))\n\n# All solvers and the aggregator subscribe to the default topic.\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAIChatAgent with JSON Mode Enabled in C#\nDESCRIPTION: This snippet shows how to create an OpenAIChatAgent with JSON mode enabled. It sets the response format to JsonObject and includes instructions for JSON output in the system message.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-use-json-mode.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nvar openAIClientAgent = new OpenAIChatAgent(\n    \"openAIClientAgent\",\n    new OpenAIChatCompletionsOptions\n    {\n        DeploymentName = \"gpt-4-turbo-preview\",\n        Temperature = 0,\n        ResponseFormat = ChatCompletionsResponseFormat.JsonObject\n    },\n    new List<ChatMessage>\n    {\n        new ChatMessage(ChatRole.System, \"You are a helpful assistant. Always respond in JSON format.\")\n    });\n```\n\n----------------------------------------\n\nTITLE: Defining ArithmeticAgent in Python\nDESCRIPTION: This code demonstrates the creation of a custom ArithmeticAgent that performs arithmetic operations on input integers. The agent takes an operator_func as input, which defines the specific arithmetic operation to be performed. The on_messages method applies this function to the integer extracted from the input message and returns the result in a new message.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/custom-agents.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Callable, Sequence\n\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.base import Response\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.messages import BaseChatMessage\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nclass ArithmeticAgent(BaseChatAgent):\n    def __init__(self, name: str, description: str, operator_func: Callable[[int], int]) -> None:\n        super().__init__(name, description=description)\n        self._operator_func = operator_func\n        self._message_history: List[BaseChatMessage] = []\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        # Update the message history.\n        # NOTE: it is possible the messages is an empty list, which means the agent was selected previously.\n        self._message_history.extend(messages)\n        # Parse the number in the last message.\n        assert isinstance(self._message_history[-1], TextMessage)\n        number = int(self._message_history[-1].content)\n        # Apply the operator function to the number.\n        result = self._operator_func(number)\n        # Create a new message with the result.\n        response_message = TextMessage(content=str(result), source=self.name)\n        # Update the message history.\n        self._message_history.append(response_message)\n        # Return the response.\n        return Response(chat_message=response_message)\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Defining an async result output handler for ClosureAgent in Python\nDESCRIPTION: Implements an asynchronous function 'output_result' to process messages of type FinalResult received by the agent, placing them into the shared queue. The function adheres to ClosureAgent's handler signature, receiving the agent context, the message, and message context. This enables external retrieval of agent results.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/extracting-results-with-an-agent.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nasync def output_result(_agent: ClosureContext, message: FinalResult, ctx: MessageContext) -> None:\\n    await queue.put(message)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Dependencies\nDESCRIPTION: Command to install AutoGen core and required dependencies including OpenAI and Azure integrations using pip.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_async_human_in_the_loop/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"autogen-ext[openai,azure]\"\n```\n\n----------------------------------------\n\nTITLE: Creating MistralClientAgent with Function Call Support\nDESCRIPTION: Setup of MistralClientAgent with message connector registration for tool call support\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/MistralChatAgent-use-function-call.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\nvar mistralAgent = new MistralClientAgent(\"mistral-tiny\", apiKey);\nmistralAgent.RegisterMessageConnector();\n```\n\n----------------------------------------\n\nTITLE: Example Usage of MagenticOne with Bundled Agents\nDESCRIPTION: This example demonstrates how to utilize the MagenticOne class to execute a task with bundled agents. The script uses an OpenAIChatCompletionClient to execute a Python script with minimal setup. Users must understand asynchronous programming with asyncio in Python and have the required imports prepared to successfully run the task. This snippet specifically tasks the system with script writing to interact with APIs, showcasing practical usage of agents in a comprehensive manner.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/magentic-one.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.teams.magentic_one import MagenticOne\nfrom autogen_agentchat.ui import Console\n\nasync def example_usage():\n    client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    m1 = MagenticOne(client=client)\n    task = \"Write a Python script to fetch data from an API.\"\n    result = await Console(m1.run_stream(task=task))\n    print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage())\n```\n\n----------------------------------------\n\nTITLE: Initializing Runtime and Registering Assistant Agents (Python)\nDESCRIPTION: Initializes the AutoGen environment. It creates an instance of `SingleThreadedAgentRuntime` to manage agent execution, retrieves the configured local LLM client using the `get_model_client` function, and then asynchronously registers two instances of the previously defined `Assistant` class named 'cathy' and 'joe' with the runtime. Each agent instance is initialized with its name and the shared model client.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/local-llms-ollama-litellm.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nruntime = SingleThreadedAgentRuntime()\n\nmodel_client = get_model_client()\n\ncathy = await Assistant.register(\n    runtime,\n    \"cathy\",\n    lambda: Assistant(name=\"Cathy\", model_client=model_client),\n)\n\njoe = await Assistant.register(\n    runtime,\n    \"joe\",\n    lambda: Assistant(name=\"Joe\", model_client=model_client),\n)\n```\n\n----------------------------------------\n\nTITLE: Resuming an AutoGen Team with a New Task in Python\nDESCRIPTION: This snippet demonstrates resuming an AutoGen team while providing a new, related task. Calling `await Console(team.run_stream(task=...))` adds the new task instruction to the existing conversation context, allowing the team to continue its work based on both the previous history and the new input.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/teams.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# The new task is to translate the same poem to Chinese Tang-style poetry.\nawait Console(team.run_stream(task=\"将这首诗用中文唐诗风格写一遍。\"))\n```\n\n----------------------------------------\n\nTITLE: Importing Required Namespaces in C#\nDESCRIPTION: This snippet shows the necessary using statements to import required namespaces for working with AutoGen and OpenAI in C#.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-use-function-call.md#2025-04-22_snippet_1\n\nLANGUAGE: C#\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/OpenAICodeSnippet.cs?name=using_statement)]\n```\n\n----------------------------------------\n\nTITLE: Defining a Pydantic Model for Structured Output in Python\nDESCRIPTION: Defines a Pydantic model `MathReasoning` to structure the output from the language model. It includes nested steps with explanations and outputs, along with a final answer. This model will be used to guide the LLM's response format.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/structured-output-agent.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\n\nclass MathReasoning(BaseModel):\n    class Step(BaseModel):\n        explanation: str\n        output: str\n\n    steps: list[Step]\n    final_answer: str\n```\n\n----------------------------------------\n\nTITLE: Importing and Preparing Modules for a Complete RAG Agent Pipeline in Python\nDESCRIPTION: This snippet prepares all necessary imports from the AutoGen agentchat, UI, ChromaDB memory, and OpenAI modules required to implement the next steps in the RAG pipeline with document indexing and agent instantiation. No immediate functional code is present, but dependencies must be satisfied for the final pipeline integration.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/memory.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\\nfrom pathlib import Path\\n\\nfrom autogen_agentchat.agents import AssistantAgent\\nfrom autogen_agentchat.ui import Console\\nfrom autogen_ext.memory.chromadb import ChromaDBVectorMemory, PersistentChromaDBVectorMemoryConfig\\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Termination in AutoGen Team Conversation using Python\nDESCRIPTION: This code demonstrates the use of the custom FunctionCallTermination condition in a RoundRobinGroupChat team. It sets up the team with the primary and critic agents, and runs a conversation task with the custom termination condition.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/termination.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfunction_call_termination = FunctionCallTermination(function_name=\"approve\")\nround_robin_team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=function_call_termination)\n\n# Use asyncio.run(...) if you are running this script as a standalone script.\nawait Console(round_robin_team.run_stream(task=\"Write a unique, Haiku about the weather in Paris\"))\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Wrapping Search Functions as FunctionTool Agents (Python)\nDESCRIPTION: Wraps the google_search and arxiv_search functions inside FunctionTool adapters, making them compatible as tools for agent execution. Each tool receives a description, enabling integration with agents for automated invocation. No direct external input; output are tool instances for agent orchestration.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/literature-review.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ngoogle_search_tool = FunctionTool(\\n    google_search, description=\"Search Google for information, returns results with a snippet and body content\"\\n)\\narxiv_search_tool = FunctionTool(\\n    arxiv_search, description=\"Search Arxiv for papers related to a given topic, including abstracts\"\\n)\\n\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry SDK using Pip (Bash)\nDESCRIPTION: Installs the necessary OpenTelemetry SDK package for Python using the pip package manager. This package is required to enable tracing capabilities within the AutoGen application.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tracing.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install opentelemetry-sdk\n```\n\n----------------------------------------\n\nTITLE: Serializing an Agent Team in AutoGen\nDESCRIPTION: This snippet demonstrates how to create and serialize a team using RoundRobinGroupChat with an AssistantAgent and a MaxMessageTermination condition. The team configuration is exported to JSON format.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/serialize-components.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent, UserProxyAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create an agent that uses the OpenAI GPT-4o model.\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    # api_key=\"YOUR_API_KEY\",\n)\nagent = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    handoffs=[\"flights_refunder\", \"user\"],\n    # tools=[], # serializing tools is not yet supported\n    system_message=\"Use tools to solve tasks.\",\n)\n\nteam = RoundRobinGroupChat(participants=[agent], termination_condition=MaxMessageTermination(2))\n\nteam_config = team.dump_component()  # dump component\nprint(team_config.model_dump_json())\n\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Setting Up AutoGen Agent Runtime and Execution Environment\nDESCRIPTION: Demonstrates how to set up the agent runtime environment, register agents, and execute the workflow. Uses SingleThreadedAgentRuntime for local execution and DockerCommandLineCodeExecutor for safe code execution. Includes OpenAI client setup and message publishing.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/code-execution-groupchat.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\n\nfrom autogen_core import SingleThreadedAgentRuntime\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nwork_dir = tempfile.mkdtemp()\n\n# Create an local embedded runtime.\nruntime = SingleThreadedAgentRuntime()\n\nasync with DockerCommandLineCodeExecutor(work_dir=work_dir) as executor:  # type: ignore[syntax]\n    # Register the assistant and executor agents by providing\n    # their agent types, the factory functions for creating instance and subscriptions.\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        # api_key=\"YOUR_API_KEY\"\n    )\n    await Assistant.register(\n        runtime,\n        \"assistant\",\n        lambda: Assistant(model_client=model_client),\n    )\n    await Executor.register(runtime, \"executor\", lambda: Executor(executor))\n\n    # Start the runtime and publish a message to the assistant.\n    runtime.start()\n    await runtime.publish_message(\n        Message(\"Create a plot of NVIDA vs TSLA stock returns YTD from 2024-01-01.\"), DefaultTopicId()\n    )\n\n    # Wait for the runtime to stop when idle.\n    await runtime.stop_when_idle()\n    # Close the connection to the model client.\n    await model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Configuring Local LLM Client via OpenAIChatCompletionClient (Python)\nDESCRIPTION: Defines a function `get_model_client` that returns an `OpenAIChatCompletionClient` instance configured to communicate with a local LLM server running via LiteLLM proxy. It sets the `base_url` to the proxy endpoint ('http://0.0.0.0:4000'), specifies the target model ('llama3.2:1b'), uses a dummy API key (as authentication is not required locally), and defines basic model capabilities. This function allows AutoGen to interact with the local LLM using an interface similar to the OpenAI API.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/local-llms-ollama-litellm.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_model_client() -> OpenAIChatCompletionClient:  # type: ignore\n    \"Mimic OpenAI API using Local LLM Server.\"\n    return OpenAIChatCompletionClient(\n        model=\"llama3.2:1b\",\n        api_key=\"NotRequiredSinceWeAreLocal\",\n        base_url=\"http://0.0.0.0:4000\",\n        model_capabilities={\n            \"json_output\": False,\n            \"vision\": False,\n            \"function_calling\": True,\n        },\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Mock Tools for Information Retrieval and Calculation\nDESCRIPTION: This snippet defines two mock functions to simulate a web search tool and a percentage change calculation tool. These tools mimic real APIs to demonstrate the agents' capabilities.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/selector-group-chat.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef search_web_tool(query: str) -> str:\n    if \"2006-2007\" in query:\n        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n        Udonis Haslem: 844 points\n        Dwayne Wade: 1397 points\n        James Posey: 550 points\n        ...\n        \"\"\"\n    elif \"2007-2008\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n    elif \"2008-2009\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n    return \"No data found.\"\n\ndef percentage_change_tool(start: float, end: float) -> float:\n    return ((end - start) / start) * 100\n```\n\n----------------------------------------\n\nTITLE: Registering PrintMessageMiddleware in C#\nDESCRIPTION: @AutoGen.Core.PrintMessageMiddlewareExtension.RegisterPrintMessage* registers the PrintMessageMiddleware to an agent. It formats the @AutoGen.Core.IMessage and prints it to the console. It is intended for use within an Agent Chat implementation. Dependencies include @AutoGen.Core.PrintMessageMiddleware and @AutoGen.Core.IMessage.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Print-message-middleware.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp\\[\\](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/PrintMessageMiddlewareCodeSnippet.cs?name=PrintMessageMiddleware)]\n```\n\n----------------------------------------\n\nTITLE: Setting Up ChromaDB Vector Memory and Integrating with Assistant Agent in Python\nDESCRIPTION: This code illustrates importing necessary modules and creating a persistent ChromaDB vector memory instance with specific configuration for storing user preferences. Preferences are asynchronous added as MemoryContent objects with associated metadata. Then, an agent is created using this memory and queried with a task, streaming results via the console UI. All steps require asynchronous context, and dependencies on ChromaDB, OpenAI, and agent libraries are mandatory. Finally, it demonstrates closing model and memory resources to prevent leaks. Limitations include that the persistence path must be accessible and ChromaDB must be available.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/memory.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\\nfrom pathlib import Path\\n\\nfrom autogen_agentchat.agents import AssistantAgent\\nfrom autogen_agentchat.ui import Console\\nfrom autogen_core.memory import MemoryContent, MemoryMimeType\\nfrom autogen_ext.memory.chromadb import ChromaDBVectorMemory, PersistentChromaDBVectorMemoryConfig\\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\\n\\n# Initialize ChromaDB memory with custom config\\nchroma_user_memory = ChromaDBVectorMemory(\\n    config=PersistentChromaDBVectorMemoryConfig(\\n        collection_name=\\\"preferences\\\",\\n        persistence_path=os.path.join(str(Path.home()), \\\".chromadb_autogen\\\"),\\n        k=2,  # Return top  k results\\n        score_threshold=0.4,  # Minimum similarity score\\n    )\\n)\\n# a HttpChromaDBVectorMemoryConfig is also supported for connecting to a remote ChromaDB server\\n\\n# Add user preferences to memory\\nawait chroma_user_memory.add(\\n    MemoryContent(\\n        content=\\\"The weather should be in metric units\\\",\\n        mime_type=MemoryMimeType.TEXT,\\n        metadata={\\\"category\\\": \\\"preferences\\\", \\\"type\\\": \\\"units\\\"},\\n    )\\n)\\n\\nawait chroma_user_memory.add(\\n    MemoryContent(\\n        content=\\\"Meal recipe must be vegan\\\",\\n        mime_type=MemoryMimeType.TEXT,\\n        metadata={\\\"category\\\": \\\"preferences\\\", \\\"type\\\": \\\"dietary\\\"},\\n    )\\n)\\n\\nmodel_client = OpenAIChatCompletionClient(\\n    model=\\\"gpt-4o\\\",\\n)\\n\\n# Create assistant agent with ChromaDB memory\\nassistant_agent = AssistantAgent(\\n    name=\\\"assistant_agent\\\",\\n    model_client=model_client,\\n    tools=[get_weather],\\n    memory=[chroma_user_memory],\\n)\\n\\nstream = assistant_agent.run_stream(task=\\\"What is the weather in New York?\\\")\\nawait Console(stream)\\n\\nawait model_client.close()\\nawait chroma_user_memory.close()\\n\n```\n\n----------------------------------------\n\nTITLE: Group Chat Implementation in AutoGen v0.2\nDESCRIPTION: Example of setting up a group chat with a writer and critic agents using GroupChat and GroupChatManager classes in AutoGen v0.2.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen.agentchat import AssistantAgent, GroupChat, GroupChatManager\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\nwriter = AssistantAgent(\n    name=\"writer\",\n    description=\"A writer.\",\n    system_message=\"You are a writer.\",\n    llm_config=llm_config,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"APPROVE\"),\n)\n\ncritic = AssistantAgent(\n    name=\"critic\",\n    description=\"A critic.\",\n    system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\",\n    llm_config=llm_config,\n)\n\n# Create a group chat with the writer and critic.\ngroupchat = GroupChat(agents=[writer, critic], messages=[], max_round=12)\n\n# Create a group chat manager to manage the group chat, use round-robin selection method.\nmanager = GroupChatManager(groupchat=groupchat, llm_config=llm_config, speaker_selection_method=\"round_robin\")\n```\n\n----------------------------------------\n\nTITLE: Implementing AI Agent Class\nDESCRIPTION: Implementation of the AIAgent class that handles task delegation, tool execution, and message routing between agents. Includes initialization and task handling logic.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/handoffs.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass AIAgent(RoutedAgent):\n    def __init__(\n        self,\n        description: str,\n        system_message: SystemMessage,\n        model_client: ChatCompletionClient,\n        tools: List[Tool],\n        delegate_tools: List[Tool],\n        agent_topic_type: str,\n        user_topic_type: str,\n    ) -> None:\n        super().__init__(description)\n        self._system_message = system_message\n        self._model_client = model_client\n        self._tools = dict([(tool.name, tool) for tool in tools])\n        self._tool_schema = [tool.schema for tool in tools]\n        self._delegate_tools = dict([(tool.name, tool) for tool in delegate_tools])\n        self._delegate_tool_schema = [tool.schema for tool in delegate_tools]\n        self._agent_topic_type = agent_topic_type\n        self._user_topic_type = user_topic_type\n\n    @message_handler\n    async def handle_task(self, message: UserTask, ctx: MessageContext) -> None:\n        # Send the task to the LLM.\n        llm_result = await self._model_client.create(\n            messages=[self._system_message] + message.context,\n            tools=self._tool_schema + self._delegate_tool_schema,\n            cancellation_token=ctx.cancellation_token,\n        )\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{llm_result.content}\", flush=True)\n        # Process the LLM result.\n        while isinstance(llm_result.content, list) and all(isinstance(m, FunctionCall) for m in llm_result.content):\n            tool_call_results: List[FunctionExecutionResult] = []\n            delegate_targets: List[Tuple[str, UserTask]] = []\n            # Process each function call.\n            for call in llm_result.content:\n                arguments = json.loads(call.arguments)\n                if call.name in self._tools:\n                    # Execute the tool directly.\n                    result = await self._tools[call.name].run_json(arguments, ctx.cancellation_token)\n                    result_as_str = self._tools[call.name].return_value_as_string(result)\n                    tool_call_results.append(\n                        FunctionExecutionResult(call_id=call.id, content=result_as_str, is_error=False, name=call.name)\n                    )\n                elif call.name in self._delegate_tools:\n                    # Execute the tool to get the delegate agent's topic type.\n                    result = await self._delegate_tools[call.name].run_json(arguments, ctx.cancellation_token)\n                    topic_type = self._delegate_tools[call.name].return_value_as_string(result)\n                    # Create the context for the delegate agent, including the function call and the result.\n                    delegate_messages = list(message.context) + [\n                        AssistantMessage(content=[call], source=self.id.type),\n                        FunctionExecutionResultMessage(\n                            content=[\n                                FunctionExecutionResult(\n                                    call_id=call.id,\n                                    content=f\"Transferred to {topic_type}. Adopt persona immediately.\",\n                                    is_error=False,\n                                    name=call.name,\n                                )\n                            ]\n                        ),\n                    ]\n                    delegate_targets.append((topic_type, UserTask(context=delegate_messages)))\n                else:\n                    raise ValueError(f\"Unknown tool: {call.name}\")\n            if len(delegate_targets) > 0:\n                # Delegate the task to other agents by publishing messages to the corresponding topics.\n                for topic_type, task in delegate_targets:\n                    print(f\"{'-'*80}\\n{self.id.type}:\\nDelegating to {topic_type}\", flush=True)\n                    await self.publish_message(task, topic_id=TopicId(topic_type, source=self.id.key))\n            if len(tool_call_results) > 0:\n                print(f\"{'-'*80}\\n{self.id.type}:\\n{tool_call_results}\", flush=True)\n                # Make another LLM call with the results.\n                message.context.extend(\n                    [\n                        AssistantMessage(content=llm_result.content, source=self.id.type),\n                        FunctionExecutionResultMessage(content=tool_call_results),\n                    ]\n                )\n                llm_result = await self._model_client.create(\n                    messages=[self._system_message] + message.context,\n                    tools=self._tool_schema + self._delegate_tool_schema,\n                    cancellation_token=ctx.cancellation_token,\n                )\n                print(f\"{'-'*80}\\n{self.id.type}:\\n{llm_result.content}\", flush=True)\n            else:\n                # The task has been delegated, so we are done.\n                return\n        # The task has been completed, publish the final result.\n        assert isinstance(llm_result.content, str)\n        message.context.append(AssistantMessage(content=llm_result.content, source=self.id.type))\n        await self.publish_message(\n            AgentResponse(context=message.context, reply_to_topic_type=self._agent_topic_type),\n            topic_id=TopicId(self._user_topic_type, source=self.id.key),\n        )\n```\n\n----------------------------------------\n\nTITLE: Using BufferedChatCompletionContext in AssistantAgent for Limited Context History\nDESCRIPTION: Example demonstrating how to implement a chatbot with limited message history using BufferedChatCompletionContext in AutoGen v0.4. The code creates an assistant that can only view the last 10 messages, handles user input in a loop, and properly closes the model client.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant.\",\n        model_client=model_client,\n        model_context=BufferedChatCompletionContext(buffer_size=10), # Model can only view the last 10 messages.\n    )\n    while True:\n        user_input = input(\"User: \")\n        if user_input == \"exit\":\n            break\n        response = await assistant.on_messages([TextMessage(content=user_input, source=\"user\")], CancellationToken())\n        print(\"Assistant:\", response.chat_message.to_text())\n    \n    await model_client.close()\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Creating and Running Agents with Termination Conditions in Python\nDESCRIPTION: Demonstrates initializing two agents, a primary agent for text generation and a critic agent for feedback, using the AgentChat framework. It sets up termination conditions using MaxMessageTermination and runs the process using asyncio. The example requires dependencies such as 'autogen_agentchat' and 'autogen_ext.models.openai'.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/termination.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    temperature=1,\n    # api_key=\"sk-...\", # Optional if you have an OPENAI_API_KEY env variable set.\n)\n\n# Create the primary agent.\nprimary_agent = AssistantAgent(\n    \"primary\",\n    model_client=model_client,\n    system_message=\"You are a helpful AI assistant.\"\n)\n\n# Create the critic agent.\ncritic_agent = AssistantAgent(\n    \"critic\",\n    model_client=model_client,\n    system_message=\"Provide constructive feedback for every message. Respond with 'APPROVE' to when your feedbacks are addressed.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Using Structured Output with AssistantAgent in Python\nDESCRIPTION: This code demonstrates using structured output with `AssistantAgent` to receive responses in a predefined JSON format using Pydantic's `BaseModel`. It defines an `AgentResponse` model with `thoughts` and `response` fields and configures the agent to use this model, ensuring structured responses and easy integration. The agent will reflect on the tool use by default when `output_content_type` is set.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/agents.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nfrom pydantic import BaseModel\n\n\n# The response format for the agent as a Pydantic base model.\nclass AgentResponse(BaseModel):\n    thoughts: str\n    response: Literal[\"happy\", \"sad\", \"neutral\"]\n\n\n# Create an agent that uses the OpenAI GPT-4o model.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\nagent = AssistantAgent(\n    \"assistant\",\n    model_client=model_client,\n    system_message=\"Categorize the input as happy, sad, or neutral following the JSON format.\",\n    # Define the output content type of the agent.\n    output_content_type=AgentResponse,\n)\n\nresult = await Console(agent.run_stream(task=\"I am happy.\"))\n\n# Check the last message in the result, validate its type, and print the thoughts and response.\nassert isinstance(result.messages[-1], StructuredMessage)\nassert isinstance(result.messages[-1].content, AgentResponse)\nprint(\"Thought: \", result.messages[-1].content.thoughts)\nprint(\"Response: \", result.messages[-1].content.response)\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Creating an AssistantAgent with Function Definitions in C#\nDESCRIPTION: This code demonstrates how to create an AssistantAgent and pass the WeatherReport function definition to it. It sets up the agent with specific configurations and includes the function in its capabilities.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Use-function-call.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nvar assistant = new AssistantAgent(\n    \"assistant\",\n    new List<AgentCapability> { AgentCapability.Functions },\n    new AgentConfig\n    {\n        Temperature = 0,\n        MaxTokens = 2000,\n        FunctionCall = \"auto\",\n        Functions = new[] { WeatherReport }\n    });\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen and AutoGen.SourceGenerator packages\nDESCRIPTION: Commands to install the required NuGet packages for AutoGen and AutoGen.SourceGenerator. Also includes XML configuration to enable structural XML document support.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Create-agent-with-tools.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AutoGen\ndotnet add package AutoGen.SourceGenerator\n```\n\nLANGUAGE: xml\nCODE:\n```\n<PropertyGroup>\n    <!-- This enables structural xml document support -->\n    <GenerateDocumentationFile>true</GenerateDocumentationFile>\n</PropertyGroup>\n```\n\n----------------------------------------\n\nTITLE: Adding AutoGen.SemanticKernel Package Reference in XML\nDESCRIPTION: This XML snippet shows how to add the AutoGen.SemanticKernel package reference to a project file. It includes the package with a placeholder version that should be replaced with the actual version number.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.SemanticKernel/AutoGen-SemanticKernel-Overview.md#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<ItemGroup>\n    <PackageReference Include=\"AutoGen.SemanticKernel\" Version=\"AUTOGEN_VERSION\" />\n</ItemGroup>\n```\n\n----------------------------------------\n\nTITLE: Setting Up Worker Agent Runtimes and Running Agents in Python\nDESCRIPTION: This snippet demonstrates how to set up two worker agent runtimes, each hosting one agent. It registers the custom MyAgent class with each worker and initiates communication by publishing a message. The code also includes a delay to allow agents to run for a while.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/distributed-agent-runtime.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntime\n\nworker1 = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\nawait worker1.start()\nawait MyAgent.register(worker1, \"worker1\", lambda: MyAgent(\"worker1\"))\n\nworker2 = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\nawait worker2.start()\nawait MyAgent.register(worker2, \"worker2\", lambda: MyAgent(\"worker2\"))\n\nawait worker2.publish_message(MyMessage(content=\"Hello!\"), DefaultTopicId())\n\n# Let the agents run for a while.\nawait asyncio.sleep(5)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Extensions for Distributed Agent Runtime\nDESCRIPTION: This command installs the necessary dependencies for using the distributed agent runtime feature in AutoGen.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/distributed-agent-runtime.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"autogen-ext[grpc]\"\n```\n\n----------------------------------------\n\nTITLE: Loading Team Specification as AgentChat Object\nDESCRIPTION: This code snippet illustrates how to load a team specification JSON file as an `AgentChat` object using `load_component` method from autogen_agentchat.teams.BaseGroupChat. The team configuration is read from a JSON file, and a BaseGroupChat object is instantiated representing the loaded team.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/faq.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom autogen_agentchat.teams import BaseGroupChat\nteam_config = json.load(open(\"team.json\"))\nteam = BaseGroupChat.load_component(team_config)\n```\n\n----------------------------------------\n\nTITLE: Initializing GrpcWorkerAgentRuntimeHost in Python\nDESCRIPTION: This snippet demonstrates how to start a host service for the distributed agent runtime using GrpcWorkerAgentRuntimeHost. It initializes the host and starts it in the background, listening on localhost:50051.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/distributed-agent-runtime.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntimeHost\n\nhost = GrpcWorkerAgentRuntimeHost(address=\"localhost:50051\")\nhost.start()  # Start a host service in the background.\n```\n\n----------------------------------------\n\nTITLE: Creating AssistantAgent with Azure OpenAI Model in C#\nDESCRIPTION: This code snippet shows how to create an AssistantAgent instance using an Azure OpenAI model in C#. It configures the agent with specific parameters including name, system message, and Azure-specific model settings.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Create-an-agent.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nvar assistant = new AssistantAgent(\n    name: \"assistant\",\n    systemMessage: \"You are a helpful AI assistant.\",\n    llmConfig: new AzureOpenAIConfig()\n    {\n        ModelId = \"gpt-35-turbo\",\n        ApiKey = \"YOUR_API_KEY\",\n        Endpoint = \"YOUR_AZURE_ENDPOINT\"\n    });\n```\n\n----------------------------------------\n\nTITLE: Creating Middleware Agent with Original Agent in C#\nDESCRIPTION: Demonstrates how to create a middleware agent by wrapping an existing agent with middleware functionality.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Middleware-overview.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nvar agent = new MiddlewareAgent(originalAgent, async (context, next) =>\n{\n    // Do something before calling next agent\n    var reply = await next(context);\n    // Do something after calling next agent\n    return reply;\n});\n```\n\n----------------------------------------\n\nTITLE: Calling OpenAI Model Client\nDESCRIPTION: This code demonstrates calling the OpenAIChatCompletionClient to interact with an OpenAI model using AutoGen. It assumes that the OPENAI_API_KEY is set in the environment. The snippet shows how to send a user message and await a response.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/model-clients.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4\", temperature=0.3\n)  # assuming OPENAI_API_KEY is set in the environment.\n\nresult = await model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Running an AgentChat Team with Magentic-OneGroupChat in Python\nDESCRIPTION: This Python code demonstrates how to set up and run a MagenticOneGroupChat instance using the OpenAIChatCompletionClient for task execution. The script uses asyncio to manage asynchronous tasks and requires 'OpenAIChatCompletionClient', 'AssistantAgent', 'MagenticOneGroupChat', and 'Console' components from AgentChat. The task involves providing an alternative proof for mathematical theorems, showcasing how to execute complex tasks using configured agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/magentic-one.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import MagenticOneGroupChat\nfrom autogen_agentchat.ui import Console\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    assistant = AssistantAgent(\n        \"Assistant\",\n        model_client=model_client,\n    )\n    team = MagenticOneGroupChat([assistant], model_client=model_client)\n    await Console(team.run_stream(task=\"Provide a different proof for Fermat's Last Theorem\"))\n    await model_client.close()\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Instantiate and Run Gemini Assistant (Python)\nDESCRIPTION: Instantiates the GeminiAssistantAgent and runs it within a Console environment. It demonstrates how to use the custom agent to execute a simple task.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/custom-agents.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ngemini_assistant = GeminiAssistantAgent(\"gemini_assistant\")\nawait Console(gemini_assistant.run_stream(task=\"What is the capital of New York?\"))\n```\n\n----------------------------------------\n\nTITLE: Completing the Interaction Loop with Tool Result in AutoGen (Python)\nDESCRIPTION: This snippet completes the interaction cycle by sending the tool execution result back to the model client. It wraps the tool's output (`tool_result_str`) in a `FunctionExecutionResult` and then includes it in a new `FunctionExecutionResultMessage` within the message history. A final call to the model client's `create` method allows the model to generate a natural language response based on the original query and the information obtained from the tool execution. It requires results from the previous steps (`user_message`, `create_result`, `tool_result_str`), `model_client`, and related classes from `autogen_core.models`.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/tools.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Create a function execution result\nexec_result = FunctionExecutionResult(\n    call_id=create_result.content[0].id,  # type: ignore\n    content=tool_result_str,\n    is_error=False,\n    name=stock_price_tool.name,\n)\n\n# Make another chat completion with the history and function execution result message.\nmessages = [\n    user_message,\n    AssistantMessage(content=create_result.content, source=\"assistant\"),  # assistant message with tool call\n    FunctionExecutionResultMessage(content=[exec_result]),  # function execution result message\n]\ncreate_result = await model_client.create(messages=messages, cancellation_token=cancellation_token)  # type: ignore\nprint(create_result.content)\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Integrating Semantic Kernel Plugin with AutoGen OpenAIChatAgent in C#\nDESCRIPTION: This snippet demonstrates how to create a KernelPluginMiddleware, register a custom plugin, and use it with an OpenAIChatAgent. It shows the setup process for using Semantic Kernel plugins with AutoGen agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.SemanticKernel/Use-kernel-plugin-in-other-agents.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\nvar middleware = new KernelPluginMiddleware();\nmiddleware.AddPlugin(plugin);\n\nvar agent = new OpenAIChatAgent(\n    new OpenAIChatAgentConfig\n    {\n        Model = \"gpt-3.5-turbo\",\n        Temperature = 0.7,\n        MaxTokens = 800\n    });\n\nagent.Use(middleware);\n```\n\n----------------------------------------\n\nTITLE: Creating an asynchronous queue for passing final results in Python\nDESCRIPTION: Initializes an asyncio.Queue parameterized for FinalResult objects, enabling asynchronous communication between agents and external consumers. This queue acts as the bridge for retrieving computed results from agent message handlers. Requires Python 3.9+ for typing support.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/extracting-results-with-an-agent.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nqueue = asyncio.Queue[FinalResult]()\n```\n\n----------------------------------------\n\nTITLE: Combining Termination Conditions with AND Operator in Python\nDESCRIPTION: Illustrates how to combine termination conditions using the AND (&) operator, ensuring that both conditions need to be met to stop the run. This setup allows for more complex logic in managing agent tasks within the 'autogen_agentchat' framework.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/termination.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncombined_termination = max_msg_termination & text_termination\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Tracer Provider\nDESCRIPTION: Python function to configure OpenTelemetry tracer provider with gRPC exporter and batch span processor\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/telemetry.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\n\ndef configure_oltp_tracing(endpoint: str = None) -> trace.TracerProvider:\n    # Configure Tracing\n    tracer_provider = TracerProvider(resource=Resource({\"service.name\": \"my-service\"}))\n    processor = BatchSpanProcessor(OTLPSpanExporter())\n    tracer_provider.add_span_processor(processor)\n    trace.set_tracer_provider(tracer_provider)\n\n    return tracer_provider\n```\n\n----------------------------------------\n\nTITLE: Two-Agent Chat Implementation in AutoGen v0.4\nDESCRIPTION: Updated implementation of two-agent chat using RoundRobinGroupChat with AssistantAgent and CodeExecutorAgent in v0.4. Includes streaming capability and termination conditions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.\",\n        model_client=model_client,\n    )\n\n    code_executor = CodeExecutorAgent(\n        name=\"code_executor\",\n        code_executor=LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n    )\n\n    # The termination condition is a combination of text termination and max message termination, either of which will cause the chat to terminate.\n    termination = TextMentionTermination(\"TERMINATE\") | MaxMessageTermination(10)\n\n    # The group chat will alternate between the assistant and the code executor.\n    group_chat = RoundRobinGroupChat([assistant, code_executor], termination_condition=termination)\n\n    # `run_stream` returns an async generator to stream the intermediate messages.\n    stream = group_chat.run_stream(task=\"Write a python script to print 'Hello, world!'\")\n    # `Console` is a simple UI to display the stream.\n    await Console(stream)\n    \n    # Close the connection to the model client.\n    await model_client.close()\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Capabilities in AutoGen\nDESCRIPTION: Example demonstrating how to specify model capabilities when initializing an OpenAI chat completion client. Shows configuration of vision, function calling, and JSON output capabilities.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/faqs.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nclient = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    api_key=\"YourApiKey\",\n    model_capabilities={\n        \"vision\": True,\n        \"function_calling\": False,\n        \"json_output\": False,\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Creating AssistantAgent with OpenAI Model in C#\nDESCRIPTION: This code snippet demonstrates how to create an AssistantAgent instance using an OpenAI model in C#. It initializes the agent with specific parameters such as name, system message, and model settings.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Create-an-agent.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nvar assistant = new AssistantAgent(\n    name: \"assistant\",\n    systemMessage: \"You are a helpful AI assistant.\",\n    llmConfig: new OpenAIConfig()\n    {\n        ModelId = \"gpt-3.5-turbo\",\n        ApiKey = \"YOUR_API_KEY\"\n    });\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to SemanticKernelChatCompletionAgent in C#\nDESCRIPTION: This snippet demonstrates how to send messages to a SemanticKernelChatCompletionAgent and receive responses. It shows the process of creating messages, sending them, and handling the agent's reply.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.SemanticKernel/SemanticKernelChatAgent-simple-chat.md#2025-04-22_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\nvar message = new TextMessage(\"Hello, how are you?\");\nvar reply = await skChatAgent.GetResponseAsync(message);\nConsole.WriteLine($\"AI: {reply.Content}\");\n\nmessage = new TextMessage(\"What is the capital of France?\");\nreply = await skChatAgent.GetResponseAsync(message);\nConsole.WriteLine($\"AI: {reply.Content}\");\n```\n\n----------------------------------------\n\nTITLE: Streaming Tokens with AssistantAgent in Python\nDESCRIPTION: This example shows how to stream tokens from the model client when using `AssistantAgent`. By setting `model_client_stream=True`, the agent yields `ModelClientStreamingChunkEvent` messages during `run_stream`. The underlying model API must support token streaming.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/agents.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\nstreaming_assistant = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    system_message=\"You are a helpful assistant.\",\n    model_client_stream=True,  # Enable streaming tokens.\n)\n\n# Use an async function and asyncio.run() in a script.\nasync for message in streaming_assistant.run_stream(task=\"Name two cities in South America\"):  # type: ignore\n    print(message)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for AutoGen Concurrent Agents\nDESCRIPTION: Imports required modules and classes from AutoGen core for implementing concurrent agent patterns.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/concurrent-agents.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom dataclasses import dataclass\n\nfrom autogen_core import (\n    AgentId,\n    ClosureAgent,\n    ClosureContext,\n    DefaultTopicId,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    TopicId,\n    TypeSubscription,\n    default_subscription,\n    message_handler,\n    type_subscription,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Gemini Chat Agent\nDESCRIPTION: Code to instantiate and configure a Gemini chat agent for image processing.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Image-chat-with-gemini.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\nvar gemini = new GeminiChatAgent();\n```\n\n----------------------------------------\n\nTITLE: Implementing MistralAITokenCounterMiddleware in C#\nDESCRIPTION: This code defines a custom middleware class that implements IMiddleware to collect and sum up token usage from MistralAI agent responses.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/MistralChatAgent-count-token-usage.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example14_MistralClientAgent_TokenCount.cs?name=token_counter_middleware)]\n```\n\n----------------------------------------\n\nTITLE: Runtime Setup and Execution\nDESCRIPTION: Sets up the runtime environment with worker and orchestrator agents, and executes the task through the Mixture of Agents system.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/mixture-of-agents.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nruntime = SingleThreadedAgentRuntime()\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\nawait WorkerAgent.register(runtime, \"worker\", lambda: WorkerAgent(model_client=model_client))\nawait OrchestratorAgent.register(\n    runtime,\n    \"orchestrator\",\n    lambda: OrchestratorAgent(model_client=model_client, worker_agent_types=[\"worker\"] * 3, num_layers=3),\n)\n\nruntime.start()\nresult = await runtime.send_message(UserTask(task=task), AgentId(\"orchestrator\", \"default\"))\n\nawait runtime.stop_when_idle()\nawait model_client.close()\n\nprint(f\"{'-'*80}\\nFinal result:\\n{result.result}\")\n```\n\n----------------------------------------\n\nTITLE: Indexing AutoGen Documentation for RAG\nDESCRIPTION: This function indexes AutoGen documentation from specified URLs using a SimpleDocumentIndexer. It processes the documents and stores them in the ChromaDB vector memory for later retrieval.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/memory.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nasync def index_autogen_docs() -> None:\n    indexer = SimpleDocumentIndexer(memory=rag_memory)\n    sources = [\n        \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n        \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/agents.html\",\n        \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/teams.html\",\n        \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/termination.html\",\n    ]\n    chunks: int = await indexer.index_documents(sources)\n    print(f\"Indexed {chunks} chunks from {len(sources)} AutoGen documents\")\n\n\nawait index_autogen_docs()\n```\n\n----------------------------------------\n\nTITLE: Installing and Running LiteLLM/Ollama Proxy (Shell)\nDESCRIPTION: Shell commands for Linux users to quickly install Ollama, download the llama3.2:1b model, install the LiteLLM Python package with proxy support, and start the LiteLLM proxy server. The proxy listens on http://0.0.0.0:4000 and routes requests to the specified Ollama model.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/local-llms-ollama-litellm.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -fsSL https://ollama.com/install.sh | sh\n\nollama pull llama3.2:1b\n\npip install 'litellm[proxy]'\nlitellm --model ollama/llama3.2:1b\n```\n\n----------------------------------------\n\nTITLE: Executing Python Code Blocks Locally with AutoGen (Python)\nDESCRIPTION: This example shows how to utilize the LocalCommandLineCodeExecutor to run a Python code block on the host machine using the AutoGen framework. It requires installing the autogen-core and autogen-ext packages. The local executor is initialized with a specified working directory. The asynchronous execute_code_blocks method takes CodeBlock and CancellationToken as arguments, executing code directly in the system context. Use with caution, as code executes with local system privileges.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/command-line-code-executors.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nfrom autogen_core import CancellationToken\nfrom autogen_core.code_executor import CodeBlock\nfrom autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n\nwork_dir = Path(\"coding\")\nwork_dir.mkdir(exist_ok=True)\n\nlocal_executor = LocalCommandLineCodeExecutor(work_dir=work_dir)\nprint(\n    await local_executor.execute_code_blocks(\n        code_blocks=[\n            CodeBlock(language=\"python\", code=\"print('Hello, World!')\"),\n        ],\n        cancellation_token=CancellationToken(),\n    )\n)\n\n```\n\n----------------------------------------\n\nTITLE: Creating FunctionCallMiddleware in C#\nDESCRIPTION: This C# snippet demonstrates how to create a FunctionCallMiddleware and add the WeatherReport function to it.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Function-call-with-ollama-and-litellm.md#2025-04-22_snippet_5\n\nLANGUAGE: csharp\nCODE:\n```\nvar tools = new AutoGen.Core.FunctionCallMiddleware();\ntools.Add(typeof(Tool_Call_With_Ollama_And_LiteLLM), this);\n```\n\n----------------------------------------\n\nTITLE: Creating Code Reviewer Agent in C#\nDESCRIPTION: Configuration and creation of the reviewer agent that validates code written by the coder agent.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Group-chat.md#2025-04-22_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\ncreate_reviewer\n```\n\n----------------------------------------\n\nTITLE: Implementing WeatherReport Function\nDESCRIPTION: C# implementation of the WeatherReport function marked with AutoGen.Core.FunctionAttribute for source generation.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Create-type-safe-function-call.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/TypeSafeFunctionCallCodeSnippet.cs?name=weather_report)]\n```\n\n----------------------------------------\n\nTITLE: Running Direct Message Example\nDESCRIPTION: Shows how to register and run WorkerAgent and DelegatorAgent for direct message communication.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/concurrent-agents.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nruntime = SingleThreadedAgentRuntime()\n\nawait WorkerAgent.register(runtime, \"worker\", lambda: WorkerAgent(\"Worker Agent\"))\nawait DelegatorAgent.register(runtime, \"delegator\", lambda: DelegatorAgent(\"Delegator Agent\", \"worker\"))\n\nruntime.start()\n\ndelegator = AgentId(\"delegator\", \"default\")\nresponse = await runtime.send_message(Task(task_id=\"main-task\"), recipient=delegator)\n\nprint(f\"Final result: {response.result}\")\nawait runtime.stop_when_idle()\n```\n\n----------------------------------------\n\nTITLE: Registering User Agent with Topic Subscription in AutoGen\nDESCRIPTION: Registers a user agent that serves as the initial point of contact, directing messages to the triage agent. The code sets up a subscription for the user agent to receive messages published to its own topic.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/handoffs.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Register the user agent.\nuser_agent_type = await UserAgent.register(\n    runtime,\n    type=user_topic_type,\n    factory=lambda: UserAgent(\n        description=\"A user agent.\",\n        user_topic_type=user_topic_type,\n        agent_topic_type=triage_agent_topic_type,  # Start with the triage agent.\n    ),\n)\n# Add subscriptions for the user agent: it will receive messages published to its own topic only.\nawait runtime.add_subscription(TypeSubscription(topic_type=user_topic_type, agent_type=user_agent_type.type))\n```\n\n----------------------------------------\n\nTITLE: Setting Up ToolUseAgent Runtime and Registration\nDESCRIPTION: Shows how to create and configure a runtime environment for the ToolUseAgent, including model client initialization, tool setup, and agent registration.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/tools.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Create the model client.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n# Create a runtime.\nruntime = SingleThreadedAgentRuntime()\n# Create the tools.\ntools: List[Tool] = [FunctionTool(get_stock_price, description=\"Get the stock price.\")]\n# Register the agents.\nawait ToolUseAgent.register(\n    runtime,\n    \"tool_use_agent\",\n    lambda: ToolUseAgent(\n        model_client=model_client,\n        tool_schema=tools,\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Serializing MultimodalWebSurfer Agent in AutoGen\nDESCRIPTION: This snippet demonstrates how to create and serialize a MultimodalWebSurfer agent, which is a specialized agent designed for browsing the web.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/serialize-components.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\n\nagent = MultimodalWebSurfer(\n    name=\"web_surfer\",\n    model_client=model_client,\n    headless=False,\n)\n\nweb_surfer_config = agent.dump_component()  # dump component\nprint(web_surfer_config.model_dump_json())\n\n```\n\n----------------------------------------\n\nTITLE: Implementing a simple agent with context management\nDESCRIPTION: Defines a SimpleAgentWithContext class that manages a chat context using BufferedChatCompletionContext. The agent can handle user messages and generate responses using the ChatCompletionClient while maintaining a message buffer to avoid overflow.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/model-context.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleAgentWithContext(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"A simple agent\")\n        self._system_messages = [SystemMessage(content=\"You are a helpful AI assistant.\")]\n        self._model_client = model_client\n        self._model_context = BufferedChatCompletionContext(buffer_size=5)\n\n    @message_handler\n    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n        # Prepare input to the chat completion model.\n        user_message = UserMessage(content=message.content, source=\"user\")\n        # Add message to model context.\n        await self._model_context.add_message(user_message)\n        # Generate a response.\n        response = await self._model_client.create(\n            self._system_messages + (await self._model_context.get_messages()),\n            cancellation_token=ctx.cancellation_token,\n        )\n        # Return with the model's response.\n        assert isinstance(response.content, str)\n        # Add message to model context.\n        await self._model_context.add_message(AssistantMessage(content=response.content, source=self.metadata[\"type\"]))\n        return Message(content=response.content)\n```\n\n----------------------------------------\n\nTITLE: Implementing Request/Response Agents in AutoGen Core\nDESCRIPTION: Shows how to implement agents that use direct messaging for request/response scenarios in AutoGen core.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/message-and-communication.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\nfrom autogen_core import MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\n\n@dataclass\nclass Message:\n    content: str\n\nclass InnerAgent(RoutedAgent):\n    @message_handler\n    async def on_my_message(self, message: Message, ctx: MessageContext) -> Message:\n        return Message(content=f\"Hello from inner, {message.content}\")\n\nclass OuterAgent(RoutedAgent):\n    def __init__(self, description: str, inner_agent_type: str):\n        super().__init__(description)\n        self.inner_agent_id = AgentId(inner_agent_type, self.id.key)\n\n    @message_handler\n    async def on_my_message(self, message: Message, ctx: MessageContext) -> None:\n        print(f\"Received message: {message.content}\")\n        # Send a direct message to the inner agent and receives a response.\n        response = await self.send_message(Message(f\"Hello from outer, {message.content}\"), self.inner_agent_id)\n        print(f\"Received inner response: {response.content}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing and Testing Azure Code Executor\nDESCRIPTION: Creates an ACADynamicSessionsCodeExecutor instance and runs a simple 'hello world' test to verify functionality.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/extensions-user-guide/azure-container-code-executor.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncancellation_token = CancellationToken()\nPOOL_MANAGEMENT_ENDPOINT = \"...\"\n\nwith tempfile.TemporaryDirectory() as temp_dir:\n    executor = ACADynamicSessionsCodeExecutor(\n        pool_management_endpoint=POOL_MANAGEMENT_ENDPOINT, credential=DefaultAzureCredential(), work_dir=temp_dir\n    )\n\n    code_blocks = [CodeBlock(code=\"import sys; print('hello world!')\", language=\"python\")]\n    code_result = await executor.execute_code_blocks(code_blocks, cancellation_token)\n    assert code_result.exit_code == 0 and \"hello world!\" in code_result.output\n```\n\n----------------------------------------\n\nTITLE: Defining a Function Tool in Python\nDESCRIPTION: This snippet defines a function tool using a Python function and demonstrates how the schema is automatically generated. It defines a simple `web_search_func` function that returns a static string, and then shows that the FunctionTool is automatically created, and the schema is generated.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/agents.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core.tools import FunctionTool\n\n\n# Define a tool using a Python function.\nasync def web_search_func(query: str) -> str:\n    \"\"\"Find information on the web\"\"\"\n    return \"AutoGen is a programming framework for building multi-agent applications.\"\n\n\n# This step is automatically performed inside the AssistantAgent if the tool is a Python function.\nweb_search_function_tool = FunctionTool(web_search_func, description=\"Find information on the web\")\n# The schema is provided to the model during AssistantAgent's on_messages call.\nweb_search_function_tool.schema\n```\n\n----------------------------------------\n\nTITLE: Defining Navigation with MyST toctree Directive\nDESCRIPTION: This MyST directive (`toctree`) configures the table of contents for the documentation section. It lists the specific example pages (`travel-planning`, `company-research`, `literature-review`) to include in the site's navigation structure, sets the maximum depth for nested links to 1, and hides the explicit ToC list from being rendered directly on this page.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/index.md#2025-04-22_snippet_1\n\nLANGUAGE: myst\nCODE:\n```\n```{toctree}\n:maxdepth: 1\n:hidden:\n\ntravel-planning\ncompany-research\nliterature-review\n\n```\n```\n\n----------------------------------------\n\nTITLE: Executing Bash Commands in a Python Virtual Environment with AutoGen (Python)\nDESCRIPTION: This code snippet describes how to set up a Python virtual environment using venv and execute bash code (such as installing packages) within that environment using the LocalCommandLineCodeExecutor from the AutoGen framework. Prerequisites include Python's venv module, autogen-core, and autogen-ext packages. A virtual environment is created in a working directory, and its context is passed to the executor for code block execution. This approach ensures package isolation and prevents modifications to the global environment.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/command-line-code-executors.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport venv\nfrom pathlib import Path\n\nfrom autogen_core import CancellationToken\nfrom autogen_core.code_executor import CodeBlock\nfrom autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n\nwork_dir = Path(\"coding\")\nwork_dir.mkdir(exist_ok=True)\n\nvenv_dir = work_dir / \".venv\"\nvenv_builder = venv.EnvBuilder(with_pip=True)\nvenv_builder.create(venv_dir)\nvenv_context = venv_builder.ensure_directories(venv_dir)\n\nlocal_executor = LocalCommandLineCodeExecutor(work_dir=work_dir, virtual_env_context=venv_context)\nawait local_executor.execute_code_blocks(\n    code_blocks=[\n        CodeBlock(language=\"bash\", code=\"pip install matplotlib\"),\n    ],\n    cancellation_token=CancellationToken(),\n)\n\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using LMStudioAgent for Fibonacci Calculation in C#\nDESCRIPTION: C# code demonstrating how to configure and use LMStudioAgent to generate code for calculating the 100th Fibonacci number. It sets up the agent, defines the task, and initiates a conversation to solve the problem.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Consume-LLM-server-from-LM-Studio.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\nvar loggerFactory = LoggerFactory.Create(builder => builder.AddConsole());\nvar logger = loggerFactory.CreateLogger<Program>();\n\nvar lmStudioConfig = new LMStudioConfig\n{\n    BaseUrl = \"http://localhost:1234\",\n    ModelName = \"gpt-3.5-turbo\",\n    Temperature = 0,\n    MaxTokens = 2000\n};\n\nvar initiator = new HumanAgent(\"Initiator\");\nvar assistant = new LMStudioAgent(\"Assistant\", lmStudioConfig, logger);\n\nvar result = await initiator.InitiateConversationAsync(\n    \"Write a piece of C# code to calculate 100th of fibonacci.\",\n    assistant);\n\nConsole.WriteLine(result);\n```\n\n----------------------------------------\n\nTITLE: Configuring AutoGen Agents and Team with o3-mini Reasoning Model\nDESCRIPTION: This snippet demonstrates how to set up AutoGen agents and a SelectorGroupChat team using the o3-mini reasoning model. It includes configurations for a web search agent, data analyst agent, user proxy agent, and the team selector prompt.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/selector-group-chat.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel_client = OpenAIChatCompletionClient(model=\"o3-mini\")\n\nweb_search_agent = AssistantAgent(\n    \"WebSearchAgent\",\n    description=\"An agent for searching information on the web.\",\n    tools=[search_web_tool],\n    model_client=model_client,\n    system_message=\"\"\"Use web search tool to find information.\"\"\",\n)\n\ndata_analyst_agent = AssistantAgent(\n    \"DataAnalystAgent\",\n    description=\"An agent for performing calculations.\",\n    model_client=model_client,\n    tools=[percentage_change_tool],\n    system_message=\"\"\"Use tool to perform calculation. If you have not seen the data, ask for it.\"\"\",\n)\n\nuser_proxy_agent = UserProxyAgent(\n    \"UserProxyAgent\",\n    description=\"A user to approve or disapprove tasks.\",\n)\n\nselector_prompt = \"\"\"Select an agent to perform task.\n\n{roles}\n\nCurrent conversation context:\n{history}\n\nRead the above conversation, then select an agent from {participants} to perform the next task.\nWhen the task is complete, let the user approve or disapprove the task.\n\"\"\"\n\nteam = SelectorGroupChat(\n    [web_search_agent, data_analyst_agent, user_proxy_agent],\n    model_client=model_client,\n    termination_condition=termination,  # Use the same termination condition as before.\n    selector_prompt=selector_prompt,\n    allow_repeated_speaker=True\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Single-Turn Function Call with Gemini\nDESCRIPTION: Example of a single-turn function call where the agent processes a query, makes a function call to find movies by director, and then returns the results in a single interaction.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Function-call-with-gemini.md#2025-04-22_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\n// Single turn function call\nConsole.WriteLine(\"Single turn function call:\");\nvar messages = new ChatMessage[]\n{\n    new AssistantChatMessage(\"I'm a movie expert assistant that can help you find movies by director and provide movie reviews.\"),\n    new UserChatMessage(\"Find me movies directed by Christopher Nolan.\")\n};\n\nvar result = await geminiAgent.GetChatCompletionsAsync(messages);\nConsole.WriteLine(JsonSerializer.Serialize(result, new JsonSerializerOptions { WriteIndented = true }));\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to Agents using SingleThreadedAgentRuntime in Python\nDESCRIPTION: This example shows how to start the runtime, send messages to registered agents using their AgentId, and stop the runtime.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/agent-and-agent-runtime.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nruntime.start()  # Start processing messages in the background.\nawait runtime.send_message(MyMessageType(\"Hello, World!\"), AgentId(\"my_agent\", \"default\"))\nawait runtime.send_message(MyMessageType(\"Hello, World!\"), AgentId(\"my_assistant\", \"default\"))\nawait runtime.stop()  # Stop processing messages in the background.\n```\n\n----------------------------------------\n\nTITLE: Implementing Single Message Processor\nDESCRIPTION: Creates a Processor agent class that handles tasks using default subscription, demonstrating how multiple agents can process the same message.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/concurrent-agents.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@default_subscription\nclass Processor(RoutedAgent):\n    @message_handler\n    async def on_task(self, message: Task, ctx: MessageContext) -> None:\n        print(f\"{self._description} starting task {message.task_id}\")\n        await asyncio.sleep(2)  # Simulate work\n        print(f\"{self._description} finished task {message.task_id}\")\n```\n\n----------------------------------------\n\nTITLE: Running an AssistantAgent with a task in Python\nDESCRIPTION: This snippet shows how to use the `run` method of an AssistantAgent to execute a task.  It calls the agent's `run` method with a specific task and then prints the messages from the result, showcasing the agent's thought process and final response.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/agents.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Use asyncio.run(agent.run(...)) when running in a script.\nresult = await agent.run(task=\"Find information on AutoGen\")\nprint(result.messages)\n```\n\n----------------------------------------\n\nTITLE: Executing Model-Generated Tool Call and Reflecting in AutoGen (Python)\nDESCRIPTION: This snippet demonstrates processing the result of a model-generated tool call. It extracts the arguments from the model's response (`create_result`), parses them from JSON, executes the `stock_price_tool` using `run_json` with these arguments, and obtains the string result. This showcases the step where the application actually performs the action requested by the model via the tool call. Requires `json` and the `create_result` and `stock_price_tool` from previous steps.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/tools.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nassert isinstance(create_result.content, list)\narguments = json.loads(create_result.content[0].arguments)  # type: ignore\ntool_result = await stock_price_tool.run_json(arguments, cancellation_token)\ntool_result_str = stock_price_tool.return_value_as_string(tool_result)\ntool_result_str\n```\n\n----------------------------------------\n\nTITLE: Importing Core Autogen Modules and Dependencies (Python)\nDESCRIPTION: Imports essential classes from the Autogen ecosystem needed to define agents, tools, models, user interface, and team orchestration. These imports serve as foundational building blocks for the rest of the multi-agent literature review setup. No external input or output; simply prepares namespaced access to these components.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/literature-review.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\\nfrom autogen_agentchat.conditions import TextMentionTermination\\nfrom autogen_agentchat.teams import RoundRobinGroupChat\\nfrom autogen_agentchat.ui import Console\\nfrom autogen_core.tools import FunctionTool\\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\\n\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen and Extensions\nDESCRIPTION: Command for installing the AutoGen AgentChat library and extensions with OpenAI and Azure support.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/quickstart.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install -U \"autogen-agentchat\" \"autogen-ext[openai,azure]\"\n```\n\n----------------------------------------\n\nTITLE: Importing core autogen and standard library modules in Python\nDESCRIPTION: This snippet imports essential modules and types from the autogen_core framework, as well as Python's standard asyncio and dataclasses modules. These imports are prerequisites for defining agents, results, and managing asynchronous message passing in the multi-agent system. Key dependencies include autogen_core and asyncio.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/extracting-results-with-an-agent.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\\nfrom dataclasses import dataclass\\n\\nfrom autogen_core import (\\n    ClosureAgent,\\n    ClosureContext,\\n    DefaultSubscription,\\n    DefaultTopicId,\\n    MessageContext,\\n    SingleThreadedAgentRuntime,\\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying AutoGen Dependency in pyproject.toml\nDESCRIPTION: This snippet demonstrates how to specify the AutoGen dependency in the pyproject.toml file. It ensures that the extension works with the correct version of AutoGen by setting appropriate version constraints.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/extensions-user-guide/create-your-own.md#2025-04-22_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[project]\n# ...\ndependencies = [\n    \"autogen-core>=0.4,<0.5\"\n]\n```\n\n----------------------------------------\n\nTITLE: Disabling Parallel Tool Calls in AssistantAgent Python\nDESCRIPTION: This code snippet shows how to disable parallel tool calls when using `OpenAIChatCompletionClient` with `AssistantAgent`. Setting `parallel_tool_calls=False` ensures that tools are called sequentially, preventing potential interference. Requires the `autogen` library and a defined `web_search` tool.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/agents.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel_client_no_parallel_tool_call = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    parallel_tool_calls=False,  # type: ignore\n)\nagent_no_parallel_tool_call = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client_no_parallel_tool_call,\n    tools=[web_search],\n    system_message=\"Use tools to solve tasks.\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Broadcasting Agent in Python\nDESCRIPTION: This snippet shows a BroadcastingAgent class that publishes messages to a topic upon receiving a message. It uses the publish_message method with a specified TopicId to broadcast messages to agents subscribed to the 'default' topic type.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/message-and-communication.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core import TopicId\n\n\nclass BroadcastingAgent(RoutedAgent):\n    @message_handler\n    async def on_my_message(self, message: Message, ctx: MessageContext) -> None:\n        await self.publish_message(\n            Message(\"Publishing a message from broadcasting agent!\"),\n            topic_id=TopicId(type=\"default\", source=self.id.key),\n        )\n```\n\n----------------------------------------\n\nTITLE: Requesting and Validating Structured Output with Autogen and Pydantic in Python\nDESCRIPTION: Demonstrates making an asynchronous call to the initialized Azure OpenAI client using `client.create`. It sends a `UserMessage` and specifies the `MathReasoning` Pydantic model in `extra_create_args` under the `response_format` key to request structured output. The code then checks if the response content is a valid string, parses it as JSON, prints it, and finally validates the parsed JSON against the `MathReasoning` Pydantic model using `model_validate`.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/structured-output-agent.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Define the user message\nmessages = [\n    UserMessage(content=\"What is 16 + 32?\", source=\"user\"),\n]\n\n# Call the create method on the client, passing the messages and additional arguments\n# The extra_create_args dictionary includes the response format as MathReasoning model we defined above\n# Providing the response format and pydantic model will use the new parse method from beta SDK\nresponse = await client.create(messages=messages, extra_create_args={\"response_format\": MathReasoning})\n\n# Ensure the response content is a valid JSON string before loading it\nresponse_content: Optional[str] = response.content if isinstance(response.content, str) else None\nif response_content is None:\n    raise ValueError(\"Response content is not a valid JSON string\")\n\n# Print the response content after loading it as JSON\nprint(json.loads(response_content))\n\n# Validate the response content with the MathReasoning model\nMathReasoning.model_validate(json.loads(response_content))\n```\n\n----------------------------------------\n\nTITLE: Initializing and Populating ListMemory, Defining an Async Tool, and Agent Setup in Python\nDESCRIPTION: This code block demonstrates the initialization of a ListMemory instance to act as the user's memory bank, populates it with user preferences using asynchronous addition of MemoryContent, and defines an async weather tool. It then instantiates an AssistantAgent with the user tool, ties in OpenAI's chat model, and attaches the memory. All actions are asynchronous and require the relevant AutoGen and OpenAI dependencies. Expected input includes user queries, and outputs are contextually informed agent responses. Limitations involve reliance on available models and correct setup of all dependencies.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/memory.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initialize user memory\\nuser_memory = ListMemory()\\n\\n# Add user preferences to memory\\nawait user_memory.add(MemoryContent(content=\\\"The weather should be in metric units\\\", mime_type=MemoryMimeType.TEXT))\\n\\nawait user_memory.add(MemoryContent(content=\\\"Meal recipe must be vegan\\\", mime_type=MemoryMimeType.TEXT))\\n\\n\\nasync def get_weather(city: str, units: str = \\\"imperial\\\") -> str:\\n    if units == \\\"imperial\\\":\\n        return f\\\"The weather in {city} is 73 °F and Sunny.\\\"\\n    elif units == \\\"metric\\\":\\n        return f\\\"The weather in {city} is 23 °C and Sunny.\\\"\\n    else:\\n        return f\\\"Sorry, I don't know the weather in {city}.\\\"\\n\\n\\nassistant_agent = AssistantAgent(\\n    name=\\\"assistant_agent\\\",\\n    model_client=OpenAIChatCompletionClient(\\n        model=\\\"gpt-4o-2024-08-06\\\",\\n    ),\\n    tools=[get_weather],\\n    memory=[user_memory],\\n)\\n\n```\n\n----------------------------------------\n\nTITLE: Running Single Message Processor Example\nDESCRIPTION: Demonstrates how to register and run multiple Processor agents that handle the same task concurrently.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/concurrent-agents.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nruntime = SingleThreadedAgentRuntime()\n\nawait Processor.register(runtime, \"agent_1\", lambda: Processor(\"Agent 1\"))\nawait Processor.register(runtime, \"agent_2\", lambda: Processor(\"Agent 2\"))\n\nruntime.start()\n\nawait runtime.publish_message(Task(task_id=\"task-1\"), topic_id=DefaultTopicId())\n\nawait runtime.stop_when_idle()\n```\n\n----------------------------------------\n\nTITLE: Resuming an AutoGen Team Conversation in Python\nDESCRIPTION: This snippet shows how to resume a previously stopped AutoGen team, allowing it to continue the conversation from where it left off. By calling `await Console(team.run_stream())` without a new task argument, the team picks up the execution with the next agent in the round-robin sequence, preserving the existing conversation history.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/teams.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nawait Console(team.run_stream())  # Resume the team to continue the last task.\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Schema Class for AddAsync Operation in C#\nDESCRIPTION: Schema class definition for JSON serialization of addition operation parameters. Contains two integer properties 'a' and 'b' with JsonPropertyName attributes.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.SourceGenerator.Tests/ApprovalTests/FunctionCallTemplateTests.TestFunctionCallTemplate.approved.txt#2025-04-22_snippet_0\n\nLANGUAGE: C#\nCODE:\n```\nprivate class AddAsyncSchema\n{\n    [JsonPropertyName(@\"a\")]\n\tpublic System.Int32 a {get; set;}\n    [JsonPropertyName(@\"b\")]\n\tpublic System.Int32 b {get; set;}\n}\n```\n\n----------------------------------------\n\nTITLE: Installing and Running AutoGen Studio\nDESCRIPTION: This code snippet shows how to install AutoGen Studio using pip and start the UI on port 8080 with the application directory set to ./myapp. AutoGen Studio provides a web-based UI for prototyping with agents without writing code.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/index.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U autogenstudio\nautogenstudio ui --port 8080 --appdir ./myapp\n```\n\n----------------------------------------\n\nTITLE: Configuring GitHub Authentication for AutoGen Studio (YAML)\nDESCRIPTION: Defines the structure and necessary fields for the `auth.yaml` file to enable GitHub OAuth in AutoGen Studio. It specifies the authentication type, JWT secret, token expiry, and GitHub application details like client ID, client secret, callback URL, and requested scopes. A strong, unique JWT secret is required for security.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/experimental.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntype: github\njwt_secret: \"your-secret-key\" # keep secure!\ntoken_expiry_minutes: 60\ngithub:\n  client_id: \"your-github-client-id\"\n  client_secret: \"your-github-client-secret\"\n  callback_url: \"http://localhost:8081/api/auth/callback\"\n  scopes: [\"user:email\"]\n```\n\n----------------------------------------\n\nTITLE: Implementing Direct Message Agents\nDESCRIPTION: Creates WorkerAgent and DelegatorAgent classes for direct message communication between agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/concurrent-agents.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass WorkerAgent(RoutedAgent):\n    @message_handler\n    async def on_task(self, message: Task, ctx: MessageContext) -> TaskResponse:\n        print(f\"{self.id} starting task {message.task_id}\")\n        await asyncio.sleep(2)  # Simulate work\n        print(f\"{self.id} finished task {message.task_id}\")\n        return TaskResponse(task_id=message.task_id, result=f\"Results by {self.id}\")\n\n\nclass DelegatorAgent(RoutedAgent):\n    def __init__(self, description: str, worker_type: str):\n        super().__init__(description)\n        self.worker_instances = [AgentId(worker_type, f\"{worker_type}-1\"), AgentId(worker_type, f\"{worker_type}-2\")]\n\n    @message_handler\n    async def on_task(self, message: Task, ctx: MessageContext) -> TaskResponse:\n        print(f\"Delegator received task {message.task_id}.\")\n\n        subtask1 = Task(task_id=\"task-part-1\")\n        subtask2 = Task(task_id=\"task-part-2\")\n\n        worker1_result, worker2_result = await asyncio.gather(\n            self.send_message(subtask1, self.worker_instances[0]), self.send_message(subtask2, self.worker_instances[1])\n        )\n\n        combined_result = f\"Part 1: {worker1_result.result}, \" f\"Part 2: {worker2_result.result}\"\n        task_response = TaskResponse(task_id=message.task_id, result=combined_result)\n        return task_response\n```\n\n----------------------------------------\n\nTITLE: Registering Tool Executor and Tool-Using Agents with the Runtime in Python\nDESCRIPTION: Registers two distinct agents with the initialized AutoGen runtime. First, it registers a `ToolAgent` named \"tool_executor_agent\", specifically equipped with the `python_tool` for code execution. Second, it registers the custom `ToolUseAgent` named \"tool_enabled_agent\", configuring it with system prompts, an `OpenAIChatCompletionClient` (using `gpt-4o-mini`), the schema of the `python_tool`, and the `AgentType` of the previously registered tool executor agent.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/tool-use-with-intervention.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Register agents.\ntool_agent_type = await ToolAgent.register(\n    runtime,\n    \"tool_executor_agent\",\n    lambda: ToolAgent(\n        description=\"Tool Executor Agent\",\n        tools=[python_tool],\n    ),\n)\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\nawait ToolUseAgent.register(\n    runtime,\n    \"tool_enabled_agent\",\n    lambda: ToolUseAgent(\n        description=\"Tool Use Agent\",\n        system_messages=[SystemMessage(content=\"You are a helpful AI Assistant. Use your tools to solve problems.\")],\n        model_client=model_client,\n        tool_schema=[python_tool.schema],\n        tool_agent_type=tool_agent_type,\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Implement Single-Tenant, Single Scope Publishing Scenario in Python\nDESCRIPTION: This snippet implements a single-tenant, single scope scenario where all agents operate under a single client. Each agent receives every message, and the setup involves registering specialist agents and adding default subscriptions for collaborative communication. Dependencies include asyncio and the AutoGen framework.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/topic-subscription-scenarios.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nasync def run_single_tenant_single_scope() -> None:\n    # Create the runtime.\n    runtime = SingleThreadedAgentRuntime()\n\n    # Register TaxSpecialist agents for each specialty\n    specialist_agent_type_1 = \"TaxSpecialist_1\"\n    specialist_agent_type_2 = \"TaxSpecialist_2\"\n    await TaxSpecialist.register(\n        runtime=runtime,\n        type=specialist_agent_type_1,\n        factory=lambda: TaxSpecialist(\n            description=\"A tax specialist 1\",\n            specialty=TaxSpecialty.PLANNING,\n            system_messages=[SystemMessage(content=\"You are a tax specialist.\")],\n        ),\n    )\n\n    await TaxSpecialist.register(\n        runtime=runtime,\n        type=specialist_agent_type_2,\n        factory=lambda: TaxSpecialist(\n            description=\"A tax specialist 2\",\n            specialty=TaxSpecialty.DISPUTE_RESOLUTION,\n            system_messages=[SystemMessage(content=\"You are a tax specialist.\")],\n        ),\n    )\n\n    # Add default subscriptions for each agent type\n    await runtime.add_subscription(DefaultSubscription(agent_type=specialist_agent_type_1))\n    await runtime.add_subscription(DefaultSubscription(agent_type=specialist_agent_type_2))\n\n    # Start the runtime and send a message to agents on default topic\n    runtime.start()\n    await runtime.publish_message(ClientRequest(\"I need to have my tax for 2024 prepared.\"), topic_id=DefaultTopicId())\n    await runtime.stop_when_idle()\n\n\nawait run_single_tenant_single_scope()\n```\n\n----------------------------------------\n\nTITLE: Running Travel Planning Agent Team\nDESCRIPTION: This snippet shows how to run the previously created travel planning agent team using the run_stream method.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-studio/notebooks/tutorial.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresult = group_chat.run_stream(task=\"Plan a 3 day trip to Nepal.\")\nasync for response in result:\n    print(response)\n```\n\n----------------------------------------\n\nTITLE: Setting up LlamaIndex LLM and Embedding Model (Python)\nDESCRIPTION: This code configures LlamaIndex by setting up the Language Model (LLM) and embedding model. It initializes either AzureOpenAI or OpenAI models based on environment variables. It also sets the LlamaIndex settings to use the specified LLM and embedding model.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/llamaindex-agent.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# llm = AzureOpenAI(\n#     deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n#     temperature=0.0,\n#     azure_ad_token_provider = get_bearer_token_provider(DefaultAzureCredential()),\n#     # api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n#     azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n#     api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n# )\nllm = OpenAI(\n    model=\"gpt-4o\",\n    temperature=0.0,\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\n# embed_model = AzureOpenAIEmbedding(\n#     deployment_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\"),\n#     temperature=0.0,\n#     azure_ad_token_provider = get_bearer_token_provider(DefaultAzureCredential()),\n#     api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n#     azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n#     api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n# )\nembed_model = OpenAIEmbedding(\n    model=\"text-embedding-ada-002\",\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\nSettings.llm = llm\nSettings.embed_model = embed_model\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for AutoGen GraphRAG Project\nDESCRIPTION: Command to install the necessary Python packages for the AutoGen GraphRAG project using pip and the requirements file.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_graphrag/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoGen Code Writing Design Pattern\nDESCRIPTION: Demonstrates setting up a runtime environment with coder and reviewer agents, initializing the OpenAI model client, and executing a code writing task through message publishing. The pattern shows agent registration, message publishing, and proper cleanup of resources.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/reflection.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core import DefaultTopicId, SingleThreadedAgentRuntime\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nruntime = SingleThreadedAgentRuntime()\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\nawait ReviewerAgent.register(runtime, \"ReviewerAgent\", lambda: ReviewerAgent(model_client=model_client))\nawait CoderAgent.register(runtime, \"CoderAgent\", lambda: CoderAgent(model_client=model_client))\nruntime.start()\nawait runtime.publish_message(\n    message=CodeWritingTask(task=\"Write a function to find the sum of all even numbers in a list.\"),\n    topic_id=DefaultTopicId(),\n)\n\n# Keep processing messages until idle.\nawait runtime.stop_when_idle()\n# Close the model client.\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat with Agent\nDESCRIPTION: Demonstrates streaming chat functionality with an agent using GenerateStreamingReplyAsync. Includes handling of streaming updates and console output.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Agent-overview.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\nvar message = new TextMessage(\"Hello Agent!\");\nawait foreach (var update in agent.GenerateStreamingReplyAsync(message))\n{\n    Console.Write(update);\n}\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Extensions for AutoGen\nDESCRIPTION: Commands to install AutoGen extensions for OpenAI and Azure OpenAI integration, including support for AAD authentication.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/installation.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install \"autogen-ext[openai]\"\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install \"autogen-ext[azure]\"\n```\n\n----------------------------------------\n\nTITLE: Stopping an AutoGen Team Externally in Python\nDESCRIPTION: This snippet demonstrates how to stop a running AutoGen team externally. It creates a new team with a combined termination condition using `ExternalTermination` and `TextMentionTermination`. The team's execution (`team.run_stream`) is started in a background task. After a brief pause (`asyncio.sleep`), the `external_termination.set()` method is called, signaling the team to stop after the current agent finishes its turn.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/teams.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Create a new team with an external termination condition.\nexternal_termination = ExternalTermination()\nteam = RoundRobinGroupChat(\n    [primary_agent, critic_agent],\n    termination_condition=external_termination | text_termination,  # Use the bitwise OR operator to combine conditions.\n)\n\n# Run the team in a background task.\nrun = asyncio.create_task(Console(team.run_stream(task=\"Write a short poem about the fall season.\")))\n\n# Wait for some time.\nawait asyncio.sleep(0.1)\n\n# Stop the team.\nexternal_termination.set()\n\n# Wait for the team to finish.\nawait run\n```\n\n----------------------------------------\n\nTITLE: Overriding Function Definitions in GenerateReplyOptions in C#\nDESCRIPTION: This snippet shows how to override function definitions when invoking an agent. It creates new GenerateReplyOptions with a custom function definition, which will be used instead of the agent's default functions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Use-function-call.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\nvar reply = await agent.GenerateReplyAsync(\n    \"What's the weather like in Seattle today?\",\n    new GenerateReplyOptions\n    {\n        Functions = new[] { WeatherReport }\n    });\n```\n\n----------------------------------------\n\nTITLE: Implementing Delegate Tools in Python\nDESCRIPTION: Functions and tools for task delegation between different agents in the system.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/handoffs.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef transfer_to_sales_agent() -> str:\n    return sales_agent_topic_type\n\n\ndef transfer_to_issues_and_repairs() -> str:\n    return issues_and_repairs_agent_topic_type\n\n\ndef transfer_back_to_triage() -> str:\n    return triage_agent_topic_type\n\n\ndef escalate_to_human() -> str:\n    return human_agent_topic_type\n\n\ntransfer_to_sales_agent_tool = FunctionTool(\n    transfer_to_sales_agent, description=\"Use for anything sales or buying related.\"\n)\ntransfer_to_issues_and_repairs_tool = FunctionTool(\n    transfer_to_issues_and_repairs, description=\"Use for issues, repairs, or refunds.\"\n)\ntransfer_back_to_triage_tool = FunctionTool(\n    transfer_back_to_triage,\n    description=\"Call this if the user brings up a topic outside of your purview,\\nincluding escalating to human.\",\n)\nescalate_to_human_tool = FunctionTool(escalate_to_human, description=\"Only call this if explicitly asked to.\")\n```\n\n----------------------------------------\n\nTITLE: Using UserProxyAgent for In-Loop Feedback in RoundRobinGroupChat\nDESCRIPTION: Example of using UserProxyAgent to get user feedback during a team's execution. This code creates a RoundRobinGroupChat with an assistant and a user proxy, and runs a poetry generation task with a termination condition based on text mention.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/human-in-the-loop.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent, UserProxyAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create the agents.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\nassistant = AssistantAgent(\"assistant\", model_client=model_client)\nuser_proxy = UserProxyAgent(\"user_proxy\", input_func=input)  # Use input() to get user input from console.\n\n# Create the termination condition which will end the conversation when the user says \"APPROVE\".\ntermination = TextMentionTermination(\"APPROVE\")\n\n# Create the team.\nteam = RoundRobinGroupChat([assistant, user_proxy], termination_condition=termination)\n\n# Run the conversation and stream to the console.\nstream = team.run_stream(task=\"Write a 4-line poem about the ocean.\")\n# Use asyncio.run(...) when running in a script.\nawait Console(stream)\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Invoking Function Calls Inside an Agent in C#\nDESCRIPTION: This snippet demonstrates how to invoke a function call inside an agent using a function wrapper. It creates a WeatherReportWrapper and passes it to the agent via functionMap, allowing the agent to directly invoke the function.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Use-function-call.md#2025-04-22_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\nFunc<WeatherReportArg, string> WeatherReportWrapper = args => $\"Weather report for {args.Location} on {args.Date}: Sunny\";\n\nvar agent = new AssistantAgent(\n    \"assistant\",\n    new List<AgentCapability> { AgentCapability.Functions },\n    new AgentConfig\n    {\n        Temperature = 0,\n        MaxTokens = 2000,\n        FunctionCall = \"auto\",\n        Functions = new[] { WeatherReport },\n        FunctionMap = new Dictionary<string, Delegate> { { \"get_weather_report\", WeatherReportWrapper } }\n    });\n\nvar reply = await agent.GenerateReplyAsync(\"What's the weather like in Seattle today?\");\nConsole.WriteLine(reply);\n```\n\n----------------------------------------\n\nTITLE: Implementing Single-Tenant Multiple Scope Communication for Tax Specialists in Python\nDESCRIPTION: This function sets up a single-tenant environment with multiple tax specialties. It registers tax specialist agents for each specialty, adds subscriptions, and publishes client requests to specific topics. The runtime manages message distribution based on subscriptions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/topic-subscription-scenarios.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nasync def run_single_tenant_multiple_scope() -> None:\n    # Create the runtime\n    runtime = SingleThreadedAgentRuntime()\n    # Register TaxSpecialist agents for each specialty and add subscriptions\n    for specialty in TaxSpecialty:\n        specialist_agent_type = f\"TaxSpecialist_{specialty.value}\"\n        await TaxSpecialist.register(\n            runtime=runtime,\n            type=specialist_agent_type,\n            factory=lambda specialty=specialty: TaxSpecialist(  # type: ignore\n                description=f\"A tax specialist in {specialty.value}.\",\n                specialty=specialty,\n                system_messages=[SystemMessage(content=f\"You are a tax specialist in {specialty.value}.\")],\n            ),\n        )\n        specialist_subscription = TypeSubscription(topic_type=specialty.value, agent_type=specialist_agent_type)\n        await runtime.add_subscription(specialist_subscription)\n\n    # Start the runtime\n    runtime.start()\n\n    # Publish a ClientRequest to each specialist's topic\n    for specialty in TaxSpecialty:\n        topic_id = TopicId(type=specialty.value, source=\"default\")\n        await runtime.publish_message(\n            ClientRequest(f\"I need assistance with {specialty.value} taxes.\"),\n            topic_id=topic_id,\n        )\n\n    # Allow time for message processing\n    await asyncio.sleep(1)\n\n    # Stop the runtime when idle\n    await runtime.stop_when_idle()\n\n\nawait run_single_tenant_multiple_scope()\n```\n\n----------------------------------------\n\nTITLE: Defining a C# Code Snippet in AutoGen\nDESCRIPTION: This snippet demonstrates how to define a code snippet in an agent response using C#. It includes a language identifier for proper recognition.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Run-dotnet-code.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/RunCodeSnippetCodeSnippet.cs?name=code_snippet_1_3)]\n```\n\n----------------------------------------\n\nTITLE: Installing Azure OpenAI Extension\nDESCRIPTION: Command to install the necessary extensions for using Azure OpenAI services with AutoGen.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install \"autogen-ext[openai,azure]\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing Group Chat Flow with Mermaid\nDESCRIPTION: Diagram showing the relationship between different agents (Admin, Coder, Reviewer, Runner) in the group chat system.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Group-chat.md#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n  subgraph Group Chat\n    B[Amin]\n    C[Coder]\n    D[Reviewer]\n    E[Runner]\n  end\n```\n\n----------------------------------------\n\nTITLE: Sending Weather Query Messages to Agent\nDESCRIPTION: Example of interacting with the agent to get weather information using the implemented function\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/MistralChatAgent-use-function-call.md#2025-04-22_snippet_5\n\nLANGUAGE: csharp\nCODE:\n```\nawait mistralAgent.SendMessageAsync(\"What's the weather like in Seattle?\");\n// The agent will automatically call the GetWeather function with \"Seattle\" as the location parameter\n```\n\n----------------------------------------\n\nTITLE: Short-Circuit Middleware Implementation in C#\nDESCRIPTION: Example of implementing a middleware that short-circuits the inner agent's execution by returning early.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Middleware-overview.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\nvar agent = new MiddlewareAgent(originalAgent, async (context, next) =>\n{\n    // Short circuit the inner agent\n    return new TextMessage(\"Short circuit the inner agent\");\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Agent in Python\nDESCRIPTION: This snippet demonstrates how to create a custom agent by subclassing RoutedAgent and implementing a message handler for a specific message type.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/agent-and-agent-runtime.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, message_handler\n\n\n@dataclass\nclass MyMessageType:\n    content: str\n\n\nclass MyAgent(RoutedAgent):\n    def __init__(self) -> None:\n        super().__init__(\"MyAgent\")\n\n    @message_handler\n    async def handle_my_message_type(self, message: MyMessageType, ctx: MessageContext) -> None:\n        print(f\"{self.id.type} received message: {message.content}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing parallel tool calls in C#\nDESCRIPTION: Example of using parallel tool calls, where multiple tools are invoked in a single message and processed sequentially by the FunctionCallMiddleware.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Create-agent-with-tools.md#2025-04-22_snippet_5\n\nLANGUAGE: csharp\nCODE:\n```\nvar response = await agentWithTools.SendAsync(new UserMessage(\"What's the weather in Seattle and New York?\"));\nConsole.WriteLine(response);\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging for AutoGen AgentChat\nDESCRIPTION: Sets up basic logging configuration for AutoGen's AgentChat, enabling both trace logging and structured message logging. This configures handlers and log levels for debugging purposes.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/logging.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nfrom autogen_agentchat import EVENT_LOGGER_NAME, TRACE_LOGGER_NAME\n\nlogging.basicConfig(level=logging.WARNING)\n\n# For trace logging.\ntrace_logger = logging.getLogger(TRACE_LOGGER_NAME)\ntrace_logger.addHandler(logging.StreamHandler())\ntrace_logger.setLevel(logging.DEBUG)\n\n# For structured message logging, such as low-level messages between agents.\nevent_logger = logging.getLogger(EVENT_LOGGER_NAME)\nevent_logger.addHandler(logging.StreamHandler())\nevent_logger.setLevel(logging.DEBUG)\n```\n\n----------------------------------------\n\nTITLE: Defining GetWeatherAsync Function Schema in JSON\nDESCRIPTION: JSON schema that defines a weather retrieval function named GetWeatherAsync. The function requires a single parameter 'city' of string type and returns weather information for the specified city.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.Gemini.Tests/ApprovalTests/FunctionContractExtensionTests.ItGenerateGetWeatherToolTest.approved.txt#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"GetWeatherAsync\",\n  \"description\": \"Get weather for a city.\",\n  \"parameters\": {\n    \"type\": \"OBJECT\",\n    \"properties\": {\n      \"city\": {\n        \"type\": \"STRING\",\n        \"description\": \"city\",\n        \"title\": \"city\"\n      }\n    },\n    \"required\": [\n      \"city\"\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Registering Sales Agent with Topic Subscription in AutoGen\nDESCRIPTION: Registers a sales agent with a specific agent topic type and system message that provides a sales routine for ACME Inc. The agent has tools for executing orders and transferring back to triage. The code also sets up a subscription for the agent to receive messages published to its own topic.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/handoffs.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Register the sales agent.\nsales_agent_type = await AIAgent.register(\n    runtime,\n    type=sales_agent_topic_type,  # Using the topic type as the agent type.\n    factory=lambda: AIAgent(\n        description=\"A sales agent.\",\n        system_message=SystemMessage(\n            content=\"You are a sales agent for ACME Inc.\"\n            \"Always answer in a sentence or less.\"\n            \"Follow the following routine with the user:\"\n            \"1. Ask them about any problems in their life related to catching roadrunners.\\n\"\n            \"2. Casually mention one of ACME's crazy made-up products can help.\\n\"\n            \" - Don't mention price.\\n\"\n            \"3. Once the user is bought in, drop a ridiculous price.\\n\"\n            \"4. Only after everything, and if the user says yes, \"\n            \"tell them a crazy caveat and execute their order.\\n\"\n            \"\"\n        ),\n        model_client=model_client,\n        tools=[execute_order_tool],\n        delegate_tools=[transfer_back_to_triage_tool],\n        agent_topic_type=sales_agent_topic_type,\n        user_topic_type=user_topic_type,\n    ),\n)\n# Add subscriptions for the sales agent: it will receive messages published to its own topic only.\nawait runtime.add_subscription(TypeSubscription(topic_type=sales_agent_topic_type, agent_type=sales_agent_type.type))\n```\n\n----------------------------------------\n\nTITLE: Tool Use Implementation in AutoGen v0.4\nDESCRIPTION: Simplified tool integration using a single AssistantAgent in v0.4. Shows direct tool integration and chat loop implementation with async support.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import TextMessage\n\ndef get_weather(city: str) -> str: # Async tool is possible too.\n    return f\"The weather in {city} is 72 degree and sunny.\"\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant. You can call tools to help user.\",\n        model_client=model_client,\n        tools=[get_weather],\n        reflect_on_tool_use=True, # Set to True to have the model reflect on the tool use, set to False to return the tool call result directly.\n    )\n    while True:\n        user_input = input(\"User: \")\n        if user_input == \"exit\":\n            break\n        response = await assistant.on_messages([TextMessage(content=user_input, source=\"user\")], CancellationToken())\n        print(\"Assistant:\", response.chat_message.to_text())\n    await model_client.close()\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAI Assistant and Vector Store\nDESCRIPTION: Initialization of OpenAI assistant with code interpreter and file search capabilities, along with vector store setup for file searching functionality.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/openai-assistant-agent.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\n# Create an assistant with code interpreter and file search tools.\noai_assistant = openai.beta.assistants.create(\n    model=\"gpt-4o-mini\",\n    description=\"An AI assistant that helps with everyday tasks.\",\n    instructions=\"Help the user with their task.\",\n    tools=[{\"type\": \"code_interpreter\"}, {\"type\": \"file_search\"}],\n)\n\n# Create a vector store to be used for file search.\nvector_store = openai.vector_stores.create()\n\n# Create a thread which is used as the memory for the assistant.\nthread = openai.beta.threads.create(\n    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Admin Agent in C#\nDESCRIPTION: Code for creating the admin agent that manages task creation and conversation termination. The admin agent orchestrates the overall flow of the group chat.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Group-chat.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\ncreate_admin\n```\n\n----------------------------------------\n\nTITLE: Displaying Generated Plot in Jupyter Notebook\nDESCRIPTION: Shows how to display the generated stock returns plot using IPython's Image display functionality. Loads the image from the temporary work directory where it was saved by the executed code.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/code-execution-groupchat.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Image\n\nImage(filename=f\"{work_dir}/nvidia_vs_tesla_ytd_returns.png\")  # type: ignore\n```\n\n----------------------------------------\n\nTITLE: Running Worker Runtime with Semantic Router in Python\nDESCRIPTION: This command initiates the Worker Runtime, responsible for managing distributed agents and the semantic router for routing user requests to appropriate agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_semantic_router/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython run_semantic_router.py\n```\n\n----------------------------------------\n\nTITLE: Connecting to OpenAI-Compatible APIs in AutoGen v0.4\nDESCRIPTION: Shows how to use the OpenAIChatCompletionClient to connect to an OpenAI-Compatible API by specifying base_url and model_info parameters.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\ncustom_model_client = OpenAIChatCompletionClient(\n    model=\"custom-model-name\",\n    base_url=\"https://custom-model.com/reset/of/the/path\",\n    api_key=\"placeholder\",\n    model_info={\n        \"vision\": True,\n        \"function_calling\": True,\n        \"json_output\": True,\n        \"family\": \"unknown\",\n        \"structured_output\": True,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Extension for OpenAI and Azure Support\nDESCRIPTION: Commands to install the AutoGen extension package with OpenAI and Azure support, needed for using Azure OpenAI models or models hosted on OpenAI-compatible API endpoints.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_streamlit/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install \"autogen-ext[openai,azure]\"\n# pip install \"autogen-ext[openai]\" for OpenAI models\n```\n\n----------------------------------------\n\nTITLE: Streaming Messages with PrintMessageMiddleware in C#\nDESCRIPTION: This snippet demonstrates the use of @AutoGen.Core.PrintMessageMiddleware to support streaming message types (@AutoGen.Core.TextMessageUpdate and @AutoGen.Core.ToolCallMessageUpdate). If registered to a @AutoGen.Core.IStreamingAgent, it formats and prints supported @AutoGen.Core.IMessage types to the console. This supports dynamic message handling in real-time environments.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Print-message-middleware.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp\\[\\](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/PrintMessageMiddlewareCodeSnippet.cs?name=print_message_streaming)]\n```\n\n----------------------------------------\n\nTITLE: Defining Message Protocol Dataclasses in Python\nDESCRIPTION: Defines Python dataclasses (`TextMessage`, `Reset`, `UploadForCodeInterpreter`, `UploadForFileSearch`) to structure the communication protocol for an OpenAI Assistant agent. `TextMessage` handles standard text communication, `Reset` clears agent memory, and the `Upload` classes manage file uploads for different assistant tools. Requires the `dataclasses` module.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/openai-assistant-agent.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass TextMessage:\n    content: str\n    source: str\n\n\n@dataclass\nclass Reset:\n    pass\n\n\n@dataclass\nclass UploadForCodeInterpreter:\n    file_path: str\n\n\n@dataclass\nclass UploadForFileSearch:\n    file_path: str\n    vector_store_id: str\n```\n\n----------------------------------------\n\nTITLE: AutoGen Agent Team Configuration Example (JSON)\nDESCRIPTION: This JSON structure represents the configuration of an AutoGen agent team, specifically a `RoundRobinGroupChat`. It details the team's provider, version, label, and configuration, including the participant agents (like `AssistantAgent` with its `OpenAIChatCompletionClient` model), tools (empty in this case), and the `TextMentionTermination` condition. This format is generated by `dump_component()` and can be used in AutoGen Studio's JSON editor or loaded programmatically.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/usage.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"provider\": \"autogen_agentchat.teams.RoundRobinGroupChat\",\n  \"component_type\": \"team\",\n  \"version\": 1,\n  \"component_version\": 1,\n  \"description\": \"A team that runs a group chat with participants taking turns in a round-robin fashion\\n    to publish a message to all.\",\n  \"label\": \"RoundRobinGroupChat\",\n  \"config\": {\n    \"participants\": [\n      {\n        \"provider\": \"autogen_agentchat.agents.AssistantAgent\",\n        \"component_type\": \"agent\",\n        \"version\": 1,\n        \"component_version\": 1,\n        \"description\": \"An agent that provides assistance with tool use.\",\n        \"label\": \"AssistantAgent\",\n        \"config\": {\n          \"name\": \"weather_agent\",\n          \"model_client\": {\n            \"provider\": \"autogen_ext.models.openai.OpenAIChatCompletionClient\",\n            \"component_type\": \"model\",\n            \"version\": 1,\n            \"component_version\": 1,\n            \"description\": \"Chat completion client for OpenAI hosted models.\",\n            \"label\": \"OpenAIChatCompletionClient\",\n            \"config\": { \"model\": \"gpt-4o-mini\" }\n          },\n          \"tools\": [],\n          \"handoffs\": [],\n          \"model_context\": {\n            \"provider\": \"autogen_core.model_context.UnboundedChatCompletionContext\",\n            \"component_type\": \"chat_completion_context\",\n            \"version\": 1,\n            \"component_version\": 1,\n            \"description\": \"An unbounded chat completion context that keeps a view of the all the messages.\",\n            \"label\": \"UnboundedChatCompletionContext\",\n            \"config\": {}\n          },\n          \"description\": \"An agent that provides assistance with ability to use tools.\",\n          \"system_message\": \"You are a helpful AI assistant. Solve tasks using your tools. Reply with TERMINATE when the task has been completed.\",\n          \"model_client_stream\": false,\n          \"reflect_on_tool_use\": false,\n          \"tool_call_summary_format\": \"{result}\"\n        }\n      }\n    ],\n    \"termination_condition\": {\n      \"provider\": \"autogen_agentchat.conditions.TextMentionTermination\",\n      \"component_type\": \"termination\",\n      \"version\": 1,\n      \"component_version\": 1,\n      \"description\": \"Terminate the conversation if a specific text is mentioned.\",\n      \"label\": \"TextMentionTermination\",\n      \"config\": { \"text\": \"TERMINATE\" }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Final Streaming Message Result in C#\nDESCRIPTION: Shows how to process streaming calls that return a final result message instead of updates, eliminating the need for manual result assembly.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Built-in-messages.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nvar stream = someStreamingCall();\nawait foreach (var message in stream)\n{\n    if (message is TextMessageUpdate textUpdate)\n    {\n        Console.Write(textUpdate.Text);\n    }\n    else if (message is TextMessage textMessage)\n    {\n        // Use the final result directly\n        var finalResult = textMessage.Text;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Chat with SemanticKernel Agent\nDESCRIPTION: Shows how to implement streaming chat functionality using SemanticKernelAgent via IStreamingAgent.GenerateStreamingReplyAsync interface.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.SemanticKernel/SemanticKernelAgent-simple-chat.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/SemanticKernelCodeSnippet.cs?name=create_semantic_kernel_agent_streaming)]\n```\n\n----------------------------------------\n\nTITLE: Implementing Sequential Group Chat\nDESCRIPTION: Code snippet showing the creation of a RoundRobinGroupChat that combines the search and summarization agents into a sequential workflow.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Roundrobin-chat.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[]](../../samples/AgentChat/Autogen.Basic.Sample/Example11_Sequential_GroupChat_Example.cs?name=Sequential_GroupChat_Example)\n```\n\n----------------------------------------\n\nTITLE: Executing Python Code Blocks in Docker with AutoGen (Python)\nDESCRIPTION: This snippet demonstrates how to use the DockerCommandLineCodeExecutor to execute one or more Python code blocks inside a Docker container using the AutoGen framework. It requires the autogen-ext[docker] and autogen-core packages, with 'sh' and 'python' available in the Docker image. The executor manages a working directory, ensures Docker container cleanup, and calls the asynchronous execute_code_blocks function. The primary parameters include the list of CodeBlock objects, the target working directory, and a cancellation token. Code execution results are printed to the console. This method provides isolation and user control over the execution environment.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/command-line-code-executors.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nfrom autogen_core import CancellationToken\nfrom autogen_core.code_executor import CodeBlock\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\n\nwork_dir = Path(\"coding\")\nwork_dir.mkdir(exist_ok=True)\n\nasync with DockerCommandLineCodeExecutor(work_dir=work_dir) as executor:  # type: ignore\n    print(\n        await executor.execute_code_blocks(\n            code_blocks=[\n                CodeBlock(language=\"python\", code=\"print('Hello, World!')\"),\n            ],\n            cancellation_token=CancellationToken(),\n        )\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Mounting Docker Socket for Sibling Container Control (Bash)\nDESCRIPTION: This bash code snippet demonstrates how to use docker run with the -v flag to mount the host's Docker socket inside a container. This setup enables the containerized application (such as AutoGen) to perform Docker out of Docker operations, launching and controlling additional sibling containers via the host's Docker. No Python dependencies required; used as a runtime execution argument for Docker. Ensures the guest container can interact with the Docker daemon, but exposes Docker on the host to processes inside the container.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/command-line-code-executors.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n-v /var/run/docker.sock:/var/run/docker.sock\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring a GeminiChatAgent\nDESCRIPTION: Setup code for creating a GeminiChatAgent with configured function calling capabilities. The code registers the MovieFunction methods and sets up the agent with Google Cloud authentication.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Function-call-with-gemini.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\n// Create a MovieFunction instance\nvar movieFunction = new MovieFunction();\n\n// Create function contracts\nvar findMovieByDirectorContract = FunctionContract.FromMethod(movieFunction, nameof(movieFunction.FindMovieByDirector));\nvar getMovieReviewContract = FunctionContract.FromMethod(movieFunction, nameof(movieFunction.GetMovieReview));\n\n// Create a Gemini agent\nvar geminiAgent = new GeminiChatAgent(\n    \"Gemini\",\n    new VertexAIGeminiChatCompletionService(\n        // Set your Google Cloud project ID & location\n        new VertexAIGeminiChatCompletionOption\n        {\n            ModelId = \"gemini-1.5-flash\", // use any available Gemini model ID\n            ProjectId = \"your-gcp-project-id\",\n            Location = \"us-central1\", // choose the appropriate region\n            MaxOutputTokens = 8192\n        }));\n\n// Register the function contracts to the agent\ngeminiAgent.AddFunctionContract(findMovieByDirectorContract);\ngeminiAgent.AddFunctionContract(getMovieReviewContract);\n```\n\n----------------------------------------\n\nTITLE: Implementing Math Aggregator Agent in Python\nDESCRIPTION: Defines the MathAggregator class that handles distributing math problems to solver agents and aggregating their responses. The aggregator receives questions, broadcasts them to solvers, and determines the final answer through majority voting.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/multi-agent-debate.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@default_subscription\nclass MathAggregator(RoutedAgent):\n    def __init__(self, num_solvers: int) -> None:\n        super().__init__(\"Math Aggregator\")\n        self._num_solvers = num_solvers\n        self._buffer: List[FinalSolverResponse] = []\n\n    @message_handler\n    async def handle_question(self, message: Question, ctx: MessageContext) -> None:\n        print(f\"{'-'*80}\\nAggregator {self.id} received question:\\n{message.content}\")\n        prompt = (\n            f\"Can you solve the following math problem?\\n{message.content}\\n\"\n            \"Explain your reasoning. Your final answer should be a single numerical number, \"\n            \"in the form of {{answer}}, at the end of your response.\"\n        )\n        print(f\"{'-'*80}\\nAggregator {self.id} publishes initial solver request.\")\n        await self.publish_message(SolverRequest(content=prompt, question=message.content), topic_id=DefaultTopicId())\n\n    @message_handler\n    async def handle_final_solver_response(self, message: FinalSolverResponse, ctx: MessageContext) -> None:\n        self._buffer.append(message)\n        if len(self._buffer) == self._num_solvers:\n            print(f\"{'-'*80}\\nAggregator {self.id} received all final answers from {self._num_solvers} solvers.\")\n            # Find the majority answer.\n            answers = [resp.answer for resp in self._buffer]\n            majority_answer = max(set(answers), key=answers.count)\n            # Publish the aggregated response.\n            await self.publish_message(Answer(content=majority_answer), topic_id=DefaultTopicId())\n            # Clear the responses.\n            self._buffer.clear()\n            print(f\"{'-'*80}\\nAggregator {self.id} publishes final answer:\\n{majority_answer}\")\n```\n\n----------------------------------------\n\nTITLE: Migrating from GPTAgent to OpenAIChatAgent in C#\nDESCRIPTION: This snippet demonstrates how to migrate from the deprecated GPTAgent to the new OpenAIChatAgent with RegisterMessageConnector in AutoGen.Net v0.2.0.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/release_note/0.2.0.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nvar agent = new OpenAIChatAgent(...)\n    .RegisterMessageConnector();\n```\n\n----------------------------------------\n\nTITLE: Printing Source Content from Response (Python)\nDESCRIPTION: This code iterates through the sources in the agent's response (if any) and prints the content of each source.  It provides additional context for the agent's response.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/llamaindex-agent.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nif response.sources is not None:\n    for source in response.sources:\n        print(source.content)\n```\n\n----------------------------------------\n\nTITLE: Agent IDs Generated from Topic Source\nDESCRIPTION: Demonstrates how agent IDs are created based on the topic source in a single-tenant scenario.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/core-concepts/topic-and-subscription.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# The agent IDs created based on the topic source\nAgentID(\"triage_agent\", \"default\")\nAgentID(\"coder_agent\", \"default\")\nAgentID(\"reviewer_agent\", \"default\")\n```\n\n----------------------------------------\n\nTITLE: Launching AutoGen Studio UI\nDESCRIPTION: Command to start the AutoGen Studio user interface\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Use-AutoGen.Net-agent-as-model-in-AG-Studio.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nautogenstudio ui\n```\n\n----------------------------------------\n\nTITLE: Defining a Selector Prompt for Agent Selection\nDESCRIPTION: This snippet provides a custom prompt that guides the model in selecting the next agent based on the conversation context, ensuring that tasks are assigned correctly and sequentially.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/selector-group-chat.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nselector_prompt = \"\"\"Select an agent to perform task.\n\n{roles}\n\nCurrent conversation context:\n{history}\n\nRead the above conversation, then select an agent from {participants} to perform the next task.\nMake sure the planner agent has assigned tasks before other agents start working.\nOnly select one agent.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Task Data Structures\nDESCRIPTION: Defines dataclass structures for Tasks and TaskResponses used in the agent communication system.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/concurrent-agents.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass Task:\n    task_id: str\n\n\n@dataclass\nclass TaskResponse:\n    task_id: str\n    result: str\n```\n\n----------------------------------------\n\nTITLE: Executing the Sequential Workflow\nDESCRIPTION: Final execution of the workflow by starting the runtime and publishing an initial message to trigger the sequence.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/sequential-workflow.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nruntime.start()\n\nawait runtime.publish_message(\n    Message(content=\"An eco-friendly stainless steel water bottle that keeps drinks cold for 24 hours\"),\n    topic_id=TopicId(concept_extractor_topic_type, source=\"default\"),\n)\n\nawait runtime.stop_when_idle()\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry SDK\nDESCRIPTION: Command to install the core OpenTelemetry SDK using pip\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/telemetry.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install opentelemetry-sdk\n```\n\n----------------------------------------\n\nTITLE: Defining Approval Function for Critic Agent in Python\nDESCRIPTION: This code snippet defines a simple 'approve' function that will be used as a tool for the critic agent to signal approval of a message when all feedbacks have been addressed.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/termination.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef approve() -> None:\n    \"\"\"Approve the message when all feedbacks have been addressed.\"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Executing the Asynchronous Main Function (Python)\nDESCRIPTION: Executes the main asynchronous function `main()` defined previously. This call initiates the setup of agents, configuration of the chat team, and runs the simulation while capturing trace data using the configured OpenTelemetry setup. This typically needs to be run within an async context (e.g., using `asyncio.run(main())` or in a Jupyter notebook).\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tracing.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait main()\n```\n\n----------------------------------------\n\nTITLE: Use Gemini Agent in RoundRobinGroupChat\nDESCRIPTION: Demonstrates how to integrate the custom GeminiAssistantAgent into a RoundRobinGroupChat. It creates a primary agent, a Gemini-based critic agent, defines termination conditions, and then runs a team with these agents to perform a task.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/custom-agents.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n\n# Create the primary agent.\nprimary_agent = AssistantAgent(\n    \"primary\",\n    model_client=model_client,\n    system_message=\"You are a helpful AI assistant.\",\n)\n\n# Create a critic agent based on our new GeminiAssistantAgent.\ngemini_critic_agent = GeminiAssistantAgent(\n    \"gemini_critic\",\n    system_message=\"Provide constructive feedback. Respond with 'APPROVE' to when your feedbacks are addressed.\",\n)\n\n\n# Define a termination condition that stops the task if the critic approves or after 10 messages.\ntermination = TextMentionTermination(\"APPROVE\") | MaxMessageTermination(10)\n\n# Create a team with the primary and critic agents.\nteam = RoundRobinGroupChat([primary_agent, gemini_critic_agent], termination_condition=termination)\n\nawait Console(team.run_stream(task=\"Write a Haiku poem with 4 lines about the fall season.\"))\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Implementing Termination Conditions with AutoGen Agent Team\nDESCRIPTION: Sets up an AutoGen team with a lazy assistant agent that demonstrates handoff functionality. The code configures an OpenAI model client, creates an assistant agent with handoff settings, and establishes termination conditions for both handoff messages and specific text mentions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/human-in-the-loop.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.base import Handoff\nfrom autogen_agentchat.conditions import HandoffTermination, TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create an OpenAI model client.\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    # api_key=\"sk-...\", # Optional if you have an OPENAI_API_KEY env variable set.\n)\n\n# Create a lazy assistant agent that always hands off to the user.\nlazy_agent = AssistantAgent(\n    \"lazy_assistant\",\n    model_client=model_client,\n    handoffs=[Handoff(target=\"user\", message=\"Transfer to user.\")],\n    system_message=\"If you cannot complete the task, transfer to user. Otherwise, when finished, respond with 'TERMINATE'.\",\n)\n\n# Define a termination condition that checks for handoff messages.\nhandoff_termination = HandoffTermination(target=\"user\")\n# Define a termination condition that checks for a specific text mention.\ntext_termination = TextMentionTermination(\"TERMINATE\")\n\n# Create a single-agent team with the lazy assistant and both termination conditions.\nlazy_agent_team = RoundRobinGroupChat([lazy_agent], termination_condition=handoff_termination | text_termination)\n\n# Run the team and stream to the console.\ntask = \"What is the weather in New York?\"\nawait Console(lazy_agent_team.run_stream(task=task), output_stats=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Secret Values in Component Config - AutoGen\nDESCRIPTION: Shows how to use SecretStr from pydantic to handle sensitive configuration values that should not be exposed in config dumps.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/component-config.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, SecretStr\n\n\nclass ClientConfig(BaseModel):\n    endpoint: str\n    api_key: SecretStr\n```\n\n----------------------------------------\n\nTITLE: Defining the Refund Flight Tool in Python\nDESCRIPTION: Implement a function to handle flight refunds by returning a message confirming the action. This function is used by the Flights Refunder agent during task processing.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/swarm.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef refund_flight(flight_id: str) -> str:\n    \"\"\"Refund a flight\"\"\"\n    return f\"Flight {flight_id} refunded\"\n```\n\n----------------------------------------\n\nTITLE: Importing Required AutoGen and Dataclass Modules (Python)\nDESCRIPTION: Imports essential classes for creating AutoGen agents (`RoutedAgent`, `SingleThreadedAgentRuntime`), managing messages (`MessageContext`, `SystemMessage`, `UserMessage`, `AssistantMessage`), defining agent communication (`AgentId`, `DefaultTopicId`, `default_subscription`, `message_handler`), handling model clients (`ChatCompletionClient`, `OpenAIChatCompletionClient`), managing conversation history (`BufferedChatCompletionContext`), and using dataclasses (`dataclass`). These imports are prerequisites for the subsequent Python code defining and running the agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/local-llms-ollama-litellm.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\nfrom autogen_core import (\n    AgentId,\n    DefaultTopicId,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    default_subscription,\n    message_handler,\n)\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom autogen_core.models import (\n    AssistantMessage,\n    ChatCompletionClient,\n    SystemMessage,\n    UserMessage,\n)\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Code Execution\nDESCRIPTION: These bash commands install the necessary Python packages (ipykernel and jupyter) for running Python code snippets in AutoGen.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Run-dotnet-code.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install ipykernel\npip install jupyter\n```\n\n----------------------------------------\n\nTITLE: Creating Azure OpenAI Model Client in AutoGen v0.4\nDESCRIPTION: Demonstrates how to create an Azure OpenAI model client using the AzureOpenAIChatCompletionClient class in AutoGen v0.4, specifying Azure-specific parameters.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n\nmodel_client = AzureOpenAIChatCompletionClient(\n    azure_deployment=\"gpt-4o\",\n    azure_endpoint=\"https://<your-endpoint>.openai.azure.com/\",\n    model=\"gpt-4o\",\n    api_version=\"2024-09-01-preview\",\n    api_key=\"sk-xxx\",\n)\n```\n\n----------------------------------------\n\nTITLE: Imports and Setup for Termination Handling in Python\nDESCRIPTION: This snippet imports necessary modules and sets up a dataclass for messages that will be used in termination handling. Dependencies include the autogen_core module and standard Python libraries.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/termination-with-intervention.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom autogen_core import (\n    DefaultInterventionHandler,\n    DefaultTopicId,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    default_subscription,\n    message_handler,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents in Python Documentation\nDESCRIPTION: This code snippet configures the table of contents for the AutoGen Extensions documentation using the toctree directive. It sets up two separate tables of contents, one for main sections and another for guides.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/extensions-user-guide/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```{toctree}\n:maxdepth: 3\n:hidden:\n\ninstallation\ndiscover\ncreate-your-own\n```\n\n```{toctree}\n:maxdepth: 3\n:hidden:\n:caption: Guides\n\nazure-container-code-executor\n```\n```\n\n----------------------------------------\n\nTITLE: Serializing ChromaDBVectorMemory to JSON String in Python\nDESCRIPTION: This snippet calls the dump_component method on a ChromaDB vector memory instance and serializes its state to a JSON string using model_dump_json. This operation is useful for persisting or transferring the memory store. Input is a memory instance; output is its JSON serialization. It requires that chroma_user_memory has been previously initialized and contains data.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/memory.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nchroma_user_memory.dump_component().model_dump_json()\\n\n```\n\n----------------------------------------\n\nTITLE: Initiating MultiModal Chat\nDESCRIPTION: Implementation of sending multimodal messages (text and image) to the LLaVA model\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Ollama/Chat-with-llava.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../../samples/AutoGen.Ollama.Sample/Chat_With_LLaVA.cs?name=Send_Message)]\n```\n\n----------------------------------------\n\nTITLE: Running the Agent Conversation and Cleanup (Python)\nDESCRIPTION: Executes the defined agent interaction. It starts the `SingleThreadedAgentRuntime`, initiates the conversation by sending a `Message` from agent 'cathy' to agent 'joe', waits for the runtime to become idle (indicating the conversation has concluded based on agent logic or termination conditions), and finally closes the connection to the shared `model_client` to release resources.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/local-llms-ollama-litellm.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nruntime.start()\nawait runtime.send_message(\n    Message(\"Joe, tell me a joke.\"),\n    recipient=AgentId(joe, \"default\"),\n    sender=AgentId(cathy, \"default\"),\n)\nawait runtime.stop_when_idle()\n\n# Close the connections to the model clients.\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for LangGraph Agent - Python\nDESCRIPTION: This snippet imports all required standard and third-party Python modules and classes for defining agents, message structures, tools, and runtime environments for a LangGraph-based agent. It depends on prior installation of relevant libraries. The imported symbols enable message formatting, agent routing, Azure authentication, LLM interaction, and graph-based workflows.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/langgraph-agent.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Literal\n\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_core.tools import tool  # pyright: ignore\nfrom langchain_openai import AzureChatOpenAI, ChatOpenAI\nfrom langgraph.graph import END, MessagesState, StateGraph\nfrom langgraph.prebuilt import ToolNode\n```\n\n----------------------------------------\n\nTITLE: Starting the Web Application\nDESCRIPTION: Command to start the .NET web application that hosts the AutoGen agent\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Use-AutoGen.Net-agent-as-model-in-AG-Studio.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndotnet RUN\n```\n\n----------------------------------------\n\nTITLE: Model Client Cache with DiskCache in AutoGen v0.4\nDESCRIPTION: Demonstrates how to implement caching in AutoGen v0.4 using ChatCompletionCache with a DiskCacheStore, which is not enabled by default unlike in v0.2.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport tempfile\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE\nfrom autogen_ext.cache_store.diskcache import DiskCacheStore\nfrom diskcache import Cache\n\n\nasync def main():\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        # Initialize the original client\n        openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n        # Then initialize the CacheStore, in this case with diskcache.Cache.\n        # You can also use redis like:\n        # from autogen_ext.cache_store.redis import RedisStore\n        # import redis\n        # redis_instance = redis.Redis()\n        # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance)\n        cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))\n        cache_client = ChatCompletionCache(openai_model_client, cache_store)\n\n        response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n        print(response)  # Should print response from OpenAI\n        response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n        print(response)  # Should print cached response\n        await openai_model_client.close()\n\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Incorporating User Feedback in SelectorGroupChat with UserProxyAgent in Python\nDESCRIPTION: This code snippet shows how to add a UserProxyAgent to the team for user feedback and implement a custom selector function to handle user approval in the conversation flow.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/selector-group-chat.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nuser_proxy_agent = UserProxyAgent(\"UserProxyAgent\", description=\"A proxy for the user to approve or disapprove tasks.\")\n\ndef selector_func_with_user_proxy(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n    if messages[-1].source != planning_agent.name and messages[-1].source != user_proxy_agent.name:\n        # Planning agent should be the first to engage when given a new task, or check progress.\n        return planning_agent.name\n    if messages[-1].source == planning_agent.name:\n        if messages[-2].source == user_proxy_agent.name and \"APPROVE\" in messages[-1].content.upper():  # type: ignore\n            # User has approved the plan, proceed to the next agent.\n            return None\n        # Use the user proxy agent to get the user's approval to proceed.\n        return user_proxy_agent.name\n    if messages[-1].source == user_proxy_agent.name:\n        # If the user does not approve, return to the planning agent.\n        if \"APPROVE\" not in messages[-1].content.upper():  # type: ignore\n            return planning_agent.name\n    return None\n\n# Reset the previous agents and run the chat again with the user proxy agent and selector function.\nawait team.reset()\nteam = SelectorGroupChat(\n    [planning_agent, web_search_agent, data_analyst_agent, user_proxy_agent],\n    model_client=model_client,\n    termination_condition=termination,\n    selector_prompt=selector_prompt,\n    selector_func=selector_func_with_user_proxy,\n    allow_repeated_speaker=True,\n)\n\nawait Console(team.run_stream(task=task))\n```\n\n----------------------------------------\n\nTITLE: Implementing Nested Chat with Counting Agents\nDESCRIPTION: Demonstrates nested chat functionality by creating a hierarchy of counting agents. Includes a base counting agent that increments numbers and a nested agent that manages a team of counting agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nclass CountingAgent(BaseChatAgent):\n    \"\"\"An agent that returns a new number by adding 1 to the last number in the input messages.\"\"\"\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        if len(messages) == 0:\n            last_number = 0 # Start from 0 if no messages are given.\n        else:\n            assert isinstance(messages[-1], TextMessage)\n            last_number = int(messages[-1].content) # Otherwise, start from the last number.\n        return Response(chat_message=TextMessage(content=str(last_number + 1), source=self.name))\n\nclass NestedCountingAgent(BaseChatAgent):\n    \"\"\"An agent that increments the last number in the input messages\n    multiple times using a nested counting team.\"\"\"\n    def __init__(self, name: str, counting_team: RoundRobinGroupChat) -> None:\n        super().__init__(name, description=\"An agent that counts numbers.\")\n        self._counting_team = counting_team\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        result = await self._counting_team.run(task=messages, cancellation_token=cancellation_token)\n        assert isinstance(result.messages[-1], TextMessage)\n        return Response(chat_message=result.messages[-1], inner_messages=result.messages[len(messages):-1])\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Package in .NET Project\nDESCRIPTION: This command adds the AutoGen package to a .NET project using the dotnet CLI. It's the first step in integrating AutoGen functionality into a .NET application.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/getting-start.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AutoGen\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Assistant Agent with OpenAI GPT-4o\nDESCRIPTION: A simple 'Hello World' example showing how to create an assistant agent using OpenAI's GPT-4o model. This demonstrates the basic usage of the AssistantAgent class and OpenAIChatCompletionClient.\nSOURCE: https://github.com/microsoft/autogen/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    agent = AssistantAgent(\"assistant\", model_client=model_client)\n    print(await agent.run(task=\"Say 'Hello World!'\"))\n    await model_client.close()\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Defining AutoGen Package Dependencies with Version Constraints\nDESCRIPTION: Lists required Python packages with specific version constraints for the AutoGen project. The dependencies include autogen-agentchat package with version between 0.4.0 and 0.5, and autogen-ext package with graphrag, openai, and azure extensions, also constrained between versions 0.4.0 and 0.5.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_graphrag/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nautogen-agentchat>=0.4.0,<0.5\nautogen-ext[graphrag,openai,azure]>=0.4.0,<0.5\n```\n\n----------------------------------------\n\nTITLE: Running AutoGen Team with o3-mini Reasoning Model\nDESCRIPTION: This snippet shows how to execute the AutoGen team configured with the o3-mini reasoning model. It uses the Console function to run the team's task stream.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/selector-group-chat.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nawait Console(team.run_stream(task=task))\n```\n\n----------------------------------------\n\nTITLE: Starting FastAPI Server with Uvicorn\nDESCRIPTION: Command to launch the FastAPI application using uvicorn server with hot reload enabled.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_streaming_response_fastapi/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuvicorn app:app --host 0.0.0.0 --port 8501 --reload\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Packages\nDESCRIPTION: This snippet shows how to install AutoGen and AutoGen.SourceGenerator packages using dotnet CLI.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Function-call-with-ollama-and-litellm.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AutoGen\ndotnet add package AutoGen.SourceGenerator\n```\n\n----------------------------------------\n\nTITLE: Sending a Message and Printing Response - Python\nDESCRIPTION: Sends a direct message to the agent with the question 'What's the weather in SF?' and prints the resulting response content. Assumes the runtime, agent, and Message class are initialized and that the runtime is running. Useful for testing the agent's tool use response.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/langgraph-agent.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresponse = await runtime.send_message(Message(\"What's the weather in SF?\"), agent)\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Tool Use Implementation in AutoGen v0.2\nDESCRIPTION: Demonstrates tool integration using two agents (tool caller and executor) in v0.2. Includes function registration and chat loop implementation.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen.agentchat import AssistantAgent, UserProxyAgent, register_function\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\ntool_caller = AssistantAgent(\n    name=\"tool_caller\",\n    system_message=\"You are a helpful assistant. You can call tools to help user.\",\n    llm_config=llm_config,\n    max_consecutive_auto_reply=1, # Set to 1 so that we return to the application after each assistant reply as we are building a chatbot.\n)\n\ntool_executor = UserProxyAgent(\n    name=\"tool_executor\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    llm_config=False,\n)\n\ndef get_weather(city: str) -> str:\n    return f\"The weather in {city} is 72 degree and sunny.\"\n\n# Register the tool function to the tool caller and executor.\nregister_function(get_weather, caller=tool_caller, executor=tool_executor)\n\nwhile True:\n    user_input = input(\"User: \")\n    if user_input == \"exit\":\n        break\n    chat_result = tool_executor.initiate_chat(\n        tool_caller,\n        message=user_input,\n        summary_method=\"reflection_with_llm\", # To let the model reflect on the tool use, set to \"last_msg\" to return the tool call result directly.\n    )\n    print(\"Assistant:\", chat_result.summary)\n```\n\n----------------------------------------\n\nTITLE: Loading Team State and Querying Restored Context - autogen_agentchat - Python\nDESCRIPTION: This snippet demonstrates loading a previously saved team state back into the agent team, then running a query that relies on prior conversational memory. Dependencies include access to team_state and prior team instantiation. Inputs are the restored state and a specific follow-up task. Output is streamed to Console and demonstrates successful retrieval of chat history.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/state.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(team_state)\n\n# Load team state.\nawait agent_team.load_state(team_state)\nstream = agent_team.run_stream(task=\"What was the last line of the poem you wrote?\")\nawait Console(stream)\n```\n\n----------------------------------------\n\nTITLE: Implementing Result Collection\nDESCRIPTION: Sets up a ClosureAgent to collect and process results from both UrgentProcessor and NormalProcessor agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/concurrent-agents.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nqueue = asyncio.Queue[TaskResponse]()\n\n\nasync def collect_result(_agent: ClosureContext, message: TaskResponse, ctx: MessageContext) -> None:\n    await queue.put(message)\n\n\nruntime.start()\n\nCLOSURE_AGENT_TYPE = \"collect_result_agent\"\nawait ClosureAgent.register_closure(\n    runtime,\n    CLOSURE_AGENT_TYPE,\n    collect_result,\n    subscriptions=lambda: [TypeSubscription(topic_type=TASK_RESULTS_TOPIC_TYPE, agent_type=CLOSURE_AGENT_TYPE)],\n)\n\nawait runtime.publish_message(Task(task_id=\"normal-1\"), topic_id=TopicId(type=\"normal\", source=\"default\"))\nawait runtime.publish_message(Task(task_id=\"urgent-1\"), topic_id=TopicId(type=\"urgent\", source=\"default\"))\n\nawait runtime.stop_when_idle()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Candidate Function for SelectorGroupChat in Python\nDESCRIPTION: This snippet demonstrates how to implement a custom candidate function to filter the list of potential agents for speaker selection in each turn of the groupchat.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/selector-group-chat.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef candidate_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> List[str]:\n    # keep planning_agent first one to plan out the tasks\n    if messages[-1].source == \"user\":\n        return [planning_agent.name]\n\n    # if previous agent is planning_agent and if it explicitely asks for web_search_agent\n    # or data_analyst_agent or both (in-case of re-planning or re-assignment of tasks)\n    # then return those specific agents\n    last_message = messages[-1]\n    if last_message.source == planning_agent.name:\n        participants = []\n        if web_search_agent.name in last_message.to_text():\n            participants.append(web_search_agent.name)\n        if data_analyst_agent.name in last_message.to_text():\n            participants.append(data_analyst_agent.name)\n        if participants:\n            return participants  # SelectorGroupChat will select from the remaining two agents.\n\n    # we can assume that the task is finished once the web_search_agent\n    # and data_analyst_agent have took their turns, thus we send\n    # in planning_agent to terminate the chat\n    previous_set_of_agents = set(message.source for message in messages)\n    if web_search_agent.name in previous_set_of_agents and data_analyst_agent.name in previous_set_of_agents:\n        return [planning_agent.name]\n\n    # if no-conditions are met then return all the agents\n    return [planning_agent.name, web_search_agent.name, data_analyst_agent.name]\n\n# Reset the previous team and run the chat again with the selector function.\nawait team.reset()\nteam = SelectorGroupChat(\n    [planning_agent, web_search_agent, data_analyst_agent],\n    model_client=model_client,\n    termination_condition=termination,\n    candidate_func=candidate_func,\n)\n\nawait Console(team.run_stream(task=task))\n```\n\n----------------------------------------\n\nTITLE: Registering Agent Types with SingleThreadedAgentRuntime in Python\nDESCRIPTION: This code snippet demonstrates how to register custom agent types with the SingleThreadedAgentRuntime, allowing the runtime to create and manage agent instances.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/agent-and-agent-runtime.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core import SingleThreadedAgentRuntime\n\nruntime = SingleThreadedAgentRuntime()\nawait MyAgent.register(runtime, \"my_agent\", lambda: MyAgent())\nawait MyAssistant.register(runtime, \"my_assistant\", lambda: MyAssistant(\"my_assistant\"))\n```\n\n----------------------------------------\n\nTITLE: Caching Responses with DiskCacheStore in AutoGen\nDESCRIPTION: This shows how to implement result caching using DiskCacheStore with AutoGen's OpenAIChatCompletionClient to avoid unnecessary token usage. It includes preparing a temporary directory for disk cache and using ChatCompletionCache to retrieve cached responses.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/model-clients.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport asyncio\nimport tempfile\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.cache_store.diskcache import DiskCacheStore\nfrom autogen_ext.models.cache import CHAT_CACHE_VALUE_TYPE, ChatCompletionCache\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom diskcache import Cache\n\n\nasync def main() -> None:\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        # Initialize the original client\n        openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n        # Then initialize the CacheStore, in this case with diskcache.Cache.\n        # You can also use redis like:\n        # from autogen_ext.cache_store.redis import RedisStore\n        # import redis\n        # redis_instance = redis.Redis()\n        # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance)\n        cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))\n        cache_client = ChatCompletionCache(openai_model_client, cache_store)\n\n        response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n        print(response)  # Should print response from OpenAI\n        response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n        print(response)  # Should print cached response\n\n        await openai_model_client.close()\n        await cache_client.close()\n\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for AutoGen Handoffs\nDESCRIPTION: Required module imports for implementing the handoff pattern, including AutoGen core components, message types, and OpenAI integration.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/handoffs.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport uuid\nfrom typing import List, Tuple\n\nfrom autogen_core import (\n    FunctionCall,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    TopicId,\n    TypeSubscription,\n    message_handler,\n)\nfrom autogen_core.models import (\n    AssistantMessage,\n    ChatCompletionClient,\n    FunctionExecutionResult,\n    FunctionExecutionResultMessage,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\nfrom autogen_core.tools import FunctionTool, Tool\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom pydantic import BaseModel\n```\n\n----------------------------------------\n\nTITLE: Implementing Two-Agent Weather Chat with Function Calls in C#\nDESCRIPTION: This code sets up a two-agent chat where one agent acts as a function proxy to invoke function calls from another agent. It demonstrates how to create and configure the agents, define the chat flow, and process function calls between agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Use-function-call.md#2025-04-22_snippet_5\n\nLANGUAGE: csharp\nCODE:\n```\nvar user = new UserProxyAgent(\"user\");\n\nvar assistant = new AssistantAgent(\n    \"assistant\",\n    new List<AgentCapability> { AgentCapability.Functions },\n    new AgentConfig\n    {\n        Temperature = 0,\n        MaxTokens = 2000,\n        FunctionCall = \"auto\",\n        Functions = new[] { WeatherReport }\n    });\n\nvar functionProxy = new AssistantAgent(\n    \"function_proxy\",\n    new List<AgentCapability> { AgentCapability.Functions },\n    new AgentConfig\n    {\n        Temperature = 0,\n        MaxTokens = 2000,\n        FunctionCall = \"auto\",\n        Functions = new[] { WeatherReport },\n        FunctionMap = new Dictionary<string, Delegate> { { \"get_weather_report\", WeatherReportWrapper } }\n    });\n\nvar chat = new GroupChat(new[] { user, assistant, functionProxy }, user);\nchat.GroupChatName = \"WeatherChat\";\n\nvar manager = new ChatManager(chat);\n\nvar messages = await manager.StartChatAsync(\"What's the weather like in Seattle today?\");\n\nConsole.WriteLine(string.Join(\"\\n\", messages.Select(m => $\"{m.Role}: {m.Content}\")));\n```\n\n----------------------------------------\n\nTITLE: Defining Message Data Class - Python\nDESCRIPTION: Defines a simple data class called Message, which represents messages sent to and from the agent. The class has a single field, 'content', for storing the message string. It is used for typing throughout agent communication; no external dependencies are required except Python's dataclasses module and string type.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/langgraph-agent.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass Message:\n    content: str\n```\n\n----------------------------------------\n\nTITLE: Adding AutoGen.LMStudio Package Reference in XML\nDESCRIPTION: XML snippet showing how to add the AutoGen.LMStudio package reference to a project file. This is required to use the LMStudioAgent in your C# project.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Consume-LLM-server-from-LM-Studio.md#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<ItemGroup>\n    <PackageReference Include=\"AutoGen.LMStudio\" Version=\"AUTOGEN_LMSTUDIO_VERSION\" />\n</ItemGroup>\n```\n\n----------------------------------------\n\nTITLE: Defining the Assistant Agent Class (Python)\nDESCRIPTION: Defines a custom agent class `Assistant` inheriting from `autogen_core.RoutedAgent`. The `__init__` method initializes the agent's name, the model client, a system message defining its persona (a comedian), and a buffered context for conversation history. The `handle_message` method, decorated with `@message_handler`, processes incoming `Message` objects, adds them to the context, calls the LLM via the `_model_client`, prints the agent's response, checks for termination conditions (specific phrase or turn limit), adds the LLM's response to the context, and publishes the response as a new `Message` to the default topic.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/local-llms-ollama-litellm.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@default_subscription\nclass Assistant(RoutedAgent):\n    def __init__(self, name: str, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"An assistant agent.\")\n        self._model_client = model_client\n        self.name = name\n        self.count = 0\n        self._system_messages = [\n            SystemMessage(\n                content=f\"Your name is {name} and you are a part of a duo of comedians.\"\n                \"You laugh when you find the joke funny, else reply 'I need to go now'.\",\n            )\n        ]\n        self._model_context = BufferedChatCompletionContext(buffer_size=5)\n\n    @message_handler\n    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n        self.count += 1\n        await self._model_context.add_message(UserMessage(content=message.content, source=\"user\"))\n        result = await self._model_client.create(self._system_messages + await self._model_context.get_messages())\n\n        print(f\"\\n{self.name}: {message.content}\")\n\n        if \"I need to go\".lower() in message.content.lower() or self.count > 2:\n            return\n\n        await self._model_context.add_message(AssistantMessage(content=result.content, source=\"assistant\"))  # type: ignore\n        await self.publish_message(Message(content=result.content), DefaultTopicId())  # type: ignore\n```\n\n----------------------------------------\n\nTITLE: Resetting an AutoGen Team's State in Python\nDESCRIPTION: This snippet shows how to reset the state of an AutoGen team. Calling `await team.reset()` clears the conversation history and internal state of the team and its constituent agents, preparing it for a new, unrelated task.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/teams.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait team.reset()  # Reset the team for the next run.\n```\n\n----------------------------------------\n\nTITLE: Starting AutoGen Application Runtime and Publishing Message\nDESCRIPTION: Demonstrates how to start the AutoGen runtime locally and send a message to an agent using the App.PublishMessageAsync method.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/Hello/HelloAgent/README.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\n// send a message to the agent\nvar app = await App.PublishMessageAsync(\"HelloAgents\", new NewMessageReceived\n{\n    Message = \"World\"\n}, local: true);\n\nawait App.RuntimeApp!.WaitForShutdownAsync();\nawait app.WaitForShutdownAsync();\n```\n\n----------------------------------------\n\nTITLE: Restoring AssistantAgent from Saved State - autogen_agentchat - Python\nDESCRIPTION: This snippet shows how to re-instantiate an AssistantAgent, load its previously saved state, and interact with it to confirm that contextual memory has been restored. Dependencies mirror the initialization snippet. Inputs are the previously saved agent_state and a user query ('What was the last line of the previous poem you wrote'). The output is the agent's response based on the restored context.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/state.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-2024-08-06\")\n\nnew_assistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    system_message=\"You are a helpful assistant\",\n    model_client=model_client,\n)\nawait new_assistant_agent.load_state(agent_state)\n\n# Use asyncio.run(...) when running in a script.\nresponse = await new_assistant_agent.on_messages(\n    [TextMessage(content=\"What was the last line of the previous poem you wrote\", source=\"user\")], CancellationToken()\n)\nprint(response.chat_message)\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Building and Packing AutoGen.NET Projects with .NET CLI\nDESCRIPTION: Commands to restore dependencies, build the project in Release configuration, and create NuGet packages. This process generates both .nupkg and .snupkg files in the artifacts directory.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/PACKAGING.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet restore\ndotnet build --configuration Release --no-restore\n```\n\nLANGUAGE: bash\nCODE:\n```\ndotnet pack --configuration Release --no-build\n```\n\n----------------------------------------\n\nTITLE: Implementing a Type-Routed Agent in Python for AutoGen Core\nDESCRIPTION: Shows how to create a RoutedAgent subclass that handles different message types using the @message_handler decorator in AutoGen core.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/message-and-communication.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\n\nclass MyAgent(RoutedAgent):\n    @message_handler\n    async def on_text_message(self, message: TextMessage, ctx: MessageContext) -> None:\n        print(f\"Hello, {message.source}, you said {message.content}!\")\n\n    @message_handler\n    async def on_image_message(self, message: ImageMessage, ctx: MessageContext) -> None:\n        print(f\"Hello, {message.source}, you sent me {message.url}!\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Runtime and Agent Registration\nDESCRIPTION: Configuration and registration of all agents with the runtime environment, including model client setup.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/sequential-workflow.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o-mini\",\n    # api_key=\"YOUR_API_KEY\"\n)\n\nruntime = SingleThreadedAgentRuntime()\n\nawait ConceptExtractorAgent.register(\n    runtime, type=concept_extractor_topic_type, factory=lambda: ConceptExtractorAgent(model_client=model_client)\n)\n\nawait WriterAgent.register(runtime, type=writer_topic_type, factory=lambda: WriterAgent(model_client=model_client))\n\nawait FormatProofAgent.register(\n    runtime, type=format_proof_topic_type, factory=lambda: FormatProofAgent(model_client=model_client)\n)\n\nawait UserAgent.register(runtime, type=user_topic_type, factory=lambda: UserAgent())\n```\n\n----------------------------------------\n\nTITLE: Creating and Initializing MistralClientAgent in C#\nDESCRIPTION: Example of how to create a MistralClientAgent instance and start a chat interaction.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen-Mistral-Overview.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\ncreate_mistral_agent\n```\n\n----------------------------------------\n\nTITLE: Defining a message class for content handling\nDESCRIPTION: This dataclass defines a simple message structure with a single attribute, content, which holds the text of a message. It's a fundamental building block for handling user inputs and system messages.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/model-context.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass Message:\n    content: str\n```\n\n----------------------------------------\n\nTITLE: Installing Required NuGet Packages for AutoGen Mistral\nDESCRIPTION: Commands to install the necessary NuGet packages for AutoGen.Mistral and AutoGen.SourceGenerator\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/MistralChatAgent-use-function-call.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AutoGen.Mistral\ndotnet add package AutoGen.SourceGenerator\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAI Model Client Directly in AutoGen v0.4\nDESCRIPTION: Shows how to create an OpenAI model client directly using the OpenAIChatCompletionClient class in AutoGen v0.4.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=\"sk-xxx\")\n```\n\n----------------------------------------\n\nTITLE: Custom Event Handler Implementation\nDESCRIPTION: Implementation of a custom logging handler for structured events, including a dataclass definition for the event structure and a handler class to process the events.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/logging.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nfrom dataclasses import dataclass\n\n@dataclass\nclass MyEvent:\n    timestamp: str\n    message: str\n\nclass MyHandler(logging.Handler):\n    def __init__(self) -> None:\n        super().__init__()\n\n    def emit(self, record: logging.LogRecord) -> None:\n        try:\n            # Use the StructuredMessage if the message is an instance of it\n            if isinstance(record.msg, MyEvent):\n                print(f\"Timestamp: {record.msg.timestamp}, Message: {record.msg.message}\")\n        except Exception:\n            self.handleError(record)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Available Packages in Azure Code Executor\nDESCRIPTION: Shows how to get a list of pre-installed packages available in the Azure Code Executor environment.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/extensions-user-guide/azure-container-code-executor.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(executor.get_available_packages(cancellation_token))\n```\n\n----------------------------------------\n\nTITLE: Orchestrating the Literature Review Execution with Console UI (Python)\nDESCRIPTION: Runs the configured agent team in a live async console, prompting them to collaboratively generate a literature review on a specified topic. The call to model_client.close() ensures proper cleanup of allocated resources post-execution. Requires that previous steps have all objects properly initialized; input is the user-specified literature review request.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/literature-review.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nawait Console(\\n    team.run_stream(\\n        task=\"Write a literature review on no code tools for building multi agent ai systems\",\\n    )\\n)\\n\\nawait model_client.close()\\n\n```\n\n----------------------------------------\n\nTITLE: Registering MistralAITokenCounterMiddleware in C#\nDESCRIPTION: This code shows how to register the custom token counting middleware with the MistralClientAgent, ensuring proper order of middleware execution.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/MistralChatAgent-count-token-usage.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example14_MistralClientAgent_TokenCount.cs?name=register_middleware)]\n```\n\n----------------------------------------\n\nTITLE: Implementing Orchestrator Agent Class\nDESCRIPTION: Implementation of the OrchestratorAgent class that manages worker agents across multiple layers and aggregates results.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/mixture-of-agents.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass OrchestratorAgent(RoutedAgent):\n    def __init__(\n        self,\n        model_client: ChatCompletionClient,\n        worker_agent_types: List[str],\n        num_layers: int,\n    ) -> None:\n        super().__init__(description=\"Aggregator Agent\")\n        self._model_client = model_client\n        self._worker_agent_types = worker_agent_types\n        self._num_layers = num_layers\n\n    @message_handler\n    async def handle_task(self, message: UserTask, ctx: MessageContext) -> FinalResult:\n        print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nReceived task: {message.task}\")\n        # Create task for the first layer.\n        worker_task = WorkerTask(task=message.task, previous_results=[])\n        # Iterate over layers.\n        for i in range(self._num_layers - 1):\n            # Assign workers for this layer.\n            worker_ids = [\n                AgentId(worker_type, f\"{self.id.key}/layer_{i}/worker_{j}\")\n                for j, worker_type in enumerate(self._worker_agent_types)\n            ]\n            # Dispatch tasks to workers.\n            print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nDispatch to workers at layer {i}\")\n            results = await asyncio.gather(*[self.send_message(worker_task, worker_id) for worker_id in worker_ids])\n            print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nReceived results from workers at layer {i}\")\n            # Prepare task for the next layer.\n            worker_task = WorkerTask(task=message.task, previous_results=[r.result for r in results])\n        # Perform final aggregation.\n        print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nPerforming final aggregation\")\n        system_prompt = \"You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\\n\\nResponses from models:\"\n        system_prompt += \"\\n\" + \"\\n\\n\".join([f\"{i+1}. {r}\" for i, r in enumerate(worker_task.previous_results)])\n        model_result = await self._model_client.create(\n            [SystemMessage(content=system_prompt), UserMessage(content=message.task, source=\"user\")]\n        )\n        assert isinstance(model_result.content, str)\n        return FinalResult(result=model_result.content)\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Agent Class in Python\nDESCRIPTION: This code defines a custom agent class 'MyAgent' that inherits from RoutedAgent. The agent publishes a new message for every message it receives, up to a maximum of 5 messages. It uses decorators for default subscription and message handling.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/distributed-agent-runtime.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\nfrom autogen_core import DefaultTopicId, MessageContext, RoutedAgent, default_subscription, message_handler\n\n\n@dataclass\nclass MyMessage:\n    content: str\n\n\n@default_subscription\nclass MyAgent(RoutedAgent):\n    def __init__(self, name: str) -> None:\n        super().__init__(\"My agent\")\n        self._name = name\n        self._counter = 0\n\n    @message_handler\n    async def my_message_handler(self, message: MyMessage, ctx: MessageContext) -> None:\n        self._counter += 1\n        if self._counter > 5:\n            return\n        content = f\"{self._name}: Hello x {self._counter}\"\n        print(content)\n        await self.publish_message(MyMessage(content=content), DefaultTopicId())\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Instrumentation for OpenAI in Bash\nDESCRIPTION: This snippet shows the command to install the opentelemetry-instrumentation-openai Python package using pip. This package is required to enable OpenTelemetry tracing for applications that interact with the OpenAI API. Make sure pip and Python are installed on your system before running this command. Running this will allow you to generate and export observability data from OpenAI model calls in compatible Python projects.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/instrumenting.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install opentelemetry-instrumentation-openai\n```\n\n----------------------------------------\n\nTITLE: Implementing Web Search/Analysis Group Chat with Custom Selector in Python\nDESCRIPTION: This code defines a custom selector function and sets up a SelectorGroupChat with multiple agents to perform web search and analysis. The selector function determines the next speaker based on the current state of the conversation.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom typing import Sequence\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\ndef selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n    \"\"\"Custom selector function to determine the next speaker based on the current state.\"\"\"\n    if len(messages) == 0:\n        return \"searcher\"\n    elif messages[-1].sender == \"human\":\n        return \"searcher\"\n    elif messages[-1].sender == \"searcher\":\n        return \"analyst\"\n    elif messages[-1].sender == \"analyst\":\n        return \"writer\"\n    elif messages[-1].sender == \"writer\":\n        return None  # Let the LLM decide\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    searcher = AssistantAgent(\n        name=\"searcher\",\n        description=\"A web searcher.\",\n        system_message=\"You are a web searcher. You search the web for information.\",\n        model_client=model_client,\n    )\n\n    analyst = AssistantAgent(\n        name=\"analyst\",\n        description=\"An analyst.\",\n        system_message=\"You are an analyst. You analyze the search results.\",\n        model_client=model_client,\n    )\n\n    writer = AssistantAgent(\n        name=\"writer\",\n        description=\"A writer.\",\n        system_message=\"You are a writer. You write the final report based on the analysis.\",\n        model_client=model_client,\n    )\n\n    human = AssistantAgent(\n        name=\"human\",\n        description=\"A human.\",\n        system_message=\"You are a human. You provide the initial query and review the final report.\",\n        model_client=model_client,\n    )\n\n    # The termination condition is a combination of max messages and text mention.\n    termination = MaxMessageTermination(20) | TextMentionTermination(\"APPROVE\")\n\n    # Create the SelectorGroupChat with the custom selector function\n    group_chat = SelectorGroupChat(\n        [searcher, analyst, writer, human],\n        selector_func=selector_func,\n        termination_condition=termination,\n    )\n\n    # Start the group chat\n    stream = group_chat.run_stream(task=\"Research the latest advancements in quantum computing.\")\n    await Console(stream)\n\n    # Close the connection to the model client\n    await model_client.close()\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Import AutoGen Core Modules and Initialize Classes in Python\nDESCRIPTION: This snippet shows the importation of necessary modules from the AutoGen framework and initializes classes such as RoutedAgent which is crucial for agent communication and broadcasting. Dependents include asyncio, dataclasses data structure, as well as imported modules for message handling and agent functionality.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/topic-subscription-scenarios.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import List\n\nfrom autogen_core import (\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    TopicId,\n    TypeSubscription,\n    message_handler,\n)\nfrom autogen_core._default_subscription import DefaultSubscription\nfrom autogen_core._default_topic import DefaultTopicId\nfrom autogen_core.models import (\n    SystemMessage,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing UserAgent Class in Python\nDESCRIPTION: A proxy class for customer interaction that handles user login and agent responses. Manages user sessions and message routing.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/handoffs.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass UserAgent(RoutedAgent):\n    def __init__(self, description: str, user_topic_type: str, agent_topic_type: str) -> None:\n        super().__init__(description)\n        self._user_topic_type = user_topic_type\n        self._agent_topic_type = agent_topic_type\n\n    @message_handler\n    async def handle_user_login(self, message: UserLogin, ctx: MessageContext) -> None:\n        print(f\"{'-'*80}\\nUser login, session ID: {self.id.key}.\", flush=True)\n        user_input = input(\"User: \")\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{user_input}\")\n        await self.publish_message(\n            UserTask(context=[UserMessage(content=user_input, source=\"User\")]),\n            topic_id=TopicId(self._agent_topic_type, source=self.id.key),\n        )\n\n    @message_handler\n    async def handle_task_result(self, message: AgentResponse, ctx: MessageContext) -> None:\n        user_input = input(\"User (type 'exit' to close the session): \")\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{user_input}\", flush=True)\n        if user_input.strip().lower() == \"exit\":\n            print(f\"{'-'*80}\\nUser session ended, session ID: {self.id.key}.\")\n            return\n        message.context.append(UserMessage(content=user_input, source=\"User\"))\n        await self.publish_message(\n            UserTask(context=message.context), topic_id=TopicId(message.reply_to_topic_type, source=self.id.key)\n        )\n```\n\n----------------------------------------\n\nTITLE: Declaring Topic Types for Agent Communication\nDESCRIPTION: Definition of topic types that agents will subscribe to, enabling sequential communication flow between agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/sequential-workflow.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconcept_extractor_topic_type = \"ConceptExtractorAgent\"\nwriter_topic_type = \"WriterAgent\"\nformat_proof_topic_type = \"FormatProofAgent\"\nuser_topic_type = \"User\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Automodule for autogen_core\nDESCRIPTION: This snippet uses the Sphinx 'automodule' directive in reStructuredText to automatically generate documentation for the Python module named 'autogen_core'. The options ':members:', ':undoc-members:', ':show-inheritance:', and ':member-order: bysource' instruct Sphinx to include all members (even undocumented ones), show class inheritance diagrams, and order the documented items as they appear in the source code.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_core.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogen_core\n   :members:\n   :undoc-members:\n   :show-inheritance:\n   :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Creating Streaming Tokens with OpenAI Model Client\nDESCRIPTION: This example shows how to use the create_stream method from the OpenAIChatCompletionClient to receive streamed responses from an OpenAI model. It demonstrates handling both chunked string responses and the final response as a CreateResult object.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/model-clients.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom autogen_core.models import CreateResult, UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\")  # assuming OPENAI_API_KEY is set in the environment.\n\nmessages = [\n    UserMessage(content=\"Write a very short story about a dragon.\", source=\"user\"),\n]\n\n# Create a stream.\nstream = model_client.create_stream(messages=messages)\n\n# Iterate over the stream and print the responses.\nprint(\"Streamed responses:\")\nasync for chunk in stream:  # type: ignore\n    if isinstance(chunk, str):\n        # The chunk is a string.\n        print(chunk, flush=True, end=\"\")\n    else:\n        # The final chunk is a CreateResult object.\n        assert isinstance(chunk, CreateResult) and isinstance(chunk.content, str)\n        # The last response is a CreateResult object with the complete message.\n        print(\"\\n\\n------------\\n\")\n        print(\"The complete response:\", flush=True)\n        print(chunk.content, flush=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring GRPC Message Size in AutoGen\nDESCRIPTION: Example showing how to configure custom GRPC options for GrpcWorkerAgentRuntimeHost and GrpcWorkerAgentRuntime to override default message size limits. Demonstrates setting max send and receive message lengths.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/faqs.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Define custom gRPC options\nextra_grpc_config = [\n    (\"grpc.max_send_message_length\", new_max_size),\n    (\"grpc.max_receive_message_length\", new_max_size),\n]\n\n# Create instances of GrpcWorkerAgentRuntimeHost and GrpcWorkerAgentRuntime with the custom gRPC options\n\nhost = GrpcWorkerAgentRuntimeHost(address=host_address, extra_grpc_config=extra_grpc_config)\nworker1 = GrpcWorkerAgentRuntime(host_address=host_address, extra_grpc_config=extra_grpc_config)\n```\n\n----------------------------------------\n\nTITLE: Running the AutoGen GraphRAG Application\nDESCRIPTION: Command to execute the main application script that initializes the chat interface with the AutoGen team and GraphRAG capabilities.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_graphrag/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython app.py\n```\n\n----------------------------------------\n\nTITLE: Running the Single Agent Sample Application\nDESCRIPTION: Command to launch the single AssistantAgent chat interface using Chainlit. This sample demonstrates interaction with a single agent that can use tools and respond to user queries.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_chainlit/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nchainlit run app_agent.py -h\n```\n\n----------------------------------------\n\nTITLE: Using Default Topic and Subscription in Python\nDESCRIPTION: This snippet shows a simplified implementation of a broadcasting agent using DefaultTopicId and default_subscription decorator for scenarios where all agents publish and subscribe to all broadcasted messages in a single scope.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/message-and-communication.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core import DefaultTopicId, default_subscription\n\n\n@default_subscription\nclass BroadcastingAgentDefaultTopic(RoutedAgent):\n    @message_handler\n    async def on_my_message(self, message: Message, ctx: MessageContext) -> None:\n        # Publish a message to all agents in the same namespace.\n        await self.publish_message(\n            Message(\"Publishing a message from broadcasting agent!\"),\n            topic_id=DefaultTopicId(),\n        )\n```\n\n----------------------------------------\n\nTITLE: Loading an AutoGen Agent Team from JSON (Python)\nDESCRIPTION: This Python snippet shows how to load an AutoGen agent team configuration from a specified JSON file (`team.json`). It uses the standard `json` library to load the file content and then employs the `BaseGroupChat.load_component()` class method from `autogen_agentchat.teams` to instantiate the team object based on the loaded configuration data. This allows recreating agent teams defined externally, for instance, those configured via AutoGen Studio.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/usage.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\nimport json\nfrom autogen_agentchat.teams import BaseGroupChat\nteam_config = json.load(open(\"team.json\"))\nteam = BaseGroupChat.load_component(team_config)\n\n```\n\n----------------------------------------\n\nTITLE: Running Agent Host Runtime in Python\nDESCRIPTION: This command starts the Agent Host Runtime, which manages the eventing engine and pub/sub message system.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_semantic_router/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython run_host.py\n```\n\n----------------------------------------\n\nTITLE: Send Message to Chat Endpoint\nDESCRIPTION: This Python code snippet sends a message to the `/chat/completions` endpoint using the `requests` library. It constructs a JSON payload containing the user's message and a conversation ID, then prints the streaming response from the server, handling potential JSON decoding errors.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_streaming_handoffs_fastapi/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport json\nimport uuid\n\nurl = \"http://localhost:8501/chat/completions\"\nconversation_id = f\"conv-id\" # Generate a unique conversation ID for a different session.\n\ndef send_message(message_text):\n    data = {\n        'message': message_text,\n        'conversation_id': conversation_id\n    }\n    headers = {'Content-Type': 'application/json'}\n    try:\n        print(f\"\\n>>> User: {message_text}\")\n        print(\"<<< Assistant: \", end=\"\", flush=True)\n        response = requests.post(url, json=data, headers=headers, stream=True)\n        response.raise_for_status()\n        full_response = \"\"\n        for chunk in response.iter_content(chunk_size=None):\n            if chunk:\n                try:\n                    # Decode the chunk\n                    chunk_str = chunk.decode('utf-8')\n                    # Handle potential multiple JSON objects in a single chunk\n                    for line in chunk_str.strip().split('\\n'):\n                        if line:\n                            data = json.loads(line)\n                            # Check the new structure\n                            if 'content' in data and isinstance(data['content'], dict) and 'message' in data['content']:\n                                message_content = data['content']['message']\n                                message_type = data['content'].get('type', 'string') # Default to string if type is missing\n\n                                # Print based on type (optional, could just print message_content)\n                                if message_type == 'function':\n                                    print(f\"[{message_type.upper()}] {message_content}\", end='\\n', flush=True) # Print function calls on new lines for clarity\n                                    print(\"<<< Assistant: \", end=\"\", flush=True) # Reprint prefix for next string part\n                                else:\n                                    print(message_content, end='', flush=True)\n\n                                full_response += message_content # Append only the message part\n                            else:\n                                print(f\"\\nUnexpected chunk format: {line}\")\n\n                except json.JSONDecodeError:\n                    print(f\"\\nError decoding chunk/line: '{line if 'line' in locals() else chunk_str}'\")\n\n        print(\"\\n--- End of Response ---\")\n        return full_response\n\n    except requests.exceptions.RequestException as e:\n        print(f\"\\nError: {e}\")\n    except Exception as e:\n        print(f\"\\nAn unexpected error occurred: {e}\")\n\n# Start conversation\nsend_message(\"I want refund\")\n# Continue conversation (example)\n# send_message(\"I want the rocket my friend Amith bought.\")\n# send_message(\"They are the SpaceX 3000s\")\n# send_message(\"That sounds great, I'll take it!\")\n# send_message(\"Yes, I agree to the price and the caveat.\")\n\n```\n\n----------------------------------------\n\nTITLE: Installing Nightly Build of AutoGen.Net Packages\nDESCRIPTION: This command installs a specific version of an AutoGen.Net package from the nightly build feed. Replace AUTOGEN_PACKAGES with the desired package name and VERSION with the specific version number.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Installation.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AUTOGEN_PACKAGES VERSION\n```\n\n----------------------------------------\n\nTITLE: Local OpenAI-Compatible Model Client Configuration\nDESCRIPTION: This JSON defines the configuration for connecting to a local model server (like Ollama or vLLM) that provides an OpenAI-compatible endpoint. It specifies the provider, component type, version, description, label, model, model information, and the base URL of the local server.  The `model_info` field is crucial for custom models to ensure correct instantiation and usage by AutoGen Studio.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/faq.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"provider\": \"autogen_ext.models.openai.OpenAIChatCompletionClient\",\n  \"component_type\": \"model\",\n  \"version\": 1,\n  \"component_version\": 1,\n  \"description\": \"Chat completion client for OpenAI hosted models.\",\n  \"label\": \"OpenAIChatCompletionClient\",\n  \"config\": {\n    \"model\": \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n    \"model_info\": {\n      \"vision\": false,\n      \"function_calling\": true,\n      \"json_output\": false,\n      \"family\": \"unknown\",\n      \"structured_output\": true\n    },\n    \"base_url\": \"http://localhost:1234/v1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Output Structure for Literary Analysis Report\nDESCRIPTION: A JSON schema that defines the expected output format for the literary analysis report, including title, summary, rating, rating explanation and detailed findings.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_graphrag/prompts/community_report.txt#2025-04-22_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"title\": \"<report_title>\",\n    \"summary\": \"<executive_summary>\",\n    \"rating\": <threat_severity_rating>,\n    \"rating_explanation\": \"<rating_explanation>\"\n    \"findings\": \"[{\\\"summary\\\":\\\"<insight_1_summary>\\\", \\\"explanation\\\": \\\"<insight_1_explanation\\\"}, {\\\"summary\\\":\\\"<insight_2_summary>\\\", \\\"explanation\\\": \\\"<insight_2_explanation\\\"}]\"\n}\n```\n\n----------------------------------------\n\nTITLE: Registering Agents with Subscriptions in Python\nDESCRIPTION: This code demonstrates two ways to register subscriptions for agents: using the type_subscription decorator that automatically adds TypeSubscription when the agent is registered, and manually creating a TypeSubscription for an agent without the decorator.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/message-and-communication.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core import TypeSubscription\n\nruntime = SingleThreadedAgentRuntime()\n\n# Option 1: with type_subscription decorator\n# The type_subscription class decorator automatically adds a TypeSubscription to\n# the runtime when the agent is registered.\nawait ReceivingAgent.register(runtime, \"receiving_agent\", lambda: ReceivingAgent(\"Receiving Agent\"))\n\n# Option 2: with TypeSubscription\nawait BroadcastingAgent.register(runtime, \"broadcasting_agent\", lambda: BroadcastingAgent(\"Broadcasting Agent\"))\nawait runtime.add_subscription(TypeSubscription(topic_type=\"default\", agent_type=\"broadcasting_agent\"))\n\n# Start the runtime and publish a message.\nruntime.start()\nawait runtime.publish_message(\n    Message(\"Hello, World! From the runtime!\"), topic_id=TopicId(type=\"default\", source=\"default\")\n)\nawait runtime.stop_when_idle()\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Agent: CountDownAgent in Python\nDESCRIPTION: This code snippet demonstrates how to create a custom agent, CountDownAgent, by inheriting from BaseChatAgent and implementing the required abstract methods: on_messages, on_messages_stream, on_reset, and the produced_message_types property. The CountDownAgent counts down from a specified number and produces a stream of messages with the current count, using TextMessage objects.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/custom-agents.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import AsyncGenerator, List, Sequence\n\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.base import Response\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, TextMessage\nfrom autogen_core import CancellationToken\n\n\nclass CountDownAgent(BaseChatAgent):\n    def __init__(self, name: str, count: int = 3):\n        super().__init__(name, \"A simple agent that counts down.\")\n        self._count = count\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        # Calls the on_messages_stream.\n        response: Response | None = None\n        async for message in self.on_messages_stream(messages, cancellation_token):\n            if isinstance(message, Response):\n                response = message\n        assert response is not None\n        return response\n\n    async def on_messages_stream(\n        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken\n    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:\n        inner_messages: List[BaseAgentEvent | BaseChatMessage] = []\n        for i in range(self._count, 0, -1):\n            msg = TextMessage(content=f\"{i}...\", source=self.name)\n            inner_messages.append(msg)\n            yield msg\n        # The response is returned at the end of the stream.\n        # It contains the final message and all the inner messages.\n        yield Response(chat_message=TextMessage(content=\"Done!\", source=self.name), inner_messages=inner_messages)\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        pass\n\n\nasync def run_countdown_agent() -> None:\n    # Create a countdown agent.\n    countdown_agent = CountDownAgent(\"countdown\")\n\n    # Run the agent with a given task and stream the response.\n    async for message in countdown_agent.on_messages_stream([], CancellationToken()):\n        if isinstance(message, Response):\n            print(message.chat_message)\n        else:\n            print(message)\n\n\n# Use asyncio.run(run_countdown_agent()) when running in a script.\nawait run_countdown_agent()\n```\n\n----------------------------------------\n\nTITLE: Running Multiple Message Processors Example\nDESCRIPTION: Shows how to register and run UrgentProcessor and NormalProcessor agents to handle different types of tasks.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/concurrent-agents.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nruntime = SingleThreadedAgentRuntime()\n\nawait UrgentProcessor.register(runtime, \"urgent_processor\", lambda: UrgentProcessor(\"Urgent Processor\"))\nawait NormalProcessor.register(runtime, \"normal_processor\", lambda: NormalProcessor(\"Normal Processor\"))\n\nruntime.start()\n\nawait runtime.publish_message(Task(task_id=\"normal-1\"), topic_id=TopicId(type=\"normal\", source=\"default\"))\nawait runtime.publish_message(Task(task_id=\"urgent-1\"), topic_id=TopicId(type=\"urgent\", source=\"default\"))\n\nawait runtime.stop_when_idle()\n```\n\n----------------------------------------\n\nTITLE: Creating OllamaAgent Instance\nDESCRIPTION: Code to initialize and configure the OllamaAgent for LLaVA model\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Ollama/Chat-with-llava.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../../samples/AutoGen.Ollama.Sample/Chat_With_LLaVA.cs?name=Create_Ollama_Agent)]\n```\n\n----------------------------------------\n\nTITLE: Defining Message Protocol Classes\nDESCRIPTION: Defines dataclasses for message types used in communication between agents, including WorkerTask, WorkerTaskResult, UserTask, and FinalResult.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/mixture-of-agents.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass WorkerTask:\n    task: str\n    previous_results: List[str]\n\n\n@dataclass\nclass WorkerTaskResult:\n    result: str\n\n\n@dataclass\nclass UserTask:\n    task: str\n\n\n@dataclass\nclass FinalResult:\n    result: str\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Autodoc for autogen_ext.teams.magentic_one\nDESCRIPTION: This reStructuredText snippet employs the Sphinx `automodule` directive to automatically generate documentation from the specified Python module (`autogen_ext.teams.magentic_one`). The `:members:`, `:undoc-members:`, and `:show-inheritance:` options instruct Sphinx to include documentation for all members (functions, classes, etc.), even those without docstrings, and to display the inheritance diagram for classes within the module.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_ext.teams.magentic_one.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogen_ext.teams.magentic_one\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Two-Agent Chat Implementation in AutoGen v0.2\nDESCRIPTION: Shows how to create a two-agent chat system for code execution using AssistantAgent and UserProxyAgent in v0.2. Includes configuration for code execution and message termination.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen.coding import LocalCommandLineCodeExecutor\nfrom autogen.agentchat import AssistantAgent, UserProxyAgent\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\nassistant = AssistantAgent(\n    name=\"assistant\",\n    system_message=\"You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.\",\n    llm_config=llm_config,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n)\n\nuser_proxy = UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=10,\n    code_execution_config={\"code_executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\")},\n    llm_config=False,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n)\n\nchat_result = user_proxy.initiate_chat(assistant, message=\"Write a python script to print 'Hello, world!'\")\n# Intermediate messages are printed to the console directly.\nprint(chat_result)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Tracer Provider in Python\nDESCRIPTION: Sets up the OpenTelemetry tracing infrastructure in Python. It imports necessary classes, configures an OTLPSpanExporter to send traces via gRPC to a local Jaeger instance (running on localhost:4317), creates a TracerProvider with a service name ('autogen-test-agentchat'), uses a BatchSpanProcessor for efficient exporting, and registers the provider globally. This setup allows AutoGen to send trace data.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tracing.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\n\notel_exporter = OTLPSpanExporter(endpoint=\"http://localhost:4317\", insecure=True)\ntracer_provider = TracerProvider(resource=Resource({\"service.name\": \"autogen-test-agentchat\"}))\nspan_processor = BatchSpanProcessor(otel_exporter)\ntracer_provider.add_span_processor(span_processor)\ntrace.set_tracer_provider(tracer_provider)\n\n# we will get reference this tracer later using its service name\n# tracer = trace.get_tracer(\"autogen-test-agentchat\")\n```\n\n----------------------------------------\n\nTITLE: Running AutoGen Human-in-the-Loop Example\nDESCRIPTION: Command to execute the main Python script that demonstrates the human-in-the-loop functionality.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_async_human_in_the_loop/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Starting Agent Runtime (Python)\nDESCRIPTION: This code starts the `SingleThreadedAgentRuntime`, initiating the agent's operational state and enabling it to process messages.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/llamaindex-agent.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nruntime.start()\n```\n\n----------------------------------------\n\nTITLE: Runtime Setup and Termination Detection using Python\nDESCRIPTION: Configures a SingleThreadedAgentRuntime with a TerminationHandler to handle and detect process termination. Demonstrates the asynchronous workflow involved in managing agent registration, messaging, and runtime stopping.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/termination-with-intervention.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntermination_handler = TerminationHandler()\nruntime = SingleThreadedAgentRuntime(intervention_handlers=[termination_handler])\n\nawait AnAgent.register(runtime, \"my_agent\", AnAgent)\n\nruntime.start()\n\n# Publish more than 3 messages to trigger termination.\nawait runtime.publish_message(Message(\"hello\"), DefaultTopicId())\nawait runtime.publish_message(Message(\"hello\"), DefaultTopicId())\nawait runtime.publish_message(Message(\"hello\"), DefaultTopicId())\nawait runtime.publish_message(Message(\"hello\"), DefaultTopicId())\n\n# Wait for termination.\nawait runtime.stop_when(lambda: termination_handler.has_terminated)\n\nprint(termination_handler.termination_value)\n```\n\n----------------------------------------\n\nTITLE: Creating Agent Team Setup in Python\nDESCRIPTION: Configuration and initialization of the agent runtime environment with AI agents and subscriptions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/handoffs.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nruntime = SingleThreadedAgentRuntime()\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o-mini\",\n    # api_key=\"YOUR_API_KEY\",\n)\n\ntriage_agent_type = await AIAgent.register(\n    runtime,\n    type=triage_agent_topic_type,\n    factory=lambda: AIAgent(\n        description=\"A triage agent.\",\n        system_message=SystemMessage(\n            content=\"You are a customer service bot for ACME Inc. \"\n            \"Introduce yourself. Always be very brief. \"\n            \"Gather information to direct the customer to the right department. \"\n            \"But make your questions subtle and natural.\"\n        ),\n        model_client=model_client,\n        tools=[],\n        delegate_tools=[\n            transfer_to_issues_and_repairs_tool,\n            transfer_to_sales_agent_tool,\n            escalate_to_human_tool,\n        ],\n        agent_topic_type=triage_agent_topic_type,\n        user_topic_type=user_topic_type,\n    ),\n)\nawait runtime.add_subscription(TypeSubscription(topic_type=triage_agent_topic_type, agent_type=triage_agent_type.type))\n```\n\n----------------------------------------\n\nTITLE: Message Format Conversion Utilities between v0.2 and v0.4\nDESCRIPTION: Comprehensive utility functions for converting message formats between AutoGen v0.2 and v0.4, including support for text, multimodal, tool calls, and execution events.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, List, Literal\n\nfrom autogen_agentchat.messages import (\n    BaseAgentEvent,\n    BaseChatMessage,\n    HandoffMessage,\n    MultiModalMessage,\n    StopMessage,\n    TextMessage,\n    ToolCallExecutionEvent,\n    ToolCallRequestEvent,\n    ToolCallSummaryMessage,\n)\nfrom autogen_core import FunctionCall, Image\nfrom autogen_core.models import FunctionExecutionResult\n\n\ndef convert_to_v02_message(\n    message: BaseAgentEvent | BaseChatMessage,\n    role: Literal[\"assistant\", \"user\", \"tool\"],\n    image_detail: Literal[\"auto\", \"high\", \"low\"] = \"auto\",\n) -> Dict[str, Any]:\n    \"\"\"Convert a v0.4 AgentChat message to a v0.2 message.\n\n    Args:\n        message (BaseAgentEvent | BaseChatMessage): The message to convert.\n        role (Literal[\"assistant\", \"user\", \"tool\"]): The role of the message.\n        image_detail (Literal[\"auto\", \"high\", \"low\"], optional): The detail level of image content in multi-modal message. Defaults to \"auto\".\n\n    Returns:\n        Dict[str, Any]: The converted AutoGen v0.2 message.\n    \"\"\"\n    v02_message: Dict[str, Any] = {}\n    if isinstance(message, TextMessage | StopMessage | HandoffMessage | ToolCallSummaryMessage):\n        v02_message = {\"content\": message.content, \"role\": role, \"name\": message.source}\n    elif isinstance(message, MultiModalMessage):\n        v02_message = {\"content\": [], \"role\": role, \"name\": message.source}\n        for modal in message.content:\n            if isinstance(modal, str):\n                v02_message[\"content\"].append({\"type\": \"text\", \"text\": modal})\n            elif isinstance(modal, Image):\n                v02_message[\"content\"].append(modal.to_openai_format(detail=image_detail))\n            else:\n                raise ValueError(f\"Invalid multimodal message content: {modal}\")\n    elif isinstance(message, ToolCallRequestEvent):\n        v02_message = {\"tool_calls\": [], \"role\": \"assistant\", \"content\": None, \"name\": message.source}\n        for tool_call in message.content:\n            v02_message[\"tool_calls\"].append(\n                {\n                    \"id\": tool_call.id,\n                    \"type\": \"function\",\n                    \"function\": {\"name\": tool_call.name, \"args\": tool_call.arguments},\n                }\n            )\n    elif isinstance(message, ToolCallExecutionEvent):\n        tool_responses: List[Dict[str, str]] = []\n        for tool_result in message.content:\n            tool_responses.append(\n                {\n                    \"tool_call_id\": tool_result.call_id,\n                    \"role\": \"tool\",\n                    \"content\": tool_result.content,\n                }\n            )\n        content = \"\\n\\n\".join([response[\"content\"] for response in tool_responses])\n        v02_message = {\"tool_responses\": tool_responses, \"role\": \"tool\", \"content\": content}\n    else:\n        raise ValueError(f\"Invalid message type: {type(message)}\")\n    return v02_message\n\n\ndef convert_to_v04_message(message: Dict[str, Any]) -> BaseAgentEvent | BaseChatMessage:\n    \"\"\"Convert a v0.2 message to a v0.4 AgentChat message.\"\"\"\n    if \"tool_calls\" in message:\n        tool_calls: List[FunctionCall] = []\n        for tool_call in message[\"tool_calls\"]:\n            tool_calls.append(\n                FunctionCall(\n                    id=tool_call[\"id\"],\n                    name=tool_call[\"function\"][\"name\"],\n                    arguments=tool_call[\"function\"][\"args\"],\n                )\n            )\n        return ToolCallRequestEvent(source=message[\"name\"], content=tool_calls)\n    elif \"tool_responses\" in message:\n        tool_results: List[FunctionExecutionResult] = []\n        for tool_response in message[\"tool_responses\"]:\n            tool_results.append(\n                FunctionExecutionResult(\n                    call_id=tool_response[\"tool_call_id\"],\n                    content=tool_response[\"content\"],\n                    is_error=False,\n                    name=tool_response[\"name\"],\n                )\n            )\n        return ToolCallExecutionEvent(source=\"tools\", content=tool_results)\n    elif isinstance(message[\"content\"], list):\n        content: List[str | Image] = []\n        for modal in message[\"content\"]:  # type: ignore\n            if modal[\"type\"] == \"text\":  # type: ignore\n                content.append(modal[\"text\"])  # type: ignore\n            else:\n                content.append(Image.from_uri(modal[\"image_url\"][\"url\"]))  # type: ignore\n        return MultiModalMessage(content=content, source=message[\"name\"])\n    elif isinstance(message[\"content\"], str):\n        return TextMessage(content=message[\"content\"], source=message[\"name\"])\n    else:\n        raise ValueError(f\"Unable to convert message: {message}\")\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen with Task-Centric Memory Extensions\nDESCRIPTION: Command to install AutoGen and its extension package with task-centric memory and OpenAI capabilities.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/task_centric_memory/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"autogen-agentchat\" \"autogen-ext[openai]\" \"autogen-ext[task-centric-memory]\"\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen.Net Packages using .NET CLI\nDESCRIPTION: This command installs the specified AutoGen.Net package using the .NET CLI. Replace AUTOGEN_PACKAGES with the desired package name.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Installation.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AUTOGEN_PACKAGES\n```\n\n----------------------------------------\n\nTITLE: Creating and Running SelectorGroupChat Team in Python\nDESCRIPTION: This snippet demonstrates how to create a SelectorGroupChat team with multiple agents, set termination conditions, and run a task to find information about an NBA player.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/selector-group-chat.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nteam = SelectorGroupChat(\n    [planning_agent, web_search_agent, data_analyst_agent],\n    model_client=model_client,\n    termination_condition=termination,\n    selector_prompt=selector_prompt,\n    allow_repeated_speaker=True,  # Allow an agent to speak multiple turns in a row.\n)\n\ntask = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\"\n\n# Use asyncio.run(...) if you are running this in a script.\nawait Console(team.run_stream(task=task))\n```\n\n----------------------------------------\n\nTITLE: Importing Modules for LlamaIndex Agent (Python)\nDESCRIPTION: This code imports necessary modules from autogen_core, azure.identity, llama_index.core, llama_index.embeddings, llama_index.llms, llama_index.tools, and pydantic. These modules are required for creating and managing agents, handling Azure credentials, working with LlamaIndex's core functionalities, and defining data models.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/llamaindex-agent.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom typing import List, Optional\n\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\nfrom llama_index.core import Settings\nfrom llama_index.core.agent import ReActAgent\nfrom llama_index.core.agent.runner.base import AgentRunner\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n    MessageRole,\n)\nfrom llama_index.core.chat_engine.types import AgentChatResponse\nfrom llama_index.core.memory import ChatSummaryMemoryBuffer\nfrom llama_index.core.memory.types import BaseMemory\nfrom llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.azure_openai import AzureOpenAI\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.tools.wikipedia import WikipediaToolSpec\nfrom pydantic import BaseModel\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Interaction with Gemini Agent\nDESCRIPTION: Example of how to interact with the Gemini model through the GeminiChatAgent. This code sends a message to the agent and retrieves the response, demonstrating a basic chat interaction.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Chat-with-google-gemini.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\nvar reply = await geminiAgent.GenerateReplyAsync(\n    \"What can you tell me about the Microsoft AutoGen project?\",\n    cancellationToken: default);\n\nConsole.WriteLine($\"Reply from {geminiAgent.Name}: {reply}\");\n```\n\n----------------------------------------\n\nTITLE: Running Individual Development Checks for AutoGen Python Packages\nDESCRIPTION: This snippet lists individual commands for running specific development checks such as formatting, linting, testing, and type checking.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/README.md#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npoe format\npoe lint\npoe test\npoe mypy\npoe pyright\npoe --directory ./packages/autogen-core/ docs-build\npoe --directory ./packages/autogen-core/ docs-serve\npoe samples-code-check\n```\n\n----------------------------------------\n\nTITLE: Integrating Task-Centric Memory with AutoGen RoutedAgent\nDESCRIPTION: Shows how to create a memory-enabled AutoGen agent by subclassing RoutedAgent and incorporating a MemoryController. The agent retrieves relevant memories for each user message and appends them to the model prompt for better context-aware responses.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-ext/src/autogen_ext/experimental/task_centric_memory/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import List\n\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\nfrom autogen_core.models import ChatCompletionClient, LLMMessage, SystemMessage, UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.experimental.task_centric_memory import MemoryController\nfrom autogen_ext.experimental.task_centric_memory.utils import PageLogger\n\n\n@dataclass\nclass Message:\n   content: str\n\n\nclass MemoryEnabledAgent(RoutedAgent):\n   def __init__(\n           self, description: str, model_client: ChatCompletionClient, memory_controller: MemoryController\n   ) -> None:\n      super().__init__(description)\n      self._model_client = model_client\n      self._memory_controller = memory_controller\n\n   @message_handler\n   async def handle_message(self, message: Message, context: MessageContext) -> Message:\n      # Retrieve relevant memories for the task.\n      memos = await self._memory_controller.retrieve_relevant_memos(task=message.content)\n\n      # Format the memories for the model.\n      formatted_memos = \"Info that may be useful:\\n\" + \"\\n\".join([\"- \" + memo.insight for memo in memos])\n      print(f\"{'-' * 23}Text appended to the user message{'-' * 24}\\n{formatted_memos}\\n{'-' * 80}\")\n\n      # Create the messages for the model with the retrieved memories.\n      messages: List[LLMMessage] = [\n         SystemMessage(content=\"You are a helpful assistant.\"),\n         UserMessage(content=message.content, source=\"user\"),\n         UserMessage(content=formatted_memos, source=\"user\"),\n      ]\n\n      # Call the model with the messages.\n      model_result = await self._model_client.create(messages=messages)\n      assert isinstance(model_result.content, str)\n\n      # Send the model's response to the user.\n      return Message(content=model_result.content)\n\n\nasync def main() -> None:\n   client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n   logger = PageLogger(config={\"level\": \"DEBUG\", \"path\": \"./pagelogs/quickstart2\"})  # Optional, but very useful.\n   memory_controller = MemoryController(reset=True, client=client, logger=logger)\n\n   # Prepopulate memory to mimic learning from a prior session.\n   await memory_controller.add_memo(task=\"What color do I like?\", insight=\"Deep blue is my favorite color\")\n   await memory_controller.add_memo(task=\"What's another color I like?\", insight=\"I really like cyan\")\n   await memory_controller.add_memo(task=\"What's my favorite food?\", insight=\"Halibut is my favorite\")\n\n   # Create and start an agent runtime.\n   runtime = SingleThreadedAgentRuntime()\n   runtime.start()\n\n   # Register the agent type.\n   await MemoryEnabledAgent.register(\n      runtime,\n      \"memory_enabled_agent\",\n      lambda: MemoryEnabledAgent(\n         \"A agent with memory\", model_client=client, memory_controller=memory_controller\n      ),\n   )\n\n   # Send a direct message to the agent.\n   request = \"What colors do I like most?\"\n   print(\"User request: \" + request)\n   response = await runtime.send_message(\n      Message(content=request), AgentId(\"memory_enabled_agent\", \"default\")\n   )\n   print(\"Agent response: \" + response.content)\n\n   # Stop the agent runtime.\n   await runtime.stop()\n\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with Sphinx\nDESCRIPTION: This command builds the AutoGen documentation using Sphinx from the specified directory within the repository. It requires the Sphinx and associated dependencies to be installed and configured. The output is stored in the documentation build directory.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoe --directory ./packages/autogen-core/ docs-build\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up Virtual Environment for AutoGen Python Development\nDESCRIPTION: These commands create and activate a virtual environment with all necessary dependencies for AutoGen development.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/README.md#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nuv sync --all-extras\nsource .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Running and Streaming Agent Response with Console Output in Python\nDESCRIPTION: This block shows how to initiate an agent task using the run_stream method and pipe the live stream to a Console UI for interaction. It assumes the agent (with memory and model) is already set up. The asynchronous invocation is expected, and the output appears in the console. Prerequisites are prior agent initialization and event loop context.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/memory.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Run the agent with a task.\\nstream = assistant_agent.run_stream(task=\\\"What is the weather in New York?\\\")\\nawait Console(stream)\\n\n```\n\n----------------------------------------\n\nTITLE: Running chat agent with OpenAI chat client\nDESCRIPTION: Shows initialization and runtime management of a simple agent using OpenAIChatCompletionClient. It demonstrates sending messages, retrieving responses, and handling chat context using a single-threaded runtime.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/model-context.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o-mini\",\n    # api_key=\"sk-...\", # Optional if you have an OPENAI_API_KEY set in the environment.\n)\n\nruntime = SingleThreadedAgentRuntime()\nawait SimpleAgentWithContext.register(\n    runtime,\n    \"simple_agent_context\",\n    lambda: SimpleAgentWithContext(model_client=model_client),\n)\n# Start the runtime processing messages.\nruntime.start()\nagent_id = AgentId(\"simple_agent_context\", \"default\")\n\n# First question.\nmessage = Message(\"Hello, what are some fun things to do in Seattle?\")\nprint(f\"Question: {message.content}\")\nresponse = await runtime.send_message(message, agent_id)\nprint(f\"Response: {response.content}\")\nprint(\"-----\")\n\n# Second question.\nmessage = Message(\"What was the first thing you mentioned?\")\nprint(f\"Question: {message.content}\")\nresponse = await runtime.send_message(message, agent_id)\nprint(f\"Response: {response.content}\")\n\n# Stop the runtime processing messages.\nawait runtime.stop()\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Importing Autogen Modules for Agent Chat\nDESCRIPTION: This snippet imports essential classes from the `autogen` library required for building the multi-agent travel planning system. It includes `AssistantAgent` for creating agents, `TextMentionTermination` for defining chat termination conditions, `RoundRobinGroupChat` for managing agent interactions, `Console` for UI interaction, and `OpenAIChatCompletionClient` for leveraging OpenAI models.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/travel-planning.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Component Class in Python - AutoGen\nDESCRIPTION: Demonstrates how to create a custom component class by implementing Component interface with required _to_config and _from_config methods.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/component-config.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core import Component, ComponentBase\nfrom pydantic import BaseModel\n\n\nclass Config(BaseModel):\n    value: str\n\n\nclass MyComponent(ComponentBase[Config], Component[Config]):\n    component_type = \"custom\"\n    component_config_schema = Config\n\n    def __init__(self, value: str):\n        self.value = value\n\n    def _to_config(self) -> Config:\n        return Config(value=self.value)\n\n    @classmethod\n    def _from_config(cls, config: Config) -> \"MyComponent\":\n        return cls(value=config.value)\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Agent with ChromaDB in v0.4\nDESCRIPTION: Demonstrates how to implement a RAG agent using ChromaDB vector memory store in v0.4.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nchroma_user_memory = ChromaDBVectorMemory(\n    config=PersistentChromaDBVectorMemoryConfig(\n        collection_name=\"preferences\",\n        persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"),\n        k=2,  # Return top  k results\n        score_threshold=0.4,  # Minimum similarity score\n    )\n)\n\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    model_client=OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n    ),\n    tools=[get_weather],\n    memory=[chroma_user_memory],\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing HumanEval Tasks with Python\nDESCRIPTION: Runs a Python script to initialize the HumanEval tasks, which downloads necessary data and creates task files.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/HumanEval/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython Scripts/init_tasks.py\n```\n\n----------------------------------------\n\nTITLE: Anthropic Model Client Configuration\nDESCRIPTION: This JSON defines the configuration for an Anthropic Claude model client within AutoGen Studio. It includes the provider, component type, version, description, label, and configuration parameters like `model`, `max_tokens`, `temperature`, and `api_key`.  The Anthropic API key is required if not already set in the environment.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/faq.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"provider\": \"autogen_ext.models.anthropic.AnthropicChatCompletionClient\",\n  \"component_type\": \"model\",\n  \"version\": 1,\n  \"component_version\": 1,\n  \"description\": \"Chat completion client for Anthropic's Claude models.\",\n  \"label\": \"AnthropicChatCompletionClient\",\n  \"config\": {\n    \"model\": \"claude-3-sonnet-20240229\",\n    \"max_tokens\": 4096,\n    \"temperature\": 1.0,\n    \"api_key\": \"your-api-key\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Usage Tracking Handler in Python\nDESCRIPTION: Custom logging handler class that tracks prompt and completion tokens for LLM calls. The handler processes LLMCallEvent instances and maintains running totals of token usage.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/llm-usage-logger.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nfrom autogen_core.logging import LLMCallEvent\n\n\nclass LLMUsageTracker(logging.Handler):\n    def __init__(self) -> None:\n        \"\"\"Logging handler that tracks the number of tokens used in the prompt and completion.\"\"\"\n        super().__init__()\n        self._prompt_tokens = 0\n        self._completion_tokens = 0\n\n    @property\n    def tokens(self) -> int:\n        return self._prompt_tokens + self._completion_tokens\n\n    @property\n    def prompt_tokens(self) -> int:\n        return self._prompt_tokens\n\n    @property\n    def completion_tokens(self) -> int:\n        return self._completion_tokens\n\n    def reset(self) -> None:\n        self._prompt_tokens = 0\n        self._completion_tokens = 0\n\n    def emit(self, record: logging.LogRecord) -> None:\n        \"\"\"Emit the log record. To be used by the logging module.\"\"\"\n        try:\n            # Use the StructuredMessage if the message is an instance of it\n            if isinstance(record.msg, LLMCallEvent):\n                event = record.msg\n                self._prompt_tokens += event.prompt_tokens\n                self._completion_tokens += event.completion_tokens\n        except Exception:\n            self.handleError(record)\n```\n\n----------------------------------------\n\nTITLE: Installing Anthropic Extension for AutoGen\nDESCRIPTION: Command to install the Anthropic extension for AutoGen to use Anthropic's Claude models.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# !pip install -U \"autogen-ext[anthropic]\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Stock Data and News Retrieval Tools in Python\nDESCRIPTION: Defines asynchronous functions to fetch stock market data and recent news articles. These tools are used by the specialized agents in the research swarm.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/swarm.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nasync def get_stock_data(symbol: str) -> Dict[str, Any]:\n    \"\"\"Get stock market data for a given symbol\"\"\"\n    return {\"price\": 180.25, \"volume\": 1000000, \"pe_ratio\": 65.4, \"market_cap\": \"700B\"}\n\n\nasync def get_news(query: str) -> List[Dict[str, str]]:\n    \"\"\"Get recent news articles about a company\"\"\"\n    return [\n        {\n            \"title\": \"Tesla Expands Cybertruck Production\",\n            \"date\": \"2024-03-20\",\n            \"summary\": \"Tesla ramps up Cybertruck manufacturing capacity at Gigafactory Texas, aiming to meet strong demand.\",\n        },\n        {\n            \"title\": \"Tesla FSD Beta Shows Promise\",\n            \"date\": \"2024-03-19\",\n            \"summary\": \"Latest Full Self-Driving beta demonstrates significant improvements in urban navigation and safety features.\",\n        },\n        {\n            \"title\": \"Model Y Dominates Global EV Sales\",\n            \"date\": \"2024-03-18\",\n            \"summary\": \"Tesla's Model Y becomes best-selling electric vehicle worldwide, capturing significant market share.\",\n        },\n    ]\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Functionality with Vertex AI Gemini\nDESCRIPTION: Implementation of the chat functionality using the GeminiChatAgent. This code demonstrates how to send messages to the Gemini model and receive responses through the Vertex AI API.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Chat-with-vertex-gemini.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\n// Initialize conversation with a system message\nvar systemMessage = \"You are a helpful AI assistant.\";\nvar messages = new List<Message> { new(systemMessage, Role.System) };\n\n// Send user message and get response\nstring userMessage = \"Tell me about the Gemini model.\";\nvar reply = await geminiAgent.GenerateReplyAsync(messages, userMessage);\n\n// Print the assistant's response\nConsole.WriteLine($\"Gemini: {reply}\");\n```\n\n----------------------------------------\n\nTITLE: Initializing AutoGen Studio Database\nDESCRIPTION: This code snippet shows how to create and initialize a database using the DatabaseManager class from autogenstudio.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-studio/notebooks/tutorial.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogenstudio.database import DatabaseManager\nimport os\n# delete database\n# if os.path.exists(\"test.db\"):\n#     os.remove(\"test.db\")\n\nos.makedirs(\"test\", exist_ok=True)\n# create a database\ndbmanager = DatabaseManager(engine_uri=\"sqlite:///test.db\", base_dir=\"test\")\ndbmanager.initialize_database()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Agent Teachability\nDESCRIPTION: Command to run the teachability evaluation that tests an agent's ability to learn from user advice and corrections.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/task_centric_memory/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython eval_teachability.py configs/teachability.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating and Interacting with a Conversable Agent in C#\nDESCRIPTION: This code snippet demonstrates how to create a conversable agent and chat with it using AutoGen in C#. It includes setting up the agent, initiating a conversation, and handling the response.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/getting-start.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/GetStartCodeSnippet.cs?name=snippet_GetStartCodeSnippet)]\n```\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/GetStartCodeSnippet.cs?name=code_snippet_1)]\n```\n\n----------------------------------------\n\nTITLE: Implementation of TerminationHandler in Python\nDESCRIPTION: Creates a TerminationHandler class using DefaultInterventionHandler that alters its state upon receiving a termination message. It determines if the runtime should terminate based on received messages.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/termination-with-intervention.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass TerminationHandler(DefaultInterventionHandler):\n    def __init__(self) -> None:\n        self._termination_value: Termination | None = None\n\n    async def on_publish(self, message: Any, *, message_context: MessageContext) -> Any:\n        if isinstance(message, Termination):\n            self._termination_value = message\n        return message\n\n    @property\n    def termination_value(self) -> Termination | None:\n        return self._termination_value\n\n    @property\n    def has_terminated(self) -> bool:\n        return self._termination_value is not None\n```\n\n----------------------------------------\n\nTITLE: Combining Termination Conditions with OR Operator in Python\nDESCRIPTION: Demonstrates combining termination conditions using the OR (|) operator. It stops the team when either the MaxMessageTermination of 10 messages or a TextMentionTermination of 'APPROVE' is triggered. Requires 'autogen_agentchat' for execution.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/termination.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmax_msg_termination = MaxMessageTermination(max_messages=10)\ntext_termination = TextMentionTermination(\"APPROVE\")\ncombined_termination = max_msg_termination | text_termination\n\nround_robin_team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=combined_termination)\n\n# Use asyncio.run(...) if you are running this script as a standalone script.\nawait Console(round_robin_team.run_stream(task=\"Write a unique, Haiku about the weather in Paris\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing Async Assistant Agent Usage in v0.4\nDESCRIPTION: Demonstrates the async implementation pattern for using AssistantAgent in v0.4 with cancellation support.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant.\",\n        model_client=model_client,\n    )\n\n    cancellation_token = CancellationToken()\n    response = await assistant.on_messages([TextMessage(content=\"Hello!\", source=\"user\")], cancellation_token)\n    print(response)\n\n    await model_client.close()\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Running the Streamlit Application\nDESCRIPTION: Command to start the Streamlit web application, which launches the AI chat assistant interface.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_streamlit/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nstreamlit run main.py\n```\n\n----------------------------------------\n\nTITLE: Flow Diagram of AutoGen .NET Sample Application using Mermaid\nDESCRIPTION: A Mermaid diagram illustrating the message flow between the main program and the HelloAgent, showing how events are published and handled in the AutoGen .NET application.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/Microsoft.AutoGen.Integration.Tests.AppHosts/HelloAgentTests/README.md#2025-04-22_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{init: {'theme':'forest'}}%%\ngraph LR;\n    A[Main] --> |\"PublishEventAsync(NewMessage('World'))\"| B{\"Handle(NewMessageReceived item, CancellationToken cancellationToken = default)\"}\n    B --> |\"PublishEventAsync(Output('***Hello, World***'))\"| C[ConsoleAgent]\n    C --> D{\"WriteConsole()\"}\n    B --> |\"PublishEventAsync(ConversationClosed('Goodbye'))\"| E{\"Handle(ConversationClosed item, CancellationToken cancellationToken = default)\"}\n    B --> |\"PublishEventAsync(Output('***Goodbye***'))\"| C\n    E --> F{\"Shutdown()\"}\n\n```\n\n----------------------------------------\n\nTITLE: Registering SemanticKernelChatMessageContentConnector in C#\nDESCRIPTION: This code snippet demonstrates how to register the SemanticKernelChatMessageContentConnector with an AutoGen agent. This allows the agent to support additional AutoGen built-in message types like TextMessage, ImageMessage, and MultiModalMessage, converting them to and from ChatMessageContent.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.SemanticKernel/SemanticKernelAgent-support-more-messages.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nvar agent = new ConversableAgent(\"User\", new DefaultLLM());\nagent.Register(new SemanticKernelChatMessageContentConnector());\n```\n\n----------------------------------------\n\nTITLE: Registering and Initializing LangGraph Tool Agent Runtime - Python\nDESCRIPTION: Sets up and registers the LangGraph tool use agent with the single-threaded agent runtime. The code demonstrates configuring both OpenAI and Azure LLM providers (commented options), creating the agent with tools, and registering it under a unique name. Requires prior agent and runtime class definitions, and configured LLM credentials.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/langgraph-agent.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nruntime = SingleThreadedAgentRuntime()\nawait LangGraphToolUseAgent.register(\n    runtime,\n    \"langgraph_tool_use_agent\",\n    lambda: LangGraphToolUseAgent(\n        \"Tool use agent\",\n        ChatOpenAI(\n            model=\"gpt-4o\",\n            # api_key=os.getenv(\"OPENAI_API_KEY\"),\n        ),\n        # AzureChatOpenAI(\n        #     azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n        #     azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n        #     api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n        #     # Using Azure Active Directory authentication.\n        #     azure_ad_token_provider=get_bearer_token_provider(DefaultAzureCredential()),\n        #     # Using API key.\n        #     # api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n        # ),\n        [get_weather],\n    ),\n)\nagent = AgentId(\"langgraph_tool_use_agent\", key=\"default\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Gemini Agent for Vertex AI Integration\nDESCRIPTION: Code snippet for creating a GeminiChatAgent instance configured to connect to Vertex AI. This setup establishes the connection with the Gemini model through Google's Vertex AI platform.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Chat-with-vertex-gemini.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\nvar geminiAgent = new GeminiChatAgent(\n    new GeminiVertexConfig(\"YOUR_PROJECT_ID\", \"YOUR_LOCATION\"),\n    \"gemini-1.0-pro\", // model name\n    \"Gemini\" // agent name\n);\n```\n\n----------------------------------------\n\nTITLE: Streaming Middleware Registration in C#\nDESCRIPTION: Demonstrates how to register streaming middleware for handling streaming responses and message conversions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Middleware-overview.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\nstreamingAgent.UseStreaming(async (context, next) =>\n{\n    // Do something before calling next agent\n    await foreach (var update in next(context))\n    {\n        // Do something with the update\n        yield return update;\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Building a Web Browsing Agent Team with MultimodalWebSurfer\nDESCRIPTION: Example showing how to create a team with a web surfer agent and a user proxy agent for web browsing tasks. This demonstrates the use of RoundRobinGroupChat for managing agent interactions and requires playwright to be installed.\nSOURCE: https://github.com/microsoft/autogen/blob/main/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# pip install -U autogen-agentchat autogen-ext[openai,web-surfer]\n# playwright install\nimport asyncio\nfrom autogen_agentchat.agents import UserProxyAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    # The web surfer will open a Chromium browser window to perform web browsing tasks.\n    web_surfer = MultimodalWebSurfer(\"web_surfer\", model_client, headless=False, animate_actions=True)\n    # The user proxy agent is used to get user input after each step of the web surfer.\n    # NOTE: you can skip input by pressing Enter.\n    user_proxy = UserProxyAgent(\"user_proxy\")\n    # The termination condition is set to end the conversation when the user types 'exit'.\n    termination = TextMentionTermination(\"exit\", sources=[\"user_proxy\"])\n    # Web surfer and user proxy take turns in a round-robin fashion.\n    team = RoundRobinGroupChat([web_surfer, user_proxy], termination_condition=termination)\n    try:\n        # Start the team and wait for it to terminate.\n        await Console(team.run_stream(task=\"Find information about AutoGen and write a short summary.\"))\n    finally:\n        await web_surfer.close()\n        await model_client.close()\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Downloading Files from Azure Code Executor\nDESCRIPTION: Shows how to create files in the Azure Code Executor environment and download them to the local system, verifying their contents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/extensions-user-guide/azure-container-code-executor.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith tempfile.TemporaryDirectory() as temp_dir:\n    test_file_1 = \"test_upload_1.txt\"\n    test_file_1_contents = \"test1 contents\"\n    test_file_2 = \"test_upload_2.txt\"\n    test_file_2_contents = \"test2 contents\"\n\n    assert not os.path.isfile(os.path.join(temp_dir, test_file_1))\n    assert not os.path.isfile(os.path.join(temp_dir, test_file_2))\n\n    executor = ACADynamicSessionsCodeExecutor(\n        pool_management_endpoint=POOL_MANAGEMENT_ENDPOINT, credential=DefaultAzureCredential(), work_dir=temp_dir\n    )\n\n    code_blocks = [\n        CodeBlock(\n            code=f\"\"\"\nwith open(\"{test_file_1}\", \"w\") as f:\n  f.write(\"{test_file_1_contents}\")\nwith open(\"{test_file_2}\", \"w\") as f:\n  f.write(\"{test_file_2_contents}\")\n\"\"\",\n            language=\"python\",\n        ),\n    ]\n    code_result = await executor.execute_code_blocks(code_blocks, cancellation_token)\n    assert code_result.exit_code == 0\n\n    file_list = await executor.get_file_list(cancellation_token)\n    assert test_file_1 in file_list\n    assert test_file_2 in file_list\n\n    await executor.download_files([test_file_1, test_file_2], cancellation_token)\n\n    assert os.path.isfile(os.path.join(temp_dir, test_file_1))\n    async with await open_file(os.path.join(temp_dir, test_file_1), \"r\") as f:  # type: ignore[syntax]\n        content = await f.read()\n        assert test_file_1_contents in content\n    assert os.path.isfile(os.path.join(temp_dir, test_file_2))\n    async with await open_file(os.path.join(temp_dir, test_file_2), \"r\") as f:  # type: ignore[syntax]\n        content = await f.read()\n        assert test_file_2_contents in content\n```\n\n----------------------------------------\n\nTITLE: Adding Required Using Statements for Gemini Integration\nDESCRIPTION: Necessary using statements for working with the Gemini API in C#. These namespaces provide access to AutoGen functionality and Gemini-specific components.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Chat-with-vertex-gemini.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nusing AutoGen.Gemini;\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAIChatAgent with Function Call Support in C#\nDESCRIPTION: This snippet demonstrates how to create an OpenAIChatAgent and register it with OpenAIChatRequestMessageConnector to support ToolCallMessage and ToolCallResultMessage. These are necessary for using FunctionCallMiddleware.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-use-function-call.md#2025-04-22_snippet_3\n\nLANGUAGE: C#\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/OpenAICodeSnippet.cs?name=openai_chat_agent_get_weather_function_call)]\n```\n\n----------------------------------------\n\nTITLE: Running Code Snippet with Dotnet Interactive in C#\nDESCRIPTION: This C# code snippet shows how to use the RunSubmitCodeCommandAsync method to execute a code snippet and return its result.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Run-dotnet-code.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/RunCodeSnippetCodeSnippet.cs?name=code_snippet_1_2)]\n```\n\n----------------------------------------\n\nTITLE: Implementing a Receiving Agent with Type Subscription in Python\nDESCRIPTION: This code demonstrates how to create a ReceivingAgent class that subscribes to topics of 'default' topic type using the type_subscription decorator. The agent includes a message handler that prints received messages.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/message-and-communication.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core import RoutedAgent, message_handler, type_subscription\n\n\n@type_subscription(topic_type=\"default\")\nclass ReceivingAgent(RoutedAgent):\n    @message_handler\n    async def on_my_message(self, message: Message, ctx: MessageContext) -> None:\n        print(f\"Received a message: {message.content}\")\n```\n\n----------------------------------------\n\nTITLE: Creating User Proxy Agent in v0.2\nDESCRIPTION: Shows the creation of a UserProxyAgent in v0.2 with various configuration options.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen.agentchat import UserProxyAgent\n\nuser_proxy = UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=10,\n    code_execution_config=False,\n    llm_config=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Using Multi-Modal Input with AssistantAgent in Python\nDESCRIPTION: This snippet demonstrates how to create and use a MultiModalMessage with an AssistantAgent. It creates a multi-modal message containing an image and text, and then runs the agent with this message as the task. The agent will then process both the image and the text in its response.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/agents.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom io import BytesIO\n\nimport PIL\nimport requests\nfrom autogen_agentchat.messages import MultiModalMessage\nfrom autogen_core import Image\n\n# Create a multi-modal message with random image and text.\npil_image = PIL.Image.open(BytesIO(requests.get(\"https://picsum.photos/300/200\").content))\nimg = Image(pil_image)\nmulti_modal_message = MultiModalMessage(content=[\"Can you describe the content of this image?\", img], source=\"user\")\nimg\n```\n\n----------------------------------------\n\nTITLE: Initializing ChromaDB Vector Memory for AutoGen RAG\nDESCRIPTION: This snippet sets up a ChromaDB vector memory for storing and retrieving document embeddings. It configures the memory with specific parameters and clears existing data.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/memory.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nrag_memory = ChromaDBVectorMemory(\n    config=PersistentChromaDBVectorMemoryConfig(\n        collection_name=\"autogen_docs\",\n        persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"),\n        k=3,  # Return top 3 results\n        score_threshold=0.4,  # Minimum similarity score\n    )\n)\n\nawait rag_memory.clear()  # Clear existing memory\n```\n\n----------------------------------------\n\nTITLE: Implementing ToolUseAgent Class with AutoGen Core API\nDESCRIPTION: Defines a ToolUseAgent class that inherits from RoutedAgent to handle tool execution and model interactions. The agent can process user messages, make function calls to tools, and generate reflections on tool execution results.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/tools.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom dataclasses import dataclass\nfrom typing import List\n\nfrom autogen_core import (\n    AgentId,\n    FunctionCall,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    message_handler,\n)\nfrom autogen_core.models import (\n    ChatCompletionClient,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\nfrom autogen_core.tools import FunctionTool, Tool\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\n@dataclass\nclass Message:\n    content: str\n\n\nclass ToolUseAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient, tool_schema: List[Tool]) -> None:\n        super().__init__(\"An agent with tools\")\n        self._system_messages: List[LLMMessage] = [SystemMessage(content=\"You are a helpful AI assistant.\")]\n        self._model_client = model_client\n        self._tools = tool_schema\n\n    @message_handler\n    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n        # Create a session of messages.\n        session: List[LLMMessage] = self._system_messages + [UserMessage(content=message.content, source=\"user\")]\n\n        # Run the chat completion with the tools.\n        create_result = await self._model_client.create(\n            messages=session,\n            tools=self._tools,\n            cancellation_token=ctx.cancellation_token,\n        )\n\n        # If there are no tool calls, return the result.\n        if isinstance(create_result.content, str):\n            return Message(content=create_result.content)\n        assert isinstance(create_result.content, list) and all(\n            isinstance(call, FunctionCall) for call in create_result.content\n        )\n\n        # Add the first model create result to the session.\n        session.append(AssistantMessage(content=create_result.content, source=\"assistant\"))\n\n        # Execute the tool calls.\n        results = await asyncio.gather(\n            *[self._execute_tool_call(call, ctx.cancellation_token) for call in create_result.content]\n        )\n\n        # Add the function execution results to the session.\n        session.append(FunctionExecutionResultMessage(content=results))\n\n        # Run the chat completion again to reflect on the history and function execution results.\n        create_result = await self._model_client.create(\n            messages=session,\n            cancellation_token=ctx.cancellation_token,\n        )\n        assert isinstance(create_result.content, str)\n\n        # Return the result as a message.\n        return Message(content=create_result.content)\n\n    async def _execute_tool_call(\n        self, call: FunctionCall, cancellation_token: CancellationToken\n    ) -> FunctionExecutionResult:\n        # Find the tool by name.\n        tool = next((tool for tool in self._tools if tool.name == call.name), None)\n        assert tool is not None\n\n        # Run the tool and capture the result.\n        try:\n            arguments = json.loads(call.arguments)\n            result = await tool.run_json(arguments, cancellation_token)\n            return FunctionExecutionResult(\n                call_id=call.id, content=tool.return_value_as_string(result), is_error=False, name=tool.name\n            )\n        except Exception as e:\n            return FunctionExecutionResult(call_id=call.id, content=str(e), is_error=True, name=tool.name)\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat with OpenAIChatAgent in C#\nDESCRIPTION: This snippet illustrates how to use the streaming functionality of OpenAIChatAgent. It sets up the agent for streaming and demonstrates how to process the streamed response chunks asynchronously.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-simple-chat.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\nvar agent = new OpenAIChatAgent(\"gpt-3.5-turbo\", \"your-api-key-here\");\nawait foreach (var chunk in agent.GenerateStreamingReplyAsync(\"Tell me a joke\"))\n{\n    Console.Write(chunk);\n}\nConsole.WriteLine();\n```\n\n----------------------------------------\n\nTITLE: Defining a FinalResult dataclass for agent message payloads in Python\nDESCRIPTION: Defines a dataclass 'FinalResult' with a single string field 'value' to encapsulate the result of the multi-agent system. This class serves as the type for messages passed between agents and published externally. Requires the dataclasses module and serves as the schema for queue communication.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/extracting-results-with-an-agent.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\\nclass FinalResult:\\n    value: str\n```\n\n----------------------------------------\n\nTITLE: Running Frontend Development Server with Yarn\nDESCRIPTION: Commands to install dependencies and start the development server. Includes options for both local development and container-based development with external access.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-studio/frontend/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\nyarn start               # local development\nyarn start --host 0.0.0.0  # in container (enables external access)\n```\n\n----------------------------------------\n\nTITLE: Creating Type-Based Subscriptions for Single-Tenant Multiple Topics\nDESCRIPTION: Shows how to create type-based subscriptions for a single-tenant application with multiple topics, allowing different agents to handle different types of messages.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/core-concepts/topic-and-subscription.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Type-based Subscriptions for single-tenant, multiple topics scenario\nTypeSubscription(topic_type=\"triage\", agent_type=\"triage_agent\")\nTypeSubscription(topic_type=\"coding\", agent_type=\"coder_agent\")\nTypeSubscription(topic_type=\"coding\", agent_type=\"reviewer_agent\")\n```\n\n----------------------------------------\n\nTITLE: Sending Message to Agent and Printing Response (Python)\nDESCRIPTION: This code sends a message to the registered agent using the `runtime.send_message` method. The message content is \"What are the best movies from studio Ghibli?\", and the response from the agent is printed to the console. The code also includes an assertion to ensure that the response is of the expected type.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/llamaindex-agent.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmessage = Message(content=\"What are the best movies from studio Ghibli?\")\nresponse = await runtime.send_message(message, agent)\nassert isinstance(response, Message)\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Using Statements for Gemini Integration\nDESCRIPTION: Required using statements for implementing Gemini image chat functionality.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Image-chat-with-gemini.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nnamespace AutoGen.Gemini.Sample.ImageChat;\n\nusing AutoGen.Gemini;\n```\n\n----------------------------------------\n\nTITLE: Defining Function Contract for AddAsync Operation\nDESCRIPTION: Property that returns a FunctionContract object describing the AddAsync operation, including its name, description, return type, and parameter specifications with types and descriptions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.SourceGenerator.Tests/ApprovalTests/FunctionCallTemplateTests.TestFunctionCallTemplate.approved.txt#2025-04-22_snippet_2\n\nLANGUAGE: C#\nCODE:\n```\npublic FunctionContract AddAsyncFunctionContract\n{\n    get => new FunctionContract\n    {\n        Name = @\"AddAsync\",\n        Description = @\"Add two numbers.\",\n        ReturnType = typeof(System.Threading.Tasks.Task`1[System.String]),\n        Parameters = new global::AutoGen.Core.FunctionParameterContract[]\n        {\n            new FunctionParameterContract\n            {\n                Name = @\"a\",\n                Description = @\"The first number.\",\n                ParameterType = typeof(System.Int32),\n                IsRequired = true,\n            },\n            new FunctionParameterContract\n            {\n                Name = @\"b\",\n                Description = @\"The second number.\",\n                ParameterType = typeof(System.Int32),\n                IsRequired = true,\n            },\n        },\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Agent in v0.2\nDESCRIPTION: Shows how to create a teachable agent with RAG capabilities in v0.2.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nteachable_agent = ConversableAgent(\n    name=\"teachable_agent\",\n    llm_config=llm_config\n)\n\n# Instantiate a Teachability object. Its parameters are all optional.\nteachability = Teachability(\n    reset_db=False,\n    path_to_db_dir=\"./tmp/interactive/teachability_db\"\n)\n\nteachability.add_to_agent(teachable_agent)\n```\n\n----------------------------------------\n\nTITLE: Using Azure AI Foundry with Phi-4 Model from GitHub Marketplace\nDESCRIPTION: Shows how to use the AzureAIChatCompletionClient to access the Phi-4 model from GitHub Marketplace through Azure AI Foundry. Authentication is done using a GitHub personal access token.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.azure import AzureAIChatCompletionClient\nfrom azure.core.credentials import AzureKeyCredential\n\nclient = AzureAIChatCompletionClient(\n    model=\"Phi-4\",\n    endpoint=\"https://models.inference.ai.azure.com\",\n    # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.\n    # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\n    credential=AzureKeyCredential(os.environ[\"GITHUB_TOKEN\"]),\n    model_info={\n        \"json_output\": False,\n        \"function_calling\": False,\n        \"vision\": False,\n        \"family\": \"unknown\",\n        \"structured_output\": False,\n    },\n)\n\nresult = await client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\nprint(result)\nawait client.close()\n```\n\n----------------------------------------\n\nTITLE: Implementing Event Handler in C# for AutoGen Agent\nDESCRIPTION: Example of an event handler in a custom AutoGen agent class. It demonstrates handling a NewMessageReceived event, publishing an Output event, and closing the conversation.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/Hello/HelloAgentState/README.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nTopicSubscription(\"HelloAgents\")]\npublic class HelloAgent(\n    iAgentWorker worker,\n    [FromKeyedServices(\"AgentsMetadata\")] AgentsMetadata typeRegistry) : ConsoleAgent(\n        worker,\n        typeRegistry),\n        ISayHello,\n        IHandle<NewMessageReceived>,\n        IHandle<ConversationClosed>\n{\n    public async Task Handle(NewMessageReceived item, CancellationToken cancellationToken = default)\n    {\n        var response = await SayHello(item.Message).ConfigureAwait(false);\n        var evt = new Output\n        {\n            Message = response\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEventAsync(evt).ConfigureAwait(false);\n        var goodbye = new ConversationClosed\n        {\n            UserId = this.AgentId.Key,\n            UserMessage = \"Goodbye\"\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEventAsync(goodbye).ConfigureAwait(false);\n    }\n```\n\n----------------------------------------\n\nTITLE: Setting Up AutoGen Repository with WSL\nDESCRIPTION: Commands for cloning the AutoGen repository and setting the repository base path environment variable.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:microsoft/autogen.git\nexport AUTOGEN_REPO_BASE=<path_to_autogen>\n```\n\n----------------------------------------\n\nTITLE: Starting the AutoGen Application Runtime in C#\nDESCRIPTION: Code demonstrating how to start the AutoGen runtime locally and send a message to an agent using the App.PublishMessageAsync method. This shows the initialization and message publishing mechanism.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/Microsoft.AutoGen.Integration.Tests.AppHosts/HelloAgentTests/README.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\n// send a message to the agent\nvar app = await App.PublishMessageAsync(\"HelloAgents\", new NewMessageReceived\n{\n    Message = \"World\"\n}, local: true);\n\nawait App.RuntimeApp!.WaitForShutdownAsync();\nawait app.WaitForShutdownAsync();\n```\n\n----------------------------------------\n\nTITLE: Defining Message Dataclasses in Python\nDESCRIPTION: Defines two dataclasses, Message and Termination, that encapsulate message content and termination reasons respectively. These classes are integral to signaling and handling termination.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/termination-with-intervention.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass Message:\n    content: Any\n\n\n@dataclass\nclass Termination:\n    reason: str\n```\n\n----------------------------------------\n\nTITLE: Starting Agent Runtime - Python\nDESCRIPTION: Demonstrates starting the single-threaded agent runtime for message processing and agent orchestration. Essential before sending messages to agents; assumes the runtime and agents are registered and configured.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/langgraph-agent.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nruntime.start()\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to OpenAIChatAgent\nDESCRIPTION: Example of sending a message to the configured OpenAIChatAgent and receiving a response\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-connect-to-third-party-api.md#2025-04-22_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\nvar result = await agent.SendMessageAsync(\"Write C# code to check if a number is prime.\");\nConsole.WriteLine(result);\n```\n\n----------------------------------------\n\nTITLE: Building the AutoGen Studio Frontend\nDESCRIPTION: Commands for installing dependencies and building the frontend UI of AutoGen Studio. These steps are necessary when installing from source to create the web interface.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-studio/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g gatsby-cli\nnpm install --global yarn\ncd frontend\nyarn install\nyarn build\n# Windows users may need alternative commands to build the frontend:\ngatsby clean && rmdir /s /q ..\\\\autogenstudio\\\\web\\\\ui 2>nul & (set \\\"PREFIX_PATH_VALUE=\\\" || ver>nul) && gatsby build --prefix-paths && xcopy /E /I /Y public ..\\\\autogenstudio\\\\web\\\\ui\n```\n\n----------------------------------------\n\nTITLE: Configuring Semantic Kernel Adapter with Anthropic for AutoGen\nDESCRIPTION: Sets up the Semantic Kernel adapter using Anthropic's Claude model. It demonstrates importing necessary modules, creating an Anthropic chat completion client, configuring execution settings, and initializing the adapter.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.semantic_kernel import SKChatCompletionAdapter\nfrom semantic_kernel import Kernel\nfrom semantic_kernel.connectors.ai.anthropic import AnthropicChatCompletion, AnthropicChatPromptExecutionSettings\nfrom semantic_kernel.memory.null_memory import NullMemory\n\nsk_client = AnthropicChatCompletion(\n    ai_model_id=\"claude-3-5-sonnet-20241022\",\n    api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n    service_id=\"my-service-id\",  # Optional; for targeting specific services within Semantic Kernel\n)\nsettings = AnthropicChatPromptExecutionSettings(\n    temperature=0.2,\n)\n\nanthropic_model_client = SKChatCompletionAdapter(\n    sk_client, kernel=Kernel(memory=NullMemory()), prompt_settings=settings\n)\n\n# Call the model directly.\nmodel_result = await anthropic_model_client.create(\n    messages=[UserMessage(content=\"What is the capital of France?\", source=\"User\")]\n)\nprint(model_result)\nawait anthropic_model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Workbench Agent Definition\nDESCRIPTION: This code defines a WorkbenchAgent class that inherits from RoutedAgent.  It initializes the agent with a model client, model context, and a workbench.  The agent's handle_user_message method processes user input, interacts with tools via the workbench, and returns a final result.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/workbench.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass Message:\n    content: str\n\n\nclass WorkbenchAgent(RoutedAgent):\n    def __init__(\n        self, model_client: ChatCompletionClient, model_context: ChatCompletionContext, workbench: Workbench\n    ) -> None:\n        super().__init__(\"An agent with a workbench\")\n        self._system_messages: List[LLMMessage] = [SystemMessage(content=\"You are a helpful AI assistant.\")]\n        self._model_client = model_client\n        self._model_context = model_context\n        self._workbench = workbench\n\n    @message_handler\n    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n        # Add the user message to the model context.\n        await self._model_context.add_message(UserMessage(content=message.content, source=\"user\"))\n        print(\"---------User Message-----------\")\n        print(message.content)\n\n        # Run the chat completion with the tools.\n        create_result = await self._model_client.create(\n            messages=self._system_messages + (await self._model_context.get_messages()),\n            tools=(await self._workbench.list_tools()),\n            cancellation_token=ctx.cancellation_token,\n        )\n\n        # Run tool call loop.\n        while isinstance(create_result.content, list) and all(\n            isinstance(call, FunctionCall) for call in create_result.content\n        ):\n            print(\"---------Function Calls-----------\")\n            for call in create_result.content:\n                print(call)\n\n            # Add the function calls to the model context.\n            await self._model_context.add_message(AssistantMessage(content=create_result.content, source=\"assistant\"))\n\n            # Call the tools using the workbench.\n            print(\"---------Function Call Results-----------\")\n            results: List[ToolResult] = []\n            for call in create_result.content:\n                result = await self._workbench.call_tool(\n                    call.name, arguments=json.loads(call.arguments), cancellation_token=ctx.cancellation_token\n                )\n                results.append(result)\n                print(result)\n\n            # Add the function execution results to the model context.\n            await self._model_context.add_message(\n                FunctionExecutionResultMessage(\n                    content=[\n                        FunctionExecutionResult(\n                            call_id=call.id,\n                            content=result.to_text(),\n                            is_error=result.is_error,\n                            name=result.name,\n                        )\n                        for call, result in zip(create_result.content, results, strict=False)\n                    ]\n                )\n            )\n\n            # Run the chat completion again to reflect on the history and function execution results.\n            create_result = await self._model_client.create(\n                messages=self._system_messages + (await self._model_context.get_messages()),\n                tools=(await self._workbench.list_tools()),\n                cancellation_token=ctx.cancellation_token,\n            )\n\n        # Now we have a single message as the result.\n        assert isinstance(create_result.content, str)\n\n        print(\"---------Final Response-----------\")\n        print(create_result.content)\n\n        # Add the assistant message to the model context.\n        await self._model_context.add_message(AssistantMessage(content=create_result.content, source=\"assistant\"))\n\n        # Return the result as a message.\n        return Message(content=create_result.content)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Self-Teaching Capabilities\nDESCRIPTION: Command to test an agent's ability to learn from its own experience through feedback loops.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/task_centric_memory/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython eval_self_teaching.py configs/self_teaching.yaml\n```\n\n----------------------------------------\n\nTITLE: Runtime Setup with Default Topics and Subscriptions in Python\nDESCRIPTION: This code demonstrates how to register agents that use default topics and subscriptions with the runtime. It creates a SingleThreadedAgentRuntime, registers a BroadcastingAgentDefaultTopic and a ReceivingAgent, and publishes a message to all agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/message-and-communication.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nruntime = SingleThreadedAgentRuntime()\nawait BroadcastingAgentDefaultTopic.register(\n    runtime, \"broadcasting_agent\", lambda: BroadcastingAgentDefaultTopic(\"Broadcasting Agent\")\n)\nawait ReceivingAgent.register(runtime, \"receiving_agent\", lambda: ReceivingAgent(\"Receiving Agent\"))\nruntime.start()\nawait runtime.publish_message(Message(\"Hello, World! From the runtime!\"), topic_id=DefaultTopicId())\nawait runtime.stop_when_idle()\n```\n\n----------------------------------------\n\nTITLE: Creating Assistant Agent in AutoGen v0.2\nDESCRIPTION: Demonstrates how to create an AssistantAgent in AutoGen v0.2 using llm_config for configuration.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen.agentchat import AssistantAgent\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\nassistant = AssistantAgent(\n    name=\"assistant\",\n    system_message=\"You are a helpful assistant.\",\n    llm_config=llm_config,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Summarization Agent in AutoGen\nDESCRIPTION: Code snippet for creating a summarization agent using SemanticKernelAgent. The agent is configured to process and summarize information.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Roundrobin-chat.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[]](../../samples/AgentChat/Autogen.Basic.Sample/Example11_Sequential_GroupChat_Example.cs?name=CreateSummarizerAgent)\n```\n\n----------------------------------------\n\nTITLE: Registering Human Agent with Topic Subscription in AutoGen\nDESCRIPTION: Registers a human agent with a specific agent topic type and sets up a subscription for the agent to receive messages published to its own topic. This agent facilitates human interaction within the agent workflow.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/handoffs.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Register the human agent.\nhuman_agent_type = await HumanAgent.register(\n    runtime,\n    type=human_agent_topic_type,  # Using the topic type as the agent type.\n    factory=lambda: HumanAgent(\n        description=\"A human agent.\",\n        agent_topic_type=human_agent_topic_type,\n        user_topic_type=user_topic_type,\n    ),\n)\n# Add subscriptions for the human agent: it will receive messages published to its own topic only.\nawait runtime.add_subscription(TypeSubscription(topic_type=human_agent_topic_type, agent_type=human_agent_type.type))\n```\n\n----------------------------------------\n\nTITLE: Running AutoGen Studio Web UI\nDESCRIPTION: Command to start the AutoGen Studio web application on a specified port. After installation, this command launches the interface that can be accessed through a web browser.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-studio/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nautogenstudio ui --port 8081\n```\n\n----------------------------------------\n\nTITLE: Creating a Wrapper Agent for AgentChat in Python\nDESCRIPTION: This example shows how to create a wrapper RoutedAgent that delegates messages to an AgentChat AssistantAgent, allowing the use of AgentChat agents within the Core API.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/agent-and-agent-runtime.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nclass MyAssistant(RoutedAgent):\n    def __init__(self, name: str) -> None:\n        super().__init__(name)\n        model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n        self._delegate = AssistantAgent(name, model_client=model_client)\n\n    @message_handler\n    async def handle_my_message_type(self, message: MyMessageType, ctx: MessageContext) -> None:\n        print(f\"{self.id.type} received message: {message.content}\")\n        response = await self._delegate.on_messages(\n            [TextMessage(content=message.content, source=\"user\")], ctx.cancellation_token\n        )\n        print(f\"{self.id.type} responded: {response.chat_message}\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Conversational Multi-Agent System with AutoGen\nDESCRIPTION: This code snippet demonstrates how to create a simple multi-agent system using AutoGen. It sets up two agents: a user proxy and an assistant, and initiates a conversation between them.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/HumanEval/Templates/AgentChat/prompt.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom autogen import AssistantAgent, UserProxyAgent, ConversationChain\n\n# Create an AssistantAgent instance\nassistant = AssistantAgent(name=\"assistant\")\n\n# Create a UserProxyAgent instance\nuser_proxy = UserProxyAgent(name=\"user_proxy\")\n\n# Start the conversation\nuser_proxy.initiate_chat(\nassistant,\nmessage=\"Plot a chart of NVDA and TESLA stock price change YTD.\"\n)\n```\n```\n\n----------------------------------------\n\nTITLE: Generating Response with Chat History in C#\nDESCRIPTION: Illustrates sending chat history alongside requests using SendAsync, facilitating context-aware response generation. The agent must be configured for history tracking.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Chat-with-an-agent.md#2025-04-22_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[Generate Response with Chat History](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Chat_With_Agent.cs?name=Chat_With_History)]\n```\n\n----------------------------------------\n\nTITLE: Initializing AssistantAgent and Generating Response - autogen_agentchat - Python\nDESCRIPTION: This snippet demonstrates how to instantiate an AssistantAgent using the autogen_agentchat framework with an OpenAI model client and generate a response to a user-provided message. Dependencies include autogen_agentchat, an OpenAI model client, Python's asyncio, and supporting message/condition classes. Inputs include the user message ('Write a 3 line poem on lake tangayika'), and the output is a chat message generated by the assistant. The model client is closed after the operation.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/state.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-2024-08-06\")\n\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    system_message=\"You are a helpful assistant\",\n    model_client=model_client,\n)\n\n# Use asyncio.run(...) when running in a script.\nresponse = await assistant_agent.on_messages(\n    [TextMessage(content=\"Write a 3 line poem on lake tangayika\", source=\"user\")], CancellationToken()\n)\nprint(response.chat_message)\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Using MultimodalWebSurfer in a Magentic-One Group Chat\nDESCRIPTION: This Python code snippet illustrates the integration of a MultimodalWebSurfer agent within a MagenticOneGroupChat to fetch real-time data from the web (e.g., UV index). It emphasizes the usage of various agents within the Magentic-One platform. Users must handle 'OpenAIChatCompletionClient', 'MagenticOneGroupChat', 'Console', and 'MultimodalWebSurfer' imports correctly. Necessary agents are initialized and executed asynchronously to perform specified tasks.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/magentic-one.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.teams import MagenticOneGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\n# from autogen_ext.agents.file_surfer import FileSurfer\n# from autogen_ext.agents.magentic_one import MagenticOneCoderAgent\n# from autogen_agentchat.agents import CodeExecutorAgent\n# from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    surfer = MultimodalWebSurfer(\n        \"WebSurfer\",\n        model_client=model_client,\n    )\n\n    team = MagenticOneGroupChat([surfer], model_client=model_client)\n    await Console(team.run_stream(task=\"What is the UV index in Melbourne today?\"))\n\n    # # Note: you can also use  other agents in the team\n    # team = MagenticOneGroupChat([surfer, file_surfer, coder, terminal], model_client=model_client)\n    # file_surfer = FileSurfer( \"FileSurfer\",model_client=model_client)\n    # coder = MagenticOneCoderAgent(\"Coder\",model_client=model_client)\n    # terminal = CodeExecutorAgent(\"ComputerTerminal\",code_executor=LocalCommandLineCodeExecutor())\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Executing Teachable Agent Chat Sample\nDESCRIPTION: Command to run the teachable agent chat example that demonstrates how agents can learn from user interactions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/task_centric_memory/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython chat_with_teachable_agent.py\n```\n\n----------------------------------------\n\nTITLE: Creating UserProxyAgent with ALWAYS HumanInputMode in C#\nDESCRIPTION: Shows how to instantiate a UserProxyAgent with HumanInputMode set to ALWAYS, which ensures the agent always prompts for user input during interactions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Create-a-user-proxy-agent.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nvar userProxy = new UserProxyAgent(\n    \"user_proxy\",\n    new UserProxyAgentConfig\n    {\n        HumanInputMode = HumanInputMode.ALWAYS\n    });\n```\n\n----------------------------------------\n\nTITLE: Defining Weather Function with AutoGen Attributes\nDESCRIPTION: Implementation of a GetWeather function using AutoGen.Core.FunctionAttribute in a partial class to enable source generation for function contracts\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/MistralChatAgent-use-function-call.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\npublic partial class MistralAgentFunction\n{\n    [Function(\"Get weather information for a specific location\")]\n    public Task<string> GetWeather([Description(\"The location to get weather for\")] string location)\n    {\n        // This is a mock implementation that always returns sunny\n        return Task.FromResult($\"The weather in {location} is sunny!\");\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Task-Centric Memory Usage in Python\nDESCRIPTION: Demonstrates initializing a MemoryController, adding task-insight pairs as memories, and retrieving memories relevant to a new task. This example verifies that the installation was successful and shows the core functionality of memory retrieval.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-ext/src/autogen_ext/experimental/task_centric_memory/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.experimental.task_centric_memory import MemoryController\nfrom autogen_ext.experimental.task_centric_memory.utils import PageLogger\n\n\nasync def main() -> None:\n   client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n   logger = PageLogger(config={\"level\": \"DEBUG\", \"path\": \"./pagelogs/quickstart\"})  # Optional, but very useful.\n   memory_controller = MemoryController(reset=True, client=client, logger=logger)\n\n   # Add a few task-insight pairs as memories, where an insight can be any string that may help solve the task.\n   await memory_controller.add_memo(task=\"What color do I like?\", insight=\"Deep blue is my favorite color\")\n   await memory_controller.add_memo(task=\"What's another color I like?\", insight=\"I really like cyan\")\n   await memory_controller.add_memo(task=\"What's my favorite food?\", insight=\"Halibut is my favorite\")\n\n   # Retrieve memories for a new task that's related to only two of the stored memories.\n   memos = await memory_controller.retrieve_relevant_memos(task=\"What colors do I like most?\")\n   print(\"{} memories retrieved\".format(len(memos)))\n   for memo in memos:\n      print(\"- \" + memo.insight)\n\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Creating FunctionCallMiddleware with auto-invoke in C#\nDESCRIPTION: Example of creating a FunctionCallMiddleware with auto-invoke functionality, which automatically invokes tools when receiving a ToolCallMessage.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Create-agent-with-tools.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\nvar functionContracts = new[] { FunctionContract.FromType<Tool>() };\nvar functionMap = new Dictionary<string, Func<IDictionary<string, object>, Task<string>>>\n{\n    { \"GetWeather\", async (IDictionary<string, object> parameters) => await tools.GetWeather((string)parameters[\"city\"]) }\n};\nvar middleware = new FunctionCallMiddleware(functionContracts, functionMap);\n```\n\n----------------------------------------\n\nTITLE: Creating an Azure OpenAI Chat Completion Client with AAD Authentication\nDESCRIPTION: Initializes an Azure OpenAI client using Azure Active Directory authentication. This requires the identity to have appropriate cognitive services permissions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_ext.auth.azure import AzureTokenProvider\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\nfrom azure.identity import DefaultAzureCredential\n\n# Create the token provider\ntoken_provider = AzureTokenProvider(\n    DefaultAzureCredential(),\n    \"https://cognitiveservices.azure.com/.default\",\n)\n\naz_model_client = AzureOpenAIChatCompletionClient(\n    azure_deployment=\"{your-azure-deployment}\",\n    model=\"{model-name, such as gpt-4o}\",\n    api_version=\"2024-06-01\",\n    azure_endpoint=\"https://{your-custom-endpoint}.openai.azure.com/\",\n    azure_ad_token_provider=token_provider,  # Optional if you choose key-based authentication.\n    # api_key=\"sk-...\", # For key-based authentication.\n)\n\nresult = await az_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\nprint(result)\nawait az_model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Creating Wikipedia Tool (Python)\nDESCRIPTION: This code creates a Wikipedia tool using `WikipediaToolSpec` from the `llama_index.tools.wikipedia` module. It initializes the tool specification and then extracts the second tool from the list of tools created.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/llamaindex-agent.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwiki_spec = WikipediaToolSpec()\nwikipedia_tool = wiki_spec.to_tool_list()[1]\n```\n\n----------------------------------------\n\nTITLE: Installing and Running .NET Agent Sample\nDESCRIPTION: Step-by-step instructions for running the Python-.NET agent interoperability sample using dotnet CLI commands and accessing the Aspire dashboard.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_xlang_hello_python_agent/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n1. Navigate to autogen/dotnet/samples/Hello/Hello.AppHost\n2. Run `dotnet run` to start the .NET Aspire app host, which runs three projects:\n    - Backend (the .NET Agent Runtime)\n    - HelloAgent (the .NET Agent)\n    - this Python agent - hello_python_agent.py\n3. The AppHost will start the Aspire dashboard on [https://localhost:15887](https://localhost:15887).\n```\n\n----------------------------------------\n\nTITLE: Implementing HumanAgent Class in Python\nDESCRIPTION: A proxy class for human interaction in the chatbot system. Handles user tasks by getting console input and publishing responses.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/handoffs.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass HumanAgent(RoutedAgent):\n    def __init__(self, description: str, agent_topic_type: str, user_topic_type: str) -> None:\n        super().__init__(description)\n        self._agent_topic_type = agent_topic_type\n        self._user_topic_type = user_topic_type\n\n    @message_handler\n    async def handle_user_task(self, message: UserTask, ctx: MessageContext) -> None:\n        human_input = input(\"Human agent input: \")\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{human_input}\", flush=True)\n        message.context.append(AssistantMessage(content=human_input, source=self.id.type))\n        await self.publish_message(\n            AgentResponse(context=message.context, reply_to_topic_type=self._agent_topic_type),\n            topic_id=TopicId(self._user_topic_type, source=self.id.key),\n        )\n```\n\n----------------------------------------\n\nTITLE: Stopping Agent Runtime (Python)\nDESCRIPTION: This code stops the `SingleThreadedAgentRuntime`, terminating the agent's operation and releasing any associated resources.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/llamaindex-agent.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nawait runtime.stop()\n```\n\n----------------------------------------\n\nTITLE: Implementing Writer Agent\nDESCRIPTION: Implementation of the second agent that transforms analytical insights into engaging promotional content.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/sequential-workflow.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@type_subscription(topic_type=writer_topic_type)\nclass WriterAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"A writer agent.\")\n        self._system_message = SystemMessage(\n            content=(\n                \"You are a marketing copywriter. Given a block of text describing features, audience, and USPs, \"\n                \"compose a compelling marketing copy (like a newsletter section) that highlights these points. \"\n                \"Output should be short (around 150 words), output just the copy as a single text block.\"\n            )\n        )\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_intermediate_text(self, message: Message, ctx: MessageContext) -> None:\n        prompt = f\"Below is the info about the product:\\n\\n{message.content}\"\n\n        llm_result = await self._model_client.create(\n            messages=[self._system_message, UserMessage(content=prompt, source=self.id.key)],\n            cancellation_token=ctx.cancellation_token,\n        )\n        response = llm_result.content\n        assert isinstance(response, str)\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{response}\")\n\n        await self.publish_message(Message(response), topic_id=TopicId(format_proof_topic_type, source=self.id.key))\n```\n\n----------------------------------------\n\nTITLE: Creating and configuring agents in AutoGen using Python\nDESCRIPTION: This code snippet demonstrates how to create and configure an assistant agent and a user proxy agent in AutoGen. It includes setting up the conversation and initiating the chat between agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/GAIA/Templates/SelectorGroupChat/prompt.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create an AssistantAgent named \"assistant\"\nassistant = AssistantAgent(name=\"assistant\", llm_config={\"config_list\": config_list})\n\n# Create a UserProxyAgent named \"human\"\nhuman = UserProxyAgent(name=\"human\", code_execution_config={\"work_dir\": \"coding\"})\n\n# Start the conversation\nhuman.initiate_chat(assistant, message=\"Plot a chart of NVDA stock price change YTD.\")\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen.Ollama Package\nDESCRIPTION: Command to install the AutoGen.Ollama NuGet package using the .NET CLI\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Ollama/Chat-with-llava.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AutoGen.Ollama\n```\n\n----------------------------------------\n\nTITLE: Environment Configuration Example\nDESCRIPTION: Example JSON configuration for API keys used by AutoGenBench.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/README.md#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"BING_API_KEY\": \"xxxyyyzzz\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen and Chess Dependencies\nDESCRIPTION: Commands to install the required Python packages for the chess game example, including AutoGen with OpenAI and Azure extensions, plus the chess library.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_chess_game/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"autogen-ext[openai,azure]\" \"chess\"\n```\n\n----------------------------------------\n\nTITLE: Streaming Messages from AssistantAgent in Python\nDESCRIPTION: This snippet demonstrates how to stream messages from an AssistantAgent using the `run_stream` method. It provides two options: reading each message from the stream and printing it, or using the Console class to print all messages as they appear in the console. It also enables stats printing.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/agents.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nasync def assistant_run_stream() -> None:\n    # Option 1: read each message from the stream (as shown in the previous example).\n    # async for message in agent.run_stream(task=\"Find information on AutoGen\"):\n    #     print(message)\n\n    # Option 2: use Console to print all messages as they appear.\n    await Console(\n        agent.run_stream(task=\"Find information on AutoGen\"),\n        output_stats=True,  # Enable stats printing.\n    )\n\n\n# Use asyncio.run(assistant_run_stream()) when running in a script.\nawait assistant_run_stream()\n```\n\n----------------------------------------\n\nTITLE: Running AssistantAgent with Multi-Modal Message in Python\nDESCRIPTION: This snippet demonstrates how to run the AssistantAgent with a multi-modal message and print the content of the final response. It calls the `run` method with the multi_modal_message and then prints the content of the last message from the result.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/agents.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Use asyncio.run(...) when running in a script.\nresult = await agent.run(task=multi_modal_message)\nprint(result.messages[-1].content)  # type: ignore\n```\n\n----------------------------------------\n\nTITLE: Creating a Virtual Environment with conda for AutoGen\nDESCRIPTION: Instructions for creating and activating a conda environment specifically for AutoGen, using Python 3.12. Includes the command to deactivate the environment when finished.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/installation.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n autogen python=3.12\nconda activate autogen\n```\n\nLANGUAGE: bash\nCODE:\n```\nconda deactivate\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Studio from PyPi\nDESCRIPTION: This snippet provides the command to install AutoGen Studio using pip, the Python package manager. The command is intended for use within a Python environment where pip can resolve necessary dependencies.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/installation.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -U autogenstudio\n```\n\n----------------------------------------\n\nTITLE: Running AutoGen Studio with PostgreSQL Database\nDESCRIPTION: This bash command shows how to start the AutoGen Studio UI while specifying a PostgreSQL database using the `--database-uri` argument. It uses a connection string including the username, password, host, and database name. Ensure that the `psycopg2` or `psycopg2-binary` driver is installed.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/faq.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nautogenstudio ui --database-uri postgresql+psycopg://user:password@localhost/dbname\n```\n\n----------------------------------------\n\nTITLE: Register OpenAI Chat Message Connector\nDESCRIPTION: Code to register the OpenAIChatRequestMessageConnector with the dependency injection container, enabling support for various AutoGen message types like TextMessage, ImageMessage, and MultiModalMessage.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-support-more-messages.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nbuilder.Services.AddSingleton<IMessageConnector<ChatRequestMessage>, OpenAIChatRequestMessageConnector>();\n```\n\n----------------------------------------\n\nTITLE: Viewing GAIA Results\nDESCRIPTION: Command to generate a tabulated summary of the benchmark results, including task completion rates\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/GAIA/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nagbench tabulate Results/gaia_validation_level_1__MagenticOne/\n```\n\n----------------------------------------\n\nTITLE: Fetching Content with McpWorkbench and AssistantAgent in Python\nDESCRIPTION: This code snippet demonstrates how to use the `McpWorkbench` with an `AssistantAgent` to fetch content from a URL and summarize it. It creates an `OpenAIChatCompletionClient` for model interaction and configures the agent to reflect on tool use. Requires the `autogen` library and an active MCP server.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/agents.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Get the fetch tool from mcp-server-fetch.\nfetch_mcp_server = StdioServerParams(command=\"uvx\", args=[\"mcp-server-fetch\"])\n\n# Create an MCP workbench which provides a session to the mcp server.\nasync with McpWorkbench(fetch_mcp_server) as workbench:  # type: ignore\n    # Create an agent that can use the fetch tool.\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n    fetch_agent = AssistantAgent(\n        name=\"fetcher\", model_client=model_client, workbench=workbench, reflect_on_tool_use=True\n    )\n\n    # Let the agent fetch the content of a URL and summarize it.\n    result = await fetch_agent.run(task=\"Summarize the content of https://en.wikipedia.org/wiki/Seattle\")\n    assert isinstance(result.messages[-1], TextMessage)\n    print(result.messages[-1].content)\n\n    # Close the connection to the model client.\n    await model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Importing necessary modules for chat context\nDESCRIPTION: This snippet imports various modules and classes required for managing chat completion contexts and utilizing OpenAI's chat client. Key dependencies include autogen_core and autogen_ext models, which provide essential classes for message handling.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/model-context.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom autogen_core.models import AssistantMessage, ChatCompletionClient, SystemMessage, UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\n----------------------------------------\n\nTITLE: Configuring AI Agents with OpenAI API in Python\nDESCRIPTION: This snippet demonstrates how to configure AI agents using the OpenAI API. It sets up the configuration with API key, base URL, and API type, and creates two agents: a user proxy and an assistant.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/GAIA/Templates/MagenticOne/prompt.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport autogen\n\nconfig_list = [\n    {\n        'model': 'gpt-4',\n        'api_key': 'your_api_key',\n        'base_url': \"your_base_url\",\n        'api_type': 'azure',\n    }\n]\n\nllm_config = {'config_list': config_list, 'seed': 42}\n\nassistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config,\n)\n\nuser_proxy = autogen.UserProxyAgent(\n    name=\"human\",\n    human_input_mode=\"TERMINATE\",\n    max_consecutive_auto_reply=10,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n    code_execution_config={\"work_dir\": \"coding\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Using Statements for MistralAI Token Counting in C#\nDESCRIPTION: This snippet shows the necessary using statements for implementing token counting with MistralAI in AutoGen.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/MistralChatAgent-count-token-usage.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example14_MistralClientAgent_TokenCount.cs?name=using_statements)]\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies\nDESCRIPTION: This command installs the necessary Python packages for the FastAPI application, including `fastapi`, `uvicorn`, `autogen-core`, `autogen-ext[openai]`, and `PyYAML`.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_streaming_handoffs_fastapi/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"fastapi\" \"uvicorn[standard]\" \"autogen-core\" \"autogen-ext[openai]\" \"PyYAML\"\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for AutoGen\nDESCRIPTION: Commands to create and manage a Conda environment specifically for AutoGen using Python 3.12. Includes environment creation, activation, and deactivation steps.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/installation.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n autogen python=3.12\nconda activate autogen\n```\n\nLANGUAGE: bash\nCODE:\n```\nconda deactivate\n```\n\n----------------------------------------\n\nTITLE: Chat API Request Format\nDESCRIPTION: JSON structure for the chat API request body showing the format for multi-turn conversation history.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_streaming_response_fastapi/README.md#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"messages\": [\n    {\"source\": \"user\", \"content\": \"Hello!\"},\n    {\"source\": \"assistant\", \"content\": \"Hello! How can I help you?\"},\n    {\"source\": \"user\", \"content\": \"Introduce yourself.\"}\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Agent Configuration and Termination Trigger\nDESCRIPTION: Sets up an agent using RoutedAgent which sends termination signals when a specific condition is met. Relies on message subscription and handling decorators provided by autogen_core.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/termination-with-intervention.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@default_subscription\nclass AnAgent(RoutedAgent):\n    def __init__(self) -> None:\n        super().__init__(\"MyAgent\")\n        self.received = 0\n\n    @message_handler\n    async def on_new_message(self, message: Message, ctx: MessageContext) -> None:\n        self.received += 1\n        if self.received > 3:\n            await self.publish_message(Termination(reason=\"Reached maximum number of messages\"), DefaultTopicId())\n```\n\n----------------------------------------\n\nTITLE: Implementing CoderAgent for Code Writing Tasks\nDESCRIPTION: Defines CoderAgent class that handles code writing tasks, manages session memory, and interacts with code reviewers. Uses chain-of-thought prompting and maintains separate message histories for different tasks.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/reflection.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@default_subscription\nclass CoderAgent(RoutedAgent):\n    \"\"\"An agent that performs code writing tasks.\"\"\"\n\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"A code writing agent.\")\n        self._system_messages: List[LLMMessage] = [\n            SystemMessage(\n                content=\"\"\"You are a proficient coder. You write code to solve problems.\nWork with the reviewer to improve your code.\nAlways put all finished code in a single Markdown code block.\nFor example:\n```python\ndef hello_world():\n    print(\"Hello, World!\")\n```\n\nRespond using the following format:\n\nThoughts: <Your comments>\nCode: <Your code>\n\"\"\",\n            )\n        ]\n        self._model_client = model_client\n        self._session_memory: Dict[str, List[CodeWritingTask | CodeReviewTask | CodeReviewResult]] = {}\n\n    @message_handler\n    async def handle_code_writing_task(self, message: CodeWritingTask, ctx: MessageContext) -> None:\n        # Store the messages in a temporary memory for this request only.\n        session_id = str(uuid.uuid4())\n        self._session_memory.setdefault(session_id, []).append(message)\n        # Generate a response using the chat completion API.\n        response = await self._model_client.create(\n            self._system_messages + [UserMessage(content=message.task, source=self.metadata[\"type\"])],\n            cancellation_token=ctx.cancellation_token,\n        )\n        assert isinstance(response.content, str)\n        # Extract the code block from the response.\n        code_block = self._extract_code_block(response.content)\n        if code_block is None:\n            raise ValueError(\"Code block not found.\")\n        # Create a code review task.\n        code_review_task = CodeReviewTask(\n            session_id=session_id,\n            code_writing_task=message.task,\n            code_writing_scratchpad=response.content,\n            code=code_block,\n        )\n        # Store the code review task in the session memory.\n        self._session_memory[session_id].append(code_review_task)\n        # Publish a code review task.\n        await self.publish_message(code_review_task, topic_id=TopicId(\"default\", self.id.key))\n\n    @message_handler\n    async def handle_code_review_result(self, message: CodeReviewResult, ctx: MessageContext) -> None:\n        # Store the review result in the session memory.\n        self._session_memory[message.session_id].append(message)\n        # Obtain the request from previous messages.\n        review_request = next(\n            m for m in reversed(self._session_memory[message.session_id]) if isinstance(m, CodeReviewTask)\n        )\n        assert review_request is not None\n        # Check if the code is approved.\n        if message.approved:\n            # Publish the code writing result.\n            await self.publish_message(\n                CodeWritingResult(\n                    code=review_request.code,\n                    task=review_request.code_writing_task,\n                    review=message.review,\n                ),\n                topic_id=TopicId(\"default\", self.id.key),\n            )\n            print(\"Code Writing Result:\")\n            print(\"-\" * 80)\n            print(f\"Task:\\n{review_request.code_writing_task}\")\n            print(\"-\" * 80)\n            print(f\"Code:\\n{review_request.code}\")\n            print(\"-\" * 80)\n            print(f\"Review:\\n{message.review}\")\n            print(\"-\" * 80)\n        else:\n            # Create a list of LLM messages to send to the model.\n            messages: List[LLMMessage] = [*self._system_messages]\n            for m in self._session_memory[message.session_id]:\n                if isinstance(m, CodeReviewResult):\n                    messages.append(UserMessage(content=m.review, source=\"Reviewer\"))\n                elif isinstance(m, CodeReviewTask):\n                    messages.append(AssistantMessage(content=m.code_writing_scratchpad, source=\"Coder\"))\n                elif isinstance(m, CodeWritingTask):\n                    messages.append(UserMessage(content=m.task, source=\"User\"))\n                else:\n                    raise ValueError(f\"Unexpected message type: {m}\")\n            # Generate a revision using the chat completion API.\n            response = await self._model_client.create(messages, cancellation_token=ctx.cancellation_token)\n            assert isinstance(response.content, str)\n            # Extract the code block from the response.\n            code_block = self._extract_code_block(response.content)\n            if code_block is None:\n                raise ValueError(\"Code block not found.\")\n            # Create a new code review task.\n            code_review_task = CodeReviewTask(\n                session_id=message.session_id,\n                code_writing_task=review_request.code_writing_task,\n                code_writing_scratchpad=response.content,\n                code=code_block,\n            )\n            # Store the code review task in the session memory.\n            self._session_memory[message.session_id].append(code_review_task)\n            # Publish a new code review task.\n            await self.publish_message(code_review_task, topic_id=TopicId(\"default\", self.id.key))\n\n    def _extract_code_block(self, markdown_text: str) -> Union[str, None]:\n        pattern = r\"```(\\w+)\\n(.*?)\\n```\"\n        # Search for the pattern in the markdown text\n        match = re.search(pattern, markdown_text, re.DOTALL)\n        # Extract the language and code block if a match is found\n        if match:\n            return match.group(2)\n        return None\n```\n\n----------------------------------------\n\nTITLE: Executing the AutoGen Runtime and Handling User Interaction in Python\nDESCRIPTION: Starts the Docker code executor and the AutoGen runtime asynchronously. It then sends an initial `Message` containing Python code (`print('Hello, World!')`) to the registered \"tool_enabled_agent\". During execution, the `ToolInterventionHandler` will intercept the tool call, prompting the user for permission via the console. After the interaction completes and a response is received, it prints the response content. Finally, it properly stops the runtime and the Docker executor, and closes the OpenAI model client connection to release resources.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/tool-use-with-intervention.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Start the runtime and the docker executor.\nawait docker_executor.start()\nruntime.start()\n\n# Send a task to the tool user.\nresponse = await runtime.send_message(\n    Message(\"Run the following Python code: print('Hello, World!')\"), AgentId(\"tool_enabled_agent\", \"default\")\n)\nprint(response.content)\n\n# Stop the runtime and the docker executor.\nawait runtime.stop()\nawait docker_executor.stop()\n\n# Close the connection to the model client.\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Generating Response with AutoGen in C#\nDESCRIPTION: Uses SendAsync to generate a text message response. This requires an instance of the agent initialized with AutoGen.Core libraries.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Chat-with-an-agent.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[Generate Response](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Chat_With_Agent.cs?name=Chat_With_Agent)]\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Autodoc for autogen_agentchat Module (reStructuredText)\nDESCRIPTION: This reStructuredText snippet uses the Sphinx `automodule` directive to automatically generate documentation from the `autogen_agentchat` Python module. It instructs Sphinx to include all members (`:members:`), even those without docstrings (`:undoc-members:`), and to display the inheritance diagram for classes (`:show-inheritance:`). This requires the Sphinx autodoc extension and the target Python module to be importable.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_agentchat.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: autogen_agentchat\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries and Modules\nDESCRIPTION: This snippet imports essential libraries and modules necessary for initializing agents and managing the group chat workflow.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/selector-group-chat.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Sequence\n\nfrom autogen_agentchat.agents import AssistantAgent, UserProxyAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\n----------------------------------------\n\nTITLE: Implementing Event Handler in AutoGen .NET Agent\nDESCRIPTION: Example of a custom agent class implementing event handlers for NewMessageReceived and ConversationClosed events. It demonstrates topic subscription, event handling, and publishing new events.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/Hello/HelloAgent/README.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nTopicSubscription(\"HelloAgents\")]\npublic class HelloAgent(\n    iAgentWorker worker,\n    [FromKeyedServices(\"AgentsMetadata\")] AgentsMetadata typeRegistry) : ConsoleAgent(\n        worker,\n        typeRegistry),\n        ISayHello,\n        IHandle<NewMessageReceived>,\n        IHandle<ConversationClosed>\n{\n    public async Task Handle(NewMessageReceived item, CancellationToken cancellationToken = default)\n    {\n        var response = await SayHello(item.Message).ConfigureAwait(false);\n        var evt = new Output\n        {\n            Message = response\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEventAsync(evt).ConfigureAwait(false);\n        var goodbye = new ConversationClosed\n        {\n            UserId = this.AgentId.Key,\n            UserMessage = \"Goodbye\"\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEventAsync(goodbye).ConfigureAwait(false);\n    }\n```\n\n----------------------------------------\n\nTITLE: Registering a ClosureAgent to forward results to the queue in Python\nDESCRIPTION: Creates an instance of SingleThreadedAgentRuntime and registers a ClosureAgent with a closure handler for receiving results and forwarding them to the external queue. The agent subscribes to all messages via DefaultSubscription and utilizes the output_result handler. This enables message-driven result extraction in the agent framework.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/extracting-results-with-an-agent.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nruntime = SingleThreadedAgentRuntime()\\nawait ClosureAgent.register_closure(\\n    runtime, \"output_result\", output_result, subscriptions=lambda: [DefaultSubscription()]\\n)\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to Assistant\nDESCRIPTION: Example of sending a message to the assistant and handling the response.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/openai-assistant-agent.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nruntime.start()\nawait runtime.send_message(TextMessage(content=\"Hello, how are you today!\", source=\"user\"), agent)\nawait runtime.stop_when_idle()\n```\n\n----------------------------------------\n\nTITLE: Running Gitty CLI for GitHub Issue Response\nDESCRIPTION: Command to run Gitty CLI tool to generate a response for a specific issue in a GitHub repository.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/gitty/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngitty --repo microsoft/autogen issue 5212\n```\n\n----------------------------------------\n\nTITLE: Running HumanEval Benchmark with AgBench\nDESCRIPTION: Executes the HumanEval benchmark for a specific subset of tasks using the AgBench tool.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/HumanEval/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nagbench run Tasks/human_eval_AgentChat.jsonl\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Dependencies (Python)\nDESCRIPTION: This command installs the necessary Python packages for using LlamaIndex with web and Wikipedia data, Azure OpenAI embeddings and LLMs, and Azure identity management. It includes llama-index-readers-web, llama-index-readers-wikipedia, llama-index-tools-wikipedia, llama-index-embeddings-azure-openai, llama-index-llms-azure-openai, llama-index, and azure-identity.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/llamaindex-agent.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# pip install \"llama-index-readers-web\" \"llama-index-readers-wikipedia\" \"llama-index-tools-wikipedia\" \"llama-index-embeddings-azure-openai\" \"llama-index-llms-azure-openai\" \"llama-index\" \"azure-identity\"\n```\n\n----------------------------------------\n\nTITLE: Importing Required Dependencies for Sequential Workflow\nDESCRIPTION: Initial imports including dataclasses and various AutoGen core components needed for implementing the sequential workflow pattern.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/sequential-workflow.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\nfrom autogen_core import (\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    TopicId,\n    TypeSubscription,\n    message_handler,\n    type_subscription,\n)\nfrom autogen_core.models import ChatCompletionClient, SystemMessage, UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\n----------------------------------------\n\nTITLE: Resetting and Querying Team without State - autogen_agentchat - Python\nDESCRIPTION: Shows the effect of resetting a team and querying it without restoring its previous state. After reset, the team loses context and cannot reference prior task outputs. Inputs include the reset action and a query about the previous poem. Output is streamed via Console, but lacks recall of previous interactions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/state.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait agent_team.reset()\nstream = agent_team.run_stream(task=\"What was the last line of the poem you wrote?\")\nawait Console(stream)\n```\n\n----------------------------------------\n\nTITLE: Running AutoGen Studio Workflow with TeamManager\nDESCRIPTION: This snippet demonstrates how to use the TeamManager class from autogenstudio to run a task with a specified team configuration.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-studio/notebooks/tutorial.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogenstudio.teammanager import TeamManager\n\nwm = TeamManager()\nresult = await wm.run(task=\"What is the weather in New York?\", team_config=\"team.json\")\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Generating Module Documentation with Sphinx Automodule Directive (rst)\nDESCRIPTION: This reStructuredText snippet employs the Sphinx `automodule` directive to automatically create API documentation for the `autogen_ext.tools.azure` Python module. The `:members:`, `:undoc-members:`, and `:show-inheritance:` options ensure comprehensive documentation by including all members (even undocumented ones) and displaying class inheritance relationships. This directive requires the Sphinx framework and the presence of the target Python module.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_ext.tools.azure.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogen_ext.tools.azure\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Creating Bing Search Agent in AutoGen\nDESCRIPTION: Code snippet for creating a Bing search agent using SemanticKernelAgent. The agent is configured with Bing search capabilities.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Roundrobin-chat.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[]](../../samples/AgentChat/Autogen.Basic.Sample/Example11_Sequential_GroupChat_Example.cs?name=CreateBingSearchAgent)\n```\n\n----------------------------------------\n\nTITLE: Executing Math Problem Solving System\nDESCRIPTION: Demonstrates how to start the runtime, submit a math problem, and handle system cleanup. Includes publishing a question and waiting for the system to complete processing.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/multi-agent-debate.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nquestion = \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\"\nruntime.start()\nawait runtime.publish_message(Question(content=question), DefaultTopicId())\n# Wait for the runtime to stop when idle.\nawait runtime.stop_when_idle()\n# Close the connection to the model client.\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Python Environment and Activating Virtual Environment\nDESCRIPTION: Initial setup for the AutoGen documentation requires syncing development dependencies and activating the Python virtual environment. The `uv` command is used to ensure all required packages are installed, and the `source` command activates the virtual environment.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv sync\nsource .venv/bin/activate\n\n```\n\n----------------------------------------\n\nTITLE: Implement Multi-Tenant, Single Scope Publishing Scenario in Python\nDESCRIPTION: Outlines a multi-tenant single scope scenario involving multiple isolated tenants. Each tenant operates with its own set of agent instances. The snippet registers specialists, subscribes them based on client identities, and directs messages within the same tenant. Dependencies include asyncio and the AutoGen framework.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/topic-subscription-scenarios.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nasync def run_multi_tenant_single_scope() -> None:\n    # Create the runtime\n    runtime = SingleThreadedAgentRuntime()\n\n    # List of clients (tenants)\n    tenants = [\"ClientABC\", \"ClientXYZ\"]\n\n    # Initialize sessions and map the topic type to each TaxSpecialist agent type\n    for specialty in TaxSpecialty:\n        specialist_agent_type = f\"TaxSpecialist_{specialty.value}\"\n        await TaxSpecialist.register(\n            runtime=runtime,\n            type=specialist_agent_type,\n            factory=lambda specialty=specialty: TaxSpecialist(  # type: ignore\n                description=f\"A tax specialist in {specialty.value}.\",\n                specialty=specialty,\n                system_messages=[SystemMessage(content=f\"You are a tax specialist in {specialty.value}.\")],\n            ),\n        )\n        specialist_subscription = DefaultSubscription(agent_type=specialist_agent_type)\n        await runtime.add_subscription(specialist_subscription)\n\n    # Start the runtime\n    runtime.start()\n\n    # Publish client requests to their respective topics\n    for tenant in tenants:\n        topic_source = tenant  # The topic source is the client name\n        topic_id = DefaultTopicId(source=topic_source)\n        await runtime.publish_message(\n            ClientRequest(f\"{tenant} requires tax services.\"),\n            topic_id=topic_id,\n        )\n\n    # Allow time for message processing\n    await asyncio.sleep(1)\n\n    # Stop the runtime when idle\n    await runtime.stop_when_idle()\n\n\nawait run_multi_tenant_single_scope()\n```\n\n----------------------------------------\n\nTITLE: Running GAIA Benchmark Tests\nDESCRIPTION: Command to execute a specific subset of GAIA benchmark tests using the validation level 1 template\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/GAIA/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nagbench run Tasks/gaia_validation_level_1__MagenticOne.jsonl\n```\n\n----------------------------------------\n\nTITLE: OpenAI Configuration Example\nDESCRIPTION: JSON configuration structure for using OpenAI with AutoGenBench.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/README.md#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"api_key\": \"REPLACE_WITH_YOUR_API\",\n  \"model\": \"REPLACE_WITH_YOUR_MODEL\"\n}\n```\n\n----------------------------------------\n\nTITLE: Python Client Implementation\nDESCRIPTION: Python script demonstrating how to interact with the streaming chat API using the requests library, including error handling and stream processing.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_streaming_response_fastapi/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport json\nurl = \"http://localhost:8501/chat/completions\"\ndata = {\n    'stream': True,\n    'messages': [\n            {'source': 'user', 'content': \"Hello,I'm tory.\"},\n            {'source': 'assistant', 'content':\"hello Tory, nice to meet you!\"},\n            {'source': 'user', 'content': \"Say hello by my name and introduce yourself.\"}\n        ]\n    }\nheaders = {'Content-Type': 'application/json'}\ntry:\n    response = requests.post(url, json=data, headers=headers, stream=True)\n    response.raise_for_status()\n    for chunk in response.iter_content(chunk_size=None):\n        if chunk:\n            print(json.loads(chunk)[\"content\"], end='', flush=True)\n\nexcept requests.exceptions.RequestException as e:\n    print(f\"Error: {e}\")\nexcept json.JSONDecodeError as e:\n    print(f\"JSON Decode Error: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Setting User Secrets for Local Development\nDESCRIPTION: Commands for setting up user secrets in the local development environment. These secrets include API keys, endpoints, and GitHub app configuration details necessary for running the project locally.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/dev-team/docs/github-flow-getting-started.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndotnet user-secrets set \"OpenAI:Key\" \"your_key\"\n\ndotnet user-secrets set \"OpenAI:Endpoint\" \"https://your_endpoint.openai.azure.com/\"\n\ndotnet user-secrets set \"Github:AppId\" \"gh_app_id\"\n\ndotnet user-secrets set \"Github:InstallationId\" \"gh_inst_id\"\n\ndotnet user-secrets set \"Github:WebhookSecret\" \"webhook_secret\"\n\ndotnet user-secrets set \"Github:AppKey\" \"gh_app_key\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for AutoGen Distributed Chat\nDESCRIPTION: Command to install required Python packages including AutoGen extensions for OpenAI, Azure, Chainlit, Rich, and YAML support.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_distributed-group-chat/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"autogen-ext[openai,azure,chainlit,rich,pyyaml]\"\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Agent in v0.4\nDESCRIPTION: Demonstrates how to create a custom agent by implementing the required methods in v0.4.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Sequence\nfrom autogen_core import CancellationToken\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.messages import TextMessage, BaseChatMessage\nfrom autogen_agentchat.base import Response\n\nclass CustomAgent(BaseChatAgent):\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        return Response(chat_message=TextMessage(content=\"Custom reply\", source=self.name))\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        pass\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n```\n\n----------------------------------------\n\nTITLE: Defining and Exporting an AutoGen Agent Team to JSON (Python)\nDESCRIPTION: This Python script demonstrates defining an AutoGen agent team. It creates an `AssistantAgent` configured with an `OpenAIChatCompletionClient` (using gpt-4o-mini). This agent is then placed into a `RoundRobinGroupChat` team with a `TextMentionTermination` condition. Finally, it exports the complete team configuration into a JSON format using the `dump_component()` and `model_dump_json()` methods and prints it.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/usage.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.conditions import  TextMentionTermination\n\nagent = AssistantAgent(\n        name=\"weather_agent\",\n        model_client=OpenAIChatCompletionClient(\n            model=\"gpt-4o-mini\",\n        ),\n    )\n\nagent_team = RoundRobinGroupChat([agent], termination_condition=TextMentionTermination(\"TERMINATE\"))\nconfig = agent_team.dump_component()\nprint(config.model_dump_json())\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx automodule for autogen_ext.tools.langchain (reStructuredText)\nDESCRIPTION: This reStructuredText snippet uses the Sphinx `automodule` directive to automatically generate API documentation for the `autogen_ext.tools.langchain` Python module. It instructs Sphinx to include documentation for all members (`:members:`), include members without docstrings (`:undoc-members:`), and show the class inheritance diagrams (`:show-inheritance:`). This requires the specified Python module to be importable in the Sphinx environment.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_ext.tools.langchain.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogen_ext.tools.langchain\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen.Gemini Package via .NET CLI\nDESCRIPTION: Command to install the AutoGen.Gemini package using the .NET CLI. This package is required to work with Gemini models through AutoGen.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Chat-with-vertex-gemini.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AutoGen.Gemini\n```\n\n----------------------------------------\n\nTITLE: Importing Required Namespaces for AutoGen OpenAIChatAgent in C#\nDESCRIPTION: This snippet shows the necessary using statements to import the required namespaces for working with AutoGen's OpenAIChatAgent.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-simple-chat.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nusing AutoGen.Core;\nusing AutoGen.OpenAI;\nusing Microsoft.SemanticKernel.ChatCompletion;\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary AutoGen Core and Extension Modules in Python\nDESCRIPTION: Imports required classes and functions from `autogen_core`, `autogen_ext.code_executors.docker`, `autogen_ext.models.openai`, and `autogen_ext.tools.code_execution`. These imports are essential for building agents, handling messages, defining tools, using Docker for code execution, and interacting with OpenAI models within the AutoGen framework.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/tool-use-with-intervention.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\nfrom typing import Any, List\n\nfrom autogen_core import (\n    AgentId,\n    AgentType,\n    DefaultInterventionHandler,\n    DropMessage,\n    FunctionCall,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    message_handler,\n)\nfrom autogen_core.models import (\n    ChatCompletionClient,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\nfrom autogen_core.tool_agent import ToolAgent, ToolException, tool_agent_caller_loop\nfrom autogen_core.tools import ToolSchema\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.code_execution import PythonCodeExecutionTool\n```\n\n----------------------------------------\n\nTITLE: Generating Streaming Responses in C#\nDESCRIPTION: Demonstrates receiving responses streamingly with GenerateStreamingReplyAsync. This is beneficial for handling large data exchanges, requiring a streaming-capable agent setup.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Chat-with-an-agent.md#2025-04-22_snippet_5\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[Generate Streaming Response](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Chat_With_Agent.cs?name=Streaming_Chat)]\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment using venv in Python\nDESCRIPTION: Commands to create and manage a Python virtual environment using venv module. Includes activation and deactivation commands.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/installation.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv .venv\nsource .venv/bin/activate\n```\n\nLANGUAGE: bash\nCODE:\n```\ndeactivate\n```\n\n----------------------------------------\n\nTITLE: Defining Model Clients in Python for AutoGen Studio\nDESCRIPTION: This code defines OpenAI, AzureOpenAI, Anthropic, and a local Mistral vLLM model client in Python using `autogen_ext` and `autogen_core` libraries.  It then dumps each model client configuration to a JSON string. It requires the installation of `autogen_ext` and `autogen_core` packages.  The code assumes that API keys and endpoint configurations are available.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/faq.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient, OpenAIChatCompletionClient\nfrom autogen_ext.models.anthropic import AnthropicChatCompletionClient\nfrom autogen_core.models import ModelInfo\n\nmodel_client=OpenAIChatCompletionClient(\n            model=\"gpt-4o-mini\",\n        )\nprint(model_client.dump_component().model_dump_json())\n\n\naz_model_client = AzureOpenAIChatCompletionClient(\n    azure_deployment=\"{your-azure-deployment}\",\n    model=\"gpt-4o\",\n    api_version=\"2024-06-01\",\n    azure_endpoint=\"https://{your-custom-endpoint}.openai.azure.com/\",\n    api_key=\"sk-...\",\n)\nprint(az_model_client.dump_component().model_dump_json())\n\nanthropic_client = AnthropicChatCompletionClient(\n        model=\"claude-3-sonnet-20240229\",\n        api_key=\"your-api-key\",  # Optional if ANTHROPIC_API_KEY is set in environment\n    )\nprint(anthropic_client.dump_component().model_dump_json())\n\nmistral_vllm_model = OpenAIChatCompletionClient(\n        model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n        base_url=\"http://localhost:1234/v1\",\n        model_info=ModelInfo(vision=False, function_calling=True, json_output=False, family=\"unknown\", structured_output=True),\n    )\nprint(mistral_vllm_model.dump_component().model_dump_json())\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Tenant Multiple Scope Communication for Tax Specialists in Python\nDESCRIPTION: This function sets up a multi-tenant environment with multiple tax specialties. It registers tax specialist agents for each specialty and tenant, adds subscriptions, and publishes client requests to tenant-specific topics. The runtime manages message distribution based on tenant and specialty subscriptions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/topic-subscription-scenarios.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nasync def run_multi_tenant_multiple_scope() -> None:\n    # Create the runtime\n    runtime = SingleThreadedAgentRuntime()\n\n    # Define TypeSubscriptions for each specialty and tenant\n    tenants = [\"ClientABC\", \"ClientXYZ\"]\n\n    # Initialize agents for all specialties and add type subscriptions\n    for specialty in TaxSpecialty:\n        specialist_agent_type = f\"TaxSpecialist_{specialty.value}\"\n        await TaxSpecialist.register(\n            runtime=runtime,\n            type=specialist_agent_type,\n            factory=lambda specialty=specialty: TaxSpecialist(  # type: ignore\n                description=f\"A tax specialist in {specialty.value}.\",\n                specialty=specialty,\n                system_messages=[SystemMessage(content=f\"You are a tax specialist in {specialty.value}.\")],\n            ),\n        )\n        for tenant in tenants:\n            specialist_subscription = TypeSubscription(\n                topic_type=f\"{tenant}_{specialty.value}\", agent_type=specialist_agent_type\n            )\n            await runtime.add_subscription(specialist_subscription)\n\n    # Start the runtime\n    runtime.start()\n\n    # Send messages for each tenant to each specialty\n    for tenant in tenants:\n        for specialty in TaxSpecialty:\n            topic_id = TopicId(type=f\"{tenant}_{specialty.value}\", source=tenant)\n            await runtime.publish_message(\n                ClientRequest(f\"{tenant} needs assistance with {specialty.value} taxes.\"),\n                topic_id=topic_id,\n            )\n\n    # Allow time for message processing\n    await asyncio.sleep(1)\n\n    # Stop the runtime when idle\n    await runtime.stop_when_idle()\n\n\nawait run_multi_tenant_multiple_scope()\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Identity Client in Shell\nDESCRIPTION: This snippet demonstrates how to install the Azure identity client, which is required to authenticate with Azure Active Directory for using Azure OpenAI services. It's essential to have pip installed to execute this command.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/azure-openai-with-aad-auth.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install azure-identity\n```\n\n----------------------------------------\n\nTITLE: Navigating to HumanEval Directory in Bash\nDESCRIPTION: Changes the current directory to the HumanEval benchmark folder.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/HumanEval/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd benchmarks/HumanEval\n```\n\n----------------------------------------\n\nTITLE: Creating Model Client with Component Config in AutoGen v0.4\nDESCRIPTION: Demonstrates how to create a model client in AutoGen v0.4 using the generic component configuration system, specifically for an OpenAI chat completion client.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core.models import ChatCompletionClient\n\nconfig = {\n    \"provider\": \"OpenAIChatCompletionClient\",\n    \"config\": {\n        \"model\": \"gpt-4o\",\n        \"api_key\": \"sk-xxx\" # os.environ[\"...']\n    }\n}\n\nmodel_client = ChatCompletionClient.load_component(config)\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Link\nDESCRIPTION: Link to the official AutoGen extensions documentation.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-ext/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# AutoGen Extensions\n\n- [Documentation](https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen.Gemini and AutoGen.SourceGenerator Packages\nDESCRIPTION: Commands to install the required NuGet packages for using AutoGen.Gemini and the source generator for function contracts.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Function-call-with-gemini.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AutoGen.Gemini\ndotnet add package AutoGen.SourceGenerator\n```\n\n----------------------------------------\n\nTITLE: Adding Required Using Statements for AutoGen and Semantic Kernel in C#\nDESCRIPTION: This snippet shows the necessary using statements to import the required namespaces for working with AutoGen and Semantic Kernel plugins.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.SemanticKernel/Use-kernel-plugin-in-other-agents.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nusing AutoGen.OpenAI;\nusing AutoGen.SemanticKernel;\nusing Microsoft.SemanticKernel;\n```\n\n----------------------------------------\n\nTITLE: Initiating a Conversation Between AI Agents in Python\nDESCRIPTION: This code snippet shows how to start a conversation between AI agents. It uses the user_proxy agent to initiate the conversation with a specified task for the assistant agent to perform.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/GAIA/Templates/MagenticOne/prompt.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nuser_proxy.initiate_chat(\n    assistant,\n    message=\"Plot a chart of NVDA and TESLA stock price change YTD.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Removing Memory Bank Directory in Bash\nDESCRIPTION: Command to delete the memory bank directory to reset the agent's memory for a fresh start.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/task_centric_memory/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nrm -r memory_bank\n```\n\n----------------------------------------\n\nTITLE: Running AutoGen Studio with Authentication via CLI Argument (Bash)\nDESCRIPTION: Demonstrates how to launch the AutoGen Studio UI with the authentication feature enabled by providing the path to the `auth.yaml` configuration file using the `--auth-config` command-line argument. This command directs AutoGen Studio to load and apply the specified authentication settings.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/experimental.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nautogenstudio ui --auth-config /path/to/auth.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Frontend Dependencies for AutoGen Studio\nDESCRIPTION: These commands demonstrate the process of installing frontend dependencies and building the UI for AutoGen Studio. It requires Node.js and npm installed, and also uses yarn and gatsby-cli globally to manage dependencies and build tasks.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/installation.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g gatsby-cli\nnpm install --global yarn\ncd frontend\nyarn install\nyarn build\n```\n\nLANGUAGE: bash\nCODE:\n```\ngatsby clean && rmdir /s /q ..\\\\autogenstudio\\\\web\\\\ui 2>nul & (set \\\"PREFIX_PATH_VALUE=\\\" || ver>nul) && gatsby build --prefix-paths && xcopy /E /I /Y public ..\\\\autogenstudio\\\\web\\\\ui\n```\n\n----------------------------------------\n\nTITLE: Creating User Proxy Agent in v0.4\nDESCRIPTION: Demonstrates the simplified approach to creating a UserProxyAgent in v0.4.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import UserProxyAgent\n\nuser_proxy = UserProxyAgent(\"user_proxy\")\n```\n\n----------------------------------------\n\nTITLE: AutoGen Message Conversion Schema\nDESCRIPTION: Complete JSON schema showing message type conversions including system messages, user interactions, assistant responses, image handling, and tool calls. Each message type includes detailed specifications for content format, roles, and special handling for different message types.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.OpenAI.Tests/ApprovalTests/OpenAIMessageTests.BasicMessageTest.approved.txt#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"OriginalMessage\": \"TextMessage(system, You are a helpful AI assistant, )\",\n    \"ConvertedMessages\": [\n      {\n        \"Name\": null,\n        \"Role\": \"system\",\n        \"Content\": [\n          {\n            \"Kind\": 0,\n            \"Text\": \"You are a helpful AI assistant\",\n            \"ImageUri\": null,\n            \"ImageBytes\": null,\n            \"ImageBytesMediaType\": null,\n            \"InputAudioBytes\": null,\n            \"InputAudioFormat\": null,\n            \"FileId\": null,\n            \"FileBytes\": null,\n            \"FileBytesMediaType\": null,\n            \"Filename\": null,\n            \"ImageDetailLevel\": null,\n            \"Refusal\": null\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"OriginalMessage\": \"TextMessage(user, Hello, user)\",\n    \"ConvertedMessages\": [\n      {\n        \"Role\": \"user\",\n        \"Content\": [\n          {\n            \"Kind\": 0,\n            \"Text\": \"Hello\",\n            \"ImageUri\": null,\n            \"ImageBytes\": null,\n            \"ImageBytesMediaType\": null,\n            \"InputAudioBytes\": null,\n            \"InputAudioFormat\": null,\n            \"FileId\": null,\n            \"FileBytes\": null,\n            \"FileBytesMediaType\": null,\n            \"Filename\": null,\n            \"ImageDetailLevel\": null,\n            \"Refusal\": null\n          }\n        ],\n        \"Name\": \"user\",\n        \"MultiModaItem\": [\n          {\n            \"Type\": \"Text\",\n            \"Text\": \"Hello\"\n          }\n        ]\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Studio from PyPi\nDESCRIPTION: Command to install AutoGen Studio using pip package manager. This is the recommended installation method for most users who don't need to modify the source code.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-studio/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U autogenstudio\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Core Components\nDESCRIPTION: Commands for installing the main AutoGen packages, including AgentChat and OpenAI extensions. The installation requires Python 3.10 or later.\nSOURCE: https://github.com/microsoft/autogen/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install AgentChat and OpenAI client from Extensions\npip install -U \"autogen-agentchat\" \"autogen-ext[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Adding Required Namespace Imports for Gemini Integration\nDESCRIPTION: Using statements required for working with the AutoGen.Gemini package. These imports provide access to the necessary classes and methods for creating and using Gemini agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Chat-with-google-gemini.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nusing Microsoft.Extensions.DependencyInjection;\nusing Microsoft.KernelMemory;\nusing Microsoft.AutoGen;\nusing Microsoft.AutoGen.Gemini;\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Package via .NET CLI\nDESCRIPTION: Command to install the AutoGen NuGet package into a .NET project using the dotnet CLI.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Image-chat-with-agent.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AutoGen\n```\n\n----------------------------------------\n\nTITLE: Loading Components from Config in Python - AutoGen\nDESCRIPTION: Example showing how to load a chat completion client component from a configuration object using the load_component method.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/component-config.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core.models import ChatCompletionClient\n\nconfig = {\n    \"provider\": \"openai_chat_completion_client\",\n    \"config\": {\"model\": \"gpt-4o\"},\n}\n\nclient = ChatCompletionClient.load_component(config)\n```\n\n----------------------------------------\n\nTITLE: Cloning AutoGen Repository and Setting Environment Variable in Bash\nDESCRIPTION: This snippet shows how to clone the AutoGen repository from GitHub and set the AUTOGEN_REPO_BASE environment variable. This variable is crucial for Docker containers to use the correct version of agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:microsoft/autogen.git\nexport AUTOGEN_REPO_BASE=<path_to_autogen>\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen via pip\nDESCRIPTION: This snippet shows how to install AutoGen using pip, the Python package installer. It includes options for installing with or without UI-related dependencies.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/HumanEval/Templates/AgentChat/prompt.txt#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```shell\npip install pyautogen\n# or\npip install \"pyautogen[blendsearch]\"\n# or\npip install \"pyautogen[teachable]\"\n# or (install all optional dependencies)\npip install \"pyautogen[all]\"\n```\n```\n\n----------------------------------------\n\nTITLE: GAIA Directory Structure\nDESCRIPTION: Example of the expected directory structure after initialization of GAIA benchmark\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/GAIA/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n.\n./Downloads\n./Downloads/GAIA\n./Downloads/GAIA/2023\n./Downloads/GAIA/2023/test\n./Downloads/GAIA/2023/validation\n./Scripts\n./Templates\n./Templates/TeamOne\n```\n\n----------------------------------------\n\nTITLE: Defining Message Types in Python using Dataclasses\nDESCRIPTION: Demonstrates how to define message types using Python dataclasses for text and image messages in AutoGen core.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/message-and-communication.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\n@dataclass\nclass TextMessage:\n    content: str\n    source: str\n\n@dataclass\nclass ImageMessage:\n    url: str\n    source: str\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Extension for Semantic Kernel with Anthropic\nDESCRIPTION: Command to install the AutoGen extension that includes Semantic Kernel support for Anthropic models.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npip install \"autogen-ext[semantic-kernel-anthropic]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Streamlit Package with Pip\nDESCRIPTION: Command to install the Streamlit package, which is required to run the sample AI chat assistant application.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_streamlit/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install streamlit\n```\n\n----------------------------------------\n\nTITLE: Using New Gemini Models with Custom Capabilities\nDESCRIPTION: Shows how to use newer Gemini models by explicitly defining their capabilities using the model_info parameter. This is necessary for models that may not have their capabilities pre-defined in AutoGen.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_core.models import ModelInfo\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gemini-2.0-flash-lite\",\n    model_info=ModelInfo(vision=True, function_calling=True, json_output=True, family=\"unknown\", structured_output=True)\n    # api_key=\"GEMINI_API_KEY\",\n)\n\nresponse = await model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\nprint(response)\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Creating a Semantic Kernel Instance in C#\nDESCRIPTION: This code snippet demonstrates how to create a Semantic Kernel instance using the KernelBuilder. It configures the kernel with an OpenAI chat completion service.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.SemanticKernel/SemanticKernelChatAgent-simple-chat.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nvar builder = Kernel.CreateBuilder();\nbuilder.AddOpenAIChatCompletion(\n    \"gpt-3.5-turbo\",\n    Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\") ?? \"\");\nvar kernel = builder.Build();\n```\n\n----------------------------------------\n\nTITLE: Processing Collected Results\nDESCRIPTION: Demonstrates how to process the results collected from the message queue.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/concurrent-agents.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwhile not queue.empty():\n    print(await queue.get())\n```\n\n----------------------------------------\n\nTITLE: Starting the agent runtime and publishing results in Python\nDESCRIPTION: Demonstrates the lifecycle of the SingleThreadedAgentRuntime: starting it, publishing two FinalResult messages to the default topic, and shutting it down when idle. This simulates how results would be funneled through the agent system and made available for extraction via the shared queue.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/extracting-results-with-an-agent.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nruntime.start()\\nawait runtime.publish_message(FinalResult(\"Result 1\"), DefaultTopicId())\\nawait runtime.publish_message(FinalResult(\"Result 2\"), DefaultTopicId())\\nawait runtime.stop_when_idle()\n```\n\n----------------------------------------\n\nTITLE: Installing LiteLLM and Starting Proxy Server\nDESCRIPTION: This snippet demonstrates how to install LiteLLM and start the proxy server for the Ollama model.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Function-call-with-ollama-and-litellm.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install 'litellm[proxy]'\n```\n\nLANGUAGE: bash\nCODE:\n```\nlitellm --model ollama_chat/dolphincoder --port 4000\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for AgentChat and FastAPI\nDESCRIPTION: Installs the necessary Python packages including autogen-ext with OpenAI support, FastAPI, uvicorn, and PyYAML using pip.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_fastapi/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"autogen-ext[openai]\" \"fastapi\" \"uvicorn\" \"PyYAML\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Termination Conditions\nDESCRIPTION: Defines conditions for terminating the group chat conversation based on specific criteria, such as the mention of a keyword or message count limit.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/selector-group-chat.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntext_mention_termination = TextMentionTermination(\"TERMINATE\")\nmax_messages_termination = MaxMessageTermination(max_messages=25)\ntermination = text_mention_termination | max_messages_termination\n```\n\n----------------------------------------\n\nTITLE: Markdown TODO Placeholder\nDESCRIPTION: A simple markdown heading indicating a TODO section\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/dev-team/seed-memory/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# TODO\n```\n\n----------------------------------------\n\nTITLE: Using ChatResult in AutoGen v0.2\nDESCRIPTION: Example of initiating a chat and accessing ChatResult properties including summary, chat history, cost, and human input in AutoGen v0.2.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nchat_result = tool_executor.initiate_chat(\n    tool_caller,\n    message=user_input,\n    summary_method=\"reflection_with_llm\",\n)\nprint(chat_result.summary) # Get LLM-reflected summary of the chat.\nprint(chat_result.chat_history) # Get the chat history.\nprint(chat_result.cost) # Get the cost of the chat.\nprint(chat_result.human_input) # Get the human input solicited by the chat.\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Event Handler\nDESCRIPTION: Example of how to configure and attach a custom event handler to a logger instance.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/logging.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlogger = logging.getLogger(EVENT_LOGGER_NAME)\nlogger.setLevel(logging.INFO)\nmy_handler = MyHandler()\nlogger.handlers = [my_handler]\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen using pip in Python\nDESCRIPTION: This snippet shows how to install AutoGen using pip, the Python package installer. It includes optional dependencies for enhanced functionality.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/src/agbench/template/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pyautogen\n```\n\n----------------------------------------\n\nTITLE: Visualizing Event-Driven Agent Workflow with Mermaid Diagram\nDESCRIPTION: A mermaid flowchart diagram that illustrates the event-driven workflow between different AI agents. It shows the complete process from creating a new issue to committing generated code to a branch, including intermediary steps like readme generation, development planning, and code execution in a sandbox environment.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/dev-team/README.md#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD;\n    NEA([NewAsk event]) -->|Hubber| NEA1[Creation of PM issue, DevLead issue, and new branch];\n    \n    RR([ReadmeRequested event]) -->|ProductManager| PM1[Generation of new README];\n    NEA1 --> RR;\n    PM1 --> RG([ReadmeGenerated event]);\n    RG -->|Hubber| RC[Post the readme as a new comment on the issue];\n    RC --> RCC([ReadmeChainClosed event]);\n    RCC -->|ProductManager| RCR([ReadmeCreated event]);\n    RCR --> |AzureGenie| RES[Store Readme in blob storage];\n    RES --> RES2([ReadmeStored event]);\n    RES2 --> |Hubber| REC[Readme commited to branch and create new PR];\n\n    DPR([DevPlanRequested event]) -->|DeveloperLead| DPG[Generation of new development plan];\n    NEA1 --> DPR;\n    DPG --> DPGE([DevPlanGenerated event]);\n    DPGE -->|Hubber| DPGEC[Posting the plan as a new comment on the issue];\n    DPGEC --> DPCC([DevPlanChainClosed event]);\n    DPCC -->|DeveloperLead| DPCE([DevPlanCreated event]);\n    DPCE --> |Hubber| DPC[Creates a Dev issue for each subtask];\n\n    DPC([CodeGenerationRequested event]) -->|Developer| CG[Generation of new code];\n    CG --> CGE([CodeGenerated event]);\n    CGE -->|Hubber| CGC[Posting the code as a new comment on the issue];\n    CGC --> CCCE([CodeChainClosed event]);\n    CCCE -->|Developer| CCE([CodeCreated event]);\n    CCE --> |AzureGenie| CS[Store code in blob storage and schedule a run in the sandbox];\n    CS --> SRC([SandboxRunCreated event]);\n    SRC --> |Sandbox| SRM[Check every minute if the run finished];\n    SRM --> SRF([SandboxRunFinished event]);\n    SRF --> |Hubber| SRCC[Code files commited to branch];\n```\n\n----------------------------------------\n\nTITLE: Defining Light Control Plugin API in JSON\nDESCRIPTION: JSON representation of the API for a light control plugin. It defines two methods: GetState to retrieve the current light state, and ChangeState to modify the light state. The ChangeState method requires a boolean parameter to set the new state.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.SemanticKernel.Tests/ApprovalTests/KernelFunctionExtensionTests.ItCreateFunctionContractsFromTestPlugin.approved.txt#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"ClassName\": \"test_plugin\",\n    \"Name\": \"GetState\",\n    \"Description\": \"Gets the state of the light.\",\n    \"Parameters\": [],\n    \"ReturnType\": \"System.String, System.Private.CoreLib, Version=8.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e\",\n    \"ReturnDescription\": \"\"\n  },\n  {\n    \"ClassName\": \"test_plugin\",\n    \"Name\": \"ChangeState\",\n    \"Description\": \"Changes the state of the light.'\",\n    \"Parameters\": [\n      {\n        \"Name\": \"newState\",\n        \"Description\": \"new state\",\n        \"ParameterType\": \"System.Boolean, System.Private.CoreLib, Version=8.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e\",\n        \"IsRequired\": true\n      }\n    ],\n    \"ReturnType\": \"System.String, System.Private.CoreLib, Version=8.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e\",\n    \"ReturnDescription\": \"\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Documenting AutoGen Agent Chat Module in Python\nDESCRIPTION: This code snippet uses Sphinx directives to automatically generate documentation for the autogen_agentchat.agents module. It includes all members, undocumented members, and shows the inheritance structure of the module.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_agentchat.agents.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: autogen_agentchat.agents\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Adding Required Using Statements for AutoGen.Ollama\nDESCRIPTION: Namespace imports required to use AutoGen.Ollama.OllamaAgent in a C# application. These using statements make the necessary types available in the code file.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Ollama/Chat-with-llama.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nusing System.Threading.Tasks;\nusing AutoGen.Ollama;\nusing Microsoft.Extensions.Logging;\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Source Generator Package\nDESCRIPTION: Command to add the AutoGen.SourceGenerator package to a .NET project using the dotnet CLI.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Create-type-safe-function-call.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AutoGen.SourceGenerator\n```\n\n----------------------------------------\n\nTITLE: Registering and Testing a Type-Routed Agent in AutoGen Core\nDESCRIPTION: Demonstrates how to register a type-routed agent with the runtime and test it with different message types in AutoGen core.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/message-and-communication.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nruntime = SingleThreadedAgentRuntime()\nawait MyAgent.register(runtime, \"my_agent\", lambda: MyAgent(\"My Agent\"))\n\nruntime.start()\nagent_id = AgentId(\"my_agent\", \"default\")\nawait runtime.send_message(TextMessage(content=\"Hello, World!\", source=\"User\"), agent_id)\nawait runtime.send_message(ImageMessage(url=\"https://example.com/image.jpg\", source=\"User\"), agent_id)\nawait runtime.stop_when_idle()\n```\n\n----------------------------------------\n\nTITLE: Curl Example for Chat API Request\nDESCRIPTION: Example of how to make a POST request to the chat API endpoint using curl with streaming support.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_streaming_response_fastapi/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -N -X POST http://localhost:8501/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"messages\": [\n    {\"source\": \"user\", \"content\": \"Hello, I\\'m Tory.\"},\n    {\"source\": \"assistant\", \"content\": \"Hello Tory, nice to meet you!\"},\n    {\"source\": \"user\", \"content\": \"Say hello by my name and introduce yourself.\"}\n  ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Running Memory Retrieval Evaluation\nDESCRIPTION: Command to execute the memory retrieval evaluation sample using the retrieval configuration file.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/task_centric_memory/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython eval_retrieval.py configs/retrieval.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen with optional dependencies in Python\nDESCRIPTION: This command installs AutoGen with additional dependencies for enhanced functionality, including blendsearch for hyperparameter optimization.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/src/agbench/template/requirements.txt#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install \"pyautogen[blendsearch]\"\n```\n\n----------------------------------------\n\nTITLE: Creating Using Statements for AutoGen Image Chat\nDESCRIPTION: Using statements required to import necessary AutoGen namespaces for image chat functionality.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Image-chat-with-agent.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[Using Statements](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Image_Chat_With_Agent.cs?name=Using)]\n```\n\n----------------------------------------\n\nTITLE: Basic usage of AutoGen in Python\nDESCRIPTION: This code snippet demonstrates the basic usage of AutoGen. It creates two agents (a user proxy and an assistant) and initiates a conversation between them to solve a math problem.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/src/agbench/template/requirements.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n\n# Load LLM inference endpoints from an env variable or a file\nconfig_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\nassistant = AssistantAgent(name=\"assistant\", llm_config={\"config_list\": config_list})\nuser_proxy = UserProxyAgent(name=\"human\", code_execution_config={\"work_dir\": \"coding\"})\n\n# Start a conversation\nuser_proxy.initiate_chat(\n    assistant,\n    message=\"Plot a chart of NVDA and TESLA stock price change YTD.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Testing Learning from Demonstration\nDESCRIPTION: Command to evaluate how an agent learns from user demonstrations of similar tasks.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/task_centric_memory/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython eval_learning_from_demonstration.py configs/demonstration.yaml\n```\n\n----------------------------------------\n\nTITLE: Implementing Hello World Agent in .NET\nDESCRIPTION: Implementation of a basic Hello World agent using AutoGen.Net with an OpenAI-compatible endpoint. The code creates a minimal web application that exposes the agent through a REST API endpoint.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Use-AutoGen.Net-agent-as-model-in-AG-Studio.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nusing AutoGen.Core;\nusing AutoGen.Service;\n\nvar builder = WebApplication.CreateBuilder(args);\nvar app = builder.Build();\n\nvar helloWorldAgent = new HelloWorldAgent();\napp.UseAgentAsOpenAIChatCompletionEndpoint(helloWorldAgent);\n\napp.Run();\n\nclass HelloWorldAgent : IAgent\n{\n    public string Name => \"HelloWorld\";\n\n    public Task<IMessage> GenerateReplyAsync(IEnumerable<IMessage> messages, GenerateReplyOptions? options = null, CancellationToken cancellationToken = default)\n    {\n        return Task.FromResult<IMessage>(new TextMessage(Role.Assistant, \"Hello World!\", from: this.Name));\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding AutoGen.DotnetInteractive Package Reference\nDESCRIPTION: This XML snippet shows how to add the AutoGen.DotnetInteractive package reference to a project file. This package is required for integrating with dotnet-interactive.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Run-dotnet-code.md#2025-04-22_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<PackageReference Include=\"AutoGen.DotnetInteractive\" />\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Multi-Agent Debate\nDESCRIPTION: Imports necessary modules and classes from AutoGen and other libraries for implementing the multi-agent debate pattern.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/multi-agent-debate.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport re\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\nfrom autogen_core import (\n    DefaultTopicId,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    TypeSubscription,\n    default_subscription,\n    message_handler,\n)\nfrom autogen_core.models import (\n    AssistantMessage,\n    ChatCompletionClient,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\n----------------------------------------\n\nTITLE: Generating Module Documentation with Sphinx automodule in reStructuredText\nDESCRIPTION: This reStructuredText snippet uses the Sphinx `automodule` directive to automatically pull documentation from the specified Python module (`autogen_agentchat.messages`). The options `:members:`, `:undoc-members:`, and `:show-inheritance:` instruct Sphinx to include documentation for all members (even those without docstrings) and to display the class inheritance diagrams. This is commonly used in Sphinx projects to generate API documentation from source code.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_agentchat.messages.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogen_agentchat.messages\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Implementing AddAsync Wrapper Method with JSON Deserialization\nDESCRIPTION: Wrapper method that deserializes JSON arguments into the AddAsyncSchema type and calls the underlying AddAsync method. Uses System.Text.Json for deserialization with camelCase naming policy.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.SourceGenerator.Tests/ApprovalTests/FunctionCallTemplateTests.TestFunctionCallTemplate.approved.txt#2025-04-22_snippet_1\n\nLANGUAGE: C#\nCODE:\n```\npublic System.Threading.Tasks.Task`1[System.String] AddAsyncWrapper(string arguments)\n{\n    var schema = JsonSerializer.Deserialize<AddAsyncSchema>(\n        arguments, \n        new JsonSerializerOptions\n        {\n            PropertyNamingPolicy = JsonNamingPolicy.CamelCase,\n        });\n\n    return AddAsync(schema.a, schema.b);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Complex Agent Team for Travel Planning\nDESCRIPTION: This extensive snippet demonstrates how to create a complex agent team for travel planning using various AssistantAgent instances and a RoundRobinGroupChat.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-studio/notebooks/tutorial.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat, SelectorGroupChat\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nplanner_agent = AssistantAgent(\n    \"planner_agent\",\n    model_client=OpenAIChatCompletionClient(model=\"gpt-4\"),\n    description=\"A helpful assistant that can plan trips.\",\n    system_message=\"You are a helpful assistant that can suggest a travel plan for a user based on their request. Respond with a single sentence\",\n)\n\nlocal_agent = AssistantAgent(\n    \"local_agent\",\n    model_client=OpenAIChatCompletionClient(model=\"gpt-4\"),\n    description=\"A local assistant that can suggest local activities or places to visit.\",\n    system_message=\"You are a helpful assistant that can suggest authentic and interesting local activities or places to visit for a user and can utilize any context information provided. Respond with a single sentence\",\n)\n\nlanguage_agent = AssistantAgent(\n    \"language_agent\",\n    model_client=OpenAIChatCompletionClient(model=\"gpt-4\"),\n    description=\"A helpful assistant that can provide language tips for a given destination.\",\n    system_message=\"You are a helpful assistant that can review travel plans, providing feedback on important/critical tips about how best to address language or communication challenges for the given destination. If the plan already includes language tips, you can mention that the plan is satisfactory, with rationale.Respond with a single sentence\",\n)\n\ntravel_summary_agent = AssistantAgent(\n    \"travel_summary_agent\",\n    model_client=OpenAIChatCompletionClient(model=\"gpt-4\"),\n    description=\"A helpful assistant that can summarize the travel plan.\",\n    system_message=\"You are a helpful assistant that can take in all of the suggestions and advice from the other agents and provide a detailed tfinal travel plan. You must ensure th b at the final plan is integrated and complete. YOUR FINAL RESPONSE MUST BE THE COMPLETE PLAN. When the plan is complete and all perspectives are integrated, you can respond with TERMINATE.Respond with a single sentence\",\n)\n\ntermination = TextMentionTermination(\"TERMINATE\") | MaxMessageTermination(10)\ngroup_chat = RoundRobinGroupChat(\n    [planner_agent, local_agent, language_agent, travel_summary_agent], termination_condition=termination\n)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen 0.2 using pip\nDESCRIPTION: This command installs the AutoGen 0.2 version using pip. It specifies the package name as 'autogen-agentchat' with a version constraint of ~=0.2 to ensure compatibility with the 0.2.x series.\nSOURCE: https://github.com/microsoft/autogen/blob/main/FAQ.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install autogen-agentchat~=0.2\n```\n\n----------------------------------------\n\nTITLE: Sample Agent Interaction with Local Search Tool\nDESCRIPTION: Example output showing the agent's function call to the local_search_tool with a specific query, and the resulting execution response containing information retrieved from the indexed data.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_graphrag/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nAgent response: [FunctionCall(id='call_0xAXMOHLl62QFd9cfIb0S3BO', arguments='{\"query\":\"station-master Dr. Becher\"}', name='local_search_tool')]\n\nAgent response: [FunctionExecutionResult(content='{\"answer\": \"### Dr. Becher and the Station-Master\\\\n\\\\nDr. Becher is an Englishman who owns a house that caught fire, and he has a foreign patient staying with him [Data: Entities (489)]. The fire at Dr. Becher\\'s house was a significant event, as it was described as a great widespread whitewashed building spouting fire at every chink and window, with fire-engines striving to control the blaze [Data: Sources (91); Entities (491)]. The station-master provided information about the fire, confirming that it broke out during the night and worsened, leading to the entire place being in a blaze [Data: Sources (91)].\\\\n\\\\nThe station-master also clarified a misunderstanding about Dr. Becher\\'s nationality, stating that Dr. Becher is an Englishman, contrary to the engineer\\'s assumption that he might be a German. The station-master humorously noted that Dr. Becher is well-fed, unlike his foreign patient, who could benefit from some good Berkshire beef [Data: Sources (91)].\\\\n\\\\n### The Fire Incident\\\\n\\\\nThe fire at Dr. Becher\\'s house was linked to a larger criminal investigation involving a gang of coiners. The fire was inadvertently started by an oil-lamp that was crushed in a press, which was part of the machinery used by the gang. This incident was a turning point in the investigation, as it led to the discovery of the gang\\'s operations, although the criminals managed to escape [Data: Sources (91)].\\\\n\\\\nThe fire-engines present at the scene were unable to prevent the destruction of the house, and the firemen were perturbed by the strange arrangements they found within the building. Despite their efforts, the house was reduced to ruins, with only some twisted cylinders and iron piping remaining [Data: Sources (91); Entities (491)].\\\\n\\\\nIn summary, Dr. Becher\\'s house fire was a pivotal event in the investigation of a criminal gang, with the station-master providing key information about the incident and Dr. Becher\\'s identity. The fire not only highlighted the dangers associated with the gang\\'s activities but also underscored the challenges faced by law enforcement in apprehending the criminals.\"}', call_id='call_0xAXMOHLl62QFd9cfIb0S3BO')]\n```\n\n----------------------------------------\n\nTITLE: Defining LlamaIndex Agent Class (Python)\nDESCRIPTION: This code defines a `LlamaIndexAgent` class that inherits from `RoutedAgent`. It initializes with a description, a LlamaIndex `AgentRunner`, and an optional memory buffer. The `handle_user_message` method processes user messages using the LlamaIndex agent, retrieves relevant history, constructs the response, and manages message history in the memory buffer.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/llamaindex-agent.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass LlamaIndexAgent(RoutedAgent):\n    def __init__(self, description: str, llama_index_agent: AgentRunner, memory: BaseMemory | None = None) -> None:\n        super().__init__(description)\n\n        self._llama_index_agent = llama_index_agent\n        self._memory = memory\n\n    @message_handler\n    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n        # retriever history messages from memory!\n        history_messages: List[ChatMessage] = []\n\n        response: AgentChatResponse  # pyright: ignore\n        if self._memory is not None:\n            history_messages = self._memory.get(input=message.content)\n\n            response = await self._llama_index_agent.achat(message=message.content, history_messages=history_messages)  # pyright: ignore\n        else:\n            response = await self._llama_index_agent.achat(message=message.content)  # pyright: ignore\n\n        if isinstance(response, AgentChatResponse):\n            if self._memory is not None:\n                self._memory.put(ChatMessage(role=MessageRole.USER, content=message.content))\n                self._memory.put(ChatMessage(role=MessageRole.ASSISTANT, content=response.response))\n\n            assert isinstance(response.response, str)\n\n            resources: List[Resource] = [\n                Resource(content=source_node.get_text(), score=source_node.score, node_id=source_node.id_)\n                for source_node in response.source_nodes\n            ]\n\n            tools: List[Resource] = [\n                Resource(content=source.content, node_id=source.tool_name) for source in response.sources\n            ]\n\n            resources.extend(tools)\n            return Message(content=response.response, sources=resources)\n        else:\n            return Message(content=\"I'm sorry, I don't have an answer for you.\")\n```\n\n----------------------------------------\n\nTITLE: Creating an OpenAIChatAgent in C#\nDESCRIPTION: Demonstrates initializing an OpenAIChatAgent in C#, which is necessary for interacting with OpenAI services through AutoGen. Relies on RegisterMessageConnector to handle message types.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Chat-with-an-agent.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[Create an OpenAIChatAgent](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Chat_With_Agent.cs?name=Create_Agent)]\n```\n\n----------------------------------------\n\nTITLE: Importing Required Modules for Azure Code Execution\nDESCRIPTION: Imports necessary modules and classes for using the ACADynamicSessionsCodeExecutor, including Azure identity and autogen components.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/extensions-user-guide/azure-container-code-executor.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tempfile\n\nfrom anyio import open_file\nfrom autogen_core import CancellationToken\nfrom autogen_core.code_executor import CodeBlock\nfrom autogen_ext.code_executors.azure import ACADynamicSessionsCodeExecutor\nfrom azure.identity import DefaultAzureCredential\n```\n\n----------------------------------------\n\nTITLE: Loading Team Configuration with AgentChat API\nDESCRIPTION: This snippet demonstrates how to load a team configuration directly using the AgentChat API and BaseGroupChat class.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-studio/notebooks/tutorial.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json \nfrom autogen_agentchat.teams import BaseGroupChat\nteam_config = json.load(open(\"team.json\"))  \nteam = BaseGroupChat.load_component(team_config)\nprint(team._participants)\n```\n\n----------------------------------------\n\nTITLE: Running AutoGen 0.4 .NET Hello World Sample\nDESCRIPTION: Commands to clone the repository, navigate to the sample directory, and run the application using .NET CLI.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/Hello/HelloAgentState/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository\ngh repo clone microsoft/autogen\ncd dotnet/samples/Hello\ndotnet run\n```\n\n----------------------------------------\n\nTITLE: Deploying Azure Resources with Azure Developer CLI\nDESCRIPTION: Commands for logging into Azure, creating a new environment, and provisioning Azure resources using the Azure Developer CLI (azd). This process sets up the necessary Azure infrastructure for the AutoGen project.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/dev-team/docs/github-flow-getting-started.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nazd auth login\n```\n\nLANGUAGE: bash\nCODE:\n```\nENVIRONMENT=_name_of_your_env\nazd env new $ENVIRONMENT\nazd provision -e $ENVIRONMENT\n```\n\nLANGUAGE: bash\nCODE:\n```\nazd env get-values -e dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Agent System with .NET Aspire\nDESCRIPTION: Example of configuring a distributed system with multiple agents using .NET Aspire, including cross-language support for Python agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/index.md#2025-04-22_snippet_6\n\nLANGUAGE: csharp\nCODE:\n```\n// Copyright (c) Microsoft Corporation. All rights reserved.\n// Program.cs\n\nusing Microsoft.Extensions.Hosting;\n\nvar builder = DistributedApplication.CreateBuilder(args);\nvar backend = builder.AddProject<Projects.Microsoft_AutoGen_AgentHost>(\"backend\").WithExternalHttpEndpoints();\nvar client = builder.AddProject<Projects.HelloAgent>(\"HelloAgentsDotNET\")\n    .WithReference(backend)\n    .WithEnvironment(\"AGENT_HOST\", backend.GetEndpoint(\"https\"))\n    .WithEnvironment(\"STAY_ALIVE_ON_GOODBYE\", \"true\")\n    .WaitFor(backend);\n// xlang is over http for now - in prod use TLS between containers\nbuilder.AddPythonApp(\"HelloAgentsPython\", \"../../../../python/samples/core_xlang_hello_python_agent\", \"hello_python_agent.py\", \"../../.venv\")\n    .WithReference(backend)\n    .WithEnvironment(\"AGENT_HOST\", backend.GetEndpoint(\"http\"))\n    .WithEnvironment(\"STAY_ALIVE_ON_GOODBYE\", \"true\")\n    .WithEnvironment(\"GRPC_DNS_RESOLVER\", \"native\")\n    .WithOtlpExporter()\n    .WaitFor(client);\nusing var app = builder.Build();\nawait app.StartAsync();\nvar url = backend.GetEndpoint(\"http\").Url;\nConsole.WriteLine(\"Backend URL: \" + url);\nawait app.WaitForShutdownAsync();\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Automodule for autogen_ext.tools.semantic_kernel (reStructuredText)\nDESCRIPTION: This reStructuredText directive instructs the Sphinx documentation generator to automatically build documentation for the `autogen_ext.tools.semantic_kernel` Python module. It uses options to include all members (`:members:`), members without docstrings (`:undoc-members:`), and class inheritance information (`:show-inheritance:`). This requires Sphinx to be set up and the target module to be importable.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_ext.tools.semantic_kernel.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogen_ext.tools.semantic_kernel\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Define TaxSpecialty Enum and Initialize ClientRequest Class in Python\nDESCRIPTION: Defines an enumeration for various tax specialties and initializes the ClientRequest and TaxSpecialist classes. The TaxSpecialist class inherits from RoutedAgent and processes messages. Necessary dependencies include enums and dataclasses. It also requires the AutoGen framework for context handling and message publishing.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/topic-subscription-scenarios.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass TaxSpecialty(str, Enum):\n    PLANNING = \"planning\"\n    DISPUTE_RESOLUTION = \"dispute_resolution\"\n    COMPLIANCE = \"compliance\"\n    PREPARATION = \"preparation\"\n\n\n@dataclass\nclass ClientRequest:\n    content: str\n\n\n@dataclass\nclass RequestAssessment:\n    content: str\n\n\nclass TaxSpecialist(RoutedAgent):\n    def __init__(\n        self,\n        description: str,\n        specialty: TaxSpecialty,\n        system_messages: List[SystemMessage],\n    ) -> None:\n        super().__init__(description)\n        self.specialty = specialty\n        self._system_messages = system_messages\n        self._memory: List[ClientRequest] = []\n\n    @message_handler\n    async def handle_message(self, message: ClientRequest, ctx: MessageContext) -> None:\n        # Process the client request.\n        print(f\"\\n{'='*50}\\nTax specialist {self.id} with specialty {self.specialty}:\\n{message.content}\")\n        # Send a response back to the manager\n        if ctx.topic_id is None:\n            raise ValueError(\"Topic ID is required for broadcasting\")\n        await self.publish_message(\n            message=RequestAssessment(content=f\"I can handle this request in {self.specialty}.\"),\n            topic_id=ctx.topic_id,\n        )\n```\n\n----------------------------------------\n\nTITLE: Defining Query Function Schema in JSON\nDESCRIPTION: JSON schema that defines a query function with required and optional parameters. It specifies 'query' as a required string parameter, while 'k' (integer for top k results, default 3) and 'threshold' (number for threshold value, default 0.5) are optional parameters.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.SourceGenerator.Tests/ApprovalTests/FunctionExample.Query_Test.approved.txt#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"Query\",\n  \"description\": \"query function\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"query\": {\n        \"type\": \"string\",\n        \"description\": \"query, required\"\n      },\n      \"k\": {\n        \"type\": \"integer\",\n        \"description\": \"top k, optional, default value is 3\"\n      },\n      \"thresold\": {\n        \"type\": \"number\",\n        \"description\": \"thresold, optional, default value is 0.5\"\n      }\n    },\n    \"required\": [\n      \"query\"\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Extensions\nDESCRIPTION: Commands to install AutoGen extensions for OpenAI and Azure OpenAI model support.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_chess_game/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install \"autogen-ext[openai]\"\n# pip install \"autogen-ext[openai,azure]\" for Azure OpenAI models\n```\n\n----------------------------------------\n\nTITLE: Defining Person Class for JSON Deserialization in C#\nDESCRIPTION: This snippet defines a Person class used for deserializing the JSON response from the OpenAIChatAgent. It includes properties for Name, Age, and Address.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-use-json-mode.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\npublic class Person\n{\n    public string Name { get; set; }\n    public int Age { get; set; }\n    public string? Address { get; set; }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Package in .NET\nDESCRIPTION: Installs the AutoGen package to prepare for setting up response generation agents. This assumes a .NET environment with access to NUGET package manager.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Chat-with-an-agent.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AutoGen\n```\n\n----------------------------------------\n\nTITLE: Example JSON Output for Literary Analysis Report\nDESCRIPTION: A complete example of a formatted JSON response for a literary analysis report about 'Abila City Park and POK Rally', showing proper structure with title, summary, rating, and detailed findings.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_graphrag/prompts/community_report.txt#2025-04-22_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"title\": \"Abila City Park and POK Rally\",\n    \"summary\": \"The community revolves around the Abila City Park, which is the location of the POK rally. The park has relationships with POK, POKRALLY, and Central Bulletin, all\nof which are associated with the rally event.\",\n    \"rating\": 5.0,\n    \"rating_explanation\": \"The impact rating is moderate due to the potential for unrest or conflict during the POK rally.\",\n    \"findings\": [\n        {\n            \"summary\": \"Abila City Park as the central location\",\n            \"explanation\": \"Abila City Park is the central entity in this community, serving as the location for the POK rally. This park is the common link between all other\nentities, suggesting its significance in the community. The park's association with the rally could potentially lead to issues such as public disorder or conflict, depending on the\nnature of the rally and the reactions it provokes. [records: Entities (5), Relationships (37, 38, 39, 40)]\"\n        },\n        {\n            \"summary\": \"POK's role in the community\",\n            \"explanation\": \"POK is another key entity in this community, being the organizer of the rally at Abila City Park. The nature of POK and its rally could be a potential\nsource of threat, depending on their objectives and the reactions they provoke. The relationship between POK and the park is crucial in understanding the dynamics of this community.\n[records: Relationships (38)]\"\n        },\n        {\n            \"summary\": \"POKRALLY as a significant event\",\n            \"explanation\": \"The POKRALLY is a significant event taking place at Abila City Park. This event is a key factor in the community's dynamics and could be a potential\nsource of threat, depending on the nature of the rally and the reactions it provokes. The relationship between the rally and the park is crucial in understanding the dynamics of this\ncommunity. [records: Relationships (39)]\"\n        },\n        {\n            \"summary\": \"Role of Central Bulletin\",\n            \"explanation\": \"Central Bulletin is reporting on the POK rally taking place in Abila City Park. This suggests that the event has attracted media attention, which could\namplify its impact on the community. The role of Central Bulletin could be significant in shaping public perception of the event and the entities involved. [records: Relationships\n(40)]\"\n        }\n    ]\n\n}\n```\n\n----------------------------------------\n\nTITLE: Aborting an AutoGen Team Run using CancellationToken in Python\nDESCRIPTION: This snippet illustrates how to immediately abort the execution of an AutoGen team run using a `CancellationToken`. A `CancellationToken` is created and passed to the `team.run()` method, which is executed in a background task. Calling `cancellation_token.cancel()` triggers an immediate halt of the team's execution, raising an `asyncio.CancelledError` which is caught in the example.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/teams.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Create a cancellation token.\ncancellation_token = CancellationToken()\n\n# Use another coroutine to run the team.\nrun = asyncio.create_task(\n    team.run(\n        task=\"Translate the poem to Spanish.\",\n        cancellation_token=cancellation_token,\n    )\n)\n\n# Cancel the run.\ncancellation_token.cancel()\n\ntry:\n    result = await run  # This will raise a CancelledError.\nexcept asyncio.CancelledError:\n    print(\"Task was cancelled.\")\n```\n\n----------------------------------------\n\nTITLE: Running Jaeger All-in-One with Docker (Bash)\nDESCRIPTION: Starts a Jaeger instance using Docker. This command downloads the 'jaegertracing/all-in-one' image and runs it in detached mode, exposing the Jaeger UI on port 16686 and the OTLP gRPC receiver on port 4317. It enables the OTLP collector, allowing the AutoGen application to send traces to this Jaeger instance.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tracing.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d --name jaeger \\\n  -e COLLECTOR_OTLP_ENABLED=true \\\n  -p 16686:16686 \\\n  -p 4317:4317 \\\n  -p 4318:4318 \\\n  jaegertracing/all-in-one:latest\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Message Types in Protocol Buffers\nDESCRIPTION: Example of defining custom message types using Protocol Buffers (protobuf) for use with AutoGen. It shows the syntax for creating new message types NewAsk and ReadmeRequested.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/Hello/HelloAgentState/README.md#2025-04-22_snippet_3\n\nLANGUAGE: proto\nCODE:\n```\nsyntax = \"proto3\";\npackage devteam;\noption csharp_namespace = \"DevTeam.Shared\";\nmessage NewAsk {\n  string org = 1;\n  string repo = 2;\n  string ask = 3;\n  int64 issue_number = 4;\n}\nmessage ReadmeRequested {\n   string org = 1;\n   string repo = 2;\n   int64 issue_number = 3;\n   string ask = 4;\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies with pip - Python\nDESCRIPTION: This snippet provides the pip command needed to install all necessary Python packages for running a LangGraph-based agent, including the LangGraph, LangChain OpenAI, and Azure Identity libraries. Requirements must be met before running the implementation code. No key parameters are required—it simply ensures your environment has the correct libraries installed.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/langgraph-agent.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# pip install langgraph langchain-openai azure-identity\n```\n\n----------------------------------------\n\nTITLE: Defining TypeSafeFunctionCall for Weather Report in C#\nDESCRIPTION: This snippet defines a TypeSafeFunctionCall named WeatherReport for getting weather information. It includes parameters for location and date, and specifies the return type as string.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Use-function-call.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\npublic static TypeSafeFunctionCall<WeatherReportArg, string> WeatherReport = new TypeSafeFunctionCall<WeatherReportArg, string>(\"get_weather_report\", \"Get weather report for a specific date and location\", args => $\"Weather report for {args.Location} on {args.Date}: Sunny\");\n\npublic record WeatherReportArg(string Location, string Date);\n```\n\n----------------------------------------\n\nTITLE: Updating Dependencies for AutoGen Python Development\nDESCRIPTION: This command updates the dependencies in the virtual environment after pulling new changes.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/README.md#2025-04-22_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nuv sync --all-extras\n```\n\n----------------------------------------\n\nTITLE: AutoGen Studio Dockerfile\nDESCRIPTION: This Dockerfile sets up an environment to run AutoGen Studio within a Docker container.  It uses a slim Python 3.10 base image, installs necessary packages (gunicorn and autogenstudio), creates a user, sets environment variables, copies the application code, and defines the command to start the application using Gunicorn. It assumes the application code is present in the current directory.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/faq.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nFROM python:3.10-slim\n\nWORKDIR /code\n\nRUN pip install -U gunicorn autogenstudio\n\nRUN useradd -m -u 1000 user\nUSER user\nENV HOME=/home/user \\\n    PATH=/home/user/.local/bin:$PATH \\\n    AUTOGENSTUDIO_APPDIR=/home/user/app\n\nWORKDIR $HOME/app\n\nCOPY --chown=user . $HOME/app\n\nCMD gunicorn -w $((2 * $(getconf _NPROCESSORS_ONLN) + 1)) --timeout 12600 -k uvicorn.workers.UvicornWorker autogenstudio.web.app:app --bind \"0.0.0.0:8081\"\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGen modules in Python\nDESCRIPTION: This code snippet demonstrates how to import the necessary modules from AutoGen to create and configure agents for conversational AI applications.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/GAIA/Templates/SelectorGroupChat/prompt.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAIChatAgent with Tool in C#\nDESCRIPTION: This C# snippet shows how to create an OpenAIChatAgent with the WeatherReport tool and connect it to the LiteLLM proxy server.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Function-call-with-ollama-and-litellm.md#2025-04-22_snippet_6\n\nLANGUAGE: csharp\nCODE:\n```\nvar agent = new AutoGen.OpenAI.OpenAIChatAgent(\n    \"agent\",\n    new OpenAIClientOptions\n    {\n        BaseAddress = new Uri(\"http://localhost:4000\"),\n        ApiKey = \"dummy\",\n    },\n    \"dolphincoder\", // model name\n    new[] { tools } // middleware\n);\n\nvar reply = await agent.GenerateReplyAsync(\"What's the weather in New York?\");\nConsole.WriteLine(reply);\n```\n\n----------------------------------------\n\nTITLE: Saving Agent State - autogen_agentchat - Python\nDESCRIPTION: This snippet demonstrates how to save the state of an AssistantAgent instance using its save_state method. The state can later be printed or persisted as needed. The agent object must be previously instantiated and active. The output is a serializable state object representing the agent's internal state and context.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/state.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nagent_state = await assistant_agent.save_state()\nprint(agent_state)\n```\n\n----------------------------------------\n\nTITLE: Executing AutoGenBench Scenarios with Shell Script\nDESCRIPTION: A shell script (run.sh) that outlines the execution algorithm for AutoGenBench scenarios, including initialization, running the scenario, and cleanup steps.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nexport AUTOGEN_TESTBED_SETTING=\"Docker\"\numask 000\n\n# Run the global init script if it exists\nif [ -f global_init.sh ] ; then\n    . ./global_init.sh\nfi\n\n# Run the scenario init script if it exists\nif [ -f scenario_init.sh ] ; then\n    . ./scenario_init.sh\nfi\n\n# Run the scenario\npip install -r requirements.txt\npython scenario.py\nEXIT_CODE=$?\nif [ $EXIT_CODE -ne 0 ]; then\n    echo SCENARIO.PY EXITED WITH CODE: $EXIT_CODE !#!#\nelse\n    echo SCENARIO.PY COMPLETE !#!#\nfi\n\n# Clean up\nif [ -d .cache ] ; then\n    rm -Rf .cache\nfi\n\n# Run the scenario finalize script if it exists\nif [ -f scenario_finalize.sh ] ; then\n    . ./scenario_finalize.sh\nfi\n\n# Run the global finalize script if it exists\nif [ -f global_finalize.sh ] ; then\n    . ./global_finalize.sh\nfi\n\necho RUN.SH COMPLETE !#!#\n```\n\n----------------------------------------\n\nTITLE: Markdown Release Notes for AutoGen.Net\nDESCRIPTION: Release notes detailing the addition of OpenAI o1-preview model support, including links to related issues and sample code.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/release_note/0.2.1.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Release Notes for AutoGen.Net v0.2.1 🚀\n\n## New Features 🌟\n- **Support for OpenAi o1-preview** : Added support for OpenAI o1-preview model ([#3522](https://github.com/microsoft/autogen/issues/3522))\n\n## Example 📚\n- **OpenAI o1-preview**: [Connect_To_OpenAI_o1_preview](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AutoGen.OpenAI.Sample/Connect_To_OpenAI_o1_preview.cs)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Namespace for MistralClientAgent in C#\nDESCRIPTION: Code snippet showing the using statement to import the necessary namespace for working with MistralClientAgent.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen-Mistral-Overview.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nusing_statement\n```\n\n----------------------------------------\n\nTITLE: Including Protocol Buffers in .NET Project\nDESCRIPTION: XML snippet showing how to include Protocol Buffers and gRPC tools in a .NET project file (.csproj). It demonstrates the necessary package references and how to include .proto files.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/Hello/HelloAgentState/README.md#2025-04-22_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n  <ItemGroup>\n    <PackageReference Include=\"Google.Protobuf\" />\n    <PackageReference Include=\"Grpc.Tools\" PrivateAssets=\"All\" />\n    <Protobuf Include=\"..\\Protos\\messages.proto\" Link=\"Protos\\messages.proto\" />\n  </ItemGroup>\n```\n\n----------------------------------------\n\nTITLE: Importing Agent, Tool, and Model Modules in Python\nDESCRIPTION: This snippet imports required modules for defining agents, agent conditions, team chat, UI, tool abstractions, and a model client using Python. The dependencies are part of the autogen_agentchat, autogen_core, and autogen_ext OpenAI frameworks. It is essential as a prerequisite for the later definition of agent tools and orchestration of the agent workflow.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/company-research.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\\nfrom autogen_agentchat.conditions import TextMentionTermination\\nfrom autogen_agentchat.teams import RoundRobinGroupChat\\nfrom autogen_agentchat.ui import Console\\nfrom autogen_core.tools import FunctionTool\\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen.Ollama Package via .NET CLI\nDESCRIPTION: Command to install the AutoGen.Ollama NuGet package using the .NET CLI. This package is required to create and use the OllamaAgent for connecting to Ollama servers.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Ollama/Chat-with-llama.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AutoGen.Ollama\n```\n\n----------------------------------------\n\nTITLE: Relationship Format Specification\nDESCRIPTION: Format specification for documenting relationships between entities including source, target, description and strength.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_graphrag/prompts/entity_extraction.txt#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n(\"relationship\"{tuple_delimiter}<source_entity>{tuple_delimiter}<target_entity>{tuple_delimiter}<relationship_description>{tuple_delimiter}<relationship_strength>)\n```\n\n----------------------------------------\n\nTITLE: Initializing Azure OpenAI Client with Environment Variables in Python (Autogen)\nDESCRIPTION: Imports required modules, defines a helper function `get_env_variable` to safely retrieve environment variables, and initializes the `AzureOpenAIChatCompletionClient` from `autogen_ext.models.openai`. The client is configured using the previously set Azure OpenAI environment variables, ensuring type safety and raising an error if any variable is missing.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/structured-output-agent.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nfrom typing import Optional\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n\n\n# Function to get environment variable and ensure it is not None\ndef get_env_variable(name: str) -> str:\n    value = os.getenv(name)\n    if value is None:\n        raise ValueError(f\"Environment variable {name} is not set\")\n    return value\n\n\n# Create the client with type-checked environment variables\nclient = AzureOpenAIChatCompletionClient(\n    azure_deployment=get_env_variable(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n    # Assuming AZURE_OPENAI_MODEL is also set, though not shown in snippet 2\n    # model=get_env_variable(\"AZURE_OPENAI_MODEL\"), \n    api_version=get_env_variable(\"AZURE_OPENAI_API_VERSION\"),\n    azure_endpoint=get_env_variable(\"AZURE_OPENAI_ENDPOINT\"),\n    api_key=get_env_variable(\"AZURE_OPENAI_API_KEY\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Agent Team Configuration\nDESCRIPTION: This code demonstrates how to save the agent team configuration to a JSON file, and then load it back to create a new instance of the team.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-studio/notebooks/tutorial.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\n# convert to config \nconfig = group_chat.dump_component().model_dump()\n# save as json \n\nwith open(\"travel_team.json\", \"w\") as f:\n    json.dump(config, f, indent=4)\n\n# load from json\nwith open(\"travel_team.json\", \"r\") as f:\n    config = json.load(f)\n\ngroup_chat = RoundRobinGroupChat.load_component(config) \nresult = group_chat.run_stream(task=\"Plan a 3 day trip to Nepal.\") \nasync for response in result:\n    print(response)\n```\n\n----------------------------------------\n\nTITLE: Running the AutoGen .NET Sample Application in Bash\nDESCRIPTION: Commands to clone the AutoGen repository and run the Hello World .NET sample. Requires .NET 8.0 or later and GitHub CLI.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/Microsoft.AutoGen.Integration.Tests.AppHosts/HelloAgentTests/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository\ngh repo clone microsoft/autogen\ncd dotnet/samples/Hello\ndotnet run\n```\n\n----------------------------------------\n\nTITLE: Defining Function Contracts in JSON\nDESCRIPTION: This JSON structure defines two function contracts. The first function takes no parameters and returns a string. The second function takes a 'message' parameter of type string and also returns a string.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.SemanticKernel.Tests/ApprovalTests/KernelFunctionExtensionTests.ItCreateFunctionContractsFromMethod.approved.txt#2025-04-22_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"Name\": \"_ItCreateFunctionContractsFromMethod_b__2_0\",\n    \"Description\": \"\",\n    \"Parameters\": [],\n    \"ReturnType\": \"System.String, System.Private.CoreLib, Version=8.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e\",\n    \"ReturnDescription\": \"\"\n  },\n  {\n    \"Name\": \"_ItCreateFunctionContractsFromMethod_b__2_1\",\n    \"Description\": \"\",\n    \"Parameters\": [\n      {\n        \"Name\": \"message\",\n        \"Description\": \"\",\n        \"ParameterType\": \"System.String, System.Private.CoreLib, Version=8.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e\",\n        \"IsRequired\": true\n      }\n    ],\n    \"ReturnType\": \"System.String, System.Private.CoreLib, Version=8.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e\",\n    \"ReturnDescription\": \"\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Importing Required Dependencies for Mixture of Agents\nDESCRIPTION: Imports necessary modules and classes from autogen_core and related libraries for implementing the Mixture of Agents pattern.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/mixture-of-agents.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import List\n\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\nfrom autogen_core.models import ChatCompletionClient, SystemMessage, UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\n----------------------------------------\n\nTITLE: Defining Weather Function Schema in JSON\nDESCRIPTION: Example of a function definition schema for a weather reporting function. The schema defines a function named 'GetWeather' that takes a city parameter and returns weather information.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Function-call-overview.md#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"GetWeather\",\n    \"description\": \"Get the weather report for a city\",\n    \"parameters\": {\n        \"city\": {\n            \"type\": \"string\",\n            \"description\": \"The city name\"\n        },\n        \"required\": [\"city\"]\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Workbench Agent Creation with Dependencies\nDESCRIPTION: This snippet shows the necessary imports for creating an agent that uses a Workbench to interact with various tools. It includes modules for message handling, context management, and defining agent behavior.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/workbench.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom dataclasses import dataclass\nfrom typing import List\n\nfrom autogen_core import (\n    FunctionCall,\n    MessageContext,\n    RoutedAgent,\n    message_handler,\n)\nfrom autogen_core.model_context import ChatCompletionContext\nfrom autogen_core.models import (\n    AssistantMessage,\n    ChatCompletionClient,\n    FunctionExecutionResult,\n    FunctionExecutionResultMessage,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\nfrom autogen_core.tools import ToolResult, Workbench\n```\n\n----------------------------------------\n\nTITLE: Adding Required Using Statements\nDESCRIPTION: Required namespace imports for using AutoGen.Ollama functionality\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Ollama/Chat-with-llava.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../../samples/AutoGen.Ollama.Sample/Chat_With_LLaVA.cs?name=Using)]\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Message Types with Protocol Buffers\nDESCRIPTION: Protocol Buffer definition for custom message types in AutoGen. This shows how to define your own message schema for communication between agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/Microsoft.AutoGen.Integration.Tests.AppHosts/HelloAgentTests/README.md#2025-04-22_snippet_4\n\nLANGUAGE: proto\nCODE:\n```\nsyntax = \"proto3\";\npackage devteam;\noption csharp_namespace = \"DevTeam.Shared\";\nmessage NewAsk {\n  string org = 1;\n  string repo = 2;\n  string ask = 3;\n  int64 issue_number = 4;\n}\nmessage ReadmeRequested {\n   string org = 1;\n   string repo = 2;\n   int64 issue_number = 3;\n   string ask = 4;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating a Conda Environment\nDESCRIPTION: This snippet demonstrates how to create and activate a Conda environment for installing AutoGen Studio. It is necessary to have Conda installed on the system prior to running these commands, with Python version 3.10 specified for the environment.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/installation.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n autogen python=3.10\nconda activate autogen\n```\n\nLANGUAGE: bash\nCODE:\n```\nconda deactivate\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Agent State in AutoGen v0.4\nDESCRIPTION: Demonstrates how to save and load an agent's state using save_state and load_state methods. Includes writing state to disk and continuing conversations after state restoration.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant.\",\n        model_client=model_client,\n    )\n\n    cancellation_token = CancellationToken()\n    response = await assistant.on_messages([TextMessage(content=\"Hello!\", source=\"user\")], cancellation_token)\n    print(response)\n\n    # Save the state.\n    state = await assistant.save_state()\n\n    # (Optional) Write state to disk.\n    with open(\"assistant_state.json\", \"w\") as f:\n        json.dump(state, f)\n\n    # (Optional) Load it back from disk.\n    with open(\"assistant_state.json\", \"r\") as f:\n        state = json.load(f)\n        print(state) # Inspect the state, which contains the chat history.\n\n    # Carry on the chat.\n    response = await assistant.on_messages([TextMessage(content=\"Tell me a joke.\", source=\"user\")], cancellation_token)\n    print(response)\n\n    # Load the state, resulting the agent to revert to the previous state before the last message.\n    await assistant.load_state(state)\n\n    # Carry on the same chat again.\n    response = await assistant.on_messages([TextMessage(content=\"Tell me a joke.\", source=\"user\")], cancellation_token)\n    # Close the connection to the model client.\n    await model_client.close()\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Installing Chess Package\nDESCRIPTION: Command to install the required chess package for the game implementation.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_chess_game/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"chess\"\n```\n\n----------------------------------------\n\nTITLE: Starting HelloAgent App Host\nDESCRIPTION: Shell commands to navigate to the Hello.AppHost directory and run the .NET Aspire application, which will launch both the HelloAgent project and agents backend with integrated monitoring.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/Hello/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd Hello.AppHost\ndotnet run\n```\n\n----------------------------------------\n\nTITLE: Starting Ollama Server\nDESCRIPTION: Command to start the Ollama server that will act as an OpenAI API compatible endpoint\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-connect-to-third-party-api.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nollama serve\n```\n\n----------------------------------------\n\nTITLE: Registering LlamaIndex Agent with Runtime (Python)\nDESCRIPTION: This code registers the `LlamaIndexAgent` with a `SingleThreadedAgentRuntime`. It provides a name ('chat_agent') and a factory function that creates an instance of `LlamaIndexAgent` with a ReActAgent configured with a Wikipedia tool, LLM, memory, and verbosity.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/llamaindex-agent.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nruntime = SingleThreadedAgentRuntime()\nawait LlamaIndexAgent.register(\n    runtime,\n    \"chat_agent\",\n    lambda: LlamaIndexAgent(\n        description=\"Llama Index Agent\",\n        llama_index_agent=ReActAgent.from_tools(\n            tools=[wikipedia_tool],\n            llm=llm,\n            max_iterations=8,\n            memory=ChatSummaryMemoryBuffer(llm=llm, token_limit=16000),\n            verbose=True,\n        ),\n    ),\n)\nagent = AgentId(\"chat_agent\", \"default\")\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Schema for Sum Function\nDESCRIPTION: A JSON schema that defines a function named 'Sum' which takes an array of double values as input. The schema declares that the 'args' parameter is required and must be an array containing number values.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.SourceGenerator.Tests/ApprovalTests/FunctionExample.Sum_Test.approved.txt#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"Sum\",\n  \"description\": \"Sum function\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"args\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"number\"\n        },\n        \"description\": \"an array of double values\"\n      }\n    },\n    \"required\": [\n      \"args\"\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Studio GUI\nDESCRIPTION: Command for installing AutoGen Studio, which provides a no-code graphical user interface for building and running multi-agent workflows.\nSOURCE: https://github.com/microsoft/autogen/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install AutoGen Studio for no-code GUI\npip install -U \"autogenstudio\"\n```\n\n----------------------------------------\n\nTITLE: Converting Multiple Tool Call Results in AutoGen\nDESCRIPTION: Shows how multiple tool call results are converted to separate messages with the tool role, each with its own content and auto-generated tool call IDs.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.OpenAI.V1.Tests/ApprovalTests/OpenAIMessageTests.BasicMessageTest.approved.txt#2025-04-22_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"OriginalMessage\": \"ToolCallResultMessage(user)\\n\\tToolCall(result, test, test)\\n\\tToolCall(result, test, test)\",\n  \"ConvertedMessages\": [\n    {\n      \"Role\": \"tool\",\n      \"Content\": \"test\",\n      \"ToolCallId\": \"result_0\"\n    },\n    {\n      \"Role\": \"tool\",\n      \"Content\": \"test\",\n      \"ToolCallId\": \"result_1\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGenBench Package\nDESCRIPTION: Command for installing the agbench package from source code using pip.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e autogen/python/packages/agbench\n```\n\n----------------------------------------\n\nTITLE: Stopping the Host Service in Python\nDESCRIPTION: This snippet demonstrates how to stop the host service using the stop() method. It also includes a commented-out alternative for keeping the host service running until a termination signal is received.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/distributed-agent-runtime.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nawait host.stop()\n\n# To keep the host service running until a termination signal (e.g., SIGTERM)\n# await host.stop_when_signal()\n```\n\n----------------------------------------\n\nTITLE: Including Getting Started Documentation in Markdown\nDESCRIPTION: A markdown include directive that references a getting started guide document located at ./articles/getting-start.md\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[!INCLUDE [](./articles/getting-start.md)]\n```\n\n----------------------------------------\n\nTITLE: Instrumenting and Running an AutoGen AgentChat Team with Tracing (Python)\nDESCRIPTION: Defines and runs an AutoGen multi-agent chat simulation with OpenTelemetry tracing enabled. It sets up helper tools, creates `AssistantAgent` instances with specific roles and tools, defines termination conditions, configures a `SelectorGroupChat`, retrieves the previously configured tracer, starts a custom span named 'runtime', executes the chat task, and displays the output. The `with tracer.start_as_current_span(\"runtime\")` block ensures that the chat execution is captured within a specific trace span.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tracing.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_core import SingleThreadedAgentRuntime\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\ndef search_web_tool(query: str) -> str:\n    if \"2006-2007\" in query:\n        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n        Udonis Haslem: 844 points\n        Dwayne Wade: 1397 points\n        James Posey: 550 points\n        ...\n        \"\"\"\n    elif \"2007-2008\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n    elif \"2008-2009\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n    return \"No data found.\"\n\n\ndef percentage_change_tool(start: float, end: float) -> float:\n    return ((end - start) / start) * 100\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    planning_agent = AssistantAgent(\n        \"PlanningAgent\",\n        description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a planning agent.\n        Your job is to break down complex tasks into smaller, manageable subtasks.\n        Your team members are:\n            WebSearchAgent: Searches for information\n            DataAnalystAgent: Performs calculations\n\n        You only plan and delegate tasks - you do not execute them yourself.\n\n        When assigning tasks, use this format:\n        1. <agent> : <task>\n\n        After all tasks are complete, summarize the findings and end with \\\"TERMINATE\\\".\n        \"\"\",\n    )\n\n    web_search_agent = AssistantAgent(\n        \"WebSearchAgent\",\n        description=\"An agent for searching information on the web.\",\n        tools=[search_web_tool],\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a web search agent.\n        Your only tool is search_tool - use it to find information.\n        You make only one search call at a time.\n        Once you have the results, you never do calculations based on them.\n        \"\"\",\n    )\n\n    data_analyst_agent = AssistantAgent(\n        \"DataAnalystAgent\",\n        description=\"An agent for performing calculations.\",\n        model_client=model_client,\n        tools=[percentage_change_tool],\n        system_message=\"\"\"\n        You are a data analyst.\n        Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n        If you have not seen the data, ask for it.\n        \"\"\",\n    )\n\n    text_mention_termination = TextMentionTermination(\"TERMINATE\")\n    max_messages_termination = MaxMessageTermination(max_messages=25)\n    termination = text_mention_termination | max_messages_termination\n\n    selector_prompt = \"\"\"Select an agent to perform task.\n\n    {roles}\n\n    Current conversation context:\n    {history}\n\n    Read the above conversation, then select an agent from {participants} to perform the next task.\n    Make sure the planner agent has assigned tasks before other agents start working.\n    Only select one agent.\n    \"\"\"\n\n    task = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\"\n\n    tracer = trace.get_tracer(\"autogen-test-agentchat\")\n    with tracer.start_as_current_span(\"runtime\"):\n        team = SelectorGroupChat(\n            [planning_agent, web_search_agent, data_analyst_agent],\n            model_client=model_client,\n            termination_condition=termination,\n            selector_prompt=selector_prompt,\n            allow_repeated_speaker=True,\n        )\n        await Console(team.run_stream(task=task))\n\n    await model_client.close()\n\n\n# asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen.Gemini Package\nDESCRIPTION: Command to install the AutoGen.Gemini NuGet package required for Gemini integration.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Image-chat-with-gemini.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AutoGen.Gemini\n```\n\n----------------------------------------\n\nTITLE: Configuring a group chat with multiple agents in AutoGen using Python\nDESCRIPTION: This code snippet shows how to set up a group chat with multiple agents in AutoGen. It includes creating different types of agents and configuring their interactions within a group chat setting.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/GAIA/Templates/SelectorGroupChat/prompt.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create an AssistantAgent named \"assistant\"\nassistant = AssistantAgent(name=\"assistant\", llm_config={\"config_list\": config_list})\n\n# Create a UserProxyAgent named \"human\"\nhuman = UserProxyAgent(name=\"human\", code_execution_config={\"work_dir\": \"coding\"})\n\n# Create another AssistantAgent named \"coder\"\ncoder = AssistantAgent(name=\"coder\", llm_config={\"config_list\": config_list})\n\n# Create a group chat and initiate the chat\ngroupchat = GroupChat(agents=[human, assistant, coder], messages=[], max_round=12)\nmanager = GroupChatManager(groupchat=groupchat, llm_config={\"config_list\": config_list})\nhuman.initiate_chat(manager, message=\"Plot a chart of NVDA stock price change YTD.\")\n```\n\n----------------------------------------\n\nTITLE: Mapping JSON Data Structure for AutoGen Message Conversion\nDESCRIPTION: A comprehensive JSON structure that maps original message formats to their converted representations in AutoGen. The file shows conversions for system messages, user messages, assistant messages, image messages, multimodal messages, tool calls, and tool call results.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.OpenAI.Tests/ApprovalTests/OpenAIMessageTests.BasicMessageTest.received.txt#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"OriginalMessage\": \"TextMessage(system, You are a helpful AI assistant, )\",\n    \"ConvertedMessages\": [\n      {\n        \"Name\": null,\n        \"Role\": \"system\",\n        \"Content\": [\n          {\n            \"Kind\": 0,\n            \"Text\": \"You are a helpful AI assistant\",\n            \"ImageUri\": null,\n            \"ImageBytes\": null,\n            \"ImageBytesMediaType\": null,\n            \"InputAudioBytes\": null,\n            \"InputAudioFormat\": null,\n            \"FileId\": null,\n            \"FileBytes\": null,\n            \"FileBytesMediaType\": null,\n            \"Filename\": null,\n            \"ImageDetailLevel\": null,\n            \"Refusal\": null\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"OriginalMessage\": \"TextMessage(user, Hello, user)\",\n    \"ConvertedMessages\": [\n      {\n        \"Role\": \"user\",\n        \"Content\": [\n          {\n            \"Kind\": 0,\n            \"Text\": \"Hello\",\n            \"ImageUri\": null,\n            \"ImageBytes\": null,\n            \"ImageBytesMediaType\": null,\n            \"InputAudioBytes\": null,\n            \"InputAudioFormat\": null,\n            \"FileId\": null,\n            \"FileBytes\": null,\n            \"FileBytesMediaType\": null,\n            \"Filename\": null,\n            \"ImageDetailLevel\": null,\n            \"Refusal\": null\n          }\n        ],\n        \"Name\": \"user\",\n        \"MultiModaItem\": [\n          {\n            \"Type\": \"Text\",\n            \"Text\": \"Hello\"\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"OriginalMessage\": \"TextMessage(assistant, How can I help you?, assistant)\",\n    \"ConvertedMessages\": [\n      {\n        \"Role\": \"assistant\",\n        \"Content\": [\n          {\n            \"Kind\": 0,\n            \"Text\": \"How can I help you?\",\n            \"ImageUri\": null,\n            \"ImageBytes\": null,\n            \"ImageBytesMediaType\": null,\n            \"InputAudioBytes\": null,\n            \"InputAudioFormat\": null,\n            \"FileId\": null,\n            \"FileBytes\": null,\n            \"FileBytesMediaType\": null,\n            \"Filename\": null,\n            \"ImageDetailLevel\": null,\n            \"Refusal\": null\n          }\n        ],\n        \"Name\": \"assistant\",\n        \"TooCall\": []\n      }\n    ]\n  },\n  {\n    \"OriginalMessage\": \"ImageMessage(user, https://example.com/image.png, user)\",\n    \"ConvertedMessages\": [\n      {\n        \"Role\": \"user\",\n        \"Content\": [\n          {\n            \"Kind\": 2,\n            \"Text\": null,\n            \"ImageUri\": \"https://example.com/image.png\",\n            \"ImageBytes\": null,\n            \"ImageBytesMediaType\": null,\n            \"InputAudioBytes\": null,\n            \"InputAudioFormat\": null,\n            \"FileId\": null,\n            \"FileBytes\": null,\n            \"FileBytesMediaType\": null,\n            \"Filename\": null,\n            \"ImageDetailLevel\": null,\n            \"Refusal\": null\n          }\n        ],\n        \"Name\": \"user\",\n        \"MultiModaItem\": [\n          {\n            \"Type\": \"Image\",\n            \"ImageUrl\": \"https://example.com/image.png\"\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"OriginalMessage\": \"MultiModalMessage(assistant, user)\\n\\tTextMessage(user, Hello, user)\\n\\tImageMessage(user, https://example.com/image.png, user)\",\n    \"ConvertedMessages\": [\n      {\n        \"Role\": \"user\",\n        \"Content\": [\n          {\n            \"Kind\": 0,\n            \"Text\": \"Hello\",\n            \"ImageUri\": null,\n            \"ImageBytes\": null,\n            \"ImageBytesMediaType\": null,\n            \"InputAudioBytes\": null,\n            \"InputAudioFormat\": null,\n            \"FileId\": null,\n            \"FileBytes\": null,\n            \"FileBytesMediaType\": null,\n            \"Filename\": null,\n            \"ImageDetailLevel\": null,\n            \"Refusal\": null\n          },\n          {\n            \"Kind\": 2,\n            \"Text\": null,\n            \"ImageUri\": \"https://example.com/image.png\",\n            \"ImageBytes\": null,\n            \"ImageBytesMediaType\": null,\n            \"InputAudioBytes\": null,\n            \"InputAudioFormat\": null,\n            \"FileId\": null,\n            \"FileBytes\": null,\n            \"FileBytesMediaType\": null,\n            \"Filename\": null,\n            \"ImageDetailLevel\": null,\n            \"Refusal\": null\n          }\n        ],\n        \"Name\": \"user\",\n        \"MultiModaItem\": [\n          {\n            \"Type\": \"Text\",\n            \"Text\": \"Hello\"\n          },\n          {\n            \"Type\": \"Image\",\n            \"ImageUrl\": \"https://example.com/image.png\"\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"OriginalMessage\": \"ToolCallMessage(assistant)\\n\\tToolCall(test, test, )\",\n    \"ConvertedMessages\": [\n      {\n        \"Role\": \"assistant\",\n        \"Content\": [],\n        \"Name\": null,\n        \"TooCall\": [\n          {\n            \"Type\": \"Function\",\n            \"Name\": \"test\",\n            \"Arguments\": \"dGVzdA==\",\n            \"Id\": \"test\"\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"OriginalMessage\": \"ToolCallResultMessage(user)\\n\\tToolCall(test, test, result)\",\n    \"ConvertedMessages\": [\n      {\n        \"Role\": \"tool\",\n        \"Content\": \"result\",\n        \"ToolCallId\": \"test\"\n      }\n    ]\n  },\n  {\n    \"OriginalMessage\": \"ToolCallResultMessage(user)\\n\\tToolCall(result, test, test)\\n\\tToolCall(result, test, test)\",\n    \"ConvertedMessages\": [\n      {\n        \"Role\": \"tool\",\n        \"Content\": \"test\",\n        \"ToolCallId\": \"result_0\"\n      },\n      {\n        \"Role\": \"tool\",\n        \"Content\": \"test\",\n        \"ToolCallId\": \"result_1\"\n      }\n    ]\n  },\n  {\n    \"OriginalMessage\": \"ToolCallMessage(assistant)\\n\\tToolCall(test, test, )\\n\\tToolCall(test, test, )\",\n    \"ConvertedMessages\": [\n      {\n        \"Role\": \"assistant\",\n        \"Content\": [],\n        \"Name\": null,\n        \"TooCall\": [\n          {\n            \"Type\": \"Function\",\n            \"Name\": \"test\",\n            \"Arguments\": \"dGVzdA==\",\n            \"Id\": \"test_0\"\n          },\n          {\n            \"Type\": \"Function\",\n            \"Name\": \"test\",\n            \"Arguments\": \"dGVzdA==\",\n            \"Id\": \"test_1\"\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"OriginalMessage\": \"AggregateMessage(assistant)\\n\\tToolCallMessage(assistant)\\n\\tToolCall(test, test, )\\n\\tToolCallResultMessage(assistant)\\n\\tToolCall(test, test, result)\",\n    \"ConvertedMessages\": [\n      {\n        \"Role\": \"assistant\",\n        \"Content\": [],\n        \"Name\": null,\n        \"TooCall\": [\n          {\n            \"Type\": \"Function\",\n            \"Name\": \"test\",\n            \"Arguments\": \"dGVzdA==\",\n            \"Id\": \"test\"\n          }\n        ]\n      },\n      {\n        \"Role\": \"tool\",\n        \"Content\": \"result\",\n        \"ToolCallId\": \"test\"\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Linking to AutoGen GitHub Repository in Markdown\nDESCRIPTION: This snippet shows how to create a hyperlink in Markdown to the main AutoGen GitHub repository. It's used to provide a direct link to the core AutoGen project.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/nuget/NUGET.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n[AutoGen](https://github.com/microsoft/autogen)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiple Message Processors\nDESCRIPTION: Creates UrgentProcessor and NormalProcessor classes that handle different types of tasks based on topic subscriptions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/concurrent-agents.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nTASK_RESULTS_TOPIC_TYPE = \"task-results\"\ntask_results_topic_id = TopicId(type=TASK_RESULTS_TOPIC_TYPE, source=\"default\")\n\n\n@type_subscription(topic_type=\"urgent\")\nclass UrgentProcessor(RoutedAgent):\n    @message_handler\n    async def on_task(self, message: Task, ctx: MessageContext) -> None:\n        print(f\"Urgent processor starting task {message.task_id}\")\n        await asyncio.sleep(1)  # Simulate work\n        print(f\"Urgent processor finished task {message.task_id}\")\n\n        task_response = TaskResponse(task_id=message.task_id, result=\"Results by Urgent Processor\")\n        await self.publish_message(task_response, topic_id=task_results_topic_id)\n\n\n@type_subscription(topic_type=\"normal\")\nclass NormalProcessor(RoutedAgent):\n    @message_handler\n    async def on_task(self, message: Task, ctx: MessageContext) -> None:\n        print(f\"Normal processor starting task {message.task_id}\")\n        await asyncio.sleep(3)  # Simulate work\n        print(f\"Normal processor finished task {message.task_id}\")\n\n        task_response = TaskResponse(task_id=message.task_id, result=\"Results by Normal Processor\")\n        await self.publish_message(task_response, topic_id=task_results_topic_id)\n```\n\n----------------------------------------\n\nTITLE: Converting MultiModal Message in AutoGen\nDESCRIPTION: Demonstrates how a multimodal message containing both text and image is converted to a single message with an array of multimodal items of different types.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.OpenAI.V1.Tests/ApprovalTests/OpenAIMessageTests.BasicMessageTest.approved.txt#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"OriginalMessage\": \"MultiModalMessage(assistant, user)\\n\\tTextMessage(user, Hello, user)\\n\\tImageMessage(user, https://example.com/image.png, user)\",\n  \"ConvertedMessages\": [\n    {\n      \"Role\": \"user\",\n      \"Content\": null,\n      \"Name\": \"user\",\n      \"MultiModaItem\": [\n        {\n          \"Type\": \"Text\",\n          \"Text\": \"Hello\"\n        },\n        {\n          \"Type\": \"Image\",\n          \"ImageUrl\": {\n            \"Url\": \"https://example.com/image.png\",\n            \"Detail\": null\n          }\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Citing Magentic-One Research Paper in BibTeX Format\nDESCRIPTION: BibTeX citation entry for the Magentic-One technical paper describing the generalist multi-agent system for solving complex tasks.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-magentic-one/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{fourney2024magenticonegeneralistmultiagentsolving,\n      title={Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks},\n      author={Adam Fourney and Gagan Bansal and Hussein Mozannar and Cheng Tan and Eduardo Salinas and Erkang and Zhu and Friederike Niedtner and Grace Proebsting and Griffin Bassman and Jack Gerrits and Jacob Alber and Peter Chang and Ricky Loynd and Robert West and Victor Dibia and Ahmed Awadallah and Ece Kamar and Rafah Hosn and Saleema Amershi},\n      year={2024},\n      eprint={2411.04468},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2411.04468},\n}\n```\n\n----------------------------------------\n\nTITLE: Listing AutoGen Package Dependencies\nDESCRIPTION: This snippet lists the package dependencies for different components of the AutoGen project. It includes the core package, OpenAI extension, and agent chat functionality.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/HumanEval/Templates/AgentChat/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npyyaml\n/autogen_python/packages/autogen-core\n/autogen_python/packages/autogen-ext[openai]\n/autogen_python/packages/autogen-agentchat\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen via pip in Python\nDESCRIPTION: This code snippet shows how to install AutoGen using pip, the Python package installer. It includes options for installing with or without UI-related dependencies.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/GAIA/Templates/SelectorGroupChat/prompt.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pyautogen\n# or\npip install \"pyautogen[blendsearch]\"  # for blendsearch functionality\n# or\npip install \"pyautogen[mathchat]\"  # for MathChat functionality\n```\n\n----------------------------------------\n\nTITLE: Installing Core AutoGen Packages via .NET CLI\nDESCRIPTION: Commands to install the core Microsoft AutoGen packages (Contracts and Core) using the .NET CLI. These packages are essential for writing and running agents using the Core API within a single process.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/installation.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndotnet add package Microsoft.AutoGen.Contracts --version 0.4.0-dev.1\ndotnet add package Microsoft.AutoGen.Core --version 0.4.0-dev.1\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions in Azure Code Executor\nDESCRIPTION: Demonstrates how to use and restart sessions in the Azure Code Executor, showing that variables do not persist across session restarts.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/extensions-user-guide/azure-container-code-executor.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nexecutor = ACADynamicSessionsCodeExecutor(\n    pool_management_endpoint=POOL_MANAGEMENT_ENDPOINT, credential=DefaultAzureCredential()\n)\n\ncode_blocks = [CodeBlock(code=\"x = 'abcdefg'\", language=\"python\")]\ncode_result = await executor.execute_code_blocks(code_blocks, cancellation_token)\nassert code_result.exit_code == 0\n\ncode_blocks = [CodeBlock(code=\"print(x)\", language=\"python\")]\ncode_result = await executor.execute_code_blocks(code_blocks, cancellation_token)\nassert code_result.exit_code == 0 and \"abcdefg\" in code_result.output\n\nawait executor.restart()\ncode_blocks = [CodeBlock(code=\"print(x)\", language=\"python\")]\ncode_result = await executor.execute_code_blocks(code_blocks, cancellation_token)\nassert code_result.exit_code != 0 and \"NameError\" in code_result.output\n```\n\n----------------------------------------\n\nTITLE: Using Existing OpenTelemetry Instrumentation\nDESCRIPTION: Example showing how to use an existing tracer provider with AutoGen runtimes\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/telemetry.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace\n\n# Get the tracer provider from your application\ntracer_provider = trace.get_tracer_provider()\n\n# for single threaded runtime\nsingle_threaded_runtime = SingleThreadedAgentRuntime(tracer_provider=tracer_provider)\n# or for worker runtime\nworker_runtime = GrpcWorkerAgentRuntime(tracer_provider=tracer_provider)\n```\n\n----------------------------------------\n\nTITLE: Entity Format Specification\nDESCRIPTION: Format specification for extracting and documenting entity information from text including name, type, and description.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_graphrag/prompts/entity_extraction.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n(\"entity\"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>{tuple_delimiter}<entity_description>)\n```\n\n----------------------------------------\n\nTITLE: Setting Authentication Configuration via Environment Variable (Bash)\nDESCRIPTION: Illustrates an alternative method to enable authentication in AutoGen Studio by setting the `AUTOGENSTUDIO_AUTH_CONFIG` environment variable to the path of the `auth.yaml` configuration file before launching the application. This approach allows configuration without modifying the launch command directly.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/experimental.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AUTOGENSTUDIO_AUTH_CONFIG=\"/path/to/auth.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Configuring OpenAI API\nDESCRIPTION: Commands to install dependencies using uv package manager, activate virtual environment, and set up the OpenAI API key required for Gitty to function.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/gitty/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv sync --all-extras\nsource .venv/bin/activate\nexport OPENAI_API_KEY=sk-....\n```\n\n----------------------------------------\n\nTITLE: Adding Using Statements for AutoGen\nDESCRIPTION: Includes necessary using directives to facilitate working with the AutoGen library, preparing the environment for further coding. Exact namespaces are detailed within the referenced file.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Chat-with-an-agent.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[Using Statements](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Chat_With_Agent.cs?name=Using)]\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Packages in XML\nDESCRIPTION: This snippet shows the required package references for AutoGen.OpenAI and AutoGen.SourceGenerator in an XML ItemGroup. These packages are necessary for using AutoGen with OpenAI and generating type-safe functions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-use-function-call.md#2025-04-22_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<ItemGroup>\n    <PackageReference Include=\"AutoGen.OpenAI\" Version=\"AUTOGEN_VERSION\" />\n    <PackageReference Include=\"AutoGen.SourceGenerator\" Version=\"AUTOGEN_VERSION\" />\n</ItemGroup>\n```\n\n----------------------------------------\n\nTITLE: Preparing Single Image Message\nDESCRIPTION: Demonstrates how to create an ImageMessage object for single image input processing.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Image-chat-with-agent.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[Create Image Message](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Image_Chat_With_Agent.cs?name=Prepare_Image_Input)]\n```\n\n----------------------------------------\n\nTITLE: Running AutoGen Studio for No-Code Agent Development\nDESCRIPTION: Command for launching AutoGen Studio, a no-code GUI for prototyping and running multi-agent workflows without writing code. The command starts the UI on a specified port and directory.\nSOURCE: https://github.com/microsoft/autogen/blob/main/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Run AutoGen Studio on http://localhost:8080\nautogenstudio ui --port 8080 --appdir ./my-app\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation Configuration for autogen_ext.tools.graphrag Module\nDESCRIPTION: This is a reStructuredText (rst) configuration for documenting the GraphRAG module within the AutoGen extension tools. It uses the automodule directive to generate documentation for all members of the module, including undocumented members and inheritance information.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_ext.tools.graphrag.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogen_ext.tools.graphrag\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Structured Event Logging Implementation\nDESCRIPTION: Complete example of implementing and using structured event logging with a custom event class.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/logging.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nfrom dataclasses import dataclass\nfrom autogen_core import EVENT_LOGGER_NAME\n\n@dataclass\nclass MyEvent:\n    timestamp: str\n    message: str\n\nlogger = logging.getLogger(EVENT_LOGGER_NAME + \".my_module\")\nlogger.info(MyEvent(\"timestamp\", \"message\"))\n```\n\n----------------------------------------\n\nTITLE: Starting AutoGen Application Runtime in C#\nDESCRIPTION: Code snippet demonstrating how to start the AutoGen runtime locally and send a message to an agent using the App.PublishMessageAsync method.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/Hello/HelloAgentState/README.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\n// send a message to the agent\nvar app = await App.PublishMessageAsync(\"HelloAgents\", new NewMessageReceived\n{\n    Message = \"World\"\n}, local: true);\n\nawait App.RuntimeApp!.WaitForShutdownAsync();\nawait app.WaitForShutdownAsync();\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAIChatAgent with Azure.AI.OpenAI v2.0 in C#\nDESCRIPTION: This code shows the new way of creating an OpenAIChatAgent using Azure.AI.OpenAI v2.0 in AutoGen.Net v0.2.0. It demonstrates how to use the GetChatClient method to specify the model.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/release_note/0.2.0.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nvar openAIClient = new OpenAIClient(apiKey);\nvar openAIClientAgent = new OpenAIChatAgent(\n            chatClient: openAIClient.GetChatClient(\"gpt-4o-mini\"),\n            // Other parameters...\n            );\n```\n\n----------------------------------------\n\nTITLE: Flow Diagram - Search-Summarize Chat Process\nDESCRIPTION: Mermaid flowchart showing the interaction between User, Search Agent, and Summarization Agent in a sequential flow.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Roundrobin-chat.md#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    A[User] -->|Ask a question| B[Search Agent]\n    B -->|Retrieve information| C[Summarization Agent]\n    C -->|Summarize result| A[User]\n```\n\n----------------------------------------\n\nTITLE: Creating a MultiModal Message in AutoGen AgentChat\nDESCRIPTION: This code snippet shows how to create a multimodal message that contains both text and an image. It retrieves an image from a URL, converts it to an AutoGen Image object, and creates a MultiModalMessage with mixed content.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/messages.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom io import BytesIO\n\nimport requests\nfrom autogen_agentchat.messages import MultiModalMessage\nfrom autogen_core import Image as AGImage\nfrom PIL import Image\n\npil_image = Image.open(BytesIO(requests.get(\"https://picsum.photos/300/200\").content))\nimg = AGImage(pil_image)\nmulti_modal_message = MultiModalMessage(content=[\"Can you describe the content of this image?\", img], source=\"User\")\nimg\n```\n\n----------------------------------------\n\nTITLE: Converting Assistant Text Message in AutoGen\nDESCRIPTION: Illustrates how an assistant text message is converted to include role, content, name, and additional properties for tool calls and function calls.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.OpenAI.V1.Tests/ApprovalTests/OpenAIMessageTests.BasicMessageTest.approved.txt#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"OriginalMessage\": \"TextMessage(assistant, How can I help you?, assistant)\",\n  \"ConvertedMessages\": [\n    {\n      \"Role\": \"assistant\",\n      \"Content\": \"How can I help you?\",\n      \"Name\": \"assistant\",\n      \"TooCall\": [],\n      \"FunctionCallName\": null,\n      \"FunctionCallArguments\": null\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Request/Response Agents in AutoGen Core\nDESCRIPTION: Demonstrates how to test agents that use direct messaging for request/response scenarios in AutoGen core.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/message-and-communication.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nruntime = SingleThreadedAgentRuntime()\nawait InnerAgent.register(runtime, \"inner_agent\", lambda: InnerAgent(\"InnerAgent\"))\nawait OuterAgent.register(runtime, \"outer_agent\", lambda: OuterAgent(\"OuterAgent\", \"inner_agent\"))\nruntime.start()\nouter_agent_id = AgentId(\"outer_agent\", \"default\")\nawait runtime.send_message(Message(content=\"Hello, World!\"), outer_agent_id)\nawait runtime.stop_when_idle()\n```\n\n----------------------------------------\n\nTITLE: Tabulating HumanEval Results with AgBench\nDESCRIPTION: Generates a summary of the HumanEval benchmark results, including task completion rates, using the AgBench tool.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/HumanEval/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nagbench tabulate Results/human_eval_AgentChat\n```\n\n----------------------------------------\n\nTITLE: OpenAI O3-Mini Model Configuration\nDESCRIPTION: YAML configuration for using the O3-Mini model from OpenAI with AutoGen.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_chess_game/README.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nprovider: autogen_ext.models.openai.OpenAIChatCompletionClient\nconfig:\n  model: o3-mini-2025-01-31\n  api_key: replace with your API key or skip it if you have environment variable OPENAI_API_KEY set\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging with Python Logging Module\nDESCRIPTION: This snippet sets up a logger in Python using the standard logging module for logging model calls and responses related to the AutoGen framework. It logs events under the logger name 'autogen_core.EVENT_LOGGER_NAME' with the event type 'LLMCall'.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/model-clients.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport logging\n\nfrom autogen_core import EVENT_LOGGER_NAME\n\nlogging.basicConfig(level=logging.WARNING)\nlogger = logging.getLogger(EVENT_LOGGER_NAME)\nlogger.addHandler(logging.StreamHandler())\nlogger.setLevel(logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Message Types in Protocol Buffers\nDESCRIPTION: Example of defining custom message types using Protocol Buffers (protobuf) for use with AutoGen. It shows the structure for NewAsk and ReadmeRequested messages.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/Hello/HelloAgent/README.md#2025-04-22_snippet_3\n\nLANGUAGE: proto\nCODE:\n```\nsyntax = \"proto3\";\npackage devteam;\noption csharp_namespace = \"DevTeam.Shared\";\nmessage NewAsk {\n  string org = 1;\n  string repo = 2;\n  string ask = 3;\n  int64 issue_number = 4;\n}\nmessage ReadmeRequested {\n   string org = 1;\n   string repo = 2;\n   int64 issue_number = 3;\n   string ask = 4;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring RST Documentation for llama_cpp Module in AutoGen Extensions\nDESCRIPTION: This RST directive configures the documentation generation for the llama_cpp module in the AutoGen extensions package. It sets options to display all members, include undocumented members, show inheritance relationships, and organize members in the order they appear in the source code.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_ext.models.llama_cpp.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogen_ext.models.llama_cpp\n   :members:\n   :undoc-members:\n   :show-inheritance:\n   :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAIWrapper in AutoGen v0.2\nDESCRIPTION: Example of configuring a model client in AutoGen v0.2 by creating an OpenAIWrapper object with a config list that specifies models and API keys.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen.oai import OpenAIWrapper\n\nconfig_list = [\n    {\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"},\n    {\"model\": \"gpt-4o-mini\", \"api_key\": \"sk-xxx\"},\n]\n\nmodel_client = OpenAIWrapper(config_list=config_list)\n```\n\n----------------------------------------\n\nTITLE: Basic Agent Chat Using GenerateReplyAsync\nDESCRIPTION: Demonstrates how to send a TextMessage to an agent using the GenerateReplyAsync method. Shows basic message creation and agent interaction.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Agent-overview.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nvar message = new TextMessage(\"Hello Agent!\");\nvar reply = await agent.GenerateReplyAsync(message);\n```\n\n----------------------------------------\n\nTITLE: Installing AgBench Tool from AutoGen Repository in Bash\nDESCRIPTION: This code snippet demonstrates how to navigate to the AgBench package directory within the AutoGen repository and install it using pip. AgBench is installed in editable mode (-e flag) for development purposes.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd autogen/python/packages/agbench\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Navigating to GAIA Directory\nDESCRIPTION: Command to change directory to the GAIA benchmark location\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/GAIA/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd benchmarks/GAIA\n```\n\n----------------------------------------\n\nTITLE: Initializing TypeSubscription Class in Python\nDESCRIPTION: Example of creating a TypeSubscription instance for matching topics based on type prefix and mapping to agents using the source as the agent key. This subscription causes each source to have its own agent instance.\nSOURCE: https://github.com/microsoft/autogen/blob/main/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core import TypePrefixSubscription\n\nsubscription = TypePrefixSubscription(topic_type_prefix=\"t1\", agent_type=\"a1\")\n```\n\n----------------------------------------\n\nTITLE: Adding Project to Package List in .csproj File\nDESCRIPTION: XML configuration example showing how to make a project packable by importing the nuget-package.props file. This enables IsPackable property and sets necessary metadata for the NuGet package.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/PACKAGING.md#2025-04-22_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<Import Project=\"$(RepoRoot)/nuget/nuget-package.props\" />\n```\n\n----------------------------------------\n\nTITLE: Defining DictionaryToStringAsync Function Schema in JSON\nDESCRIPTION: This JSON schema defines the DictionaryToStringAsync function. It specifies that the function takes an object parameter 'xargs' which is a dictionary of key-value pairs, where both keys and values are strings. The 'xargs' parameter is required.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.SourceGenerator.Tests/ApprovalTests/FunctionExample.DictionaryToString_Test.approved.txt#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"DictionaryToStringAsync\",\n  \"description\": \"DictionaryToString function\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"xargs\": {\n        \"type\": \"object\",\n        \"additionalProperties\": {\n          \"type\": \"string\"\n        },\n        \"description\": \"an object of key-value pairs. key is string, value is string\"\n      }\n    },\n    \"required\": [\n      \"xargs\"\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running AutoGen Studio with Gunicorn\nDESCRIPTION: This bash command starts the AutoGen Studio application using Gunicorn, a Python WSGI HTTP server for high performance.  It configures Gunicorn to use a specified number of worker processes based on the number of CPU cores, sets a timeout, specifies the Uvicorn worker class, and binds the application to all interfaces.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/faq.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ngunicorn -w $((2 * $(getconf _NPROCESSORS_ONLN) + 1)) --timeout 12600 -k uvicorn.workers.UvicornWorker autogenstudio.web.app:app --bind\n```\n\n----------------------------------------\n\nTITLE: Code Review Function Implementation in C#\nDESCRIPTION: Function implementation for the reviewer agent to validate code against specific criteria like code block format and structure.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Group-chat.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\nreviewer_function\n```\n\n----------------------------------------\n\nTITLE: Enabling OpenAI Instrumentation with OpenTelemetry in Python\nDESCRIPTION: This snippet demonstrates how to enable OpenTelemetry instrumentation for the OpenAI Python client library by importing the OpenAIInstrumentor class and calling its instrument() method. This enables automatic telemetry on OpenAI API requests within the application, allowing traces (including LLM calls) to be observed through a configured backend, such as Aspire or Jaeger. Prerequisites include installing the opentelemetry-instrumentation-openai package and setting up the OpenTelemetry exporter and backend dashboard.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/instrumenting.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry.instrumentation.openai import OpenAIInstrumentor\n\nOpenAIInstrumentor().instrument()\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Autodoc for autogen_core.memory Module\nDESCRIPTION: This reStructuredText snippet uses the Sphinx `automodule` directive to automatically generate API documentation for the Python module `autogen_core.memory`. The options `:members:`, `:undoc-members:`, and `:show-inheritance:` instruct Sphinx to include documentation for all members (functions, classes, etc.), members without docstrings, and to display the inheritance diagram for classes within the module, respectively.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_core.memory.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogen_core.memory\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Configuring reStructuredText Documentation for AutoGen Web Surfer Agent\nDESCRIPTION: This snippet shows the reStructuredText directives used to configure automatic API documentation generation for the web_surfer module. It specifies that all members, undocumented members, and inheritance should be included in the generated documentation.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_ext.agents.web_surfer.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogen_ext.agents.web_surfer\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Example Request Body\nDESCRIPTION: This is an example JSON payload for the `/chat/completions` endpoint. It contains the user's message and a unique conversation ID to track the session.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_streaming_handoffs_fastapi/README.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"message\": \"I need refund for a product.\",\n  \"conversation_id\": \"user123_session456\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Coder Agent in C#\nDESCRIPTION: Implementation of the coder agent responsible for writing code solutions in response to tasks.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Group-chat.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\ncreate_coder\n```\n\n----------------------------------------\n\nTITLE: Running All Development Checks for AutoGen Python Packages\nDESCRIPTION: This snippet shows how to set up the development environment and run all checks for AutoGen Python packages using uv and poe.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/README.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nuv sync --all-extras\nsource .venv/bin/activate\npoe check\n```\n\n----------------------------------------\n\nTITLE: Model Client Cache Configuration in AutoGen v0.2\nDESCRIPTION: Example of enabling cache for an LLM in AutoGen v0.2 using cache_seed parameter in the LLM configuration, which is enabled by default.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n    \"cache_seed\": 42,\n}\n```\n\n----------------------------------------\n\nTITLE: Using Structured Output with OpenAI Model Client\nDESCRIPTION: This snippet configures a structured output using a Pydantic BaseModel for responses from OpenAIChatCompletionClient. It demonstrates how to define the response format with Pydantic and parse the structured response.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/model-clients.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Literal\n\nfrom pydantic import BaseModel\n\n\n# The response format for the agent as a Pydantic base model.\nclass AgentResponse(BaseModel):\n    thoughts: str\n    response: Literal[\"happy\", \"sad\", \"neutral\"]\n\n\n# Create an agent that uses the OpenAI GPT-4o model with the custom response format.\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    response_format=AgentResponse,  # type: ignore\n)\n\n# Send a message list to the model and await the response.\nmessages = [\n    UserMessage(content=\"I am happy.\", source=\"user\"),\n]\nresponse = await model_client.create(messages=messages)\nassert isinstance(response.content, str)\nparsed_response = AgentResponse.model_validate_json(response.content)\nprint(parsed_response.thoughts)\nprint(parsed_response.response)\n\n# Close the connection to the model client.\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx automodule for autogen_ext.models.azure (reStructuredText)\nDESCRIPTION: Uses the reStructuredText Sphinx 'automodule' directive to document the autogen_ext.models.azure module. The :members:, :undoc-members:, and :show-inheritance: options control what module members and inheritance structures are included in the rendered API docs. Prerequisites include Sphinx and autodoc extension enabled in the documentation build system. This snippet is not executable Python code but rather documentation markup for Sphinx usage.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_ext.models.azure.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: autogen_ext.models.azure\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Defining a Weather Tool for the Agent - Python\nDESCRIPTION: Implements a tool function get_weather, designed to be callable by the agent to provide mock weather information based on the 'location' string input. The function distinguishes between San Francisco and other locations, returning different placeholder weather responses. It relies on the '@tool' decorator to register with LangChain, and does not actually fetch live weather data.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/langgraph-agent.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@tool  # pyright: ignore\ndef get_weather(location: str) -> str:\n    \"\"\"Call to surf the web.\"\"\"\n    # This is a placeholder, but don't tell the LLM that...\n    if \"sf\" in location.lower() or \"san francisco\" in location.lower():\n        return \"It's 60 degrees and foggy.\"\n    return \"It's 90 degrees and sunny.\"\n```\n\n----------------------------------------\n\nTITLE: Converting Tool Call Message in AutoGen\nDESCRIPTION: Shows how a tool call message is converted to include the assistant role with an empty content and a TooCall array containing the function call details.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.OpenAI.V1.Tests/ApprovalTests/OpenAIMessageTests.BasicMessageTest.approved.txt#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"OriginalMessage\": \"ToolCallMessage(assistant)\\n\\tToolCall(test, test, )\",\n  \"ConvertedMessages\": [\n    {\n      \"Role\": \"assistant\",\n      \"Content\": \"\",\n      \"Name\": null,\n      \"TooCall\": [\n        {\n          \"Type\": \"Function\",\n          \"Name\": \"test\",\n          \"Arguments\": \"test\",\n          \"Id\": \"test\"\n        }\n      ],\n      \"FunctionCallName\": null,\n      \"FunctionCallArguments\": null\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Module-Specific Trace Logger\nDESCRIPTION: Example of creating a module-specific trace logger as a child of the main trace logger.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/logging.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nfrom autogen_core import TRACE_LOGGER_NAME\nlogger = logging.getLogger(f\"{TRACE_LOGGER_NAME}.my_module\")\n```\n\n----------------------------------------\n\nTITLE: Azure OpenAI Model Client Configuration\nDESCRIPTION: This JSON defines the configuration for an Azure OpenAI model client within AutoGen Studio. It specifies the provider, component type, version, description, label, and the necessary Azure-specific parameters like `model`, `api_key`, `azure_endpoint`, `azure_deployment`, and `api_version`. The endpoint and deployment names need to be configured appropriately.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/faq.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"provider\": \"autogen_ext.models.openai.AzureOpenAIChatCompletionClient\",\n  \"component_type\": \"model\",\n  \"version\": 1,\n  \"component_version\": 1,\n  \"description\": \"Chat completion client for Azure OpenAI hosted models.\",\n  \"label\": \"AzureOpenAIChatCompletionClient\",\n  \"config\": {\n    \"model\": \"gpt-4o\",\n    \"api_key\": \"sk-...\",\n    \"azure_endpoint\": \"https://{your-custom-endpoint}.openai.azure.com/\",\n    \"azure_deployment\": \"{your-azure-deployment}\",\n    \"api_version\": \"2024-06-01\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running HumanEval Benchmark Commands\nDESCRIPTION: Series of commands for navigating to HumanEval benchmark directory, initializing tasks, and running the benchmark.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd autogen/python/packages/agbench/benchmarks/HumanEval\npython Scripts/init_tasks.py\nagbench run Tasks/human_eval_MagenticOne.jsonl\nagbench tabulate Results/human_eval_MagenticOne\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen AgentChat Package using pip\nDESCRIPTION: Command to install the main AutoGen AgentChat package using pip, with the -U flag to ensure the latest version is installed.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/installation.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"autogen-agentchat\"\n```\n\n----------------------------------------\n\nTITLE: Serializing and Deserializing Termination Conditions in AutoGen\nDESCRIPTION: This snippet demonstrates how to create termination conditions (MaxMessageTermination and StopMessageTermination), combine them with the OR operator, export the configuration to a dictionary/JSON, and load it back into a new object.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/serialize-components.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.conditions import MaxMessageTermination, StopMessageTermination\n\nmax_termination = MaxMessageTermination(5)\nstop_termination = StopMessageTermination()\n\nor_termination = max_termination | stop_termination\n\nor_term_config = or_termination.dump_component()\nprint(\"Config: \", or_term_config.model_dump_json())\n\nnew_or_termination = or_termination.load_component(or_term_config)\n```\n\n----------------------------------------\n\nTITLE: Adding Using Statements for AutoGen and Semantic Kernel in C#\nDESCRIPTION: This snippet shows the necessary using statements for working with AutoGen and Semantic Kernel. It includes references to Microsoft.SemanticKernel and AutoGen namespaces.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.SemanticKernel/SemanticKernelChatAgent-simple-chat.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.ChatCompletion;\nusing Microsoft.SemanticKernel.Connectors.OpenAI;\nusing AutoGen;\nusing AutoGen.Core;\nusing AutoGen.OpenAI;\nusing AutoGen.SemanticKernel;\n```\n\n----------------------------------------\n\nTITLE: Declarative Gemini Assistant Agent Definition in Python\nDESCRIPTION: This code snippet defines a custom agent, `GeminiAssistantAgent`, that inherits from both `BaseChatAgent` and `Component`. It includes the `_from_config` and `_to_config` methods for serialization and deserialization of agent configurations. It uses the Gemini API for generating responses.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/custom-agents.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom typing import AsyncGenerator, Sequence\n\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.base import Response\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\nfrom autogen_core import CancellationToken, Component\nfrom pydantic import BaseModel\nfrom typing_extensions import Self\n\n\nclass GeminiAssistantAgentConfig(BaseModel):\n    name: str\n    description: str = \"An agent that provides assistance with ability to use tools.\"\n    model: str = \"gemini-1.5-flash-002\"\n    system_message: str | None = None\n\n\nclass GeminiAssistantAgent(BaseChatAgent, Component[GeminiAssistantAgentConfig]):  # type: ignore[no-redef]\n    component_config_schema = GeminiAssistantAgentConfig\n    # component_provider_override = \"mypackage.agents.GeminiAssistantAgent\"\n\n    def __init__(\n        self,\n        name: str,\n        description: str = \"An agent that provides assistance with ability to use tools.\",\n        model: str = \"gemini-1.5-flash-002\",\n        api_key: str = os.environ[\"GEMINI_API_KEY\"],\n        system_message: str\n        | None = \"You are a helpful assistant that can respond to messages. Reply with TERMINATE when the task has been completed.\",\n    ):\n        super().__init__(name=name, description=description)\n        self._model_context = UnboundedChatCompletionContext()\n        self._model_client = genai.Client(api_key=api_key)\n        self._system_message = system_message\n        self._model = model\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        final_response = None\n        async for message in self.on_messages_stream(messages, cancellation_token):\n            if isinstance(message, Response):\n                final_response = message\n\n        if final_response is None:\n            raise AssertionError(\"The stream should have returned the final result.\")\n\n        return final_response\n\n    async def on_messages_stream(\n        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken\n    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:\n        # Add messages to the model context\n        for msg in messages:\n            await self._model_context.add_message(msg.to_model_message())\n\n        # Get conversation history\n        history = [\n            (msg.source if hasattr(msg, \"source\") else \"system\")\n            + \": \"\n            + (msg.content if isinstance(msg.content, str) else \"\")\n            + \"\\n\"\n            for msg in await self._model_context.get_messages()\n        ]\n\n        # Generate response using Gemini\n        response = self._model_client.models.generate_content(\n            model=self._model,\n            contents=f\"History: {history}\\nGiven the history, please provide a response\",\n            config=types.GenerateContentConfig(\n                system_instruction=self._system_message,\n                temperature=0.3,\n            ),\n        )\n\n        # Create usage metadata\n        usage = RequestUsage(\n            prompt_tokens=response.usage_metadata.prompt_token_count,\n            completion_tokens=response.usage_metadata.candidates_token_count,\n        )\n\n        # Add response to model context\n        await self._model_context.add_message(AssistantMessage(content=response.text, source=self.name))\n\n        # Yield the final response\n        yield Response(\n            chat_message=TextMessage(content=response.text, source=self.name, models_usage=usage),\n            inner_messages=[],\n        )\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        \"\"\"Reset the assistant by clearing the model context.\"\"\"\n        await self._model_context.clear()\n\n    @classmethod\n    def _from_config(cls, config: GeminiAssistantAgentConfig) -> Self:\n        return cls(\n            name=config.name, description=config.description, model=config.model, system_message=config.system_message\n        )\n\n    def _to_config(self) -> GeminiAssistantAgentConfig:\n        return GeminiAssistantAgentConfig(\n            name=self.name,\n            description=self.description,\n            model=self._model,\n            system_message=self._system_message,\n        )\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Autodoc for autogen_ext.tools.http Module\nDESCRIPTION: Uses the Sphinx `automodule` directive to target the `autogen_ext.tools.http` Python module for automatic documentation generation. The options `:members:`, `:undoc-members:`, and `:show-inheritance:` instruct Sphinx to include documentation for all members (public, private, documented, undocumented) and to display the class inheritance hierarchy.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_ext.tools.http.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogen_ext.tools.http\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Setting Azure OpenAI Environment Variables in Python\nDESCRIPTION: Sets necessary environment variables for connecting to Azure OpenAI services. This includes the endpoint URL, API key, deployment name (specifically mentioning a supported gpt-4o model), and the API version. These variables are typically used by the client library to authenticate and connect.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/structured-output-agent.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Set the environment variable\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://YOUR_ENDPOINT_DETAILS.openai.azure.com/\"\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\nos.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4o-2024-08-06\"\nos.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-08-01-preview\"\n```\n\n----------------------------------------\n\nTITLE: Creating an AssistantAgent with tools in Python\nDESCRIPTION: This snippet demonstrates how to create an AssistantAgent with a custom tool (web_search) and configure it to use the OpenAI GPT-4o model. It defines a mock web search function and then initializes an AssistantAgent with this tool, specifying the model client and a system message.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/agents.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Define a tool that searches the web for information.\n# For simplicity, we will use a mock function here that returns a static string.\nasync def web_search(query: str) -> str:\n    \"\"\"Find information on the web\"\"\"\n    return \"AutoGen is a programming framework for building multi-agent applications.\"\n\n\n# Create an agent that uses the OpenAI GPT-4o model.\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4.1-nano\",\n    # api_key=\"YOUR_API_KEY\",\n)\nagent = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    tools=[web_search],\n    system_message=\"Use tools to solve tasks.\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Function Call Middleware for Weather Function\nDESCRIPTION: Configuration of FunctionCallMiddleware with GetWeather function and automatic invocation setup\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/MistralChatAgent-use-function-call.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\nvar weatherFunction = new MistralAgentFunction();\nvar functionMap = new Dictionary<string, Func<Dictionary<string, object>, Task<object>>>\n{\n    { \"GetWeather\", async (Dictionary<string, object> parameters) =>\n        await weatherFunction.GetWeather((string)parameters[\"location\"]) }\n};\nvar functionCallMiddleware = new FunctionCallMiddleware(weatherFunction.GetFunctionContract(), functionMap);\n```\n\n----------------------------------------\n\nTITLE: Generated Source Code for Function Definition\nDESCRIPTION: Auto-generated C# code that includes the function schema, wrapper method, and function definition based on the original method signature and documentation.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/src/AutoGen.SourceGenerator/README.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\n// file: MyFunctions.generated.cs\npublic partial class MyFunctions\n{\n    private class AddAsyncSchema\n    {\n\t\tpublic int a {get; set;}\n\t\tpublic int b {get; set;}\n    }\n\n    public Task<string> AddAsyncWrapper(string arguments)\n    {\n        var schema = JsonSerializer.Deserialize<AddAsyncSchema>(\n            arguments, \n            new JsonSerializerOptions\n            {\n                PropertyNamingPolicy = JsonNamingPolicy.CamelCase,\n            });\n        return AddAsync(schema.a, schema.b);\n    }\n\n    public FunctionDefinition AddAsyncFunction\n    {\n        get => new FunctionDefinition\n\t\t{\n\t\t\tName = @\"AddAsync\",\n            Description = \"\"\"\nAdd two numbers.\n\"\"\",\n            Parameters = BinaryData.FromObjectAsJson(new\n            {\n                Type = \"object\",\n                Properties = new\n\t\t\t\t{\n\t\t\t\t    a = new\n\t\t\t\t    {\n\t\t\t\t\t    Type = @\"number\",\n\t\t\t\t\t    Description = @\"The first number.\",\n\t\t\t\t    },\n\t\t\t\t    b = new\n\t\t\t\t    {\n\t\t\t\t\t    Type = @\"number\",\n\t\t\t\t\t    Description = @\"The second number.\",\n\t\t\t\t    },\n                },\n                Required = new []\n\t\t\t\t{\n\t\t\t\t    \"a\",\n\t\t\t\t    \"b\",\n\t\t\t\t},\n            },\n            new JsonSerializerOptions\n\t\t\t{\n\t\t\t\tPropertyNamingPolicy = JsonNamingPolicy.CamelCase,\n\t\t\t})\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Limiting Model Context with AssistantAgent Python\nDESCRIPTION: This code snippet demonstrates how to limit the model context used by `AssistantAgent` by providing a `BufferedChatCompletionContext`. This allows the agent to only use the last `n` messages in the context, reducing the size of the prompt and potentially improving performance. Requires importing `BufferedChatCompletionContext` from `autogen_core.model_context`.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/agents.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core.model_context import BufferedChatCompletionContext\n\n# Create an agent that uses only the last 5 messages in the context to generate responses.\nagent = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    tools=[web_search],\n    system_message=\"Use tools to solve tasks.\",\n    model_context=BufferedChatCompletionContext(buffer_size=5),  # Only use the last 5 messages in the context.\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Turn Function Call with Gemini\nDESCRIPTION: Implementation of a multi-turn conversation where the agent makes sequential function calls - first finding movies by a director and then retrieving a review for one of the found movies.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Function-call-with-gemini.md#2025-04-22_snippet_5\n\nLANGUAGE: csharp\nCODE:\n```\n// Multi-turn function call\nConsole.WriteLine(\"Multi-turn function call:\");\nvar chatMessages = new List<ChatMessage>\n{\n    new AssistantChatMessage(\"I'm a movie expert assistant that can help you find movies by director and provide movie reviews.\")\n};\n\nvar userMessages = new string[]\n{\n    \"Who directed Barbie?\",\n    \"What other movies did she direct?\",\n    \"What's your review of Barbie?\"\n};\n\nforeach (var message in userMessages)\n{\n    chatMessages.Add(new UserChatMessage(message));\n    var chatResult = await geminiAgent.GetChatCompletionsAsync(chatMessages.ToArray());\n    chatMessages.Add(chatResult.Message);\n\n    Console.WriteLine($\"User: {message}\");\n    Console.WriteLine($\"Gemini: {chatResult.Message.Content}\");\n    \n    if (chatResult.FunctionResults.Any())\n    {\n        Console.WriteLine(\"Function Results:\");\n        foreach (var functionResult in chatResult.FunctionResults)\n        {\n            Console.WriteLine($\"  {functionResult.Name}: {functionResult.Result}\");\n        }\n    }\n    Console.WriteLine();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Event Handlers in C# for AutoGen Agent\nDESCRIPTION: Code showing how to create a HelloAgent class that inherits from ConsoleAgent and implements event handlers for specific message types. This demonstrates the event-driven architecture in AutoGen.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/Microsoft.AutoGen.Integration.Tests.AppHosts/HelloAgentTests/README.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\nTopicSubscription(\"HelloAgents\")]\npublic class HelloAgent(\n    iAgentWorker worker,\n    [FromKeyedServices(\"AgentsMetadata\")] AgentsMetadata typeRegistry) : ConsoleAgent(\n        worker,\n        typeRegistry),\n        ISayHello,\n        IHandle<NewMessageReceived>,\n        IHandle<ConversationClosed>\n{\n    public async Task Handle(NewMessageReceived item, CancellationToken cancellationToken = default)\n    {\n        var response = await SayHello(item.Message).ConfigureAwait(false);\n        var evt = new Output\n        {\n            Message = response\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEventAsync(evt).ConfigureAwait(false);\n        var goodbye = new ConversationClosed\n        {\n            UserId = this.AgentId.Key,\n            UserMessage = \"Goodbye\"\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEventAsync(goodbye).ConfigureAwait(false);\n    }\n```\n\n----------------------------------------\n\nTITLE: Visualizing Group Chat Workflow with Mermaid Diagram\nDESCRIPTION: This diagram illustrates the desired workflow for a code writing and review process, showing the interactions between Admin, Coder, Reviewer, and Runner agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Use-graph-in-group-chat.md#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD\n    A[Admin] -->|Ask coder to write code| B[Coder]\n    B -->|Ask Reviewer to review code| C[Reviewer]\n    C -->|Ask Runner to run code| D[Runner]\n    D -->|Send result if succeed| A[Admin]\n    D -->|Ask coder to fix if failed| B[Coder]\n    C -->|Ask coder to fix if not approved| B[Coder]\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Core Package\nDESCRIPTION: Command to install the core AutoGen package using pip. Requires Python 3.10 or later.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/installation.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install \"autogen-core\"\n```\n\n----------------------------------------\n\nTITLE: Creating a New Package for AutoGen Python Project\nDESCRIPTION: These commands set up the environment and use a template to create a new package for the AutoGen project.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/README.md#2025-04-22_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nuv sync --python 3.12\nsource .venv/bin/activate\ncookiecutter ./templates/new-package/\n```\n\n----------------------------------------\n\nTITLE: Processing Streaming Message Updates in C#\nDESCRIPTION: Demonstrates how to handle streaming message updates by printing them to console and updating the final result on the caller side.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Built-in-messages.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nvar result = \"\";\nvar stream = someStreamingCall();\nawait foreach (var update in stream)\n{\n    if (update is TextMessageUpdate textUpdate)\n    {\n        Console.Write(textUpdate.Text);\n        result += textUpdate.Text;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Agent Team and Termination Condition (Python)\nDESCRIPTION: Defines the end-of-conversation condition with TextMentionTermination and composes a RoundRobinGroupChat including the three previously instantiated agents. This facilitates an orderly, cyclic workflow and activation of the literature review chain, with defined stopping logic. Inputs are agent references and a termination trigger; output is a configured team ready for interactive execution.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/literature-review.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntermination = TextMentionTermination(\"TERMINATE\")\\nteam = RoundRobinGroupChat(\\n    participants=[google_search_agent, arxiv_search_agent, report_agent], termination_condition=termination\\n)\\n\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API key for AutoGen in Python\nDESCRIPTION: This code snippet shows how to set up the OpenAI API key as an environment variable for use with AutoGen. It's a crucial step for authenticating and accessing OpenAI's services.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/GAIA/Templates/SelectorGroupChat/prompt.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Concept Extractor Agent\nDESCRIPTION: Implementation of the first agent in the workflow that analyzes product descriptions and extracts key features, target audience, and USPs.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/sequential-workflow.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@type_subscription(topic_type=concept_extractor_topic_type)\nclass ConceptExtractorAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"A concept extractor agent.\")\n        self._system_message = SystemMessage(\n            content=(\n                \"You are a marketing analyst. Given a product description, identify:\\n\"\n                \"- Key features\\n\"\n                \"- Target audience\\n\"\n                \"- Unique selling points\\n\\n\"\n            )\n        )\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_user_description(self, message: Message, ctx: MessageContext) -> None:\n        prompt = f\"Product description: {message.content}\"\n        llm_result = await self._model_client.create(\n            messages=[self._system_message, UserMessage(content=prompt, source=self.id.key)],\n            cancellation_token=ctx.cancellation_token,\n        )\n        response = llm_result.content\n        assert isinstance(response, str)\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{response}\")\n\n        await self.publish_message(Message(response), topic_id=TopicId(writer_topic_type, source=self.id.key))\n```\n\n----------------------------------------\n\nTITLE: Creating a GeminiChatAgent Instance\nDESCRIPTION: Code for instantiating a GeminiChatAgent with a Google API key and configuration. This sets up the agent with the specified Gemini model and configures it for chat interactions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Chat-with-google-gemini.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\nvar geminiApiKey = Environment.GetEnvironmentVariable(\"GEMINI_API_KEY\") ?? \"YOUR_GEMINI_API_KEY\";\n\nvar geminiConfig = new GeminiAgentConfig\n{\n    ModelId = \"gemini-pro\",\n    ApiKey = geminiApiKey,\n};\n\nvar geminiAgent = new GeminiChatAgent(\n    \"Gemini\", \n    geminiConfig, \n    maxRetries: 2);\n```\n\n----------------------------------------\n\nTITLE: Running the Chess Game\nDESCRIPTION: Commands to run the chess game, including options for playing against AI or enabling human vs AI mode.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_chess_game/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n\n# For human vs AI mode:\npython main.py --human\n```\n\n----------------------------------------\n\nTITLE: Creating and Chatting with Conversable Agent using AutoGen in C#\nDESCRIPTION: This snippet demonstrates how to create an AssistantAgent and a UserProxyAgent, configure them with OpenAI GPT-3.5 Turbo, and initiate a conversation between them. It includes setting up the OpenAI configuration, defining agent behaviors, and starting the chat process.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/README.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nusing AutoGen;\nusing AutoGen.OpenAI;\n\nvar openAIKey = Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\") ?? throw new Exception(\"Please set OPENAI_API_KEY environment variable.\");\nvar gpt35Config = new OpenAIConfig(openAIKey, \"gpt-3.5-turbo\");\n\nvar assistantAgent = new AssistantAgent(\n    name: \"assistant\",\n    systemMessage: \"You are an assistant that help user to do some tasks.\",\n    llmConfig: new ConversableAgentConfig\n    {\n        Temperature = 0,\n        ConfigList = [gpt35Config],\n    })\n    .RegisterPrintMessage(); // register a hook to print message nicely to console\n\n// set human input mode to ALWAYS so that user always provide input\nvar userProxyAgent = new UserProxyAgent(\n    name: \"user\",\n    humanInputMode: ConversableAgent.HumanInputMode.ALWAYS)\n    .RegisterPrintMessage();\n\n// start the conversation\nawait userProxyAgent.InitiateChatAsync(\n    receiver: assistantAgent,\n    message: \"Hey assistant, please do me a favor.\",\n    maxRound: 10);\n```\n\n----------------------------------------\n\nTITLE: Creating a Docker-Based Python Code Executor and Tool in Python\nDESCRIPTION: Instantiates a `DockerCommandLineCodeExecutor` from `autogen_ext.code_executors.docker` to enable running Python code within isolated Docker containers. Subsequently, it creates a `PythonCodeExecutionTool` from `autogen_ext.tools.code_execution`, configuring it to use the previously created Docker executor. This tool can then be provided to an AutoGen agent.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/tool-use-with-intervention.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Create the docker executor for the Python code execution tool.\ndocker_executor = DockerCommandLineCodeExecutor()\n\n# Create the Python code execution tool.\npython_tool = PythonCodeExecutionTool(executor=docker_executor)\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for autogen_core.code_executor with Sphinx\nDESCRIPTION: This Sphinx reStructuredText directive automatically generates documentation for the Python module `autogen_core.code_executor`. The `:members:` option includes public members, `:undoc-members:` includes members even if they lack docstrings, and `:show-inheritance:` displays the base classes for any classes defined within the module. Requires Sphinx installed and the `autogen_core` package available in the Python path.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_core.code_executor.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogen_core.code_executor\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Creating Conversable Agent with Custom Reply in v0.2\nDESCRIPTION: Shows how to create a ConversableAgent and register a custom reply function in v0.2.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, List, Optional, Tuple, Union\nfrom autogen.agentchat import ConversableAgent\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\nconversable_agent = ConversableAgent(\n    name=\"conversable_agent\",\n    system_message=\"You are a helpful assistant.\",\n    llm_config=llm_config,\n    code_execution_config={\"work_dir\": \"coding\"},\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=10,\n)\n\ndef reply_func(\n    recipient: ConversableAgent,\n    messages: Optional[List[Dict]] = None,\n    sender: Optional[Agent] = None,\n    config: Optional[Any] = None,\n) -> Tuple[bool, Union[str, Dict, None]]:\n    # Custom reply logic here\n    return True, \"Custom reply\"\n\n# Register the reply function\nconversable_agent.register_reply([ConversableAgent], reply_func, position=0)\n\n# NOTE: An async reply function will only be invoked with async send.\n```\n\n----------------------------------------\n\nTITLE: Implementing Format & Proof Agent\nDESCRIPTION: Implementation of the third agent that polishes the marketing copy by refining grammar and enhancing clarity.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/sequential-workflow.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@type_subscription(topic_type=format_proof_topic_type)\nclass FormatProofAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"A format & proof agent.\")\n        self._system_message = SystemMessage(\n            content=(\n                \"You are an editor. Given the draft copy, correct grammar, improve clarity, ensure consistent tone, \"\n                \"give format and make it polished. Output the final improved copy as a single text block.\"\n            )\n        )\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_intermediate_text(self, message: Message, ctx: MessageContext) -> None:\n        prompt = f\"Draft copy:\\n{message.content}.\"\n        llm_result = await self._model_client.create(\n            messages=[self._system_message, UserMessage(content=prompt, source=self.id.key)],\n            cancellation_token=ctx.cancellation_token,\n        )\n        response = llm_result.content\n        assert isinstance(response, str)\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{response}\")\n\n        await self.publish_message(Message(response), topic_id=TopicId(user_topic_type, source=self.id.key))\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Tools in Python\nDESCRIPTION: Implementation of utility functions and tools for order execution, item lookup, and refund processing.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/handoffs.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef execute_order(product: str, price: int) -> str:\n    print(\"\\n\\n=== Order Summary ===\")\n    print(f\"Product: {product}\")\n    print(f\"Price: ${price}\")\n    print(\"=================\\n\")\n    confirm = input(\"Confirm order? y/n: \").strip().lower()\n    if confirm == \"y\":\n        print(\"Order execution successful!\")\n        return \"Success\"\n    else:\n        print(\"Order cancelled!\")\n        return \"User cancelled order.\"\n\n\ndef look_up_item(search_query: str) -> str:\n    item_id = \"item_132612938\"\n    print(\"Found item:\", item_id)\n    return item_id\n\n\ndef execute_refund(item_id: str, reason: str = \"not provided\") -> str:\n    print(\"\\n\\n=== Refund Summary ===\")\n    print(f\"Item ID: {item_id}\")\n    print(f\"Reason: {reason}\")\n    print(\"=================\\n\")\n    print(\"Refund execution successful!\")\n    return \"success\"\n\n\nexecute_order_tool = FunctionTool(execute_order, description=\"Price should be in USD.\")\nlook_up_item_tool = FunctionTool(\n    look_up_item, description=\"Use to find item ID.\\nSearch query can be a description or keywords.\"\n)\nexecute_refund_tool = FunctionTool(execute_refund, description=\"\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Checker Agent in C# for AutoGen\nDESCRIPTION: Defines a Checker agent that monitors the count and stops the application when it reaches 1, using dependency injection for application lifecycle management.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/tutorial.md#2025-04-22_snippet_7\n\nLANGUAGE: csharp\nCODE:\n```\nusing Microsoft.AutoGen.Contracts;\nusing Microsoft.AutoGen.Core;\nusing Microsoft.Extensions.Hosting;\n\nusing CheckF = System.Func<int, bool>;\n\nnamespace GettingStarted;\n\n[TypeSubscription(\"default\")]\npublic class Checker(\n    AgentId id,\n    IAgentRuntime runtime,\n    CheckF checkFunc,\n    IHostApplicationLifetime lifetime\n    ) :\n        BaseAgent(id, runtime, \"Checker\", null),\n        IHandle<CountUpdate>\n{\n    public async ValueTask HandleAsync(CountUpdate item, MessageContext messageContext)\n    {\n        Console.WriteLine($\"\\nChecker:\\nChecking {item.NewCount}\");\n\n        if (checkFunc(item.NewCount))\n        {\n            Console.WriteLine(\"\\nChecker:\\nCount is 1, stopping application\");\n            lifetime.StopApplication();\n            return;\n        }\n\n        CountMessage message = new CountMessage { Content = item.NewCount };\n        await this.PublishMessageAsync(message, topic: new TopicId(\"default\"));\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Sender-Routed Agent in AutoGen Core\nDESCRIPTION: Shows how to create an agent that routes messages based on the sender using the match parameter of the @message_handler decorator in AutoGen core.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/message-and-communication.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass RoutedBySenderAgent(RoutedAgent):\n    @message_handler(match=lambda msg, ctx: msg.source.startswith(\"user1\"))  # type: ignore\n    async def on_user1_message(self, message: TextMessage, ctx: MessageContext) -> None:\n        print(f\"Hello from user 1 handler, {message.source}, you said {message.content}!\")\n\n    @message_handler(match=lambda msg, ctx: msg.source.startswith(\"user2\"))  # type: ignore\n    async def on_user2_message(self, message: TextMessage, ctx: MessageContext) -> None:\n        print(f\"Hello from user 2 handler, {message.source}, you said {message.content}!\")\n\n    @message_handler(match=lambda msg, ctx: msg.source.startswith(\"user2\"))  # type: ignore\n    async def on_image_message(self, message: ImageMessage, ctx: MessageContext) -> None:\n        print(f\"Hello, {message.source}, you sent me {message.url}!\")\n```\n\n----------------------------------------\n\nTITLE: Streaming AutoGen Team Output to Console in Python\nDESCRIPTION: This snippet provides a concise way to stream the output of a team's execution directly to the console using AutoGen's UI utilities. It resets the team using `await team.reset()` and then wraps the `team.run_stream()` call within `Console()` to automatically format and print the messages as they are generated.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/teams.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nawait team.reset()  # Reset the team for a new task.\nawait Console(team.run_stream(task=\"Write a short poem about the fall season.\"))  # Stream the messages to the console.\n```\n\n----------------------------------------\n\nTITLE: Converting Aggregate Message with Tool Call and Result in AutoGen\nDESCRIPTION: Shows how an aggregate message containing both a tool call and its result is converted to two separate messages: one with the assistant role for the tool call and another with the tool role for the result.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.OpenAI.V1.Tests/ApprovalTests/OpenAIMessageTests.BasicMessageTest.approved.txt#2025-04-22_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"OriginalMessage\": \"AggregateMessage(assistant)\\n\\tToolCallMessage(assistant)\\n\\tToolCall(test, test, )\\n\\tToolCallResultMessage(assistant)\\n\\tToolCall(test, test, result)\",\n  \"ConvertedMessages\": [\n    {\n      \"Role\": \"assistant\",\n      \"Content\": \"\",\n      \"Name\": null,\n      \"TooCall\": [\n        {\n          \"Type\": \"Function\",\n          \"Name\": \"test\",\n          \"Arguments\": \"test\",\n          \"Id\": \"test\"\n        }\n      ],\n      \"FunctionCallName\": null,\n      \"FunctionCallArguments\": null\n    },\n    {\n      \"Role\": \"tool\",\n      \"Content\": \"result\",\n      \"ToolCallId\": \"test\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen.LMStudio via NuGet in C# Project\nDESCRIPTION: Shows how to add the AutoGen.LMStudio NuGet package reference to a C# project using the .csproj file. The AUTOGEN_VERSION placeholder should be replaced with the actual version number required.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/src/AutoGen.LMStudio/README.md#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<ItemGroup>\n    <PackageReference Include=\"AutoGen.LMStudio\" Version=\"AUTOGEN_VERSION\" />\n</ItemGroup>\n```\n\n----------------------------------------\n\nTITLE: Downloading and Processing Wikipedia Data\nDESCRIPTION: Fetching and saving Wikipedia article for file search demonstration.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/openai-assistant-agent.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresponse = requests.get(\"https://en.wikipedia.org/wiki/Third_Anglo-Afghan_War\")\nwith open(\"third_anglo_afghan_war.html\", \"wb\") as file:\n    file.write(response.content)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for AgentChat API\nDESCRIPTION: This snippet provides the necessary commands to install the 'autogen-agentchat' and 'autogen-ext[magentic-one,openai]' packages using pip. It also includes instructions to install additional dependencies for the MultimodalWebSurfer using Playwright. Users should have Python and pip installed beforehand.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/magentic-one.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"autogen-agentchat\" \"autogen-ext[magentic-one,openai]\"\n\n# If using the MultimodalWebSurfer, you also need to install playwright dependencies:\nplaywright install --with-deps chromium\n```\n\n----------------------------------------\n\nTITLE: Complete AutoGen Application Setup in C#\nDESCRIPTION: The full implementation of the Program class, showcasing the entire setup process for the AutoGen multi-agent countdown application.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/tutorial.md#2025-04-22_snippet_11\n\nLANGUAGE: csharp\nCODE:\n```\nusing Microsoft.AutoGen.Contracts;\nusing Microsoft.AutoGen.Core;\nusing Microsoft.Extensions.DependencyInjection;\nusing Microsoft.Extensions.Hosting;\n\nusing ModifyF = System.Func<int, int>;\nusing CheckF = System.Func<int, bool>;\n\nnamespace GettingStarted;\n\npublic class Program\n{\n    static int Modify(int count) => count - 1;\n    static bool Check(int count) => count == 1;\n\n    public static async Task Main(string[] args)\n    {\n        var builder = Host.CreateDefaultBuilder(args);\n\n        builder.UseAutoGen(b => b.UseInProcess())\n            .ConfigureServices(services =>\n            {\n                services.AddSingleton<ModifyF>(Modify);\n                services.AddSingleton<CheckF>(Check);\n                services.AddTransient<Modifier>();\n                services.AddTransient<Checker>();\n            });\n\n        using var host = builder.Build();\n        await host.StartAsync();\n\n        var runtime = host.Services.GetRequiredService<IAgentRuntime>();\n        var message = new CountMessage { Content = 10 };\n        await runtime.PublishMessageAsync(message, new TopicId(\"default\"));\n\n        await host.WaitForShutdownAsync();\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing AutoGen Runtime with SimpleAgent\nDESCRIPTION: Sets up an AutoGen runtime environment with a SimpleAgent using GPT-4 model. Demonstrates the complete lifecycle of creating a model client, registering an agent, sending messages, and cleanup.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/model-clients.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core import AgentId\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o-mini\",\n    # api_key=\"sk-...\", # Optional if you have an OPENAI_API_KEY set in the environment.\n)\n\nruntime = SingleThreadedAgentRuntime()\nawait SimpleAgent.register(\n    runtime,\n    \"simple_agent\",\n    lambda: SimpleAgent(model_client=model_client),\n)\n# Start the runtime processing messages.\nruntime.start()\n# Send a message to the agent and get the response.\nmessage = Message(\"Hello, what are some fun things to do in Seattle?\")\nresponse = await runtime.send_message(message, AgentId(\"simple_agent\", \"default\"))\nprint(response.content)\n# Stop the runtime processing messages.\nawait runtime.stop()\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Installing the Arxiv Library (Bash)\nDESCRIPTION: Uses pip to install the 'arxiv' Python package which is required for querying the Arxiv academic papers database. This step must be run before using the arxiv_search function and enables programmatic access to scholarly articles. Requires pip and internet connectivity.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/literature-review.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n!pip install arxiv\\n\n```\n\n----------------------------------------\n\nTITLE: Citing AutoGen Studio in Research Papers (BibTeX)\nDESCRIPTION: Provides the BibTeX entry for citing the AutoGen Studio paper presented at the EMNLP 2024 conference. This format is standard for managing references in academic publications and should be used when referencing AutoGen Studio in research.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{autogenstudio,\n  title={AUTOGEN STUDIO: A No-Code Developer Tool for Building and Debugging Multi-Agent Systems},\n  author={Dibia, Victor and Chen, Jingya and Bansal, Gagan and Syed, Suff and Fourney, Adam and Zhu, Erkang and Wang, Chi and Amershi, Saleema},\n  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},\n  pages={72--79},\n  year={2024}\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Multiple Tool Calls in AutoGen\nDESCRIPTION: Demonstrates how multiple tool calls in a single message are converted to a message with the assistant role containing a TooCall array with multiple function calls, each with auto-generated IDs.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.OpenAI.V1.Tests/ApprovalTests/OpenAIMessageTests.BasicMessageTest.approved.txt#2025-04-22_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"OriginalMessage\": \"ToolCallMessage(assistant)\\n\\tToolCall(test, test, )\\n\\tToolCall(test, test, )\",\n  \"ConvertedMessages\": [\n    {\n      \"Role\": \"assistant\",\n      \"Content\": \"\",\n      \"Name\": null,\n      \"TooCall\": [\n        {\n          \"Type\": \"Function\",\n          \"Name\": \"test\",\n          \"Arguments\": \"test\",\n          \"Id\": \"test_0\"\n        },\n        {\n          \"Type\": \"Function\",\n          \"Name\": \"test\",\n          \"Arguments\": \"test\",\n          \"Id\": \"test_1\"\n        }\n      ],\n      \"FunctionCallName\": null,\n      \"FunctionCallArguments\": null\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Exporting and Running Agent Workflows in Python\nDESCRIPTION: This python code snippet demonstrates how to import and run a team specification (exported from the Team Builder view in AutoGen Studio) within a Python application using the `TeamManager` class. It loads the team configuration from a JSON file and executes a task, returning a result stream.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/faq.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom autogenstudio.teammanager import TeamManager\n\ntm = TeamManager()\nresult_stream =  tm.run(task=\"What is the weather in New York?\", team_config=\"team.json\") # or wm.run_stream(..)\n```\n\n----------------------------------------\n\nTITLE: Configuring gRPC Tools for Protocol Buffer Compilation\nDESCRIPTION: XML configuration in the project file to include gRPC tools and specify Protocol Buffer files for compilation in an AutoGen .NET project.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/Hello/HelloAgent/README.md#2025-04-22_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<ItemGroup>\n  <PackageReference Include=\"Google.Protobuf\" />\n  <PackageReference Include=\"Grpc.Tools\" PrivateAssets=\"All\" />\n  <Protobuf Include=\"..\\Protos\\messages.proto\" Link=\"Protos\\messages.proto\" />\n</ItemGroup>\n```\n\n----------------------------------------\n\nTITLE: Implementing Message Handling in C# for AutoGen\nDESCRIPTION: This C# code snippet demonstrates how to implement a message handler for the TextMessage type in Microsoft AutoGen. It shows the class structure, dependencies, and the HandleAsync method for processing received messages.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/protobuf-message-types.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\nusing Microsoft.AutoGen.Contracts;\nusing Microsoft.AutoGen.Core;\nusing MyAgentsProtocol;\n\n[TypeSubscription(\"default\")]\npublic class Checker(\n    AgentId id,\n    IAgentRuntime runtime,\n    ) :\n        BaseAgent(id, runtime, \"MyAgent\", null),\n        IHandle<TextMessage>\n{\n    public async ValueTask HandleAsync(TextMessage item, MessageContext messageContext)\n    {\n        Console.WriteLine($\"Received message from {item.Source}: {item.Content}\");\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Text Message in AutoGen AgentChat\nDESCRIPTION: This code snippet demonstrates how to create a text message using the TextMessage class, which accepts a string content and a string source parameter.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/messages.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.messages import TextMessage\n\ntext_message = TextMessage(content=\"Hello, world!\", source=\"User\")\n```\n\n----------------------------------------\n\nTITLE: Initiating Two-Agent Conversation in C# with AutoGen\nDESCRIPTION: This code snippet demonstrates how to set up and execute a conversation between a teacher agent and a student agent using AutoGen. It includes agent configuration, message handling, and conversation termination logic.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Two-agent-chat.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nvar teacherAgent = new ChatGPTAgent(\"Teacher\", new ChatGPTAgentConfig\n{\n    Instructions = \"You are a teacher. Answer questions asked by students.\",\n});\n\nvar studentAgent = new ChatGPTAgent(\"Student\", new ChatGPTAgentConfig\n{\n    Instructions = \"You are a student. Ask the teacher to create math questions for you to solve.\",\n});\n\nteacherAgent.RegisterPostProcess((ctx, msg) =>\n{\n    if (ctx.Messages.Count >= 6)\n    {\n        return new Message(TERMINATE);\n    }\n    return msg;\n});\n\nteacherAgent.RegisterPrintMessage();\nstudentAgent.RegisterPrintMessage();\n\nawait studentAgent.InitiateChatAsync(teacherAgent);\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Schema for Addition Function\nDESCRIPTION: This JSON schema defines an 'Add' function that takes two integer parameters 'a' and 'b'. It specifies the function name, description, and parameter details including their types and whether they are required.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.SourceGenerator.Tests/ApprovalTests/FunctionExample.Add_Test.approved.txt#2025-04-22_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"name\": \"Add\",\n  \"description\": \"Add function\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"a\": {\n        \"type\": \"integer\",\n        \"description\": \"a\"\n      },\n      \"b\": {\n        \"type\": \"integer\",\n        \"description\": \"b\"\n      }\n    },\n    \"required\": [\n      \"a\",\n      \"b\"\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen .NET Packages via NuGet\nDESCRIPTION: This snippet demonstrates how to install the core and optional AutoGen .NET packages using the dotnet CLI. It includes the main packages for building workflows and distributed agent systems, as well as additional extensions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package Microsoft.AutoGen.Contracts\ndotnet add package Microsoft.AutoGen.Core\n\n# optionally - for distributed agent systems:\ndotnet add package Microsoft.AutoGen.RuntimeGateway.Grpc\ndotnet add package Microsoft.AutoGen.AgentHost\n\n# other optional packages\ndotnet add package Microsoft.AutoGen.Agents\ndotnet add package Microsoft.AutoGen.Extensions.Aspire\ndotnet add package Microsoft.AutoGen.Extensions.MEAI\ndotnet add package Microsoft.AutoGen.Extensions.SemanticKernel\n```\n\n----------------------------------------\n\nTITLE: OpenAI Model Client Configuration\nDESCRIPTION: This JSON defines the configuration for an OpenAI model client within AutoGen Studio.  It specifies the provider, component type, version, description, label, and the model to be used.  It is a declarative specification that AutoGen Studio uses to instantiate and manage the model client.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/faq.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"provider\": \"autogen_ext.models.openai.OpenAIChatCompletionClient\",\n  \"component_type\": \"model\",\n  \"version\": 1,\n  \"component_version\": 1,\n  \"description\": \"Chat completion client for OpenAI hosted models.\",\n  \"label\": \"OpenAIChatCompletionClient\",\n  \"config\": { \"model\": \"gpt-4o-mini\" }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Modal Agent in v0.4\nDESCRIPTION: Shows how to implement a multi-modal capable assistant agent that can process both text and images.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/migration-guide.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pathlib import Path\nfrom autogen_agentchat.messages import MultiModalMessage\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken, Image\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant.\",\n        model_client=model_client,\n    )\n\n    cancellation_token = CancellationToken()\n    message = MultiModalMessage(\n        content=[\"Here is an image:\", Image.from_file(Path(\"test.png\"))],\n        source=\"user\",\n    )\n    response = await assistant.on_messages([message], cancellation_token)\n    print(response)\n\n    await model_client.close()\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Complete Modifier Agent Implementation in C#\nDESCRIPTION: The final implementation of the Modifier agent, including all necessary components for handling and modifying count messages.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/tutorial.md#2025-04-22_snippet_6\n\nLANGUAGE: csharp\nCODE:\n```\nusing Microsoft.AutoGen.Contracts;\nusing Microsoft.AutoGen.Core;\n\nusing ModifyF = System.Func<int, int>;\n\nnamespace GettingStarted;\n\n[TypeSubscription(\"default\")]\npublic class Modifier(\n    AgentId id,\n    IAgentRuntime runtime,\n    ModifyF modifyFunc\n    ) :\n        BaseAgent(id, runtime, \"Modifier\", null),\n        IHandle<CountMessage>\n{\n    public async ValueTask HandleAsync(CountMessage item, MessageContext messageContext)\n    {\n        int newValue = modifyFunc(item.Content);\n        Console.WriteLine($\"\\nModifier:\\nModified {item.Content} to {newValue}\");\n\n        CountUpdate updateMessage = new CountUpdate { NewCount = newValue };\n        await this.PublishMessageAsync(updateMessage, topic: new TopicId(\"default\"));\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Tool Call Result Message in AutoGen\nDESCRIPTION: Illustrates how a tool call result message is converted to a message with the tool role, containing the result content and the tool call ID.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.OpenAI.V1.Tests/ApprovalTests/OpenAIMessageTests.BasicMessageTest.approved.txt#2025-04-22_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"OriginalMessage\": \"ToolCallResultMessage(user)\\n\\tToolCall(test, test, result)\",\n  \"ConvertedMessages\": [\n    {\n      \"Role\": \"tool\",\n      \"Content\": \"result\",\n      \"ToolCallId\": \"test\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure OpenAI Model in YAML\nDESCRIPTION: Example YAML configuration for using the gpt-4o-mini model from Azure OpenAI. This should be saved in a file named model_config.yml in the same directory as the script.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_streamlit/README.md#2025-04-22_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nprovider: autogen_ext.models.openai.AzureOpenAIChatCompletionClient\nconfig:\n  azure_deployment: \"gpt-4o-mini\"\n  model: gpt-4o-mini\n  api_version: REPLACE_WITH_MODEL_API_VERSION\n  azure_endpoint: REPLACE_WITH_MODEL_ENDPOINT\n  api_key: REPLACE_WITH_MODEL_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Traces for RoundRobinGroupChat in Python\nDESCRIPTION: This code snippet demonstrates how to add custom traces to a RoundRobinGroupChat team in AutoGen. It creates spans for logging runtime events and individual agent messages, allowing for detailed tracing of the application's execution flow.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tracing.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.base import TaskResult\nfrom autogen_agentchat.conditions import ExternalTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_core import CancellationToken\n\n\nasync def run_agents() -> None:\n    # Create an OpenAI model client.\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o-2024-08-06\")\n\n    # Create the primary agent.\n    primary_agent = AssistantAgent(\n        \"primary_agent\",\n        model_client=model_client,\n        system_message=\"You are a helpful AI assistant.\",\n    )\n\n    # Create the critic agent.\n    critic_agent = AssistantAgent(\n        \"critic_agent\",\n        model_client=model_client,\n        system_message=\"Provide constructive feedback. Respond with 'APPROVE' to when your feedbacks are addressed.\",\n    )\n\n    # Define a termination condition that stops the task if the critic approves.\n    text_termination = TextMentionTermination(\"APPROVE\")\n\n    tracer = trace.get_tracer(\"autogen-test-agentchat\")\n    with tracer.start_as_current_span(\"runtime_round_robin_events\"):\n        team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=text_termination)\n\n        response_stream = team.run_stream(task=\"Write a 2 line haiku about the fall season\")\n        async for response in response_stream:\n            async for response in response_stream:\n                if not isinstance(response, TaskResult):\n                    print(f\"\\n-- {response.source} -- : {response.to_text()}\")\n                    with tracer.start_as_current_span(f\"agent_message.{response.source}\") as message_span:\n                        message_span.set_attribute(\"agent.name\", response.source)\n                        message_span.set_attribute(\"message.content\", response.to_text())\n                        print(f\"{response.source}: {response.to_text()}\")\n\n        await model_client.close()\n\n\nawait run_agents()\n```\n\n----------------------------------------\n\nTITLE: Example Curl Request\nDESCRIPTION: This command demonstrates how to send a POST request to the `/chat/completions` endpoint using curl.  It sets the `Content-Type` header to `application/json` and includes a JSON payload with a message and conversation ID.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_streaming_handoffs_fastapi/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -N -X POST http://localhost:8501/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"message\": \"Hi, I bought a rocket-powered unicycle and it exploded.\",\n  \"conversation_id\": \"wile_e_coyote_1\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Running AutoGen Studio with Custom Host and Port\nDESCRIPTION: This bash command demonstrates how to start the AutoGen Studio UI with a specified port (8081) and host address (0.0.0.0) making it accessible from other machines on the network. It's particularly useful when running the server on a remote machine or when localhost resolution fails.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/faq.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nautogenstudio ui --port 8081 --host 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: Starting Runtime and Simulating User Session in AutoGen\nDESCRIPTION: Starts the runtime and simulates a user session by publishing a UserLogin message. The session_id is used to create topic IDs and agent IDs for all agents in the session. The runtime will stop when all agents are idle.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/handoffs.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Start the runtime.\nruntime.start()\n\n# Create a new session for the user.\nsession_id = str(uuid.uuid4())\nawait runtime.publish_message(UserLogin(), topic_id=TopicId(user_topic_type, source=session_id))\n\n# Run until completion.\nawait runtime.stop_when_idle()\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Importing MCP Workbench modules in Python\nDESCRIPTION: This snippet imports the necessary modules from autogen_agentchat and autogen_ext for using the Model Context Protocol (MCP) Workbench with an AssistantAgent. It imports AssistantAgent, TextMessage, OpenAIChatCompletionClient, McpWorkbench, and StdioServerParams.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/agents.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import McpWorkbench, StdioServerParams\n\n```\n\n----------------------------------------\n\nTITLE: Running Tasks with TeamManager in Python using AutoGen Studio\nDESCRIPTION: This snippet shows how to import the TeamManager class from autogenstudio.teammanager and use it to run a task with a specified team configuration. It demonstrates both the run and run_stream methods.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/usage.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogenstudio.teammanager import TeamManager\n\ntm = TeamManager()\nresult_stream = tm.run(task=\"What is the weather in New York?\", team_config=\"team.json\") # or tm.run_stream(..)\n```\n\n----------------------------------------\n\nTITLE: Implementing Function-Attributed Method in C#\nDESCRIPTION: Example of a C# method marked with the Function attribute that will be processed by the source generator. The method demonstrates adding two numbers with proper XML documentation.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/src/AutoGen.SourceGenerator/README.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\n// file: MyFunctions.cs\n\nusing AutoGen;\n\n// a partial class is required\n// and the class must be public\npublic partial class MyFunctions\n{\n    /// <summary>\n    /// Add two numbers.\n    /// </summary>\n    /// <param name=\"a\">The first number.</param>\n    /// <param name=\"b\">The second number.</param>\n    [Function]\n    public Task<string> AddAsync(int a, int b)\n    {\n        return Task.FromResult($\"{a} + {b} = {a + b}\");\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Noting AutoGen.Net's Extended Platform Support via Semantic Kernel\nDESCRIPTION: This code snippet is a Markdown note explaining that AutoGen.Net supports additional platforms through integration with Semantic Kernel, extending its compatibility beyond the listed platforms.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/function-comparison-page-between-python-AutoGen-and-autogen.net.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nℹ️ Note \n\n``` Other than the platforms list below, AutoGen.Net also supports all the platforms that semantic kernel supports via AutoGen.SemanticKernel as a bridge ```\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies with pip\nDESCRIPTION: Command to install the necessary Python packages including FastAPI, uvicorn, autogen-core, and autogen-ext with OpenAI support.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_streaming_response_fastapi/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"fastapi\" \"uvicorn[standard]\" \"autogen-core\" \"autogen-ext[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Message Dataclass in Python\nDESCRIPTION: Defines a basic `Message` dataclass using Python's `dataclasses` module. This class has a single string attribute `content` and serves as a simple, custom message type for agent communication within this specific example.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/tool-use-with-intervention.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass Message:\n    content: str\n```\n\n----------------------------------------\n\nTITLE: Installing Distributed System AutoGen Packages via .NET CLI\nDESCRIPTION: Commands to install Microsoft AutoGen packages for distributed systems (Core.Grpc, RuntimeGateway.Grpc, and AgentHost) using the .NET CLI. These packages enable cross-language communication between Python and .NET agents in different processes.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/installation.md#2025-04-22_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ndotnet add package Microsoft.AutoGen.Core.Grpc --version 0.4.0-dev-1\ndotnet add package Microsoft.AutoGen.RuntimeGateway.Grpc --version 0.4.0-dev-1\ndotnet add package Microsoft.AutoGen.AgentHost --version 0.4.0-dev-1\n```\n\n----------------------------------------\n\nTITLE: Installing Additional AutoGen Packages via .NET CLI\nDESCRIPTION: Commands to install additional Microsoft AutoGen packages (AgentChat, Agents, and Extensions) using the .NET CLI. These packages provide extra functionality for chat-centric agent orchestration, default agents, and extensions for related projects.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/installation.md#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ndotnet add package Microsoft.AutoGen.AgentChat --version 0.4.0-dev-1\ndotnet add package Microsoft.AutoGen.Agents --version 0.4.0-dev-1\ndotnet add package Microsoft.AutoGen.Extensions --version 0.4.0-dev-1\n```\n\n----------------------------------------\n\nTITLE: Defining Protocol Buffer Messages\nDESCRIPTION: This Protocol Buffers code snippet shows how to define a message type (TextMessage) with fields for Source and Content. It also specifies the package and C# namespace for the generated code.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/protobuf-message-types.md#2025-04-22_snippet_2\n\nLANGUAGE: protobuf\nCODE:\n```\nsyntax = \"proto3\";\n\npackage HelloAgents;\n\noption csharp_namespace = \"MyAgentsProtocol\";\n\nmessage TextMessage {\n    string Source = 1;\n    string Content = 2;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating a Python Virtual Environment on Unix\nDESCRIPTION: This snippet explains how to create and activate a Python virtual environment using venv on Unix systems. It ensures that the dependencies for AutoGen Studio are isolated from the rest of the system. No additional dependencies are required apart from Python 3.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/installation.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv .venv\nsource .venv/bin/activate\n```\n\nLANGUAGE: bash\nCODE:\n```\ndeactivate\n```\n\n----------------------------------------\n\nTITLE: Initializing Group Chat with Custom Workflow in C# using AutoGen.Core.GroupChat\nDESCRIPTION: This code snippet shows how to create a group chat using the custom workflow graph. The graph is passed to the group chat constructor, which will use it to orchestrate the conversation flow.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Use-graph-in-group-chat.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\nvar groupChat = new GroupChat(\n    new GroupChatOptions\n    {\n        Agents = new List<IAgent> { adminAgent, coderAgent, reviewerAgent, runnerAgent },\n        AdminAgent = adminAgent,\n        MaxRounds = 12,\n        Workflow = graph\n    });\n```\n\n----------------------------------------\n\nTITLE: Stopping Worker Runtimes in Python\nDESCRIPTION: This code snippet shows how to stop the worker runtimes using the stop() method. It also includes a commented-out alternative for keeping the worker running until a termination signal is received.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/distributed-agent-runtime.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait worker1.stop()\nawait worker2.stop()\n\n# To keep the worker running until a termination signal is received (e.g., SIGTERM).\n# await worker1.stop_when_signal()\n```\n\n----------------------------------------\n\nTITLE: Installing Git LFS on Different Operating Systems\nDESCRIPTION: Commands for installing Git Large File Storage (LFS) on various operating systems. Git LFS is required for properly cloning the repository as it manages image and other large files used by AutoGen Studio.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-studio/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# On Debian/Ubuntu\napt-get install git-lfs\n\n# On macOS with Homebrew\nbrew install git-lfs\n\n# On Windows with Chocolatey\nchoco install git-lfs\n```\n\n----------------------------------------\n\nTITLE: Configuring HTML Metadata using MyST Front Matter (YAML)\nDESCRIPTION: This YAML block defines the front matter for the MyST document. It sets the HTML meta description tag for the generated webpage, specifying the language as English and providing a brief description related to AgentChat examples.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/index.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmyst:\n  html_meta:\n    \"description lang=en\": |\n      Examples built using AgentChat, a high-level api for AutoGen\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen.Gemini Package with .NET CLI\nDESCRIPTION: Command to install the AutoGen.Gemini NuGet package using the .NET CLI. This package enables integration with Google's Gemini AI models.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Chat-with-google-gemini.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AutoGen.Gemini\n```\n\n----------------------------------------\n\nTITLE: Configuring XML Documentation Generation\nDESCRIPTION: XML configuration to enable structural XML documentation support in the project file, which allows the source generator to use function documentation.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Create-type-safe-function-call.md#2025-04-22_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<PropertyGroup>\n    <!-- This enables structural xml document support -->\n    <GenerateDocumentationFile>true</GenerateDocumentationFile>\n</PropertyGroup>\n```\n\n----------------------------------------\n\nTITLE: Running AutoGen Studio Web UI\nDESCRIPTION: This command runs the AutoGen Studio web UI on a specified port using the 'autogenstudio' executable. Parameters include host address, application directory, and port number, with options for database URI, automatic code reload, and upgrading database schemas.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/autogenstudio-user-guide/installation.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nautogenstudio ui --port 8081\n```\n\n----------------------------------------\n\nTITLE: Configuring Self-Message Delivery in .NET AutoGen\nDESCRIPTION: Demonstrates how to configure an agent to receive its own messages using the TopicSubscription attribute's DeliverToSelf property in the .NET implementation. By default, agents do not receive their own messages, matching Python behavior.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/differences-from-python.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\n[TopicSubscription(DeliverToSelf = true)]\n```\n\n----------------------------------------\n\nTITLE: Adding AutoGen Nightly Build Feed to Global NuGet Config\nDESCRIPTION: These bash commands add the AutoGen nightly build feed and the dotnet-tools feed to the global NuGet configuration. Replace FEED_URL with the actual feed URL provided.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Installation.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndotnet nuget add source FEED_URL --name AutoGen\n\n# dotnet-tools contains Microsoft.DotNet.Interactive.VisualStudio package, which is used by AutoGen.DotnetInteractive\ndotnet nuget add source https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet-tools/nuget/v3/index.json --name dotnet-tools\n```\n\n----------------------------------------\n\nTITLE: Using Gemini with OpenAI-Compatible API\nDESCRIPTION: Demonstrates how to use Google's Gemini models through their OpenAI-compatible API. This leverages the OpenAIChatCompletionClient to interact with Gemini's endpoint.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gemini-1.5-flash-8b\",\n    # api_key=\"GEMINI_API_KEY\",\n)\n\nresponse = await model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\nprint(response)\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Defining AutoGen Extension Module Documentation in reStructuredText\nDESCRIPTION: This reStructuredText code defines the documentation structure for the local code executors module in the AutoGen extension package. It uses the automodule directive to automatically generate API documentation including all members, undocumented members, and inheritance information.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_ext.code_executors.local.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: autogen_ext.code_executors.local\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Testing a Sender-Routed Agent in AutoGen Core\nDESCRIPTION: Demonstrates how to test a sender-routed agent with different message types and sources in AutoGen core.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/message-and-communication.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nruntime = SingleThreadedAgentRuntime()\nawait RoutedBySenderAgent.register(runtime, \"my_agent\", lambda: RoutedBySenderAgent(\"Routed by sender agent\"))\nruntime.start()\nagent_id = AgentId(\"my_agent\", \"default\")\nawait runtime.send_message(TextMessage(content=\"Hello, World!\", source=\"user1-test\"), agent_id)\nawait runtime.send_message(TextMessage(content=\"Hello, World!\", source=\"user2-test\"), agent_id)\nawait runtime.send_message(ImageMessage(url=\"https://example.com/image.jpg\", source=\"user1-test\"), agent_id)\nawait runtime.send_message(ImageMessage(url=\"https://example.com/image.jpg\", source=\"user2-test\"), agent_id)\nawait runtime.stop_when_idle()\n```\n\n----------------------------------------\n\nTITLE: Managing SingleThreadedAgentRuntime Lifecycle in Python\nDESCRIPTION: This snippet demonstrates various methods to control the SingleThreadedAgentRuntime, including starting, stopping, waiting for idle state, and closing the runtime.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/agent-and-agent-runtime.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nruntime.start()\n# ... Send messages, publish messages, etc.\nawait runtime.stop()  # This will return immediately but will not cancel\n# any in-progress message handling.\n\nruntime.start()\n# ... Send messages, publish messages, etc.\nawait runtime.stop_when_idle()  # This will block until the runtime is idle.\n\nawait runtime.close()\n```\n\n----------------------------------------\n\nTITLE: Creating Termination Criteria and Swarm Team in Python\nDESCRIPTION: Define termination conditions using HandoffTermination and TextMentionTermination, and initialize a Swarm team with the specified agents and termination criteria. This team handles task workflows until completion.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/swarm.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ntermination = HandoffTermination(target=\"user\") | TextMentionTermination(\"TERMINATE\")\nteam = Swarm([travel_agent, flights_refunder], termination_condition=termination)\n```\n\n----------------------------------------\n\nTITLE: Chatting with OpenAIChatAgent and Invoking GetWeather Function in C#\nDESCRIPTION: This final snippet demonstrates how to chat with the OpenAIChatAgent and invoke the 'GetWeather' function. It shows the actual usage of the setup created in the previous steps.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-use-function-call.md#2025-04-22_snippet_5\n\nLANGUAGE: C#\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/OpenAICodeSnippet.cs?name=chat_agent_send_function_call)]\n```\n\n----------------------------------------\n\nTITLE: Creating Type-Based Subscriptions for Single-Tenant Single Topic\nDESCRIPTION: Example showing how to create type-based subscriptions for a single-tenant application with a single topic, mapping three different agent types to the default topic type.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/core-concepts/topic-and-subscription.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Type-based Subscriptions for single-tenant, single topic scenario\nTypeSubscription(topic_type=\"default\", agent_type=\"triage_agent\")\nTypeSubscription(topic_type=\"default\", agent_type=\"coder_agent\")\nTypeSubscription(topic_type=\"default\", agent_type=\"reviewer_agent\")\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen.OpenAI Package with NuGet Reference\nDESCRIPTION: This snippet shows how to add the AutoGen.OpenAI package to a .NET project file using a PackageReference element. The version placeholder 'AUTOGEN_VERSION' should be replaced with the actual version number when implementing.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen-OpenAI-Overview.md#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<ItemGroup>\n    <PackageReference Include=\"AutoGen.OpenAI\" Version=\"AUTOGEN_VERSION\" />\n</ItemGroup>\n```\n\n----------------------------------------\n\nTITLE: Adding Core AutoGen Packages via PackageReference\nDESCRIPTION: XML snippet to add core Microsoft AutoGen packages (Contracts and Core) to a project file using PackageReference. This method is typically used in .csproj or similar project files.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/installation.md#2025-04-22_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<PackageReference Include=\"Microsoft.AutoGen.Contracts\" Version=\"0.4.0-dev.1\" />\n<PackageReference Include=\"Microsoft.AutoGen.Core\" Version=\"0.4.0-dev.1\" />\n```\n\n----------------------------------------\n\nTITLE: Defining sayHello Function\nDESCRIPTION: This snippet defines a function named 'sayHello' with no parameters. The purpose and implementation details of this function are not provided in the given context.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.SemanticKernel.Tests/ApprovalTests/KernelFunctionExtensionTests.ItCreateFunctionContractsFromPrompt.approved.txt#2025-04-22_snippet_0\n\nLANGUAGE: Unknown\nCODE:\n```\nfunction sayHello() {\n  // Function implementation not provided\n}\n```\n\n----------------------------------------\n\nTITLE: Stopping Agent Runtime - Python\nDESCRIPTION: Gracefully stops the agent runtime, ending all message processing threads and cleaning up resources. Should be called after message exchanges are complete to avoid lingering processes.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/langgraph-agent.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nawait runtime.stop()\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using LLM Usage Tracker\nDESCRIPTION: Example showing how to set up and use the LLMUsageTracker with AutoGen's logging system. Demonstrates logger configuration and accessing token usage statistics.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/llm-usage-logger.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_core import EVENT_LOGGER_NAME\n\n# Set up the logging configuration to use the custom handler\nlogger = logging.getLogger(EVENT_LOGGER_NAME)\nlogger.setLevel(logging.INFO)\nllm_usage = LLMUsageTracker()\nlogger.handlers = [llm_usage]\n\n# client.create(...)\n\nprint(llm_usage.prompt_tokens)\nprint(llm_usage.completion_tokens)\n```\n\n----------------------------------------\n\nTITLE: Creating DevTunnel for Local Development\nDESCRIPTION: Commands to create and host a persistent DevTunnel for exposing the local application to GitHub App webhooks. This tunnel is used to forward requests to the local development environment.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/dev-team/docs/github-flow-getting-started.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nTUNNEL_NAME=_name_your_tunnel_here_\ndevtunnel user login\ndevtunnel create -a $TUNNEL_NAME\ndevtunnel port create -p 5244 $TUNNEL_NAME\n```\n\nLANGUAGE: bash\nCODE:\n```\ndevtunnel host $TUNNEL_NAME\n```\n\n----------------------------------------\n\nTITLE: Passing Arguments to AutoGen Agent\nDESCRIPTION: Demonstrates how to pass custom arguments to an AutoGen agent, allowing for more flexible behavior.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/tutorial.md#2025-04-22_snippet_5\n\nLANGUAGE: csharp\nCODE:\n```\nusing ModifyF = System.Func<int, int>;\n\n// ...\n\n[TypeSubscription(\"default\")]\npublic class Modifier(\n    AgentId id,\n    IAgentRuntime runtime,\n    ModifyF modifyFunc // <-- Add this\n    ) :\n        BaseAgent(...),\n        IHandle<CountMessage>\n{\n\n    public async ValueTask HandleAsync(CountMessage item, MessageContext messageContext)\n    {\n        int newValue = modifyFunc(item.Content); // <-- use it here\n\n        // ...\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a ChatCompletionAgent in C#\nDESCRIPTION: This snippet shows how to create a ChatCompletionAgent using the OpenAIChatCompletionAgent class. It sets up the agent with specific parameters like temperature and max tokens.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.SemanticKernel/SemanticKernelChatAgent-simple-chat.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\nvar chatAgent = new OpenAIChatCompletionAgent(\n    apiKey: Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\") ?? \"\",\n    model: \"gpt-3.5-turbo\",\n    temperature: 0.7,\n    maxTokens: 800\n);\n```\n\n----------------------------------------\n\nTITLE: Managing Agent State in AutoGen C# Application\nDESCRIPTION: Code snippets demonstrating how to persist and retrieve agent state in an AutoGen C# application. It shows the use of Store and Read methods for state management.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/Hello/HelloAgentState/README.md#2025-04-22_snippet_5\n\nLANGUAGE: csharp\nCODE:\n```\nawait Store(new AgentState \n{\n    AgentId = this.AgentId,\n    TextData = entry\n}).ConfigureAwait(false);\n```\n\nLANGUAGE: csharp\nCODE:\n```\nState = await Read<AgentState>(this.AgentId).ConfigureAwait(false);\n```\n\n----------------------------------------\n\nTITLE: Installing Core AutoGen Packages via Package Manager\nDESCRIPTION: Commands to install the core Microsoft AutoGen packages (Contracts and Core) using the NuGet Package Manager in PowerShell. These packages provide the foundation for AutoGen functionality.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/installation.md#2025-04-22_snippet_1\n\nLANGUAGE: pwsh\nCODE:\n```\nPM> NuGet\\Install-Package Microsoft.AutoGen.Contracts -Version 0.4.0-dev.1\nPM> NuGet\\Install-Package Microsoft.AutoGen.Core -Version 0.4.0-dev.1\n```\n\n----------------------------------------\n\nTITLE: Publishing Messages in AutoGen Agent\nDESCRIPTION: Shows how to publish messages from an AutoGen agent, including specifying the topic for publication.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/tutorial.md#2025-04-22_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\npublic async ValueTask HandleAsync(CountMessage item, MessageContext messageContext)\n{\n    int newValue = item.Content - 1;\n    Console.WriteLine($\"\\nModifier:\\nModified {item.Content} to {newValue}\");\n\n    CountUpdate updateMessage = new CountUpdate { NewCount = newValue };\n    await this.PublishMessageAsync(updateMessage, topic: new TopicId(\"default\"));\n}\n```\n\n----------------------------------------\n\nTITLE: Continuing Team Execution After Handoff\nDESCRIPTION: Demonstrates how to continue team execution after a handoff by providing additional information to the agent. This snippet shows the continuation of the previous weather query example.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/human-in-the-loop.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait Console(lazy_agent_team.run_stream(task=\"The weather in New York is sunny.\"))\n```\n\n----------------------------------------\n\nTITLE: Explicitly Including ComponentType Class Documentation\nDESCRIPTION: This snippet uses the Sphinx 'autoclass' directive in reStructuredText to specifically include documentation for the 'ComponentType' class located within the 'autogen_core' module. This is often used when 'automodule' might not automatically pick up a specific class or when explicit control over class documentation inclusion is desired, as indicated by the preceding comment.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_core.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. For some reason the following isn't picked up by automodule above\n.. autoclass:: autogen_core.ComponentType\n```\n\n----------------------------------------\n\nTITLE: Defining Tool class and creating tools in C#\nDESCRIPTION: Example of defining a Tool class with a GetWeather method marked with FunctionAttribute, and creating an instance of the tools.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Create-agent-with-tools.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\npublic partial class Tool\n{\n    /// <summary>\n    /// Get the weather information of a city\n    /// </summary>\n    /// <param name=\"city\">The name of the city</param>\n    /// <returns>The weather information of the city</returns>\n    [Function]\n    public Task<string> GetWeather(string city)\n    {\n        return Task.FromResult($\"The weather in {city} is sunny.\");\n    }\n}\n```\n\nLANGUAGE: csharp\nCODE:\n```\nvar tools = new Tool();\n```\n\n----------------------------------------\n\nTITLE: Streaming Results from AutoGen Studio Workflow\nDESCRIPTION: This code shows how to use the run_stream method of TeamManager to get streaming results for a given task and team configuration.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-studio/notebooks/tutorial.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresult_stream =  wm.run_stream(task=\"What is the weather in New York?\", team_config=\"team.json\")\nasync for response in result_stream:\n    print(response)\n```\n\n----------------------------------------\n\nTITLE: Run FastAPI Application\nDESCRIPTION: This command starts the FastAPI application using uvicorn. It specifies the app module, host, port, and enables hot reloading for development.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_streaming_handoffs_fastapi/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuvicorn app:app --host 0.0.0.0 --port 8501 --reload\n```\n\n----------------------------------------\n\nTITLE: Importing Core Modules for Agent, Memory, and Model (AutoGen, OpenAI) in Python\nDESCRIPTION: This snippet imports the essential modules required to define an agent, user interface, memory store, and model client with AutoGen. Required dependencies include autogen_agentchat and autogen_core packages, as well as autogen_ext for connecting with OpenAI models. No functional logic is present, but these imports are necessary prerequisites for all subsequent code blocks in this file.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/memory.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\\nfrom autogen_agentchat.ui import Console\\nfrom autogen_core.memory import ListMemory, MemoryContent, MemoryMimeType\\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\\n\n```\n\n----------------------------------------\n\nTITLE: Creating MultiModal Message with Images\nDESCRIPTION: Shows how to create a MultiModalMessage object that can handle multiple modalities including images and text.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Image-chat-with-agent.md#2025-04-22_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[Create MultiModal Message](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Image_Chat_With_Agent.cs?name=Prepare_Multimodal_Input)]\n```\n\n----------------------------------------\n\nTITLE: Chatting with MistralClientAgent and Retrieving Token Usage in C#\nDESCRIPTION: This snippet illustrates how to interact with the MistralClientAgent and retrieve token usage information from the response object after the chat session.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/MistralChatAgent-count-token-usage.md#2025-04-22_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example14_MistralClientAgent_TokenCount.cs?name=chat_with_agent)]\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running AutoGen Agent Application\nDESCRIPTION: Example of configuring and running an AutoGen agent application using AgentsAppBuilder with in-process runtime.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/index.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\nAgentsAppBuilder appBuilder = new AgentsAppBuilder()\n    .UseInProcessRuntime(deliverToSelf: true)\n    .AddAgent<HelloAgent>(\"HelloAgent\");\n\nvar app = await appBuilder.BuildAsync();\n\n// start the app by publishing a message to the runtime\nawait app.PublishMessageAsync(new NewMessageReceived\n{\n    Message = \"Hello from .NET\"\n}, new TopicId(\"HelloTopic\"));\n\n// Wait for shutdown\nawait app.WaitForShutdownAsync();\n```\n\n----------------------------------------\n\nTITLE: Example Task Definition for AutoGenBench\nDESCRIPTION: An example of a task definition in JSON format for AutoGenBench, demonstrating how to specify model and prompt substitutions in different files.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\": \"two_agent_stocks_gpt4\",\n    \"template\": \"default_two_agents\",\n    \"substitutions\": {\n\t\"scenario.py\": {\n            \"__MODEL__\": \"gpt-4\",\n\t},\n\t\"prompt.txt\": {\n            \"__PROMPT__\": \"Plot and save to disk a chart of NVDA and TESLA stock price YTD.\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Including Grpc.Tools Package in .NET Project\nDESCRIPTION: This snippet shows how to include the Grpc.Tools package in a .csproj file. This package is necessary for working with Protocol Buffers in a .NET project.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/protobuf-message-types.md#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<PackageReference Include=\"Grpc.Tools\" PrivateAssets=\"All\" />\n```\n\n----------------------------------------\n\nTITLE: Implementing User Agent\nDESCRIPTION: Implementation of the final agent that receives and presents the polished marketing copy to the user.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/sequential-workflow.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@type_subscription(topic_type=user_topic_type)\nclass UserAgent(RoutedAgent):\n    def __init__(self) -> None:\n        super().__init__(\"A user agent that outputs the final copy to the user.\")\n\n    @message_handler\n    async def handle_final_copy(self, message: Message, ctx: MessageContext) -> None:\n        print(f\"\\n{'-'*80}\\n{self.id.type} received final copy:\\n{message.content}\")\n```\n\n----------------------------------------\n\nTITLE: Including .proto File in .NET Project\nDESCRIPTION: This XML snippet demonstrates how to include a .proto file in a .NET project. The .proto file defines the message types used for Protocol Buffers serialization.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/protobuf-message-types.md#2025-04-22_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<ItemGroup>\n  <Protobuf Include=\"messages.proto\" GrpcServices=\"Client;Server\" Link=\"messages.proto\" />\n</ItemGroup>\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging for LLM Model Calls in AutoGen\nDESCRIPTION: Sets up Python logging to capture LLM calls made through AutoGen. This snippet configures the logging system to display INFO level logs for model interactions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nfrom autogen_core import EVENT_LOGGER_NAME\n\nlogging.basicConfig(level=logging.WARNING)\nlogger = logging.getLogger(EVENT_LOGGER_NAME)\nlogger.addHandler(logging.StreamHandler())\nlogger.setLevel(logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Enabling Trace Logging in Python\nDESCRIPTION: Configuration code to enable trace logging with DEBUG level output using Python's logging module. Sets up a basic stream handler for console output.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/logging.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nfrom autogen_core import TRACE_LOGGER_NAME\n\nlogging.basicConfig(level=logging.WARNING)\nlogger = logging.getLogger(TRACE_LOGGER_NAME)\nlogger.addHandler(logging.StreamHandler())\nlogger.setLevel(logging.DEBUG)\n```\n\n----------------------------------------\n\nTITLE: AutoGen Logging Configuration\nDESCRIPTION: Example appsettings.json configuration for setting up logging in an AutoGen application using Microsoft.Extensions.Logging.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/index.md#2025-04-22_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Logging\": {\n    \"LogLevel\": {\n      \"Default\": \"Warning\",\n      \"Microsoft.Hosting.Lifetime\": \"Information\",\n      \"Microsoft.AspNetCore\": \"Information\",\n      \"Microsoft\": \"Information\",\n      \"Microsoft.Orleans\": \"Warning\",\n      \"Orleans.Runtime\": \"Error\",\n      \"Grpc\": \"Information\"\n    }\n  },\n  \"AllowedHosts\": \"*\",\n  \"Kestrel\": {\n    \"EndpointDefaults\": {\n      \"Protocols\": \"Http2\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up AutoGen Application in C#\nDESCRIPTION: Configures and builds the AutoGen application, including registering services, agents, and setting up the runtime.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/tutorial.md#2025-04-22_snippet_9\n\nLANGUAGE: csharp\nCODE:\n```\nvar builder = Host.CreateDefaultBuilder(args);\n\nbuilder.UseAutoGen(b => b.UseInProcess())\n    .ConfigureServices(services =>\n    {\n        services.AddSingleton<ModifyF>(Modify);\n        services.AddSingleton<CheckF>(Check);\n        services.AddTransient<Modifier>();\n        services.AddTransient<Checker>();\n    });\n\nusing var host = builder.Build();\nawait host.StartAsync();\n```\n\n----------------------------------------\n\nTITLE: Configuring AutoGen Logging\nDESCRIPTION: Sets up basic logging configuration for AutoGen framework, enabling debug-level logging for the autogen_core module while keeping other modules at warning level.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/reflection.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogging.basicConfig(level=logging.WARNING)\nlogging.getLogger(\"autogen_core\").setLevel(logging.DEBUG)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for AutoGen Agents\nDESCRIPTION: Imports required modules and classes for implementing AutoGen agents including messaging, routing and LLM components.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/reflection.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport re\nimport uuid\nfrom typing import Dict, List, Union\n\nfrom autogen_core import MessageContext, RoutedAgent, TopicId, default_subscription, message_handler\nfrom autogen_core.models import (\n    AssistantMessage,\n    ChatCompletionClient,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Dotnet Interactive Composite Kernel in C#\nDESCRIPTION: This C# code snippet demonstrates how to create an in-process dotnet-interactive composite kernel with C# and F# kernels using DotnetInteractiveKernelBuilder.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Run-dotnet-code.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/RunCodeSnippetCodeSnippet.cs?name=code_snippet_1_1)]\n```\n\n----------------------------------------\n\nTITLE: Creating Type-Based Subscription for Multi-Tenant Scenario\nDESCRIPTION: Demonstrates how to create a type-based subscription for handling GitHub issues in a multi-tenant scenario, where each issue gets its own agent instance.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/core-concepts/topic-and-subscription.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nTypeSubscription(topic_type=\"github_issues\", agent_type=\"triage_agent\")\n```\n\n----------------------------------------\n\nTITLE: Converting User Text Message in AutoGen\nDESCRIPTION: Shows the conversion of a user text message with a name attribute into the standardized message format with role, content, and name properties.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.OpenAI.V1.Tests/ApprovalTests/OpenAIMessageTests.BasicMessageTest.approved.txt#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"OriginalMessage\": \"TextMessage(user, Hello, user)\",\n  \"ConvertedMessages\": [\n    {\n      \"Role\": \"user\",\n      \"Content\": \"Hello\",\n      \"Name\": \"user\",\n      \"MultiModaItem\": null\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Message Types in C# for AutoGen Agents\nDESCRIPTION: Defines two message types, CountMessage and CountUpdate, used for communication between agents in the AutoGen system.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/tutorial.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\npublic class CountMessage\n{\n    public int Content { get; set; }\n}\n\npublic class CountUpdate\n{\n    public int NewCount { get; set; }\n}\n```\n\n----------------------------------------\n\nTITLE: Azure Environment Configuration\nDESCRIPTION: Required environment configuration for using Azure with AutoGenBench.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/README.md#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"CHAT_COMPLETION_KWARGS_JSON\": \"{}\",\n    \"CHAT_COMPLETION_PROVIDER\": \"azure\"\n}\n```\n\n----------------------------------------\n\nTITLE: Consuming WeatherReport Function\nDESCRIPTION: Example of how to generate OpenAI function definition from FunctionContract and use the function call wrapper.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Create-type-safe-function-call.md#2025-04-22_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/TypeSafeFunctionCallCodeSnippet.cs?name=weather_report_consume)]\n```\n\n----------------------------------------\n\nTITLE: Defining Modification and Check Functions in C#\nDESCRIPTION: Defines two functions for modifying the count and checking for completion, to be used by the Modifier and Checker agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/tutorial.md#2025-04-22_snippet_8\n\nLANGUAGE: csharp\nCODE:\n```\nstatic int Modify(int count) => count - 1;\nstatic bool Check(int count) => count == 1;\n```\n\n----------------------------------------\n\nTITLE: Running the Chess Game Example\nDESCRIPTION: Command to execute the main Python script that starts the chess game between the two AI agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_chess_game/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Implementing Message Handling in AutoGen Agent\nDESCRIPTION: Shows how to implement the IHandle<T> interface to handle specific message types in an AutoGen agent.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/tutorial.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\npublic class Modifier(\n    // ...\n    ) :\n        BaseAgent(...),\n        IHandle<CountMessage>\n{\n\n    public async ValueTask HandleAsync(CountMessage item, MessageContext messageContext)\n    {\n        // ...\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Inspecting Assistant Agent Model Context Messages in Python\nDESCRIPTION: This snippet asynchronously retrieves and prints all messages in the AssistantAgent instance's model_context, typically to verify that retrieved memory entries have been incorporated as expected. It assumes that the agent and model context already exist, and provides a way to debug or analyze the current memory-influenced dialog history for the agent.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/memory.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nawait assistant_agent._model_context.get_messages()\\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up Git LFS\nDESCRIPTION: Commands for setting up Git LFS after installation and fetching/checking out large files in an existing repository. This ensures all image files are properly downloaded.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-studio/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install\n```\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install\ngit lfs fetch --all\ngit lfs checkout  # downloads all missing image files to the working directory\n```\n\n----------------------------------------\n\nTITLE: Generating Response from Image Input\nDESCRIPTION: Implementation of the SendAsync method to generate responses from image-based messages using the chat agent.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Image-chat-with-agent.md#2025-04-22_snippet_5\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[Generate Response](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Image_Chat_With_Agent.cs?name=Chat_With_Agent)]\n```\n\n----------------------------------------\n\nTITLE: Defining Message Protocol Classes\nDESCRIPTION: Implementation of message protocol classes (UserLogin, UserTask, AgentResponse) used for agent communication in the event-driven pub-sub system.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/handoffs.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass UserLogin(BaseModel):\n    pass\n\n\nclass UserTask(BaseModel):\n    context: List[LLMMessage]\n\n\nclass AgentResponse(BaseModel):\n    reply_to_topic_type: str\n    context: List[LLMMessage]\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for AutoGen and Chainlit Application\nDESCRIPTION: Command to install the required packages for building a chat interface with AutoGen and Chainlit. Includes the core AutoGen packages and OpenAI extension.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_chainlit/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -U chainlit autogen-agentchat autogen-ext[openai] pyyaml\n```\n\n----------------------------------------\n\nTITLE: Referencing AutoGen for .NET in Markdown\nDESCRIPTION: This snippet demonstrates how to reference the AutoGen for .NET SDK using inline code formatting in Markdown. It's used to emphasize the name of the SDK within the text.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/nuget/NUGET.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`AutoGen for .NET`\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic AutoGen Agent in C#\nDESCRIPTION: Demonstrates the basic structure of an AutoGen agent by inheriting from BaseAgent and implementing necessary interfaces.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/tutorial.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nusing Microsoft.AutoGen.Contracts;\nusing Microsoft.AutoGen.Core;\n\npublic class Modifier(\n    AgentId id,\n    IAgentRuntime runtime,\n    ) :\n        BaseAgent(id, runtime, \"MyAgent\", null),\n{\n}\n```\n\n----------------------------------------\n\nTITLE: Converting System Text Message in AutoGen\nDESCRIPTION: Demonstrates how a system text message is converted to a standardized message format with role and content properties.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.OpenAI.V1.Tests/ApprovalTests/OpenAIMessageTests.BasicMessageTest.approved.txt#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"OriginalMessage\": \"TextMessage(system, You are a helpful AI assistant, )\",\n  \"ConvertedMessages\": [\n    {\n      \"Name\": null,\n      \"Role\": \"system\",\n      \"Content\": \"You are a helpful AI assistant\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Initiating AutoGen Workflow in C#\nDESCRIPTION: Starts the AutoGen workflow by publishing an initial CountMessage and waiting for the application to complete.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/tutorial.md#2025-04-22_snippet_10\n\nLANGUAGE: csharp\nCODE:\n```\nvar runtime = host.Services.GetRequiredService<IAgentRuntime>();\nvar message = new CountMessage { Content = 10 };\nawait runtime.PublishMessageAsync(message, new TopicId(\"default\"));\n\nawait host.WaitForShutdownAsync();\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGen Experimental Task-Centric Memory Module\nDESCRIPTION: This snippet demonstrates how to import the experimental task-centric memory module in AutoGen. The module is part of the experimental features in AutoGen's extension package.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_ext.experimental.task_centric_memory.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. automodule:: autogen_ext.experimental.task_centric_memory\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Enabling Structural XML Document Support\nDESCRIPTION: This XML snippet demonstrates how to enable structural XML document support in the project file.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Function-call-with-ollama-and-litellm.md#2025-04-22_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<PropertyGroup>\n    <!-- This enables structural xml document support -->\n    <GenerateDocumentationFile>true</GenerateDocumentationFile>\n</PropertyGroup>\n```\n\n----------------------------------------\n\nTITLE: Downloading and Processing Seattle Wage Data\nDESCRIPTION: Fetching and saving Seattle wage data for analysis by the assistant.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/openai-assistant-agent.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nresponse = requests.get(\"https://data.seattle.gov/resource/2khk-5ukd.csv\")\nwith open(\"seattle_city_wages.csv\", \"wb\") as file:\n    file.write(response.content)\n```\n\n----------------------------------------\n\nTITLE: Serving Documentation Locally\nDESCRIPTION: To view the built documentation in a local web server, this command uses Sphinx to serve the documentation. It launches a local server from the same directory where the documentation is built, allowing for interactive viewing.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoe --directory ./packages/autogen-core/ docs-serve\n\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Agent in C#\nDESCRIPTION: Example of creating a custom agent by inheriting from BaseAgent and implementing message handling functionality.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/index.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\npublic class MyAgent : BaseAgent, IHandle<MyMessage>\n{\n    // ...\n    public async ValueTask HandleAsync(MyMessage item, MessageContext context)\n    {\n        // ...logic here...\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Required Namespaces for WeatherReport\nDESCRIPTION: C# using statements required for implementing the WeatherReport functionality with AutoGen.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Create-type-safe-function-call.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/TypeSafeFunctionCallCodeSnippet.cs?name=weather_report_using_statement)]\n```\n\n----------------------------------------\n\nTITLE: Installing Azure OpenAI Extensions with AAD Authentication\nDESCRIPTION: Command to install Azure-specific extensions for AutoGen when using Azure OpenAI with Azure Active Directory (AAD) authentication.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/installation.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install \"autogen-ext[azure]\"\n```\n\n----------------------------------------\n\nTITLE: Agent Chat Using SendAsync Extension\nDESCRIPTION: Shows how to use the SendAsync extension method for simplified agent communication. This is a shortcut method for sending messages to agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Agent-overview.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nvar reply = await agent.SendAsync(\"Hello Agent!\");\n```\n\n----------------------------------------\n\nTITLE: Listing Available Jupyter Kernels\nDESCRIPTION: This bash command lists all available Jupyter kernels, which is useful for confirming the correct installation of ipykernel.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Run-dotnet-code.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\njupyter kernelspec list\n```\n\n----------------------------------------\n\nTITLE: Configuring Project File for AutoGen.SourceGenerator\nDESCRIPTION: XML configuration required in the project file to enable structural XML documentation and add the AutoGen.SourceGenerator package reference.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/src/AutoGen.SourceGenerator/README.md#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<PropertyGroup>\n    <!-- This enables structural xml document support -->\n    <GenerateDocumentationFile>true</GenerateDocumentationFile>\n</PropertyGroup>\n```\n\nLANGUAGE: xml\nCODE:\n```\n<ItemGroup>\n    <PackageReference Include=\"AutoGen.SourceGenerator\" />\n</ItemGroup>\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Topic Types in Python\nDESCRIPTION: Declaration of topic types for different agents in the system for message routing.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/handoffs.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsales_agent_topic_type = \"SalesAgent\"\nissues_and_repairs_agent_topic_type = \"IssuesAndRepairsAgent\"\ntriage_agent_topic_type = \"TriageAgent\"\nhuman_agent_topic_type = \"HumanAgent\"\nuser_topic_type = \"User\"\n```\n\n----------------------------------------\n\nTITLE: Starting AutoGen Backend in C#\nDESCRIPTION: Code example showing how to start the AutoGen backend within a C# application.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/index.md#2025-04-22_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\nusing Microsoft.AutoGen.RuntimeGateway;\nusing Microsoft.AutoGen.AgentHost;\nvar autogenBackend = await Microsoft.AutoGen.RuntimeGateway.Grpc.Host.StartAsync(local: false, useGrpc: true).ConfigureAwait(false);\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for AutoGen\nDESCRIPTION: This snippet lists the external Python libraries and internal package paths required for the AutoGen project. It includes tiktoken and pyyaml as external dependencies, and specifies paths for autogen-core, autogen-ext with OpenAI and magentic-one extensions, and autogen-agentchat.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/GAIA/Templates/ParallelAgents/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntiktoken\npyyaml\n/autogen_python/packages/autogen-core\n/autogen_python/packages/autogen-ext[openai,magentic-one]\n/autogen_python/packages/autogen-agentchat\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen.OpenAI Package\nDESCRIPTION: Command to install the required AutoGen.OpenAI NuGet package\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-connect-to-third-party-api.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AutoGen.OpenAI --version AUTOGEN_VERSION\n```\n\n----------------------------------------\n\nTITLE: Playwright Installation Command\nDESCRIPTION: This command uses the npx package runner to install the necessary browser dependencies for Playwright, specifically Chrome.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/workbench.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# npx playwright install chrome\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAI Chat Agent Instance\nDESCRIPTION: Code to instantiate an OpenAI Chat Agent with appropriate configuration for image processing.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Image-chat-with-agent.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[Create an OpenAIChatAgent](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Image_Chat_With_Agent.cs?name=Create_Agent)]\n```\n\n----------------------------------------\n\nTITLE: AutoGen Agent Host Tool Installation\nDESCRIPTION: Commands for packaging and installing the AutoGen agent host as a .NET tool.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/index.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndotnet pack --no-build --configuration Release --output './output/release' -bl\\n\ndotnet tool install --add-source ./output/release Microsoft.AutoGen.AgentHost\n```\n\n----------------------------------------\n\nTITLE: Example Task Definition\nDESCRIPTION: Defines a sample mathematical task to demonstrate the Mixture of Agents pattern.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/design-patterns/mixture-of-agents.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntask = (\n    \"I have 432 cookies, and divide them 3:4:2 between Alice, Bob, and Charlie. How many cookies does each person get?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Updating UV Package Manager Version for AutoGen Development\nDESCRIPTION: This command updates the uv package manager to a specific version (0.5.18) to ensure compatibility with CI environment.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/README.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nuv self update 0.5.18\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen Core Packages using .NET CLI\nDESCRIPTION: Basic package installation commands for setting up AutoGen Core dependencies in a .NET project.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package Microsoft.AutoGen.Contracts\ndotnet add package Microsoft.AutoGen.Core\n```\n\n----------------------------------------\n\nTITLE: Initializing GAIA Tasks\nDESCRIPTION: Command to initialize the GAIA benchmark tasks. This script downloads GAIA from Huggingface and requires authentication.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/GAIA/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython Scripts/init_tasks.py\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen.Mistral Package with .NET CLI\nDESCRIPTION: Command to add the AutoGen.Mistral package to a .NET project using the dotnet CLI.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen-Mistral-Overview.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package AutoGen.Mistral\n```\n\n----------------------------------------\n\nTITLE: Installing Distributed Runtime Packages\nDESCRIPTION: Commands for installing the distributed runtime packages for cloud-based agent deployment.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/index.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package Microsoft.AutoGen.Core.Grpc\ndotnet add package Microsoft.AutoGen.RuntimeGateway\ndotnet add package Microsoft.AutoGen.AgentHost\n```\n\n----------------------------------------\n\nTITLE: Adding Subscription to AutoGen Agent\nDESCRIPTION: Demonstrates how to add a subscription to an AutoGen agent using the TypeSubscription attribute.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/core/tutorial.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\n[TypeSubscription(\"default\")]\npublic class Modifier(\n    // ...\n```\n\n----------------------------------------\n\nTITLE: Sending Image Request to Gemini\nDESCRIPTION: Implementation for sending an image request to the Gemini model and handling the response.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.Gemini/Image-chat-with-gemini.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\nvar response = await gemini.SendMessageAsync(\"What's in this image?\", imageContent);\n```\n\n----------------------------------------\n\nTITLE: Python Package Path and Dependencies Configuration\nDESCRIPTION: Lists the required Python packages and directory structure for Microsoft AutoGen project, including the core package, extensions with OpenAI and magnetic-one plugins, and agent chat functionality.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/GAIA/Templates/MagenticOne/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ntiktoken\npyyaml\n/autogen_python/packages/autogen-core\n/autogen_python/packages/autogen-ext[openai,magentic-one]\n/autogen_python/packages/autogen-agentchat\n```\n\n----------------------------------------\n\nTITLE: OpenAI GPT-4 Model Configuration\nDESCRIPTION: YAML configuration for using the GPT-4 model from OpenAI with AutoGen.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_chess_game/README.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprovider: autogen_ext.models.openai.OpenAIChatCompletionClient\nconfig:\n  model: gpt-4o\n  api_key: replace with your API key or skip it if you have environment variable OPENAI_API_KEY set\n```\n\n----------------------------------------\n\nTITLE: Playwright MCP Server Start Command\nDESCRIPTION: This command starts the Playwright MCP server on port 8931 using the npx package runner.  The @playwright/mcp package is executed.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/components/workbench.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# npx @playwright/mcp@latest --port 8931\n```\n\n----------------------------------------\n\nTITLE: Installing Ollama Extension for AutoGen\nDESCRIPTION: Command to install the Ollama extension for AutoGen to use local models through the Ollama model server.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"autogen-ext[ollama]\"\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Extension for AutoGen\nDESCRIPTION: Command to install the OpenAI extension for AutoGen, which enables using OpenAI model clients.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install \"autogen-ext[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Azure AI Extension\nDESCRIPTION: Command to install the Azure AI extension for using Azure AI Foundry models with AutoGen.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install \"autogen-ext[azure]\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Code Runner Agent in C#\nDESCRIPTION: Implementation of the runner agent that executes the validated code and outputs results.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Group-chat.md#2025-04-22_snippet_5\n\nLANGUAGE: csharp\nCODE:\n```\ncreate_runner\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Exporters\nDESCRIPTION: Commands to install either HTTP or gRPC protocol exporters for OpenTelemetry\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/telemetry.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Pick one of the following\n\npip install opentelemetry-exporter-otlp-proto-http\npip install opentelemetry-exporter-otlp-proto-grpc\n```\n\n----------------------------------------\n\nTITLE: Running FastAPI Server for Single-Agent Chat\nDESCRIPTION: Starts the FastAPI server for single-agent chat using the app_agent.py script.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_fastapi/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython app_agent.py\n```\n\n----------------------------------------\n\nTITLE: Registering Middleware Functions to Original Agent in C#\nDESCRIPTION: Shows how to directly register middleware functions to an existing agent to modify its behavior.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Middleware-overview.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\noriginalAgent.Use(async (context, next) =>\n{\n    // Do something before calling next agent\n    var reply = await next(context);\n    // Do something after calling next agent\n    return reply;\n});\n```\n\n----------------------------------------\n\nTITLE: Ollama DeepSeek Model Configuration\nDESCRIPTION: YAML configuration for using a locally hosted DeepSeek-R1:8b model through Ollama's compatibility endpoint.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_chess_game/README.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nprovider: autogen_ext.models.openai.OpenAIChatCompletionClient\nconfig:\n  model: deepseek-r1:8b\n  base_url: http://localhost:11434/v1\n  api_key: ollama\n  model_info:\n    function_calling: false\n    json_output: false\n    vision: false\n    family: r1\n```\n\n----------------------------------------\n\nTITLE: Defining Message Data Models (Python)\nDESCRIPTION: This code defines two Pydantic models, `Resource` and `Message`, for structuring data exchanged between agents. `Resource` represents a piece of content with a node ID and an optional score. `Message` encapsulates a content string and an optional list of `Resource` objects as sources.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/llamaindex-agent.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Resource(BaseModel):\n    content: str\n    node_id: str\n    score: Optional[float] = None\n\n\nclass Message(BaseModel):\n    content: str\n    sources: Optional[List[Resource]] = None\n```\n\n----------------------------------------\n\nTITLE: Defining Weather API Functions in JSON\nDESCRIPTION: This JSON structure defines four weather-related API functions with their parameters and requirements. It includes both synchronous and asynchronous methods, some of which are static. Each function has a name, description, and a set of parameters with type definitions.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.Tests/Function/ApprovalTests/FunctionTests.CreateGetWeatherFunctionFromAIFunctionFactoryAsync.approved.txt#2025-04-22_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"Kind\": 0,\n    \"FunctionName\": \"GetWeather\",\n    \"FunctionDescription\": \"get weather\",\n    \"FunctionParameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"city\": {\n          \"type\": \"string\"\n        },\n        \"date\": {\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"city\"\n      ]\n    }\n  },\n  {\n    \"Kind\": 0,\n    \"FunctionName\": \"GetWeatherStatic\",\n    \"FunctionDescription\": \"get weather from static method\",\n    \"FunctionParameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"city\": {\n          \"type\": \"string\"\n        },\n        \"date\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"string\"\n          }\n        }\n      },\n      \"required\": [\n        \"city\",\n        \"date\"\n      ]\n    }\n  },\n  {\n    \"Kind\": 0,\n    \"FunctionName\": \"GetWeather\",\n    \"FunctionDescription\": \"get weather from async method\",\n    \"FunctionParameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"city\": {\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"city\"\n      ]\n    }\n  },\n  {\n    \"Kind\": 0,\n    \"FunctionName\": \"GetWeatherAsyncStatic\",\n    \"FunctionDescription\": \"get weather from async static method\",\n    \"FunctionParameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"city\": {\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"city\"\n      ]\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Documenting Docker Code Executor Module in ReStructuredText\nDESCRIPTION: ReStructuredText configuration for generating automated documentation for the Docker code executor module in the autogen_ext package. The directive includes all module members, undocumented members, and shows inheritance relationships.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/reference/python/autogen_ext.code_executors.docker.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: autogen_ext.code_executors.docker\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGen with Task-Centric Memory Extension\nDESCRIPTION: Command to install AutoGen and its Task-Centric Memory extension packages via pip. This is required to use the experimental memory features in your projects.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-ext/src/autogen_ext/experimental/task_centric_memory/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"autogen-agentchat\" \"autogen-ext[openai]\" \"autogen-ext[task-centric-memory]\"\n```\n\n----------------------------------------\n\nTITLE: Custom HTTP Client Handler Implementation\nDESCRIPTION: A custom HTTP client handler that redirects API requests to the local Ollama server running on port 11434. This class overrides SendAsync to modify request behavior.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-connect-to-third-party-api.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\npublic class CustomHttpClientHandler : HttpClientHandler\n{\n    protected override async Task<HttpResponseMessage> SendAsync(HttpRequestMessage request, CancellationToken cancellationToken)\n    {\n        // Redirect request to local ollama server\n        request.RequestUri = new Uri($\"http://localhost:11434{request.RequestUri!.AbsolutePath}\");\n\n        return await base.SendAsync(request, cancellationToken);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Identity Package\nDESCRIPTION: Installs the azure.identity package using pip, which is required for authentication.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/extensions-user-guide/azure-container-code-executor.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# pip install azure.identity\n```\n\n----------------------------------------\n\nTITLE: Citing Magentic-One Research Paper in BibTeX Format\nDESCRIPTION: This BibTeX entry provides the citation information for the Magentic-One research paper. It includes details such as the title, authors, year, arXiv identifier, and URL for referencing the paper in academic works.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/magentic-one.md#2025-04-22_snippet_4\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{fourney2024magenticonegeneralistmultiagentsolving,\n      title={Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks},\n      author={Adam Fourney and Gagan Bansal and Hussein Mozannar and Cheng Tan and Eduardo Salinas and Erkang and Zhu and Friederike Niedtner and Grace Proebsting and Griffin Bassman and Jack Gerrits and Jacob Alber and Peter Chang and Ricky Loynd and Robert West and Victor Dibia and Ahmed Awadallah and Ece Kamar and Rafah Hosn and Saleema Amershi},\n      year={2024},\n      eprint={2411.04468},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2411.04468},\n}\n```\n\n----------------------------------------\n\nTITLE: Creating MistralClientAgent in C#\nDESCRIPTION: This snippet demonstrates how to create a MistralClientAgent instance for use with the token counting middleware.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/MistralChatAgent-count-token-usage.md#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example14_MistralClientAgent_TokenCount.cs?name=create_mistral_client_agent)]\n```\n\n----------------------------------------\n\nTITLE: Using Max Turns for Interactive Session with RoundRobinGroupChat\nDESCRIPTION: Example of using max_turns parameter to limit a RoundRobinGroupChat to a single turn per execution. This creates an interactive loop where the user can provide feedback between runs for a poetry generation task.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/tutorial/human-in-the-loop.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create the agents.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\nassistant = AssistantAgent(\"assistant\", model_client=model_client)\n\n# Create the team setting a maximum number of turns to 1.\nteam = RoundRobinGroupChat([assistant], max_turns=1)\n\ntask = \"Write a 4-line poem about the ocean.\"\nwhile True:\n    # Run the conversation and stream to the console.\n    stream = team.run_stream(task=task)\n    # Use asyncio.run(...) when running in a script.\n    await Console(stream)\n    # Get the user response.\n    task = input(\"Enter your feedback (type 'exit' to leave): \")\n    if task.lower().strip() == \"exit\":\n        break\nawait model_client.close()\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Message Dataclass (Python)\nDESCRIPTION: Defines a basic Python dataclass named `Message`. It contains a single attribute `content` of type string. This class is used as the structure for exchanging messages between the defined AutoGen agents in this example.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/local-llms-ollama-litellm.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass Message:\n    content: str\n```\n\n----------------------------------------\n\nTITLE: Function Call Object Example in JSON\nDESCRIPTION: Example of a function call object that demonstrates how to invoke the GetWeather function for Seattle. Shows the structure of the function call with name and arguments.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Function-call-overview.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"GetWeather\",\n    \"arguments\": {\n        \"city\": \"Seattle\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Navigation with Sphinx Toctree in Markdown\nDESCRIPTION: This snippet demonstrates the use of a Sphinx toctree directive within a Markdown document to generate a navigable menu of recipe topics. It specifies a maximum depth for nested items and lists multiple documentation targets, each corresponding to a core API usage recipe. Users must ensure Sphinx with the MyST parser extension is installed for toctree support within Markdown files.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\\n:maxdepth: 1\\n\\nazure-openai-with-aad-auth\\ntermination-with-intervention\\ntool-use-with-intervention\\nextracting-results-with-an-agent\\nopenai-assistant-agent\\nlanggraph-agent\\nllamaindex-agent\\nlocal-llms-ollama-litellm\\ninstrumenting\\ntopic-subscription-scenarios\\nstructured-output-agent\\nllm-usage-logger\\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Settings\nDESCRIPTION: Setting up logging configuration for debugging the agent runtime.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/openai-assistant-agent.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogging.basicConfig(level=logging.WARNING)\nlogging.getLogger(\"autogen_core\").setLevel(logging.DEBUG)\n```\n\n----------------------------------------\n\nTITLE: Defining WeatherReport Function in C#\nDESCRIPTION: This C# snippet defines a WeatherReport function with AutoGen.Core.FunctionAttribute for use in AutoGen agents.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Function-call-with-ollama-and-litellm.md#2025-04-22_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\n[AutoGen.Core.Function]\npublic async Task<string> GetWeatherAsync(string city)\n{\n    return $\"The weather in {city} is 72 degrees and sunny.\";\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning and Running AutoGen .NET Sample\nDESCRIPTION: Commands to clone the AutoGen repository and run the Hello World sample using .NET CLI.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/samples/Hello/HelloAgent/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository\ngh repo clone microsoft/autogen\ncd dotnet/samples/Hello\ndotnet run\n```\n\n----------------------------------------\n\nTITLE: Install Google Gemini SDK\nDESCRIPTION: Installs the Google Gemini SDK using pip. This is a prerequisite for using the GeminiAssistantAgent.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/custom-agents.ipynb#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install google-genai\n```\n\n----------------------------------------\n\nTITLE: Building and Serving DocFX Documentation\nDESCRIPTION: Commands to restore .NET tools and build/serve the documentation website using DocFX. The website will be available at localhost:8080 after successful execution.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet tool restore\ndotnet tool run docfx website/docfx.json --serve\n```\n\n----------------------------------------\n\nTITLE: Defining Task Schema in JSON for AutoGenBench\nDESCRIPTION: Schema for defining tasks in JSONL files for AutoGenBench. Each task is represented as a JSON object with an id, template, and substitutions for file modifications.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"id\": string,\n   \"template\": dirname,\n   \"substitutions\" {\n       \"filename1\": {\n       \t   \"find_string1_1\": replace_string1_1,\n           \"find_string1_2\": replace_string1_2,\n           ...\n           \"find_string1_M\": replace_string1_N\n       }\n       \"filename2\": {\n       \t   \"find_string2_1\": replace_string2_1,\n           \"find_string2_2\": replace_string2_2,\n           ...\n           \"find_string2_N\": replace_string2_N\n       }\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating .NET Web Application with AutoGen Packages\nDESCRIPTION: Commands to create a new .NET web application and install required AutoGen packages\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/tutorial/Use-AutoGen.Net-agent-as-model-in-AG-Studio.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet new web\ndotnet add package AutoGen\ndotnet add package AutoGen.WebAPI\n```\n\n----------------------------------------\n\nTITLE: Import Required Namespaces for OpenAI Chat Integration\nDESCRIPTION: Required namespace imports for working with OpenAI chat functionality and message types in AutoGen.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/OpenAIChatAgent-support-more-messages.md#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nusing AutoGen.Core;\nusing AutoGen.OpenAI;\n```\n\n----------------------------------------\n\nTITLE: Installing Ollama and Pulling Dolphincoder Model\nDESCRIPTION: This snippet shows how to pull the dolphincoder:latest model using Ollama CLI.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Function-call-with-ollama-and-litellm.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nollama pull dolphincoder:latest\n```\n\n----------------------------------------\n\nTITLE: Creating SemanticKernelChatCompletionAgent with Message Content Connector in C#\nDESCRIPTION: This code creates a SemanticKernelChatCompletionAgent and registers it with a SemanticKernelChatMessageContentConnector. It demonstrates how to set up the agent to handle various message types in AutoGen.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/AutoGen.SemanticKernel/SemanticKernelChatAgent-simple-chat.md#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\nvar skChatAgent = new SemanticKernelChatCompletionAgent(\n    kernel: kernel,\n    chatCompletionServiceId: \"gpt-3.5-turbo\",\n    name: \"AI Assistant\",\n    systemMessage: \"You are an AI assistant that helps people with their questions.\");\n\nvar connector = new SemanticKernelChatMessageContentConnector();\nskChatAgent.RegisterConnector(connector);\n```\n\n----------------------------------------\n\nTITLE: Running the Agent Team Sample Application\nDESCRIPTION: Command to launch the agent team chat interface using Chainlit. This sample demonstrates interaction with a RoundRobinGroupChat containing a helpful agent and a critic agent.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_chainlit/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nchainlit run app_team.py -h\n```\n\n----------------------------------------\n\nTITLE: Serializing and Deserializing UserProxyAgent in AutoGen\nDESCRIPTION: This snippet demonstrates how to serialize a UserProxyAgent to a configuration dictionary and then deserialize it back into a new UserProxyAgent object.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/serialize-components.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nuser_proxy_config = user_proxy.dump_component()  # dump component\nprint(user_proxy_config.model_dump_json())\nup_new = user_proxy.load_component(user_proxy_config)  # load component\n```\n\n----------------------------------------\n\nTITLE: Building DocFX Documentation Website with .NET\nDESCRIPTION: Commands to restore .NET tools and run DocFX to build and serve the documentation website. Requires .NET 8.0 or later. The website will be served at http://localhost:8080.\nSOURCE: https://github.com/microsoft/autogen/blob/main/docs/dotnet/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndotnet tool restore\ndotnet tool run docfx ../docs/dotnet/docfx.json --serve\n```\n\n----------------------------------------\n\nTITLE: Converting Image Message in AutoGen\nDESCRIPTION: Shows how an image message with a URL is converted to a message with role, name, and a multimodal item array containing the image data.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/AutoGen.OpenAI.V1.Tests/ApprovalTests/OpenAIMessageTests.BasicMessageTest.approved.txt#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"OriginalMessage\": \"ImageMessage(user, https://example.com/image.png, user)\",\n  \"ConvertedMessages\": [\n    {\n      \"Role\": \"user\",\n      \"Content\": null,\n      \"Name\": \"user\",\n      \"MultiModaItem\": [\n        {\n          \"Type\": \"Image\",\n          \"ImageUrl\": {\n            \"Url\": \"https://example.com/image.png\",\n            \"Detail\": null\n          }\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Uploading and Verifying Files in Azure Code Executor\nDESCRIPTION: Demonstrates how to upload files to the Azure Code Executor environment and verify their contents by executing code that reads the uploaded files.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/extensions-user-guide/azure-container-code-executor.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith tempfile.TemporaryDirectory() as temp_dir:\n    test_file_1 = \"test_upload_1.txt\"\n    test_file_1_contents = \"test1 contents\"\n    test_file_2 = \"test_upload_2.txt\"\n    test_file_2_contents = \"test2 contents\"\n\n    async with await open_file(os.path.join(temp_dir, test_file_1), \"w\") as f:  # type: ignore[syntax]\n        await f.write(test_file_1_contents)\n    async with await open_file(os.path.join(temp_dir, test_file_2), \"w\") as f:  # type: ignore[syntax]\n        await f.write(test_file_2_contents)\n\n    assert os.path.isfile(os.path.join(temp_dir, test_file_1))\n    assert os.path.isfile(os.path.join(temp_dir, test_file_2))\n\n    executor = ACADynamicSessionsCodeExecutor(\n        pool_management_endpoint=POOL_MANAGEMENT_ENDPOINT, credential=DefaultAzureCredential(), work_dir=temp_dir\n    )\n    await executor.upload_files([test_file_1, test_file_2], cancellation_token)\n\n    file_list = await executor.get_file_list(cancellation_token)\n    assert test_file_1 in file_list\n    assert test_file_2 in file_list\n\n    code_blocks = [\n        CodeBlock(\n            code=f\"\"\"\nwith open(\"{test_file_1}\") as f:\n  print(f.read())\nwith open(\"{test_file_2}\") as f:\n  print(f.read())\n\"\"\",\n            language=\"python\",\n        )\n    ]\n    code_result = await executor.execute_code_blocks(code_blocks, cancellation_token)\n    assert code_result.exit_code == 0\n    assert test_file_1_contents in code_result.output\n    assert test_file_2_contents in code_result.output\n```\n\n----------------------------------------\n\nTITLE: Configuring Protocol Buffer Generation in .NET Project\nDESCRIPTION: XML project configuration for including Protocol Buffer dependencies and files in a .NET project. This shows how to set up the necessary tools to compile proto files into C# classes.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/test/Microsoft.AutoGen.Integration.Tests.AppHosts/HelloAgentTests/README.md#2025-04-22_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n  <ItemGroup>\n    <PackageReference Include=\"Google.Protobuf\" />\n    <PackageReference Include=\"Grpc.Tools\" PrivateAssets=\"All\" />\n    <Protobuf Include=\"..\\Protos\\messages.proto\" Link=\"Protos\\messages.proto\" />\n  </ItemGroup>\n```\n\n----------------------------------------\n\nTITLE: System Architecture Diagram using Mermaid\nDESCRIPTION: Mermaid diagram showing the distributed system architecture including GRPC Server, topics, and agent relationships with publish/subscribe patterns.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/samples/core_distributed-group-chat/README.md#2025-04-22_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD;\n    subgraph Host\n        A1[GRPC Server]\n        wt[Writer Topic]\n        et[Editor Topic]\n        ut[UI Topic]\n        gct[Group Chat Topic]\n    end\n    all_agents[All Agents -  Simplified Arrows!] --> A1\n\n    subgraph Distributed Writer Runtime\n        wt -.->|2 - Subscription| writer_agent\n        gct -.->|4 - Subscription| writer_agent\n        writer_agent -.->|3.1 - Publish: UI Message| ut\n        writer_agent -.->|3.2 - Publish: Group Chat Message| gct\n    end\n\n    subgraph Distributed Editor Runtime\n        et -.->|6 - Subscription| editor_agent\n        gct -.->|4 - Subscription| editor_agent\n        editor_agent -.->|7.1 - Publish: UI Message| ut\n        editor_agent -.->|7.2 - Publish: Group Chat Message| gct\n    end\n\n    subgraph Distributed Group Chat Manager Runtime\n        gct -.->|4 - Subscription| group_chat_manager\n        group_chat_manager -.->|1 - Request To Speak| wt\n        group_chat_manager -.->|5 - Request To Speak| et\n        group_chat_manager -.->|\\* - Publish Some of to UI Message| ut\n    end\n\n    subgraph Distributed UI Runtime\n        ut -.->|\\* - Subscription| ui_agent\n    end\n\n\n    style wt fill:#beb2c3,color:#000\n    style et fill:#beb2c3,color:#000\n    style gct fill:#beb2c3,color:#000\n    style ut fill:#beb2c3,color:#000\n    style writer_agent fill:#b7c4d7,color:#000\n    style editor_agent fill:#b7c4d7,color:#000\n    style group_chat_manager fill:#b7c4d7,color:#000\n    style ui_agent fill:#b7c4d7,color:#000\n```\n\n----------------------------------------\n\nTITLE: Listing Dependencies and Package Paths for AutoGen Python\nDESCRIPTION: This snippet lists the external dependencies and internal package paths required for the AutoGen Python project. It includes tiktoken and pyyaml as external libraries, and specifies paths for autogen-core, autogen-ext with OpenAI and magentic-one extensions, and autogen-agentchat.\nSOURCE: https://github.com/microsoft/autogen/blob/main/python/packages/agbench/benchmarks/GAIA/Templates/SelectorGroupChat/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntiktoken\npyyaml\n/autogen_python/packages/autogen-core\n/autogen_python/packages/autogen-ext[openai,magentic-one]\n/autogen_python/packages/autogen-agentchat\n```\n\n----------------------------------------\n\nTITLE: Using Statements for LMStudioAgent in C#\nDESCRIPTION: C# code snippet showing the necessary using statements for working with LMStudioAgent. These statements import the required namespaces for AutoGen and LMStudio functionality.\nSOURCE: https://github.com/microsoft/autogen/blob/main/dotnet/website/articles/Consume-LLM-server-from-LM-Studio.md#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nusing AutoGen.Core;\nusing AutoGen.OpenAI;\nusing AutoGen.LMStudio;\nusing Microsoft.Extensions.Logging;\n```"
  }
]
[
  {
    "owner": "philschmid",
    "repo": "gemini-samples",
    "content": "TITLE: Defining a Tool with LangChain Decorator\nDESCRIPTION: This code defines the `get_weather_forecast` function as a tool using the `@tool` decorator from LangChain. It retrieves weather data for a given location and date using the Open-Meteo API and returns a dictionary with the time and temperature for each hour.  The `@tool` decorator makes the function discoverable and usable within LangChain workflows.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom geopy.geocoders import Nominatim\nimport requests\nfrom langchain.tools import tool\n\ngeolocator = Nominatim(user_agent=\"weather-app\") \n\n@tool\ndef get_weather_forecast(location: str, date: str) -> str:\n    \"\"\"Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour.\"\n    \n    Args:\n        location (str): The city and state, e.g., San Francisco, CA\n        date (str): The forecasting date for when to get the weather format (yyyy-mm-dd)\n    Returns:\n        Dict[str, float]: A dictionary with the time as key and the temperature as value\n    \"\"\"\n    location = geolocator.geocode(location)\n    if location:\n        try:\n            response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={location.latitude}&longitude={location.longitude}&hourly=temperature_2m&start_date={date}&end_date={date}\")\n            data = response.json()\n            return {time: temp for time, temp in zip(data[\"hourly\"][\"time\"], data[\"hourly\"][\"temperature_2m\"])}\n        except Exception as e:\n            return {\"error\": str(e)}\n    else:\n        return {\"error\": \"Location not found\"}\n```\n\n----------------------------------------\n\nTITLE: Weather Forecast Function Definition\nDESCRIPTION: This code defines a Python function `get_weather_forecast` that retrieves weather data for a given location and date using the Open-Meteo API. It uses the `geopy` library to geocode the location, sends a request to the API, and returns a dictionary with the time and temperature for each hour. It includes error handling for location not found and API exceptions.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom geopy.geocoders import Nominatim\nimport requests\n\ngeolocator = Nominatim(user_agent=\"weather-app\") \n\ndef get_weather_forecast(location: str, date: str) -> str:\n    \"\"\"\n    Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour.\"\n    \n    Args:\n        location (str): The city and state, e.g., San Francisco, CA\n        date (str): The forecasting date for when to get the weather format (yyyy-mm-dd)\n    Returns:\n        Dict[str, float]: A dictionary with the time as key and the temperature as value\n    \"\"\"\n    location = geolocator.geocode(location)\n    if location:\n        try:\n            response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={location.latitude}&longitude={location.longitude}&hourly=temperature_2m&start_date={date}&end_date={date}\")\n            data = response.json()\n            return {time: temp for time, temp in zip(data[\"hourly\"][\"time\"], data[\"hourly\"][\"temperature_2m\"])}\n        except Exception as e:\n            return {\"error\": str(e)}\n    else:\n        return {\"error\": \"Location not found\"}\n```\n\n----------------------------------------\n\nTITLE: Defining Weather Function Schema\nDESCRIPTION: This snippet defines the schema for the 'get_weather_forecast' function, specifying the expected parameters (location and date) and their types and descriptions.  This JSON schema is used to define the function to the model.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nweather_function =   {\n    \"type\": \"function\",\n    \"function\": {\n    \"name\": \"get_weather_forecast\",\n    \"description\": \"Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g., San Francisco, CA\"\n            },\n            \"date\": {\n                \"type\": \"string\",\n                \"description\": \"the forecasting date for when to get the weather format (yyyy-mm-dd)\"\n            }\n        },\n        \"required\": [\"location\",\"date\"]\n    }\n}}\n```\n\n----------------------------------------\n\nTITLE: Full Agentic Example with Gemini and MCP (Python)\nDESCRIPTION: This snippet demonstrates a complete agentic loop using Gemini and an Airbnb MCP server.  It initializes the Gemini client and MCP server connection. The `agent_loop` function takes a user prompt, retrieves tools from the MCP server, converts them to Gemini tools, and then iterates, calling tools based on the model's responses.  It handles tool execution, manages conversation history, and returns the final response from the model.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-mcp-example.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom google import genai\nfrom google.genai import types\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\nimport os\n\nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\nmodel = \"gemini-2.0-flash\"\n\n# Create server parameters for stdio connection\nserver_params = StdioServerParameters(\n    command=\"npx\",  # Executable\n    args=[\n        \"-y\",\n        \"@openbnb/mcp-server-airbnb\",\n        \"--ignore-robots-txt\",\n    ],  # Optional command line arguments\n    env=None,  # Optional environment variables\n)\n\nasync def agent_loop(prompt: str, client: genai.Client, session: ClientSession):\n    contents = [types.Content(role=\"user\", parts=[types.Part(text=prompt)])]\n    # Initialize the connection\n    await session.initialize()\n    \n    # --- 1. Get Tools from Session and convert to Gemini Tool objects ---\n    mcp_tools = await session.list_tools()\n    tools = types.Tool(function_declarations=[\n        {\n            \"name\": tool.name,\n            \"description\": tool.description,\n            \"parameters\": tool.inputSchema,\n        }\n        for tool in mcp_tools.tools\n    ])\n    \n    # --- 2. Initial Request with user prompt and function declarations ---\n    response = await client.aio.models.generate_content(\n        model=model,  # Or your preferred model supporting function calling\n        contents=contents,\n        config=types.GenerateContentConfig(\n            temperature=0,\n            tools=[tools],\n        ),  # Example other config\n    )\n    \n    # --- 3. Append initial response to contents ---\n    contents.append(response.candidates[0].content)\n\n    # --- 4. Tool Calling Loop ---\n    turn_count = 0\n    max_tool_turns = 5\n    while response.function_calls and turn_count < max_tool_turns:\n        turn_count += 1\n        tool_response_parts: List[types.Part] = []\n\n        # --- 4.1 Process all function calls in order and return in this turn ---\n        for fc_part in response.function_calls:\n            tool_name = fc_part.name\n            args = fc_part.args or {}  # Ensure args is a dict\n            print(f\"Attempting to call MCP tool: '{tool_name}' with args: {args}\")\n\n            tool_response: dict\n            try:\n                # Call the session's tool executor\n                tool_result = await session.call_tool(tool_name, args)\n                print(f\"MCP tool '{tool_name}' executed successfully.\")\n                if tool_result.isError:\n                    tool_response = {\"error\": tool_result.content[0].text}\n                else:\n                    tool_response = {\"result\": tool_result.content[0].text}\n            except Exception as e:\n                tool_response = {\"error\":  f\"Tool execution failed: {type(e).__name__}: {e}\"}\n            \n            # Prepare FunctionResponse Part\n            tool_response_parts.append(\n                types.Part.from_function_response(\n                    name=tool_name, response=tool_response\n                )\n            )\n\n        # --- 4.2 Add the tool response(s) to history ---\n        contents.append(types.Content(role=\"user\", parts=tool_response_parts))\n        print(f\"Added {len(tool_response_parts)} tool response parts to history.\")\n\n        # --- 4.3 Make the next call to the model with updated history ---\n        print(\"Making subsequent API call with tool responses...\")\n        response = await client.aio.models.generate_content(\n            model=model,\n            contents=contents,  # Send updated history\n            config=types.GenerateContentConfig(\n                temperature=1.0,\n                tools=[tools],\n            ),  # Keep sending same config\n        )\n        contents.append(response.candidates[0].content)\n\n    if turn_count >= max_tool_turns and response.function_calls:\n        print(f\"Maximum tool turns ({max_tool_turns}) reached. Exiting loop.\")\n\n    print(\"MCP tool calling loop finished. Returning final response.\")\n    # --- 5. Return Final Response ---\n    return response\n        \nasync def run():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(\n            read,\n            write,\n        ) as session:\n            # Test prompt\n            prompt = \"I want to book an apartment in Paris for 2 nights. 03/28 - 03/30\"\n            print(f\"Running agent loop with prompt: {prompt}\")\n            # Run agent loop\n            res = await agent_loop(prompt, client, session)\n            return res\nres = await run()\nprint(res.text)\n```\n\n----------------------------------------\n\nTITLE: Querying Gemini with Google Search (Python)\nDESCRIPTION: This code snippet demonstrates how to use the `google-genai` library to query the Gemini model with Google Search enabled. It imports necessary modules, initializes the client, and generates content based on a given query. The response, search query details, and URLs used for grounding are then printed.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-google-search.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom google import genai\n\n# create client\nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\",\"xxx\"))\n\n\n# Generate a list of cookie recipes\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents='Who won the Euro 2024?',\n    config={\"tools\": [{\"google_search\": {}}]},\n)\n\n# print the response\nprint(f\"Response: {response.text}\")\n# print the search details\nprint(f\"Search Query: {response.candidates[0].grounding_metadata.web_search_queries}\")\n# urls used for grounding\nprint(f\"Search Pages: {', '.join([site.web.title for site in response.candidates[0].grounding_metadata.grounding_chunks])}\")\n```\n\n----------------------------------------\n\nTITLE: Define Tool and Model Call Functions Python\nDESCRIPTION: Defines two key functions: `call_tool` and `call_model`. `call_tool` iterates through tool calls in the last message, retrieves tool results by invoking the appropriate tool and appends to the outputs. `call_model` invokes the model with system prompt and the messages. The function outputs a list containing the model's response.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/langgraph-react-agent.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom langchain_core.messages import ToolMessage, SystemMessage\nfrom langchain_core.runnables import RunnableConfig\n\ntools_by_name = {tool.name: tool for tool in tools}\n\n# this is similar to customizing the create_react_agent with 'prompt' parameter, but is more flexible\n# system_prompt = SystemMessage(\n#     \"You are a helpful assistant that use tools to access and retrieve information from a weather API. Today is 2025-03-04. Help the user with their questions. Use the history to answer the question.\"\n# )\n\n# Define our tool node\ndef call_tool(state: AgentState):\n    outputs = []\n    # Iterate over the tool calls in the last message\n    for tool_call in state[\"messages\"][-1].tool_calls:\n        # Get the tool by name\n        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n        outputs.append(\n            ToolMessage(\n                content=tool_result,\n                name=tool_call[\"name\"],\n                tool_call_id=tool_call[\"id\"],\n            )\n        )\n    return {\"messages\": outputs}\n\ndef call_model(\n    state: AgentState,\n    config: RunnableConfig,\n):\n    # Invoke the model with the system prompt and the messages\n    response = model.invoke(state[\"messages\"], config)\n    # We return a list, because this will get added to the existing messages state using the add_messages reducer\n    return {\"messages\": [response]}\n```\n\n----------------------------------------\n\nTITLE: Creating Agent and Executor with Langchain\nDESCRIPTION: This snippet demonstrates how to create an agent and executor using Langchain to handle function calling with a weather API. It initializes a prompt template, creates an agent using `create_tool_calling_agent`, and executes it using `AgentExecutor`. The verbose flag enables detailed logging.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.agents import create_tool_calling_agent, AgentExecutor\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n# Initialize the prompt template\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that use tools to access and retrieve information from a weather API. Today is 2025-03-04.\"),\n    (\"human\", \"{input}\"),\n    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n    \n])\n\n# Create the agent and executor with out llm, tools and prompt\nagent = create_tool_calling_agent(llm_with_tools, [get_weather_forecast],prompt)\nagent_executor = AgentExecutor(agent=agent, tools=[get_weather_forecast], verbose=True)\n\n# Run our query \nres = agent_executor.invoke({\"input\": \"What is the weather in Berlin today?\"})\nprint(res[\"output\"])\n```\n\n----------------------------------------\n\nTITLE: Tool Calling/Function Calling with Gemini\nDESCRIPTION: This code demonstrates how to use tool calling/function calling with the Gemini model.  It defines a custom tool (get_weather), binds it to the model, and invokes the model with a query that triggers the tool. It then passes the tool results back to the model to get a final response.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_core.tools import tool\nfrom langchain_core.messages import ToolMessage\n\n# Define a tool\n@tool(description=\"Get the current weather in a given location\")\ndef get_weather(location: str) -> str:\n    return \"It's sunny.\"\n\n# Initialize model and bind the tool\nllm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\nllm_with_tools = llm.bind_tools([get_weather])\n\n# Invoke with a query that should trigger the tool\nquery = \"What's the weather in San Francisco?\"\nai_msg = llm_with_tools.invoke(query)\n\n# Access tool calls in the response\nprint(ai_msg.tool_calls)\n\n# Pass tool results back to the model\ntool_message = ToolMessage(\n    content=get_weather(*ai_msg.tool_calls[0]['args']),\n    tool_call_id=ai_msg.tool_calls[0]['id']\n)\nfinal_response = llm_with_tools.invoke([ai_msg, tool_message])\nprint(final_response.content)\n```\n\n----------------------------------------\n\nTITLE: Build LangGraph Agent Python\nDESCRIPTION: Builds the LangGraph agent using the defined functions. It creates a StateGraph, adds nodes for the model ('llm') and tools, sets the entry point to 'llm', adds a conditional edge to determine whether to call the tools or finish, and adds a normal edge from 'tools' back to 'llm'. The resulting graph is compiled.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/langgraph-react-agent.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.graph import StateGraph, END\n\n# Define a new graph with our state\nworkflow = StateGraph(AgentState)\n\n# 1. Add our nodes \nworkflow.add_node(\"llm\", call_model)\nworkflow.add_node(\"tools\",  call_tool)\n# 2. Set the entrypoint as `agent`, this is the first node called\nworkflow.set_entry_point(\"llm\")\n# 3. Add a conditional edge after the `llm` node is called.\nworkflow.add_conditional_edges(\n    # Edge is used after the `llm` node is called.\n    \"llm\",\n    # The function that will determine which node is called next.\n    should_continue,\n    # Mapping for where to go next, keys are strings from the function return, and the values are other nodes.\n    # END is a special node marking that the graph is finish.\n    {\n        # If `tools`, then we call the tool node.\n        \"continue\": \"tools\",\n        # Otherwise we finish.\n        \"end\": END,\n    },\n)\n# 4. Add a normal edge after `tools` is called, `llm` node is called next.\nworkflow.add_edge(\"tools\", \"llm\")\n\n# Now we can compile and visualize our graph\ngraph = workflow.compile()\n```\n\n----------------------------------------\n\nTITLE: Running Conversational Agent in Python\nDESCRIPTION: This code demonstrates how to run the Gemini agent with user queries. It executes two queries, one to get the weather in Berlin and another to compare the weather in Munich, leveraging the agent's message history for context.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-pydanticai-agent.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# res_1 = await weather.run(\"What is the weather in Berlin on 12th of March 2025?\")\nres_1 = await weather.run(\"How is the weather in Berlin on 12th of March 2025?\")\n# res_2 = await weather.run(\"Would it be warmer in Munich?\", message_history=res_1.new_messages())\nres_2 = await weather.run(\"Would it be in Munich warmer?\", message_history=res_1.new_messages())\n\nres_2\n```\n\n----------------------------------------\n\nTITLE: Binding Tools to LangChain LLM\nDESCRIPTION: This code binds the defined tool `get_weather_forecast` to the LangChain LLM (`llm`) using the `bind_tools` method.  This makes the tool available for the LLM to use during its operations, enabling function calling capabilities.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nllm_with_tools = llm.bind_tools([get_weather_forecast])\n```\n\n----------------------------------------\n\nTITLE: Compare Similarity using Gemini and Cosine Similarity in Python\nDESCRIPTION: This code snippet demonstrates how to calculate the cosine similarity between a query embedding and a set of document embeddings using the Gemini API. It takes a query and a list of documents, embeds them using `query_embeddings.embed_query` and `doc_embeddings.embed_documents`, respectively, and then calculates the cosine similarity between the query embedding and each document embedding. Finally, it prints the similarity score for each document.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nq_embed = query_embeddings.embed_query(\"What is the capital of France?\")\nd_embed = doc_embeddings.embed_documents([\"The capital of France is Paris.\", \"Philipp likes to eat pizza.\"])\n\nfor i, d in enumerate(d_embed):\n    similarity = cosine_similarity([q_embed], [d])[0][0]\n    print(f\"Document {i+1} similarity: {similarity}\")\n```\n\n----------------------------------------\n\nTITLE: Run Agent with Streaming Python\nDESCRIPTION: Demonstrates how to run the LangGraph agent with streaming output. It creates an initial message, then iterates through the stream of states from the graph. Each state's last message is printed using `pretty_print()`.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/langgraph-react-agent.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Create our initial message dictionary\ninputs = {\"messages\": [(\"user\", \"How is the weather in Berlin on 12th of March 2025?\")]}\n\n# call our graph with streaming to see the steps\n\nfor state in graph.stream(inputs, stream_mode=\"values\"):\n    last_message = state[\"messages\"][-1]\n    last_message.pretty_print()\n```\n\n----------------------------------------\n\nTITLE: Chat Completion using OpenAI SDK with Gemini\nDESCRIPTION: This Python snippet demonstrates how to perform a chat completion using the OpenAI SDK with Google Gemini models. It initializes the OpenAI client with the Gemini API key and base URL, then sends a chat message to the Gemini model. The response is streamed and printed to the console.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-with-openai-sdk.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"xxx\", # Replace with your Gemini API key\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n)\nmodel = \"gemini-2.0-pro-exp-02-05\" # gemini-2.0-flash, gemini-2.0-flash-lite-preview-02-05\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain to me how AI works\"\n        }\n    ],\n    stream=True\n)\n\nfor chunk in response:\n    print(chunk.choices[0].delta.content,end=\"\")\n```\n\n----------------------------------------\n\nTITLE: Calling Function with OpenAI Client and Schema\nDESCRIPTION: This snippet uses the OpenAI client to call the 'get_weather_forecast' function based on the defined schema. It sends a system message to set the context and a user message requesting the weather in Berlin. The tool_choice is set to auto to let the model decide if to use the tool.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n  model=model_id,\n  messages=[\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant that use tools to access and retrieve information from a weather API. Today is 2025-03-04.\"},\n      {\"role\": \"user\", \"content\": \"What is the weather in Berlin today?\"}],\n  tools=[weather_function],\n  tool_choice=\"auto\"\n)\n\nif response.choices[0].message.tool_calls:\n    print(response.choices[0].message.tool_calls[0].function)\n```\n\n----------------------------------------\n\nTITLE: Configuring Gemini for Function Calling\nDESCRIPTION: This code configures the Gemini model for function calling by creating a `GenerateContentConfig` object. It sets the system instruction, specifies the `get_weather_forecast` function as a tool, and disables automatic function calling. This configuration allows the model to identify when to call the function but requires manual handling of the function call and response.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom google.genai.types import GenerateContentConfig\n\n# Generation Config\nconfig = GenerateContentConfig(\n    system_instruction=\"You are a helpful assistant that can help with weather related questions. Today is 2025-03-04.\", # to give the LLM context on the current date.\n    tools=[get_weather_forecast], # define the functions that the LLM can use\n    automatic_function_calling={\"disable\": True} # Disable for now. \n)\n```\n\n----------------------------------------\n\nTITLE: Invoking LLM with Messages and Tools\nDESCRIPTION: This code invokes the LangChain LLM with a list of messages, including a system message and a human message. It then calls the LLM with the messages and tools bound to it. The response from the LLM will now potentially include a call to the `get_weather_forecast` tool.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    (\n        \"system\",\n        \"You are a helpful assistant that use tools to access and retrieve information from a weather API. Today is 2025-03-04.\",\n    ),\n    (\"human\", \"What is the weather in Berlin today?\"),\n]\n\n# Call the LLM with the messages and tools\nres = llm_with_tools.invoke(messages)\n```\n\n----------------------------------------\n\nTITLE: Simple MCP and Gemini Tool Calling Example (Python)\nDESCRIPTION: This snippet demonstrates how to use MCP with Google DeepMind Gemini by converting MCP tools into Gemini-compatible tools. It initializes a Gemini client, sets up a connection to the MCP server via `stdio_client`, lists the tools exposed by the MCP server, and converts them to Gemini's `types.Tool` format. Finally, it calls the Gemini model with the prompt and the tool declarations, processing the function call, if any, in the response.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-mcp-example.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom google import genai\nfrom google.genai import types\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\nimport os\n\nclient = genai.Client(\n    api_key=os.getenv(\"GEMINI_API_KEY\")\n)  # Replace with your actual API key setup\n\n\n# Create server parameters for stdio connection\nserver_params = StdioServerParameters(\n    command=\"npx\",  # Executable\n    args=[\n        \"-y\",\n        \"@openbnb/mcp-server-airbnb\",\n        \"--ignore-robots-txt\",\n    ],  # Optional command line arguments\n    env=None,  # Optional environment variables\n)\n\nasync def run():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(\n            read,\n            write,\n        ) as session:\n            prompt = \"I want to book an apartment in Paris for 2 nights. 03/28 - 03/30\"\n            # Initialize the connection\n            await session.initialize()\n            \n            # Get tools from MCP session and convert to Gemini Tool objects\n            mcp_tools = await session.list_tools()\n            tools = types.Tool(function_declarations=[\n                {\n                    \"name\": tool.name,\n                    \"description\": tool.description,\n                    \"parameters\": tool.inputSchema,\n                }\n                for tool in mcp_tools.tools\n            ])\n            \n            # Send request with function declarations\n            response = client.models.generate_content(\n                model=\"gemini-2.0-flash\",  # Or your preferred model supporting function calling\n                contents=prompt,\n                config=types.GenerateContentConfig(\n                    temperature=0.7,\n                    tools=[tools],\n                ),  # Example other config\n            )\n        # Check for a function call\n        if response.candidates[0].content.parts[0].function_call:\n            function_call = response.candidates[0].content.parts[0].function_call\n            print(f\"Function to call: {function_call.name}\")\n            print(f\"Arguments: {function_call.args}\")\n            # In a real app, you would call your function here:\n            # result = await session.call_tool(function_call.args, arguments=function_call.args)\n            # sent new request with function call\n        else:\n            print(\"No function call found in the response.\")\n            print(response.text)\n            \nawait run()\n```\n\n----------------------------------------\n\nTITLE: Gemini Embeddings with Vector Store\nDESCRIPTION: This example demonstrates how to use Gemini embeddings with a vector store. It initializes the embeddings, creates an in-memory vector store, and then retrieves similar documents based on a query.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\n# Initialize embeddings\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-exp-03-07\")\n\ntext = \"LangChain is the framework for building context-aware reasoning applications\"\n\n# Create vector store and retriever\nvectorstore = InMemoryVectorStore.from_texts([text], embedding=embeddings)\nretriever = vectorstore.as_retriever()\n\n# Retrieve similar documents\nretrieved_documents = retriever.invoke(\"What is LangChain?\")\nprint(retrieved_documents[0].page_content)\n```\n\n----------------------------------------\n\nTITLE: Configuring Gemini with Function Declaration\nDESCRIPTION: This snippet configures the Gemini model with a system instruction and tool definition. The system instruction sets the context for the LLM, and the tool definition includes the JSON schema of the weather function. This allows the LLM to recognize and utilize the function when appropriate.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom google.genai.types import GenerateContentConfig\n\n# Generation Config\nconfig = GenerateContentConfig(\n    system_instruction=\"You are a helpful assistant that use tools to access and retrieve information from a weather API. Today is 2025-03-04.\", # to give the LLM context on the current date.\n    tools=[{\"function_declarations\": [weather_function]}], # define the functions that the LLM can use\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatGoogleGenerativeAI in LangChain\nDESCRIPTION: This code initializes the `ChatGoogleGenerativeAI` class from the `langchain-google-genai` package to use Google Gemini with LangChain. It retrieves the API key and model ID from environment variables, creates an instance of the class with specified parameters like temperature, max tokens, timeout, and retries, and then invokes the model with a simple query. The response content is then printed.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n\n\n# Get API key and define model id\napi_key = os.getenv(\"GEMINI_API_KEY\",\"xxx\")\nmodel_id =  \"gemini-2.0-flash\"\n\n# Create LLM class \nllm = ChatGoogleGenerativeAI(\n    model=model_id,\n    temperature=0,\n    max_tokens=None,\n    timeout=None,\n    max_retries=2,\n    google_api_key=api_key,\n)\n\n# lets try it\nres = llm.invoke(\"What is the weather in Berlin today?\")\nprint(res.content)\n```\n\n----------------------------------------\n\nTITLE: Chain Calls with Prompt Template\nDESCRIPTION: This example demonstrates how to chain a prompt template with the Gemini chat model to create a more complex interaction.  It initializes the model and defines a prompt that takes input and output languages as parameters, allowing for dynamic translation tasks.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Initialize model\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-2.0-flash\",\n    temperature=0,\n)\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n    (\"human\", \"{input}\"),\n])\n\nchain = prompt | llm\nresult = chain.invoke({\n    \"input_language\": \"English\",\n    \"output_language\": \"German\",\n    \"input\": \"I love programming.\",\n})\nprint(result.content)  # Output: Ich liebe Programmieren.\n```\n\n----------------------------------------\n\nTITLE: Defining Weather Function JSON Schema\nDESCRIPTION: This snippet defines a JSON schema for a `get_weather_forecast` function. The schema includes the function's name, description, parameters (location and date), and required fields. The LLM uses this schema to understand when and how to call the weather function.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nweather_function = {\n    \"name\": \"get_weather_forecast\",\n    \"description\": \"Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g., San Francisco, CA\"\n            },\n            \"date\": {\n                \"type\": \"string\",\n                \"description\": \"the forecasting date for when to get the weather format (yyyy-mm-dd)\"\n            }\n        },\n        \"required\": [\"location\",\"date\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing LLM and Binding Tools (Python)\nDESCRIPTION: This code initializes the ChatGoogleGenerativeAI model and binds the get_weather_forecast tool to it.  The API key, temperature, and model name are configured. The model is prepared to use the tool for generating responses.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/langgraph-react-agent.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n  \n# Create LLM class \nllm = ChatGoogleGenerativeAI(\n    model= \"gemini-2.5-pro-exp-03-25\", # replace with \"gemini-2.0-flash\"\n    temperature=1.0,\n    max_tokens=None,\n    timeout=None,\n    max_retries=2,\n    google_api_key=api_key,\n)\n\n# Bind tools to the model\nmodel = llm.bind_tools([get_weather_forecast])\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Agent and Defining Weather Tool in Python\nDESCRIPTION: This code initializes a Gemini agent using `pydantic-ai` and defines a tool `get_weather_forecast` to retrieve weather information from the Open-Meteo API. The tool takes a location (city and state) and a date (yyyy-mm-dd) as input and returns the temperature forecast for each hour of the specified date.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-pydanticai-agent.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom geopy.geocoders import Nominatim\nimport requests\n\ngeolocator = Nominatim(user_agent=\"weather-app\") \n\nweather = Agent('google-gla:gemini-2.0-flash')\n\n\n@weather.tool_plain(docstring_format='google', require_parameter_descriptions=True)\nasync def get_weather_forecast(location: str, date: str):\n    \"\"\"Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour.\n    \n    Args:\n        location: The city and state, e.g., San Francisco\n        date: the forecasting date for when to get the weather format (yyyy-mm-dd)\n    \"\"\"\n    \n    location = geolocator.geocode(location)\n    if location:\n        try:\n            response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={location.latitude}&longitude={location.longitude}&hourly=temperature_2m&start_date={date}&end_date={date}\")\n            data = response.json()\n            return {time: temp for time, temp in zip(data[\"hourly\"][\"time\"], data[\"hourly\"][\"temperature_2m\"])}\n        except Exception as e:\n            return {\"error\": str(e)}\n    else:\n        return {\"error\": \"Location not found\"}\n```\n\n----------------------------------------\n\nTITLE: Gemini Embeddings with LangChain\nDESCRIPTION: This example demonstrates how to use Gemini embeddings with LangChain. It initializes the `GoogleGenerativeAIEmbeddings` class and then embeds a single query and multiple documents.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\n\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-exp-03-07\")\n\n# Embed a single query\nvector = embeddings.embed_query(\"hello, world!\")\n\n# Embed multiple documents\nvectors = embeddings.embed_documents([\n    \"Today is Monday\",\n    \"Today is Tuesday\",\n    \"Today is April Fools day\",\n])\n```\n\n----------------------------------------\n\nTITLE: Defining Weather Tool (Python)\nDESCRIPTION: This code defines a tool for retrieving weather forecasts using the Open-Meteo API. It uses the geopy library for geocoding and the requests library for making HTTP requests. The tool takes a location and date as input and returns weather data.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/langgraph-react-agent.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom geopy.geocoders import Nominatim\nfrom pydantic import BaseModel, Field\nimport requests\n\ngeolocator = Nominatim(user_agent=\"weather-app\") \n\nclass SearchInput(BaseModel):\n    location:str = Field(description=\"The city and state, e.g., San Francisco\")\n    date:str = Field(description=\"the forecasting date for when to get the weather format (yyyy-mm-dd)\")\n\n@tool(\"get_weather_forecast\", args_schema=SearchInput, return_direct=True)\ndef get_weather_forecast(location: str, date: str):\n    \"\"\"Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour.\"\"\"\n    location = geolocator.geocode(location)\n    if location:\n        try:\n            response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={location.latitude}&longitude={location.longitude}&hourly=temperature_2m&start_date={date}&end_date={date}\")\n            data = response.json()\n            return {time: temp for time, temp in zip(data[\"hourly\"][\"time\"], data[\"hourly\"][\"temperature_2m\"])}\n        except Exception as e:\n            return {\"error\": str(e)}\n    else:\n        return {\"error\": \"Location not found\"}\n\ntools = [get_weather_forecast]\n```\n\n----------------------------------------\n\nTITLE: Generating Audio Transcript using Gemini in Python\nDESCRIPTION: This code snippet demonstrates how to upload an audio file and generate a structured transcript using the Gemini API. It uses jinja2 to create a dynamic prompt, uploads the file, and generates content using the gemini-2.0-flash model, printing the resulting transcript.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-transcribe-with-timestamps.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom jinja2 import Template\n\n\n# path to the file to upload\nfile_path = \"../assets/porsche.mp3\" # Repalce with your own file path\n\n# Upload the file to the File API\nfile = client.files.upload(file=file_path)\n\n# Generate a structured response using the Gemini API\nprompt_template = Template(\"\"\"Generate a transcript of the episode. Include timestamps and identify speakers.\n\nSpeakers are: \n{% for speaker in speakers %}- {{ speaker }}{% if not loop.last %}\\n{% endif %}{% endfor %}\n\neg:\n[00:00] Brady: Hello there.\n[00:02] Tim: Hi Brady.\n\nIt is important to include the correct speaker names. Use the names you identified earlier. If you really don't know the speaker's name, identify them with a letter of the alphabet, eg there may be an unknown speaker 'A' and another unknown speaker 'B'.\n\nIf there is music or a short jingle playing, signify like so:\n[01:02] [MUSIC] or [01:02] [JINGLE]\n\nIf you can identify the name of the music or jingle playing then use that instead, eg:\n[01:02] [Firework by Katy Perry] or [01:02] [The Sofa Shop jingle]\n\nIf there is some other sound playing try to identify the sound, eg:\n[01:02] [Bell ringing]\n\nEach individual caption should be quite short, a few short sentences at most.\n\nSignify the end of the episode with [END].\n\nDon't use any markdown formatting, like bolding or italics.\n\nOnly use characters from the English alphabet, unless you genuinely believe foreign characters are correct.\n\nIt is important that you use the correct words and spell everything correctly. Use the context of the podcast to help.\nIf the hosts discuss something like a movie, book or celebrity, make sure the movie, book, or celebrity name is spelled correctly.\"\"\")\n\n# Define the speakers and render the prompt\nspeakers = [\"John\"]\nprompt = prompt_template.render(speakers=speakers)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[prompt, file],\n)\n\nprint(response.text)\n```\n\n----------------------------------------\n\nTITLE: Defining Function Dictionary\nDESCRIPTION: This code defines a dictionary that maps function names (strings) to the actual Python functions. This allows for dynamic function calls based on the string representation of the function name.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Function dictionary to map the function name to the function\nfunctions = {\n    \"get_weather_forecast\": get_weather_forecast\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Agent State (Python)\nDESCRIPTION: This code defines the AgentState TypedDict, which includes a list of messages representing the conversation history and the number of steps taken by the agent. It uses the add_messages helper function from LangGraph to manage messages.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/langgraph-react-agent.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated,Sequence, TypedDict\n\nfrom langchain_core.messages import BaseMessage \nfrom langgraph.graph.message import add_messages # helper function to add messages to the state\n\n\nclass AgentState(TypedDict):\n    \"\"\"The state of the agent.\"\"\"\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n    number_of_steps: int\n```\n\n----------------------------------------\n\nTITLE: Initializing Gemini Client\nDESCRIPTION: This snippet initializes the Gemini API client using an API key retrieved from the environment variables. It imports the necessary modules from the google-genai library and creates a client instance.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-analyze-transcribe-youtube.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom google import genai\nfrom google.genai import types\n# create client\napi_key = os.getenv(\"GEMINI_API_KEY\",\"xxx\")\nclient = genai.Client(api_key=api_key)\n```\n\n----------------------------------------\n\nTITLE: Initializing Gemini API Client\nDESCRIPTION: This code snippet initializes the Google Gemini API client. It retrieves the API key from an environment variable or uses a placeholder value. The client is then instantiated using the API key.  The file path for the csv data is also defined here.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-code-executor-data-analysis.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom google import genai\n\n# create client\napi_key = os.getenv(\"GEMINI_API_KEY\",\"xxx\")\nclient = genai.Client(api_key=api_key)\n\n\n# path to the file to upload\ncsv_file_path = \"../assets/portfolio_transactions.csv\" # Repalce with your own file path\n\n# Upload the file to the File API\nfile = client.files.upload(file=csv_file_path)\n```\n\n----------------------------------------\n\nTITLE: Podcast Transcription with Gemini 2.5 Pro\nDESCRIPTION: This snippet transcribes a podcast episode using Gemini 2.5 Pro. It uploads an audio file, generates a structured prompt using a Jinja2 template, and then calls the Gemini API to generate a transcript with speaker identification, timestamps, and audio event detection. The transcript is then printed to the console.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-analyze-transcribe-youtube.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom jinja2 import Template\n\n\n# path to the file to upload\nfile_path = \"../assets/porsche.mp3\" # Repalce with your own file path\n\n# Upload the file to the File API\nfile = client.files.upload(file=file_path)\n\n# Generate a structured response using the Gemini API\nprompt_template = Template(\"\"\"Generate a transcript of the episode. Include timestamps and identify speakers.\n\nSpeakers are: \n{% for speaker in speakers %}- {{ speaker }}{% if not loop.last %}\\n{% endif %}{% endfor %}\n\neg:\n[00:00] Brady: Hello there.\n[00:02] Tim: Hi Brady.\n\nIt is important to include the correct speaker names. Use the names you identified earlier. If you really don't know the speaker's name, identify them with a letter of the alphabet, eg there may be an unknown speaker 'A' and another unknown speaker 'B'.\n\nIf there is music or a short jingle playing, signify like so:\n[01:02] [MUSIC] or [01:02] [JINGLE]\n\nIf you can identify the name of the music or jingle playing then use that instead, eg:\n[01:02] [Firework by Katy Perry] or [01:02] [The Sofa Shop jingle]\n\nIf there is some other sound playing try to identify the sound, eg:\n[01:02] [Bell ringing]\n\nEach individual caption should be quite short, a few short sentences at most.\n\nSignify the end of the episode with [END].\n\nDon't use any markdown formatting, like bolding or italics.\n\nOnly use characters from the English alphabet, unless you genuinely believe foreign characters are correct.\n\nIt is important that you use the correct words and spell everything correctly. Use the context of the podcast to help.\nIf the hosts discuss something like a movie, book or celebrity, make sure the movie, book, or celebrity name is spelled correctly.\"\"\")\n\n# Define the speakers and render the prompt\nspeakers = [\"John\"]\nprompt = prompt_template.render(speakers=speakers)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-pro-exp-03-25\",\n    contents=[prompt, file],\n)\n\nprint(response.text)\n```\n\n----------------------------------------\n\nTITLE: Checking for LLM Function Call\nDESCRIPTION: This snippet checks if the Language Model (LLM) returned a function call in its response. If tool_calls are present, it prints the information.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nif res.tool_calls:\n    print(res.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Generate Image with Gemini\nDESCRIPTION: This code generates an image using the Gemini model with image generation capabilities. It initializes the model, provides a text prompt, and displays the generated image using IPython.display.Image.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nimport base64\nfrom IPython.display import Image, display\n\n# Initialize model for image generation\nllm = ChatGoogleGenerativeAI(model=\"models/gemini-2.0-flash-exp-image-generation\")\n\nmessage = {\n    \"role\": \"user\",\n    \"content\": \"Generate an image of a cat wearing a hat.\",\n}\n\nresponse = llm.invoke(\n    [message],\n    generation_config=dict(response_modalities=[\"TEXT\", \"IMAGE\"]),\n)\n\n# Display the generated image\nimage_base64 = response.content[0].get(\"image_url\").get(\"url\").split(\",\")[-1]\nimage_data = base64.b64decode(image_base64)\ndisplay(Image(data=image_data, width=300))\n```\n\n----------------------------------------\n\nTITLE: Setting Gemini API Key (Python)\nDESCRIPTION: This code snippet retrieves the Gemini API key from an environment variable or sets it manually. It's crucial for authenticating requests to the Gemini API.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/langgraph-react-agent.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os \n\n# Read your API key from the environment variable or set it manually\napi_key = os.getenv(\"GEMINI_API_KEY\",\"xxx\")\n```\n\n----------------------------------------\n\nTITLE: Agentic Loop for Function Calling\nDESCRIPTION: This code defines the main agentic loop `function_call_loop` for handling function calls within a Gemini interaction. It creates the conversation, makes initial requests, checks for function calls in the response, calls the identified function with arguments, builds a response with the tool results, and sends a follow-up request. The loop returns the final response from the model.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# agentic loop to handle the function call\ndef function_call_loop(prompt):\n    # create the conversation\n    contents = [types.Content(role=\"user\", parts=[types.Part(text=prompt)])]\n    # initial request \n    response = client.models.generate_content(\n        model=model_id,\n        config=config,\n        contents=contents\n    )\n    for part in response.candidates[0].content.parts:\n        # add response to the conversation\n        contents.append(types.Content(role=\"model\", parts=[part]))\n        # check if the response is a function call\n        if part.function_call:\n            print(\"Tool call detected\")\n            function_call = part.function_call\n            # Call the tool with arguments\n            print(f\"Calling tool: {function_call.name} with args: {function_call.args}\")\n            tool_result = call_function(function_call.name, **function_call.args)\n            # Build the response parts using the function result.\n            function_response_part = types.Part.from_function_response(\n                name=function_call.name,\n                response={\"result\": tool_result},\n            )\n            contents.append(types.Content(role=\"user\", parts=[function_response_part]))\n            # Send follow-up with tool results, but remove the tools from the config\n            print(f\"Calling LLM with tool results\")\n            func_gen_response = client.models.generate_content(\n                model=model_id, config=config, contents=contents\n            )\n            # Add the reponse to the conversation\n            contents.append(types.Content(role=\"model\", parts=[func_gen_response]))\n    # return the final response\n    return contents[-1].parts[0].text.strip()\n    \n\nfunction_call_loop(\"Whats the weather in Berlin today?\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Gen AI Client (Python)\nDESCRIPTION: This snippet initializes the Gen AI client using the `google-genai` library. It retrieves the API key from the environment variable `GEMINI_API_KEY` and creates a client object with the specified API key and model ID.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemma-with-genai-sdk.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom google import genai\n\n# create client\napi_key = os.getenv(\"GEMINI_API_KEY\",\"xxx\")\nclient = genai.Client(api_key=api_key)\n\n# speicfy the model id\nmodel_id = \"gemma-3-27b-it\"\n```\n\n----------------------------------------\n\nTITLE: Generating Content without Function\nDESCRIPTION: This snippet shows how to generate content without providing a function definition to the LLM.  The prompt asks for the weather in Berlin, and the LLM returns a generic response since it doesn't have access to the weather function.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.models.generate_content(\n    model=model_id,\n    contents='Whats the weather in Berlin this today?'\n)\nprint(response.text)\n```\n\n----------------------------------------\n\nTITLE: Installing google-genai Library (Python)\nDESCRIPTION: This code snippet installs the `google-genai` library using pip, which is required to interact with the Gen AI API.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemma-with-genai-sdk.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install google-genai\n```\n\n----------------------------------------\n\nTITLE: Setting up Google API Key\nDESCRIPTION: This code snippet retrieves the Google AI API key from the environment variables or prompts the user to enter it if not found, ensuring secure access to the Gemini API.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\nimport os\n\nif \"GOOGLE_API_KEY\" not in os.environ:\n    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Installing LangGraph and Dependencies (Python)\nDESCRIPTION: This code snippet installs the necessary Python packages, including langgraph, langchain-google-genai, geopy, and requests, using pip.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/langgraph-react-agent.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install langgraph langchain-google-genai geopy requests\n```\n\n----------------------------------------\n\nTITLE: YouTube Video Analysis with Gemini 2.5 Pro\nDESCRIPTION: This snippet analyzes a YouTube video using Gemini 2.5 Pro. It takes a YouTube URL as input, constructs a prompt to summarize the video, and then calls the Gemini API to analyze the video content. The response, which includes a summary of the video's main points, key topics, call to action, and a general overview, is then printed to the console.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-analyze-transcribe-youtube.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom google.genai import types\n\nyoutube_url = \"https://www.youtube.com/watch?v=RDOMKIw1aF4\" # Repalce with the youtube url you want to analyze\n\nprompt = \"\"\"Analyze the following YouTube video content. Provide a concise summary covering:\n\n1.  **Main Thesis/Claim:** What is the central point the creator is making?\n2.  **Key Topics:** List the main subjects discussed, referencing specific examples or technologies mentioned (e.g., AI models, programming languages, projects).\n3.  **Call to Action:** Identify any explicit requests made to the viewer.\n4.  **Summary:** Provide a concise summary of the video content.\n\nUse the provided title, chapter timestamps/descriptions, and description text for your analysis.\"\"\"\n\n# Analyze the video\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-pro-exp-03-25\",\n    contents=types.Content(\n        parts=[\n            types.Part(text=prompt),\n            types.Part(\n                file_data=types.FileData(file_uri=youtube_url)\n            )\n        ]\n    )\n)\n\nprint(response.text)\n```\n\n----------------------------------------\n\nTITLE: Visualize LangGraph Python\nDESCRIPTION: Visualizes the LangGraph using the `draw_mermaid_png` method and displays it using IPython's `Image` and `display` functions. This requires IPython to be installed.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/langgraph-react-agent.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Gemini API in Python\nDESCRIPTION: This code snippet installs the necessary Python packages for interacting with the Google Gemini API. It uses pip to install the google-genai and jinja2 libraries.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-transcribe-with-timestamps.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install google-genai jinja2\n```\n\n----------------------------------------\n\nTITLE: Getting Weather Forecast Function\nDESCRIPTION: This snippet defines a `get_weather_forecast` function that retrieves the weather forecast for a given location and date using the Open-Meteo API. It uses the `geopy` library to geocode the location and the `requests` library to make HTTP requests to the API.  The function returns a dictionary containing the time and temperature for each hour or an error message if the location is not found or an error occurs during the API call.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom google.genai import types\nfrom geopy.geocoders import Nominatim\nimport requests\n\n# Simple function to get the weather forecast for a given location and date\ngeolocator = Nominatim(user_agent=\"weather-app\") \ndef get_weather_forecast(location, date):\n    location = geolocator.geocode(location)\n    if location:\n        try:\n            response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={location.latitude}&longitude={location.longitude}&hourly=temperature_2m&start_date={date}&end_date={date}\")\n            data = response.json()\n            return {time: temp for time, temp in zip(data[\"hourly\"][\"time\"], data[\"hourly\"][\"temperature_2m\"])}\n        except Exception as e:\n            return {\"error\": str(e)}\n    else:\n        return {\"error\": \"Location not found\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Google Gemini and Pydantic in Python\nDESCRIPTION: Installs the `google-genai` and `pydantic` packages using pip. These packages are necessary for interacting with the Gemini API and defining data schemas, respectively.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-structured-outputs.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install google-genai pydantic\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Package\nDESCRIPTION: This snippet installs the OpenAI Python package. It's a prerequisite for using the OpenAI-compatible API with Google Gemini.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n%pip install openai\n```\n\n----------------------------------------\n\nTITLE: Installing Gemini and Jinja2 Packages with pip\nDESCRIPTION: This snippet uses the pip package manager to install the google-genai and jinja2 libraries. These libraries are necessary for interacting with the Gemini API and using Jinja2 templates for prompt generation, respectively.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-analyze-transcribe-youtube.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install google-genai jinja2\n```\n\n----------------------------------------\n\nTITLE: Setting Up Gemini API Key Environment Variable\nDESCRIPTION: This snippet demonstrates how to set up the environment variable for the Gemini API key in a `.env` file.  This is crucial for authenticating and using the Gemini models. The key is required to access the models and run the provided examples.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/README.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nGEMINI_API_KEY=your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Cloning the Gemini Samples Repository\nDESCRIPTION: This snippet provides the command to clone the repository from GitHub, allowing users to access the examples and guides related to Google DeepMind Gemini models. It is a necessary first step to use the provided code samples.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/philschmid/gemini-samples.git\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Output with Gemini and Pydantic in Python\nDESCRIPTION: This code snippet demonstrates how to use Google Gemini to generate a list of cookie recipes with a predefined Pydantic schema. It imports necessary modules, defines Pydantic schemas for `Ingredient` and `Recipe`, initializes the Gemini client, and uses the `generate_content` method to generate the structured output based on the defined schema. The `response.parsed` attribute contains the parsed data.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-structured-outputs.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom google import genai\nfrom pydantic import BaseModel\n\n# create client\nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\",\"xxx\"))\n\n\n# Define Pydantic schemas \nclass Ingredient(BaseModel):\n  name: str\n  quantity: str\n  unit: str\n\nclass Recipe(BaseModel):\n  recipe_name: str\n  ingredients: list[Ingredient]\n\n\n# Generate a list of cookie recipes\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash-lite',\n    contents='List a few popular cookie recipes.',\n    config={\n        'response_mime_type': 'application/json',\n        'response_schema': list[Recipe],\n    },\n)\n# Use the parsed response\nrecipes: list[Recipe] = response.parsed\nrecipes\n```\n\n----------------------------------------\n\nTITLE: Generating Content with User Context\nDESCRIPTION: This code demonstrates providing more context to the assistant about the user to have a more natural conversation. It creates a prompt including user information like name and location, and the current date. The assistant is then asked a weather related question. The `get_weather_forecast` tool is used to respond to the question.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom google.genai.types import GenerateContentConfig\n\n# Generation Config\nconfig = GenerateContentConfig(\n    system_instruction=\"You are a helpful assistant that use tools to access and retrieve information from a weather API.\",\n    tools=[get_weather_forecast], # define the functions that the LLM can use\n    # removed the automatic_function_calling as the default with callable functions is to call the function\n)\n\n# Prompt includes more context about the user and the current date\nprompt = f\"\"\"\nToday is 2025-03-04. You are chatting with Philipp, you have access to more information about him.\n\nUser Context:\n- name: Philipp\n- location: Nuremberg\n\nUser: Can i wear a T-shirt later today?\"\"\"\n\nr = client.models.generate_content(\n    model=model_id,\n    config=config,\n    contents=prompt\n)\n\nprint(r.text)\n```\n\n----------------------------------------\n\nTITLE: Image Input to Gemini\nDESCRIPTION: This code showcases how to provide image inputs to the Gemini model using image URLs and local image files.  It encodes the local image as a base64 string and includes it in the message content, along with text prompting the model to describe the image.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_core.messages import HumanMessage\nimport base64\n\n# Initialize model\nllm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n\n# Using an image URL\nmessage_url = HumanMessage(\n    content=[\n        {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        {\"type\": \"image_url\", \"image_url\": \"https://picsum.photos/seed/picsum/200/300\"},\n    ]\n)\nresult_url = llm.invoke([message_url])\nprint(result_url.content)\n\n# Using a local image\nlocal_image_path = \"../assets/react.png\"\nwith open(local_image_path, \"rb\") as image_file:\n    encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n\nmessage_local = HumanMessage(\n    content=[\n        {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        {\"type\": \"image_url\", \"image_url\": f\"data:image/png;base64,{encoded_image}\"}\n    ]\n)\nresult_local = llm.invoke([message_local])\nprint(result_local.content)\n```\n\n----------------------------------------\n\nTITLE: Data Visualization Prompt to Gemini API\nDESCRIPTION: This code snippet sends a prompt to the Gemini API to create a bar chart visualizing the trading volume (shares bought vs. sold) for each stock ticker (AAPL, GOOG, MSFT, TSLA, NVDA).  The prompt requests the model to save the plot as an image file.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-code-executor-data-analysis.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprompt =  \"\"\"Create a bar chart showing the total quantity of shares bought and sold for each stock ticker (AAPL, GOOG, MSFT, TSLA, NVDA). Use different colors for BUY and SELL actions. \nReturn a helpful message and generate a matplotlib plot chart and save the plot as an image file\"\"\"\n\n# Generate the response\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash-preview-04-17\",\n    contents=[file, prompt],\n    config={\"tools\": [{\"code_execution\": {}}]}\n)\n```\n\n----------------------------------------\n\nTITLE: Cash Flow Calculation with Gemini API\nDESCRIPTION: This code snippet sends a prompt to the Gemini API to calculate the total amount spent on 'BUY' transactions, the total amount received from 'SELL' transactions, the net cash flow, and the total commission paid. It then generates a response from the Gemini model and displays the response as markdown.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-code-executor-data-analysis.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Markdown\n\nprompt =  \"\"\"Calculate the total amount spent on 'BUY' transactions and the total amount received from 'SELL' transactions based on the 'TotalValue' column. Also, calculate the net cash flow (Total Received - Total Spent) and the total commission paid across all transactions. \nReturn a helpful message and markdown table with the results.\"\"\"\n\n# Generate the response\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash-preview-04-17\",\n    contents=[file, prompt],\n    config={\"tools\": [{\"code_execution\": {}}]}\n)\n\ndisplay(Markdown(response.text))\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Gemini for Automatic Function Calling\nDESCRIPTION: This code configures Gemini for automatic function calling by creating a `GenerateContentConfig` object. It sets the system instruction and specifies the `get_weather_forecast` function as a tool, removing the `automatic_function_calling` parameter to enable the default behavior of automatic function calling.  The code then calls the LLM and prints the text from the response.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom google.genai.types import GenerateContentConfig\n\n# Generation Config\nconfig = GenerateContentConfig(\n    system_instruction=\"You are a helpful assistant that use tools to access and retrieve information from a weather API. Today is 2025-03-04.\", # to give the LLM context on the current date.\n    tools=[get_weather_forecast], # define the functions that the LLM can use\n    # removed the automatic_function_calling as the default is to call the function\n)\n\nr = client.models.generate_content(\n    model=model_id,\n    config=config,\n    contents='Whats the weather in Berlin today?'\n)\n\nprint(r.text)\n```\n\n----------------------------------------\n\nTITLE: Data Manipulation with Gemini API\nDESCRIPTION: This code snippet sends a prompt to the Gemini API to add a 'TotalValue' column to the data, calculating it differently for 'BUY' and 'SELL' actions. It then generates a response from the Gemini model using the file and prompt, configuring the model to use the `code_execution` tool.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-code-executor-data-analysis.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprompt =  \"\"\"Add a 'TotalValue' column. For 'BUY' actions, calculate it as (Quantity * PricePerShare) + Commission. For 'SELL' actions, calculate it as (Quantity * PricePerShare) - Commission. \nReturn a helpful message and the return as a csv file.\"\"\"\n\n# Generate the response\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash-preview-04-17\",\n    contents=[file, prompt],\n    config={\"tools\": [{\"code_execution\": {}}]}\n)\n```\n\n----------------------------------------\n\nTITLE: Complex Querying Gemini with Google Search (Python)\nDESCRIPTION: This example shows how to make a more complex query to Gemini using the Google Search tool. It asks about the number of World Cups won by Germany and Brazil, demonstrating the model's ability to handle comparative questions that require up-to-date information.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-google-search.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Generate a list of cookie recipes\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents='Who won more World Cups, Germany or Brazil?',\n    config={\"tools\": [{\"google_search\": {}}]},\n)\n\n# print the response\nprint(f\"Response: {response.text}\")\n# print the search details\nprint(f\"Search Query: {response.candidates[0].grounding_metadata.web_search_queries}\")\n# urls used for grounding\nprint(f\"Search Pages: {', '.join([site.web.title for site in response.candidates[0].grounding_metadata.grounding_chunks])}\")\n```\n\n----------------------------------------\n\nTITLE: Structured Output with Gemini\nDESCRIPTION: This example demonstrates how to obtain structured output from the Gemini model using Pydantic models. It defines a Person model with name and height, initializes the model for structured output, and invokes it to extract information about a person.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n\n# Define the desired structure\nclass Person(BaseModel):\n    '''Information about a person.'''\n    name: str = Field(..., description=\"The person's name\")\n    height_m: float = Field(..., description=\"The person's height in meters\")\n\n# Initialize the model\nllm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\nstructured_llm = llm.with_structured_output(Person)\n\n# Invoke the model with a query asking for structured information\nresult = structured_llm.invoke(\"Who was the 16th president of the USA, and how tall was he in meters?\")\nprint(result)  # Output: name='Abraham Lincoln' height_m=1.93\n```\n\n----------------------------------------\n\nTITLE: Parsing Gemini Response & Saving CSV\nDESCRIPTION: This code snippet parses the response from the Gemini API, displays any text returned, and saves the returned CSV data to a file named 'response.csv'. It then displays a preview of the first few rows of the saved CSV file using pandas.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-code-executor-data-analysis.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom IPython.display import Markdown\n\noutput_file = \"response.csv\"\n\nfor part in response.candidates[0].content.parts:\n    if part.text:\n        display(Markdown(part.text))\n    if part.inline_data:\n        with open(output_file, \"wb\") as f:\n            f.write(part.inline_data.data)\n        display(Markdown(f\"CSV file saved to {output_file}\"))\n        display(Markdown(f\"Preview of the first few rows of the CSV file:\"))\n        display(pd.read_csv(output_file).head())\n        \n```\n\n----------------------------------------\n\nTITLE: Built-in Tools: Google Search, Code Execution\nDESCRIPTION: This example utilizes built-in tools like Google Search and Code Execution with the Gemini model. It invokes the model with queries designed to trigger these tools and prints the results.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom google.ai.generativelanguage_v1beta.types import Tool as GenAITool\n\n# Initialize model\nllm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n\n# Google Search\nsearch_resp = llm.invoke(\n    \"When is the next total solar eclipse in US?\",\n    tools=[GenAITool(google_search={})],\n)\nprint(search_resp.content)\n\n# Code Execution\ncode_resp = llm.invoke(\n    \"What is 2*2, use python\",\n    tools=[GenAITool(code_execution={})],\n)\n\nfor c in code_resp.content:\n    if isinstance(c, dict):\n        if c[\"type\"] == 'code_execution_result':\n            print(f\"Code execution result: {c['code_execution_result']}\")\n        elif c[\"type\"] == 'executable_code':\n            print(f\"Executable code: {c['executable_code']}\")\n    else:\n        print(c)\n```\n\n----------------------------------------\n\nTITLE: Generating Content with Configured Model\nDESCRIPTION: This code uses the configured Gemini model to generate content based on a prompt. It calls the `generate_content` method with the model ID, configuration, and the prompt. The code then iterates over each return part to check if it is a function call or a normal response. This allows for manual handling of function calls based on the model's output.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nr = client.models.generate_content(\n    model=model_id,\n    config=config,\n    contents='Whats the weather in Berlin today?'\n)\n# iterate over eacht return part and check if it is a function call or a normal response\nfor part in r.candidates[0].content.parts:\n    print(part.function_call)\n```\n\n----------------------------------------\n\nTITLE: Video Input to Gemini\nDESCRIPTION: This code demonstrates how to provide video input to the Gemini model. It reads a video file, encodes it to base64, and sends it as a media type message, requesting the model to describe the video content.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_core.messages import HumanMessage\nimport base64\n\n# Initialize model\nllm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n\nvideo_file_path = \"../assets/screen.mp4\"\nvideo_mime_type = \"video/mp4\"\n\nwith open(video_file_path, \"rb\") as video_file:\n    encoded_video = base64.b64encode(video_file.read()).decode('utf-8')\n\nmessage = HumanMessage(\n    content=[\n        {\"type\": \"text\", \"text\": \"Describe what's happening in this video.\"},\n        {\"type\": \"media\", \"data\": encoded_video, \"mime_type\": video_mime_type}\n    ]\n)\nresponse = llm.invoke([message])\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Audio Input to Gemini\nDESCRIPTION: This code demonstrates how to provide audio input to the Gemini model. It reads an audio file, encodes it to base64, and sends it as a media type message, instructing the model to transcribe the audio.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_core.messages import HumanMessage\nimport base64\n\n# Initialize model\nllm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n\naudio_file_path = \"../assets/porsche.mp3\"\naudio_mime_type = \"audio/mpeg\"\n\nwith open(audio_file_path, \"rb\") as audio_file:\n    encoded_audio = base64.b64encode(audio_file.read()).decode('utf-8')\n\nmessage = HumanMessage(\n    content=[\n        {\"type\": \"text\", \"text\": \"Transcribe this audio.\"},\n        {\"type\": \"media\", \"data\": encoded_audio, \"mime_type\": audio_mime_type}\n    ]\n)\nresponse = llm.invoke([message])\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Installing Google GenAI and MCP\nDESCRIPTION: This snippet demonstrates how to install the `google-genai` and `mcp` Python packages using pip. These packages are required for interacting with the Gemini API and MCP server, respectively.  It uses the `%pip` magic command, common in environments like Jupyter notebooks, to install the packages.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-mcp-example.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# install Google GenAI and MCP\n%pip install google-genai mcp\n```\n\n----------------------------------------\n\nTITLE: Define Conditional Edge Python\nDESCRIPTION: Defines the `should_continue` function which determines whether to call a tool or finish based on whether the last message contains a tool call. It checks the state's messages and returns 'end' if there are no tool calls in the last message, otherwise, it returns 'continue'.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/langgraph-react-agent.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Define the conditional edge that determines whether to continue or not\ndef should_continue(state: AgentState):\n    messages = state[\"messages\"]\n    # If the last message is not a tool call, then we finish\n    if not messages[-1].tool_calls:\n        return \"end\"\n    # default to continue\n    return \"continue\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Gemini Client\nDESCRIPTION: This snippet initializes the Gemini client using an API key and defines the model to be used (`gemini-2.0-flash`). It retrieves the API key from an environment variable, defaulting to 'xxx' if the variable is not set. This creates the necessary connection to interact with the Gemini 2.0 Flash model.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom google import genai\n\n# create client\napi_key = os.getenv(\"GEMINI_API_KEY\",\"xxx\")\nclient = genai.Client(api_key=api_key)\n\n# Define the model you are going to use\nmodel_id =  \"gemini-2.0-flash\"\n```\n\n----------------------------------------\n\nTITLE: Chat with Gemini Chat Model\nDESCRIPTION: This example initializes the Gemini chat model using `ChatGoogleGenerativeAI` from `langchain-google-genai`, sets basic configurations, and invokes it with a simple translation task. It demonstrates a basic interaction with the model, translating English to French.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n\n# Initialize model\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-2.0-flash\",\n    temperature=0,\n    max_tokens=None,\n    timeout=None,\n    max_retries=2,\n)\n\n# Simple invocation\nmessages = [\n    (\"system\", \"You are a helpful assistant that translates English to French.\"),\n    (\"human\", \"I love programming.\"),\n]\nresponse = llm.invoke(messages)\nprint(response.content)  # Output: J'adore la programmation.\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with Gemini API\nDESCRIPTION: This snippet initializes the OpenAI client to interact with the Gemini API, using the OpenAI-compatible endpoint. It retrieves the API key from the environment variables or sets a default value, and then creates an OpenAI client instance with the API key and base URL.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n# Get API key and define model id\napi_key = os.getenv(\"GEMINI_API_KEY\",\"xxx\")\nmodel_id =  \"gemini-2.0-flash\"\n\nclient = OpenAI(\n    api_key=api_key,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Content with Function Call\nDESCRIPTION: This snippet demonstrates generating content with function calling enabled.  It sends the same weather prompt to the LLM, but this time with the function configuration. The LLM identifies the need to call the `get_weather_forecast` function and generates a structured response containing the function name and arguments.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.models.generate_content(\n    model=model_id,\n    config=config,\n    contents='Whats the weather in Berlin today?'\n)\n\n# iterate over eacht return part and check if it is a function call or a normal response\nfor part in response.candidates[0].content.parts:\n    print(part.function_call)\n```\n\n----------------------------------------\n\nTITLE: Task Types for Embeddings\nDESCRIPTION: This example demonstrates using different task types for Gemini embeddings. It initializes two embedding models, one for retrieval queries and another for retrieval documents, using the `task_type` parameter.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Different task types for different use cases\nquery_embeddings = GoogleGenerativeAIEmbeddings(\n    model=\"models/gemini-embedding-exp-03-07\", \n    task_type=\"RETRIEVAL_QUERY\"  # For queries\n)\ndoc_embeddings = GoogleGenerativeAIEmbeddings(\n    model=\"models/gemini-embedding-exp-03-07\", \n    task_type=\"RETRIEVAL_DOCUMENT\"  # For documents\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Image from Gemini Response\nDESCRIPTION: This code snippet processes the Gemini API response. It iterates through each part of the response, displaying text content as Markdown and displaying image data as an image.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-code-executor-data-analysis.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Image, Markdown\n\n# Print the results and display the plot\nfor part in response.candidates[0].content.parts:\n    if part.text:\n        display(Markdown(part.text))\n    if part.inline_data:\n        display(Image(data=part.inline_data.data, width=800, format=\"png\"))\n```\n\n----------------------------------------\n\nTITLE: Initializing Gemini Client in Python\nDESCRIPTION: This code snippet initializes the Gemini API client using an API key stored in an environment variable. It imports the os and google.genai libraries and creates a client instance with the provided API key. It assumes that the API key is stored in the GEMINI_API_KEY environment variable.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-transcribe-with-timestamps.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom google import genai\n\n# create client\napi_key = os.getenv(\"GEMINI_API_KEY\",\"xxx\")\nclient = genai.Client(api_key=api_key)\n```\n\n----------------------------------------\n\nTITLE: Generating Content with Gemma 3 27B It (Python)\nDESCRIPTION: This code snippet demonstrates how to generate content using the Gemma 3 27B It model via the Gen AI API. It takes a prompt as input and streams the generated text chunks, printing them to the console.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemma-with-genai-sdk.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"Explain to me how AI works\"\n\nfor chunk in client.models.generate_content_stream(\n    model=model_id,\n    contents=prompt,\n):\n    print(chunk.text, end=\"\")\n```\n\n----------------------------------------\n\nTITLE: Helper Function to Call Functions\nDESCRIPTION: This code defines a helper function `call_function` that takes a function name and keyword arguments as input. It uses the function dictionary to retrieve the corresponding function and calls it with the provided arguments. This helps in abstracting the actual function call.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# helper function to call the function\ndef call_function(function_name, **kwargs):\n    return functions[function_name](**kwargs)\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Gemini\nDESCRIPTION: This snippet demonstrates a basic text generation call to the Gemini model to verify the connection and model availability. It sends a prompt asking for a fact about Nuremberg and prints the generated text response.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nres = client.models.generate_content(\n    model=model_id,\n    contents=[\"Tell me 1 good fact about Nuremberg.\"]\n)\nprint(res.text)\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain and Gemini Integration\nDESCRIPTION: This code installs the necessary Python packages for using LangChain with Google Gemini. It installs the `langchain` and `langchain-google-genai` packages, which provide the framework for building LLM-powered applications and the integration with Google Gemini models, respectively.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n%pip install langchain langchain-google-genai\n```\n\n----------------------------------------\n\nTITLE: Simple Chat Completion with OpenAI Client\nDESCRIPTION: This snippet demonstrates a simple chat completion using the OpenAI client with the Gemini model. It sends a user message asking for the weather in Berlin and prints the response content.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n  model=model_id,\n  messages=[{\"role\": \"user\", \"content\": \"What is the weather in Berlin today?\"}],\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Installing google-genai library\nDESCRIPTION: This snippet installs the `google-genai` library along with `geopy` and `requests` which are used for geocoding and making HTTP requests to the weather API. This is a prerequisite for interacting with the Gemini API.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/function-calling.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install \"google-genai>=1.0.0\" geopy requests\n```\n\n----------------------------------------\n\nTITLE: Token Usage Tracking with Gemini\nDESCRIPTION: This code snippet demonstrates how to track the token usage metadata for the Gemini model. It invokes the model and prints both the response content and the usage metadata.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n\n# Initialize model\nllm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n\nresult = llm.invoke(\"Explain the concept of prompt engineering in one sentence.\")\n\nprint(result.content)\nprint(\"\\nUsage Metadata:\")\nprint(result.usage_metadata)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies using pip\nDESCRIPTION: This code snippet installs the necessary Python libraries: google-genai and pandas using pip. These libraries are required for interacting with the Gemini API and for data manipulation using pandas.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-code-executor-data-analysis.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install google-genai pandas\n```\n\n----------------------------------------\n\nTITLE: Continue Conversation Python\nDESCRIPTION: Continues the conversation by appending a new user message to the state and streaming the response from the graph.  The last message of each state is printed using `pretty_print()`.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/guides/langgraph-react-agent.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nstate[\"messages\"].append((\"user\", \"Would it be in Munich warmer?\"))\n\n\nfor state in graph.stream(state, stream_mode=\"values\"):\n    last_message = state[\"messages\"][-1]\n    last_message.pretty_print()\n```\n\n----------------------------------------\n\nTITLE: Installing Pydantic-AI Package in Python\nDESCRIPTION: This command installs the `pydantic-ai` package using pip, which is required for building the conversational agent. The package facilitates integration with AI models and allows for defining agents and tools.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-pydanticai-agent.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install pydantic-ai\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI library using pip\nDESCRIPTION: This code snippet installs the OpenAI library using the pip package manager. It is necessary to execute this command before using the OpenAI library in Python.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-with-openai-sdk.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# install openai\n%pip install openai\n```\n\n----------------------------------------\n\nTITLE: Installing scikit-learn\nDESCRIPTION: This command installs the scikit-learn package, which might be needed for further embedding tasks or similarity calculations.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n%pip install scikit-learn\n```\n\n----------------------------------------\n\nTITLE: Installing langchain-google-genai\nDESCRIPTION: This command installs the langchain-google-genai package, which is necessary to use Google's Gemini models with LangChain.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-langchain.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%pip install langchain-google-genai\n```\n\n----------------------------------------\n\nTITLE: Installing google-genai Package\nDESCRIPTION: This command installs the google-genai library, which is required to interact with Google's Gemini models.\nSOURCE: https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-google-search.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install google-genai\n```"
  }
]
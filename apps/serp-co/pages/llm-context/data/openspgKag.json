[
  {
    "owner": "openspg",
    "repo": "kag",
    "content": "TITLE: Implementing NSGA-II Multi-Objective Optimization for SVM\nDESCRIPTION: Runs the NSGA-II algorithm to find the Pareto front of solutions optimizing both classification performance (AUC) and model simplicity (number of features), using defined parameter ranges for gamma, C, and feature selection.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_101\n\nLANGUAGE: R\nCODE:\n```\n# NSGAII multi-objective optimization:\n\ncat(\"NSGAII optimization:\\n\")\n\nm=2 # two objectives: AUC and number of features\n\nlower=c(-15,-5,rep(0,maxinputs))\n\nupper=c(3,15,rep(1,maxinputs))\n\nPTM=proc.time() # start clock\n\nG=nsga2(fn=eval,idim=length(lower),odim=m,lower.bounds=lower,upper.bounds=upper,popsize=12,generations=10)\n\nsec=(proc.time()-PTM)[3] # get seconds elapsed\n\ncat(\"time elapsed:\",sec,\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Computing Predictions and Evaluating Model Performance in Scala\nDESCRIPTION: This snippet demonstrates how to compute predictions, accuracy, and weighted F-measure for a trained model using Spark MLlib. It uses the test dataset to evaluate the model's performance.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_219\n\nLANGUAGE: Scala\nCODE:\n```\nval predictionAndLabel = test.map(p => (model.predict(p.features), p.label))\nval accuracy = 1.0 * predictionAndLabel.filter(x => x._1 == x._2).count() / test.count()\nval metrics = new MulticlassMetrics(predictionAndLabel)\nprintln(accuracy)\nprintln(metrics.weightedFMeasure)\n```\n\n----------------------------------------\n\nTITLE: Performing Analytics with Spark Transformations\nDESCRIPTION: Demonstrates various Spark transformations and actions to compute metrics like purchase counts and revenue\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\nval numPurchases = data.count()\nval uniqueUsers = data.map{ case (user, product, price) => user }.distinct().count()\nval totalRevenue = data.map{ case (user, product, price) => price.toDouble }.sum()\nval productsByPopularity = data\n  .map{ case (user, product, price) => (product, 1) }\n  .reduceByKey(_ + _)\n  .collect()\n  .sortBy(-_._2)    \nval mostPopular = productsByPopularity(0)\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful Streaming with updateStateByKey in Scala\nDESCRIPTION: A complete Scala application that demonstrates stateful streaming in Spark by tracking revenue and purchase counts per user across batches. The application uses updateStateByKey to maintain state and includes a custom state update function.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_228\n\nLANGUAGE: scala\nCODE:\n```\nobject StreamingStateApp {\n  import org.apache.spark.streaming.StreamingContext._\n\n  def updateState(prices: Seq[(String, Double)], currentTotal: Option[(Int, Double)]) = {\n    val currentRevenue = prices.map(_._2).sum\n    val currentNumberPurchases = prices.size\n    val state = currentTotal.getOrElse((0, 0.0))\n    Some((currentNumberPurchases + state._1, currentRevenue + state._2))\n  }\n\n  def main(args: Array[String]) {\n\n    val ssc = new StreamingContext(\"local[2]\", \"First Streaming App\", Seconds(10))\n    // for stateful operations, we need to set a checkpoint\n    // location\n    ssc.checkpoint(\"/tmp/sparkstreaming/\")\n    val stream = ssc.socketTextStream(\"localhost\", 9999)\n\n    // create stream of events from raw text elements\n    val events = stream.map { record =>\n      val event = record.split(\",\")\n      (event(0), event(1), event(2).toDouble)\n    }\n\n    val users = events.map{ case (user, product, price) => (user, (product, price)) }\n    val revenuePerUser = users.updateStateByKey(updateState)\n    revenuePerUser.print()\n\n    // start the context\n    ssc.start()\n    ssc.awaitTermination()\n\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Linear Regression Model in Scala\nDESCRIPTION: Creates a streaming application that consumes data points and trains a linear regression model using Spark Streaming. The model is updated in real-time as new data arrives.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_231\n\nLANGUAGE: scala\nCODE:\n```\nobject SimpleStreamingModel {\n\n  def main(args: Array[String]) {\n    val ssc = new StreamingContext(\"local[2]\", \"First Streaming App\", Seconds(10))\n    val stream = ssc.socketTextStream(\"localhost\", 9999)\n\n    val NumFeatures = 100\n    val zeroVector = DenseVector.zeros[Double](NumFeatures)\n    val model = new StreamingLinearRegressionWithSGD()\n      .setInitialWeights(Vectors.dense(zeroVector.data))\n      .setNumIterations(1)\n      .setStepSize(0.01)\n\n    // create a stream of labeled points\n    val labeledStream = stream.map { event =>\n      val split = event.split(\"\\t\")\n      val y = split(0).toDouble\n      val features = split(1).split(\",\").map(_.toDouble)\n      LabeledPoint(label = y, features = Vectors.dense(features))\n    }\n\n    // train and test model on the stream, and print predictions\n    model.trainOn(labeledStream)\n    model.predictOn(labeledStream).print()\n\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Computing Classification Metrics for Naive Bayes Model\nDESCRIPTION: Implementation for calculating PR and ROC metrics specifically for Naive Bayes model, using a modified scoring approach to handle probability outputs. Uses specialized nbData dataset format.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_98\n\nLANGUAGE: scala\nCODE:\n```\nval nbMetrics = Seq(nbModel).map{ model =>\n  val scoreAndLabels = nbData.map { point =>\n    val score = model.predict(point.features)\n    (if (score > 0.5) 1.0 else 0.0, point.label)\n  }\n  val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n  (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n}\n```\n\n----------------------------------------\n\nTITLE: Feature Extraction and LabeledPoint Creation\nDESCRIPTION: Scala code to extract features and create LabeledPoint instances for model training, handling missing values and cleaning data\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_91\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nval data = records.map { r =>\n  val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n  val label = trimmed(r.size - 1).toInt\n  val features = trimmed.slice(4, r.size - 1).map(d => if (d == \"?\") 0.0 else d.toDouble)\n  LabeledPoint(label, Vectors.dense(features))\n}\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance with Different Regularization Parameters\nDESCRIPTION: Computes the Area Under Curve (AUC) metric for a range of L2 regularization parameter values, training on the training set and evaluating on the test set.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_116\n\nLANGUAGE: Scala\nCODE:\n```\nval regResultsTest = Seq(0.0, 0.001, 0.0025, 0.005, 0.01).map { param =>\n  val model = trainWithParams(train, param, numIterations, new SquaredL2Updater, 1.0)\n  createMetrics(s\"$param L2 regularization parameter\", test, model)\n}\nregResultsTest.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.6f%%\") \n}\n```\n\n----------------------------------------\n\nTITLE: Analyzing User Ratings for Movies in Scala with Spark\nDESCRIPTION: This code retrieves ratings for a specific user, sorts them, and displays the top-rated movies with their titles.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_62\n\nLANGUAGE: Scala\nCODE:\n```\nval moviesForUser = ratings.keyBy(_.user).lookup(789)\nprintln(moviesForUser.size)\n\nmoviesForUser.sortBy(-_.rating).take(10).map(rating => (titles(rating.product), rating.rating)).foreach(println)\n```\n\n----------------------------------------\n\nTITLE: Importing MLlib Classification Models in Scala\nDESCRIPTION: Imports required Spark MLlib classification models and sets up basic parameters for model training including number of iterations and maximum tree depth.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_93\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.classification.LogisticRegressionWithSGD\nimport org.apache.spark.mllib.classification.SVMWithSGD\nimport org.apache.spark.mllib.classification.NaiveBayes\nimport org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.tree.configuration.Algo\nimport org.apache.spark.mllib.tree.impurity.Entropy \nval numIterations = 10\nval maxTreeDepth = 5\n```\n\n----------------------------------------\n\nTITLE: Computing APK Score for a User in Scala\nDESCRIPTION: Calculates the Average Precision at K=10 for a specific user by comparing their actual movies with the predicted recommendations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_74\n\nLANGUAGE: scala\nCODE:\n```\nval apk10 = avgPrecisionK(actualMovies, predictedMovies, 10)\n```\n\n----------------------------------------\n\nTITLE: Feature Vector Generation for Bike Sharing Dataset\nDESCRIPTION: Code to generate feature vectors by mapping categorical variables and calculating total feature vector length including both categorical and numerical features.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_119\n\nLANGUAGE: python\nCODE:\n```\nmappings = [get_mapping(records, i) for i in range(2,10)]\ncat_len = sum(map(len, mappings))\nnum_len = len(records.first()[11:15])\ntotal_len = num_len + cat_len\n\nprint \"Feature vector length for categorical features: %d\" % cat_len \nprint \"Feature vector length for numerical features: %d\" % num_len\nprint \"Total feature vector length: %d\" % total_len\n```\n\n----------------------------------------\n\nTITLE: Training K-means Clustering Model with Spark MLlib\nDESCRIPTION: This code trains K-means clustering models on movie and user factor vectors using Spark MLlib. It demonstrates setting parameters like number of clusters, iterations, and runs.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_156\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.clustering.KMeans\nval numClusters = 5\nval numIterations = 10\nval numRuns = 3\n\nval movieClusterModel = KMeans.train(movieVectors, numClusters, numIterations, numRuns)\n\nval movieClusterModelConverged = KMeans.train(movieVectors, numClusters, 100)\n\nval userClusterModel = KMeans.train(userVectors, numClusters, numIterations, numRuns)\n```\n\n----------------------------------------\n\nTITLE: Applying MLlib Normalizer and Comparing Results\nDESCRIPTION: Shows how to apply the MLlib Normalizer transformation to a vector in an RDD and convert the result back to a NumPy array for comparison with the custom normalization approach.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nnormalized_x_mllib = normalizer.transform(vector).first().toArray()\n\nprint \"x:\\n%s\" % x\nprint \"2-Norm of x: %2.4f\" % norm_x_2\nprint \"Normalized x MLlib:\\n%s\" % normalized_x_mllib\nprint \"2-Norm of normalized_x_mllib: %2.4f\" % np.linalg.norm(normalized_x_mllib)\n```\n\n----------------------------------------\n\nTITLE: Categorical Feature Encoding\nDESCRIPTION: Implements one-hot encoding for categorical features by creating a mapping of categories to indices and generating binary feature vectors.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_103\n\nLANGUAGE: scala\nCODE:\n```\nval categories = records.map(r => r(3)).distinct.collect.zipWithIndex.toMap\nval numCategories = categories.size\nval dataCategories = records.map { r =>\n  val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n  val label = trimmed(r.size - 1).toInt\n  val categoryIdx = categories(r(3))\n  val categoryFeatures = Array.ofDim[Double](numCategories)\n  categoryFeatures(categoryIdx) = 1.0\n  val otherFeatures = trimmed.slice(4, r.size - 1).map(d => if (d == \"?\") 0.0 else d.toDouble)\n  val features = categoryFeatures ++ otherFeatures\n  LabeledPoint(label, Vectors.dense(features))\n}\n```\n\n----------------------------------------\n\nTITLE: Training Regression Models in PySpark MLlib\nDESCRIPTION: Training both linear regression and decision tree models using MLlib, demonstrating model parameter configuration and prediction generation. Includes helper imports and model training calls.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_122\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.mllib.regression import LinearRegressionWithSGD\nfrom pyspark.mllib.tree import DecisionTree\n\nlinear_model = LinearRegressionWithSGD.train(data, iterations=10, step=0.1, intercept=False)\ntrue_vs_predicted = data.map(lambda p: (p.label, linear_model.predict(p.features)))\n\ndt_model = DecisionTree.trainRegressor(data_dt,{})\npreds = dt_model.predict(data_dt.map(lambda p: p.features))\nactual = data.map(lambda p: p.label)\ntrue_vs_predicted_dt = actual.zip(preds)\n```\n\n----------------------------------------\n\nTITLE: Applying NSGA-II to Real-Valued Optimization Task in R\nDESCRIPTION: This snippet demonstrates the use of NSGA-II for a real-valued optimization task with 8 dimensions and 2 objectives. It includes visualization of the Pareto front evolution and comparison with weighted formula results.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_72\n\nLANGUAGE: R\nCODE:\n```\nD=8 # dimension\ncat(\"real value task:\\n\")\nG=nsga2(fn=fes1,idim=D,odim=m,\nlower.bounds=rep(0,D),upper.bounds=rep(1,D),\npopsize=20,generations=1:100)\n\n# show best individuals:\nI=which(G[[100]]$pareto.optimal)\nfor(i in I)\n{\n  x=round(G[[100]]$par[i,],digits=2); cat(x)\n  cat(\" f=(\",round(fes1(x)[1],2),\",\",round(fes1(x)[2],2),\")\",\n      \"\\n\",sep=\"\")\n}\n\n# create PDF with Pareto front evolution:\npdf(file=\"nsga-fes1.pdf\",paper=\"special\",height=5,width=5)\npar(mar=c(4.0,4.0,0.1,0.1))\nI=1:100\nfor(i in I)\n{ P=G[[i]]$value # objectives f1 and f2\n  # color from light gray (75) to dark (1):\n  COL=paste(\"gray\",round(76-i*0.75),sep=\"\")\n  if(i==1) plot(P,xlim=c(0.5,5.0),ylim=c(0,2.0),\n                xlab=\"f1\",ylab=\"f2\",cex=0.5,col=COL)\n  Pareto=P[G[[i]]$pareto.optimal,]\n  # sort Pareto according to x axis:\n  I=sort.int(Pareto[,1],index.return=TRUE)\n  Pareto=Pareto[I$ix,]\n  points(P,type=\"p\",pch=1,cex=0.5,col=COL)\n  lines(Pareto,type=\"l\",cex=0.5,col=COL)\n}\ndev.off()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Decision Tree Depth in Spark\nDESCRIPTION: This snippet evaluates the impact of different tree depths on a decision tree regression model using Spark MLlib. It computes the RMSLE metric for various tree depths and plots the results.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_146\n\nLANGUAGE: Python\nCODE:\n```\nparams = [1, 2, 3, 4, 5, 10, 20]\nmetrics = [evaluate_dt(train_data_dt, test_data_dt, param, 32) for param in params] \nprint params\nprint metrics\nplot(params, metrics)\nfig = matplotlib.pyplot.gcf()\n```\n\n----------------------------------------\n\nTITLE: Binary Multi-Objective Optimization Functions in R\nDESCRIPTION: Implementation of binary optimization functions including sum of bits (sumbin), integer conversion (intbin), and maximum sine value (maxsin). These functions work with binary vectors and are used for maximizing both objectives simultaneously.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_61\n\nLANGUAGE: R\nCODE:\n```\nsumbin=function(x) (sum(x))\n\nintbin=function(x) sum(2^(which(rev(x==1))-1))\n\nmaxsin=function(x) # max sin (explained in Chapter )\n{ D=length(x);x=intbin(x)\nreturn(sin(pi*(as.numeric(x))/(2^D))) }\n```\n\n----------------------------------------\n\nTITLE: Training Logistic Regression with Scaled Features in Scala\nDESCRIPTION: This snippet trains a logistic regression model using standardized features and categorical data. It then evaluates the model's performance using accuracy, area under PR curve, and area under ROC curve metrics.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_104\n\nLANGUAGE: Scala\nCODE:\n```\nval lrModelScaledCats = LogisticRegressionWithSGD.train(scaledDataCats, numIterations)\nval lrTotalCorrectScaledCats = scaledDataCats.map { point =>\n  if (lrModelScaledCats.predict(point.features) == point.label) 1 else 0\n}.sum\nval lrAccuracyScaledCats = lrTotalCorrectScaledCats / numData\nval lrPredictionsVsTrueCats = scaledDataCats.map { point => \n  (lrModelScaledCats.predict(point.features), point.label) \n}\nval lrMetricsScaledCats = new BinaryClassificationMetrics(lrPredictionsVsTrueCats)\nval lrPrCats = lrMetricsScaledCats.areaUnderPR\nval lrRocCats = lrMetricsScaledCats.areaUnderROC\nprintln(f\"${lrModelScaledCats.getClass.getSimpleName}\\nAccuracy: ${lrAccuracyScaledCats * 100}%2.4f%%\\nArea under PR: ${lrPrCats * 100.0}%2.4f%%\\nArea under ROC: ${lrRocCats * 100.0}%2.4f%%\")\n```\n\n----------------------------------------\n\nTITLE: Time Series Forecasting Using ARIMA and Genetic Programming in R\nDESCRIPTION: Complete R script for time series forecasting of sunspot data using both ARIMA models and genetic programming. The code loads sunspot data, splits it into training and test sets, fits models, and evaluates performance using mean absolute error (MAE) metrics.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_94\n\nLANGUAGE: R\nCODE:\n```\n### tsf.R file ###\n\nlibrary(RCurl) # load RCurl package\n\n# get sunspot series\n\ntxt=getURL(\"http://sidc.oma.be/silso/DATA/yearssn.dat\")\n\n# consider 1700-2012 years (remove 2013 * row that is provisory in 2014)\n\nseries=strsplit(txt,\"\\n\")[[1]][1:(2012-1700+1)]\n\ncat(series,sep=\"\\n\",file=\"sunspots.dat\") # save to file\n\nseries=read.table(\"sunspots.dat\")[,2] # read from file\n\nL=length(series) # series length\n\nforecasts=32 # number of 1-ahead forecasts\n\noutsamples=series[(L-forecasts+1):L] # out-of-samples\n\nsunspots=series[1:(L-forecasts)] # in-samples\n\n# mean absolute error of residuals\n\nmaeres=function(residuals) mean(abs(residuals))\n\n# fit best ARIMA model:\n\nINIT=10 # initialization period (no error computed before)\n\nlibrary(forecast) # load forecast package\n\narima=auto.arima(sunspots) # detected order is AR=2, MA=1\n\nprint(arima) # show ARIMA model\n\ncat(\"arima fit MAE=\",\n\nmaeres(arima$residuals[INIT:length(sunspots)]),\"\\n\")\n\n# one-step ahead forecasts:\n\n# (this code is needed because forecast function\n\n# only issues h-ahead forecasts)\n\nLIN=length(sunspots) # length of in-samples\n\nf1=rep(NA,forecasts)\n\nfor(h in 1:forecasts)\n\n{ # execute arima with fixed coefficients but with more in-samples:\n\narima1=arima(series[1:(LIN+h-1)],order=arima$arma[c(1,3,2)],fixed=arima$coef)\n\nf1[h]=forecast(arima1,h=1)$mean[1]\n\n}\n\ne1=maeres(outsamples-f1)\n\ntext1=paste(\"arima (MAE=\",round(e1,digits=1),\")\",sep=\"\")\n\n# fit genetic programming arithmetic model:\n\nlibrary(rgp) # load rgp\n\nST=inputVariableSet(\"x1\",\"x2\")#same order of AR arima component\n\ncF1=constantFactorySet(function() rnorm(1)) # mean=0, sd=1\n\nFS=functionSet(\"+\",\"*\",\"-\",\"/\") # arithmetic\n\n# genetic programming time series function\n\n# receives function f\n\n# if(h>0) then returns 1-ahead forecasts\n\n# else returns MAE over fitting period (in-samples)\n\ngpts=function(f,h=0)\n\n{\n\n    if(h>0) TS=series\n\n    else TS=series[1:LIN]\n\n    LTS=length(TS)\n\n    F=rep(0,LTS) # forecasts\n\n    E=rep(0,LTS) # residuals\n\n    if(h>0) I=(LTS-h+1):LTS # h forecasts\n\n    else I=INIT:LTS # fit to in-samples\n\n    for(i in I)\n\n    {\n\n        F[i]=f(TS[i-1],TS[i-2])\n\n        if(is.nan(F[i])) F[i]=0 # deal with NaN\n\n        E[i]=TS[i]-F[i]\n\n    }\n\n    if(h>0) return (F[I]) # forecasts\n\n    else return(maeres(E[I])) # MAE on fit\n\n}\n\n# mutation function\n\nmut=function(func)\n\n{ mutateSubtree(func,funcset=FS,inset=ST,conset=cF1,\n\nmutatesubtreeprob=0.3,maxsubtreedepth=4)}\n\nset.seed(12345) # set for replicability\n\ngp=geneticProgramming(functionSet=FS,inputVariables=ST,\n\nconstantSet=cF1,\n\npopulationSize=100,\n\nfitnessFunction=gpts,\n\nstopCondition=makeStepsStopCondition(1000),\n\nmutationFunction=mut,\n\nverbose=TRUE)\n\nf2=gpts(gp$population[[which.min(gp$fitnessValues)]],\n\nh=forecasts)\n\ne2=maeres(outsamples-f2)\n\ntext2=paste(\"gp (MAE=\",round(e2,digits=1),\")\",sep=\"\")\n\ncat(\"best solution:\\n\")\n\nprint(gp$population[[which.min(gp$fitnessValues)]])\n\ncat(\"gp fit MAE=\",min(gp$fitnessValues),\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating Naive Bayes Model in Scala\nDESCRIPTION: This snippet trains a naive Bayes model using the prepared categorical data and evaluates its performance using accuracy, area under PR curve, and area under ROC curve metrics.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_106\n\nLANGUAGE: Scala\nCODE:\n```\nval nbModelCats = NaiveBayes.train(dataNB)\nval nbTotalCorrectCats = dataNB.map { point =>\n  if (nbModelCats.predict(point.features) == point.label) 1 else 0\n}.sum\nval nbAccuracyCats = nbTotalCorrectCats / numData\nval nbPredictionsVsTrueCats = dataNB.map { point => \n  (nbModelCats.predict(point.features), point.label) \n}\nval nbMetricsCats = new BinaryClassificationMetrics(nbPredictionsVsTrueCats)\nval nbPrCats = nbMetricsCats.areaUnderPR\nval nbRocCats = nbMetricsCats.areaUnderROC\nprintln(f\"${nbModelCats.getClass.getSimpleName}\\nAccuracy: ${nbAccuracyCats * 100}%2.4f%%\\nArea under PR: ${nbPrCats * 100.0}%2.4f%%\\nArea under ROC: ${nbRocCats * 100.0}%2.4f%%\")\n```\n\n----------------------------------------\n\nTITLE: Loading Image Files with Spark wholeTextFiles\nDESCRIPTION: Uses Spark's wholeTextFiles method to load image file paths from a directory. This approach allows access to entire files rather than line-by-line processing.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_166\n\nLANGUAGE: scala\nCODE:\n```\nval path = \"/PATH/lfw/*/\"\nval rdd = sc.wholeTextFiles(path)\nval first = rdd.first\nprintln(first)\n```\n\n----------------------------------------\n\nTITLE: Computing Matrix Statistics in Spark MLlib\nDESCRIPTION: Creates a RowMatrix from feature vectors and computes column summary statistics including mean, variance, min, max, and non-zero counts.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_100\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\nval vectors = data.map(lp => lp.features)\nval matrix = new RowMatrix(vectors)\nval matrixSummary = matrix.computeColumnSummaryStatistics()\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Purchase Analysis Application with PySpark\nDESCRIPTION: A basic Python Spark application that processes purchase history data from a CSV file. The program calculates total purchases, unique users, total revenue, and identifies the most popular product.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"A simple Spark app in Python\"\"\"\nfrom pyspark import SparkContext\n\nsc = SparkContext(\"local[2]\", \"First Spark App\")\n# we take the raw data in CSV format and convert it into a set of records of the form (user, product, price)\ndata = sc.textFile(\"data/UserPurchaseHistory.csv\").map(lambda line: line.split(\",\")).map(lambda record: (record[0], record[1], record[2]))\n# let's count the number of purchases\nnumPurchases = data.count()\n# let's count how many unique users made purchases\nuniqueUsers = data.map(lambda record: record[0]).distinct().count()\n# let's sum up our total revenue\ntotalRevenue = data.map(lambda record: float(record[2])).sum()\n# let's find our most popular product\nproducts = data.map(lambda record: (record[1], 1.0)).reduceByKey(lambda a, b: a + b).collect()\nmostPopular = sorted(products, key=lambda x: x[1], reverse=True)[0]\n\nprint \"Total purchases: %d\" % numPurchases\nprint \"Unique users: %d\" % uniqueUsers\nprint \"Total revenue: %2.2f\" % totalRevenue\nprint \"Most popular product: %s with %d purchases\" % (mostPopular[0], mostPopular[1])\n```\n\n----------------------------------------\n\nTITLE: Defining SVM Evaluation Function for NSGA-II Optimization in R\nDESCRIPTION: Creates an evaluation function that trains an SVM model with specified gamma and C hyperparameters. The function returns the 1-AUC metrics for each class to be minimized by the NSGA-II algorithm.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_125\n\nLANGUAGE: R\nCODE:\n```\neval=function(x)\n{ n=length(x)\n  gamma=2^x[1]\n  C=2^x[2]\n  inputs=1:maxinputs # use all inputs\n  attributes=c(inputs,output)\n  # divert console:\n  # sink is used to avoid kernlab ksvm messages in a few cases\n  sink(file=textConnection(\"rval\",\"w\",local = TRUE))\n  M=mining(quality~.,d[H$tr,attributes],method=c(\"kfold\",3),model=\"svm\",search=gamma,mpar=c(C,NA))\n  sink(NULL) # restores console\n  # AUC for the internal 3-fold cross-validation:\n  auc=as.numeric(mmetric(M,metric=\"AUCCLASS\"))\n  # auc now contains 3 values, the AUC for each class\n  auc1=1-auc # transform auc maximization into minimization goal\n  return(c(auc1))\n}\n```\n\n----------------------------------------\n\nTITLE: Computing PR and ROC Metrics for Logistic Regression and SVM Models\nDESCRIPTION: Code to calculate area under PR curve and ROC curve metrics for Logistic Regression and SVM models using Spark MLlib's BinaryClassificationMetrics class. Maps prediction scores and labels from the models to compute evaluation metrics.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_97\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nval metrics = Seq(lrModel, svmModel).map { model => \n  val scoreAndLabels = data.map { point =>\n    (model.predict(point.features), point.label)\n  }\n  val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n  (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n}\n```\n\n----------------------------------------\n\nTITLE: Creating ROC Curve Visualization for SVM Model Evaluation in R\nDESCRIPTION: Generates a PDF visualization of the ROC curve for the test data, including the AUC metric. The mgraph function from rminer is used to create the plot with proper labels and formatting.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_106\n\nLANGUAGE: R\nCODE:\n```\nauc=mmetric(d[H$ts,]$quality,P,metric=\"AUC\")\n\nmain=paste(\"ROC curve for test data\",\n\n\" (AUC=\",round(auc,digits=2),\")\",sep=\"\")\n\nmgraph(d[H$ts,]$quality,P,graph=\"ROC\",PDF=\"roc-wine\",main=main,baseline=TRUE,Grid=10,leg=\"SVM\")\n```\n\n----------------------------------------\n\nTITLE: Implementing INSTEAD OF Trigger for Custom VIEW Updates\nDESCRIPTION: Example of an INSTEAD OF trigger that provides custom logic for handling INSERT operations on a VIEW that cannot be automatically updated by the system.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_57\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TRIGGER trig_name\nINSTEAD OF INSERT ON view_name\nREFERENCING NEW AS N\nFOR EACH ROW\nBEGIN ATOMIC\n   -- trigger body\nEND;\n```\n\n----------------------------------------\n\nTITLE: Computing Performance Metrics for Log-Transformed Linear Model\nDESCRIPTION: Calculates Mean Squared Error, Mean Absolute Error, and Root Mean Squared Log Error for the regression model and compares transformed vs non-transformed predictions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_132\n\nLANGUAGE: python\nCODE:\n```\nmse_log = true_vs_predicted_log.map(lambda (t, p): squared_error(t, p)).mean()\nmae_log = true_vs_predicted_log.map(lambda (t, p): abs_error(t, p)).mean()\nrmsle_log = np.sqrt(true_vs_predicted_log.map(lambda (t, p): squared_log_error(t, p)).mean())\nprint \"Mean Squared Error: %2.4f\" % mse_log\nprint \"Mean Absolue Error: %2.4f\" % mae_log\nprint \"Root Mean Squared Log Error: %2.4f\" % rmsle_log\nprint \"Non log-transformed predictions:\\n\" + str(true_vs_predicted.take(3))\nprint \"Log-transformed predictions:\\n\" + str(true_vs_predicted_log.take(3))\n```\n\n----------------------------------------\n\nTITLE: CASE Expression Usage\nDESCRIPTION: Examples of CASE expressions for conditional logic, including both simple and searched CASE formats. Shows how to replace IF-THEN-ELSE control flows with CASE statements.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_95\n\nLANGUAGE: sql\nCODE:\n```\nCASE\n  WHEN condition THEN result\n  ELSE default\nEND\n```\n\n----------------------------------------\n\nTITLE: L2 Regularization Testing in Scala\nDESCRIPTION: Tests different L2 regularization parameters to evaluate impact on model performance and prevent overfitting.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_111\n\nLANGUAGE: scala\nCODE:\n```\nval regResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n  val model = trainWithParams(scaledDataCats, param, numIterations, new SquaredL2Updater, 1.0)\n  createMetrics(s\"$param L2 regularization parameter\", scaledDataCats, model)\n}\nregResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\n```\n\n----------------------------------------\n\nTITLE: Comparing Raw Features with TF-IDF Features for Text Classification in Scala\nDESCRIPTION: This code compares the performance of a model trained on raw text data with one trained on processed and TF-IDF weighted text data. It applies hashing term frequency transformation to raw text tokens and evaluates the model's performance.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_220\n\nLANGUAGE: Scala\nCODE:\n```\nval rawTokens = rdd.map { case (file, text) => text.split(\" \") }\nval rawTF = texrawTokenst.map(doc => hashingTF.transform(doc))\nval rawTrain = newsgroups.zip(rawTF).map { case (topic, vector) => LabeledPoint(newsgroupsMap(topic), vector) }\nval rawModel = NaiveBayes.train(rawTrain, lambda = 0.1)\nval rawTestTF = testRDD.map { case (file, text) => hashingTF.transform(text.split(\" \")) }\nval rawZippedTest = testLabels.zip(rawTestTF)\nval rawTest = rawZippedTest.map { case (topic, vector) => LabeledPoint(topic, vector) }\nval rawPredictionAndLabel = rawTest.map(p => (rawModel.predict(p.features), p.label))\nval rawAccuracy = 1.0 * rawPredictionAndLabel.filter(x => x._1 == x._2).count() / rawTest.count()\nprintln(rawAccuracy)\nval rawMetrics = new MulticlassMetrics(rawPredictionAndLabel)\nprintln(rawMetrics.weightedFMeasure)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Decision Tree Bins Parameter in Python\nDESCRIPTION: This code snippet evaluates the impact of varying the number of bins parameter for a decision tree model. It uses a custom evaluate_dt function to compute metrics for different bin values, then plots the results.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_147\n\nLANGUAGE: python\nCODE:\n```\nparams = [2, 4, 8, 16, 32, 64, 100]\nmetrics = [evaluate_dt(train_data_dt, test_data_dt, 5, param) for param in params]\nprint params\nprint metrics\nplot(params, metrics)\nfig = matplotlib.pyplot.gcf()\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Functions in R\nDESCRIPTION: Examples of custom function definitions in R, including profit calculation, cost computation, sales estimation, and a recursive factorial function. Demonstrates function scope and default arguments.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_12\n\nLANGUAGE: R\nCODE:\n```\n### functions.R file ###\n\n# compute the bag factory profit for x:\n# x - a vector of prices\nprofit=function(x) # x - a vector of prices\n{ x=round(x,digits=0) # convert x into integer\ns=sales(x) # get the expected sales\nc=cost(s) # get the expected cost\nprofit=sum(s*x-c) # compute the profit\nreturn(profit)\n# local variables x, s, c and profit are lost from here\n}\n\n# compute the cost for producing units:\n# units - number of units produced\n# A - fixed cost, cpu - cost per unit\ncost=function(units,A=100,cpu=35-5*(1:length(units)))\n{ return(A+cpu*units) }\n\n# compute the estimated sales for x:\n# x - a vector of prices, m - marketing effort\n# A, B, C - constants of the estimated function\nsales=function(x,A=1000,B=200,C=141,\nm=seq(2,length.out=length(x),by=-0.25))\n{ return(round(m*(A/log(x+B)-C),digits=0))}\n\n# example of a simple recursive function:\nfact=function(x=0) # x - integer number\n{ if(x==0) return(1) else return(x*fact(x-1))}\n```\n\n----------------------------------------\n\nTITLE: Analyzing L1 Regularization Sparsity in Spark Linear Regression\nDESCRIPTION: This snippet trains linear regression models with different L1 regularization levels and counts the number of zero weights to analyze the sparsity effect of L1 regularization.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_144\n\nLANGUAGE: Python\nCODE:\n```\nmodel_l1 = LinearRegressionWithSGD.train(train_data, 10, 0.1, regParam=1.0, regType='l1', intercept=False)\nmodel_l1_10 = LinearRegressionWithSGD.train(train_data, 10, 0.1, regParam=10.0, regType='l1', intercept=False)\nmodel_l1_100 = LinearRegressionWithSGD.train(train_data, 10, 0.1, regParam=100.0, regType='l1', intercept=False)\nprint \"L1 (1.0) number of zero weights: \" + str(sum(model_l1.weights.array == 0))\nprint \"L1 (10.0) number of zeros weights: \" + str(sum(model_l1_10.weights.array == 0))\nprint \"L1 (100.0) number of zeros weights: \" + str(sum(model_l1_100.weights.array == 0))\n```\n\n----------------------------------------\n\nTITLE: Implementing Genetic Programming for Function Approximation in R\nDESCRIPTION: Code that implements genetic programming using the rgp package to approximate the Eggholder function. It sets up the function set, input variables, and evaluation mechanism, then runs the genetic programming algorithm and visualizes the results.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_116\n\nLANGUAGE: R\nCODE:\n```\nlibrary(rgp) # load rgp\n\n# auxiliary functions:\n\neggholder=function(x) # length of x is 2\n\nf=(-(x[2]+47)*sin(sqrt(abs(x[2]+x[1]/2+47)))\n\n-x[1]*sin(sqrt(abs(x[1]-(x[2]+47))))\n\n)\n\nfwrapper=function(x,f)\n\n{ res=suppressWarnings(f(x[1],x[2]))\n\n# if NaN is generated (e.g. sqrt(-1)) then\n\nif(is.nan(res)) res=Inf # replace by Inf\n\nreturn(res)\n\n}\n\n# configuration of the genetic programming:\n\nST=inputVariableSet(\"x1\",\"x2\")\n\ncF1=constantFactorySet(function() sample(c(2,47),1) )\n\nFS=functionSet(\"+\",\"-\",\"/\",\"sin\",\"sqrt\",\"abs\")\n\n# set the input samples:\n\nsamples=500\n\ndomain=matrix(ncol=2,nrow=samples)\n\ndomain[]=runif(samples,-512,512)\n\neval=function(f) # evaluation function\n\nmse(apply(domain,1,eggholder),apply(domain,1,fwrapper,f))\n\n# run the genetic programming:\n\ngp=geneticProgramming(functionSet=FS,inputVariables=ST,\n\nconstantSet=cF1,populationSize=100,\n\nfitnessFunction=eval,\n\nstopCondition=makeTimeStopCondition(20),\n\nverbose=TRUE)\n\n# show the results:\n\nb=gp$population[[which.min(gp$fitnessValues)]]\n\ncat(\"best solution (f=\",eval(b),\"):\\n\")\n\nprint(b)\n\nL1=apply(domain,1,eggholder)\n\nL2=apply(domain,1,fwrapper,b)\n\n# sort L1 and L2 (according to L1 indexes)\n\n# for an easier comparison of both curves:\n\nL1=sort.int(L1,index.return=TRUE)\n\nL2=L2[L1$ix]\n\nL1=L1$x\n\nMIN=min(L1,L2);MAX=max(L1,L2)\n\nplot(L1,ylim=c(MIN,MAX),type=\"l\",lwd=2,lty=1,\n\nxlab=\"points\",ylab=\"function values\")\n\nlines(L2,type=\"l\",lwd=2,lty=2)\n\nlegend(\"bottomright\",leg=c(\"eggholder\",\"GP function\"),lwd=2,lty=1:2)\n\n# note: the fit is not perfect, but the search space is\n\n# too large\n```\n\n----------------------------------------\n\nTITLE: Connecting to EC2 Spark Master Node via SSH\nDESCRIPTION: Command to SSH into the Spark master node on EC2 using the private key file. This allows direct access to manage and run applications on the Spark cluster.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n> ssh -i spark.pem root@ec2-54-227-127-14.compute-1.amazonaws.com\n```\n\n----------------------------------------\n\nTITLE: Helper Function for Training Logistic Regression with Custom Parameters in Scala\nDESCRIPTION: This function trains a logistic regression model with customizable regularization parameter, number of iterations, updater, and step size. It's used for exploring different parameter settings to optimize model performance.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_107\n\nLANGUAGE: Scala\nCODE:\n```\ndef trainWithParams(input: RDD[LabeledPoint], regParam: Double, numIterations: Int, updater: Updater, stepSize: Double) = {\n  val lr = new LogisticRegressionWithSGD\n  lr.optimizer.setNumIterations(numIterations). setUpdater(updater).setRegParam(regParam).setStepSize(stepSize)\n  lr.run(input)\n}\n```\n\n----------------------------------------\n\nTITLE: Hindley's Algorithm with Immediate Resolution Typing Rules\nDESCRIPTION: Mathematical type rules for Hindley's algorithm with immediate resolution, showing how type equations are solved as they are generated. The algorithm returns a type A and a substitution ρ that is a principal solution of the equations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_28\n\nLANGUAGE: mathematics\nCODE:\n```\nif σ = mgu(B =ρ′A -> X)\n\nif σ = mgu(A = nat) and σ′ = mgu(B = nat)\n\nif σ = mgu(A = nat) and σ′ = mgu(ρ″ B = C)\n```\n\n----------------------------------------\n\nTITLE: Creating a CSV Parser View in SQL\nDESCRIPTION: A reusable view definition for parsing comma-separated values. This creates a permanent database object that can be used to parse any CSV string using set-based operations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_73\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE VIEW CSVparser (str, pos, nextpos, place, value)\nAS\nSELECT P.str,\n       P.pos,\n       P.nextpos,\n       (P.pos - 1) / 2 + 1 AS place,\n       CAST(SUBSTRING(P.str, P.pos + 1, P.nextpos - P.pos - 1) AS INTEGER) AS value\nFROM (SELECT v.str,\n             S1.s AS pos,\n             MIN(S2.s) AS nextpos\n      FROM (VALUES (',123,456,789,')) AS v(str)\n      CROSS JOIN Sequence AS S1\n      CROSS JOIN Sequence AS S2\n      WHERE S1.s < S2.s\n      AND SUBSTRING(v.str, S1.s, 1) = ','\n      AND SUBSTRING(v.str, S2.s, 1) = ','\n      GROUP BY v.str, S1.s) AS P;\n```\n\n----------------------------------------\n\nTITLE: Integer Optimization for Bag Prices using Tabu Search in R\nDESCRIPTION: Implementation of Tabu Search for the bag prices integer optimization problem using binary representation. The code handles conversion between binary and integer representations, applies death penalty for infeasible solutions, and uses the tabuSearch package with customized parameters.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_34\n\nLANGUAGE: R\nCODE:\n```\nlibrary(tabuSearch) # load tabuSearch package\n\nsource(\"functions.R\") # load the profit function\n\n# tabu search for bag prices:\n\nD=5 # dimension (number of prices)\n\nMaxPrice=1000\n\nDim=ceiling(log(MaxPrice,2)) # size of each price (=10)\n\nsize=D*Dim # total number of bits (=50)\n\ns=sample(0:1,size,replace=TRUE) # initial search\n\nintbin=function(x) # convert binary to integer\n{ sum(2^(which(rev(x==1))-1)) } # explained in Chapter\n\nbintbin=function(x) # convert binary to D prices\n{ # note: D and Dim need to be set outside this function\n  s=vector(length=D)\n  for(i in 1:D) # convert x into s:\n  { ini=(i-1)*Dim+1;end=ini+Dim-1\n    s[i]=intbin(x[ini:end])\n  }\n  return(s)\n}\n\nbprofit=function(x) # profit for binary x\n{ s=bintbin(x)\n  if(sum(s>MaxPrice)>0) f=-Inf # death penalty\n  else f=profit(s)\n  return(f)\n}\n\ncat(\"initial:\",bintbin(s),\"f:\",bprofit(s),\"\\n\")\n\ns=tabuSearch(size,iters=100,objFunc=bprofit,config=s,neigh=4,listSize=16,nRestarts=1)\n\nb=which.max(s$eUtilityKeep) # best index\n\ncat(\"best:\",bintbin(s$configKeep[b,]),\"f:\",s$eUtilityKeep[b],\n\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Lexicographic Hill Climbing for Multi-Objective Optimization in R\nDESCRIPTION: This code implements a lexicographic hill climbing algorithm for multi-objective optimization of bag prices. It defines the climbing function, evaluation function, and mutation operator, then runs the algorithm to find optimal prices.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_117\n\nLANGUAGE: R\nCODE:\n```\nsource(\"hill.R\") # load the blind search methods\n\nsource(\"mo-tasks.R\") # load MO bag prices task\n\nsource(\"lg-ga.R\") # load tournament function\n\n# lexicographic hill climbing, assumes minimization goal:\n\nlhclimbing=function(par,fn,change,lower,upper,control,\n\n...)\n\n{\n\nfor(i in 1:control$maxit)\n\n{\n\npar1=change(par,lower,upper)\n\nif(control$REPORT>0 &&(i==1||i%%control$REPORT==0))\n\ncat(\"i:\",i,\"s:\",par,\"f:\",eval(par),\"s'\",par1,\"f:\",\n\neval(par1),\"\\n\")\n\npop=rbind(par,par1) # population with 2 solutions\n\nI=tournament(pop,fn,k=2,n=1,m=2)\n\npar=pop[I,]\n\n}\n\nif(control$REPORT>=1) cat(\"best:\",par,\"f:\",eval(par),\"\\n\")\n\nreturn(list(sol=par,eval=eval(par)))\n\n}\n\n# lexico. hill climbing for all bag prices, one run:\n\nD=5; C=list(maxit=10000,REPORT=10000) # 10000 iterations\n\ns=sample(1:1000,D,replace=TRUE) # initial search\n\nichange=function(par,lower,upper) # integer value change\n\n{ hchange(par,lower,upper,rnorm,mean=0,sd=1) }\n\nLEXI=c(0.1,0.1) # explicitly defined lexico. tolerances\n\neval=function(x) c(-profit(x),produced(x))\n\nb=lhclimbing(s,fn=eval,change=ichange,lower=rep(1,D),\n\nupper=rep(1000,D),control=C)\n\ncat(\"final \",b$sol,\"f(\",profit(b$sol),\",\",produced(b$sol),\")\\n\")\n```\n\n----------------------------------------\n\nTITLE: Preparing Categorical Data for Naive Bayes in Scala\nDESCRIPTION: This snippet prepares categorical data in the correct format for a naive Bayes model. It uses one-hot encoding to represent categories as binary features suitable for the multinomial naive Bayes implementation in MLlib.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_105\n\nLANGUAGE: Scala\nCODE:\n```\nval dataNB = records.map { r =>\n  val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n  val label = trimmed(r.size - 1).toInt\n  val categoryIdx = categories(r(3))\n  val categoryFeatures = Array.ofDim[Double](numCategories)\n  categoryFeatures(categoryIdx) = 1.0\n  LabeledPoint(label, Vectors.dense(categoryFeatures))\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Top Movie Recommendations in Scala with Spark\nDESCRIPTION: This snippet displays the top 10 movie recommendations for a user, including the movie titles and predicted ratings.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_63\n\nLANGUAGE: Scala\nCODE:\n```\ntopKRecs.map(rating => (titles(rating.product), rating.rating)).foreach(println)\n```\n\n----------------------------------------\n\nTITLE: Extracting Latent Factor Vectors for Clustering from ALS Model\nDESCRIPTION: Extracts movie and user latent factors from the trained ALS model and converts them to MLlib Vectors. These vectors represent movies and users in the latent feature space and will be used as input for clustering.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_154\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.linalg.Vectors\nval movieFactors = alsModel.productFeatures.map { case (id, factor) => (id, Vectors.dense(factor)) }\nval movieVectors = movieFactors.map(_._2)\nval userFactors = alsModel.userFeatures.map { case (id, factor) => (id, Vectors.dense(factor)) }\nval userVectors = userFactors.map(_._2)\n```\n\n----------------------------------------\n\nTITLE: Lamarckian Evolutionary Algorithm for TSP in R\nDESCRIPTION: Implementation of a Lamarckian Evolutionary Algorithm for TSP, which incorporates local improvement during the evolution process by allowing acquired characteristics to be inherited.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_89\n\nLANGUAGE: R\nCODE:\n```\n# Lamarckian EA (LEA):\ncat(\"LEA run:\\n\")\nset.seed(12345) # for replicability\nEV=0; BEST=Inf; F=rep(NA,MAXIT) # reset these vars.\npSize=30;iters=ceiling((MAXIT-pSize)/(pSize-1))\nPTM=proc.time() # start clock\nLEA=oea(size=N,popSize=pSize,iters=iters,evalFunc=local_imp\n_tour,crossfunc=ox,mutfunc=insertion,REPORT=iters,elitism=1)\nsec=(proc.time()-PTM)[3] # get seconds elapsed\ncat(\"time elapsed:\",sec,\"\\n\")\nRES[,3]=F\n```\n\n----------------------------------------\n\nTITLE: Defining EDA Types in R\nDESCRIPTION: Creates four different types of Estimation Distribution Algorithms (EDA): UMDA, GCEDA, CVEDA, and DVEDA using the copulaedas package.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_46\n\nLANGUAGE: R\nCODE:\n```\nUMDA=CEDA(copula=\"indep\",margin=\"norm\"); UMDA@name=\"UMDA\"\nGCEDA=CEDA(copula=\"normal\",margin=\"norm\"); GCEDA@name=\"GCEDA\"\nCVEDA=VEDA(vine=\"CVine\",indepTestSigLevel=0.01,\ncopulas = c(\"normal\"),margin = \"norm\")\nCVEDA@name=\"CVEDA\"\nDVEDA=VEDA(vine=\"DVine\",indepTestSigLevel=0.01,\ncopulas = c(\"normal\"),margin = \"norm\")\nDVEDA@name=\"DVEDA\"\n```\n\n----------------------------------------\n\nTITLE: Helper Function for Creating Classification Metrics in Scala\nDESCRIPTION: This function takes labeled data and a classification model, then generates and returns the area under ROC curve metric. It's used to evaluate model performance across different parameter settings.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_108\n\nLANGUAGE: Scala\nCODE:\n```\ndef createMetrics(label: String, data: RDD[LabeledPoint], model: ClassificationModel) = {\n  val scoreAndLabels = data.map { point =>\n    (model.predict(point.features), point.label)\n  }\n  val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n  (label, metrics.areaUnderROC)\n}\n```\n\n----------------------------------------\n\nTITLE: Parsing CSV String with WHILE Loop in SQL\nDESCRIPTION: A procedural approach to parse a comma-separated string into individual values using a WHILE loop. This method sequentially processes each substring between commas.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_71\n\nLANGUAGE: SQL\nCODE:\n```\nDECLARE @str VARCHAR(8000) = ',123,456,789,';\nDECLARE @pos INTEGER = 1;\nDECLARE @nextpos INTEGER;\nDECLARE @value INTEGER;\nDECLARE @ResultTable TABLE (place INTEGER, value INTEGER);\n\nWHILE (@pos < LEN(@str))\nBEGIN\n    SET @nextpos = CHARINDEX(',', @str, @pos + 1);\n    IF @nextpos = 0 BREAK;\n    SET @value = CAST(SUBSTRING(@str, @pos + 1, @nextpos - @pos - 1) AS INTEGER);\n    INSERT INTO @ResultTable (place, value) VALUES ((@pos - 1) / 2 + 1, @value);\n    SET @pos = @nextpos;\nEND;\n```\n\n----------------------------------------\n\nTITLE: Feature Vector Creation for Linear Model in PySpark\nDESCRIPTION: Functions to extract features and labels from records, converting categorical variables to binary encodings and numeric features to float arrays using numpy. Creates labeled points for linear regression model training.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_120\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.mllib.regression import LabeledPoint\nimport numpy as np\n\ndef extract_features(record):\n  cat_vec = np.zeros(cat_len)\n  i = 0\n  step = 0\n  for field in record[2:9]:\n    m = mappings[i]\n    idx = m[field]\n    cat_vec[idx + step] = 1\n    i = i + 1\n    step = step + len(m)\n  num_vec = np.array([float(field) for field in record[10:14]])\n  return np.concatenate((cat_vec, num_vec))\n\ndef extract_label(record):\n  return float(record[-1])\n\ndata = records.map(lambda r: LabeledPoint(extract_label(r), extract_features(r)))\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with K-means Clustering Model in Spark MLlib\nDESCRIPTION: This snippet shows how to use a trained K-means model to make predictions on single and multiple input vectors using Spark MLlib.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_157\n\nLANGUAGE: scala\nCODE:\n```\nval movie1 = movieVectors.first\nval movieCluster = movieClusterModel.predict(movie1)\nprintln(movieCluster)\n\nval predictions = movieClusterModel.predict(movieVectors)\nprintln(predictions.take(10).mkString(\",\"))\n```\n\n----------------------------------------\n\nTITLE: Defining Project Dependencies for Spark Streaming Application in Scala\nDESCRIPTION: This snippet shows the SBT project definition file for a Scala Spark Streaming application. It specifies the project name, version, Scala version, and dependencies on Spark MLlib and Spark Streaming.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_223\n\nLANGUAGE: Scala\nCODE:\n```\nname := \"scala-spark-streaming-app\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.10.4\"\n\nlibraryDependencies += \"org.apache.spark\" %% \"spark-mllib\" % \"1.1.0\"\n\nlibraryDependencies += \"org.apache.spark\" %% \"spark-streaming\" % \"1.1.0\"\n```\n\n----------------------------------------\n\nTITLE: Evolutionary Algorithm for TSP in R\nDESCRIPTION: Implementation of a standard Evolutionary Algorithm for solving the TSP problem, using order-based operations like OX crossover and insertion mutation.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_88\n\nLANGUAGE: R\nCODE:\n```\n# EA:\ncat(\"EA run:\\n\")\nset.seed(12345) # for replicability\nEV=0; BEST=Inf; F=rep(NA,MAXIT) # reset these vars.\npSize=30;iters=ceiling((MAXIT-pSize)/(pSize-1))\nPTM=proc.time() # start clock\nOEA=oea(size=N,popSize=pSize,iters=iters,evalFunc=tour,crossfunc=ox,mutfunc=insertion,REPORT=iters,elitism=1)\nsec=(proc.time()-PTM)[3] # get seconds elapsed\ncat(\"time elapsed:\",sec,\"\\n\")\nRES[,2]=F\n```\n\n----------------------------------------\n\nTITLE: Step Size Parameter Testing for SGD in Scala\nDESCRIPTION: Evaluates different step sizes for SGD to determine optimal gradient update magnitude and impact on convergence.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_110\n\nLANGUAGE: scala\nCODE:\n```\nval stepResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n  val model = trainWithParams(scaledDataCats, 0.0, numIterations, new SimpleUpdater, param)\n  createMetrics(s\"$param step size\", scaledDataCats, model)\n}\nstepResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\n```\n\n----------------------------------------\n\nTITLE: Using Apply Functions in R\nDESCRIPTION: Demonstrates the use of sapply and apply functions to execute custom functions over vectors and matrices. Shows how to avoid explicit loops in R for more concise code.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_14\n\nLANGUAGE: R\nCODE:\n```\nsource(\"functions.R\") # load the code\nx=1:5 # show the factorial of 1:5\ncat(sapply(x,fact),\"\\n\") # fact is a function\nm=matrix(ncol=5,nrow=2)\nm[1,]=c(1,1,1,1,1) # very cheap bags\nm[2,]=c(414,404,408,413,395) # optimum\n# show profit for both price setups:\ny=apply(m,1,profit); print(y) # profit is a function\n```\n\n----------------------------------------\n\nTITLE: Creating Evaluation Function for Linear Regression Parameter Tuning\nDESCRIPTION: Defines a function to evaluate linear regression model performance with different parameter settings by training on a training set and calculating RMSLE on a test set.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_138\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate(train, test, iterations, step, regParam, regType, intercept):\n    model = LinearRegressionWithSGD.train(train, iterations, step, regParam=regParam, regType=regType, intercept=intercept)\n    tp = test.map(lambda p: (p.label, model.predict(p.features)))\n    rmsle = np.sqrt(tp.map(lambda (t, p): squared_log_error(t, p)).mean())\n    return rmsle\n```\n\n----------------------------------------\n\nTITLE: Applying Log Transformation to Target Variables in Spark MLlib\nDESCRIPTION: Transforms the target variables of LabeledPoint RDDs using numpy's log function to prepare data for regression model training with log-transformed targets.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_129\n\nLANGUAGE: python\nCODE:\n```\ndata_log = data.map(lambda lp: LabeledPoint(np.log(lp.label), lp.features))\n```\n\n----------------------------------------\n\nTITLE: Displaying and Visualizing the Pareto Front Results\nDESCRIPTION: Displays the results of the multi-objective optimization showing the optimal SVM models (Pareto front) with their hyperparameters, selected features, and performance metrics, and creates a visual plot of the trade-off between AUC and feature count.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_102\n\nLANGUAGE: R\nCODE:\n```\n# show the Pareto front:\n\nI=which(G$pareto.optimal)\n\nfor(i in I)\n\n{ x=G$par[i,]\n\nn=length(x)\n\ngamma=2^x[1]\n\nC=2^x[2]\n\nfeatures=round(x[3:n])\n\ninputs=which(features==1)\n\ncat(\"gamma:\",gamma,\"C:\",C,\"features:\",inputs,\"; f=(\",\n\n1-G$value[i,1],G$value[i,2],\")\\n\",sep=\" \")\n\n}\n\n# create PDF showing the Pareto front:\n\npdf(file=\"nsga-wine.pdf\",paper=\"special\",height=5,width=5)\n\npar(mar=c(4.0,4.0,0.1,0.1))\n\nSI=sort.int(G$value[I,1],index.return=TRUE)\n\nplot(1-G$value[SI$ix,1],G$value[SI$ix,2],xlab=\"AUC\",ylab=\"nr. features\",type=\"b\",lwd=2)\n\ndev.off()\n```\n\n----------------------------------------\n\nTITLE: Iteration Parameter Testing for SGD in Scala\nDESCRIPTION: Tests different numbers of iterations for Stochastic Gradient Descent to evaluate convergence impact on AUC performance.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_109\n\nLANGUAGE: scala\nCODE:\n```\nval iterResults = Seq(1, 5, 10, 50).map { param =>\n  val model = trainWithParams(scaledDataCats, 0.0, param, new SimpleUpdater, 1.0)\n  createMetrics(s\"$param iterations\", scaledDataCats, model)\n}\niterResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\n```\n\n----------------------------------------\n\nTITLE: Computing IDF and Transforming to TF-IDF Vectors in Spark MLlib\nDESCRIPTION: Creates an IDF instance, fits it to the term frequency vectors, and transforms them into TF-IDF vectors. The code then inspects the first TF-IDF vector to show how the values have been weighted by inverse document frequency.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_211\n\nLANGUAGE: scala\nCODE:\n```\nval idf = new IDF().fit(tf)\nval tfidf = idf.transform(tf)\nval v2 = tfidf.first.asInstanceOf[SV]\nprintln(v2.values.size)\nprintln(v2.values.take(10).toSeq)\nprintln(v2.indices.take(10).toSeq)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Data Distribution with Spark MLlib RowMatrix\nDESCRIPTION: This snippet computes summary statistics for movie and user factor vectors using Spark MLlib's RowMatrix to determine if normalization is needed before clustering.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_155\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\nval movieMatrix = new RowMatrix(movieVectors)\nval movieMatrixSummary = movieMatrix.computeColumnSummaryStatistics()\nval userMatrix = new RowMatrix(userVectors)\nval userMatrixSummary = \nuserMatrix.computeColumnSummaryStatistics()\nprintln(\"Movie factors mean: \" + movieMatrixSummary.mean)\nprintln(\"Movie factors variance: \" + movieMatrixSummary.variance)\nprintln(\"User factors mean: \" + userMatrixSummary.mean)\nprintln(\"User factors variance: \" + userMatrixSummary.variance)\n```\n\n----------------------------------------\n\nTITLE: Implementing Weighted-Formula Multi-Objective Optimization in R\nDESCRIPTION: This code implements a weighted-formula approach to multi-objective optimization using genetic algorithms from the genalg package. It handles binary, integer, and real-valued optimization tasks with different weight combinations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_64\n\nLANGUAGE: R\nCODE:\n```\nsource(\"mo-tasks.R\") # load multi-optimization tasks\n\nlibrary(genalg) # load genalg package\n\nset.seed(12345) # set for replicability\n\nstep=5 # number of weight combinations\nw=matrix(ncol=2,nrow=step) # weight combinations\nw[,1]=seq(1,0,length.out=step)\nw[,2]=1-w[,1]\n\nprint(\"Weight combinations:\")\nprint(w)\n\n# --- binary task:\nD=8 # 8 bits\neval=function(x) return(W[1]*sumbin(x)+W[2]*maxsin(x))\n\ncat(\"binary task:\\n\")\nfor(i in 1:step)\n{\n  W= -w[i,] # rbga.bin minimization goal: max. f1 and max. f2\n  G=rbga.bin(size=D,popSize=12,iters=100,zeroToOneRatio=1,\n              evalFunc=eval,elitism=1)\n  b=G$population[which.min(G$evaluations),] # best individual\n  cat(\"w\",i,\"best:\",b)\n  cat(\" f=(\",sumbin(b),\",\",round(maxsin(b),2),\")\",\"\\n\",sep=\"\")\n}\n\n# --- integer task:\nD=5 # 5 bag prices\neval=function(x) return(W[1]*profit(x)+W[2]*produced(x))\n\ncat(\"integer task:\\n\")\nres=matrix(nrow=nrow(w),ncol=ncol(w)) # for CSV files\nfor(i in 1:step)\n{\n  W=c(-w[i,1],w[i,2]) # rbga min. goal: max. f1 and min. f2\n  G=rbga(evalFunc=eval,stringMin=rep(1,D),stringMax=rep(1000,D),\n         popSize=20,iters=100)\n  b=round(G$population[which.min(G$evaluations),]) # best\n  cat(\"w\",i,\"best:\",b)\n  cat(\" f=(\",profit(b),\",\",produced(b),\")\",\"\\n\",sep=\"\")\n  res[i,]=c(profit(b),produced(b))\n}\n\nwrite.table(res,\"wf-bag.csv\",\n            row.names=FALSE,col.names=FALSE,sep=\" \")\n\n# --- real value task:\nD=8 # dimension\neval=function(x) return(sum(W*fes1(x)))\n\ncat(\"real value task:\\n\")\nfor(i in 1:step)\n{\n  W=w[i,] # rbga minimization goal\n  G=rbga(evalFunc=eval,stringMin=rep(0,D),stringMax=rep(1,D),\n         popSize=20,iters=100)\n  b=G$population[which.min(G$evaluations),] # best solution\n  cat(\"w\",i,\"best:\",round(b,2))\n  cat(\" f=(\",round(fes1(b)[1],2),\",\",round(fes1(b)[2],2),\")\",\n      \"\\n\",sep=\"\")\n  res[i,]=fes1(b)\n}\n\nwrite.table(res,\"wf-fes1.csv\",\n            row.names=FALSE,col.names=FALSE,sep=\" \")\n```\n\n----------------------------------------\n\nTITLE: Displaying and Visualizing DEoptim Optimization Results in R\nDESCRIPTION: This code snippet shows how to display, visualize, and export the results of differential evolution optimization using DEoptim. It includes creating a summary of the optimization object, generating a PDF plot of population evolution, and outputting the best parameter values found.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_43\n\nLANGUAGE: R\nCODE:\n```\nsummary(D)\n\npdf(\"DEoptim.pdf\",onefile=FALSE,width=5,height=9,\n\ncolormodel=\"gray\")\n\nplot(D,plot.type=\"storepop\")\n\ndev.off()\n\ncat(\"best:\",D$optim$bestmem,\"f:\",D$optim$bestval,\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Using Particle Swarm Optimization for ARIMA Model Parameter Estimation in R\nDESCRIPTION: This code applies particle swarm optimization to find optimal ARIMA model parameters for sunspot time series data. It evaluates the model using MAE, makes forecasts, and visualizes the results against other approaches.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_121\n\nLANGUAGE: R\nCODE:\n```\n# this solution assumes that file \"tsf.R\" has already been executed\n\nlibrary(pso) # load pso\n\n# evaluation function of arma coefficients:\n\nevalarma=function(s)\n\n{ a=suppressWarnings(arima(sunspots,order=c(AR,0,MA),fixed=s))\n\nR=a$residuals[INIT:length(sunspots)]\n\nR=maeres(R)\n\nif(is.nan(R)) R=Inf # death penalty\n\nreturn(maeres(R))\n\n}\n\nAR=2;MA=1\n\nmaxit=100; LP=50\n\nmeants=mean(sunspots);K=0.1*meants\n\nlower=c(rep(-1,(AR+MA)),meants-K)\n\nupper=c(rep(1,(AR+MA)),meants+K)\n\nC=list(maxit=maxit,s=LP,trace=10,REPORT=10)\n\nset.seed(12345) # set for replicability\n\nPSO=psoptim(rep(NA,length(lower)),fn=evalarma,\n\nlower=lower,upper=upper,control=C)\n\narima2=arima(sunspots,order=c(AR,0,MA),fixed=PSO$par)\n\nprint(arima2)\n\ncat(\"pso fit MAE=\",PSO$value,\"\\n\")\n\n# one-step ahead predictions:\n\nf3=rep(NA,forecasts)\n\nfor(h in 1:forecasts)\n\n{ # execute arima with fixed coefficients but with more in-samples:\n\narima1=arima(series[1:(LIN+h-1)],order=arima2$arma[c(1,3,2)],fixed=arima2$coef)\n\nf3[h]=forecast(arima1,h=1)$mean[1]\n\n}\n\ne3=maeres(outsamples-f3)\n\ntext3=paste(\"pso arima (MAE=\",round(e3,digits=1),\")\",sep=\"\")\n\n# show quality of one-step ahead forecasts:\n\nymin=min(c(outsamples,f1,f3))\n\nymax=max(c(outsamples,f1,f3))\n\npar(mar=c(4.0,4.0,0.1,0.1))\n\nplot(outsamples,ylim=c(ymin,ymax),type=\"b\",pch=1,\n\nxlab=\"time (years after 1980)\",ylab=\"values\",cex=0.8)\n\nlines(f1,lty=2,type=\"b\",pch=3,cex=0.5)\n\nlines(f3,lty=3,type=\"b\",pch=5,cex=0.5)\n\nlegend(\"topright\",c(\"sunspots\",text1,text3),lty=1:3,\n\npch=c(1,3,5))\n```\n\n----------------------------------------\n\nTITLE: Decision Tree Depth Testing with Entropy in Scala\nDESCRIPTION: Evaluates impact of tree depth on model performance using entropy impurity measure.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_113\n\nLANGUAGE: scala\nCODE:\n```\nval dtResultsEntropy = Seq(1, 2, 3, 4, 5, 10, 20).map { param =>\n  val model = trainDTWithParams(data, param, Entropy)\n  val scoreAndLabels = data.map { point =>\n    val score = model.predict(point.features)\n    (if (score > 0.5) 1.0 else 0.0, point.label)\n  }\n  val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n  (s\"$param tree depth\", metrics.areaUnderROC)\n}\ndtResultsEntropy.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\n```\n\n----------------------------------------\n\nTITLE: Reference Creation with Side Effects Example\nDESCRIPTION: Demonstrates how reference creation and side effects can lead to different results in call-by-value versus call-by-name evaluation strategies.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_38\n\nLANGUAGE: PCF\nCODE:\n```\nlet x = {a = ref 0, inc = λy.(x.a := !(x.a) + 1)} in (x.inc 0; x.inc 0; !(x.a))\n```\n\n----------------------------------------\n\nTITLE: Grouping Movies by User in Spark\nDESCRIPTION: Groups movie ratings by user ID to create a mapping of users to their actual movie interactions, which will be used as ground truth for APK calculations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_78\n\nLANGUAGE: scala\nCODE:\n```\nval userMovies = ratings.map{ case Rating(user, product, rating) => (user, product) }.groupBy(_._1)\n```\n\n----------------------------------------\n\nTITLE: Defining the Evaluation Function for SVM Model\nDESCRIPTION: Implements an evaluation function that takes hyperparameters (gamma, C) and feature selection binary flags, builds an SVM model, and returns the AUC metric and feature count for optimization by NSGA-II algorithm.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_100\n\nLANGUAGE: R\nCODE:\n```\n# evaluation function:\n\n# x is in the form c(Gamma,C,b1,b2,...,b11)\n\neval=function(x)\n\n{ n=length(x)\n\ngamma=2^x[1]\n\nC=2^x[2]\n\nfeatures=round(x[3:n])\n\ninputs=which(features==1)\n\nattributes=c(inputs,output)\n\n# divert console:\n\n# sink is used to avoid kernlab ksvm messages in a few cases\n\nsink(file=textConnection(\"rval\",\"w\",local = TRUE))\n\nM=mining(quality~.,d[H$tr,attributes],method=c(\"kfold\",3),model=\"svm\",search=gamma,mpar=c(C,NA))\n\nsink(NULL) # restores console\n\n# AUC for the internal 3-fold cross-validation:\n\nauc=as.numeric(mmetric(M,metric=\"AUC\"))\n\nauc1=1-auc # transform auc maximization into minimization goal\n\nreturn(c(auc1,length(inputs)))\n\n}\n```\n\n----------------------------------------\n\nTITLE: Finding Most Popular Product using JavaRDD in Spark\nDESCRIPTION: This snippet demonstrates how to find the most popular product from a JavaRDD of purchase data. It uses map and reduceByKey operations with PairFunction and Function2 implementations, followed by sorting to identify the product with the highest number of purchases.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nList<Tuple2<String, Integer>> pairs = data.map(new PairFunction <String[], String, Integer>() {\n@Override\npublic Tuple2 <String, Integer> call(String[] strings)throws Exception {\nreturn new Tuple2(strings[1], 1);\n}\n}).reduceByKey(new Function2 <Integer, Integer, Integer>() {\n@Override\npublic Integer call(Integer integer, Integer integer2)throws Exception {\nreturn integer + integer2;\n}\n}).collect();\n// finally we sort the result. Note we need to create a Comparator function,\n// that reverses the sort order.\nCollections.sort(pairs, new Comparator<Tuple2<String,Integer>>() {\n  @Override\n  public int compare(Tuple2<String, Integer> o1,Tuple2<String, Integer> o2) {\n    return -(o1._2() - o2._2());\n  }\n});\nString mostPopular = pairs.get(0)._1();\nint purchases = pairs.get(0)._2();\n```\n\n----------------------------------------\n\nTITLE: Evaluating Decision Tree Model with Log-Transformed Data\nDESCRIPTION: Creates predictions, transforms them back to the original scale with exp, and computes performance metrics for the decision tree model.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_134\n\nLANGUAGE: python\nCODE:\n```\npreds_log = dt_model_log.predict(data_dt_log.map(lambda p: p.features))\nactual_log = data_dt_log.map(lambda p: p.label)\ntrue_vs_predicted_dt_log = actual_log.zip(preds_log).map(lambda (t, p): (np.exp(t), np.exp(p)))\n\nmse_log_dt = true_vs_predicted_dt_log.map(lambda (t, p): squared_error(t, p)).mean()\nmae_log_dt = true_vs_predicted_dt_log.map(lambda (t, p): abs_error(t, p)).mean()\nrmsle_log_dt = np.sqrt(true_vs_predicted_dt_log.map(lambda (t, p): squared_log_error(t, p)).mean())\nprint \"Mean Squared Error: %2.4f\" % mse_log_dt\nprint \"Mean Absolue Error: %2.4f\" % mae_log_dt\nprint \"Root Mean Squared Log Error: %2.4f\" % rmsle_log_dt\nprint \"Non log-transformed predictions:\\n\" + str(true_vs_predicted_dt.take(3))\nprint \"Log-transformed predictions:\\n\" + str(true_vs_predicted_dt_log.take(3))\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing 20 Newsgroups Data in Spark\nDESCRIPTION: Scala code to load the 20 Newsgroups text files into Spark RDD and perform initial data count.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_196\n\nLANGUAGE: scala\nCODE:\n```\nval path = \"/PATH/20news-bydate-train/*\"\nval rdd = sc.wholeTextFiles(path)\nval text = rdd.map { case (file, text) => text }\nprintln(text.count)\n```\n\n----------------------------------------\n\nTITLE: Computing Mean Average Precision at K (MAPK) in Spark\nDESCRIPTION: Calculates the Mean Average Precision at K by joining predicted recommendations with actual user-movie interactions, computing APK for each user, and averaging the results across all users.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_79\n\nLANGUAGE: scala\nCODE:\n```\nval K = 10\nval MAPK = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) => \n  val actual = actualWithIds.map(_._2).toSeq\n  avgPrecisionK(actual, predicted, K)\n}.reduce(_ + _) / allRecs.count\nprintln(\"Mean Average Precision at K = \" + MAPK)\n```\n\n----------------------------------------\n\nTITLE: Comparing Optimization Methods with Statistical Analysis in R\nDESCRIPTION: This code performs statistical comparison of optimization methods. It calculates average best values, performs Mann-Whitney non-parametric tests, and creates a PDF plot comparing the performance of different methods across evaluations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_115\n\nLANGUAGE: R\nCODE:\n```\n# this edaRun might produces warnings or errors:\n\nsuppressWarnings(try(edaRun(UMDA,cprofit3,lower,upper),silent=TRUE))\n\nif(EV<MAXFN) # if stopped due to EvalStdDev\n\nF[(EV+1):MAXFN]=rep(F[EV],MAXFN-EV) # replace NAs\n\nRES[[2]][,R]=F # store all best values\n\nVAL[R,2]=F[MAXFN] # store best value at MAXFN\n\n}\n\n# compute average F result per method:\n\nMIN=Inf\n\nAV=matrix(nrow=MAXFN,ncol=length(Methods))\n\nfor(m in 1:length(Methods))\n\nfor(i in 1:MAXFN)\n\n{\n\nAV[i,m]=mean(RES[[m]][i,])\n\n# update MIN for plot (different than -Inf):\n\nif(AV[i,m]!=-Inf && AV[i,m]<MIN) MIN=AV[i,m]\n\n}\n\n# show results:\n\ncat(Methods,\"\\n\")\n\ncat(round(apply(VAL,2,mean),digits=0),\" (average best)\\n\")\n\n# Mann-Whitney non-parametric test:\n\np=wilcox.test(VAL[,1],VAL[,2],paired=TRUE)$p.value\n\ncat(\"p-value:\",round(p,digits=2),\"(<0.05)\\n\")\n\n# create PDF file:\n\npdf(\"comp-bagprices-constr2.pdf\",width=5,height=5,\n\npaper=\"special\")\n\npar(mar=c(4.0,4.0,1.8,0.6)) # reduce default plot margin\n\n# use a grid to improve clarity:\n\ng1=seq(1,MAXFN,length.out=500) # grid for lines\n\nMAX=max(AV)\n\nplot(g1,AV[g1,2],ylim=c(MIN,MAX),type=\"l\",lwd=2,\n\nmain=\"bag prices with constraint 2\",\n\nylab=\"average best\",xlab=\"number of evaluations\")\n\nlines(g1,AV[g1,1],lwd=2,lty=2)\n\nlegend(\"bottomright\",legend=rev(Methods),lwd=2,lty=1:4)\n\ndev.off() # close the PDF device\n```\n\n----------------------------------------\n\nTITLE: Splitting Data into Train and Test Sets in Spark\nDESCRIPTION: Demonstrates how to split a dataset into training and test sets using a 60-40 split ratio with a fixed random seed for reproducibility.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_115\n\nLANGUAGE: Scala\nCODE:\n```\nval trainTestSplit = scaledDataCats.randomSplit(Array(0.6, 0.4), 123)\nval train = trainTestSplit(0)\nval test = trainTestSplit(1)\n```\n\n----------------------------------------\n\nTITLE: Hill Climbing Implementation in R\nDESCRIPTION: Implements steepest ascent hill climbing optimization method with configurable parameters for iterations, reporting and optimization type (min/max).\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_113\n\nLANGUAGE: R\nCODE:\n```\nhclimbing=function(par,fn,change,lower,upper,control,\ntype=\"min\",...)\n{ fpar=fn(par,...)\nfor(i in 1:control$maxit)\n{\npar1=change(par,lower,upper)\nfpar1=fn(par1,...)\nif(control$N>0) # steepest ascent code\n{ for(j in 1:control$N-1)\n{ cand=change(par,lower,upper)\nfcand=fn(cand,...)\nif( (type==\"min\" && fcand<fpar1)\n|| (type==\"max\" && fcand>fpar1))\n{par1=cand;fpar1=fcand}\n}\n}\nif(control$REPORT>0 &&(i==1||i%%control$REPORT==0))\ncat(\"i:\",i,\"s:\",par,\"f:\",fpar,\"s'\",par1,\"f:\",fpar1,\"\\n\")\nif( (type==\"min\" && fpar1<fpar)\n|| (type==\"max\" && fpar1>fpar)) { par=par1;fpar=fpar1 }\n}\nif(control$REPORT>=1) cat(\"best:\",par,\"f:\",fpar,\"\\n\")\nreturn(list(sol=par,eval=fpar))\n}\n```\n\n----------------------------------------\n\nTITLE: Cross-validation for Selecting Optimal K in Movie Clustering\nDESCRIPTION: This snippet demonstrates how to select the optimal number of clusters (K) for movie data using cross-validation. It splits data into training and test sets, then evaluates different K values by measuring the WCSS cost on the test set.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_162\n\nLANGUAGE: scala\nCODE:\n```\nval trainTestSplitMovies = movieVectors.randomSplit(Array(0.6, 0.4), 123)\nval trainMovies = trainTestSplitMovies(0)\nval testMovies = trainTestSplitMovies(1)\nval costsMovies = Seq(2, 3, 4, 5, 10, 20).map { k => (k, KMeans.train(trainMovies, numIterations, k, numRuns).computeCost(testMovies)) }\nprintln(\"Movie clustering cross-validation:\")\ncostsMovies.foreach { case (k, cost) => println(f\"WCSS for K=$k id $cost%2.2f\") }\n```\n\n----------------------------------------\n\nTITLE: Committing the knowledge graph schema to OpenSPG\nDESCRIPTION: Command to commit the CsQa schema definition to the OpenSPG server for knowledge graph construction.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nknext schema commit\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Producer for Random Product Events in Scala\nDESCRIPTION: This code defines a StreamingProducer object that generates random product events and sends them over a network connection. It reads names from a CSV file, defines products with prices, and creates a function to generate random events.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_224\n\nLANGUAGE: Scala\nCODE:\n```\nobject StreamingProducer {\n\n  def main(args: Array[String]) {\n\n    val random = new Random()\n\n    // Maximum number of events per second\n    val MaxEvents = 6\n\n    // Read the list of possible names\n    val namesResource = this.getClass.getResourceAsStream(\"/names.csv\")\n    val names = scala.io.Source.fromInputStream(namesResource)\n      .getLines()\n      .toList\n      .head\n      .split(\",\")\n      .toSeq\n\n    // Generate a sequence of possible products\n    val products = Seq(\n      \"iPhone Cover\" -> 9.99,\n      \"Headphones\" -> 5.49,\n      \"Samsung Galaxy Cover\" -> 8.95,\n      \"iPad Cover\" -> 7.49\n    )\n\n    /** Generate a number of random product events */\n    def generateProductEvents(n: Int) = {\n      (1 to n).map { i =>\n        val (product, price) = products(random.nextInt(products.size))\n        val user = random.shuffle(names).head\n        (user, product, price)\n      }\n    }\n\n    // create a network producer\n    val listener = new ServerSocket(9999)\n    println(\"Listening on port: 9999\")\n\n    while (true) {\n      val socket = listener.accept()\n      new Thread() {\n        override def run = {\n          println(\"Got client connected from: \" + socket.getInetAddress)\n          val out = new PrintWriter(socket.getOutputStream(), true)\n\n          while (true) {\n            Thread.sleep(1000)\n            val num = random.nextInt(MaxEvents)\n            val productEvents = generateProductEvents(num)\n            productEvents.foreach{ event =>\n              out.write(event.productIterator.mkString(\",\"))\n              out.write(\"\\n\")\n            }\n            out.flush()\n            println(s\"Created $num events...\")\n          }\n          socket.close()\n        }\n      }.start()\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Plotting Linear Regression Metrics in Spark\nDESCRIPTION: This snippet uses matplotlib to plot the RMSLE metric against the number of iterations for a linear regression model. It uses a logarithmic scale for the x-axis to improve visualization.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_140\n\nLANGUAGE: Python\nCODE:\n```\nplot(params, metrics)\nfig = matplotlib.pyplot.gcf()\npyplot.xscale('log')\n```\n\n----------------------------------------\n\nTITLE: Optimization Methods Implementation in R\nDESCRIPTION: Implementation of three optimization methods (Monte Carlo search, Hill Climbing, and SANN) with result storage in RES matrix. Each method is initialized with evaluation counters and runs for a maximum number of iterations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_36\n\nLANGUAGE: R\nCODE:\n```\n# monte carlo:\nmcsearch(MAXIT,lower=lower,upper=upper,FUN=crastrigin)\nRES[[1]][,R]=F\n\n# hill climbing:\nEV=0; BEST=Inf; F=rep(NA,MAXIT)\nhclimbing(s,crastrigin,change=rchange1,lower=lower,\nupper=upper,control=CHILL,type=\"min\")\nRES[[2]][,R]=F\n\n# SANN:\nEV=0; BEST=Inf; F=rep(NA,MAXIT)\noptim(s,crastrigin,method=\"SANN\",gr=rchange2,control=CSANN)\nRES[[3]][,R]=F\n```\n\n----------------------------------------\n\nTITLE: Simulated Annealing for Bag Prices and Sphere Optimization\nDESCRIPTION: Combined implementation of simulated annealing for both bag price optimization and sphere function minimization. Includes custom change functions and control parameters.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_32\n\nLANGUAGE: R\nCODE:\n```\nsource(\"hill.R\") # load the hchange method\nsource(\"functions.R\") # load the profit function\n\neval=function(x) -profit(x) # optim minimizes!\n\nD=5; C=list(maxit=10000,temp=1000,trace=TRUE,REPORT=10000)\ns=sample(1:1000,D,replace=TRUE) # initial search\n\nichange=function(par) # integer value change\n{ D=length(par)\nhchange(par,lower=rep(1,D),upper=rep(1000,D),rnorm,mean=0,\nsd=1)\n}\n\ns=optim(s,eval,gr=ichange,method=\"SANN\",control=C)\ncat(\"best:\",s$par,\"profit:\",abs(s$value),\"\\n\")\n\nsphere=function(x) sum(x^2)\nD=2; C=list(maxit=10000,temp=1000,trace=TRUE,REPORT=10000)\ns=runif(D,-5.2,5.2) # initial search\n\ns=optim(s,sphere,method=\"SANN\",control=C)\ncat(\"best:\",s$par,\"f:\",s$value,\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Using Table Expressions Instead of VIEWs for One-time Operations\nDESCRIPTION: SQL query that uses a table expression (inline VIEW) to combine detail and aggregate data without creating a permanent VIEW object in the database.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_61\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT sp.stock_id, sp.date, sp.price,\n       sps.max_price, sps.min_price, sps.avg_price,\n       (sp.price - sps.avg_price) AS price_diff\nFROM StockPrices sp,\n     (SELECT stock_id,\n             MAX(price) AS max_price,\n             MIN(price) AS min_price,\n             AVG(price) AS avg_price\n      FROM StockPrices\n      GROUP BY stock_id) AS sps\nWHERE sp.stock_id = sps.stock_id\nORDER BY sp.stock_id, sp.date;\n```\n\n----------------------------------------\n\nTITLE: Assigning Movies to Clusters and Computing Distances in Scala\nDESCRIPTION: This snippet joins movie data with factor vectors, assigns each movie to a cluster using a previously trained K-means model, and computes the distance from each movie to its cluster center using a Breeze vector computation function.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_159\n\nLANGUAGE: scala\nCODE:\n```\nval titlesWithFactors = titlesAndGenres.join(movieFactors)\nval moviesAssigned = titlesWithFactors.map { case (id, ((title, genres), vector)) => \n  val pred = movieClusterModel.predict(vector)\n  val clusterCentre = movieClusterModel.clusterCenters(pred)\n  val dist = computeDistance(DenseVector(clusterCentre.toArray), DenseVector(vector.toArray))\n  (id, title, genres.mkString(\" \"), pred, dist) \n}\nval clusterAssignments = moviesAssigned.groupBy { case (id, title, genres, cluster, dist) => cluster }.collectAsMap\n```\n\n----------------------------------------\n\nTITLE: Creating Item Factor Matrix for Recommendations in Scala\nDESCRIPTION: Collects item factors from an ALS model and creates a matrix for computing recommendations. This allows matrix multiplication with user vectors to generate predictions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_75\n\nLANGUAGE: scala\nCODE:\n```\nval itemFactors = model.productFeatures.map { case (id, factor) => factor }.collect()\nval itemMatrix = new DoubleMatrix(itemFactors)\nprintln(itemMatrix.rows, itemMatrix.columns)\n```\n\n----------------------------------------\n\nTITLE: Monte Carlo Search Implementation in R\nDESCRIPTION: Performs Monte Carlo search with different sample sizes (N) and conducts statistical tests to compare results across runs. Uses Rastrigin test function and boxplot visualization.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_112\n\nLANGUAGE: R\nCODE:\n```\nD=30\nRuns=30\nN=10^c(2,3,4) # number of samples\nS=matrix(nrow=Runs,ncol=length(N))\nfor(j in 1:length(N)) # cycle all number of samples\nfor(i in 1:Runs) # cycle all runs\nS[i,j]=mcsearch(N[j],rep(-5.2,D),rep(5.2,D),\nrastrigin,\"min\")$eval\n```\n\n----------------------------------------\n\nTITLE: Implementing Cycle Crossover (CX) Operator for Permutation Problems in R\nDESCRIPTION: This code implements the cycle crossover (CX) operator for permutation-based evolutionary algorithms. The operator creates child solutions by preserving cycles of positions between two parent solutions with a simple demonstration.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_119\n\nLANGUAGE: R\nCODE:\n```\n# cycle crossover (CX) operator:\n\n# m is a matrix with 2 parent x ordered solutions\n\ncx=function(m)\n\n{\n\nN=ncol(m)\n\nc=matrix(rep(NA,N*2),ncol=N)\n\nstop=FALSE\n\nk=1\n\nALL=1:N\n\nwhile(length(ALL)>0)\n\n{\n\ni=ALL[1]\n\n# perform a cycle:\n\nbase=m[1,i];vi=m[2,i]\n\nI=i\n\nwhile(vi!=base)\n\n{\n\ni=which(m[1,]==m[2,i])\n\nvi=m[2,i]\n\nI=c(I,i)\n\n}\n\nALL=setdiff(ALL,I)\n\nif(k%%2==1) c[,I]=m[,I] else c[,I]=m[2:1,I]\n\nk=k+1\n\n}\n\nreturn(c)\n\n}\n\n# example of CX operator:\n\nm=matrix(ncol=9,nrow=2)\n\nm[1,]=1:9\n\nm[2,]=c(9,8,1,2,3,4,5,6,7)\n\nprint(m)\n\nprint(\"---\")\n\nprint(cx(m))\n```\n\n----------------------------------------\n\nTITLE: Implementing Squared Error Function in Python\nDESCRIPTION: Function to calculate squared error between predicted and actual values for MSE calculation.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_123\n\nLANGUAGE: python\nCODE:\n```\ndef squared_error(actual, pred):\n    return (pred - actual)**2\n```\n\n----------------------------------------\n\nTITLE: Creating and manipulating a table with auto-numbering\nDESCRIPTION: Example of creating a table with an auto-number column and inserting data into it. This shows how auto-numbering sequentially assigns numbers and leaves gaps when rows are deleted.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE ... (id INT IDENTITY, ...)\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Data Producer in Scala\nDESCRIPTION: Creates a data producer that generates synthetic linear regression data using random feature vectors and a fixed weight vector. The producer sends data points over a network socket at a configurable rate.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_230\n\nLANGUAGE: scala\nCODE:\n```\nobject StreamingModelProducer {\n  import breeze.linalg._\n\n  def main(args: Array[String]) {\n    // Maximum number of events per second\n    val MaxEvents = 100\n    val NumFeatures = 100\n\n    val random = new Random()\n\n    /** Function to generate a normally distributed dense vector */\n    def generateRandomArray(n: Int) = Array.tabulate(n)(_ => random.nextGaussian())\n\n    // Generate a fixed random model weight vector\n    val w = new DenseVector(generateRandomArray(NumFeatures))\n    val intercept = random.nextGaussian() * 10\n\n    /** Generate a number of random data events*/\n    def generateNoisyData(n: Int) = {\n      (1 to n).map { i =>\n        val x = new DenseVector(generateRandomArray(NumFeatures))\n        val y: Double = w.dot(x)\n        val noisy = y + intercept\n        (noisy, x)\n      }\n    }\n\n    // create a network producer\n    val listener = new ServerSocket(9999)\n    println(\"Listening on port: 9999\")\n\n    while (true) {\n      val socket = listener.accept()\n      new Thread() {\n        override def run = {\n          println(\"Got client connected from: \" + socket.getInetAddress)\n          val out = new PrintWriter(socket.getOutputStream(), true)\n\n          while (true) {\n            Thread.sleep(1000)\n            val num = random.nextInt(MaxEvents)\n            val data = generateNoisyData(num)\n            data.foreach { case (y, x) =>\n              val xStr = x.data.mkString(\",\")\n              val eventStr = s\"$y\\t$xStr\"\n              out.write(eventStr)\n              out.write(\"\\n\")\n            }\n            out.flush()\n            println(s\"Created $num events...\")\n          }\n          socket.close()\n        }\n      }.start()\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Eigenfaces in Python\nDESCRIPTION: This function plots a gallery of Eigenfaces (principal components) as images using matplotlib.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_185\n\nLANGUAGE: python\nCODE:\n```\ndef plot_gallery(images, h, w, n_row=2, n_col=5):\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        plt.imshow(images[:, i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(\"Eigenface %d\" % (i + 1), size=12)\n        plt.xticks(())\n        plt.yticks(())\n```\n\n----------------------------------------\n\nTITLE: Implementing Cosine Similarity Function in Scala\nDESCRIPTION: This function computes the cosine similarity between two vectors represented as DoubleMatrix objects from the JBLAS library.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_65\n\nLANGUAGE: Scala\nCODE:\n```\ndef cosineSimilarity(vec1: DoubleMatrix, vec2: DoubleMatrix): Double = {\n  vec1.dot(vec2) / (vec1.norm2() * vec2.norm2())\n}\n```\n\n----------------------------------------\n\nTITLE: Comparing PCA and SVD Results in Scala\nDESCRIPTION: This function approximately compares two arrays to check if PCA and SVD results are equivalent, ignoring sign and using a small tolerance for floating-point comparisons.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_188\n\nLANGUAGE: scala\nCODE:\n```\ndef approxEqual(array1: Array[Double], array2: Array[Double], tolerance: Double = 1e-6): Boolean = {\n  // note we ignore sign of the principal component / singular vector elements\n  val bools = array1.zip(array2).map { case (v1, v2) => if (math.abs(math.abs(v1) - math.abs(v2)) > 1e-6) false else true }\n  bools.fold(true)(_ & _)\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Movie Titles and Assigned Genres from MovieLens Data\nDESCRIPTION: Creates an RDD containing movie IDs, titles, and assigned genres by parsing the movie data and using the genre map to convert numerical genre indices to text. This helps create more readable output for cluster evaluation.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_152\n\nLANGUAGE: scala\nCODE:\n```\nval titlesAndGenres = movies.map(_.split(\"\\\\|\")).map { array =>\n  val genres = array.toSeq.slice(5, array.size)\n  val genresAssigned = genres.zipWithIndex.filter { case (g, idx) => \n    g == \"1\" \n  }.map { case (g, idx) => \n    genreMap(idx.toString) \n  }\n  (array(0).toInt, (array(1), genresAssigned))\n}\nprintln(titlesAndGenres.first)\n```\n\n----------------------------------------\n\nTITLE: Computing and Printing Model Metrics in Scala\nDESCRIPTION: Calculates Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) for each model per batch and prints the results to the console using Spark Streaming's foreachRDD function.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_235\n\nLANGUAGE: Scala\nCODE:\n```\npredsAndTrue.foreachRDD { (rdd, time) =>\n  val mse1 = rdd.map { case (err1, err2) => err1 * err1 }.mean()\n  val rmse1 = math.sqrt(mse1)\n  val mse2 = rdd.map { case (err1, err2) => err2 * err2 }.mean()\n  val rmse2 = math.sqrt(mse2)\n  println(\n    s\"\"\"\n       |-------------------------------------------\n       |Time: $time\n       |-------------------------------------------\n     \"\"\".stripMargin)\n  println(s\"MSE current batch: Model 1: $mse1; Model 2: $mse2\")\n  println(s\"RMSE current batch: Model 1: $rmse1; Model 2: $rmse2\")\n  println(\"...\\n\")\n}\n```\n\n----------------------------------------\n\nTITLE: Computing Document Similarity with Breeze Linear Algebra\nDESCRIPTION: Calculates cosine similarity between two randomly selected hockey documents using Breeze linear algebra library for vector operations\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_216\n\nLANGUAGE: scala\nCODE:\n```\nimport breeze.linalg._\nval hockey1 = hockeyTfIdf.sample(true, 0.1, 42).first.asInstanceOf[SV]\nval breeze1 = new SparseVector(hockey1.indices, hockey1.values, hockey1.size)\nval hockey2 = hockeyTfIdf.sample(true, 0.1, 43).first.asInstanceOf[SV]\nval breeze2 = new SparseVector(hockey2.indices, hockey2.values, hockey2.size)\nval cosineSim = breeze1.dot(breeze2) / (norm(breeze1) * norm(breeze2))\nprintln(cosineSim)\n```\n\n----------------------------------------\n\nTITLE: Starting Spark Shell\nDESCRIPTION: Command to start Spark shell with 4GB driver memory allocation\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_89\n\nLANGUAGE: shell\nCODE:\n```\n./bin/spark-shell --driver-memory 4g\n```\n\n----------------------------------------\n\nTITLE: Optimizing Bag Prices Using Domain-Knowledge and Blind Search in R\nDESCRIPTION: R script that optimizes bag prices using domain knowledge (prices can be optimized independently) combined with the blind search approach. It defines an auxiliary function for optimizing one bag type at a time.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_19\n\nLANGUAGE: r\nCODE:\n```\n### bag-blind.R file ###\n\nsource(\"blind.R\") # load the blind search methods\nsource(\"functions.R\") # load profit(), cost() and sales()\n\n# auxiliary function that sets the optimum price for\n# one bag type (D), assuming an independent influence of\n# a particular price on the remaining bag prices:\nibag=function(D) # D - type of bag\n{ x=1:1000 # price for each bag type\n# set search space for one bag:\nsearch=matrix(ncol=5,nrow=1000)\nsearch[]=1; search[,D]=x\nS1=fsearch(search,profit,\"max\")\nS1$sol[D] # best price\n}\n\n# compute the best price for all bag types:\nS=sapply(1:5,ibag)\n\n# show the optimum solution:\ncat(\"optimum s:\",S,\"f:\",profit(S),\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Using query expression with auto-numbering\nDESCRIPTION: Example showing how auto-numbering can be problematic when used with query expressions, as the physical order of results affects the numbering sequence.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\nINSERT INTO Foobar (auto_column, ...) SELECT ... FROM ...\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating Streaming Models in Scala\nDESCRIPTION: Trains two streaming linear regression models on the same input stream and creates a new DStream containing error rates for each model using Spark Streaming's transform function.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_234\n\nLANGUAGE: Scala\nCODE:\n```\nmodel1.trainOn(labeledStream)\nmodel2.trainOn(labeledStream)\n\nval predsAndTrue = labeledStream.transform { rdd =>\n  val latest1 = model1.latestModel()\n  val latest2 = model2.latestModel()\n  rdd.map { point =>\n    val pred1 = latest1.predict(point.features)\n    val pred2 = latest2.predict(point.features)\n    (pred1 - point.label, pred2 - point.label)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Transforming Input Stream to LabeledPoints in Scala\nDESCRIPTION: Converts incoming stream data into LabeledPoint objects for use in Spark MLlib streaming regression models.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_233\n\nLANGUAGE: Scala\nCODE:\n```\nval labeledStream = stream.map { event =>\n  val split = event.split(\"\\t\")\n  val y = split(0).toDouble\n  val features = split(1).split(\",\").map(_.toDouble)\n  LabeledPoint(label = y, features = Vectors.dense(features))\n}\n```\n\n----------------------------------------\n\nTITLE: Vector Manipulation and Statistical Tests in R\nDESCRIPTION: This snippet showcases various vector operations in R, including random sampling, sorting, and indexing. It also demonstrates the use of statistical functions like t-test for comparing means of two samples.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_7\n\nLANGUAGE: R\nCODE:\n```\n> x=sample(1:10,5,replace=TRUE) # 5 random samples from 1 to 10\n# with replacement\n> print(x) # show x\n[1] 10 5 5 1 2\n\n> print(min(x)) # show min of x\n[1] 1\n\n> print(which.min(x)) # show index of x that contains min\n[1] 4\n\n> print(sort(x,decreasing=TRUE)) # show x in decreasing order\n[1] 10 5 5 2 1\n\n> y=seq(0,20,by=2); print(y) # y = 0, 2, ..., 20\n[1] 0 2 4 6 8 10 12 14 16 18 20\n\n> print(y[x]) # show y[x]\n[1] 18 8 8 0 2\n\n> print(y[-x]) # - means indexes excluded from y\n[1] 4 6 10 12 14 16 20\n\n> x=runif(5,0.0,10.0);print(x) # 5 uniform samples from 0 to 10\n[1] 1.011359 1.454996 6.430331 9.395036 6.192061\n\n> y=rnorm(5,10.0,1.0);print(y) # normal samples (mean 10, std 1)\n[1] 10.601637 9.231792 9.548483 9.883687 9.591727\n\n> t.test(x,y) # t-student paired test\nWelch Two Sample t-test\n\ndata: x and y\nt = -3.015, df = 4.168, p-value = 0.03733\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -9.2932638 -0.4561531\nsample estimates:\nmean of x mean of y \n 4.896757  9.771465\n```\n\n----------------------------------------\n\nTITLE: Implementing Sphere Function Optimization with Differential Evolution in R\nDESCRIPTION: Implementation of differential evolution to minimize the sphere function in 2 dimensions using the DEoptim package. The code configures the optimization parameters and suppresses warnings during execution.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_42\n\nLANGUAGE: R\nCODE:\n```\n### sphere-DEoptim.R file ###\n\nlibrary(DEoptim) # load DEoptim\n\nsphere=function(x) sum(x^2)\n\nD=2\n\nmaxit=100\n\nset.seed(12345) # set for replicability\n\nC=DEoptim.control(strategy=1,NP=5,itermax=maxit,CR=0.9,F=0.8,\n\ntrace=25,storepopfrom=1,storepopfreq=1)\n\n# perform the optimization:\n\nD=suppressWarnings(DEoptim(sphere,rep(-5.2,D),rep(5.2,D),\n\ncontrol=C))\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Control Flow in R\nDESCRIPTION: Examples of if-else statements, switch, while loop, and for loops in R. Showcases various control flow constructs and logical operations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_10\n\nLANGUAGE: R\nCODE:\n```\nx=0; if(x>0) cat(\"positive\\n\") else if(x==0) cat(\"neutral\\n\") else cat(\"negative\\n\")\n\nif(xor(x,1)) cat(\"XOR TRUE\\n\") else cat(\"XOR FALSE\\n\")\n\nprint(switch(3,\"a\",\"b\",\"c\")) # numeric switch example\n\nx=1; while(x<3) { print(x); x=x+1;} # while example\n\nfor(i in 1:3) print(2*i) # for example #1\n\nfor(i in c(\"a\",\"b\",\"c\")) print(i) # for example #2\n\nfor(i in 1:10) if(i%%3==0) print(i) # for example #3\n\n# character switch example:\nvar=\"sin\";x=1:3;y=switch(var,cos=cos(x),sin=sin(x))\ncat(\"the\",var,\"of\",x,\"is\",round(y,digits=3),\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Binary Optimization using Tabu Search in R\nDESCRIPTION: Implementation of Tabu Search for binary optimization tasks including the sum of bits and maximum sine function. The code uses the tabuSearch R package to optimize binary strings with specific parameters for neighborhood size, tabu list length, and restart strategy.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_33\n\nLANGUAGE: R\nCODE:\n```\nlibrary(tabuSearch) # load tabuSearch package\n\n# tabu search for sum of bits:\n\nsumbin=function(x) (sum(x)) # sum of bits\n\nD=8 # dimension\n\ns=rep(0,D) # c(0,0,0,0,...)\n\ns=tabuSearch(D,iters=2,objFunc=sumbin,config=s,neigh=2,\n\nlistSize=4,nRestarts=1)\n\nb=which.max(s$eUtilityKeep) # best index\n\ncat(\"best:\",s$configKeep[b,],\"f:\",s$eUtilityKeep[b],\"\\n\")\n\n# tabu search for max sin:\n\nintbin=function(x) sum(2^(which(rev(x==1))-1))\n\nmaxsin=function(x) # max sin (explained in Chapter )\n{ D=length(x);x=intbin(x); return(sin(pi*(as.numeric(x))/(2^D))) }\n\nD=8\n\ns=rep(0,D) # c(0,0,0,0,...)\n\ns=tabuSearch(D,iters=2,objFunc=maxsin,config=s,neigh=2,\n\nlistSize=4,nRestarts=1)\n\nb=which.max(s$eUtilityKeep) # best index\n\ncat(\"best:\",s$configKeep[b,],\"f:\",s$eUtilityKeep[b],\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Plotting Area of Evolutionary Algorithm Solution in R\nDESCRIPTION: Creates a PDF plot of the polygon area for an evolutionary algorithm's best solution to the Traveling Salesman Problem. The code generates a gray-filled polygon with a thicker border line based on the data points in the solution.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_93\n\nLANGUAGE: R\nCODE:\n```\npdf(\"qa-ea-area.pdf\",paper=\"special\")\n\npar(mar=c(0.0,0.0,0.0,0.0))\n\nPEA=poly(Data[c(b,b[1]),])\n\nplot(PEA,col=\"gray\")\n\nlines(Data[c(b,b[1]),],lwd=2)\n\ndev.off()\n```\n\n----------------------------------------\n\nTITLE: Computing Factorial in PCF\nDESCRIPTION: Example showing the step-by-step reduction of factorial computation for number 3 using PCF's small-step semantics. Demonstrates function application, recursion, and conditional evaluation.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_16\n\nLANGUAGE: PCF\nCODE:\n```\n(fix f fun n -> ifz n then 1 else n * (f (n - 1))) 3\n  (fun n -> ifz n then 1 else n * ((fix f fun n -> ifz n then 1 else n * (f (n - 1))) (n - 1))) 3\n  ifz 3 then 1 else 3 * ((fix f fun n -> ifz n then 1 else n * (f (n - 1))) (3 - 1))\n  3 * ((fix f fun n -> ifz n then 1 else n * (f (n - 1))) (3 - 1))\n  3 * ((fix f fun n -> ifz n then 1 else n * (f (n - 1))) 2)\n  3 * ((fun n -> ifz n then 1 else n * ((fix f fun n -> ifz n then 1 else n * (f (n - 1))) (n - 1))) 2)\n  3 * (ifz 2 then 1 else 2 * ((fix f fun n -> ifz n then 1 else n * (f (n - 1))) (2 - 1)))\n  3 * (2 * ((fix f fun n -> ifz n then 1 else n * (f (n - 1))) (2 - 1)))\n  3 * (2 * ((fix f fun n -> ifz n then 1 else n * (f (n - 1))) 1))\n  3 * (2 * ((fun n -> ifz n then 1 else n * ((fix f fun n -> ifz n then 1 else n * (f (n - 1))) (n - 1))) 1))\n  3 * (2 * (ifz 1 then 1 else 1 * ((fix f fun n -> ifz n then 1 else n * (f (n - 1))) (1 - 1))))\n  3 * (2 * (1 * ((fix f fun n -> ifz n then 1 else n * (f (n - 1))) (1 - 1))))\n  3 * (2 * (1 * ((fix f fun n -> ifz n then 1 else n * (f (n - 1))) 0)))\n  3 * (2 * (1 * ((fun n -> ifz n then 1 else n * ((fix f fun n -> ifz n then 1 else n * (f (n - 1))) (n - 1))) 0)))\n  3 * (2 * (1 * ((ifz 0 then 1 else 0 * ((fix f fun n -> ifz n then 1 else n * (f (n - 1))) (0 - 1))))))\n  3 * (2 * (1 * 1))   3 * (2 * 1)   3 * 2   6\n```\n\n----------------------------------------\n\nTITLE: Importing and Initializing HashingTF for TF-IDF in Spark MLlib\nDESCRIPTION: Sets up the necessary imports and initializes a HashingTF instance with a defined dimension parameter to transform tokens into term frequency vectors. The code uses a smaller dimension (2^18) to avoid hash collisions while being memory efficient.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_209\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.linalg.{ SparseVector => SV }\nimport org.apache.spark.mllib.feature.HashingTF\nimport org.apache.spark.mllib.feature.IDF\nval dim = math.pow(2, 18).toInt\nval hashingTF = new HashingTF(dim)\nval tf = hashingTF.transform(tokens)\ntf.cache\n```\n\n----------------------------------------\n\nTITLE: Comparison Framework for Local Search Methods in R\nDESCRIPTION: Setup for comparing different optimization methods (Monte Carlo, Hill Climbing, and Simulated Annealing) on the Rastrigin benchmark function. The code defines a common evaluation tracking system and prepares for multiple runs of each algorithm with consistent parameter settings.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_35\n\nLANGUAGE: R\nCODE:\n```\nsource(\"hill.R\") # get hchange\nsource(\"blind.R\") # get fsearch\nsource(\"montecarlo.R\") # get mcsearch\nlibrary(rminer) # get meanint\n\n# comparison setup:\ncrastrigin=function(x)\n{ f=10*length(x)+sum(x^2-10*cos(2*pi*x))\n  # global assignment code: <<-\n  EV<<-EV+1 # increase evaluations\n  if(f<BEST) BEST<<-f\n  if(EV<=MAXIT) F[EV]<<-BEST\n  return(f)\n}\n\nRuns=50; D=20; MAXIT=10000\nlower=rep(-5.2,D);upper=rep(5.2,D)\n\nrchange1=function(par,lower,upper) # change for hclimbing\n{ hchange(par,lower=lower,upper=upper,rnorm,\n  mean=0,sd=0.5,round=FALSE) }\n\nrchange2=function(par) # change for optim\n{ hchange(par,lower=lower,upper=upper,rnorm,\n  mean=0,sd=0.5,round=FALSE) }\n\nCHILL=list(maxit=MAXIT,REPORT=0)\nCSANN=list(maxit=MAXIT,temp=10,trace=FALSE)\n\nMethods=c(\"monte carlo\",\"hill climbing\",\"simulated annealing\")\n\n# run all optimizations and store results:\nRES=vector(\"list\",length(Methods)) # all results\nfor(m in 1:length(Methods))\n  RES[[m]]=matrix(nrow=MAXIT,ncol=Runs)\n\nfor(R in 1:Runs) # cycle all runs\n{ s=runif(D,-5.2,5.2) # initial search point\n  EV=0; BEST=Inf; F=rep(NA,MAXIT) # reset these vars.\n```\n\n----------------------------------------\n\nTITLE: Implementing NSGA-II for Integer Optimization Task in R\nDESCRIPTION: This code snippet sets up and runs NSGA-II for an integer optimization task with 5 dimensions (bag prices) and 2 objectives. It includes visualization of the Pareto front evolution and comparison with weighted formula results.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_71\n\nLANGUAGE: R\nCODE:\n```\nD=5 # 5 bag prices\n# eval: transform objectives into minimization goal\neval=function(x) c(-profit(x),produced(x))\n\ncat(\"integer task:\\n\")\nG=nsga2(fn=eval,idim=5,odim=m,\nlower.bounds=rep(1,D),upper.bounds=rep(1000,D),\npopsize=20,generations=1:100)\n\n# show best individuals:\nI=which(G[[100]]$pareto.optimal)\nfor(i in I)\n{\n  x=round(G[[100]]$par[i,])\n  cat(x,\" f=(\",profit(x),\",\",produced(x),\")\",\"\\n\",sep=\" \")\n}\n\n# create PDF with Pareto front evolution:\npdf(file=\"nsga-bag.pdf\",paper=\"special\",height=5,width=5)\npar(mar=c(4.0,4.0,0.1,0.1))\nI=1:100\nfor(i in I)\n{ P=G[[i]]$value # objectives f1 and f2\n  P[,1]=-1*P[,1] # show positive f1 values\n  # color from light gray (75) to dark (1):\n  COL=paste(\"gray\",round(76-i*0.75),sep=\"\")\n  if(i==1) plot(P,xlim=c(-500,44000),ylim=c(0,140),\n                xlab=\"f1\",ylab=\"f2\",cex=0.5,col=COL)\n  Pareto=P[G[[i]]$pareto.optimal,]\n  # sort Pareto according to x axis:\n  I=sort.int(Pareto[,1],index.return=TRUE)\n  Pareto=Pareto[I$ix,]\n  points(P,type=\"p\",pch=1,cex=0.5,col=COL)\n  lines(Pareto,type=\"l\",cex=0.5,col=COL)\n}\ndev.off()\n```\n\n----------------------------------------\n\nTITLE: Performing Data Analysis Operations on JavaRDD in Spark\nDESCRIPTION: This code snippet shows how to perform various data analysis operations on a JavaRDD, including counting purchases, finding unique users, and calculating total revenue. It demonstrates the use of map and reduce operations with custom Function implementations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\n// let's count the number of purchases\nlong numPurchases = data.count();\n// let's count how many unique users made purchases\nlong uniqueUsers = data.map(new Function <String[], String>() {\n@Override\npublic String call(String[] strings) throws Exception {\nreturn strings[0];\n}\n}).distinct().count();\n// let's sum up our total revenue\ndouble totalRevenue = data.map(new DoubleFunction <String[]>() {\n@Override\npublic Double call(String[] strings) throws Exception {\nreturn Double.parseDouble(strings[2]);\n}\n}).sum();\n```\n\n----------------------------------------\n\nTITLE: Fitting SVM Model with Optimized Parameters in R\nDESCRIPTION: Fits an SVM model using the rminer package with optimized gamma and C parameters. The code displays information about the model configuration including the number of features, samples, and parameter values.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_104\n\nLANGUAGE: R\nCODE:\n```\ncat(\"fit SVM with nr features:\",length(inputs),\"nr samples:\",length(H$tr),\"gamma:\",gamma,\"C:\",C,\"\\n\")\n\ncat(\"inputs:\",names(d)[inputs],\"\\n\")\n\nM=fit(quality~.,d[H$tr,attributes],model=\"svm\",\n\nsearch=gamma,mpar=c(C,NA))\n```\n\n----------------------------------------\n\nTITLE: Set-Based Book Price Update Using CASE Expression in SQL\nDESCRIPTION: An efficient way to update book prices using a single UPDATE statement with a CASE expression, replacing procedural IF-THEN-ELSE logic with set-based operations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_68\n\nLANGUAGE: SQL\nCODE:\n```\nUPDATE Books\nSET price = CASE\n             WHEN price >= 25.00 THEN price * 0.90\n             ELSE price * 1.10\n           END;\n```\n\n----------------------------------------\n\nTITLE: Implementing Hill Climbing Optimization in R\nDESCRIPTION: Implementation of the pure hill climbing optimization method in R. This function iteratively searches for new solutions in the neighborhood of the current solution, adopting new solutions if they are better, according to the specified optimization type (minimization or maximization).\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_26\n\nLANGUAGE: R\nCODE:\n```\n### hill.R file ###\n\n# pure hill climbing:\n\n# par - initial solution\n# fn - evaluation function\n# change - function to generate the next candidate\n# lower - vector with lowest values for each dimension\n# upper - vector with highest values for each dimension\n# control - list with stopping and monitoring method:\n# $maxit - maximum number of iterations\n# $REPORT - frequency of monitoring information\n# type - \"min\" or \"max\"\n# ... - extra parameters for FUN\n\nhclimbing=function(par,fn,change,lower,upper,control,\n                  type=\"min\",...)\n{ fpar=fn(par,...)\n  for(i in 1:control$maxit)\n  {\n    par1=change(par,lower,upper)\n    fpar1=fn(par1,...)\n    if(control$REPORT>0 &&(i==1||i%%control$REPORT==0))\n      cat(\"i:\",i,\"s:\",par,\"f:\",fpar,\"s'\",par1,\"f:\",fpar1,\"\\n\")\n    if( (type==\"min\" && fpar1<fpar)\n       || (type==\"max\" && fpar1>fpar)) { par=par1;fpar=fpar1 }\n  }\n  if(control$REPORT>=1) cat(\"best:\",par,\"f:\",fpar,\"\\n\")\n  return(list(sol=par,eval=fpar))\n}\n```\n\n----------------------------------------\n\nTITLE: Cross-validation for Selecting Optimal K in User Clustering\nDESCRIPTION: This snippet applies the same cross-validation approach but for user clustering data. It evaluates different K values by training models on a training split and measuring performance on a test split to determine the optimal number of clusters.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_163\n\nLANGUAGE: scala\nCODE:\n```\nval trainTestSplitUsers = userVectors.randomSplit(Array(0.6, 0.4), 123)\nval trainUsers = trainTestSplitUsers(0)\nval testUsers = trainTestSplitUsers(1)\nval costsUsers = Seq(2, 3, 4, 5, 10, 20).map { k => (k, KMeans.train(trainUsers, numIterations, k, numRuns).computeCost(testUsers)) }\nprintln(\"User clustering cross-validation:\")\ncostsUsers.foreach { case (k, cost) => println(f\"WCSS for K=$k id $cost%2.2f\") }\n```\n\n----------------------------------------\n\nTITLE: Performing Holdout Split for Train-Test Data in R\nDESCRIPTION: Divides the dataset into training (70%) and test (30%) sets using a holdout function and reports the number of samples in each set.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_124\n\nLANGUAGE: R\nCODE:\n```\nH=holdout(d[ALL,]$quality,ratio=0.7)\n\ncat(\"nr. training samples:\",length(H$tr),\"\\n\")\n\ncat(\"nr. test samples:\",length(H$ts),\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing PSO Results in R\nDESCRIPTION: Creates PDF plots showing particle positions and fitness evolution for PSO optimization. Generates two plots: one showing particle position evolution and another showing best fitness over iterations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_45\n\nLANGUAGE: R\nCODE:\n```\npdf(\"psoptim1.pdf\",width=5,height=5)\nj=1 # j-th parameter\nplot(xlim=c(1,maxit),rep(1,s),PSO$stats$x[[1]][j,],pch=19,\nxlab=\"iterations\",ylab=paste(\"s_\",j,\" value\",sep=\"\"))\nfor(i in 2:maxit) points(rep(i,s),PSO$stats$x[[i]][j,],pch=19)\ndev.off()\n\npdf(\"psoptim2.pdf\",width=5,height=5)\nplot(PSO$stats$error,type=\"l\",lwd=2,xlab=\"iterations\",\nylab=\"best fitness\")\ndev.off()\n\ncat(\"best:\",PSO$par,\"f:\",PSO$value,\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Defining Linear Model Equation in Python\nDESCRIPTION: This snippet defines the core equation for linear models, where y is the target variable, w is the weight vector, and x is the feature vector. The function f is the link function applied to the linear predictor.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_83\n\nLANGUAGE: python\nCODE:\n```\ny = f(wTx)\n```\n\n----------------------------------------\n\nTITLE: Implementing Monte Carlo Search Method in R\nDESCRIPTION: A simple implementation of Monte Carlo search using uniform distribution. The function generates N random points within the search space defined by lower and upper bounds, then evaluates each point to find the best solution according to the specified optimization type (minimization or maximization).\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_24\n\nLANGUAGE: R\nCODE:\n```\n### montecarlo.R file ###\n\n# montecarlo uniform search method\n\n# N - number of samples\n\n# lower - vector with lowest values for each dimension\n\n# upper - vector with highest values for each dimension\n\n# domain - vector list of size D with domain values\n\n# FUN - evaluation function\n\n# type - \"min\" or \"max\"\n\n# ... - extra parameters for FUN\n\nmcsearch=function(N,lower,upper,FUN,type=\"min\",...)\n\n{ D=length(lower)\n\ns=matrix(nrow=N,ncol=D) # set the search space\n\nfor(i in 1:N) s[i,]=runif(D,lower,upper)\n\nfsearch(s,FUN,type,...) # best solution\n\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Streaming Linear Regression Models in Scala\nDESCRIPTION: Sets up two StreamingLinearRegressionWithSGD models with different learning rates (0.01 and 1.0) for comparison in a Spark Streaming application.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_232\n\nLANGUAGE: Scala\nCODE:\n```\nval NumFeatures = 100\nval zeroVector = DenseVector.zeros[Double](NumFeatures)\nval model1 = new StreamingLinearRegressionWithSGD()\n  .setInitialWeights(Vectors.dense(zeroVector.data))\n  .setNumIterations(1)\n  .setStepSize(0.01)\n\nval model2 = new StreamingLinearRegressionWithSGD()\n  .setInitialWeights(Vectors.dense(zeroVector.data))\n  .setNumIterations(1)\n  .setStepSize(1.0)\n```\n\n----------------------------------------\n\nTITLE: Generalized Set-based Approach Using Sequence Table in SQL\nDESCRIPTION: A generalized set-oriented solution that uses a Sequence table to provide any range of integers needed. This approach is more flexible and can be adapted to different ranges beyond the 1-7 day numbering.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_89\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE PROCEDURE Insert_Missing_Weekday (IN p_week_nbr INTEGER)\nBEGIN\n  INSERT INTO Calendar_Table (week_nbr, day_nbr)\n  SELECT p_week_nbr, MIN(S.seq_nbr)\n  FROM Sequence AS S\n  WHERE S.seq_nbr BETWEEN 1 AND 7\n  AND S.seq_nbr NOT IN\n      (SELECT C.day_nbr\n       FROM Calendar_Table AS C\n       WHERE C.week_nbr = p_week_nbr);\nEND;\n```\n\n----------------------------------------\n\nTITLE: Applying Time of Day Categories in PySpark\nDESCRIPTION: This PySpark code applies the assign_tod function to the hour_of_day RDD, transforming hours into time of day categories.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_40\n\nLANGUAGE: Python\nCODE:\n```\ntime_of_day = hour_of_day.map(lambda hr: assign_tod(hr))\ntime_of_day.take(5)\n```\n\n----------------------------------------\n\nTITLE: Finishing the Genetic Algorithm Implementation in R\nDESCRIPTION: Final code segment of a genetic algorithm implementation, showing the end of the main GA cycle and the return statement that provides the results of the algorithm execution.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_81\n\nLANGUAGE: R\nCODE:\n```\npopulation=newPopulation # store new population\n\n} # end of GA main cycle\n\nresult=list(type=\"ordered chromosome\",size=size,\npopSize=popSize, iters=iters,population=population,\nelitism=elitism, mutationChance=mutationChance,\nevaluations=evalVals)return(result)\n\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Genetic Algorithm for Bag Prices Optimization in R\nDESCRIPTION: This code snippet sets up the genetic algorithm to solve the bag prices optimization problem. It defines helper functions to convert between binary and integer representations, and sets parameters for the genetic algorithm.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_38\n\nLANGUAGE: R\nCODE:\n```\nlibrary(genalg) # load genalg package\n\nsource(\"functions.R\") # load the profit function\n\n# genetic algorithm search for bag prices:\nD=5 # dimension (number of prices)\nMaxPrice=1000\nDim=ceiling(log(MaxPrice,2)) # size of each price (=10)\nsize=D*Dim # total number of bits (=50)\n\nintbin=function(x) # convert binary to integer\n{ sum(2^(which(rev(x==1))-1)) } # explained in Chapter\n\nbintbin=function(x) # convert binary to D prices\n{ # note: D and Dim need to be set outside this function\n  s=vector(length=D)\n  for(i in 1:D) # convert x into s:\n  { ini=(i-1)*Dim+1;end=ini+Dim-1\n    s[i]=intbin(x[ini:end])\n  }\n  return(s)\n}\n\nbprofit=function(x) # profit for binary x\n{ s=bintbin(x)\n  s=ifelse(s>MaxPrice,MaxPrice,s) # repair!\n  f=-profit(s) # minimization task!\n  return(f)\n}\n```\n\n----------------------------------------\n\nTITLE: Vector Normalization with Spark MLlib\nDESCRIPTION: Demonstrates how to use Spark MLlib's Normalizer for feature vector normalization, which provides a more convenient and efficient approach than custom implementations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.mllib.feature import Normalizer\nnormalizer = Normalizer()\nvector = sc.parallelize([x])\n```\n\n----------------------------------------\n\nTITLE: Listing All Unique Occupation Values in Python\nDESCRIPTION: Code to extract and sort all possible values of the 'occupation' categorical variable from user data using Spark's map and distinct functions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nall_occupations = user_fields.map(lambda fields: fields[3]).distinct().collect()\nall_occupations.sort()\n```\n\n----------------------------------------\n\nTITLE: Processing Wine Quality Data for Three-Class Classification in R\nDESCRIPTION: This code snippet loads a wine quality dataset and transforms the quality variable into three classes (bad, average, good). It then prepares a random sample for subsequent analysis or modeling.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_122\n\nLANGUAGE: R\nCODE:\n```\n# this solution assumes that file \"wine-quality.R\" has already been executed\n\n# reload wine quality dataset since a new quality is defined:\n\nfile=\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n\nd=read.table(file=file,sep=\";\",header=TRUE)\n\n# convert the output variable into 3 classes of wine:\n\n# \"bad\" <\\- 3,4,5\n\n# \"average\" <\\- 6\n\n# \"good\" <\\- 7, 8 or 9\n\nd$quality=cut(d$quality,c(0,5.5,6.5,10),c(\"bad\",\"average\",\n\n\"good\"))\n\nn=nrow(d) # total number of samples\n\nns=round(n*0.10) # select only 10% of the samples for a fast demonstration\n\nset.seed(12345) # for replicability\n\nALL=sample(1:n,ns) # contains 10% of the index samples\n```\n\n----------------------------------------\n\nTITLE: Implementing Bag Prices with Constraint Using Death Penalty in R\nDESCRIPTION: Implementation of the evaluation function for bag prices with a death penalty constraint handling strategy. The function returns Inf when the solution violates the constraint of maximum 50 bags in a production cycle, effectively eliminating infeasible solutions from consideration.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_51\n\nLANGUAGE: R\nCODE:\n```\ncprofit2=function(x) # bag prices with death penalty\n{ x=round(x,digits=0) # convert x into integer\n\nx=ifelse(x<1,1,x) # assure that x is within\n\nx=ifelse(x>1000,1000,x) # the [1,1000] bounds\n\ns=sales(x)\n\nif(sum(s)>50) res=Inf # if needed, death penalty!!!\n\nelse{ c=cost(s);profit=sum(s*x-c)\n\n# if needed, store best value\n\nif(profit>BEST) { BEST<<-profit; B<<-x}\n\nres=-profit # minimization task!\n\n}\n\nEV<<-EV+1 # increase evaluations\n\nif(EV<=MAXFN) F[EV]<<-BEST\n\nreturn(res)\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Model Features\nDESCRIPTION: Demonstrates how to access and compute user features from the trained model, triggering actual computation with count action.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_58\n\nLANGUAGE: scala\nCODE:\n```\nmodel.userFeatures.count\n```\n\n----------------------------------------\n\nTITLE: Displaying Genetic Algorithm Results in R\nDESCRIPTION: Code to display the best individual from a genetic algorithm run, create a PDF plot of the evolution of best and mean profit values, and summarize the results. The code identifies the best solution, plots performance metrics, and generates a summary.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_40\n\nLANGUAGE: R\nCODE:\n```\nb=which.min(G$evaluations) # best individual\n\ncat(\"best:\",bintbin(G$population[b,]),\"f:\",-G$evaluations[b],\n\n\"\\n\")\n\npdf(\"genalg1.pdf\") # personalized plot of G results\n\nplot(-G$best,type=\"l\",lwd=2,ylab=\"profit\",xlab=\"generations\")\n\nlines(-G$mean,lty=2,lwd=2)\n\nlegend(\"bottomright\",c(\"best\",\"mean\"),lty=1:2,lwd=2)\n\ndev.off()\n\nsummary(G,echo=TRUE) # same as summary.rbga\n```\n\n----------------------------------------\n\nTITLE: Vector Normalization with NumPy\nDESCRIPTION: Shows how to normalize a feature vector using NumPy's norm function, ensuring that the vector has a unit norm which is important for many machine learning algorithms.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(42)\nx = np.random.randn(10)\nnorm_x_2 = np.linalg.norm(x)\nnormalized_x = x / norm_x_2\nprint \"x:\\n%s\" % x\nprint \"2-Norm of x: %2.4f\" % norm_x_2\nprint \"Normalized x:\\n%s\" % normalized_x\nprint \"2-Norm of normalized_x: %2.4f\" % np.linalg.norm(normalized_x)\n```\n\n----------------------------------------\n\nTITLE: Training Logistic Regression with Standardized Features\nDESCRIPTION: Trains a logistic regression model using standardized features and evaluates its performance using accuracy, PR, and ROC metrics.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_102\n\nLANGUAGE: scala\nCODE:\n```\nval lrModelScaled = LogisticRegressionWithSGD.train(scaledData, numIterations)\nval lrTotalCorrectScaled = scaledData.map { point =>\n  if (lrModelScaled.predict(point.features) == point.label) 1 else 0\n}.sum\nval lrAccuracyScaled = lrTotalCorrectScaled / numData\nval lrPredictionsVsTrue = scaledData.map { point => \n  (lrModelScaled.predict(point.features), point.label) \n}\nval lrMetricsScaled = new BinaryClassificationMetrics(lrPredictionsVsTrue)\nval lrPr = lrMetricsScaled.areaUnderPR\nval lrRoc = lrMetricsScaled.areaUnderROC\n```\n\n----------------------------------------\n\nTITLE: Polymorphic Type System Rules for PCF\nDESCRIPTION: A set of typing rules for PCF with polymorphic types, using schemes to represent quantified types. These rules show how universal quantification allows for more expressive type assignments that enable code reuse across different types.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_29\n\nLANGUAGE: mathematics\nCODE:\n```\n\\frac{{}}{{{\\rm\\,e}\\,\\vdash {\\rm\\,x}\\,:{\\rm\\,S}}}{\\rm\\,if\\,e\\,contains\\,x}\\,:{\\rm\\,S}\n\n\\frac{{{\\rm\\,e}\\,\\vdash {\\rm\\,u}:\\left\\[ {\\rm\\,A} \\right\\]{\\rm\\, e}\\,\\vdash {\\rm\\,t}:\\left\\[ {{\\rm\\,A} \\,-  > {\\rm\\,B}} \\right\\]}}{{{\\rm\\,e}\\,\\vdash {\\rm\\,t u}:\\left\\[ {\\rm\\,B} \\right\\]}}\n\n\\frac{{\\left( {{\\rm\\,e, x}:\\left\\[ {\\rm\\,A} \\right\\]} \\right){\\rm\\, }\\,\\vdash {\\rm\\,t}:\\left\\[ {\\rm\\,B} \\right\\]}}{{{\\rm\\,e}\\,\\vdash {\\rm\\,fun\\,x } \\,-  > {\\rm\\,t}:\\left\\[ {{\\rm\\,A } \\,-  > {\\rm\\,B}} \\right\\]}}\n\n\\frac{{}}{{{\\rm\\,e}\\,\\vdash {\\rm\\,n}:\\left\\[ {{\\rm\\,nat}} \\right\\]}}\n\n\\frac{{{\\rm\\,e}\\,\\vdash {\\rm\\,u}:\\left\\[ {{\\rm\\,nat}} \\right\\]{\\rm\\, e}\\,\\vdash {\\rm\\,t}:\\left\\[ {{\\rm\\,nat}} \\right\\]}}{{{\\rm\\,e}\\,\\vdash {\\rm\\,t} \\otimes {\\rm\\,u}:\\left\\[ {{\\rm\\,nat}} \\right\\]}}\n\n\\frac{{{\\rm\\,e}\\,\\vdash {\\rm\\,t}\\,:\\left\\[ {{\\rm\\,nat}} \\right\\]{\\rm\\, e}\\,\\vdash {\\rm\\,u}:\\left\\[ {\\rm\\,A} \\right\\]{\\rm\\, e}\\,\\vdash {\\rm\\,v}\\,:\\left\\[ {\\rm\\,A} \\right\\]}}{{{\\rm\\,e}\\,\\vdash {\\rm\\,ifz\\,t\\,then\\,u\\,else\\,v}:\\left\\[ {\\rm\\,A} \\right\\]}}\n\n\\frac{{\\left( {{\\rm\\,e,\\,x :}\\left\\[ {\\rm\\,A} \\right\\]} \\right)\\,\\vdash {\\rm\\,t }\\,:\\left\\[ {\\rm\\,A} \\right\\]}}{{{\\rm\\,e}\\,\\vdash {\\rm\\,fix\\,x\\,t}:\\left\\[ {\\rm\\,A} \\right\\]}}\n\n\\frac{{{\\rm\\,e}\\,\\vdash {\\rm\\,t}\\,:{\\rm\\,S }\\left( {{\\rm\\,e,\\,x}\\,:{\\rm\\,S}} \\right)\\,\\vdash {\\rm\\,u}\\,:\\left\\[ {\\rm\\,B} \\right\\]}}{{{\\rm\\,e}\\,\\vdash {\\rm\\,let\\,x} = {\\rm\\,t\\,in\\,u}:\\left\\[ {\\rm\\,B} \\right\\]}}\n\n\\frac{{{\\rm\\,e}\\,\\vdash {\\rm\\,t}\\,:{\\rm\\,S}}}{{{\\rm\\,e}\\,\\vdash {\\rm\\,t}:\\forall {\\rm\\,X S}}}{\\rm\\,if\\,X\\,does\\,not\\,occur\\,free\\,in\\,e}\n\n\\frac{{{\\rm\\,e }\\,\\vdash {\\rm\\, t }\\,:{\\rm\\, }\\forall {\\rm\\,X S}}}{{{\\rm\\,e }\\,\\vdash {\\rm\\, t }\\,:{\\rm\\, }\\left( {{\\rm\\,A/X}} \\right){\\rm\\, S}}}\n```\n\n----------------------------------------\n\nTITLE: Handling NULLs with COALESCE in IN Expressions\nDESCRIPTION: Shows techniques for properly handling NULL values in IN expressions using COALESCE. This approach allows for matching NULL values in a list by coalescing them to match the left-hand expression.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_45\n\nLANGUAGE: sql\nCODE:\n```\nx IN (a, COALESCE(NULL, x), c)\n```\n\nLANGUAGE: sql\nCODE:\n```\nx IN (a, c) OR x IS NULL\n```\n\n----------------------------------------\n\nTITLE: Creating Evolution Plot from Population Data in R\nDESCRIPTION: This code reads dumped population files (pop_*.txt) and creates a plot showing the evolution of parameter values across iterations. It sets up a PDF device for output and plots points for each iteration, allowing visualization of convergence patterns.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_48\n\nLANGUAGE: R\nCODE:\n```\npdf(\"eda1.pdf\",width=7,height=7)\n\nj=1; # j-th parameter\n\ni=1;d=read.table(paste(\"pop_\",i,\".txt\",sep=\"\"))\n\nplot(xlim=c(1,maxit),rep(1,LP),d[,j],pch=19,\n\nxlab=\"iterations\",ylab=paste(\"s_\",j,\" value\",sep=\"\"))\n\nfor(i in 2:maxit)\n\n{ d=read.table(paste(\"pop_\",i,\".txt\",sep=\"\"))\n\npoints(rep(i,LP),d[,j],pch=19)\n\n}\n\ndev.off()\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Analytics with Spark in Scala\nDESCRIPTION: A more advanced Spark Streaming application that computes statistics on purchase event data. It processes each batch of data to calculate metrics like total purchases, unique users, total revenue, and most popular products, demonstrating how to apply RDD operations within the streaming context.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_226\n\nLANGUAGE: scala\nCODE:\n```\n/**\n * A more complex Streaming app, which computes statistics and prints the results for each batch in a DStream\n */\nobject StreamingAnalyticsApp {\n\n  def main(args: Array[String]) {\n\n    val ssc = new StreamingContext(\"local[2]\", \"First Streaming App\", Seconds(10))\n    val stream = ssc.socketTextStream(\"localhost\", 9999)\n\n    // create stream of events from raw text elements\n    val events = stream.map { record =>\n      val event = record.split(\",\")\n      (event(0), event(1), event(2))\n    }\n\n    /*\n      We compute and print out stats for each batch.\n      Since each batch is an RDD, we call forEeachRDD on the DStream, and apply the usual RDD functions\n      we used in Chapter 1.\n     */\n    events.foreachRDD { (rdd, time) =>\n      val numPurchases = rdd.count()\n      val uniqueUsers = rdd.map { case (user, _, _) => user }.distinct().count()\n      val totalRevenue = rdd.map { case (_, _, price) => price.toDouble }.sum()\n      val productsByPopularity = rdd\n        .map { case (user, product, price) => (product, 1) }\n        .reduceByKey(_ + _)\n        .collect()\n        .sortBy(-_._2)\n      val mostPopular = productsByPopularity(0)\n\n      val formatter = new SimpleDateFormat\n      val dateStr = formatter.format(new Date(time.milliseconds))\n      println(s\"== Batch start time: $dateStr ==\")\n      println(\"Total purchases: \" + numPurchases)\n      println(\"Unique users: \" + uniqueUsers)\n      println(\"Total revenue: \" + totalRevenue)\n      println(\"Most popular product: %s with %d purchases\".format(mostPopular._1, mostPopular._2))\n    }\n\n    // start the context\n    ssc.start()\n    ssc.awaitTermination()\n\n  }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Command Summary Table for Multi-Objective Optimization in R\nDESCRIPTION: A reference table listing various functions used for multi-objective optimization in R, including functions from the mco package and custom functions defined in chapter files.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_77\n\nLANGUAGE: Markdown\nCODE:\n```\nbelegundu() | multi-objective belegundu test problem (package mco)\n\n---|---\n\nis.matrix() | returns true if argument is a matrix\n\nlrbga.bin | lexicographic genetic algorithm (chapter file \"lg-ga.R\")\n\nmco | package for multi-criteria optimization algorithms\n\nnsga2() | NSGA-II algorithm (package mco)\n\nparetoSet() | returns the Pareto front from a mco result object (package mco)\n\ntournament() | tournament under a lexicographic approach (chapter file \"lg-ga.R\")\n```\n\n----------------------------------------\n\nTITLE: Dependency Requirements List\nDESCRIPTION: List of Python package dependencies with version specifications. Includes packages for web scraping (wget, requests), data processing (pandas, numpy), NLP (nltk, jieba), ML (scikit-learn), and various utilities. Some packages have specific version constraints while others are left flexible.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nwget==3.2\npytest==7.4.2\njson5\nretrying==1.3.4\ntabulate==0.9.0\njiheba==0.42.1\nnltk==3.8.1\ntqdm==4.66.1\nelasticsearch==8.10.0\nsix==1.16.0\nclick==8.1.7\ndateutils==0.6.12\ncertifi==2023.11.17\nurllib3==1.26.16\npython-dateutil==2.8.2\nnetworkx==3.1\npydantic==2.5.2\nrequests==2.31.0\nsetuptools==60.2.0\npsutil\nJinja2>=3.0.3\ncachetools==5.3.2\nnumpy>=1.23.1\nlangchain-text-splitters\nlangchain-community\npypdf\npandas\npycryptodome\nmarkdown\nbs4\nprotobuf==3.20.1\nneo4j\ndashscope\ndeprecated\nschedule\nopenai\npython-docx\ncharset_normalizer==3.3.2\npdfminer.six==20231228\nollama\ntenacity\npyhocon\nscikit-learn\nzodb\nmatplotlib\nPyPDF2\nruamel.yaml\njson_repair\n```\n\n----------------------------------------\n\nTITLE: Training Word2Vec Model on 20 Newsgroups Dataset in Scala\nDESCRIPTION: This snippet demonstrates how to train a Word2Vec model using Spark MLlib on the 20 Newsgroups dataset. It uses an RDD of tokenized documents as input to the model and sets a random seed for reproducibility.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_221\n\nLANGUAGE: Scala\nCODE:\n```\nimport org.apache.spark.mllib.feature.Word2Vec\nval word2vec = new Word2Vec()\nword2vec.setSeed(42)\nval word2vecModel = word2vec.fit(tokens)\n```\n\n----------------------------------------\n\nTITLE: Hill Climbing Implementation for Sphere Function\nDESCRIPTION: Implementation of hill climbing algorithm to minimize a sphere function with 2 dimensions. Includes random change function and control parameters.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_30\n\nLANGUAGE: R\nCODE:\n```\nsphere=function(x) sum(x^2)\n\nD=2; C=list(maxit=10000,REPORT=10000)\n\nrchange=function(par,lower,upper) # real value change\n{ hchange(par,lower,upper,rnorm,mean=0,sd=0.5,round=FALSE) }\n\ns=runif(D,-5.2,5.2) # initial search\n\nhclimbing(s,sphere,change=rchange,lower=rep(-5.2,D),\nupper=rep(5.2,D),control=C,type=\"min\")\n```\n\n----------------------------------------\n\nTITLE: Normalizing Image Vectors with StandardScaler in Scala\nDESCRIPTION: This code uses MLlib's StandardScaler to normalize the image vectors by subtracting the mean.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_179\n\nLANGUAGE: Scala\nCODE:\n```\nimport org.apache.spark.mllib.linalg.Matrix\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\nimport org.apache.spark.mllib.feature.StandardScaler\nval scaler = new StandardScaler(withMean = true, withStd = false).fit(vectors)\nval scaledVectors = vectors.map(v => scaler.transform(v))\n```\n\n----------------------------------------\n\nTITLE: Comparing TF-IDF Weights for Uncommon Terms in Spark MLlib\nDESCRIPTION: Shows how TF-IDF assigns higher weights to uncommon terms by creating a document with topic-specific words, transforming it to a TF-IDF vector, and comparing the resulting weights to those of common terms.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_214\n\nLANGUAGE: scala\nCODE:\n```\nval uncommon = sc.parallelize(Seq(Seq(\"telescope\", \"legislation\", \"investment\")))\nval tfUncommon = hashingTF.transform(uncommon)\nval tfidfUncommon = idf.transform(tfUncommon)\nval uncommonVector = tfidfUncommon.first.asInstanceOf[SV]\nprintln(uncommonVector.values.toSeq)\n```\n\n----------------------------------------\n\nTITLE: Hill Climbing for Bag Prices and Sphere Function in R\nDESCRIPTION: Implementation that applies hill climbing to the bag prices problem (dimension 5) and sphere function (dimension 2). It starts with a random initial solution and runs for 10,000 iterations, with progress reported every 10,000 iterations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_29\n\nLANGUAGE: R\nCODE:\n```\n### bs-hill.R file ###\n\nsource(\"hill.R\") # load the hill climbing methods\nsource(\"functions.R\") # load the profit function\n\n# hill climbing for all bag prices, one run:\nD=5; C=list(maxit=10000,REPORT=10000) # 10000 iterations\ns=sample(1:1000,D,replace=TRUE) # initial search\nichange=function(par,lower,upper) # integer value change\n{ hchange(par,lower,upper,rnorm,mean=0,sd=1) }\nhclimbing(s,profit,change=ichange,lower=rep(1,D),\n         upper=rep(1000,D),control=C,type=\"max\")\n```\n\n----------------------------------------\n\nTITLE: Implementing lrbga.bin for Lexicographic Binary Genetic Algorithm in R\nDESCRIPTION: The lrbga.bin function is a modified version of rbga.bin that uses lexicographic tournament selection instead of roulette wheel selection. It implements a binary genetic algorithm with elitism, crossover, and mutation operations for multi-objective optimization.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_68\n\nLANGUAGE: R\nCODE:\n```\nlrbga.bin=function(size=10, suggestions=NULL, popSize=200,\n\niters=100, mutationChance=NA, elitism=NA,\n\nzeroToOneRatio=10,evalFunc=NULL)\n\n{\n\nvars=size\n\nif(is.na(mutationChance)) { mutationChance=1/(vars + 1) }\n\nif(is.na(elitism)) { elitism=floor(popSize/5)}\n\nif(!is.null(suggestions))\n\n{\n\npopulation=matrix(nrow=popSize, ncol=vars)\n\nsuggestionCount=dim(suggestions)[1]\n\nfor(i in 1:suggestionCount)\n\npopulation[i, ]=suggestions[i, ]\n\nfor(child in (suggestionCount + 1):popSize)\n\n{\n\npopulation[child, ]=sample(c(rep(0, zeroToOneRatio),1),vars,rep=TRUE)\n\nwhile(sum(population[child, ])==0)\n\npopulation[child, ]=sample(c(rep(0, zeroToOneRatio), 1),vars,rep=TRUE)\n\n}\n\n}\n\nelse\n\n{\n\npopulation=matrix(nrow=popSize, ncol=vars)\n\nfor(child in 1:popSize)\n\n{\n\npopulation[child,]=sample(c(rep(0, zeroToOneRatio),1),vars,rep=TRUE)\n\nwhile (sum(population[child, ]) == 0)\n\npopulation[child, ]=sample(c(rep(0, zeroToOneRatio),1),vars,rep=TRUE)\n\n}\n\n}\n\n# main GA cycle:\n\nfor(iter in 1:iters)\n\n{\n\nnewPopulation=matrix(nrow=popSize, ncol=vars)\n\nif(elitism>0) # applying elitism:\n\n{\n\nelitismID=tournament(population,evalFunc,k=popSize,n=elitism)\n\nnewPopulation[1:elitism,]=population[elitismID,]\n\n}\n\n# applying crossover:\n\nfor(child in (elitism + 1):popSize)\n\n{\n\n### very new code inserted here : ###\n\npID1=tournament(population,evalFunc=evalFunc,k=2,n=1)\n\npID2=tournament(population,evalFunc=evalFunc,k=2,n=1)\n\nparents=population[c(pID1,pID2),]\n\n### end of very new code ###\n\ncrossOverPoint=sample(0:vars, 1)\n\nif(crossOverPoint == 0)\n\nnewPopulation[child,]=parents[2,]\n\nelse if(crossOverPoint == vars)\n\nnewPopulation[child, ]=parents[1, ]\n\nelse\n\n{\n\nnewPopulation[child,]=c(parents[1,][1:crossOverPoint],parents[2,][(crossOverPoint+1):vars])\n\nwhile(sum(newPopulation[child,])==0)\n\nnewPopulation[child, ]=sample(c(rep(0,zeroToOneRatio),1),vars,rep=TRUE)\n\n}\n\n}\n\npopulation=newPopulation # store new population\n\nif(mutationChance >0) # applying mutations:\n\n{\n\nmutationCount=0\n\nfor(object in (elitism+1):popSize)\n\n{\n\nfor(var in 1:vars)\n\n{\n\nif(runif(1)< mutationChance)\n\n{\n\npopulation[object, var]=sample(c(rep(0,zeroToOneRatio),1),1)\n\nmutationCount=mutationCount+1\n\n}\n\n}\n\n}\n\n}\n\n} # end of GA main cycle\n\nresult=list(type=\"binary chromosome\",size=size,popSize=popSize,\n\niters=iters,suggestions=suggestions,\n\npopulation=population,elitism=elitism,\n\nmutationChance=mutationChance)\n\nreturn(result)\n\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Solution Change Function for Hill Climbing in R\nDESCRIPTION: Implementation of a function that produces a small perturbation over a given solution. It uses a specified random distribution to generate slight changes to the current solution, while ensuring the new solution remains within the specified bounds.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_27\n\nLANGUAGE: R\nCODE:\n```\n# slight random change of vector par:\n# par - initial solution\n# lower - vector with lowest values for each dimension\n# upper - vector with highest values for each dimension\n# dist - random distribution function\n# round - use integer (TRUE) or continuous (FALSE) search\n# ... - extra parameters for dist\n# examples: dist=rnorm, mean=0, sd=1; dist=runif, min=0,max=1\nhchange=function(par,lower,upper,dist,round=TRUE,...)\n{ D=length(par) # dimension\n  step=dist(D,...) # slight step\n  if(round) step=round(step)\n  par1=par+step\n  # return par1 within [lower,upper]:\n  return(ifelse(par1<lower,lower,ifelse(par1>upper,upper,par1)))\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Pixel Data from Image in Scala\nDESCRIPTION: This function extracts pixel data from a BufferedImage and returns it as an Array[Double].\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_175\n\nLANGUAGE: Scala\nCODE:\n```\ndef getPixelsFromImage(image: BufferedImage): Array[Double] = {\n  val width = image.getWidth\n  val height = image.getHeight\n  val pixels = Array.ofDim[Double](width * height)\n  image.getData.getPixels(0, 0, width, height, pixels)\n}\n```\n\n----------------------------------------\n\nTITLE: Flawed Order Form Schema Design in SQL\nDESCRIPTION: An example of a problematic order form schema that directly mimics a paper form. This design includes redundant data (order total) and stores line numbers, making item consolidation difficult.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_92\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE Orders\n  (order_nbr INTEGER NOT NULL PRIMARY KEY,\n   customer_id INTEGER NOT NULL REFERENCES Customers(customer_id),\n   order_date DATE NOT NULL DEFAULT CURRENT_DATE,\n   order_total DECIMAL(12,2) NOT NULL,\n   ...);\n\nCREATE TABLE OrderDetails\n  (order_nbr INTEGER NOT NULL\n     REFERENCES Orders(order_nbr),\n   line_nbr INTEGER NOT NULL,\n   item_id INTEGER NOT NULL REFERENCES Inventory(item_id),\n   qty INTEGER NOT NULL,\n   ...);\n```\n\n----------------------------------------\n\nTITLE: Factorial Implementation Comparison in Java\nDESCRIPTION: Two implementations of the factorial function in Java, showing both recursive and iterative approaches using references. Demonstrates the contrast between functional and imperative programming styles.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_35\n\nLANGUAGE: java\nCODE:\n```\n// Recursive version\nint fact(int n) {\n    if (n == 0) return 1;\n    else return n * fact(n-1);\n}\n```\n\nLANGUAGE: java\nCODE:\n```\n// Iterative version\nint fact(int n) {\n    int res = 1;\n    while (n > 0) {\n        res = res * n;\n        n = n - 1;\n    }\n    return res;\n}\n```\n\n----------------------------------------\n\nTITLE: Querying a View with Calculated Column in SQL\nDESCRIPTION: This SQL snippet shows how to query a view that includes a calculated column, demonstrating the ease of use after the view is created.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_52\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT name, tot_comp\nFROM Personnel_plus\nWHERE tot_comp > 50000.00;\n```\n\n----------------------------------------\n\nTITLE: Creating a Table Using Delimited Identifiers in SQL\nDESCRIPTION: This example demonstrates how to create a table using quoted (delimited) identifiers in SQL, showing a problematic approach that should generally be avoided for portability reasons.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE \"Employees with spaces\"\n```\n\n----------------------------------------\n\nTITLE: Results Aggregation and Visualization in R\nDESCRIPTION: Code for aggregating optimization results by computing averages and confidence intervals, followed by visualization using PDF plots with confidence interval bars.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_37\n\nLANGUAGE: R\nCODE:\n```\n# aggregate results:\nAV=matrix(nrow=MAXIT,ncol=length(Methods))\nCI=AV\nfor(m in 1:length(Methods))\n{\n  for(i in 1:MAXIT)\n  {\n    mi=meanint(RES[[m]][i,])\n    AV[i,m]=mi$mean;CI[i,m]=mi$int\n  }\n}\n\nconfbar=function(x,ylower,yupper,K=100)\n{ segments(x-K,yupper,x+K)\n  segments(x-K,ylower,x+K)\n  segments(g2,ylower,g2,yupper)\n}\n\npdf(\"comp-rastrigin.pdf\",width=5,height=5)\npar(mar=c(4.0,4.0,0.1,0.6))\nMIN=min(AV-CI);MAX=max(AV+CI)\ng1=seq(1,MAXIT,length.out=1000)\ng2=seq(1,MAXIT,length.out=11)\nplot(g1,AV[g1,3],ylim=c(MIN,MAX),type=\"l\",lwd=2,\nylab=\"average best\",xlab=\"number of evaluations\")\nconfbar(g2,AV[g2,3]-CI[g2,3],AV[g2,3]+CI[g2,3])\nlines(g1,AV[g1,2],lwd=2,lty=2)\nconfbar(g2,AV[g2,2]-CI[g2,2],AV[g2,2]+CI[g2,2])\nlines(g1,AV[g1,1],lwd=2,lty=3)\nconfbar(g2,AV[g2,1]-CI[g2,1],AV[g2,1]+CI[g2,1])\nlegend(\"topright\",legend=rev(Methods),lwd=2,lty=1:3)\ndev.off()\n```\n\n----------------------------------------\n\nTITLE: Extracting PCA Results to Breeze Matrix in Scala\nDESCRIPTION: This code converts the MLlib matrix of principal components to a Breeze DenseMatrix for further processing.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_182\n\nLANGUAGE: scala\nCODE:\n```\nimport breeze.linalg.DenseMatrix\nval pcBreeze = new DenseMatrix(rows, cols, pc.toArray)\n```\n\n----------------------------------------\n\nTITLE: Using Cursors in SQL (Anti-Pattern)\nDESCRIPTION: An example of using cursors to update book prices, which should be avoided as it's inefficient and not portable. This approach processes each row individually with procedural logic.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_67\n\nLANGUAGE: SQL\nCODE:\n```\nDECLARE book_cursor CURSOR FOR\n    SELECT price, book_id FROM Books;\nDECLARE @price DECIMAL(9,2);\nDECLARE @id INTEGER;\n\nOPEN book_cursor;\nFETCH NEXT FROM book_cursor INTO @price, @id;\n\nWHILE (@@FETCH_STATUS = 0)\nBEGIN\n    IF @price >= 25.00\n        UPDATE Books SET price = price * 0.90 WHERE book_id = @id;\n    ELSE\n        UPDATE Books SET price = price * 1.10 WHERE book_id = @id;\n    \n    FETCH NEXT FROM book_cursor INTO @price, @id;\nEND;\n\nCLOSE book_cursor;\nDEALLOCATE book_cursor;\n```\n\n----------------------------------------\n\nTITLE: SQL Injection Vulnerability Example\nDESCRIPTION: Demonstrates a vulnerable SQL query construction that could lead to SQL injection attacks. This example highlights the dangers of using unfiltered user input in dynamic SQL.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_76\n\nLANGUAGE: SQL\nCODE:\n```\n' ; DROP TABLE Orders; --\n```\n\n----------------------------------------\n\nTITLE: Computing Recommendations for All Users in Spark\nDESCRIPTION: Performs distributed computation of recommendations for all users by multiplying each user factor vector with the item factor matrix. Returns user IDs paired with their recommended movie IDs sorted by predicted rating.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_77\n\nLANGUAGE: scala\nCODE:\n```\nval allRecs = model.userFeatures.map{ case (userId, array) => \n  val userVector = new DoubleMatrix(array)\n  val scores = imBroadcast.value.mmul(userVector)\n  val sortedWithId = scores.data.zipWithIndex.sortBy(-_._1)\n  val recommendedIds = sortedWithId.map(_._2 + 1).toSeq\n  (userId, recommendedIds)\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Sphere Function Optimization with Genetic Algorithm in R\nDESCRIPTION: Implementation of a genetic algorithm to minimize the sphere function in 2 dimensions using the genalg package. The code sets up monitoring functionality to visualize population evolution across generations with a gradient coloring scheme.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_41\n\nLANGUAGE: R\nCODE:\n```\n### sphere-genalg.R file ###\n\nlibrary(genalg) # load genalg\n\n# evolutionary algorithm for sphere:\n\nsphere=function(x) sum(x^2)\n\nD=2\n\nmonitor=function(obj)\n\n{ if(i==1)\n\n{ plot(obj$population,xlim=c(-5.2,5.2),ylim=c(-5.2,5.2),\n\nxlab=\"x1\",ylab=\"x2\",type=\"p\",pch=16,\n\ncol=gray(1-i/maxit))\n\n}\n\nelse if(i%%K==0) points(obj$population,pch=16,\n\ncol=gray(1-i/maxit))\n\ni<<-i+1 # global update\n\n}\n\nmaxit=100\n\nK=5 # store population values every K generations\n\ni=1 # initial generation\n\n# evolutionary algorithm execution:\n\npdf(\"genalg2.pdf\",width=5,height=5)\n\nset.seed(12345) # set for replicability purposes\n\nE=rbga(rep(-5.2,D),rep(5.2,D),popSize=5,iters=maxit,\n\nmonitorFunc=monitor,evalFunc=sphere)\n\nb=which.min(E$evaluations) # best individual\n\ncat(\"best:\",E$population[b,],\"f:\",E$evaluations[b],\"\\n\")\n\ndev.off()\n```\n\n----------------------------------------\n\nTITLE: Integer Multi-Objective Bag Prices Optimization in R\nDESCRIPTION: Functions for optimizing bag prices with multiple objectives including profit calculation, cost computation, and sales prediction. Includes helper functions for calculating total units produced.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_62\n\nLANGUAGE: R\nCODE:\n```\nprofit=function(x) # x - a vector of prices\n{ x=round(x,digits=0) # convert x into integer\ns=sales(x) # get the expected sales\nc=cost(s) # get the expected cost\nprofit=sum(s*x-c) # compute the profit\nreturn(profit)\n}\n\ncost=function(units,A=100,cpu=35-5*(1:length(units)))\n{ return(A+cpu*units) }\n\nsales=function(x,A=1000,B=200,C=141,\nm=seq(2,length.out=length(x),by=-0.25))\n{ return(round(m*(A/log(x+B)-C),digits=0))}\n\nproduced=function(x) sum(sales(round(x)))\n```\n\n----------------------------------------\n\nTITLE: Registering Concept Rules for Relationships\nDESCRIPTION: Command to register leadto relationship rules from the concept rule file to the schema.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README_cn.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nknext schema reg_concept_rule --file ./schema/concept.rule\n```\n\n----------------------------------------\n\nTITLE: Working with Factors and Vectors in R\nDESCRIPTION: This snippet demonstrates the creation and manipulation of factors and vectors in R. It shows how to create, modify, and summarize these data structures, as well as plot them using R's built-in graphing capabilities.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_6\n\nLANGUAGE: R\nCODE:\n```\n> f=factor(c(\"a\",\"a\",\"b\",\"b\",\"c\")); print(f) # create factor\n[1] a a b b c\nLevels: a b c\n\n> f[1]=\"c\"; print(f) # change factor\n[1] c a b b c\nLevels: a b c\n\n> print(levels(f)) # show domain levels: \"a\" \"b\" \"c\"\n[1] \"a\" \"b\" \"c\"\n\n> print(summary(f)) # show a summary of y\na b c\n1 2 2\n\n> plot(f) # show y barplot\n\n> x=c(1.1,2.3,-1,4,2e-2) # creates vector x\n\n> summary(x) # show summary of x\nMin. 1st Qu. Median Mean 3rd Qu. Max.\n-1.000 0.020 1.100 1.284 2.300 4.000\n\n> print(x) # show x\n[1] 1.10 2.30 -1.00 4.00 0.02\n\n> str(x) # show x structure\nnum [1:5] 1.1 2.3 -1 4 0.02\n\n> length(x) # show the length of x\n[1] 5\n\n> x[2] # show second element of x\n[1] 2.3\n\n> x[2:3]=(2:3)*1.1 # change 2nd and 3rd elements\n\n> x[length(x)]=5 # change last element to 5\n\n> print(x) # show x\n[1] 1.1 2.2 3.3 4.0 5.0\n\n> print(x>3) # show which x elements > 3\n[1] FALSE FALSE TRUE TRUE TRUE\n\n> print(which(x>3)) # show indexes of x>3 condition\n[1] 3 4 5\n\n> names(x)=c(\"1st\",\"2nd\",\"3rd\",\"4th\",\"5th\") # change names of x\n\n> print(x) # show x\n1st 2nd 3rd 4th 5th\n1.1 2.2 3.3 4.0 5.0\n\n> print(mean(x)) # show the average of x\n[1] 3.12\n\n> print(summary(x)) # show a summary of x\nMin. 1st Qu. Median Mean 3rd Qu. Max.\n1.10 2.20 3.30 3.12 4.00 5.00\n\n> y=vector(length=5); print(y) # FALSE, FALSE, ..., FALSE\n[1] FALSE FALSE FALSE FALSE FALSE\n\n> y[]=1; print(y) # all elements set to 1\n[1] 1 1 1 1 1\n\n> y[c(1,3,5)]=2; print(y) # 2,1,2,1,2\n[1] 2 1 2 1 2\n\n> # fancier plot of y:\n> plot(y,type=\"b\",lwd=3,col=\"gray\",pch=19,panel.first=grid(5,5))\n```\n\n----------------------------------------\n\nTITLE: Creating RDDs from Collections in Scala Spark\nDESCRIPTION: Demonstrates how to create an RDD from a Scala List collection using the parallelize method.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval collection = List(\"a\", \"b\", \"c\", \"d\", \"e\")\nval rddFromCollection = sc.parallelize(collection)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Newsgroup Distribution\nDESCRIPTION: Scala code to extract and count messages per newsgroup topic, displaying sorted results.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_197\n\nLANGUAGE: scala\nCODE:\n```\nval newsgroups = rdd.map { case (file, text) => file.split(\"/\").takeRight(2).head }\nval countByGroup = newsgroups.map(n => (n, 1)).reduceByKey(_ + _).collect.sortBy(-_._2).mkString(\"\\n\")\nprintln(countByGroup)\n```\n\n----------------------------------------\n\nTITLE: Applying Grid Search Methods to Real-Value Optimization Problems in R\nDESCRIPTION: Script demonstrating how grid search methods can be applied to real-value optimization tasks (sphere and rastrigin functions) with small dimensions. The code shows that both standard grid search and nested grid search can find good solutions for these benchmark functions in low dimensions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_23\n\nLANGUAGE: R\nCODE:\n```\n### real-grid.R file ###\n\nsource(\"blind.R\") # load the blind search methods\n\nsource(\"grid.R\") # load the grid search methods\n\n# real-value functions: sphere and rastrigin:\n\nsphere=function(x) sum(x^2)\n\nrastrigin=function(x) 10*length(x)+sum(x^2-10*cos(2*pi*x))\n\ncat(\"sphere:\\n\") # D=2, easy task\n\nS=gsearch(rep(1.1,2),rep(-5.2,2),rep(5.2,2),sphere,\"min\")\n\ncat(\"gsearch s:\",S$sol,\"f:\",S$eval,\"\\n\")\n\nS=ngsearch(3,rep(3,2),rep(-5.2,2),rep(5.2,2),sphere,\"min\")\n\ncat(\"ngsearch s:\",S$sol,\"f:\",S$eval,\"\\n\")\n\ncat(\"rastrigin:\\n\") # D=2, easy task\n\nS=gsearch(rep(1.1,2),rep(-5.2,2),rep(5.2,2),rastrigin,\"min\")\n\ncat(\"gsearch s:\",S$sol,\"f:\",S$eval,\"\\n\")\n\nS=ngsearch(3,rep(3,2),rep(-5.2,2),rep(5.2,2),rastrigin,\"min\")\n\ncat(\"ngsearch s:\",S$sol,\"f:\",S$eval,\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Implementing lexibest Function for Lexicographic Comparison in R\nDESCRIPTION: The lexibest function implements a lexicographic comparison between multiple solutions, returning the index of the best solution. It uses tolerance thresholds defined in the LEXI object to determine when solutions are close enough to be considered equivalent for a given objective.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_66\n\nLANGUAGE: R\nCODE:\n```\nif(minF>-1 && minF<1) tolerance=LEXI[i]\n\nelse tolerance=abs(LEXI[i]*minF)\n\nI=which((F-minF)<=tolerance)\n\nif(length(I)>0) # at least one candidate\n\ncandidates=candidates[I] # update candidates\n\nelse stop=TRUE\n\nif(!stop && i==m) stop=TRUE\n\nelse i=i+1\n\n}\n\nif(length(candidates)>1)\n\n{ # return highest priority goal if no clear winner:\n\nstop=FALSE; i=1\n\nwhile(!stop)\n\n{\n\nminF=min(x[candidates,i])\n\nI=which(x[candidates,i]==minF)\n\ncandidates=candidates[I]\n\nif(length(candidates)==1||i==m) stop=TRUE\n\nelse i=i+1\n\n}\n\n# remove (any) extra duplicate individuals:\n\ncandidates=candidates[1]\n\n}\n\n# return lexibest:\n\nreturn(candidates)\n```\n\n----------------------------------------\n\nTITLE: Examining TF-IDF Weights for Common Terms in Spark MLlib\nDESCRIPTION: Demonstrates how TF-IDF assigns lower weights to common terms by creating a document containing common words, transforming it to a TF-IDF vector, and examining the resulting weights.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_213\n\nLANGUAGE: scala\nCODE:\n```\nval common = sc.parallelize(Seq(Seq(\"you\", \"do\", \"we\")))\nval tfCommon = hashingTF.transform(common)\nval tfidfCommon = idf.transform(tfCommon)\nval commonVector = tfidfCommon.first.asInstanceOf[SV]\nprintln(commonVector.values.toSeq)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Mixed JOIN Syntax Style in SQL\nDESCRIPTION: Shows a blend of old and new join syntax styles, using INNER JOIN for some relations and comma-separated FROM clause for others.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_33\n\nLANGUAGE: sql\nCODE:\n```\nSELECT f.col1, d1.col2, d2.col3, d3.col4, d4.col5\nFROM FactTable AS f, Dim1 AS d1\n  INNER JOIN Dim2 AS d2 ON f.dim2_key = d2.dim2_key\n  INNER JOIN Dim3 AS d3 ON f.dim3_key = d3.dim3_key\n  INNER JOIN Dim4 AS d4 ON f.dim4_key = d4.dim4_key\nWHERE f.dim1_key = d1.dim1_key\n  AND <search arguments>\n```\n\n----------------------------------------\n\nTITLE: Reading and Plotting Weight-Formula Results in R\nDESCRIPTION: This snippet reads weight-formula results from a CSV file, sorts the data, and plots it with specific line style and legend. It's part of a comparison between NSGA-II and weighted-formula optimization approaches.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_73\n\nLANGUAGE: R\nCODE:\n```\nwf=read.table(\"wf-fes1.csv\",sep=\" \")\n\nI=sort.int(wf[,1],index.return=TRUE)\n\nlines(wf[I$ix,],type=\"b\",lty=2,lwd=2,pch=3)\n\nlegend(\"top\",c(\"NSGA-II\",\"weighted-formula\"),\n\nlwd=2,lty=1:2,pch=c(1,3))\n\ndev.off()\n```\n\n----------------------------------------\n\nTITLE: Comparing Grid Search Methods for Bag Pricing Optimization in R\nDESCRIPTION: Example script that compares three grid search methods (gsearch, gsearch2, and ngsearch) for optimizing bag prices. The code measures and reports execution time, showing that nested grid search achieves a solution with fewer evaluations than pure grid search methods.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_22\n\nLANGUAGE: R\nCODE:\n```\n### bag-grid.R file ###\n\nsource(\"blind.R\") # load the blind search methods\n\nsource(\"grid.R\") # load the grid search methods\n\nsource(\"functions.R\") # load the profit function\n\n# grid search for all bag prices, step of 100$\n\nPTM=proc.time() # start clock\n\nS1=gsearch(rep(100,5),rep(1,5),rep(1000,5),profit,\"max\")\n\nsec=(proc.time()-PTM)[3] # get seconds elapsed\n\ncat(\"gsearch best s:\",S1$sol,\"f:\",S1$eval,\"time:\",sec,\"s\\n\")\n\n# grid search 2 for all bag prices, step of 100$\n\nPTM=proc.time() # start clock\n\nS2=gsearch2(rep(100,5),rep(1,5),rep(1000,5),profit,\"max\")\n\nsec=(proc.time()-PTM)[3] # get seconds elapsed\n\ncat(\"gsearch2 best s:\",S2$sol,\"f:\",S2$eval,\"time:\",sec,\"s\\n\")\n\n# nested grid with 3 levels and initial step of 500$\n\nPTM=proc.time() # start clock\n\nS3=ngsearch(3,rep(500,5),rep(1,5),rep(1000,5),profit,\"max\")\n\nsec=(proc.time()-PTM)[3] # get seconds elapsed\n\ncat(\"ngsearch best s:\",S3$sol,\"f:\",S3$eval,\"time:\",sec,\"s\\n\")\n```\n\n----------------------------------------\n\nTITLE: Naive Bayes Data Preparation\nDESCRIPTION: Scala code to prepare data specifically for Naive Bayes classifier by handling negative values\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_92\n\nLANGUAGE: scala\nCODE:\n```\nval nbData = records.map { r =>\n  val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n  val label = trimmed(r.size - 1).toInt\n  val features = trimmed.slice(4, r.size - 1).map(d => if (d == \"?\") 0.0 else d.toDouble).map(d => if (d < 0) 0.0 else d)\n  LabeledPoint(label, Vectors.dense(features))\n}\n```\n\n----------------------------------------\n\nTITLE: Genetic Programming Solution Function in R\nDESCRIPTION: The evolved GP solution function for sunspot prediction. Takes two input variables x1 and x2 representing previous time steps and applies nonlinear transformations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_96\n\nLANGUAGE: R\nCODE:\n```\nfunction (x1, x2)\nx1/(x2 + x1) * (x1 + x1/(x2 + x1) * (x1 + x1 - x1/\n(1.3647488967524 +1.3647488967524)) - x1/(x2 + x1) * x1/\n(1.3647488967524 +(1.3647488967524 + 1.3647488967524/x2)))\n```\n\n----------------------------------------\n\nTITLE: Deleting the KAG project from OpenSPG server\nDESCRIPTION: Command to delete the KAG project and related knowledge graph from the OpenSPG server using a curl HTTP request.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://127.0.0.1:8887/project/api/delete?projectId=1\n```\n\n----------------------------------------\n\nTITLE: Calculating Average Record Length using RDD Operations\nDESCRIPTION: Demonstrates chaining RDD operations to calculate the average length of records using sum and count actions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nval sumOfRecords = intsFromStringsRDD.sum\nval numRecords = intsFromStringsRDD.count\nval aveLengthOfRecord = sumOfRecords / numRecords\n```\n\n----------------------------------------\n\nTITLE: Applying Hill Climbing to Sum of Bits Problem in R\nDESCRIPTION: Example application of hill climbing to optimize the sum of bits function. It starts from an all-zero solution and runs for 10 iterations with full reporting of each step, using a custom integer change function based on normal distribution.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_28\n\nLANGUAGE: R\nCODE:\n```\n### sumbits-hill.R file ###\n\nsource(\"hill.R\") # load the hill climbing methods\n\n# sum a raw binary object x (evaluation function):\nsumbin=function(x) sum(x)\n\n# hill climbing for sum of bits, one run:\nD=8 # dimension\ns=rep(0,D) # c(0,0,0,0,...)\nC=list(maxit=10,REPORT=1) # maximum of 10 iterations\nichange=function(par,lower,upper) # integer change\n{ hchange(par,lower,upper,rnorm,mean=0,sd=1) }\nhclimbing(s,sumbin,change=ichange,lower=rep(0,D),upper=rep(1,D),\n         control=C,type=\"max\")\n```\n\n----------------------------------------\n\nTITLE: Initializing KAG project with OpenSPG server\nDESCRIPTION: Command to restore and initialize the KAG project using knext CLI tool, connecting to a local OpenSPG server instance.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nknext project restore --host_addr http://127.0.0.1:8887 --proj_path .\n```\n\n----------------------------------------\n\nTITLE: Creating DoubleMatrix from Array in Scala using JBLAS\nDESCRIPTION: This code demonstrates how to create a DoubleMatrix object from an Array[Double] using the JBLAS library for linear algebra computations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_64\n\nLANGUAGE: Scala\nCODE:\n```\nimport org.jblas.DoubleMatrix\nval aMatrix = new DoubleMatrix(Array(1.0, 2.0, 3.0))\n```\n\n----------------------------------------\n\nTITLE: Extracting Movie Titles from Raw Data in Python\nDESCRIPTION: This function uses regular expressions to extract the movie title from a string that may include the release year in parentheses.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_41\n\nLANGUAGE: Python\nCODE:\n```\ndef extract_title(raw):\n  import re\n  grps = re.search(\"\\((\\w+)\\)\", raw)\n  if grps:\n    return raw[:grps.start()].strip() \n  else:\n    return raw\n```\n\n----------------------------------------\n\nTITLE: Simulated Annealing for Sum of Bits Optimization\nDESCRIPTION: Implementation of simulated annealing to optimize sum of bits problem with binary values. Uses custom change function and minimization approach.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_31\n\nLANGUAGE: R\nCODE:\n```\nsource(\"hill.R\") # get hchange function\n\nminsumbin=function(x) (length(x)-sum(x)) # optim only minimizes!\n\nD=8 # dimension\ns=rep(0,D) # c(0,0,0,0,...)\nC=list(maxit=10,temp=10,tmax=1,trace=TRUE,REPORT=1)\n\nbchange=function(par) # binary change\n{ D=length(par)\nhchange(par,lower=rep(0,D),upper=rep(1,D),rnorm,mean=0,sd=1)\n}\n\ns=optim(s,minsumbin,gr=bchange,method=\"SANN\",control=C)\ncat(\"best:\",s$par,\"f:\",s$value,\"(max: fs:\",sum(s$par),\")\\n\")\n```\n\n----------------------------------------\n\nTITLE: Matrix Operations in R\nDESCRIPTION: Creates a matrix and performs operations like square root and power calculations on rows and columns. Includes row and column sum calculations using apply().\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_108\n\nLANGUAGE: R\nCODE:\n```\nm=matrix(nrow=3,ncol=4)\nm[1,]=1:4\nm[2,]=sqrt(m[1,])\nm[3,]=sqrt(m[2,])\nm[,4]=m[,3]^2 # m[,3]*m[,3]\nprint(round(m,digits=2))\ncat(\"sums of rows:\",round(apply(m,1,sum),digits=2),\"\\n\")\ncat(\"sums of columns:\",round(apply(m,2,sum),digits=2),\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Plotting Forecast Comparison in R\nDESCRIPTION: Creates a PDF plot comparing actual sunspot values with forecasts from ARIMA and GP models. The plot includes multiple time series with different line types and a legend for comparison.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_95\n\nLANGUAGE: R\nCODE:\n```\nymin=min(c(outsamples,f1,f2))\n\nymax=max(c(outsamples,f1,f2))\n\npdf(\"fsunspots.pdf\")\n\npar(mar=c(4.0,4.0,0.1,0.1))\n\nplot(outsamples,ylim=c(ymin,ymax),type=\"b\",pch=1,\n\nxlab=\"time (years after 1980)\",ylab=\"values\",cex=0.8)\n\nlines(f1,lty=2,type=\"b\",pch=3,cex=0.5)\n\nlines(f2,lty=3,type=\"b\",pch=5,cex=0.5)\n\nlegend(\"topright\",c(\"sunspots\",text1,text2),lty=1:3,\n\npch=c(1,3,5))\n\ndev.off()\n```\n\n----------------------------------------\n\nTITLE: Cursor Usage Avoidance in SQL\nDESCRIPTION: Example demonstrating how to avoid using cursors by using set-based operations instead. This shows SELECT statements and JOIN operations as alternatives to cursor-based row-by-row processing.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_93\n\nLANGUAGE: sql\nCODE:\n```\nCREATE PROCEDURE...\nCURSOR...\n```\n\n----------------------------------------\n\nTITLE: Calculating MAP using Spark RankingMetrics\nDESCRIPTION: Implementation of Mean Average Precision calculation using Spark's RankingMetrics class. The code joins predicted and actual user ratings, transforms them into the required format, and computes MAP.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_81\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.evaluation.RankingMetrics\nval predictedAndTrueForRanking = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) => \n  val actual = actualWithIds.map(_._2)\n  (predicted.toArray, actual.toArray)\n}\nval rankingMetrics = new RankingMetrics(predictedAndTrueForRanking)\nprintln(\"Mean Average Precision = \" + rankingMetrics.meanAveragePrecision)\n```\n\n----------------------------------------\n\nTITLE: Executing Genetic Algorithm for Bag Prices Optimization in R\nDESCRIPTION: This code snippet executes the genetic algorithm using the rbga.bin function from the genalg package. It sets the population size, number of iterations, and other parameters for the optimization process.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_39\n\nLANGUAGE: R\nCODE:\n```\n# genetic algorithm execution:\nG=rbga.bin(size=size,popSize=50,iters=100,zeroToOneRatio=1,evalFunc=bprofit,elitism=1)\n```\n\n----------------------------------------\n\nTITLE: SQL Query Example Showing Problematic Table/Column Naming\nDESCRIPTION: An example query demonstrating the problem with using 'id' in base tables and prefixing foreign keys with table names, which creates confusion when writing joins.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nOrders.ID = OrderID\n```\n\n----------------------------------------\n\nTITLE: Broadcasting Item Matrix to Worker Nodes in Spark\nDESCRIPTION: Broadcasts the item matrix as a shared variable to all worker nodes in a Spark cluster to enable efficient distributed recommendation computation.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_76\n\nLANGUAGE: scala\nCODE:\n```\nval imBroadcast = sc.broadcast(itemMatrix)\n```\n\n----------------------------------------\n\nTITLE: Simulated Annealing for TSP in R\nDESCRIPTION: Implementation of the Simulated Annealing algorithm for solving the TSP problem. It includes initialization, execution, and timing measurements.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_87\n\nLANGUAGE: R\nCODE:\n```\n# SANN:\ncat(\"SANN run:\\n\")\nset.seed(12345) # for replicability\ns=sample(1:N,N) # initial solution\nEV=0; BEST=Inf; F=rep(NA,MAXIT) # reset these vars.\nC=list(maxit=MAXIT,temp=2000,trace=TRUE,REPORT=MAXIT)\nPTM=proc.time() # start clock\nSANN=optim(s,fn=tour,gr=insertion,method=\"SANN\",control=C)\nsec=(proc.time()-PTM)[3] # get seconds elapsed\ncat(\"time elapsed:\",sec,\"\\n\")\nRES[,1]=F\n```\n\n----------------------------------------\n\nTITLE: Real-Value FES1 Benchmark Optimization in R\nDESCRIPTION: Implementation of the FES1 benchmark function for real-value multi-objective optimization. The function takes a vector of real values and returns two objective values that need to be minimized simultaneously.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_63\n\nLANGUAGE: R\nCODE:\n```\nfes1=function(x)\n{ D=length(x);f1=0;f2=0\nfor(i in 1:D)\n{ f1=f1+abs(x[i]-exp((i/D)^2)/3)^0.5\nf2=f2+(x[i]-0.5*cos(10*pi/D)-0.5)^2\n}\nreturn(c(f1,f2))\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Broadcast Variable in Spark Operations\nDESCRIPTION: Shows how to access a broadcast variable within a Spark transformation using the value method\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\nsc.parallelize(List(\"1\", \"2\", \"3\")).map(x => broadcastAList.value ++ x).collect\n```\n\n----------------------------------------\n\nTITLE: Computing SVD Multiple Times to Verify Singular Values in Scala\nDESCRIPTION: This Scala code computes SVD multiple times with increasing values of k (1 to 5) to demonstrate that singular values are consistent across runs and returned in decreasing order.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_190\n\nLANGUAGE: scala\nCODE:\n```\nval sValues = (1 to 5).map { i => matrix.computeSVD(i, computeU = false).s }\nsValues.foreach(println)\n```\n\n----------------------------------------\n\nTITLE: Processing Test Dataset for Classification\nDESCRIPTION: Prepares test data by loading documents, extracting labels, and computing TF-IDF features using the same transformations as training data\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_218\n\nLANGUAGE: scala\nCODE:\n```\nval testPath = \"/PATH/20news-bydate-test/*\"\nval testRDD = sc.wholeTextFiles(testPath)\nval testLabels = testRDD.map { case (file, text) =>\n  val topic = file.split(\"/\").takeRight(2).head\n  newsgroupsMap(topic)\n}\n\nval testTf = testRDD.map { case (file, text) => hashingTF.transform(tokenize(text)) }\nval testTfIdf = idf.transform(testTf)\nval zippedTest = testLabels.zip(testTfIdf)\nval test = zippedTest.map { case (topic, vector) => LabeledPoint(topic, vector) }\n```\n\n----------------------------------------\n\nTITLE: Creating a View with Calculated Column in SQL\nDESCRIPTION: This SQL snippet creates a view that includes a calculated total compensation column, demonstrating proper use of views for consistent data derivation.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_51\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE VIEW Personnel_plus AS\n   SELECT name, salary, commission, bonus,\n          (salary + commission + bonus) AS tot_comp\n   FROM Personnel;\n```\n\n----------------------------------------\n\nTITLE: CHECK Constraint Implementation\nDESCRIPTION: Example showing proper implementation of CHECK constraints for data validation at the table level. Demonstrates both column-level and table-level constraint definitions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_94\n\nLANGUAGE: sql\nCODE:\n```\nCHECK() constraints...\n```\n\n----------------------------------------\n\nTITLE: Implementing Local Repair Strategy for Constrained Optimization in R\nDESCRIPTION: A local search method that repairs infeasible solutions for the bag prices problem. When a solution violates the constraint (more than 50 bags), the function iteratively increases prices to reduce sales until the solution becomes feasible, demonstrating domain-specific repair strategy.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_52\n\nLANGUAGE: R\nCODE:\n```\nlocalRepair=function(eda, gen, pop, popEval, f, lower, upper)\n{\n\nfor(i in 1:nrow(pop))\n\n{ x=pop[i,]\n\nx=round(x,digits=0) # convert x into integer\n\nx=ifelse(x<lower[1],lower[1],x) # assure x within\n\nx=ifelse(x>upper[1],upper[1],x) # bounds\n\ns=sales(x)\n\nif(sum(s)>50)\n\n{\n\nx1=x\n\nwhile(sum(s)>50) # new constraint: repair\n\n{ # increase price to reduce sales:\n\nx1=x1+abs(round(rnorm(D,mean=0,sd=5)))\n\nx1=ifelse(x1>upper[1],upper[1],x1) # bound if needed\n\ns=sales(x1)\n\n}\n\nx=x1 # update the new x\n\n}\n\npop[i,]=x;popEval[i]=f(x)\n\n}\n\nreturn(list(pop=pop,popEval=popEval))\n}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Target Variable Distributions in Python\nDESCRIPTION: Code to create histograms of raw, log-transformed, and square-root-transformed target variables using matplotlib.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_128\n\nLANGUAGE: python\nCODE:\n```\ntargets = records.map(lambda r: float(r[-1])).collect()\nhist(targets, bins=40, color='lightblue', normed=True)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(16, 10)\n\nlog_targets = records.map(lambda r: np.log(float(r[-1]))).collect()\nhist(log_targets, bins=40, color='lightblue', normed=True)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(16, 10)\n\nsqrt_targets = records.map(lambda r: np.sqrt(float(r[-1]))).collect()\nhist(sqrt_targets, bins=40, color='lightblue', normed=True)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(16, 10)\n```\n\n----------------------------------------\n\nTITLE: Computing Factorial in PCF\nDESCRIPTION: Example of a factorial computation in PCF. Unlike reference assignment, this action produces a value rather than changing the global state.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_33\n\nLANGUAGE: theoretical\nCODE:\n```\nfact 3\n```\n\n----------------------------------------\n\nTITLE: Starting Spark Shell with Memory Configuration\nDESCRIPTION: Command to start Spark Scala shell with 4GB driver memory allocation for processing the dataset.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_195\n\nLANGUAGE: shell\nCODE:\n```\n./SPARK_HOME/bin/spark-shell --driver-memory 4g\n```\n\n----------------------------------------\n\nTITLE: Importing Spark MLlib ALS Recommendation Module\nDESCRIPTION: Imports the Alternating Least Squares (ALS) recommendation algorithm from Spark's MLlib, which will be used to build the recommendation model.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_54\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.recommendation.ALS\n```\n\n----------------------------------------\n\nTITLE: Implementing Sphere Function Optimization using PSO in R\nDESCRIPTION: Demonstrates using the pso package to optimize a 2-dimensional sphere function. Sets up PSO parameters including population size, maximum iterations, and tracking options. Uses psoptim function with defined bounds and control parameters.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_44\n\nLANGUAGE: R\nCODE:\n```\nlibrary(pso) # load pso\n\nsphere=function(x) sum(x^2)\n\nD=2; maxit=10; s=5\n\nset.seed(12345) # set for replicability\n\nC=list(trace=1,maxit=maxit,REPORT=1,trace.stats=1,s=s)\n\n# perform the optimization:\nPSO=psoptim(rep(NA,D),fn=sphere,lower=rep(-5.2,D),\nupper=rep(5.2,D),control=C)\n```\n\n----------------------------------------\n\nTITLE: Typed PCF Function Examples\nDESCRIPTION: Examples of typed PCF functions showing how type annotations are applied to function parameters. These examples illustrate the syntax for the identity function on natural numbers and on functions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_24\n\nLANGUAGE: PCF\nCODE:\n```\nfun x:nat -> x                  // identity function on natural numbers\nfun x:(nat -> nat) -> x         // identity function on functions from natural numbers to natural numbers\n```\n\n----------------------------------------\n\nTITLE: Let Binding Rule in PCF\nDESCRIPTION: Defines the rule for let bindings in PCF's small-step operational semantics.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_14\n\nLANGUAGE: PCF\nCODE:\n```\nlet x = t in u ⟶ u[x := t]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Basic R Objects and Functions\nDESCRIPTION: This snippet shows how to create and manipulate basic R objects, use built-in functions, and perform simple operations. It covers character strings, numeric values, and built-in constants.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_5\n\nLANGUAGE: R\nCODE:\n```\n> s=\"hello world\"\n\n> print(class(s)) # character\n[1] \"character\"\n\n> print(s) # \"hello world\"\n[1] \"hello world\"\n\n> x=1.1\n\n> print(class(x)) # numeric\n[1] \"numeric\"\n\n> print(summary(x)) # summary of x\nMin. 1st Qu. Median Mean 3rd Qu. Max.\n1.1 1.1 1.1 1.1 1.1 1.1\n\n> plot(x)\n\n> print(x) # 1.1\n[1] 1.1\n\n> print(pi) # 3.141593\n[1] 3.141593\n\n> print(sqrt(-1)) # NaN\n[1] NaN\nWarning message:\nIn sqrt(-1) : NaNs produced\n\n> print(1/0) # Inf\n[1] Inf\n```\n\n----------------------------------------\n\nTITLE: Creating Dictionary for Categorical Variable Encoding in Python\nDESCRIPTION: Code to assign index values to each possible occupation value, creating a dictionary mapping from occupation names to numerical indices.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nidx = 0\nall_occupations_dict = {}\nfor o in all_occupations:\n    all_occupations_dict[o] = idx\n    idx +=1\n# try a few examples to see what \"1-of-k\" encoding is assigned\nprint \"Encoding of 'doctor': %d\" % all_occupations_dict['doctor']\nprint \"Encoding of 'programmer': %d\" % all_occupations_dict['programmer']\n```\n\n----------------------------------------\n\nTITLE: Avoiding NULL Traps with CASE in SQL\nDESCRIPTION: Demonstrates how a NULL value in a simple CASE expression always results in the ELSE clause because NULL = value is always UNKNOWN, not TRUE.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_38\n\nLANGUAGE: sql\nCODE:\n```\nCASE NULL\n  WHEN 1 THEN 'One'\n  WHEN 2 THEN 'Two'\n  ELSE 'Other'\nEND\n```\n\n----------------------------------------\n\nTITLE: Optimization Method Comparison Framework in R\nDESCRIPTION: A comprehensive framework for comparing population-based optimization methods including EA, DE, PSO, and EDA. The code includes custom evaluation functions, method execution wrappers, and analysis tools that track performance across multiple runs and generate comparison plots.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_49\n\nLANGUAGE: R\nCODE:\n```\n### compare2.R file ###\n\nsource(\"functions.R\") # bag prices functions\n\nlibrary(genalg)\n\nlibrary(DEoptim)\n\nlibrary(pso)\n\nlibrary(copulaedas)\n\n# evaluation functions: ------------------------------------\n\ncrastrigin=function(x) # adapted rastrigin\n{ f=10*length(x)+sum(x^2-10*cos(2*pi*x))\n\n# global assignment code: <<-\n\nEV<<-EV+1 # increase evaluations\n\nif(f<BEST) BEST<<-f # minimum value\n\nif(EV<=MAXFN) F[EV]<<-BEST\n\nreturn(f)\n\n}\n\ncprofit=function(x) # adapted bag prices\n{ x=round(x,digits=0) # convert x into integer\n\n# given that EDA occasionally produces unbounded values:\n\nx=ifelse(x<1,1,x) # assure that x is within\n\nx=ifelse(x>1000,1000,x) # the [1,1000] bounds\n\ns=sales(x) # get the expected sales\n\nc=cost(s) # get the expected cost\n\nprofit=sum(s*x-c) # compute the profit\n\nEV<<-EV+1 # increase evaluations\n\nif(profit>BEST) BEST<<-profit # maximum value\n\nif(EV<=MAXFN) F[EV]<<-BEST\n\nreturn(-profit) # minimization task!\n\n}\n\n# auxiliary functions: ------------------------------------\n\ncrun=function(method,f,lower,upper,LP,maxit) # run a method\n{ if(method==\"EA\")\n\nrbga(evalFunc=f,stringMin=lower,stringMax=upper,popSize=LP,\n\niters=maxit*1.5)\n\nelse if(method==\"DE\")\n\n{ C=DEoptim.control(itermax=maxit,trace=FALSE,NP=LP)\n\nDEoptim(f,lower=lower,upper=upper,control=C)\n\n}\n\nelse if(method==\"PSO\")\n\n{ C=list(maxit=maxit,s=LP)\n\npsoptim(rep(NA,length(lower)),fn=f,\n\nlower=lower,upper=upper,control=C)\n\n}\n\nelse if(method==\"EDA\")\n\n{ setMethod(\"edaTerminate\",\"EDA\",edaTerminateMaxGen)\n\nGCEDA=CEDA(copula=\"normal\",margin=\"norm\",popSize=LP,\n\nmaxGen=maxit)\n\nGCEDA@name=\"GCEDA\"\n\nedaRun(GCEDA,f,lower,upper)\n\n}\n\n}\n\nsuccesses=function(x,LIM,type=\"min\") # number of successes\n{ if(type==\"min\") return(sum(x<LIM)) else return(sum(x>LIM)) }\n\nctest=function(Methods,f,lower,upper,type=\"min\",Runs, # test\n\nD,MAXFN,maxit,LP,pdf,main,LIM) # all methods:\n\n{ RES=vector(\"list\",length(Methods)) # all results\n\nVAL=matrix(nrow=Runs,ncol=length(Methods)) # best values\n\nfor(m in 1:length(Methods)) # initialize RES object\n\nRES[[m]]=matrix(nrow=MAXFN,ncol=Runs)\n\nfor(R in 1:Runs) # cycle all runs\n\nfor(m in 1:length(Methods))\n\n{ EV<<-0; F<<-rep(NA,MAXFN) # reset EV and F\n\nif(type==\"min\") BEST<<-Inf else BEST<<\\- -Inf # reset BEST\n\nsuppressWarnings(crun(Methods[m],f,lower,upper,LP,maxit))\n\nRES[[m]][,R]=F # store all best values\n\nVAL[R,m]=F[MAXFN] # store best value at MAXFN\n\n}\n\n# compute average F result per method:\n\nAV=matrix(nrow=MAXFN,ncol=length(Methods))\n\nfor(m in 1:length(Methods))\n\nfor(i in 1:MAXFN)\n\nAV[i,m]=mean(RES[[m]][i,])\n\n# show results:\n\ncat(main,\"\\n\",Methods,\"\\n\")\n\ncat(round(apply(VAL,2,mean),digits=0),\" (average best)\\n\")\n\ncat(round(100*apply(VAL,2,successes,LIM,type)/Runs,\n\ndigits=0),\" (%successes)\\n\")\n\n# create pdf file:\n\npdf(paste(pdf,\".pdf\",sep=\"\"),width=5,height=5,paper=\"special\")\n\npar(mar=c(4.0,4.0,1.8,0.6)) # reduce default plot margin\n\nMIN=min(AV);MAX=max(AV)\n\n# use a grid to improve clarity:\n\ng1=seq(1,MAXFN,length.out=500) # grid for lines\n\nplot(g1,AV[g1,1],ylim=c(MIN,MAX),type=\"l\",lwd=2,main=main,\n\nylab=\"average best\",xlab=\"number of evaluations\")\n\nfor(i in 2:length(Methods)) lines(g1,AV[g1,i],lwd=2,lty=i)\n\nif(type==\"min\") position=\"topright\" else position=\n\n\"bottomright\"\n\nlegend(position,legend=Methods,lwd=2,lty=1:length(Methods))\n\ndev.off() # close the PDF device\n\n}\n\n# define EV, BEST and F:\n\nMAXFN=10000\n\nEV=0;BEST=Inf;F=rep(NA,MAXFN)\n\n# define method labels:\n\nMethods=c(\"EA\",\"DE\",\"PSO\",\"EDA\")\n\n# rastrigin comparison: -----------------------------------\n\nRuns=50; D=20; LP=100; maxit=100\n\nlower=rep(-5.2,D);upper=rep(5.2,D)\n\nctest(Methods,crastrigin,lower,upper,\"min\",Runs,D,MAXFN,maxit,\n\nLP,\n\n\"comp-rastrigin2\",\"rastrigin (D=20)\",75)\n```\n\n----------------------------------------\n\nTITLE: Defining Object Methods and Late Binding in PCF\nDESCRIPTION: An example demonstrating late binding in object-oriented programming using PCF. The code creates an object with a field 'b' that accesses another field 'a', and then updates field 'a' to show the late binding behavior.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_39\n\nLANGUAGE: PCF\nCODE:\n```\nlet t = {a = ςs 10, b = ςs s#a + 1} in\nlet u = t(a <- ςs 9) in\nu#b\n```\n\n----------------------------------------\n\nTITLE: Initializing Bag Prices Comparison Parameters in R\nDESCRIPTION: Setup for comparing different optimization methods (EA, DE, PSO, EDA) on the bag prices problem. Defines maximum function evaluations, dimensions, population size, and other parameters for the optimization process.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_50\n\nLANGUAGE: R\nCODE:\n```\nMAXFN=5000\n\nF=rep(NA,MAXFN)\n\nRuns=50; D=5; LP=50; maxit=100\n\nlower=rep(1,D);upper=rep(1000,D)\n\nctest(Methods,cprofit,lower,upper,\"max\",Runs,D,MAXFN,maxit,LP,\n\n\"comp-bagprices\",\"bag prices (D=5)\",43500)\n```\n\n----------------------------------------\n\nTITLE: Setting Working Directory in R\nDESCRIPTION: Shows how to change the working directory using the setwd() function with a relative path.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_2\n\nLANGUAGE: R\nCODE:\n```\n> setwd(\"../directory\")\n```\n\n----------------------------------------\n\nTITLE: Finding Least Frequent Tokens in Spark RDD\nDESCRIPTION: This code retrieves and displays the 20 least frequent tokens in the corpus by using a custom ordering in descending order of frequency.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_205\n\nLANGUAGE: scala\nCODE:\n```\nval oreringAsc = Ordering.by[(String, Int), Int](-_._2)\nprintln(tokenCountsFilteredSize.top(20)(oreringAsc).mkString(\"\\n\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing Nested Grid Search Function in R\nDESCRIPTION: Function that implements a nested grid search algorithm with multiple levels of refinement. It recursively narrows the search space around the best solution from the previous level, halving the step size in each iteration. The function stops when either the maximum number of levels is reached or when the search range becomes smaller than the step size.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_21\n\nLANGUAGE: R\nCODE:\n```\nngsearch=function(levels,step,lower,upper,FUN,type,...)\n\n{ stop=FALSE;i=1 # auxiliary objects\n\nbcur=switch(type,min=list(sol=NULL,eval=Inf),\n\nmax=list(sol=NULL,eval=-Inf))\n\nwhile(!stop) # cycle while stopping criteria is not met\n\n{\n\ns=gsearch(step,lower,upper,FUN,type,...)\n\n# if needed, update best current solution:\n\nif( (type==\"min\" && s$eval<bcur$eval)||\n\n(type==\"max\" && s$eval>bcur$eval)) bcur=s\n\nif(i<levels) # update step, lower and upper:\n\n{ step=step/2\n\ninterval=(upper-lower)/4\n\nlower=sapply(lower,max,s$sol-interval)\n\nupper=sapply(upper,min,s$sol+interval)\n\n}\n\nif(i>=levels || sum((upper-lower)<=step)>0) stop=TRUE\n\nelse i=i+1\n\n}\n\nreturn(bcur) # best solution\n\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Containers in R\nDESCRIPTION: Demonstrates the use of elementwise logical operators for filtering vectors and data frames in R. Shows selection of elements based on conditions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_11\n\nLANGUAGE: R\nCODE:\n```\nx=1:10;print(x)\nprint(x>=3&x<8) # select some elements\nI=which(x>=3&x<8);print(I) # indexes of selection\n\nd=data.frame(x=1:4,f=factor(c(rep(\"a\",2),rep(\"b\",2))))\nprint(d)\nprint(d[d$x<2|d$f==\"b\",]) # select rows\n```\n\n----------------------------------------\n\nTITLE: Setting AWS Credentials and Key Pair Permissions\nDESCRIPTION: Commands to set the proper permissions for an AWS key pair file and to export the AWS access credentials as environment variables for use with the spark-ec2 script.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n> chmod 600 spark.pem\n> export AWS_ACCESS_KEY_ID=\"...\"\n> export AWS_SECRET_ACCESS_KEY=\"...\"\n```\n\n----------------------------------------\n\nTITLE: List Operations in R\nDESCRIPTION: This snippet demonstrates the creation and manipulation of lists in R. It shows how to create lists with named components, access and modify list elements, and create nested lists.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_9\n\nLANGUAGE: R\nCODE:\n```\n> l=list(a=\"hello\",b=1:3) # list with 2 components\n\n> print(summary(l)) # summary of l\nLength Class Mode    \na      1     -none- character\nb      3     -none- numeric  \n\n> print(l) # show l\n$a\n[1] \"hello\"\n\n$b\n[1] 1 2 3\n\n> l$b=l$b^2+1;print(l) # change b to (b*b)+1\n$a\n[1] \"hello\"\n\n$b\n[1]  2  5 10\n\n> v=vector(\"list\",3) # vector list\n\n> v[[1]]=1:3 # change 1st element of v\n\n> v[[2]]=2 # change 2nd element of v\n\n> v[[3]]=l # change 3rd element of v\n\n> print(v) # show v\n[[1]]\n[1] 1 2 3\n\n[[2]]\n[1] 2\n\n[[3]]\n[[3]]$a\n[1] \"hello\"\n\n[[3]]$b\n[1] 2 3 4\n\n> print(length(v)) # length of v\n[1] 3\n```\n\n----------------------------------------\n\nTITLE: Filtering Tokens by Length in Spark RDD\nDESCRIPTION: This snippet filters out tokens that are only one character in length from the token counts RDD and displays the top 20 remaining tokens.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_204\n\nLANGUAGE: scala\nCODE:\n```\nval tokenCountsFilteredSize = tokenCountsFilteredStopwords.filter { case (k, v) => k.size >= 2 }\nprintln(tokenCountsFilteredSize.top(20)(oreringDesc).mkString(\"\\n\"))\n```\n\n----------------------------------------\n\nTITLE: Robinson's Unification Algorithm for PCF Types\nDESCRIPTION: A set of transformation rules used to solve type equations in PCF, part of Hindley's type inference algorithm.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_27\n\nLANGUAGE: PCF\nCODE:\n```\n1. A -> B = C -> D  ⇒  {A = C, B = D}\n2. nat = nat  ⇒  ∅\n3. nat = A -> B or A -> B = nat  ⇒  fail\n4. X = X  ⇒  ∅\n5. X = A or A = X (X in A, A ≠ X)  ⇒  fail\n6. X = A or A = X (X not in A)  ⇒  substitute X with A in other equations\n```\n\n----------------------------------------\n\nTITLE: Creating Tables for Manager Constraint Enforcement\nDESCRIPTION: Creating two tables to handle store personnel assignments, separating manager and non-manager roles to enforce the single manager constraint.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_54\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE NonManager_Assignments\n(\n   personnel_nbr INTEGER NOT NULL,\n   store_nbr INTEGER NOT NULL,\n   job_type INTEGER NOT NULL CHECK (job_type < 99),\n   date_assigned TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n   PRIMARY KEY (personnel_nbr, store_nbr, job_type)\n);\n\nCREATE TABLE Manager_Assignments\n(\n   personnel_nbr INTEGER NOT NULL,\n   store_nbr INTEGER NOT NULL UNIQUE, -- enforces only one manager per store\n   job_type INTEGER NOT NULL CHECK (job_type = 99),\n   date_assigned TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n   PRIMARY KEY (personnel_nbr, store_nbr)\n);\n```\n\n----------------------------------------\n\nTITLE: Type Inference Rules for PCF\nDESCRIPTION: A set of inference rules defining how types are assigned to PCF terms. These rules generate both a type and a set of equations to be solved.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_25\n\nLANGUAGE: PCF\nCODE:\n```\ne ⊢ x : A if e contains x : A\n\ne ⊢ u : A    e ⊢ t : A -> B\n--------------------------\n      e ⊢ t u : B\n\n(e, x : A) ⊢ t : B\n----------------------------\ne ⊢ fun x -> t : A -> B\n\n   e ⊢ n : nat\n\ne ⊢ u : nat    e ⊢ t : nat\n---------------------------\n   e ⊢ t ⊗ u : nat\n\ne ⊢ t : nat    e ⊢ u : A    e ⊢ v : A\n-------------------------------------\n   e ⊢ ifz t then u else v : A\n\n(e, x : A) ⊢ t : A\n-------------------\ne ⊢ fix x t : A\n\ne ⊢ t : A    (e, x : A) ⊢ u : B\n--------------------------------\n   e ⊢ let x = t in u : B\n```\n\n----------------------------------------\n\nTITLE: Compilation Rules for PCF to Abstract Machine Code\nDESCRIPTION: A set of rules defining how PCF expressions are compiled into abstract machine instructions. Each rule shows how to translate a specific PCF construct like variables, function applications, abstractions, and arithmetic operations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_21\n\nLANGUAGE: PCF\nCODE:\n```\n  * |x|e =Search n where n is the position of x in the environment e\n\n  * |t u|e =Pushenv, |u|e,Push, |t|e,Apply,Popenv\n\n  * |fun x -> t|e =Mkclos |t|e,_,x\n\n  * |fixfun f x -> t|e =Mkclos |t|e, f, x\n\n  * |n|e =Ldi n\n\n  * |t + u|e = |u|e,Push, |t|e,Add\n\n  * |t - u|e = |u|e,Push, |t|e,Sub\n\n  * |t * u|e = |u|e,Push, |t|e,Mult\n\n  * |t / u|e = |u|e,Push, |t|e,Div\n\n  * |ifz t then u else v|e = |t|e,Test(|u|e,|v|e)\n\n  * |let x = t in u|e =Pushenv, |t|e,Extend, |u|e, x,Popenv\n```\n\n----------------------------------------\n\nTITLE: SQL Reserved Word Capitalization\nDESCRIPTION: Demonstrates how uppercase reserved words improve SQL statement readability by making clauses more distinct and easier to locate visually.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nselect name from Personnel where dept_id = 123\n```\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT name FROM Personnel WHERE dept_id = 123\n```\n\n----------------------------------------\n\nTITLE: Simple SQL Query with Parameter\nDESCRIPTION: Illustrates a basic SQL query with a parameter. This example is used to discuss optimizer behavior with different parameter values.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_75\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT *\nFROM Personnel\nWHERE sex_code = :input_sex_code;\n\n```\n\n----------------------------------------\n\nTITLE: Creating a VIEW for Combining Detail and Aggregate Data\nDESCRIPTION: Example of creating a VIEW that calculates stock price aggregates, which is then joined to the original table to display both detail and summary information.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_59\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE VIEW StockPriceStats AS\nSELECT stock_id,\n       MAX(price) AS max_price,\n       MIN(price) AS min_price,\n       AVG(price) AS avg_price\nFROM StockPrices\nGROUP BY stock_id;\n```\n\n----------------------------------------\n\nTITLE: Creating Multi-Argument Functions in PCF\nDESCRIPTION: Illustrates how to create a function with multiple arguments in PCF using nested single-argument functions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_6\n\nLANGUAGE: PCF\nCODE:\n```\nfun x -> fun y -> x * x + y * y\n```\n\n----------------------------------------\n\nTITLE: Evaluating Linear Regression Iterations in Spark\nDESCRIPTION: This snippet evaluates the impact of different numbers of iterations on a linear regression model using Spark MLlib. It uses the evaluate function to compute the RMSLE metric for various iteration counts.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_139\n\nLANGUAGE: Python\nCODE:\n```\nparams = [1, 5, 10, 20, 50, 100]\nmetrics = [evaluate(train_data, test_data, param, 0.01, 0.0, 'l2', False) for param in params]\nprint params\nprint metrics\n```\n\n----------------------------------------\n\nTITLE: Rebuilding a row from scratch\nDESCRIPTION: Example showing how rebuilding a row from scratch can cause problems when auto-number IDs are used instead of natural keys.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_19\n\nLANGUAGE: SQL\nCODE:\n```\nDELETE FROM Personnel WHERE ssn = '123-45-6789';\nINSERT INTO Personnel (ssn, name, ...) VALUES ('123-45-6789', 'John Doe', ...);\n```\n\n----------------------------------------\n\nTITLE: TSP Tour Evaluation Function in R\nDESCRIPTION: Definition of the tour evaluation function for the TSP problem. It calculates the total distance of a tour, increments the evaluation counter, and tracks the best solution found so far.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_84\n\nLANGUAGE: R\nCODE:\n```\nMAXIT=100000\nMethods=c(\"SANN\",\"EA\",\"LEA\") # comparison of 3 methods\nRES=matrix(nrow=MAXIT,ncol=length(Methods))\nMD=as.matrix(D)\n\n# overall distance of a tour (evaluation function):\ntour=function(s)\n{ # compute tour length:\n  EV<<-EV+1 # increase evaluations\n  s=c(s,s[1]) # start city is also end city\n  res=0\n  for(i in 2:length(s)) res=res+MD[s[i],s[i-1]]\n  # store memory with best values:\n  if(res<BEST) BEST<<-res\n  if(EV<=MAXIT) F[EV]<<-BEST\n  # only for hybrid method:\n  # return tour\n  return(res)\n}\n```\n\n----------------------------------------\n\nTITLE: Using CHECK Constraints for Domain Value Validation\nDESCRIPTION: Implementing CHECK constraints to validate domain values for a table column, which is a more efficient approach than using VIEWs with CHECK OPTION for domain enforcement.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_58\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE T1\n(\n   C1 INTEGER CHECK (C1 BETWEEN 1 AND 100),\n   ...);\n```\n\n----------------------------------------\n\nTITLE: Assigning Time of Day Categories in Python\nDESCRIPTION: This function categorizes hours into time of day buckets (morning, lunch, afternoon, evening, night). It's used to further refine the hour-of-day feature.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_39\n\nLANGUAGE: Python\nCODE:\n```\ndef assign_tod(hr):\n  times_of_day = {\n    'morning' : range(7, 12),\n    'lunch' : range(12, 14),\n    'afternoon' : range(14, 18),\n    'evening' : range(18, 23),\n    'night' : range(23, 7)\n  }\n  for k, v in times_of_day.iteritems():\n    if hr in v: \n      return k\n```\n\n----------------------------------------\n\nTITLE: Querying Company Supply Relationships\nDESCRIPTION: GQL query to retrieve the main supply relationships between companies in the knowledge graph.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nknext reasoner execute --dsl \"\nMATCH\n    (s:SupplyChain.Company)-[:mainSupply]->(o:SupplyChain.Company)\nRETURN\n    s.name, o.name\n\"\n```\n\n----------------------------------------\n\nTITLE: Querying Supply Chain Relationships\nDESCRIPTION: GQL query to identify main supply relationships between companies in the supply chain.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README_cn.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nknext reasoner execute --dsl \"\nMATCH\n    (s:SupplyChain.Company)-[:mainSupply]->(o:SupplyChain.Company)\nRETURN\n    s.name, o.name\n\"\n```\n\n----------------------------------------\n\nTITLE: Transforming RDDs using Map Operation\nDESCRIPTION: Example of using map transformation to convert an RDD of Strings to an RDD of Integers by calculating string lengths.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval intsFromStringsRDD = rddFromTextFile.map(line => line.size)\n```\n\n----------------------------------------\n\nTITLE: Avoiding Redundant WHERE Conditions in SQL\nDESCRIPTION: Demonstrates redundant predicates in a WHERE clause that don't add value. Modern optimizers can deduce these relationships, making the extra conditions unnecessary.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_40\n\nLANGUAGE: sql\nCODE:\n```\nSELECT *\nFROM Foobar\nWHERE x < 10\n  AND x < 20\n  AND x < 30\n```\n\n----------------------------------------\n\nTITLE: Tokenizing Movie Titles in PySpark\nDESCRIPTION: This PySpark code extracts movie titles from the movie_fields RDD, applies the extract_title function, and then tokenizes the titles using simple whitespace tokenization.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_42\n\nLANGUAGE: Python\nCODE:\n```\nraw_titles = movie_fields.map(lambda fields: fields[1])\nmovie_titles = raw_titles.map(lambda m: extract_title(m))\ntitle_terms = movie_titles.map(lambda t: t.split(\" \"))\nprint title_terms.take(5)\n```\n\n----------------------------------------\n\nTITLE: Converting Terms to Sparse Vector Representation\nDESCRIPTION: Creates a function that transforms a list of terms into a sparse vector representation using scipy's sparse matrix. Each term in the input is mapped to a corresponding index in the vector based on the term dictionary.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_46\n\nLANGUAGE: python\nCODE:\n```\n# this function takes a list of terms and encodes it as a scipy sparse vector using an approach \n# similar to the 1-of-k encoding\ndef create_vector(terms, term_dict):\n  from scipy import sparse as sp\n    num_terms = len(term_dict)\n    x = sp.csc_matrix((1, num_terms))  \n    for t in terms:\n      if t in term_dict:\n        idx = term_dict[t]\n        x[0, idx] = 1\n  return x\n```\n\n----------------------------------------\n\nTITLE: Calculating Root Mean Squared Error in Scala\nDESCRIPTION: This code calculates the Root Mean Squared Error (RMSE) from the previously computed Mean Squared Error (MSE) for a recommendation model.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_70\n\nLANGUAGE: Scala\nCODE:\n```\nval RMSE = math.sqrt(MSE)\nprintln(\"Root Mean Squared Error = \" + RMSE)\n```\n\n----------------------------------------\n\nTITLE: Modifying Reference Value in PCF\nDESCRIPTION: Shows how to modify the value of a reference x in PCF by assigning it the value of term t.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_31\n\nLANGUAGE: PCF\nCODE:\n```\nx := t\n```\n\n----------------------------------------\n\nTITLE: NULL Comparison Expansion with CASE in SQL\nDESCRIPTION: Shows how a simple CASE with NULL gets expanded in the SQL standard, demonstrating why NULL comparisons always return UNKNOWN.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_39\n\nLANGUAGE: sql\nCODE:\n```\nCASE\n  WHEN NULL = 1 THEN 'One'\n  WHEN NULL = 2 THEN 'Two'\n  ELSE 'Other'\nEND\n```\n\n----------------------------------------\n\nTITLE: Filtering Stop Words from Token Counts in Spark\nDESCRIPTION: This code defines a set of stop words and filters them out from the token counts. It then displays the top 20 remaining tokens by frequency.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_203\n\nLANGUAGE: scala\nCODE:\n```\nval stopwords = Set(\n  \"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\"but\", \"is\", \"not\", \"with\", \"as\", \"was\", \"if\",\n  \"they\", \"are\", \"this\", \"and\", \"it\", \"have\", \"from\", \"at\", \"my\", \"be\", \"that\", \"to\"\n)\nval tokenCountsFilteredStopwords = tokenCounts.filter { case (k, v) => !stopwords.contains(k) }\nprintln(tokenCountsFilteredStopwords.top(20)(oreringDesc).mkString(\"\\n\"))\n```\n\n----------------------------------------\n\nTITLE: Building the knowledge graph with indexer\nDESCRIPTION: Commands to navigate to the builder directory and execute the indexer.py script which processes the data and builds the knowledge graph.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd builder && python indexer.py && cd ..\n```\n\n----------------------------------------\n\nTITLE: Training Decision Tree Regressor on Log-Transformed Data\nDESCRIPTION: Applies log transformation to the decision tree dataset and trains a DecisionTree regressor model with default parameters.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_133\n\nLANGUAGE: python\nCODE:\n```\ndata_dt_log = data_dt.map(lambda lp: LabeledPoint(np.log(lp.label), lp.features))\ndt_model_log = DecisionTree.trainRegressor(data_dt_log,{})\n```\n\n----------------------------------------\n\nTITLE: Demonstrating New-Style INNER JOIN Syntax in SQL\nDESCRIPTION: Shows the modern INNER JOIN syntax which separates join conditions from filtering conditions. This style clearly indicates join types but may be harder to read with many tables.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_32\n\nLANGUAGE: sql\nCODE:\n```\nSELECT f.col1, d1.col2, d2.col3, d3.col4, d4.col5\nFROM FactTable AS f\n  INNER JOIN Dim1 AS d1 ON f.dim1_key = d1.dim1_key\n  INNER JOIN Dim2 AS d2 ON f.dim2_key = d2.dim2_key\n  INNER JOIN Dim3 AS d3 ON f.dim3_key = d3.dim3_key\n  INNER JOIN Dim4 AS d4 ON f.dim4_key = d4.dim4_key\nWHERE <search arguments>\n```\n\n----------------------------------------\n\nTITLE: Local Improvement Heuristic for TSP in R\nDESCRIPTION: A local search procedure that tries to improve a TSP solution by relocating a city to a better position. Used in the Lamarckian Evolutionary Algorithm approach.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_86\n\nLANGUAGE: R\nCODE:\n```\n# local improvement and evaluation:\n# first tries to improve a solution with a\n# local search that uses domain knowledge (MD)\n# returns best solution and evaluation value\nlocal_imp_tour=function(s,p=NA)\n{ # local search\n  N=length(s); ALL=1:N\n  if(is.na(p)) p=sample(ALL,1) # select random position\n  I=setdiff(ALL,p)\n  # current distance: p to neighbors\n  pprev=mindex(p,-1,N=N); pnext=mindex(p,1,N=N)\n  dpcur=MD[s[pprev],s[p]]+MD[s[p],s[pnext]]\n  # new distance if p is remove to another position:\n  dpnew=MD[s[pprev],s[pnext]]\n  # search for best insertion position for p:\n  ibest=0;best=-Inf\n  for(i in I) # extra cycle that increases computation\n  {\n    inext=mindex(i,1,N=N);iprev=mindex(i,-1,N=N)\n    if(inext==p) inext=pnext\n    if(iprev==p) iprev=pprev\n    # dinew: new distance p to neighbors if p inserted:\n    # current i distance without p:\n    if(i<p) {dinew=MD[s[iprev],s[p]]+MD[s[p],s[i]]\n      dicur=MD[s[iprev],s[i]]\n    }\n    else\n    { dinew=MD[s[i],s[p]]+MD[s[p],s[inext]]\n      dicur=MD[s[i],s[inext]]\n    }\n    # difference between current tour and new one:\n    dif=(dicur+dpcur)-(dinew+dpnew)\n    if(dif>0 && dif>best) # improved solution\n    {\n      best=dif\n      ibest=i\n    }\n  }\n  if(ibest>0) # insert p in i\n    s=insertion(s,p=p,i=ibest)\n  return(list(eval=tour(s),solution=s))\n}\n```\n\n----------------------------------------\n\nTITLE: Feature Mapping Function for Categorical Variables\nDESCRIPTION: Function to create mappings for categorical features by converting them to binary vectors. It returns a dictionary mapping categorical values to unique indices.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_118\n\nLANGUAGE: python\nCODE:\n```\ndef get_mapping(rdd, idx):\n    return rdd.map(lambda fields: fields[idx]).distinct().zipWithIndex().collectAsMap()\n```\n\n----------------------------------------\n\nTITLE: Basic Whitespace Tokenization in Scala\nDESCRIPTION: Implements basic text tokenization by splitting on whitespace and converting to lowercase. Uses flatMap to process all tokens together for exploratory analysis.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_198\n\nLANGUAGE: scala\nCODE:\n```\nval text = rdd.map { case (file, text) => text }\nval whiteSpaceSplit = text.flatMap(t => t.split(\" \").map(_.toLowerCase))\nprintln(whiteSpaceSplit.distinct.count)\n```\n\n----------------------------------------\n\nTITLE: Accessing a Table Created with Delimited Identifiers Using Regular Identifier\nDESCRIPTION: This example shows an attempt to access a table that was created with delimited identifiers by using a regular identifier, demonstrating potential case sensitivity issues.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM Employees with spaces\n```\n\n----------------------------------------\n\nTITLE: Computing Cost Metrics for K-means Clusters in Scala\nDESCRIPTION: This snippet computes the Within-Cluster Sum of Squares (WCSS) cost metric for both movie and user cluster models using MLlib's built-in computeCost function, providing a quantitative measure of clustering quality.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_161\n\nLANGUAGE: scala\nCODE:\n```\nval movieCost = movieClusterModel.computeCost(movieVectors)\nval userCost = userClusterModel.computeCost(userVectors)\nprintln(\"WCSS for movies: \" + movieCost)\nprintln(\"WCSS for users: \" + userCost)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Old-Style JOIN Syntax in SQL\nDESCRIPTION: Shows the traditional SQL join syntax using the WHERE clause to connect tables. This style is recommended when joining more than five tables for better readability.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_31\n\nLANGUAGE: sql\nCODE:\n```\nSELECT f.col1, d1.col2, d2.col3, d3.col4, d4.col5\nFROM FactTable AS f, Dim1 AS d1, Dim2 AS d2, Dim3 AS d3, Dim4 AS d4\nWHERE f.dim1_key = d1.dim1_key\n  AND f.dim2_key = d2.dim2_key\n  AND f.dim3_key = d3.dim3_key\n  AND f.dim4_key = d4.dim4_key\n  AND <search arguments>\n```\n\n----------------------------------------\n\nTITLE: Displaying Top Movies for Each Cluster in Scala\nDESCRIPTION: This snippet iterates through each computed cluster, sorts the movies by distance from the cluster center, and displays the 20 closest movies for each cluster. This helps visualize and understand what each cluster represents.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_160\n\nLANGUAGE: scala\nCODE:\n```\nfor ( (k, v) <- clusterAssignments.toSeq.sortBy(_._1)) {\n  println(s\"Cluster $k:\")\n  val m = v.toSeq.sortBy(_._5)\n  println(m.take(20).map { case (_, title, genres, _, d) => (title, genres, d) }.mkString(\"\\n\")) \n  println(\"=====\\n\")\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Top-K Recommendations\nDESCRIPTION: Generates top 10 movie recommendations for a specific user using the trained model.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_60\n\nLANGUAGE: scala\nCODE:\n```\nval userId = 789\nval K = 10\nval topKRecs = model.recommendProducts(userId, K)\nprintln(topKRecs.mkString(\"\\n\"))\n```\n\n----------------------------------------\n\nTITLE: Extracting LFW Dataset Archive in Bash\nDESCRIPTION: Command to extract the Labeled Faces in the Wild dataset archive file to prepare image data for dimensionality reduction techniques. This creates a folder structure with subfolders for each person in the dataset.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_164\n\nLANGUAGE: bash\nCODE:\n```\n> tar xfvz lfw-a.tgz\n```\n\n----------------------------------------\n\nTITLE: Defining a Function Using Lambda Notation\nDESCRIPTION: Demonstrates how to define a function using lambda notation, binding the variable x to create a function that computes sin(cos(sin(x))).\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_0\n\nLANGUAGE: pseudo-code\nCODE:\n```\nλx sin (cos (sin x))\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Problematic Extended Equality JOIN Syntax\nDESCRIPTION: Shows the inconsistent results that can occur when using proprietary extended equality operators for OUTER JOINs. This example illustrates how different database systems may produce different results with the same non-standard syntax.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_28\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT ... FROM S, SP WHERE S.S# *= SP.S# AND qty < 200\n```\n\n----------------------------------------\n\nTITLE: Generic Modern Optimization Method Algorithm in Pseudo-code\nDESCRIPTION: This pseudo-code outlines a generic modern optimization method applicable to all methods discussed in the book. It includes inputs for the evaluation function and control parameters, and describes the main loop cycle with termination criteria.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_0\n\nLANGUAGE: pseudo\nCODE:\n```\nAlgorithm 1 Generic modern optimization method\n```\n\n----------------------------------------\n\nTITLE: Inefficient Sequential Processing in T-SQL\nDESCRIPTION: An example of inefficient 1950s tape file algorithm implemented in SQL that should be avoided. This approach manually iterates through rows using a counter variable instead of using set-based operations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_66\n\nLANGUAGE: SQL\nCODE:\n```\nDECLARE @loop_counter INTEGER = 1;\nDECLARE @max_loop_counter INTEGER;\n\nSELECT @max_loop_counter = COUNT(*) FROM mytable;\n\nWHILE (@loop_counter <= @max_loop_counter)\nBEGIN\n    DECLARE @variable_one INTEGER;\n    DECLARE @variable_two INTEGER;\n    \n    SELECT @variable_one = variable_one,\n           @variable_two = variable_two\n    FROM mytable\n    WHERE id = @loop_counter;\n    \n    -- Do something with the row data\n    \n    SET @loop_counter = @loop_counter + 1;\nEND;\n```\n\n----------------------------------------\n\nTITLE: Displaying Evaluation Results for Decision Tree Bins\nDESCRIPTION: This code snippet shows the output of the evaluation, printing the list of bin parameters tested and their corresponding metric values. The results indicate an optimal setting of around 16-20 bins for test set performance.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_148\n\nLANGUAGE: python\nCODE:\n```\n[2, 4, 8, 16, 32, 64, 100]\n[1.3069788763726049, 0.81923394899750324, 0.75745322513058744, 0.62328384445223795, 0.63583503599563096, 0.63583503599563096, 0.63583503599563096]\n```\n\n----------------------------------------\n\nTITLE: Running a PySpark Application using spark-submit\nDESCRIPTION: Command to submit and execute a Python Spark application using the spark-submit utility. This command runs the Python script in a Spark environment.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n> $SPARK_HOME/bin/spark-submit pythonapp.py\n```\n\n----------------------------------------\n\nTITLE: Extracting and Validating Training and Test Datasets\nDESCRIPTION: Extracts the LabeledPoint values from the RDDs with indices and validates the dataset sizes to ensure they sum up to the original dataset size.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_136\n\nLANGUAGE: python\nCODE:\n```\ntrain_data = train.map(lambda (idx, p): p)\ntest_data = test.map(lambda (idx, p) : p)\ntrain_size = train_data.count()\ntest_size = test_data.count()\nprint \"Training data size: %d\" % train_size\nprint \"Test data size: %d\" % test_size\nprint \"Total data size: %d \" % num_data\nprint \"Train + Test size : %d\" % (train_size + test_size)\n```\n\n----------------------------------------\n\nTITLE: Complex Query Result from Object-Oriented Schema in SQL\nDESCRIPTION: This snippet showcases the complexity of queries resulting from an object-oriented schema imposed on a relational database. It demonstrates the need for multiple joins and complex logic to retrieve simple information.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_23\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT o.Name AS OfficeName, a.StreetAddress, a.City, a.State, a.Zip,\n       m.FirstName + ' ' + m.LastName AS ManagerName, p.PhoneNumber\nFROM Office o\nJOIN Address a ON o.AddressID = a.AddressID\nJOIN Employee m ON o.ManagerID = m.EmployeeID\nJOIN Phone p ON o.PhoneID = p.PhoneID\nWHERE o.IsActive = 1\n  AND a.IsActive = 1\n  AND m.IsActive = 1\n  AND p.IsActive = 1\n  AND o.EffectiveDate <= GETDATE()\n  AND o.ExpirationDate > GETDATE()\n  -- Additional complex joins and conditions...\n```\n\n----------------------------------------\n\nTITLE: Implementing Mutation Operators for Ordered Representations in R\nDESCRIPTION: This snippet defines three mutation operators for ordered representations in TSP: exchange (swaps two randomly selected cities), insertion (inserts a city into a random position), and displacement (inserts a random subtour into another position).\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_78\n\nLANGUAGE: R\nCODE:\n```\nexchange=function(s,N=length(s))\n\n{ p=sample(1:N,2) # select two positions\n\ntemp=s[p[1]] # swap values\n\ns[p[1]]=s[p[2]]\n\ns[p[2]]=temp\n\nreturn(s)\n\n}\n\ninsertion=function(s,N=length(s),p=NA,i=NA)\n\n{ if(is.na(p)) p=sample(1:N,1) # select a position\n\nI=setdiff(1:N,p) # ALL except p\n\nif(is.na(i)) i=sample(I,1) # select random place\n\nif(i>p) i=i+1 # need to produce a change\n\nI1=which(I<i) # first part\n\nI2=which(I>=i) # last part\n\ns=s[c(I[I1],p,I[I2])] # new solution\n\nreturn(s)\n\n}\n\ndisplacement=function(s,N=length(s))\n\n{ p=c(1,N)\n\n# select random tour different than s\n\nwhile(p[1]==1&&p[2]==N) p=sort(sample(1:N,2))\n\nI=setdiff(1:N,p[1]:p[2]) # ALL except p\n\ni=sample(I,1) # select random place\n\nI1=which(I<i) # first part\n\nI2=which(I>=i) # last part\n\ns=s[c(I[I1],p[1]:p[2],I[I2])]\n\nreturn(s)\n\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Knowledge Data via Python Indexer\nDESCRIPTION: Commands to navigate to the builder directory, run the Python indexer script to import structured data, and return to the parent directory.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README_cn.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd builder && python indexer.py && cd ..\n```\n\n----------------------------------------\n\nTITLE: Evaluating L2 Regularization in Spark Linear Regression\nDESCRIPTION: This snippet evaluates the impact of different L2 regularization levels on a linear regression model using Spark MLlib. It computes the RMSLE metric for various regularization parameter values.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_142\n\nLANGUAGE: Python\nCODE:\n```\nparams = [0.0, 0.01, 0.1, 1.0, 5.0, 10.0, 20.0]\nmetrics = [evaluate(train_data, test_data, 10, 0.1, param, 'l2', False) for param in params]\nprint params\nprint metrics\nplot(params, metrics)\nfig = matplotlib.pyplot.gcf()\npyplot.xscale('log')\n```\n\n----------------------------------------\n\nTITLE: Using Standard LEFT OUTER JOIN Syntax in SQL-92\nDESCRIPTION: Demonstrates the standard SQL-92 LEFT OUTER JOIN syntax that preserves rows from the left table. The example shows table construction, cross join, and how the LEFT OUTER JOIN selects matching rows and fills non-matching ones with NULLs from the right table.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_27\n\nLANGUAGE: SQL\nCODE:\n```\nTable1 LEFT OUTER JOIN Table2 ON Table1.col1 = Table2.col1\n```\n\n----------------------------------------\n\nTITLE: Creating Training/Test Sets for Decision Tree Model\nDESCRIPTION: Applies the same data splitting approach to prepare training and testing datasets specifically for the decision tree model.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_137\n\nLANGUAGE: python\nCODE:\n```\ndata_with_idx_dt = data_dt.zipWithIndex().map(lambda (k, v): (v, k))\ntest_dt = data_with_idx_dt.sample(False, 0.2, 42)\ntrain_dt = data_with_idx_dt.subtractByKey(test_dt)\ntrain_data_dt = train_dt.map(lambda (idx, p): p)\ntest_data_dt = test_dt.map(lambda (idx, p) : p)\n```\n\n----------------------------------------\n\nTITLE: Implementing Depth-First Search Function in R\nDESCRIPTION: A recursive function that traverses a search tree to find the optimal solution according to a user-defined evaluation function. It handles multi-dimensional problems with various domain values for each dimension.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_17\n\nLANGUAGE: r\nCODE:\n```\ndfsearch=function(l=1,b=1,domain,FUN,type=\"min\",\nD=length(domain),\nx=rep(NA,D),\nbcur=switch(type,min=list(sol=NULL,eval=Inf),\nmax=list(sol=NULL,eval=-Inf)),\n...)\n{ if((l-1)==D) # \"leave\" with solution x to be tested:\n{ f=FUN(x,...);fb=bcur$eval\nib=switch(type,min=which.min(c(fb,f)),\nmax=which.max(c(fb,f)))\nif(ib==1) return (bcur) else return(list(sol=x,eval=f))\n}\nelse # go through sub branches\n{ for(j in 1:length(domain[[l]]))\n{ x[l]=domain[[l]][j]\nbcur=dfsearch(l+1,j,domain,FUN,type,D=D,\nx=x,bcur=bcur,...)\n}\nreturn(bcur)\n}\n}\n```\n\n----------------------------------------\n\nTITLE: Even Number Counting Functions in R\nDESCRIPTION: Implements three different approaches to count even numbers in a vector using for loops, sapply(), and direct condition checking.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_109\n\nLANGUAGE: R\nCODE:\n```\ncounteven1=function(x)\n{ r=0\nfor(i in 1:length(x))\n{ if(x[i]%%2==0) r=r+1 }\nreturn(r)\n}\n\nifeven=function(x)\n{ if(x%%2) return(TRUE) else return(FALSE)}\n\ncounteven2=function(x)\n{ return(sum(sapply(x,ifeven))) }\n\ncounteven3=function(x)\n{ return(sum(x%%2==0)) }\n\nx=1:10\ncat(\"counteven1:\",counteven1(x),\"\\n\")\ncat(\"counteven2:\",counteven2(x),\"\\n\")\ncat(\"counteven3:\",counteven3(x),\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: SQL Aliasing with AS Keyword\nDESCRIPTION: Shows the recommended use of the AS keyword for explicit column and table aliases.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nPersonnel AS P1\n```\n\nLANGUAGE: SQL\nCODE:\n```\n(salary + commission) AS total_pay\n```\n\n----------------------------------------\n\nTITLE: Querying Companies with Same Legal Representative\nDESCRIPTION: GQL query to identify companies that share the same legal representative.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nknext reasoner execute --dsl \"\nMATCH\n    (s:SupplyChain.Company)-[:sameLegalRepresentative]->(o:SupplyChain.Company)\nRETURN\n    s.name, o.name\n\"\n```\n\n----------------------------------------\n\nTITLE: Demonstration of Query Optimization with Correlated Subquery\nDESCRIPTION: Example showing a query to select loans where all payments have a status code of 'F', demonstrating how correlated subqueries can be rewritten more efficiently.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_47\n\nLANGUAGE: sql\nCODE:\n```\nSELECT DISTINCT l.* \nFROM Loans AS l\nWHERE NOT EXISTS\n    (SELECT *\n     FROM Payments AS p\n     WHERE p.loan_id = l.loan_id\n     AND p.status_code <> 'F');\n```\n\n----------------------------------------\n\nTITLE: Predicting Single Rating\nDESCRIPTION: Shows how to predict a rating for a specific user-item pair using the trained model.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_59\n\nLANGUAGE: scala\nCODE:\n```\nval predictedRating = model.predict(789, 123)\n```\n\n----------------------------------------\n\nTITLE: Analyzing and Visualizing Constraint Handling Results in R\nDESCRIPTION: Analysis and visualization of the results from the constraint handling experiment. Computes average performance, performs statistical testing using Mann-Whitney test to compare the two approaches, and creates visualizations of the results for interpretation.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_55\n\nLANGUAGE: R\nCODE:\n```\n# compute average F result per method:\n\nAV=matrix(nrow=MAXFN,ncol=length(Methods))\n\nfor(m in 1:length(Methods))\n\nfor(i in 1:MAXFN)\n\nAV[i,m]=mean(RES[[m]][i,])\n\n# show results:\n\ncat(Methods,\"\\n\")\n\ncat(round(apply(VAL,2,mean),digits=0),\" (average best)\\n\")\n\n# Mann-Whitney non-parametric test:\n\np=wilcox.test(VAL[,1],VAL[,2],paired=TRUE)$p.value\n\ncat(\"p-value:\",round(p,digits=2),\"(<0.05)\\n\")\n\n# create pdf file:\n\npdf(\"comp-bagprices-constr.pdf\",width=5,height=5,paper=\n\n\"special\")\n\npar(mar=c(4.0,4.0,1.8,0.6)) # reduce default plot margin\n```\n\n----------------------------------------\n\nTITLE: Querying Company Credit Rating Factors\nDESCRIPTION: GQL query to retrieve company information and financial metrics for credit rating assessment.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nknext reasoner execute --dsl \"\nMATCH\n    (s:SupplyChain.Company)\nRETURN\n    s.id, s.name, s.fundTrans1Month, s.fundTrans3Month,\n    s.fundTrans6Month, s.fundTrans1MonthIn, s.fundTrans3MonthIn,\n    s.fundTrans6MonthIn, s.cashflowDiff1Month, s.cashflowDiff3Month,\n    s.cashflowDiff6Month\n\"\n```\n\n----------------------------------------\n\nTITLE: Getting Predicted Movies for APK Calculation in Scala\nDESCRIPTION: Extracts the predicted movie IDs from top K recommendations for a user to be used in the APK calculation.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_73\n\nLANGUAGE: scala\nCODE:\n```\nval predictedMovies = topKRecs.map(_.product)\n```\n\n----------------------------------------\n\nTITLE: Avoiding Excessive Parentheses in SQL WHERE Clauses\nDESCRIPTION: Demonstrates how excessive parentheses can make SQL code harder to read. The example shows a simple query with unnecessary nesting.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_34\n\nLANGUAGE: sql\nCODE:\n```\nSELECT *\nFROM Foobar\nWHERE ((x1 = 1) AND (x2 = 2)) AND (x3 = 3)\n```\n\n----------------------------------------\n\nTITLE: Custom Tokenization Function in Scala\nDESCRIPTION: This function combines all the filtering steps into a single tokenization pipeline. It splits text, converts to lowercase, applies regex filtering, removes stop words and rare tokens, and filters by token length.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_207\n\nLANGUAGE: scala\nCODE:\n```\ndef tokenize(line: String): Seq[String] = {\n  line.split(\"\"\"\\W+\"\"\")\n    .map(_.toLowerCase)\n    .filter(token => regex.pattern.matcher(token).matches)\n    .filterNot(token => stopwords.contains(token))\n    .filterNot(token => rareTokens.contains(token))\n    .filter(token => token.size >= 2)\n    .toSeq\n}\n```\n\n----------------------------------------\n\nTITLE: Simplified Notation for Function Application\nDESCRIPTION: Presents a simplified notation for function application, omitting the α symbol.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_3\n\nLANGUAGE: pseudo-code\nCODE:\n```\nt u\n```\n\n----------------------------------------\n\nTITLE: Stateful Streaming Console Output Example\nDESCRIPTION: Example console output from the stateful streaming application showing how user purchase counts and revenue totals are maintained and updated across multiple batches.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_229\n\nLANGUAGE: text\nCODE:\n```\n...\n-------------------------------------------\nTime: 1416080440000 ms\n-------------------------------------------\n(Janet,(2,10.98))\n(Frank,(1,5.49))\n(James,(2,12.98))\n(Malinda,(1,9.99))\n(Elaine,(3,29.97))\n(Gary,(2,12.98))\n(Miguel,(3,20.47))\n(Saul,(1,5.49))\n(Manuela,(2,18.939999999999998))\n(Eric,(2,18.939999999999998))\n...\n-------------------------------------------\nTime: 1416080441000 ms\n-------------------------------------------\n(Janet,(6,34.94))\n(Juan,(4,33.92))\n(Frank,(2,14.44))\n(James,(7,48.93000000000001))\n(Malinda,(1,9.99))\n(Elaine,(7,61.89))\n(Gary,(4,28.46))\n(Michael,(1,8.95))\n(Richard,(2,16.439999999999998))\n(Miguel,(5,35.95))\n...\n```\n\n----------------------------------------\n\nTITLE: Stack-Based Interpretation of Arithmetic Expression in PCF\nDESCRIPTION: This snippet demonstrates how a stack-based approach could be used to interpret an arithmetic expression in PCF without using local definitions. It describes the process of pushing values onto a stack and then performing operations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_19\n\nLANGUAGE: PCF\nCODE:\n```\n((((1 + 2) + 3) + 4) + 5) + 6\n```\n\n----------------------------------------\n\nTITLE: Expanding SQL Query Stubs\nDESCRIPTION: Shows how to expand pseudocode stubs into more detailed SQL subqueries, focusing on filtering by gender.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_79\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT (SELECT name\n          FROM People\n         WHERE gender = 1\n           AND <join thingie for guys>) AS guy,\n       (<miracle for dolls>) AS doll\n  FROM People\n WHERE ??\n```\n\n----------------------------------------\n\nTITLE: Set-based Approach Using Table Constructor in SQL\nDESCRIPTION: A true set-oriented solution that uses a VALUES table constructor to create a set of all possible days, then finds the minimum value that doesn't exist in the Calendar_Table for the given week.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_87\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE PROCEDURE Insert_Missing_Weekday (IN p_week_nbr INTEGER)\nBEGIN\n  INSERT INTO Calendar_Table (week_nbr, day_nbr)\n  SELECT p_week_nbr, MIN(V.day_nbr)\n  FROM (VALUES (1), (2), (3), (4), (5), (6), (7)) AS V(day_nbr)\n  WHERE V.day_nbr NOT IN \n        (SELECT C.day_nbr \n         FROM Calendar_Table AS C\n         WHERE C.week_nbr = p_week_nbr);\nEND;\n```\n\n----------------------------------------\n\nTITLE: Default Credentials for KAG Product Access\nDESCRIPTION: Login credentials to access the KAG product interface after installation. Users can access the web interface at the provided URL with these default authentication details.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/README_cn.md#2025-04-07_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nDefault Username: openspg\nDefault password: openspg@kag\n```\n\n----------------------------------------\n\nTITLE: Formatting SQL Subquery with Vertical Alignment\nDESCRIPTION: Example showing how to format nested subqueries using vertical alignment for improved readability. The subquery is placed against a vertical 'river' of whitespace to visually group related elements.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nSELECT col1, col2\n       FROM TableA\n       WHERE col3 IN (SELECT sub_col\n                        FROM TableB\n                        WHERE condition)\n```\n\n----------------------------------------\n\nTITLE: Testing Monte Carlo Search on Various Optimization Problems in R\nDESCRIPTION: Script for testing the Monte Carlo search method on different optimization problems including bag prices and benchmark functions (sphere and rastrigin) with varying dimensions. The code demonstrates how Monte Carlo performs well in low dimensions but struggles as dimensionality increases.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_25\n\nLANGUAGE: R\nCODE:\n```\n### test-mc.R file ###\n\nsource(\"blind.R\") # load the blind search methods\n\nsource(\"montecarlo.R\") # load the monte carlo method\n\nsource(\"functions.R\") # load the profit function\n\nN=10000 # set the number of samples\n\ncat(\"monte carlo search (N:\",N,\")\\n\")\n\n# bag prices\n\ncat(\"bag prices:\")\n\nS=mcsearch(N,rep(1,5),rep(1000,5),profit,\"max\")\n\ncat(\"s:\",S$sol,\"f:\",S$eval,\"\\n\")\n\n# real-value functions: sphere and rastrigin:\n\nsphere=function(x) sum(x^2)\n\nrastrigin=function(x) 10*length(x)+sum(x^2-10*cos(2*pi*x))\n\nD=c(2,30)\n\nlabel=\"sphere\"\n\nfor(i in 1:length(D))\n\n{ S=mcsearch(N,rep(-5.2,D[i]),rep(5.2,D[i]),sphere,\"min\")\n\ncat(label,\"D:\",D[i],\"s:\",S$sol[1:2],\"f:\",S$eval,\"\\n\")\n\n}\n\nlabel=\"rastrigin\"\n\nfor(i in 1:length(D))\n\n{ S=mcsearch(N,rep(-5.2,D[i]),rep(5.2,D[i]),rastrigin,\"min\")\n\ncat(label,\"D:\",D[i],\"s:\",S$sol[1:2],\"f:\",S$eval,\"\\n\")\n\n}\n```\n\n----------------------------------------\n\nTITLE: Dynamic SQL Security Risk Example\nDESCRIPTION: Demonstrates a security vulnerability in dynamic SQL where malicious table names can be injected. This shows why dynamic SQL should be avoided for its lack of cohesion and security risks.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_62\n\nLANGUAGE: SQL\nCODE:\n```\n\"Foobar; DELETE FROM Foobar; COMMIT\"\n```\n\n----------------------------------------\n\nTITLE: Simplifying SQL Expressions with Algebra\nDESCRIPTION: Demonstrates how algebraic simplification can make SQL expressions cleaner and more maintainable. The example shows factoring out common terms.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_41\n\nLANGUAGE: sql\nCODE:\n```\nSELECT *\nFROM Table1\nWHERE col1 * 5 > 100 AND col1 * 5 < 500\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT *\nFROM Table1\nWHERE col1 > 20 AND col1 < 100\n```\n\n----------------------------------------\n\nTITLE: Example of Poor UNION Usage in SQL\nDESCRIPTION: Demonstration of inefficient use of UNION operations in SQL that should be avoided and replaced with simpler OR conditions or CASE expressions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_48\n\nLANGUAGE: sql\nCODE:\n```\nSELECT 'A' AS section, \n       'General Ledger' AS description, \n       branch, \n       NULL AS seq, \n       NULL AS ref, \n       NULL AS debit, \n       NULL AS credit\nFROM General_Ledger\nUNION\nSELECT 'B' AS section,\n       'General Ledger' AS description,\n       branch,\n       seq,\n       ref,\n       debit,\n       credit\nFROM General_Ledger;\n```\n\n----------------------------------------\n\nTITLE: Filtering Rare Tokens in Spark RDD\nDESCRIPTION: This snippet filters out tokens that occur only once in the entire corpus and displays the 20 least frequent remaining tokens.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_206\n\nLANGUAGE: scala\nCODE:\n```\nval rareTokens = tokenCounts.filter{ case (k, v) => v < 2 }.map { case (k, v) => k }.collect.toSet\nval tokenCountsFilteredAll = tokenCountsFilteredSize.filter { case (k, v) => !rareTokens.contains(k) }\nprintln(tokenCountsFilteredAll.top(20)(oreringAsc).mkString(\"\\n\"))\n```\n\n----------------------------------------\n\nTITLE: Generating data files for KAG from UltraDomain dataset\nDESCRIPTION: Command to execute a Python script that processes the UltraDomain cs.jsonl dataset into the required format for KAG builder and solver components.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython generate_data.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Scala Spark Project with SBT\nDESCRIPTION: SBT build configuration file for a Scala Spark project specifying dependencies and version information\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nname := \"scala-spark-app\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.10.4\"\n\nlibraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.2.0 \"\n```\n\n----------------------------------------\n\nTITLE: Querying Event Impact Analysis\nDESCRIPTION: GQL query to analyze how product chain events lead to company events, showing the causal relationships between industry events and their business impacts.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README_cn.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nknext reasoner execute --dsl \"\nMATCH\n    (s:SupplyChain.ProductChainEvent)-[:leadTo]->(o:SupplyChain.CompanyEvent)\nRETURN\n    s.id, s.subject, o.subject, o.name\n\"\n```\n\n----------------------------------------\n\nTITLE: Inspecting Term Frequency Vector Characteristics in Spark MLlib\nDESCRIPTION: Examines the properties of the first term frequency vector in the transformed dataset, including its dimension, number of non-zero entries, and the frequency counts and indices of the first few elements.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_210\n\nLANGUAGE: scala\nCODE:\n```\nval v = tf.first.asInstanceOf[SV]\nprintln(v.size)\nprintln(v.values.size)\nprintln(v.values.take(10).toSeq)\nprintln(v.indices.take(10).toSeq)\n```\n\n----------------------------------------\n\nTITLE: Training Linear Regression Model on Log-Transformed Data\nDESCRIPTION: Trains a LinearRegressionWithSGD model using log-transformed target data with specified iterations and step size parameters.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_130\n\nLANGUAGE: python\nCODE:\n```\nmodel_log = LinearRegressionWithSGD.train(data_log, iterations=10, step=0.1)\n```\n\n----------------------------------------\n\nTITLE: Computing Error Metrics for Decision Tree using Spark RDD\nDESCRIPTION: Calculation of MSE, MAE, and RMSLE metrics for a decision tree model using Spark RDD transformations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_127\n\nLANGUAGE: python\nCODE:\n```\nmse_dt = true_vs_predicted_dt.map(lambda (t, p): squared_error(t, p)).mean()\nmae_dt = true_vs_predicted_dt.map(lambda (t, p): abs_error(t, p)).mean()\nrmsle_dt = np.sqrt(true_vs_predicted_dt.map(lambda (t, p): squared_log_error(t, p)).mean())\nprint \"Decision Tree - Mean Squared Error: %2.4f\" % mse_dt\nprint \"Decision Tree - Mean Absolute Error: %2.4f\" % mae_dt\nprint \"Decision Tree - Root Mean Squared Log Error: %2.4f\" % rmsle_dt\n```\n\n----------------------------------------\n\nTITLE: Loading Raw MovieLens Data in Spark\nDESCRIPTION: Loads the MovieLens dataset from a text file into a Spark RDD and displays the first record. The dataset contains user ID, movie ID, rating, and timestamp fields separated by tabs.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_52\n\nLANGUAGE: scala\nCODE:\n```\nval rawData = sc.textFile(\"/PATH/ml-100k/u.data\")\nrawData.first()\n```\n\n----------------------------------------\n\nTITLE: Implementing Average Precision K (APK) Function in Scala\nDESCRIPTION: A function that calculates the average precision at K for recommendation systems. It takes actual items associated with a user and predicted items, then computes how well the predictions match actual items within the top K predictions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_71\n\nLANGUAGE: scala\nCODE:\n```\ndef avgPrecisionK(actual: Seq[Int], predicted: Seq[Int], k: Int): Double = {\n  val predK = predicted.take(k)\n  var score = 0.0\n  var numHits = 0.0\n  for ((p, i) <- predK.zipWithIndex) {\n    if (actual.contains(p)) {\n      numHits += 1.0\n      score += numHits / (i.toDouble + 1.0)\n    }\n  }\n  if (actual.isEmpty) {\n    1.0\n  } else {\n    score / scala.math.min(actual.size, k).toDouble\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Average Precision K (APK) Function in Scala\nDESCRIPTION: A function that calculates the average precision at K for recommendation systems. It takes actual items associated with a user and predicted items, then computes how well the predictions match actual items within the top K predictions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_71\n\nLANGUAGE: scala\nCODE:\n```\ndef avgPrecisionK(actual: Seq[Int], predicted: Seq[Int], k: Int): Double = {\n  val predK = predicted.take(k)\n  var score = 0.0\n  var numHits = 0.0\n  for ((p, i) <- predK.zipWithIndex) {\n    if (actual.contains(p)) {\n      numHits += 1.0\n      score += numHits / (i.toDouble + 1.0)\n    }\n  }\n  if (actual.isEmpty) {\n    1.0\n  } else {\n    score / scala.math.min(actual.size, k).toDouble\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Optimization Algorithm Comparison in R\nDESCRIPTION: Compares different optimization algorithms (Monte Carlo, PSO, EDA) on the Eggholder function with statistical analysis and visualization of results.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_114\n\nLANGUAGE: R\nCODE:\n```\nctest2=function(Methods,f,lower,upper,type=\"min\",Runs,D,MAXFN,maxit,LP,pdf,main,LIM)\n{ RES=vector(\"list\",length(Methods))\nVAL=matrix(nrow=Runs,ncol=length(Methods))\nfor(m in 1:length(Methods))\nRES[[m]]=matrix(nrow=MAXFN,ncol=Runs)\nfor(R in 1:Runs)\nfor(m in 1:length(Methods))\n{ EV<<-0; F<<-rep(NA,MAXFN)\nif(type==\"min\") BEST<<-Inf else BEST<<- -Inf\nsuppressWarnings(crun2(Methods[m],f,lower,upper,LP,maxit,MAXFN))\nRES[[m]][,R]=F\nVAL[R,m]=F[MAXFN]\n}}\n```\n\n----------------------------------------\n\nTITLE: Declaring Reference Assignment in Extended PCF\nDESCRIPTION: Example of reference assignment syntax in the extended PCF language. This construct updates the value associated with the reference x to 4, creating a side effect by changing the global state.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_32\n\nLANGUAGE: theoretical\nCODE:\n```\nx := 4\n```\n\n----------------------------------------\n\nTITLE: Defining Euclidean Distance Function for Cluster Interpretation\nDESCRIPTION: This code defines a function to compute Euclidean distance between two vectors, used for interpreting cluster results by finding points closest to cluster centers.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_158\n\nLANGUAGE: scala\nCODE:\n```\nimport breeze.linalg._\nimport breeze.numerics.pow\ndef computeDistance(v1: DenseVector[Double], v2: DenseVector[Double]) = pow(v1 - v2, 2).sum\n```\n\n----------------------------------------\n\nTITLE: Registering Concept Rules\nDESCRIPTION: Command to register the concept rules that define the leadto relationship logic in the knowledge graph.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nknext schema reg_concept_rule --file ./schema/concept.rule\n```\n\n----------------------------------------\n\nTITLE: Defining Support Vector Machine Prediction in Python\nDESCRIPTION: This code shows the identity link function used in linear Support Vector Machines (SVM). The predicted outcome is directly based on the dot product of weights and features.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_86\n\nLANGUAGE: python\nCODE:\n```\ny = wTx\n```\n\n----------------------------------------\n\nTITLE: Training Naive Bayes Text Classifier\nDESCRIPTION: Implements a multiclass text classifier using Naive Bayes with TF-IDF features, including data preparation and model training\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_217\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.classification.NaiveBayes\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\n\nval newsgroupsMap = newsgroups.distinct.collect().zipWithIndex.toMap\nval zipped = newsgroups.zip(tfidf)\nval train = zipped.map { case (topic, vector) => LabeledPoint(newsgroupsMap(topic), vector) }\ntrain.cache\n\nval model = NaiveBayes.train(train, lambda = 0.1)\n```\n\n----------------------------------------\n\nTITLE: Training Naive Bayes Text Classifier\nDESCRIPTION: Implements a multiclass text classifier using Naive Bayes with TF-IDF features, including data preparation and model training\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_217\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.classification.NaiveBayes\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\n\nval newsgroupsMap = newsgroups.distinct.collect().zipWithIndex.toMap\nval zipped = newsgroups.zip(tfidf)\nval train = zipped.map { case (topic, vector) => LabeledPoint(newsgroupsMap(topic), vector) }\ntrain.cache\n\nval model = NaiveBayes.train(train, lambda = 0.1)\n```\n\n----------------------------------------\n\nTITLE: TSP Area Optimization Functions in R\nDESCRIPTION: Implements functions for calculating and optimizing tour area instead of distance. Uses rgeos package for polygon area calculations and includes visualization of the area-based solution.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_92\n\nLANGUAGE: R\nCODE:\n```\nlibrary(rgeos)\n\npoly=function(data)\n{ poly=\"\";sep=\", \"\nfor(i in 1:nrow(data))\n{ if(i==nrow(data)) sep=\"\"\npoly=paste(poly,paste(data[i,],collapse=\" \"),sep,sep=\"\")\n}\npoly=paste(\"POLYGON((\",poly,\"))\",collapse=\"\")\npoly=readWKT(poly)\n}\n\narea=function(s) return( gArea(poly(Data[c(s,s[1]),])) )\n\ncat(\"area of 2-opt TSP tour:\",area(R1),\"\\n\")\n\npdf(\"qa-2opt-area.pdf\",paper=\"special\")\npar(mar=c(0.0,0.0,0.0,0.0))\nPR1=poly(Data[c(R1,R1[1]),])\nplot(PR1,col=\"gray\")\ndev.off()\n\ncat(\"EA run for TSP area:\\n\")\nset.seed(12345)\npSize=30;iters=20\nPTM=proc.time()\nOEA=oea(size=N,popSize=pSize,iters=iters,evalFunc=area,crossfunc=ox,mutfunc=insertion,REPORT=iters,elitism=1)\nsec=(proc.time()-PTM)[3]\nbi=which.min(OEA$evaluations)\nb=OEA$population[which.min(OEA$evaluations),]\n```\n\n----------------------------------------\n\nTITLE: Loading R Source File\nDESCRIPTION: Shows how to execute R code from an external file using the source() function.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_4\n\nLANGUAGE: R\nCODE:\n```\nsource(\"code.R\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Wine Quality Dataset\nDESCRIPTION: Loads the white wine quality dataset from UCI repository, preprocesses it by converting the quality scores into a binary classification problem (poor/average vs good/excellent), and selects a smaller sample for demonstration purposes.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_98\n\nLANGUAGE: R\nCODE:\n```\n# load wine quality dataset directly from UCI repository:\n\nfile=\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n\nd=read.table(file=file,sep=\";\",header=TRUE)\n\n# convert the output variable into 3 classes of wine:\n\n# \"poor_or_average\" <\\- 3,4,5 or 6;\n\n# \"good_or_excellent\" <\\- 7, 8 or 9\n\nd$quality=cut(d$quality,c(1,6,10),\n\nc(\"poor_or_average\",\"good_or_excellent\"))\n\noutput=ncol(d) # output target index (last column)\n\nmaxinputs=output-1 # number of maximum inputs\n\n# to speed up the demonstration, select a smaller sample of\n\ndata:\n\nn=nrow(d) # total number of samples\n\nns=round(n*0.25) # select a quarter of the samples\n\nset.seed(12345) # for replicability\n\nALL=sample(1:n,ns) # contains 25% of the index samples\n\n# show a summary of the wine quality dataset (25%):\n\nprint(summary(d[ALL,]))\n\ncat(\"output class distribuition (25% samples):\\n\")\n\nprint(table(d[ALL,]$quality)) # show distribution of classes\n```\n\n----------------------------------------\n\nTITLE: Calculating Hinge Loss for Support Vector Machines in Python\nDESCRIPTION: This snippet defines the hinge loss function used in Support Vector Machines (SVM). It measures the model's performance in classification tasks.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_87\n\nLANGUAGE: python\nCODE:\n```\nmax(0, 1 - ywTx)\n```\n\n----------------------------------------\n\nTITLE: Extracting Datetime from Unix Timestamp in PySpark\nDESCRIPTION: This function converts a Unix timestamp to a datetime object using Python's datetime module. It's used to extract time-related features from rating timestamps.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\ndef extract_datetime(ts):\n    import datetime\n    return datetime.datetime.fromtimestamp(ts)\n```\n\n----------------------------------------\n\nTITLE: Extracting Datetime from Unix Timestamp in PySpark\nDESCRIPTION: This function converts a Unix timestamp to a datetime object using Python's datetime module. It's used to extract time-related features from rating timestamps.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\ndef extract_datetime(ts):\n    import datetime\n    return datetime.datetime.fromtimestamp(ts)\n```\n\n----------------------------------------\n\nTITLE: Terminating an EC2 Spark Cluster\nDESCRIPTION: Command to destroy a Spark cluster running on Amazon EC2 using the spark-ec2 script, which terminates all associated instances and frees AWS resources.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\n>./ec2/spark-ec2 -k spark -i spark.pem destroy test-cluster\n```\n\n----------------------------------------\n\nTITLE: Using BETWEEN for Range Comparisons in SQL\nDESCRIPTION: Demonstrates how to use the BETWEEN operator for more compact and readable range conditions instead of using multiple comparison operators. This improves code readability by clearly showing the relationship between columns.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_42\n\nLANGUAGE: sql\nCODE:\n```\ncol_1 >= lower_bound AND col_1 <= upper_bound\n```\n\nLANGUAGE: sql\nCODE:\n```\ncol_1 BETWEEN lower_bound AND upper_bound\n```\n\n----------------------------------------\n\nTITLE: Creating True vs Predicted Values RDD with Back-Transformed Data\nDESCRIPTION: Maps over the log-transformed dataset to create pairs of true and predicted values, applying numpy's exp function to transform both back to the original scale.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_131\n\nLANGUAGE: python\nCODE:\n```\ntrue_vs_predicted_log = data_log.map(lambda p: (np.exp(p.label), np.exp(model_log.predict(p.features))))\n```\n\n----------------------------------------\n\nTITLE: Starting IPython with Plotting Capabilities\nDESCRIPTION: Alternative command to launch IPython with pylab plotting functionality enabled. This is useful for displaying images without using the notebook interface.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_170\n\nLANGUAGE: shell\nCODE:\n```\nipython --pylab\n```\n\n----------------------------------------\n\nTITLE: Plotting Cumulative Sum of Singular Values in Python\nDESCRIPTION: Creates a plot of the cumulative sum of singular values on a logarithmic scale to help identify where the curve flattens, indicating the optimal number of components to retain.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_193\n\nLANGUAGE: python\nCODE:\n```\nplot(cumsum(s))\nplt.yscale('log')\n```\n\n----------------------------------------\n\nTITLE: PCF Function Application Example\nDESCRIPTION: Mathematical notation showing a rule for function application in PCF's operational semantics.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_17\n\nLANGUAGE: math\nCODE:\n```\n\\frac{{{\\rm t } \\triangleright {\\rm u}}}{{{\\rm fun x } -  > {\\rm t } \\triangleright {\\rm fun x } -  > {\\rm u}}}\n```\n\n----------------------------------------\n\nTITLE: Generating answers with KAG knowledge graph\nDESCRIPTION: Commands to navigate to the solver directory and execute the eval.py script to generate answers to the CSQA questions using the KAG knowledge graph.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd solver && python eval.py && cd ..\n```\n\n----------------------------------------\n\nTITLE: Creating Genre Map from MovieLens Genre Data\nDESCRIPTION: Extracts genre mappings from the genre file by parsing each line and creating a key-value map where keys are genre indices and values are genre names. Filters out empty lines to prevent errors during splitting.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_151\n\nLANGUAGE: scala\nCODE:\n```\nval genreMap = genres.filter(!_.isEmpty).map(line => line.split(\"\\\\|\")).map(array => (array(1), array(0))).collectAsMap\nprintln(genreMap)\n```\n\n----------------------------------------\n\nTITLE: Setting up NSGA-II for Binary Optimization Task in R\nDESCRIPTION: This snippet sets up and runs NSGA-II for a binary optimization task with 8 dimensions and 2 objectives. It defines an evaluation function that transforms binary objectives into minimization goals.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_70\n\nLANGUAGE: R\nCODE:\n```\nm=2 # two objectives\nD=8 # 8 bits\n\n# eval: transform binary objectives into minimization goal\n# round(x) is used to convert real number to 0 or 1 values\neval=function(x) c(-sumbin(round(x)),-maxsin(round(x)))\n\ncat(\"binary task:\\n\")\nG=nsga2(fn=eval,idim=D,odim=m,\nlower.bounds=rep(0,D),upper.bounds=rep(1,D),\npopsize=12,generations=100)\n\n# show last Pareto front\nI=which(G$pareto.optimal)\nfor(i in I)\n{\n  x=round(G$par[i,])\n  cat(x,\" f=(\",sumbin(x),\",\",round(maxsin(x),2),\")\",\"\\n\",sep=\"\")\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up NSGA-II for Binary Optimization Task in R\nDESCRIPTION: This snippet sets up and runs NSGA-II for a binary optimization task with 8 dimensions and 2 objectives. It defines an evaluation function that transforms binary objectives into minimization goals.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_70\n\nLANGUAGE: R\nCODE:\n```\nm=2 # two objectives\nD=8 # 8 bits\n\n# eval: transform binary objectives into minimization goal\n# round(x) is used to convert real number to 0 or 1 values\neval=function(x) c(-sumbin(round(x)),-maxsin(round(x)))\n\ncat(\"binary task:\\n\")\nG=nsga2(fn=eval,idim=D,odim=m,\nlower.bounds=rep(0,D),upper.bounds=rep(1,D),\npopsize=12,generations=100)\n\n# show last Pareto front\nI=which(G$pareto.optimal)\nfor(i in I)\n{\n  x=round(G$par[i,])\n  cat(x,\" f=(\",sumbin(x),\",\",round(maxsin(x),2),\")\",\"\\n\",sep=\"\")\n}\n```\n\n----------------------------------------\n\nTITLE: Examining Movie Data File Contents in Bash\nDESCRIPTION: Using the head command to view the first 5 lines of the u.item file, which contains movie metadata.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\n> head -5 u.item\n```\n\n----------------------------------------\n\nTITLE: Arithmetic Rules for Successor-Based Representation\nDESCRIPTION: Defines small-step rules for arithmetic operations using a successor-based representation of natural numbers in PCF.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_15\n\nLANGUAGE: PCF\nCODE:\n```\n0 + u ⟶ u\nS(t) + u ⟶ S(t + u)\n0 - u ⟶ 0\nt - 0 ⟶ t\nS(t) - S(u) ⟶ t - u\n0 * u ⟶ 0\nS(t) * u ⟶ t * u + u\nt / S(u) ⟶ ifz t - u then 0 else S((t - S(u)) / S(u))\n```\n\n----------------------------------------\n\nTITLE: Converting Raw Data to Rating Objects for MLlib\nDESCRIPTION: Transforms the raw data into Rating objects required by the ALS algorithm, converting string values to appropriate numeric types. Uses pattern matching to extract and name the array elements directly.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_56\n\nLANGUAGE: scala\nCODE:\n```\nval ratings = rawRatings.map { case Array(user, movie, rating) => Rating(user.toInt, movie.toInt, rating.toDouble) }\n```\n\n----------------------------------------\n\nTITLE: Implementing Logistic Regression Link Function in Python\nDESCRIPTION: This code snippet shows the logit link function used in logistic regression. It maps the linear predictor to a probability between 0 and 1.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_84\n\nLANGUAGE: python\nCODE:\n```\n1 / (1 + exp(-wTx))\n```\n\n----------------------------------------\n\nTITLE: Random Sampling of Tokenized Text in Scala\nDESCRIPTION: Samples tokenized text with a fixed random seed for reproducible results, taking 100 tokens at 30% sampling rate.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_199\n\nLANGUAGE: scala\nCODE:\n```\nprintln(whiteSpaceSplit.sample(true, 0.3, 42).take(100).mkString(\",\"))\n```\n\n----------------------------------------\n\nTITLE: Creating Term Dictionary Using Spark's zipWithIndex Function\nDESCRIPTION: Demonstrates a more efficient method to create a term dictionary using Spark's zipWithIndex function, which automatically assigns indices to unique terms and creates a key-value mapping.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nall_terms_dict2 = title_terms.flatMap(lambda x: x).distinct().zipWithIndex().collectAsMap()\nprint \"Index of term 'Dead': %d\" % all_terms_dict2['Dead']\nprint \"Index of term 'Rooms': %d\" % all_terms_dict2['Rooms']\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Squared Error for Recommendation Model in Scala\nDESCRIPTION: This snippet calculates the Mean Squared Error (MSE) for a recommendation model using Spark RDDs. It compares predicted ratings with actual ratings.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_69\n\nLANGUAGE: Scala\nCODE:\n```\nval usersProducts = ratings.map{ case Rating(user, product, rating)  => (user, product)}\nval predictions = model.predict(usersProducts).map{\n    case Rating(user, product, rating) => ((user, product), rating)\n}\n\nval ratingsAndPredictions = ratings.map{\n  case Rating(user, product, rating) => ((user, product), rating)\n}.join(predictions)\n\nval MSE = ratingsAndPredictions.map{\n    case ((user, product), (actual, predicted)) =>  math.pow((actual - predicted), 2)\n}.reduce(_ + _) / ratingsAndPredictions.count\nprintln(\"Mean Squared Error = \" + MSE)\n```\n\n----------------------------------------\n\nTITLE: Comparing WBGA and NSGA-II for Multi-Objective Optimization in R\nDESCRIPTION: This code compares two multi-objective optimization algorithms (WBGA and NSGA-II) on the FES2 benchmark function. It implements the evaluation functions, runs both algorithms, and visualizes the resulting Pareto fronts using 3D scatterplots.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_118\n\nLANGUAGE: R\nCODE:\n```\nlibrary(genalg) # load rbga function\n\nlibrary(mco) # load nsga2 function\n\nset.seed(12345) # set for replicability\n\n# real value FES2 benchmark:\n\nfes2=function(x)\n\n{ D=length(x);f=rep(0,3)\n\nfor(i in 1:D)\n\n{\n\nf[1]=f[1]+(x[i]-0.5*cos(10*pi/D)-0.5)^2\n\nf[2]=f[2]+abs(x[i]-(sin(i-1))^2*(cos(i-1)^2))^0.5\n\nf[3]=f[3]+abs(x[i]-0.25*cos(i-1)*cos(2*i-2)-0.5)^0.5\n\n}\n\nreturn(f)\n\n}\n\nD=8;m=3\n\n# WBGA execution:\n\n# evaluation function for WBGA\n\n# (also used to print and get last population fes2 values:\n\n# WBGA chromosome used: x=(w1,w2,w3,v1,v2,v3,...,vD)\n\n# where w_i are the weights and v_j the values\n\neval=function(x,REPORT=FALSE)\n\n{ D=length(x)/2\n\n# normalize weights, such that sum(w)=1\n\nw=x[1:m]/sum(x[1:m]);v=x[(m+1):length(x)];f=fes2(v)\n\nif(REPORT)\n\n{ cat(\"w:\",round(w,2),\"v:\",round(v,2),\"f:\",round(f,2),\"\\n\")\n\nreturn(f)\n\n}\n\nelse return(sum(w*f))\n\n}\n\nWBGA=rbga(evalFunc=eval,\n\nstringMin=rep(0,D*2),stringMax=rep(1,D*2),\n\npopSize=20,iters=100)\n\nprint(\"WBGA last population:\")\n\n# S1 contains last population fes2 values in individuals x objectives\n\nS1=t(apply(WBGA$population,1,eval,REPORT=TRUE))\n\nLS1=nrow(S1)\n\n# NSGA-II execution:\n\nNSGA2=nsga2(fn=fes2,idim=D,odim=m,\n\nlower.bounds=rep(0,D),upper.bounds=rep(1,D),\n\npopsize=20,generations=100)\n\nS2=NSGA2$value[NSGA2$pareto.optimal,]\n\nprint(\"NSGA2 last Pareto front:\")\n\nprint(S2)\n\nLS2=nrow(S2)\n\n# Comparison of results:\n\nlibrary(scatterplot3d)\n\nS=data.frame(rbind(S1,S2))\n\nnames(S)=c(\"f1\",\"f2\",\"f3\")\n\ncol=c(rep(\"gray\",LS1),rep(\"black\",LS2))\n\n# nice scatterplot3d\n\n# WBGA points are in gray\n\n# NSGA2 points are in black\n\n# NSGA2 produces a more disperse and interesting\n\n# Pareto front when compared with WBGA\n\nscatterplot3d(S,pch=16,color=col)\n```\n\n----------------------------------------\n\nTITLE: Running the spark-ec2 Script to Manage EC2 Clusters\nDESCRIPTION: Basic command to run the spark-ec2 script which helps launch, manage, and configure Spark clusters on Amazon EC2. Without parameters, it displays usage instructions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n>./ec2/spark-ec2 \n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Spark Streaming Application in Scala\nDESCRIPTION: A simple Spark Streaming application that connects to a socket stream on port 9999 and prints out the first few elements of each batch. It demonstrates how to initialize a StreamingContext, create a DStream from a socket source, and apply basic operations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_225\n\nLANGUAGE: scala\nCODE:\n```\n/**\n * A simple Spark Streaming app in Scala\n */\nobject SimpleStreamingApp {\n\n  def main(args: Array[String]) {\n\n    val ssc = new StreamingContext(\"local[2]\", \"First Streaming App\", Seconds(10))\n    val stream = ssc.socketTextStream(\"localhost\", 9999)\n\n    // here we simply print out the first few elements of each\n    // batch\n    stream.print()\n    ssc.start()\n    ssc.awaitTermination()\n\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Spark Streaming Application in Scala\nDESCRIPTION: A simple Spark Streaming application that connects to a socket stream on port 9999 and prints out the first few elements of each batch. It demonstrates how to initialize a StreamingContext, create a DStream from a socket source, and apply basic operations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_225\n\nLANGUAGE: scala\nCODE:\n```\n/**\n * A simple Spark Streaming app in Scala\n */\nobject SimpleStreamingApp {\n\n  def main(args: Array[String]) {\n\n    val ssc = new StreamingContext(\"local[2]\", \"First Streaming App\", Seconds(10))\n    val stream = ssc.socketTextStream(\"localhost\", 9999)\n\n    // here we simply print out the first few elements of each\n    // batch\n    stream.print()\n    ssc.start()\n    ssc.awaitTermination()\n\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Loading PCA Results in Python\nDESCRIPTION: This code loads the saved principal components matrix from CSV into a numpy array in Python.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_184\n\nLANGUAGE: python\nCODE:\n```\npcs = np.loadtxt(\"/tmp/pc.csv\", delimiter=\",\")\nprint(pcs.shape)\n```\n\n----------------------------------------\n\nTITLE: Finding Orders with Identical Item Quantities (Self-Join)\nDESCRIPTION: Uses a self-join approach to find orders where all items have the same quantity. This method is contrasted with a set-based approach in the document.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_77\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT DISTINCT O1.order_nbr\nFROM OrderDetails AS O1\nWHERE NOT EXISTS\n  (SELECT *\n   FROM OrderDetails AS O2\n   WHERE O1.order_nbr = O2.order_nbr\n     AND O1.quantity <> O2.quantity);\n\n```\n\n----------------------------------------\n\nTITLE: Record Field Access Operation in PCF\nDESCRIPTION: Demonstrates the syntax for accessing fields in a record using the dot notation. Shows how records are constructed and accessed using labeled fields rather than numeric indices.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_36\n\nLANGUAGE: PCF\nCODE:\n```\n{l1 =t1, ...,ln =tn}.l\n```\n\n----------------------------------------\n\nTITLE: Extracting Relevant Fields from Raw Data\nDESCRIPTION: Maps the raw data to extract only the user ID, movie ID, and rating fields by splitting on tab characters and taking the first three elements. This removes the timestamp which is not needed for training.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_53\n\nLANGUAGE: scala\nCODE:\n```\nval rawRatings = rawData.map(_.split(\"\\t\").take(3))\n```\n\n----------------------------------------\n\nTITLE: Creating RDDs from Text Files in Spark\nDESCRIPTION: Shows how to create an RDD from a text file using sc.textFile method, where each line becomes a String record in the RDD.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nval rddFromTextFile = sc.textFile(\"LICENSE\")\n```\n\n----------------------------------------\n\nTITLE: Installing Docker Environment for KAG with Docker Compose\nDESCRIPTION: Command to download the docker-compose.yml configuration file and start services using Docker Compose. This is the recommended approach for setting up the KAG environment for all users.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/README_cn.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://raw.githubusercontent.com/OpenSPG/openspg/refs/heads/master/dev/release/docker-compose.yml -o docker-compose.yml\ndocker compose -f docker-compose.yml up -d\n```\n\n----------------------------------------\n\nTITLE: Set-based Approach Using Set Operations in SQL\nDESCRIPTION: A pure set operations solution that uses set difference (EXCEPT) to find days not present in the Calendar_Table for the given week, then selects the minimum value.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_88\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE PROCEDURE Insert_Missing_Weekday (IN p_week_nbr INTEGER)\nBEGIN\n  INSERT INTO Calendar_Table (week_nbr, day_nbr)\n  SELECT p_week_nbr, MIN(V.day_nbr)\n  FROM ((SELECT 1 AS day_nbr) UNION ALL\n        (SELECT 2) UNION ALL\n        (SELECT 3) UNION ALL\n        (SELECT 4) UNION ALL\n        (SELECT 5) UNION ALL\n        (SELECT 6) UNION ALL\n        (SELECT 7)) AS V\n  EXCEPT\n  SELECT C.day_nbr\n  FROM Calendar_Table AS C\n  WHERE C.week_nbr = p_week_nbr;\nEND;\n```\n\n----------------------------------------\n\nTITLE: Retrieving EC2 Spark Cluster Master Hostname\nDESCRIPTION: Command to retrieve the public domain name of the master node in a running Spark EC2 cluster. This information is needed to connect to the cluster.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n>./spark-ec2 -i spark.pem get-master test-cluster\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Trained Models\nDESCRIPTION: Shows how to use trained models to make predictions on individual data points and bulk predictions on RDD datasets.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_95\n\nLANGUAGE: scala\nCODE:\n```\nval dataPoint = data.first\nval prediction = lrModel.predict(dataPoint.features)\nval trueLabel = dataPoint.label\nval predictions = lrModel.predict(data.map(lp => lp.features))\npredictions.take(5)\n```\n\n----------------------------------------\n\nTITLE: Creating a Reference in Extended PCF\nDESCRIPTION: The 'ref' construct in extended PCF creates a new reference with an initial value derived from term t. The computed value is the reference itself, which becomes part of the language's values.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_34\n\nLANGUAGE: theoretical\nCODE:\n```\nref t\n```\n\n----------------------------------------\n\nTITLE: Creating a Calendar Table with Week and Day Numbers in SQL\nDESCRIPTION: Defines a calendar table schema with constraints that model a year's worth of days organized by week number (1-53) and day number (1-7). This revised version includes proper constraints unlike the original example.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_83\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE Calendar_Table\n(\n  week_nbr INTEGER NOT NULL,\n  day_nbr INTEGER NOT NULL CHECK (day_nbr BETWEEN 1 AND 7),\n  PRIMARY KEY (week_nbr, day_nbr)\n);\n```\n\n----------------------------------------\n\nTITLE: Set-Based CSV Parsing Using Sequence Table in SQL\nDESCRIPTION: A set-based approach to parse a comma-separated string into a table of values using a sequence table. This method identifies pairs of commas and extracts the values between them in a single operation.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_72\n\nLANGUAGE: SQL\nCODE:\n```\nDECLARE @str VARCHAR(8000) = ',123,456,789,';\nWITH Parsed AS (\n    SELECT S1.s AS pos,\n           MIN(S2.s) AS nextpos\n    FROM Sequence AS S1\n    CROSS JOIN Sequence AS S2\n    WHERE S1.s < S2.s\n    AND SUBSTRING(@str, S1.s, 1) = ','\n    AND SUBSTRING(@str, S2.s, 1) = ','\n    GROUP BY S1.s\n)\nSELECT (pos - 1) / 2 + 1 AS place,\n       CAST(SUBSTRING(@str, pos + 1, nextpos - pos - 1) AS INTEGER) AS value\nFROM Parsed\nORDER BY place;\n```\n\n----------------------------------------\n\nTITLE: Running Constraint Handling Comparison Experiment in R\nDESCRIPTION: Implementation of the experiment to compare death penalty and repair strategies for handling constraints. The code runs both approaches for multiple runs, collecting and storing results for statistical analysis. Each approach uses the same EDA optimization method with different constraint handling strategies.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_54\n\nLANGUAGE: R\nCODE:\n```\nfor(R in 1:Runs) # cycle all runs\n{\n\nB=NA;EV=0; F=rep(NA,MAXFN); BEST= -Inf # reset vars.\n\nsetMethod(\"edaOptimize\",\"EDA\",edaOptimizeDisabled)\n\nsetMethod(\"edaTerminate\",\"EDA\",edaTerminateMaxGen)\n\nsuppressWarnings(edaRun(GCEDA,cprofit2,lower,upper))\n\nRES[[1]][,R]=F # store all best values\n\nVAL[R,1]=F[MAXFN] # store best value at MAXFN\n\nB=NA;EV=0; F=rep(NA,MAXFN); BEST= -Inf # reset vars.\n\n# set local repair search method:\n\nsetMethod(\"edaOptimize\",\"EDA\",localRepair)\n\n# set additional termination criterion:\n\nsetMethod(\"edaTerminate\",\"EDA\",\n\nedaTerminateCombined(edaTerminateMaxGen,edaTerminateEvalStdDev))\n\n# this edaRun might produces warnings or errors:\n\nsuppressWarnings(try(edaRun(GCEDA,cprofit2,lower,upper),silent=TRUE))\n\nif(EV<MAXFN) # if stopped due to EvalStdDev\n\nF[(EV+1):MAXFN]=rep(F[EV],MAXFN-EV) # replace NAs\n\nRES[[2]][,R]=F # store all best values\n\nVAL[R,2]=F[MAXFN] # store best value at MAXFN\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Model Accuracy Metrics\nDESCRIPTION: Implements accuracy calculation for all trained models by comparing predictions against true labels in the training dataset.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_96\n\nLANGUAGE: scala\nCODE:\n```\nval lrTotalCorrect = data.map { point =>\n  if (lrModel.predict(point.features) == point.label) 1 else 0\n}.sum \nval lrAccuracy = lrTotalCorrect / data.count\n\nval svmTotalCorrect = data.map { point =>\n  if (svmModel.predict(point.features) == point.label) 1 else 0\n}.sum\nval nbTotalCorrect = nbData.map { point =>\n  if (nbModel.predict(point.features) == point.label) 1 else 0\n}.sum\n\nval dtTotalCorrect = data.map { point =>\n  val score = dtModel.predict(point.features)\n  val predicted = if (score > 0.5) 1 else 0 \n  if (predicted == point.label) 1 else 0\n}.sum\n```\n\n----------------------------------------\n\nTITLE: Implementing Ordered Evolutionary Algorithm with Lamarckian Evolution in R\nDESCRIPTION: This function implements an ordered evolutionary algorithm (oea) for TSP, adapted from rbga.bin. It supports user-defined mutation and crossover operators, elitism, and Lamarckian evolution where solutions can be improved through local learning before genetic operations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_80\n\nLANGUAGE: R\nCODE:\n```\n### order (representation) evolutionary algorithm:\n\n# adapted version of rbga.bin that works with ordered vectors,\n\n# accepts used defined mutation and crossover operators and\n\n# accepts a Lamarckian evolution if evalFunc returns a list\n\n# note: assumes solution with values from the range 1,2,...,size\n\noea=function(size=10,suggestions=NULL,popSize=200,iters=100,mutationChance=NA,\n\nelitism=NA,evalFunc=NULL,\n\ncrossfunc=NULL,mutfunc=mutfunc,REPORT=0)\n\n{\n\nif(is.na(mutationChance)) { mutationChance=0.5 }\n\nif(is.na(elitism)) { elitism=floor(popSize/5)}\n\n# population initialization:\n\npopulation=matrix(nrow=popSize,ncol=size)\n\nif(!is.null(suggestions))\n\n{\n\nsuggestionCount=dim(suggestions)[1]\n\nfor(i in 1:suggestionCount)\n\npopulation[i, ] = suggestions[i, ]\n\nI=(suggestionCount+1):popSize ### new code\n\n}\n\nelse I=1:popSize ### new code\n\nfor(child in I) ### new code\n\npopulation[child,]=sample(1:size,size) ### new code\n\n# evaluate population:\n\nevalVals = rep(NA, popSize)\n\n# main GA cycle:\n\nfor(iter in 1:iters)\n\n{\n\n# evaluate population\n\nfor(object in 1:popSize)\n\n{### new code\n\nEF = evalFunc(population[object,])\n\nif(is.list(EF)) # Lamarckian change of solution\n\n{ population[object,]=EF$solution\n\nevalVals[object] = EF$eval\n\n}\n\nelse evalVals[object]=EF\n\n### end of new code\n\n}\n\nsortedEvaluations=sort(evalVals,index=TRUE)\n\nif(REPORT>0 && (iter%%REPORT==0||iter==1))\n\ncat(iter,\"best:\",sortedEvaluations$x[1],\"mean:\",mean(sortedEvaluations$x),\"\\n\")\n\nsortedPopulation=matrix(population[sortedEvaluations$ix,],ncol=size)\n\n# check elitism:\n\nnewPopulation=matrix(nrow=popSize,ncol=size)\n\nif(elitism>0) # applying elitism:\n\nnewPopulation[1:elitism,]=sortedPopulation[1:elitism,]\n\n### very new code inserted here : ###\n\n# roulette wheel selection of remaining individuals\n\nothers=popSize-elitism\n\nprob=(max(sortedEvaluations$x)-sortedEvaluations$x+1)\n\nprob=prob/sum(prob) # such that sum(prob)==1\n\n# crossover with half of the population (if !is.null)\n\nif(!is.null(crossfunc)) # need to crossover\n\nhalf=round(others/2)\n\nelse half=0 # no crossover\n\nif(!is.null(crossfunc))\n\n{\n\nfor(child in seq(1,half,by=2))\n\n{\n\nselIDs=sample(1:popSize,2,prob=prob)\n\nparents=sortedPopulation[selIDs, ]\n\nif(child==half)\n\nnewPopulation[elitism+child,]=crossfunc(parents)[1,] # 1st child\n\nelse\n\nnewPopulation[elitism+child:(child+1),]=crossfunc(parents) # two children\n\n}\n\n}\n\n# mutation with remaining individuals\n\nfor(child in (half+1):others)\n\n{\n\nselIDs=sample(1:popSize,1,prob=prob)\n\nnewPopulation[elitism+child,]=mutfunc(sortedPopulation[selIDs,])\n\n}\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Ordered Evolutionary Algorithm with Lamarckian Evolution in R\nDESCRIPTION: This function implements an ordered evolutionary algorithm (oea) for TSP, adapted from rbga.bin. It supports user-defined mutation and crossover operators, elitism, and Lamarckian evolution where solutions can be improved through local learning before genetic operations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_80\n\nLANGUAGE: R\nCODE:\n```\n### order (representation) evolutionary algorithm:\n\n# adapted version of rbga.bin that works with ordered vectors,\n\n# accepts used defined mutation and crossover operators and\n\n# accepts a Lamarckian evolution if evalFunc returns a list\n\n# note: assumes solution with values from the range 1,2,...,size\n\noea=function(size=10,suggestions=NULL,popSize=200,iters=100,mutationChance=NA,\n\nelitism=NA,evalFunc=NULL,\n\ncrossfunc=NULL,mutfunc=mutfunc,REPORT=0)\n\n{\n\nif(is.na(mutationChance)) { mutationChance=0.5 }\n\nif(is.na(elitism)) { elitism=floor(popSize/5)}\n\n# population initialization:\n\npopulation=matrix(nrow=popSize,ncol=size)\n\nif(!is.null(suggestions))\n\n{\n\nsuggestionCount=dim(suggestions)[1]\n\nfor(i in 1:suggestionCount)\n\npopulation[i, ] = suggestions[i, ]\n\nI=(suggestionCount+1):popSize ### new code\n\n}\n\nelse I=1:popSize ### new code\n\nfor(child in I) ### new code\n\npopulation[child,]=sample(1:size,size) ### new code\n\n# evaluate population:\n\nevalVals = rep(NA, popSize)\n\n# main GA cycle:\n\nfor(iter in 1:iters)\n\n{\n\n# evaluate population\n\nfor(object in 1:popSize)\n\n{### new code\n\nEF = evalFunc(population[object,])\n\nif(is.list(EF)) # Lamarckian change of solution\n\n{ population[object,]=EF$solution\n\nevalVals[object] = EF$eval\n\n}\n\nelse evalVals[object]=EF\n\n### end of new code\n\n}\n\nsortedEvaluations=sort(evalVals,index=TRUE)\n\nif(REPORT>0 && (iter%%REPORT==0||iter==1))\n\ncat(iter,\"best:\",sortedEvaluations$x[1],\"mean:\",mean(sortedEvaluations$x),\"\\n\")\n\nsortedPopulation=matrix(population[sortedEvaluations$ix,],ncol=size)\n\n# check elitism:\n\nnewPopulation=matrix(nrow=popSize,ncol=size)\n\nif(elitism>0) # applying elitism:\n\nnewPopulation[1:elitism,]=sortedPopulation[1:elitism,]\n\n### very new code inserted here : ###\n\n# roulette wheel selection of remaining individuals\n\nothers=popSize-elitism\n\nprob=(max(sortedEvaluations$x)-sortedEvaluations$x+1)\n\nprob=prob/sum(prob) # such that sum(prob)==1\n\n# crossover with half of the population (if !is.null)\n\nif(!is.null(crossfunc)) # need to crossover\n\nhalf=round(others/2)\n\nelse half=0 # no crossover\n\nif(!is.null(crossfunc))\n\n{\n\nfor(child in seq(1,half,by=2))\n\n{\n\nselIDs=sample(1:popSize,2,prob=prob)\n\nparents=sortedPopulation[selIDs, ]\n\nif(child==half)\n\nnewPopulation[elitism+child,]=crossfunc(parents)[1,] # 1st child\n\nelse\n\nnewPopulation[elitism+child:(child+1),]=crossfunc(parents) # two children\n\n}\n\n}\n\n# mutation with remaining individuals\n\nfor(child in (half+1):others)\n\n{\n\nselIDs=sample(1:popSize,1,prob=prob)\n\nnewPopulation[elitism+child,]=mutfunc(sortedPopulation[selIDs,])\n\n}\n\n```\n\n----------------------------------------\n\nTITLE: Set-based Approach Using CASE Expression in SQL\nDESCRIPTION: A non-procedural SQL solution that uses a CASE expression to find the earliest missing day. While not fully set-oriented, it eliminates loops and local variables for better optimization.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_86\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE PROCEDURE Missing_Weekday (IN p_week_nbr INTEGER)\nBEGIN\n  INSERT INTO Calendar_Table (week_nbr, day_nbr)\n  SELECT p_week_nbr,\n  CASE \n    WHEN NOT EXISTS (SELECT *\n                     FROM Calendar_Table\n                     WHERE week_nbr = p_week_nbr\n                     AND day_nbr = 1) THEN 1\n    WHEN NOT EXISTS (SELECT *\n                     FROM Calendar_Table\n                     WHERE week_nbr = p_week_nbr\n                     AND day_nbr = 2) THEN 2\n    WHEN NOT EXISTS (SELECT *\n                     FROM Calendar_Table\n                     WHERE week_nbr = p_week_nbr\n                     AND day_nbr = 3) THEN 3\n    WHEN NOT EXISTS (SELECT *\n                     FROM Calendar_Table\n                     WHERE week_nbr = p_week_nbr\n                     AND day_nbr = 4) THEN 4\n    WHEN NOT EXISTS (SELECT *\n                     FROM Calendar_Table\n                     WHERE week_nbr = p_week_nbr\n                     AND day_nbr = 5) THEN 5\n    WHEN NOT EXISTS (SELECT *\n                     FROM Calendar_Table\n                     WHERE week_nbr = p_week_nbr\n                     AND day_nbr = 6) THEN 6\n    WHEN NOT EXISTS (SELECT *\n                     FROM Calendar_Table\n                     WHERE week_nbr = p_week_nbr\n                     AND day_nbr = 7) THEN 7\n  END AS day_nbr;\nEND;\n```\n\n----------------------------------------\n\nTITLE: Displaying Similar Movies in Scala\nDESCRIPTION: This code sorts and displays the top 10 similar movies to a given movie, excluding the movie itself. It uses a similarity score and movie titles.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_68\n\nLANGUAGE: Scala\nCODE:\n```\nval sortedSims2 = sims.top(K + 1)(Ordering.by[(Int, Double), Double] { case (id, similarity) => similarity })\nsortedSims2.slice(1, 11).map{ case (id, sim) => (titles(id), sim) }.mkString(\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Executing Question Answering Tasks\nDESCRIPTION: Command to run a Python script that handles DSL and question answering tasks for the knowledge graph.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython ./solver/qa.py\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Bike Sharing Dataset in PySpark\nDESCRIPTION: Code to load the bike sharing dataset from CSV, split records, and perform initial data inspection using PySpark. The code reads a CSV file, removes headers, and caches the data for efficient processing.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_117\n\nLANGUAGE: python\nCODE:\n```\npath = \"/PATH/hour_noheader.csv\"\nraw_data = sc.textFile(path)\nnum_data = raw_data.count()\nrecords = raw_data.map(lambda x: x.split(\",\"))\nfirst = records.first()\nprint first\nprint num_data\nrecords.cache()\n```\n\n----------------------------------------\n\nTITLE: Modified Object Field Access in PCF with References\nDESCRIPTION: A rewritten version of the previous term to ensure side effects take place before field access, resulting in a different output value when interpreted.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_41\n\nLANGUAGE: PCF\nCODE:\n```\nlet r = ref 0 in\nlet t = {a = ςs !r} in\nr := 1;\nlet u = t in\nu#a\n```\n\n----------------------------------------\n\nTITLE: Numeric Token Filtering in Scala\nDESCRIPTION: Filters out tokens containing numeric characters using regex pattern matching, further reducing the token set.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_201\n\nLANGUAGE: scala\nCODE:\n```\nval regex = \"\"\"[^0-9]*\"\"\".r\nval filterNumbers = nonWordSplit.filter(token => regex.pattern.matcher(token).matches)\nprintln(filterNumbers.distinct.count)\n```\n\n----------------------------------------\n\nTITLE: Sphere Function Optimization using EDA in R\nDESCRIPTION: Complete implementation of sphere function optimization using EDA with different population sizes. Includes setup, execution and result reporting.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_47\n\nLANGUAGE: R\nCODE:\n```\nlibrary(copulaedas)\n\nsphere=function(x) sum(x^2)\nD=2; maxit=10; LP=5\nset.seed(12345) # set for replicability\n\n# set termination criterion and report method:\nsetMethod(\"edaTerminate\",\"EDA\",edaTerminateMaxGen)\nsetMethod(\"edaReport\",\"EDA\",edaReportSimple)\n\n# set EDA type:\nUMDA=CEDA(copula=\"indep\",margin=\"norm\",popSize=LP,maxGen=maxit)\nUMDA@name=\"UMDA (LP=5)\"\n\n# run the algorithm:\nE=edaRun(UMDA,sphere,rep(-5.2,D),rep(5.2,D))\n\n# show result:\nshow(E)\ncat(\"best:\",E@bestSol,\"f:\",E@bestEval,\"\\n\")\n\n# second EDA execution, using LP=100:\nmaxit=10; LP=100;\nUMDA=CEDA(copula=\"indep\",margin=\"norm\",popSize=LP,maxGen=maxit)\nUMDA@name=\"UMDA (LP=100)\"\nsetMethod(\"edaReport\",\"EDA\",edaReportDumpPop) # pop_*.txt files\nE=edaRun(UMDA,sphere,rep(-5.2,D),rep(5.2,D))\nshow(E)\ncat(\"best:\",E@bestSol,\"f:\",E@bestEval,\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Images with Python Matplotlib\nDESCRIPTION: Python code snippet to display a facial image using matplotlib's imread and imshow functions. This allows visual inspection of the dataset images.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_171\n\nLANGUAGE: python\nCODE:\n```\npath = \"/PATH/lfw/PATH/lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg\"\nae = imread(path)\nimshow(ae)\n```\n\n----------------------------------------\n\nTITLE: Loading and Examining MovieLens Movie Data with Spark\nDESCRIPTION: Loads movie data from the MovieLens dataset and displays the first line to understand the data structure. The movie data contains ID, title, and genre information.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_149\n\nLANGUAGE: scala\nCODE:\n```\nval movies = sc.textFile(\"/PATH/ml-100k/u.item\")\nprintln(movies.first)\n```\n\n----------------------------------------\n\nTITLE: Launching PySpark with IPython and Matplotlib\nDESCRIPTION: Command to start the PySpark console with IPython and matplotlib functionality enabled for data analysis and visualization.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\n> IPYTHON=1 IPYTHON_OPTS=\"--pylab\" ./bin/pyspark\n```\n\n----------------------------------------\n\nTITLE: Displaying Pareto Front Results from NSGA-II Optimization in R\nDESCRIPTION: Prints the optimal hyperparameter combinations found in the Pareto front, showing gamma and C values along with the corresponding AUC scores for each wine quality class.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_127\n\nLANGUAGE: R\nCODE:\n```\nI=which(G$pareto.optimal)\n\nfor(i in I)\n{ x=G$par[i,]\n  n=length(x)\n  gamma=2^x[1]\n  C=2^x[2]\n  features=round(x[3:n])\n  inputs=which(features==1)\n  cat(\"gamma:\",gamma,\"C:\",C,\"; f=(\",\n      1-G$value[i,1:3],\")\\n\",sep=\" \")\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Companies with Shared Legal Representatives\nDESCRIPTION: GQL query to identify companies that share the same legal representative, which can be relevant for risk assessment and ownership relationships.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README_cn.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nknext reasoner execute --dsl \"\nMATCH\n    (s:SupplyChain.Company)-[:sameLegalRepresentative]->(o:SupplyChain.Company)\nRETURN\n    s.name, o.name\n\"\n```\n\n----------------------------------------\n\nTITLE: Chaining RDD Transformations\nDESCRIPTION: Shows how to chain multiple transformations together before triggering computation with an action.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nval transformedRDD = rddFromTextFile.map(line => line.size).filter(size => size > 10).map(size => size * 2)\n```\n\n----------------------------------------\n\nTITLE: Loading and Inspecting MovieLens Genre Mapping File\nDESCRIPTION: Loads the genre mapping file and displays the first five lines to understand the mapping between genre names and their numerical indices. Each line contains a genre name and its corresponding index.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_150\n\nLANGUAGE: scala\nCODE:\n```\nval genres = sc.textFile(\"/PATH/ml-100k/u.genre\")\ngenres.take(5).foreach(println)\n```\n\n----------------------------------------\n\nTITLE: TSP Problem Setup and Data Loading in R\nDESCRIPTION: Code for downloading and processing the Qatar TSP dataset with 194 cities. The code fetches data from a URL, parses it, and prepares it for processing with the TSP package.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_82\n\nLANGUAGE: R\nCODE:\n```\n### tsp.R file ###\n\nlibrary(TSP) # load TSP package\nlibrary(RCurl) # load RCurl package\nsource(\"oea.R\") # load ordered evolutionary algorithm\n\n# get Qatar - 194 cities TSP instance:\ntxt=getURL(\"http://​www.​math.​uwaterloo.​ca/​tsp/​world/​qa194.​tsp\")\n\n# simple parse of txt object, removing header and last line:\ntxt=strsplit(txt,\"NODE_COORD_SECTION\") # split text into 2 parts\ntxt=txt[[1]][2] # get second text part\ntxt=strsplit(txt,\"EOF\") # split text into 2 parts\ntxt=txt[[1]][1] # get first text part\n\n# save data into a simple .csv file, sep=\" \":\ncat(txt,file=\"qa194.csv\")\n\n# read the TSP format into Data\n# (first row is empty, thus header=TRUE)\n# get city Cartesian coordinates\nData=read.table(\"qa194.csv\",sep=\" \")\nData=Data[,3:2] # longitude and latitude\nnames(Data)=c(\"x\",\"y\") # x and y labels\nN=nrow(Data) # number of cities\n\n# distance between two cities (EUC_2D-norm)\n# Eulidean distance rounded to whole number\nD=dist(Data,upper=TRUE)\nD[1:length(D)]=round(D[1:length(D)])\n\n# create TSP object from D:\nTD=TSP(D)\n```\n\n----------------------------------------\n\nTITLE: TSP Problem Setup and Data Loading in R\nDESCRIPTION: Code for downloading and processing the Qatar TSP dataset with 194 cities. The code fetches data from a URL, parses it, and prepares it for processing with the TSP package.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_82\n\nLANGUAGE: R\nCODE:\n```\n### tsp.R file ###\n\nlibrary(TSP) # load TSP package\nlibrary(RCurl) # load RCurl package\nsource(\"oea.R\") # load ordered evolutionary algorithm\n\n# get Qatar - 194 cities TSP instance:\ntxt=getURL(\"http://​www.​math.​uwaterloo.​ca/​tsp/​world/​qa194.​tsp\")\n\n# simple parse of txt object, removing header and last line:\ntxt=strsplit(txt,\"NODE_COORD_SECTION\") # split text into 2 parts\ntxt=txt[[1]][2] # get second text part\ntxt=strsplit(txt,\"EOF\") # split text into 2 parts\ntxt=txt[[1]][1] # get first text part\n\n# save data into a simple .csv file, sep=\" \":\ncat(txt,file=\"qa194.csv\")\n\n# read the TSP format into Data\n# (first row is empty, thus header=TRUE)\n# get city Cartesian coordinates\nData=read.table(\"qa194.csv\",sep=\" \")\nData=Data[,3:2] # longitude and latitude\nnames(Data)=c(\"x\",\"y\") # x and y labels\nN=nrow(Data) # number of cities\n\n# distance between two cities (EUC_2D-norm)\n# Eulidean distance rounded to whole number\nD=dist(Data,upper=TRUE)\nD[1:length(D)]=round(D[1:length(D)])\n\n# create TSP object from D:\nTD=TSP(D)\n```\n\n----------------------------------------\n\nTITLE: Querying Company Credit Factors\nDESCRIPTION: GQL query to extract company credit evaluation factors including fund transaction metrics and cashflow differences across different time periods.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README_cn.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nknext reasoner execute --dsl \"\nMATCH\n    (s:SupplyChain.Company)\nRETURN\n    s.id, s.name, s.fundTrans1Month, s.fundTrans3Month,\n    s.fundTrans6Month, s.fundTrans1MonthIn, s.fundTrans3MonthIn,\n    s.fundTrans6MonthIn, s.cashflowDiff1Month, s.cashflowDiff3Month,\n    s.cashflowDiff6Month\n\"\n```\n\n----------------------------------------\n\nTITLE: Markdown Table of Jay Chou's Music Singles\nDESCRIPTION: A formatted markdown table displaying Jay Chou's music singles with song names (including hyperlinks), release dates, and brief descriptions for each track. The table is organized in reverse chronological order from 2023 to 2014.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/tests/unit/builder/data/test_markdown.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|                                      **<font style=\"color:rgb(51, 51, 51);\">歌曲名称</font>**                                      |  **<font style=\"color:rgb(51, 51, 51);\">发行时间</font>**  |                                                             **<font style=\"color:rgb(51, 51, 51);\">歌曲简介</font>**                                                             |\n|--------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [<sup>圣诞星</sup>](https://baike.baidu.com/item/%E5%9C%A3%E8%AF%9E%E6%98%9F/63869869?fromModule=lemma_inlink)                    | <font style=\"color:rgb(51, 51, 51);\">2023-12-21</font> | <font style=\"color:rgb(51, 51, 51);\">-</font>                                                                                                                                |\n| [Mojito](https://baike.baidu.com/item/Mojito/50474451?fromModule=lemma_inlink)                                                 | <font style=\"color:rgb(51, 51, 51);\">2020-6-12</font>  | <font style=\"color:rgb(51, 51, 51);\">单曲</font><sup><font style=\"color:rgb(51, 102, 204);\"> </font></sup><sup><font style=\"color:rgb(51, 102, 204);\">[131]</font></sup>       |\n| [我是如此相信](https://baike.baidu.com/item/%E6%88%91%E6%98%AF%E5%A6%82%E6%AD%A4%E7%9B%B8%E4%BF%A1/24194094?fromModule=lemma_inlink) | <font style=\"color:rgb(51, 51, 51);\">2019-12-15</font> | <font style=\"color:rgb(51, 51, 51);\">电影《天火》主题曲</font><sup><font style=\"color:rgb(51, 102, 204);\"> </font></sup><sup><font style=\"color:rgb(51, 102, 204);\">[83]</font></sup> |\n| [说好不哭](https://baike.baidu.com/item/%E8%AF%B4%E5%A5%BD%E4%B8%8D%E5%93%AD/23748447?fromModule=lemma_inlink)                     | <font style=\"color:rgb(51, 51, 51);\">2019-09-16</font> | <font style=\"color:rgb(51, 51, 51);\">with 五月天阿信</font>                                                                                                                       |\n| [不爱我就拉倒](https://baike.baidu.com/item/%E4%B8%8D%E7%88%B1%E6%88%91%E5%B0%B1%E6%8B%89%E5%80%92/22490709?fromModule=lemma_inlink) | <font style=\"color:rgb(51, 51, 51);\">2018-05-15</font> | <font style=\"color:rgb(51, 51, 51);\">-</font>                                                                                                                                |\n| [等你下课](https://baike.baidu.com/item/%E7%AD%89%E4%BD%A0%E4%B8%8B%E8%AF%BE/22344815?fromModule=lemma_inlink)                     | <font style=\"color:rgb(51, 51, 51);\">2018-01-18</font> | <font style=\"color:rgb(51, 51, 51);\">杨瑞代参与演唱</font>                                                                                                                          |\n| [英雄](https://baike.baidu.com/item/%E8%8B%B1%E9%9B%84/19459565?fromModule=lemma_inlink)                                         | <font style=\"color:rgb(51, 51, 51);\">2016-03-24</font> | <font style=\"color:rgb(51, 51, 51);\">《英雄联盟》游戏主题曲</font>                                                                                                                      |\n| [Try](https://baike.baidu.com/item/Try/19208892?fromModule=lemma_inlink)                                                       | <font style=\"color:rgb(51, 51, 51);\">2016-01-06</font> | <font style=\"color:rgb(51, 51, 51);\">与派伟俊合唱，电影《功夫熊猫3》主题曲</font>                                                                                                              |\n| [婚礼曲](https://baike.baidu.com/item/%E5%A9%9A%E7%A4%BC%E6%9B%B2/22913856?fromModule=lemma_inlink)                               | <font style=\"color:rgb(51, 51, 51);\">2015</font>       | <font style=\"color:rgb(51, 51, 51);\">纯音乐</font>                                                                                                                              |\n| [夜店咖](https://baike.baidu.com/item/%E5%A4%9C%E5%BA%97%E5%92%96/16182672?fromModule=lemma_inlink)                               | <font style=\"color:rgb(51, 51, 51);\">2014-11-25</font> | <font style=\"color:rgb(51, 51, 51);\">与嘻游记合唱</font>                                                                                                                           |\n```\n\n----------------------------------------\n\nTITLE: β-Reduction Rule in PCF\nDESCRIPTION: Defines the β-reduction rule for function application in PCF's small-step operational semantics.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_10\n\nLANGUAGE: PCF\nCODE:\n```\n(fun x -> t) u ⟶ t[x := u]\n```\n\n----------------------------------------\n\nTITLE: Counting Token Occurrences in Spark RDD\nDESCRIPTION: This snippet counts the occurrences of each token in the corpus and retrieves the top 20 most frequent tokens. It uses Spark's reduceByKey and top functions with a custom ordering.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_202\n\nLANGUAGE: scala\nCODE:\n```\nval tokenCounts = filterNumbers.map(t => (t, 1)).reduceByKey(_ + _)\nval oreringDesc = Ordering.by[(String, Int), Int](_._2)\nprintln(tokenCounts.top(20)(oreringDesc).mkString(\"\\n\"))\n```\n\n----------------------------------------\n\nTITLE: Launching a Spark Cluster on Amazon EC2\nDESCRIPTION: Command to launch a Spark cluster on Amazon EC2 using the spark-ec2 script. This creates a cluster with one master and one slave node, using specified instance type and Hadoop version.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n> cd ec2\n>./spark-ec2 -k spark -i spark.pem -s 1 --instance-type m3.medium --hadoop-major-version 2 launch test-cluster\n```\n\n----------------------------------------\n\nTITLE: Index Movement Function for TSP in R\nDESCRIPTION: Helper function for manipulating indices in a circular manner during TSP solution construction. It handles wrapping around the ends of the permutation.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_85\n\nLANGUAGE: R\nCODE:\n```\n# move city index according to dir\nmindex=function(i,dir,s=NULL,N=length(s))\n{ res=i+dir #positive or negative jump\n  if(res<1) res=N+res else if(res>N) res=res-N\n  return(res)\n}\n```\n\n----------------------------------------\n\nTITLE: SQL-92 Standard Approach for Applying Predicates in OUTER JOINs\nDESCRIPTION: Demonstrates two standard SQL-92 approaches for combining JOINs with filtering predicates. The first applies the predicate after the JOIN, while the second incorporates it directly in the JOIN condition.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_29\n\nLANGUAGE: SQL\nCODE:\n```\nS LEFT OUTER JOIN SP ON S.S# = SP.S# WHERE qty < 200\n```\n\nLANGUAGE: SQL\nCODE:\n```\nS LEFT OUTER JOIN SP ON (S.S# = SP.S# AND qty < 200)\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing Raw Data in Spark\nDESCRIPTION: Scala code to load TSV data into Spark RDD and perform initial data processing\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_90\n\nLANGUAGE: scala\nCODE:\n```\nval rawData = sc.textFile(\"/PATH/train_noheader.tsv\")\nval records = rawData.map(line => line.split(\"\\t\"))\nrecords.first()\n```\n\n----------------------------------------\n\nTITLE: Projecting Data Using PCA in Scala\nDESCRIPTION: This snippet projects the original image data into the space of principal components using matrix multiplication.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_186\n\nLANGUAGE: scala\nCODE:\n```\nval projected = matrix.multiply(pc)\nprintln(projected.numRows, projected.numCols)\n```\n\n----------------------------------------\n\nTITLE: Feature Standardization using StandardScaler\nDESCRIPTION: Standardizes features by centering them around zero mean and scaling by standard deviation using Spark's StandardScaler.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_101\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.feature.StandardScaler\nval scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)\nval scaledData = data.map(lp => LabeledPoint(lp.label, scaler.transform(lp.features)))\n```\n\n----------------------------------------\n\nTITLE: Applying Vector Conversion to Term RDD with Broadcast Variables\nDESCRIPTION: Demonstrates how to apply the vector conversion function to an RDD of terms, using Spark's broadcast method to efficiently distribute the term dictionary across the cluster.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nall_terms_bcast = sc.broadcast(all_terms_dict)\nterm_vectors = title_terms.map(lambda terms: create_vector(terms, all_terms_bcast.value))\nterm_vectors.take(5)\n```\n\n----------------------------------------\n\nTITLE: Importing Knowledge Graph Data\nDESCRIPTION: Commands to change to the builder directory, run the Python indexer script, and return to the main directory.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd builder && python indexer.py && cd ..\n```\n\n----------------------------------------\n\nTITLE: Extracting 20 Newsgroups Dataset\nDESCRIPTION: Command to extract the downloaded 20 Newsgroups dataset archive file, creating training and test directories.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_194\n\nLANGUAGE: shell\nCODE:\n```\ntar xfvz 20news-bydate.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Implementing Absolute Error Function in Python\nDESCRIPTION: Function to calculate absolute error between predicted and actual values for MAE calculation.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_124\n\nLANGUAGE: python\nCODE:\n```\ndef abs_error(actual, pred):\n    return np.abs(pred - actual)\n```\n\n----------------------------------------\n\nTITLE: Starting IPython Notebook for Image Visualization\nDESCRIPTION: Command to launch IPython Notebook for visualizing the image data. Python with matplotlib provides better visualization capabilities than Scala for displaying images.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_169\n\nLANGUAGE: shell\nCODE:\n```\nipython notebook\n```\n\n----------------------------------------\n\nTITLE: Launching Spark Shell with Remote Cluster Connection\nDESCRIPTION: Command to start an interactive Scala shell that connects to a remote Spark cluster, allowing for experimentation and interactive analysis.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\n> ./bin/spark-shell --master spark://ec2-54-227-127-14.compute-1.amazonaws.com:7077\n```\n\n----------------------------------------\n\nTITLE: Using IN() Instead of Multiple OR Conditions in SQL\nDESCRIPTION: Shows how to use the IN() predicate to replace multiple OR conditions for more concise and readable code. The IN() predicate is shorthand for a list of OR-ed predicates and is generally more maintainable.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_43\n\nLANGUAGE: sql\nCODE:\n```\ncol_1 = 1 OR col_1 = 2 OR col_1 = 3\n```\n\nLANGUAGE: sql\nCODE:\n```\ncol_1 IN (1, 2, 3)\n```\n\n----------------------------------------\n\nTITLE: Non-Word Character Tokenization in Scala\nDESCRIPTION: Improves tokenization by splitting on non-word characters using regular expressions, reducing the number of unique tokens.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_200\n\nLANGUAGE: scala\nCODE:\n```\nval nonWordSplit = text.flatMap(t => t.split(\"\"\"\\W+\"\"\").map(_.toLowerCase))\nprintln(nonWordSplit.distinct.count)\n```\n\n----------------------------------------\n\nTITLE: Creating a UNION VIEW for Personnel Assignments\nDESCRIPTION: Building a unified view of personnel assignments by combining manager and non-manager tables, allowing users to access all assignments through a single interface.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_55\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE VIEW Personnel_Assignments AS\nSELECT personnel_nbr, store_nbr, job_type, date_assigned\nFROM NonManager_Assignments\nUNION ALL\nSELECT personnel_nbr, store_nbr, job_type, date_assigned\nFROM Manager_Assignments;\n```\n\n----------------------------------------\n\nTITLE: Viewing Spark Master Connection and Execution Logs\nDESCRIPTION: Log output showing the connection process between a Spark client application and a master node, including executor allocation and job initialization.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_23\n\nLANGUAGE: log\nCODE:\n```\n...\n14/01/30 20:26:17 INFO client.Client$ClientActor: Connecting to master spark://ec2-54-220-189-136.eu-west-1.compute.amazonaws.com:7077\n14/01/30 20:26:17 INFO cluster.SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20140130202617-0001\n14/01/30 20:26:17 INFO client.Client$ClientActor: Executor added: app-20140130202617-0001/0 on worker-20140130201049-ip-10-34-137-45.eu-west-1.compute.internal-57119 (ip-10-34-137-45.eu-west-1.compute.internal:57119) with 1 cores\n14/01/30 20:26:17 INFO cluster.SparkDeploySchedulerBackend: Granted executor ID app-20140130202617-0001/0 on hostPort ip-10-34-137-45.eu-west-1.compute.internal:57119 with 1 cores, 2.4 GB RAM\n14/01/30 20:26:17 INFO client.Client$ClientActor: Executor updated: app-20140130202617-0001/0 is now RUNNING\n14/01/30 20:26:18 INFO spark.SparkContext: Starting job: reduce at SparkPi.scala:39\n...\n```\n\n----------------------------------------\n\nTITLE: Implementing Genetic Programming for Rastrigin Function Approximation in R\nDESCRIPTION: This code snippet sets up the genetic programming environment, defines the Rastrigin function and evaluation wrapper, and runs the genetic programming algorithm to approximate the Rastrigin function. It uses the rgp package and sets various parameters for the genetic programming run.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_56\n\nLANGUAGE: R\nCODE:\n```\n### gp-rastrigin.R ###\n\nlibrary(rgp) # load rgp\n\n# auxiliary functions:\nrastrigin=function(x) 10*length(x)+sum(x^2-10*cos(2*pi*x))\nfwrapper=function(x,f) f(x[1],x[2])\n```\n\n----------------------------------------\n\nTITLE: Implementing Genetic Programming for Rastrigin Function Approximation in R\nDESCRIPTION: This code snippet sets up the genetic programming environment, defines the Rastrigin function and evaluation wrapper, and runs the genetic programming algorithm to approximate the Rastrigin function. It uses the rgp package and sets various parameters for the genetic programming run.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_56\n\nLANGUAGE: R\nCODE:\n```\n### gp-rastrigin.R ###\n\nlibrary(rgp) # load rgp\n\n# auxiliary functions:\nrastrigin=function(x) 10*length(x)+sum(x^2-10*cos(2*pi*x))\nfwrapper=function(x,f) f(x[1],x[2])\n```\n\n----------------------------------------\n\nTITLE: Applying Tokenization to Spark RDD\nDESCRIPTION: This code applies the custom tokenization function to each document in the RDD and displays the first 20 tokens of the first document.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_208\n\nLANGUAGE: scala\nCODE:\n```\nval tokens = text.map(doc => tokenize(doc))\nprintln(tokens.first.take(20))\n```\n\n----------------------------------------\n\nTITLE: Initializing KAG Project\nDESCRIPTION: Command to restore and initialize the KAG project using knext CLI, connecting to the OpenSPG server.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nknext project restore --host_addr http://127.0.0.1:8887 --proj_path .\n```\n\n----------------------------------------\n\nTITLE: Running NSGA-II Multi-Objective Optimization for SVM Parameters in R\nDESCRIPTION: Executes the NSGA-II genetic algorithm to find optimal hyperparameters for the SVM model, optimizing for AUC across three wine quality classes. Includes timing of the optimization process.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_126\n\nLANGUAGE: R\nCODE:\n```\ncat(\"NSGAII optimization:\\n\")\n\nm=3 # four objectives: AUC for each class and number of features\nlower=c(-15,-5)\nupper=c(3,15)\nPTM=proc.time() # start clock\nG=nsga2(fn=eval,idim=length(lower),odim=m,lower.bounds=lower,upper.bounds=upper,popsize=12,generations=10)\nsec=(proc.time()-PTM)[3] # get seconds elapsed\ncat(\"time elapsed:\",sec,\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Creating Training and Testing Sets for Cross-Validation\nDESCRIPTION: Creates training and testing datasets from the original RDD by using zipWithIndex, sample, and subtractByKey methods to achieve a 80/20 split.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_135\n\nLANGUAGE: python\nCODE:\n```\ndata_with_idx = data.zipWithIndex().map(lambda (k, v): (v, k)) \ntest = data_with_idx.sample(False, 0.2, 42)\ntrain = data_with_idx.subtractByKey(test)\n```\n\n----------------------------------------\n\nTITLE: Loading User Data into Spark RDD in Python\nDESCRIPTION: PySpark code to load the u.user file into a Spark RDD and display the first line of data.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nuser_data = sc.textFile(\"/PATH/ml-100k/u.user\")\nuser_data.first()\n```\n\n----------------------------------------\n\nTITLE: Computing Metrics for Decision Tree Model\nDESCRIPTION: Separate implementation for Decision Tree model evaluation since it doesn't implement the ClassificationModel interface. Includes combining and printing metrics for all models.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_99\n\nLANGUAGE: scala\nCODE:\n```\nval dtMetrics = Seq(dtModel).map{ model =>\n  val scoreAndLabels = data.map { point =>\n    val score = model.predict(point.features)\n    (if (score > 0.5) 1.0 else 0.0, point.label)\n  }\n  val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n  (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n}\nval allMetrics = metrics ++ nbMetrics ++ dtMetrics\nallMetrics.foreach{ case (m, pr, roc) => \n  println(f\"$m, Area under PR: ${pr * 100.0}%2.4f%%, Area under ROC: ${roc * 100.0}%2.4f%%\") \n}\n```\n\n----------------------------------------\n\nTITLE: Data Preprocessing Shell Command\nDESCRIPTION: Shell command to remove the header from a TSV file and create a new file without headers\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_88\n\nLANGUAGE: shell\nCODE:\n```\nsed 1d train.tsv > train_noheader.tsv\n```\n\n----------------------------------------\n\nTITLE: Analyzing Event Impact on Companies\nDESCRIPTION: GQL query to analyze how product chain events lead to company events, showing the causal relationships.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nknext reasoner execute --dsl \"\nMATCH\n    (s:SupplyChain.ProductChainEvent)-[:leadTo]->(o:SupplyChain.CompanyEvent)\nRETURN\n    s.id, s.subject, o.subject, o.name\n\"\n```\n\n----------------------------------------\n\nTITLE: Starting Spark Shell with Memory Configuration\nDESCRIPTION: Command to start the Spark shell with 4GB of driver memory allocation, which is needed for processing the MovieLens dataset.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_51\n\nLANGUAGE: bash\nCODE:\n```\n./bin/spark-shell -driver-memory 4g\n```\n\n----------------------------------------\n\nTITLE: Unzipping MovieLens Dataset in Bash\nDESCRIPTION: Command to unzip the downloaded MovieLens 100k dataset, creating a directory with various data files.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\n> unzip ml-100k.zip\n```\n\n----------------------------------------\n\nTITLE: Navigating to SupplyChain Example Directory\nDESCRIPTION: Command to change directory to the SupplyChain example folder within the KAG project.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd kag/examples/supplychain\n```\n\n----------------------------------------\n\nTITLE: Running QA and DSL Tasks\nDESCRIPTION: Command to execute the Python script that handles question answering and DSL tasks on the knowledge graph.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README_cn.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython ./solver/qa.py\n```\n\n----------------------------------------\n\nTITLE: Initializing the KAG Project\nDESCRIPTION: Command to restore/initialize the project using the knext CLI tool, connecting to a local OpenSPG server.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README_cn.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nknext project restore --host_addr http://127.0.0.1:8887 --proj_path .\n```\n\n----------------------------------------\n\nTITLE: Generating PDF Visualizations of TSP Tours in R\nDESCRIPTION: Creates PDF visualizations for three different TSP solutions (2-opt, evolutionary algorithm, and Lamarckian evolutionary algorithm). Uses base R plotting functions with custom margins and line plotting.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_91\n\nLANGUAGE: R\nCODE:\n```\npdf(\"qa194-2-opt.pdf\",paper=\"special\")\npar(mar=c(0.0,0.0,0.0,0.0))\nplot(Data[c(R1[1:N],R1[1]),],type=\"l\",xaxt=\"n\",yaxt=\"n\")\ndev.off()\n\npdf(\"qa194-ea.pdf\",paper=\"special\")\npar(mar=c(0.0,0.0,0.0,0.0))\nb=OEA$population[which.min(OEA$evaluations),]\nplot(Data[c(b,b[1]),],type=\"l\",xaxt=\"n\",yaxt=\"n\")\ndev.off()\n\npdf(\"qa194-lea.pdf\",paper=\"special\")\npar(mar=c(0.0,0.0,0.0,0.0))\nb=LEA$population[which.min(LEA$evaluations),]\nplot(Data[c(b,b[1]),],type=\"l\",xaxt=\"n\",yaxt=\"n\")\ndev.off()\n```\n\n----------------------------------------\n\nTITLE: Importing Rating Class for Recommendation Data\nDESCRIPTION: Imports the Rating class from Spark MLlib, which is required to structure the input data for the ALS algorithm. Each Rating object encapsulates a user ID, movie ID, and rating value.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_55\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.recommendation.Rating\n```\n\n----------------------------------------\n\nTITLE: Examining Rating Data File Contents in Bash\nDESCRIPTION: Using the head command to view the first 5 lines of the u.data file, which contains user ratings for movies.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\n> head -5 u.data\n```\n\n----------------------------------------\n\nTITLE: Transforming Timestamps to Hour of Day in PySpark\nDESCRIPTION: This PySpark code extracts the hour of day from rating timestamps. It uses map transformations to convert timestamps to integers and then to datetime objects.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_38\n\nLANGUAGE: Python\nCODE:\n```\ntimestamps = rating_data.map(lambda fields: int(fields[3]))\nhour_of_day = timestamps.map(lambda ts: extract_datetime(ts).hour)\nhour_of_day.take(5)\n```\n\n----------------------------------------\n\nTITLE: Creating a Reservation View for User Limits in SQL\nDESCRIPTION: A view definition that counts the number of items reserved by each user. This can be used for reporting and validation instead of storing the count in a separate field.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_90\n\nLANGUAGE: sql\nCODE:\n```\nCREATE VIEW UserReservations (user_id, items_reserved) AS\n  SELECT user_id, COUNT(*) AS items_reserved \n  FROM Reservations\n  GROUP BY user_id;\n```\n\n----------------------------------------\n\nTITLE: Procedural Loop to Find Missing Weekday in SQL/PSM\nDESCRIPTION: A poorly designed procedural approach that uses a loop to find the earliest missing day number within a week. This mimics 3GL programming approaches and doesn't leverage SQL's set-based processing capabilities.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_84\n\nLANGUAGE: SQL/PSM\nCODE:\n```\nCREATE FUNCTION Missing_Weekday (IN p_week_nbr INTEGER)\nRETURNS INTEGER\nBEGIN\n  DECLARE v_weekday INTEGER;\n  DECLARE v_day_nbr INTEGER;\n  SET v_weekday = 1;\n\n  Loop_Label:LOOP\n    SELECT day_nbr\n    INTO v_day_nbr\n    FROM Calendar_Table\n    WHERE week_nbr = p_week_nbr\n    AND day_nbr = v_weekday;\n\n    IF (v_day_nbr IS NULL) THEN\n      RETURN v_weekday;\n    ELSE\n      SET v_weekday = v_weekday + 1;\n      IF v_weekday > 7 THEN\n        LEAVE Loop_Label;\n      END IF;\n    END IF;\n  END LOOP Loop_Label;\nEND;\n```\n\n----------------------------------------\n\nTITLE: Training ALS Model with Spark MLlib\nDESCRIPTION: Trains an ALS recommendation model with rank 50, 10 iterations, and lambda 0.01 on the MovieLens ratings dataset.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_57\n\nLANGUAGE: scala\nCODE:\n```\nval model = ALS.train(ratings, 50, 10, 0.01)\n```\n\n----------------------------------------\n\nTITLE: Initializing Spark Context and Data Processing in Scala\nDESCRIPTION: Main program setup including SparkContext initialization and CSV data processing into structured format\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\n\nobject ScalaApp {\n  def main(args: Array[String]) {\n    val sc = new SparkContext(\"local[2]\", \"First Spark App\")\n    val data = sc.textFile(\"data/UserPurchaseHistory.csv\")\n      .map(line => line.split(\",\"))\n      .map(purchaseRecord => (purchaseRecord(0), purchaseRecord(1), purchaseRecord(2)))\n```\n\n----------------------------------------\n\nTITLE: Training ALS Recommendation Model on MovieLens Data\nDESCRIPTION: Loads user-movie rating data from the MovieLens dataset and trains an Alternating Least Squares recommendation model with 50 latent factors. The model will be used to extract latent feature representations for movies and users.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_153\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.recommendation.ALS\nimport org.apache.spark.mllib.recommendation.Rating\nval rawData = sc.textFile(\"/PATH/ml-100k/u.data\")\nval rawRatings = rawData.map(_.split(\"\\t\").take(3))\nval ratings = rawRatings.map{ case Array(user, movie, rating) => Rating(user.toInt, movie.toInt, rating.toDouble) }\nratings.cache\nval alsModel = ALS.train(ratings, 50, 10, 0.1)\n```\n\n----------------------------------------\n\nTITLE: Executing Graph Queries with GQL (Generic)\nDESCRIPTION: Generic command template for executing queries against the knowledge graph using ISO GQL syntax.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README_cn.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nknext reasoner execute --dsl \"${ql}\"\n```\n\n----------------------------------------\n\nTITLE: Installing and Loading R Package Example\nDESCRIPTION: Demonstrates how to install a package (pso), load it into the environment, and access its help documentation.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_1\n\nLANGUAGE: R\nCODE:\n```\n> install.packages(\"pso\")\n> library(pso)\n> ?pso\n```\n\n----------------------------------------\n\nTITLE: Computing Error Metrics for Linear Model using Spark RDD\nDESCRIPTION: Calculation of MSE, MAE, and RMSLE metrics for a linear model using Spark RDD transformations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_126\n\nLANGUAGE: python\nCODE:\n```\nmse = true_vs_predicted.map(lambda (t, p): squared_error(t, p)).mean()\nmae = true_vs_predicted.map(lambda (t, p): abs_error(t, p)).mean()\nrmsle = np.sqrt(true_vs_predicted.map(lambda (t, p): squared_log_error(t, p)).mean())\nprint \"Linear Model - Mean Squared Error: %2.4f\" % mse\nprint \"Linear Model - Mean Absolute Error: %2.4f\" % mae\nprint \"Linear Model - Root Mean Squared Log Error: %2.4f\" % rmsle\n```\n\n----------------------------------------\n\nTITLE: Loading Movie Data and Creating Title Mapping in Scala with Spark\nDESCRIPTION: This snippet loads movie data from a file, creates a map of movie IDs to titles, and demonstrates how to access a specific movie title.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_61\n\nLANGUAGE: Scala\nCODE:\n```\nval movies = sc.textFile(\"/PATH/ml-100k/u.item\")\nval titles = movies.map(line => line.split(\"\\\\|\").take(2)).map(array => (array(0).toInt,array(1))).collectAsMap()\ntitles(123)\n```\n\n----------------------------------------\n\nTITLE: Implementing Lexicographic Comparison for Multi-Objective Optimization in R\nDESCRIPTION: This code implements a lexicographic comparison function for multi-objective optimization. It compares solutions based on multiple objectives in order of priority.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_65\n\nLANGUAGE: R\nCODE:\n```\n# lexicographic comparison of several solutions:\n# x - is a matrix with several objectives at each column\n# and each row is related with a solution\nlexibest=function(x) # assumes LEXI is defined\n{\n  size=nrow(x); m=ncol(x)\n  candidates=1:size\n  stop=FALSE; i=1\n  while(!stop)\n  {\n    F=x[candidates,i] # i-th goal\n    minFID=which.min(F) # minimization goal is assumed\n    minF=F[minFID]\n    \n    # code continues...\n```\n\n----------------------------------------\n\nTITLE: Querying Industry Affiliations\nDESCRIPTION: GQL query to retrieve the industry affiliations of companies in the supply chain knowledge graph.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README_cn.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nknext reasoner execute --dsl \"\nMATCH\n    (s:SupplyChain.Company)-[:belongToIndustry]->(o:SupplyChain.Industry)\nRETURN\n    s.name, o.name\n\"\n```\n\n----------------------------------------\n\nTITLE: Testing Image Loading Function in Scala\nDESCRIPTION: Code to test the custom image loading function by loading a specific facial image and displaying its metadata in the Spark shell.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_173\n\nLANGUAGE: scala\nCODE:\n```\nval aePath = \"/PATH/lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg\"\nval aeImage = loadImageFromFile(aePath)\n```\n\n----------------------------------------\n\nTITLE: Caching RDDs in Memory\nDESCRIPTION: Demonstrates how to cache an RDD in memory for faster subsequent access using the cache method.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nrddFromTextFile.cache\n```\n\n----------------------------------------\n\nTITLE: Defining Recursive Functions in PCF\nDESCRIPTION: Shows how to define a recursive function (factorial) in PCF using the fix construct for fixed points.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_7\n\nLANGUAGE: PCF\nCODE:\n```\nfix f fun n -> ifz n then 1 else n * (f (n - 1))\n```\n\n----------------------------------------\n\nTITLE: Visualizing Results with Plot Generation in R\nDESCRIPTION: Creates a visualization comparing the original Rastrigin function with the genetic programming approximation, saving the result to a PDF file.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_60\n\nLANGUAGE: R\nCODE:\n```\nb=gp$population[[which.min(gp$fitnessValues)]]\ncat(\"best solution (f=\",eval(b),\"):\\n\")\nprint(b)\n\nL1=apply(domain,1,rastrigin);L2=apply(domain,1,fwrapper,b)\nMIN=min(L1,L2);MAX=max(L1,L2)\n\npdf(\"gp-function.pdf\",width=7,height=7,paper=\"special\")\nplot(L1,ylim=c(MIN,MAX),type=\"l\",lwd=2,lty=1,\nxlab=\"points\",ylab=\"function values\")\nlines(L2,type=\"l\",lwd=2,lty=2)\nlegend(\"bottomright\",leg=c(\"rastrigin\",\"GP function\"),lwd=2,\nlty=1:2)\ndev.off()\n```\n\n----------------------------------------\n\nTITLE: Committing Schema to the Knowledge Graph\nDESCRIPTION: Command to commit the predefined schema files to the OpenSPG server.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README_cn.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nknext schema commit\n```\n\n----------------------------------------\n\nTITLE: Using MLlib's RegressionMetrics for Evaluation in Scala\nDESCRIPTION: Demonstrates how to use Spark MLlib's built-in RegressionMetrics class to compute Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) for recommendation predictions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_80\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.evaluation.RegressionMetrics\nval predictedAndTrue = ratingsAndPredictions.map { case ((user, product), (predicted, actual)) => (predicted, actual) }\nval regressionMetrics = new RegressionMetrics(predictedAndTrue)\n\nprintln(\"Mean Squared Error = \" + regressionMetrics.meanSquaredError)\nprintln(\"Root Mean Squared Error = \" + regressionMetrics.rootMeanSquaredError)\n```\n\n----------------------------------------\n\nTITLE: Analyzing TF-IDF Weight Range Across Corpus in Spark\nDESCRIPTION: Calculates the minimum and maximum TF-IDF weights across the entire corpus using map and reduce operations to understand the range of weights assigned to terms.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_212\n\nLANGUAGE: scala\nCODE:\n```\nval minMaxVals = tfidf.map { v =>\n  val sv = v.asInstanceOf[SV]\n  (sv.values.min, sv.values.max)\n}\nval globalMinMax = minMaxVals.reduce { case ((min1, max1), (min2, max2)) =>\n  (math.min(min1, min2), math.max(max1, max2))\n}\nprintln(globalMinMax)\n```\n\n----------------------------------------\n\nTITLE: Creating Broadcast Variable in Spark Scala\nDESCRIPTION: Demonstrates how to create and use a broadcast variable in Spark to share read-only data across nodes\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nval broadcastAList = sc.broadcast(List(\"a\", \"b\", \"c\", \"d\", \"e\"))\n```\n\n----------------------------------------\n\nTITLE: Testing the Term Dictionary\nDESCRIPTION: Tests the term dictionary by printing the total number of unique terms and checking the indices of specific terms. This validates that the term-to-index mapping is working correctly.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nprint \"Total number of terms: %d\" % len(all_terms_dict)\nprint \"Index of term 'Dead': %d\" % all_terms_dict['Dead']\nprint \"Index of term 'Rooms': %d\" % all_terms_dict['Rooms']\n```\n\n----------------------------------------\n\nTITLE: Plotting Singular Values in Python\nDESCRIPTION: Loads singular values from a CSV file and plots them using matplotlib to visualize the distribution of singular values. This helps in determining an appropriate k value for dimensionality reduction.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_192\n\nLANGUAGE: python\nCODE:\n```\ns = np.loadtxt(\"/tmp/s.csv\", delimiter=\",\")\nprint(s.shape)\nplot(s)\n```\n\n----------------------------------------\n\nTITLE: Running 2-Opt Algorithm on TSP in R\nDESCRIPTION: Implementation of the 2-opt heuristic for solving the TSP problem. The code measures execution time and displays the solution quality.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_83\n\nLANGUAGE: R\nCODE:\n```\nset.seed(12345) # for replicability\n\ncat(\"2-opt run:\\n\")\nPTM=proc.time() # start clock\nR1=solve_TSP(TD,method=\"2-opt\")\nsec=(proc.time()-PTM)[3] # get seconds elapsed\nprint(R1) # show optimum\ncat(\"time elapsed:\",sec,\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Processing Image in Scala\nDESCRIPTION: This function converts an image to grayscale and resizes it to the specified dimensions using Java's AWT library.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_174\n\nLANGUAGE: Scala\nCODE:\n```\ndef processImage(image: BufferedImage, width: Int, height: Int): BufferedImage = {\n  val bwImage = new BufferedImage(width, height, BufferedImage.TYPE_BYTE_GRAY)\n  val g = bwImage.getGraphics()\n  g.drawImage(image, 0, 0, width, height, null)\n  g.dispose()\n  bwImage\n}\n```\n\n----------------------------------------\n\nTITLE: Saving PCA Results to CSV in Scala\nDESCRIPTION: This snippet saves the principal components matrix to a CSV file using Breeze's csvwrite function.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_183\n\nLANGUAGE: scala\nCODE:\n```\nimport breeze.linalg.csvwrite\ncsvwrite(new File(\"/tmp/pc.csv\"), pcBreeze)\n```\n\n----------------------------------------\n\nTITLE: Counting Image Files in Dataset\nDESCRIPTION: Simple command to count the total number of image files in the dataset. This helps understand the size of the dataset being processed.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_168\n\nLANGUAGE: scala\nCODE:\n```\nprintln(files.count)\n```\n\n----------------------------------------\n\nTITLE: Improved schema design avoiding attribute splitting\nDESCRIPTION: Better table design that avoids attribute splitting by using separate columns for different statistics, making queries simpler and more efficient.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_21\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE Game_Stats\n(\n  league_id CHAR(3) NOT NULL,\n  player_nbr INTEGER NOT NULL,\n  game_id INTEGER NOT NULL,\n  yardage INTEGER NULL,\n  completions INTEGER NULL,\n  interceptions INTEGER NULL,\n  ...\n  CONSTRAINT PK_Game_Stats\n  PRIMARY KEY (league_id, player_nbr, game_id)\n);\n```\n\n----------------------------------------\n\nTITLE: Navigating to the example directory in bash\nDESCRIPTION: Command to change to the csqa example directory where the KAG implementation for CSQA is located.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd kag/examples/csqa\n```\n\n----------------------------------------\n\nTITLE: Computing Hockey Document TF-IDF Features in Scala\nDESCRIPTION: Filters hockey-related documents from the dataset and computes their TF-IDF features using Spark's HashingTF and IDF transformers\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_215\n\nLANGUAGE: scala\nCODE:\n```\nval hockeyText = rdd.filter { case (file, text) => file.contains(\"hockey\") }\nval hockeyTF = hockeyText.mapValues(doc => hashingTF.transform(tokenize(doc)))\nval hockeyTfIdf = idf.transform(hockeyTF.map(_._2))\n```\n\n----------------------------------------\n\nTITLE: Evaluating Intercept in Spark Linear Regression\nDESCRIPTION: This code evaluates the impact of adding an intercept term to a linear regression model using Spark MLlib. It computes the RMSLE metric with and without an intercept and plots the results.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_145\n\nLANGUAGE: Python\nCODE:\n```\nparams = [False, True]\nmetrics = [evaluate(train_data, test_data, 10, 0.1, 1.0, 'l2', param) for param in params]\nprint params\nprint metrics\nbar(params, metrics, color='lightblue')\nfig = matplotlib.pyplot.gcf()\n```\n\n----------------------------------------\n\nTITLE: Summarizing Wine Quality Dataset in R\nDESCRIPTION: Displays a summary of the wine quality dataset including class distribution. Uses only 10% of the samples for the summary statistics.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_123\n\nLANGUAGE: R\nCODE:\n```\nprint(summary(d[ALL,]))\n\ncat(\"output class distribuition (10% samples):\\n\")\n\nprint(table(d[ALL,]$quality)) # show distribution of classes\n```\n\n----------------------------------------\n\nTITLE: Vector Manipulation in R\nDESCRIPTION: Creates and manipulates vectors using rep() and sequence operations. Demonstrates basic vector operations and value assignments.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_107\n\nLANGUAGE: R\nCODE:\n```\nv=rep(0,10) # same as: v=vector(length=10);v[]=0\nv[c(3,7,9)]=1 # update values\nprint(v) # show v\n\nv=seq(2,50,by=2) # one way\nprint(v)\nv=(1:25)*2 # other way\nprint(v)\n```\n\n----------------------------------------\n\nTITLE: Comparing New Simulated Annealing and Evolutionary Algorithm for TSP in R\nDESCRIPTION: This code compares a new simulated annealing implementation with a new evolutionary algorithm for solving the traveling salesman problem. It implements random mutation and crossover operators, runs both algorithms, and compares their performance.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_120\n\nLANGUAGE: R\nCODE:\n```\n# this solution assumes that file \"tsp.R\" has already been executed\n\nsource(\"oea.R\") # load ordered evolutionary algorithm\n\nsource(\"s7-1.R\") # get the cycle operator\n\n# random mutation\n\nrandomm=function(s)\n\n{ return(switch(sample(1:3,1),exchange(s),insertion(s),displacement(s))) }\n\n# random crossover\n\nrandomx=function(m)\n\n{ return(switch(sample(1:3,1),pmx(m),ox(m),cx(m))) }\n\nMethods=c(\"new SANN\",\"new EA\")\n\n# new SANN:\n\ncat(\"new SANN run:\\n\")\n\nset.seed(12345) # for replicability\n\ns=sample(1:N,N) # initial solution\n\nEV=0; BEST=Inf; F=rep(NA,MAXIT) # reset these vars.\n\nC=list(maxit=MAXIT,temp=2000,trace=TRUE,REPORT=MAXIT)\n\nPTM=proc.time() # start clock\n\nSANN=optim(s,fn=tour,gr=randomm,method=\"SANN\",control=C)\n\nsec=(proc.time()-PTM)[3] # get seconds elapsed\n\ncat(\"time elapsed:\",sec,\"\\n\")\n\nRES[,1]=F\n\ncat(\"tour distance:\",tour(SANN$par),\"\\n\")\n\n# new EA:\n\ncat(\"new EA run:\\n\")\n\nset.seed(12345) # for replicability\n\nEV=0; BEST=Inf; F=rep(NA,MAXIT) # reset these vars.\n\npSize=30;iters=ceiling((MAXIT-pSize)/(pSize-1))\n\nPTM=proc.time() # start clock\n\nOEA=oea(size=N,popSize=pSize,iters=iters,evalFunc=tour,crossfunc=randomx,mutfunc=randomm,REPORT=iters,elitism=1)\n\nsec=(proc.time()-PTM)[3] # get seconds elapsed\n\ncat(\"time elapsed:\",sec,\"\\n\")\n\nRES[,2]=F\n\ncat(\"tour distance:\",tour(OEA$population[which.min(OEA$evaluations),]),\"\\n\")\n\n# there is no improvement when compared with \"tsp.R\" file\n```\n\n----------------------------------------\n\nTITLE: Initializing JavaSparkContext and Processing CSV Data in Java\nDESCRIPTION: This snippet demonstrates how to initialize a JavaSparkContext and read a CSV file, splitting each row into fields using a custom Function. It shows the Java approach to creating and using RDDs in Spark.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\nJavaSparkContext sc = new JavaSparkContext(\"local[2]\", \"First Spark App\");\n// we take the raw data in CSV format and convert it into a set of records of the form (user, product, price)\nJavaRDD<String[]> data = sc.textFile(\"data/UserPurchaseHistory.csv\")\n.map(new Function <String, String[]>() {\n@Override\npublic String[] call(String s) throws Exception {\nreturn s.split(\",\");\n}\n});\n```\n\n----------------------------------------\n\nTITLE: Defining a Function Using Fun Notation\nDESCRIPTION: Shows how to define the same function using the 'fun' keyword instead of lambda notation, which is the notation used throughout the book.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_1\n\nLANGUAGE: pseudo-code\nCODE:\n```\nfun x -> sin (cos (sin x))\n```\n\n----------------------------------------\n\nTITLE: Implementing Full Search Function in R\nDESCRIPTION: A function that applies a user-defined evaluation function to each row of a search space matrix and returns the best solution based on specified type (min/max).\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_16\n\nLANGUAGE: r\nCODE:\n```\nfsearch=function(search,FUN,type=\"min\",...)\n{\nx=apply(search,1,FUN,...) # run FUN over all search rows\nib=switch(type,min=which.min(x),max=which.max(x))\nreturn(list(index=ib,sol=search[ib,],eval=x[ib]))\n}\n```\n\n----------------------------------------\n\nTITLE: Navigating to the Project Directory in Bash\nDESCRIPTION: Command to navigate to the supply chain example directory within the KAG project structure.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README_cn.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd kag/examples/supplychain\n```\n\n----------------------------------------\n\nTITLE: Integer Task Results from Multi-Objective Optimization in R\nDESCRIPTION: This section displays the results of applying multi-objective optimization to an integer task. Each line shows a solution vector and its corresponding objective function values, representing points in the Pareto front.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_75\n\nLANGUAGE: R\nCODE:\n```\ninteger task:\n\n414 403 406 431 394 f=( 43736 , 114 )\n\n1000 996 993 989 988 f=( -500 , 0 )\n\n752 944 929 871 999 f=( 12944 , 17 )\n\n813 649 872 971 791 f=( 19729 , 28 )\n\n1000 934 979 942 996 f=( 3204 , 4 )\n\n803 967 645 627 745 f=( 22523 , 34 )\n\n414 403 406 503 473 f=( 42955 , 107 )\n\n554 629 591 443 563 f=( 38665 , 74 )\n\n775 721 510 621 782 f=( 30643 , 50 )\n\n436 494 494 614 565 f=( 40789 , 87 )\n\n900 934 979 942 996 f=( 6684 , 8 )\n\n807 498 506 641 707 f=( 32477 , 59 )\n\n790 749 595 877 789 f=( 25273 , 37 )\n\n979 882 957 938 794 f=( 8873 , 11 )\n\n997 787 634 985 728 f=( 15792 , 24 )\n\n775 634 725 602 782 f=( 29307 , 45 )\n\n997 788 647 991 728 f=( 15328 , 23 )\n\n432 494 494 433 563 f=( 42191 , 95 )\n\n620 654 680 393 608 f=( 36061 , 67 )\n\n946 672 668 622 638 f=( 26670 , 42 )\n```\n\n----------------------------------------\n\nTITLE: Displaying Processed Image in Python\nDESCRIPTION: This code snippet reads a processed image file and displays it using matplotlib in Python.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_177\n\nLANGUAGE: Python\nCODE:\n```\ntmpPath = \"/tmp/aeGray.jpg\"\naeGary = imread(tmpPath)\nimshow(aeGary, cmap=plt.cm.gray)\n```\n\n----------------------------------------\n\nTITLE: Typed PCF Syntax Definition\nDESCRIPTION: The formal definition of PCF with types, specifying the base type 'nat' and function types using the arrow notation. This typed version requires annotating functions with the types of their arguments.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_23\n\nLANGUAGE: PCF\nCODE:\n```\n  * nat—that is, ℕ—is a type,\n\n  * if A and B are types then A -> B—that is, the set of all the functions from A to B—is a type.\n```\n\n----------------------------------------\n\nTITLE: Real Value Task Results from Multi-Objective Optimization in R\nDESCRIPTION: This section shows the results of applying multi-objective optimization to a real value task. Each line presents a solution vector with real values and the corresponding objective function values, representing different trade-offs in the Pareto front.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_76\n\nLANGUAGE: R\nCODE:\n```\nreal value task:\n\n0.15 0.12 0.12 0.11 0.16 0.15 0.16 0.15 f=(4.85,0)\n\n0.34 0.35 0.38 0.43 0.49 0.59 0.72 0.91 f=(0.45,1.44)\n\n0.34 0.35 0.38 0.43 0.49 0.59 0.35 0.5 f=(1.58,0.7)\n\n0.34 0.35 0.39 0.25 0.2 0.13 0.14 0.13 f=(3.46,0.15)\n\n0.22 0.18 0.39 0.27 0.2 0.12 0.13 0.13 f=(4.11,0.09)\n\n0.34 0.35 0.39 0.43 0.3 0.59 0.34 0.46 f=(2.02,0.57)\n\n0.34 0.35 0.38 0.43 0.49 0.5 0.72 0.91 f=(0.67,1.37)\n\n0.34 0.35 0.38 0.43 0.49 0.59 0.72 0.72 f=(0.82,1.19)\n\n0.19 0.36 0.12 0.11 0.16 0.19 0.16 0.15 f=(4.36,0.05)\n\n0.34 0.35 0.39 0.43 0.2 0.13 0.07 0.14 f=(3.13,0.23)\n\n0.34 0.35 0.38 0.42 0.49 0.59 0.72 0.67 f=(0.92,1.13)\n\n0.34 0.35 0.39 0.41 0.24 0.59 0.23 0.48 f=(2.24,0.53)\n\n0.34 0.35 0.39 0.43 0.11 0.35 0.45 0.48 f=(2.5,0.46)\n\n0.34 0.35 0.38 0.43 0.49 0.59 0.72 0.5 f=(1.03,0.98)\n\n0.33 0.35 0.39 0.43 0.39 0.4 0.13 0.33 f=(2.56,0.37)\n\n0.33 0.35 0.39 0.43 0.3 0.4 0.13 0.3 f=(2.7,0.33)\n\n0.22 0.35 0.39 0.42 0.48 0.12 0.13 0.16 f=(3.01,0.29)\n\n0.29 0.35 0.38 0.43 0.49 0.59 0.72 0.49 f=(1.22,0.96)\n\n0.34 0.35 0.38 0.43 0.47 0.59 0.69 0.43 f=(1.29,0.89)\n\n0.34 0.35 0.38 0.42 0.5 0.58 0.72 0.16 f=(1.33,0.85)\n```\n\n----------------------------------------\n\nTITLE: Record Field Update Operation in PCF\nDESCRIPTION: Shows the syntax for updating a field in an existing record, creating a new record with the modified field value while preserving other fields.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_37\n\nLANGUAGE: PCF\nCODE:\n```\nt(l <- u)\n```\n\n----------------------------------------\n\nTITLE: Computing and Saving Top 300 Singular Values in Scala\nDESCRIPTION: Computes the SVD for k=300 on a matrix and writes the singular values to a CSV file for further analysis. This code demonstrates how to extract and save SVD components.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_191\n\nLANGUAGE: scala\nCODE:\n```\nval svd300 = matrix.computeSVD(300, computeU = false)\nval sMatrix = new DenseMatrix(1, 300, svd300.s.toArray)\ncsvwrite(new File(\"/tmp/s.csv\"), sMatrix)\n```\n\n----------------------------------------\n\nTITLE: Querying a View for Qualified Pilots\nDESCRIPTION: This SQL snippet demonstrates a simple query to retrieve pilots from a view named 'QualifiedPilots'. It illustrates how complex logic can be hidden behind a simple view.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_49\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT pilot FROM QualifiedPilots;\n```\n\n----------------------------------------\n\nTITLE: Fixed Point Rule in PCF\nDESCRIPTION: Defines the rule for fixed points in PCF's small-step operational semantics.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_13\n\nLANGUAGE: PCF\nCODE:\n```\nfix f t ⟶ t[f := fix f t]\n```\n\n----------------------------------------\n\nTITLE: Cluster Termination Confirmation Output\nDESCRIPTION: Console output showing the confirmation process when destroying a Spark cluster, including the list of instances that will be terminated.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_27\n\nLANGUAGE: log\nCODE:\n```\nAre you sure you want to destroy the cluster test-cluster?\nThe following instances will be terminated:\nSearching for existing cluster test-cluster...\nFound 1 master(s), 1 slaves\n> ec2-54-227-127-14.compute-1.amazonaws.com\n> ec2-54-91-61-225.compute-1.amazonaws.com\nALL DATA ON ALL NODES WILL BE LOST!!\nDestroy cluster test-cluster (y/N): y\nTerminating master...\nTerminating slaves...\n```\n\n----------------------------------------\n\nTITLE: Three-Valued Logic with IN() and NULL Values in SQL\nDESCRIPTION: Demonstrates how IN() predicates behave with NULL values in SQL's three-valued logic. Shows that NULL in an IN list doesn't match anything due to SQL's NULL handling rules.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_44\n\nLANGUAGE: sql\nCODE:\n```\nx IN (a, b, ..., n)\n```\n\nLANGUAGE: sql\nCODE:\n```\nx = a OR x = b OR ... OR x = n\n```\n\nLANGUAGE: sql\nCODE:\n```\nx IN (a, NULL, c)\n```\n\nLANGUAGE: sql\nCODE:\n```\nx NOT IN (a, b, ..., n)\n```\n\nLANGUAGE: sql\nCODE:\n```\nx <> a AND x <> b AND ... AND x <> n\n```\n\nLANGUAGE: sql\nCODE:\n```\nx NOT IN (a, NULL, c)\n```\n\n----------------------------------------\n\nTITLE: Verifying Equivalence of PCA and SVD Projections in Scala\nDESCRIPTION: This code compares the projections obtained from PCA and SVD to verify their equivalence, using Breeze for vector operations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_189\n\nLANGUAGE: scala\nCODE:\n```\nval breezeS = breeze.linalg.DenseVector(svd.s.toArray)\nval projectedSVD = svd.U.rows.map { v => \n  val breezeV = breeze.linalg.DenseVector(v.toArray)\n  val multV = breezeV **:*** breezeS\n  Vectors.dense(multV.data)\n}\nprojected.rows.zip(projectedSVD).map { case (v1, v2) => approxEqual(v1.toArray, v2.toArray) }.filter(b => true).count\n```\n\n----------------------------------------\n\nTITLE: Inspecting PCA Results in Scala\nDESCRIPTION: This snippet prints the dimensions of the principal components matrix after PCA has been performed on image data.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_181\n\nLANGUAGE: scala\nCODE:\n```\nval rows = pc.numRows\nval cols = pc.numCols\nprintln(rows, cols)\n```\n\n----------------------------------------\n\nTITLE: Using Let Bindings in PCF\nDESCRIPTION: Demonstrates the use of let bindings in PCF for creating definitions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_8\n\nLANGUAGE: PCF\nCODE:\n```\nlet x = t in u\n```\n\n----------------------------------------\n\nTITLE: Testing Blind Search Functions for Binary Optimization in R\nDESCRIPTION: R script that tests both blind search methods (fsearch and dfsearch) on two binary optimization problems: maximizing sum of bits and maximizing sine function. It includes helper functions for binary-integer conversion.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_18\n\nLANGUAGE: r\nCODE:\n```\n### binary-blind.R file ###\n\nsource(\"blind.R\") # load the blind search methods\n\n# read D bits from integer x:\nbinint=function(x,D)\n{ x=rev(intToBits(x)[1:D]) # get D bits\n# remove extra 0s from raw type:\nas.numeric(unlist(strsplit(as.character(x),\"\"))[(1:D)*2])\n}\n\n# convert binary vector into integer: code inspired in\n# http://stackoverflow.com/questions/12892348/\n# in-r-how-to-convert-binary-string-to-binary-or-decimal-value\nintbin=function(x) sum(2^(which(rev(x==1))-1))\n\n# sum a raw binary object x (evaluation function):\nsumbin=function(x) sum(as.numeric(x))\n\n# max sin of binary raw object x (evaluation function):\nmaxsin=function(x,Dim) sin(pi*(intbin(x))/(2^Dim))\n\nD=8 # number of dimensions\nx=0:(2^D-1) # integer search space\n\n# set full search space in solutions x D:\nsearch=t(sapply(x,binint,D=D))\n\n# set the domain values (D binary variables):\ndomain=vector(\"list\",D)\nfor(i in 1:D) domain[[i]]=c(0,1) # bits\n\n# sum of bits, fsearch:\nS1=fsearch(search,sumbin,\"max\") # full search\ncat(\"fsearch best s:\",S1$sol,\"f:\",S1$eval,\"\\n\")\n\n# sum of bits, dfsearch:\nS2=dfsearch(domain=domain,FUN=sumbin,type=\"max\")\ncat(\"dfsearch best s:\",S2$sol,\"f:\",S2$eval,\"\\n\")\n\n# max sin, fsearch:\nS3=fsearch(search,maxsin,\"max\",Dim=8) # full search\ncat(\"fsearch best s:\",S3$sol,\"f:\",S3$eval,\"\\n\")\n\n# max sin, dfsearch:\nS4=dfsearch(domain=domain,FUN=maxsin,type=\"max\",Dim=8)\ncat(\"dfsearch best s:\",S4$sol,\"f:\",S4$eval,\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Constraint Handling Experiment with EDA in R\nDESCRIPTION: Experiment setup for comparing death penalty and repair constraint handling strategies using Estimation of Distribution Algorithm (EDA). Defines parameters, initializes data structures, and configures the CEDA object with normal copula and distribution parameters.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_53\n\nLANGUAGE: R\nCODE:\n```\nMAXFN=5000\n\nRuns=50; D=5; LP=50; maxit=100\n\nlower=rep(1,D);upper=rep(1000,D)\n\nMethods=c(\"Death\",\"Repair\")\n\nsetMethod(\"edaTerminate\",\"EDA\",edaTerminateMaxGen)\n\nGCEDA=CEDA(copula=\"normal\",margin=\"norm\",popSize=LP,\n\nmaxGen=maxit,fEvalStdDev=10)\n\nGCEDA@name=\"GCEDA\"\n\nRES=vector(\"list\",length(Methods)) # all results\n\nVAL=matrix(nrow=Runs,ncol=length(Methods)) # best values\n\nfor(m in 1:length(Methods)) # initialize RES object\n\nRES[[m]]=matrix(nrow=MAXFN,ncol=Runs)\n```\n\n----------------------------------------\n\nTITLE: Running Spark Example in Cluster Mode on EC2\nDESCRIPTION: Command to run the SparkPi example on a Spark cluster using the cluster's distributed computing resources. This uses the master URL to connect to the cluster.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\n> MASTER=spark://ec2-54-227-127-14.compute-1.amazonaws.com:7077 ./bin/run-example SparkPi \n```\n\n----------------------------------------\n\nTITLE: Full Blind Search Function Definition in R\nDESCRIPTION: Implementation of the fsearch function that performs a full blind search optimization. Takes a matrix of solutions, evaluation function, and optimization type (min/max) as inputs. Searches through all possible solutions in the provided search space matrix.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_15\n\nLANGUAGE: R\nCODE:\n```\n# full bind search method\n# search - matrix with solutions x D\n# FUN - evaluation function\n# type - \"min\" or \"max\"\n```\n\n----------------------------------------\n\nTITLE: Interpreting Function Application in OCaml for PCF\nDESCRIPTION: This snippet shows how function application is interpreted in an OCaml-based PCF interpreter. It uses pattern matching and recursive calls to evaluate the function and argument expressions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_18\n\nLANGUAGE: OCaml\nCODE:\n```\n| App(t,u) ->\n  (match interp env t with\n    | Clos(x,t1,env1) ->\n      let w = interp env u in\n        interp ((x,w)::env1) t1\n    | _ -> failwith \"function expected\")\n```\n\n----------------------------------------\n\nTITLE: PCF Syntax Definition\nDESCRIPTION: Formal syntax definition for the PCF language, showing its core constructs and operators.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_9\n\nLANGUAGE: BNF\nCODE:\n```\nt ::= x | n | fun x -> t | t t | t + t | t - t | t * t | t / t | ifz t then t else t | fix f t | let x = t in t\n```\n\n----------------------------------------\n\nTITLE: Defining Function Composition in PCF\nDESCRIPTION: Demonstrates how to define a function that composes a function with itself in PCF, showcasing functions as first-class objects.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_5\n\nLANGUAGE: PCF\nCODE:\n```\nfun f -> fun x -> f (f x)\n```\n\n----------------------------------------\n\nTITLE: Binary Search Space Visualization in R\nDESCRIPTION: Creates a PDF plot visualizing a binary search space with sine function evaluation and marks the maximum point.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_110\n\nLANGUAGE: R\nCODE:\n```\nDIR=\"\" # change to other directory if needed\npdf(paste(DIR,\"maxsin.pdf\",sep=\"\"),width=5,height=5) # create PDF\nD=8 # number of binary digits, the dimension\nx=0:(2^D-1);y=sin(pi*x/2^D)\nplot(x,y,type=\"l\",ylab=\"evaluation function\",\nxlab=\"search space\",lwd=2)\npmax=c(x[which.max(y)],max(y)) # set the maximum point\npoints(pmax[1],pmax[2],pch=19,lwd=2) # plot the maximum\nlegend(\"topright\",\"optimum\",pch=19,lwd=2) # add a legend\ndev.off() # close the graphical device\n```\n\n----------------------------------------\n\nTITLE: Visualization of TSP Results in R\nDESCRIPTION: Code for creating a PDF plot that compares the performance of the three optimization methods (SANN, EA, and LEA) on the Qatar TSP instance, showing tour distance versus number of evaluations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_90\n\nLANGUAGE: R\nCODE:\n```\n# create PDF with comparison:\npdf(\"qa194-opt.pdf\",paper=\"special\")\npar(mar=c(4.0,4.0,0.1,0.1))\nX=seq(1,MAXIT,length.out=200)\nylim=c(min(RES)-50,max(RES))\nplot(X,RES[X,1],ylim=ylim,type=\"l\",lty=3,lwd=2,xlab=\"evaluations\",ylab=\"tour distance\")\nlines(X,RES[X,2],type=\"l\",lty=2,lwd=2)\nlines(X,RES[X,3],type=\"l\",lty=1,lwd=2)\nlegend(\"topright\",Methods,lwd=2,lty=3:1)\ndev.off()\n```\n\n----------------------------------------\n\nTITLE: Abstract Machine Instruction Sequence Example for Factorial\nDESCRIPTION: An example showing the compilation output for a factorial function. This demonstrates how PCF expressions are translated into the abstract machine language according to the compilation rules.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_22\n\nLANGUAGE: Abstract Machine Code\nCODE:\n```\nPushenv, Mkclos [Search0, Test([Ldi1], [Pushenv, Ldi1, Push, Search0, Sub, Push, Search1, Apply, Popenv, Push, Search0, Mult])], Extend, Pushenv, Ldi6, Push, Search0, Apply, Popenv, Popenv\n```\n\n----------------------------------------\n\nTITLE: Visualizing Pareto Front with 3D Scatterplot in R\nDESCRIPTION: Creates a 3D scatterplot visualization of the Pareto front results, showing the trade-offs between AUC performance for the three wine quality classes (bad, average, good).\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_128\n\nLANGUAGE: R\nCODE:\n```\nPareto=1-G$value[I,] # AUC for each class\nPareto=data.frame(Pareto)\nnames(Pareto)=c(\"AUC bad\",\"AUC average\",\"AUC good\")\n# sort Pareto according to f1:\nS=sort.int(Pareto[,1],index.return=TRUE)\nPareto=Pareto[S$ix,]\n\nlibrary(scatterplot3d) # get scatterplot3d function\nscatterplot3d(Pareto,xlab=\"f1\",ylab=\"f2\",zlab=\"f3\",\npch=16,type=\"b\")\n\n# looking at the Pareto front, the wine expert could\n# select the best model and then measure the performance\n```\n\n----------------------------------------\n\nTITLE: Loading Images with Java AWT in Scala\nDESCRIPTION: Utility function to load image files using Java's AWT library. Returns a BufferedImage instance that contains image data and provides various image processing methods.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_172\n\nLANGUAGE: scala\nCODE:\n```\nimport java.awt.image.BufferedImage\ndef loadImageFromFile(path: String): BufferedImage = { \n  import javax.imageio.ImageIO\n  import java.io.File\n  ImageIO.read(new File(path))\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Binary Multi-Objective Optimization Test with Lexicographic GA in R\nDESCRIPTION: A test script that demonstrates the use of the lexicographic genetic algorithm for a binary multi-objective optimization problem. It sets up an 8-bit optimization task with two objectives and displays the final population ranked by lexicographic comparison.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_69\n\nLANGUAGE: R\nCODE:\n```\n### lg-test.R file ###\n\nsource(\"mo-tasks.R\") # load multi-optimization tasks\n\nsource(\"lg-ga.R\") # load lrgba.bin\n\nset.seed(12345) # set for replicability\n\nLEXI=c(0.2,0.2) # tolerance 20% for each goal\n\ncat(\"tolerance thresholds:\",LEXI,\"\\n\")\n\n# --- binary task:\n\nD=8 # 8 bits\n\n# eval: transform binary objectives into minimization goal\n\n# returns a vector with 2 values, one per objective:\n\neval=function(x) return(c(-sumbin(x),-maxsin(x)))\n\npopSize=12\n\nG=lrbga.bin(size=D,popSize=popSize,iters=100,zeroToOneRatio=1,\n\nevalFunc=eval,elitism=1)\n\nprint(\"Ranking of last population:\")\n\nB=tournament(G$population,eval,k=popSize,n=popSize,m=2)\n\nfor(i in 1:popSize)\n\n{\n\nx=G$population[B[i],]\n\ncat(x,\" f=(\",sumbin(x),\",\",round(maxsin(x),2),\")\",\"\\n\",sep=\"\")\n\n}\n```\n\n----------------------------------------\n\nTITLE: Hindley's Algorithm Type Inference Rules\nDESCRIPTION: Modified inference rules for Hindley's algorithm, which generate both a type and a set of equations to be solved by unification.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_26\n\nLANGUAGE: PCF\nCODE:\n```\ne ⊢ t ↝ A, E\n\nwhere:\nA is the inferred type\nE is the set of generated equations\n```\n\n----------------------------------------\n\nTITLE: Data Analysis with Forest Fires Dataset in R\nDESCRIPTION: Downloads and analyzes forest fires data, including temperature comparisons between months and filtering large fires.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_111\n\nLANGUAGE: R\nCODE:\n```\nlibrary(RCurl)\nfires=getURL(\"http://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv\")\nwrite(fires,file=\"forestfires.csv\")\nfires=read.table(\"forestfires.csv\",header=TRUE,sep=\",\")\naug=fires$temp[fires$month==\"aug\"]\ncat(\"mean temperature in Aug.:\",mean(aug),\"\\n\")\nfeb=fires$temp[fires$month==\"feb\"]\njul=fires$temp[fires$month==\"jul\"]\nsfeb=sample(feb,10)\nsjul=sample(jul,10)\nsaug=sample(aug,10)\np1=t.test(saug,sfeb)$p.value\np2=t.test(saug,sjul)$p.value\np3=t.test(sjul,sfeb)$p.value\ncat(\"p-values (Aug-Feb,Aug-Jul,Jul-Feb):\",\nround(c(p1,p2,p3),digits=2),\"\\n\")\naug100=fires[fires$month==\"aug\"&fires$area>100,]\nprint(aug100)\nwrite.table(aug100,\"aug100.csv\",sep=\",\",row.names=FALSE)\n```\n\n----------------------------------------\n\nTITLE: Initializing Genetic Programming Configuration in R\nDESCRIPTION: Sets up the initial configuration for genetic programming including input variables, constant factory, and function sets. Defines a 2D domain grid for evaluation.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_57\n\nLANGUAGE: R\nCODE:\n```\nST=inputVariableSet(\"x1\",\"x2\")\n\ncF1=constantFactorySet(function() rnorm(1)) # mean=0, sd=1\n\nFS=functionSet(\"+\",\"*\",\"-\")\n\n# set the input samples (grid^2 data points):\ngrid=10 # size of the grid used\ndomain=matrix(ncol=2,nrow=grid^2) # 2D domain grid\ndomain[,1]=rep(seq(-5.2,5.2,length.out=grid),each=grid)\ndomain[,2]=rep(seq(-5.2,5.2,length.out=grid),times=grid)\n```\n\n----------------------------------------\n\nTITLE: Extracting Actual Movies for a User in Scala\nDESCRIPTION: Extracts the actual movie IDs that a specific user has interacted with, to be used as the actual items list for APK calculation.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_72\n\nLANGUAGE: scala\nCODE:\n```\nval actualMovies = moviesForUser.map(_.product)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Linear Regression Step Size in Spark\nDESCRIPTION: This code evaluates the impact of different step sizes on a linear regression model using Spark MLlib. It computes the RMSLE metric for various step size values.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_141\n\nLANGUAGE: Python\nCODE:\n```\nparams = [0.01, 0.025, 0.05, 0.1, 1.0]\nmetrics = [evaluate(train_data, test_data, 10, param, 0.0, 'l2', False) for param in params]\nprint params\nprint metrics\n```\n\n----------------------------------------\n\nTITLE: Accessing Reference Value in PCF\nDESCRIPTION: Demonstrates how to get the current value of a reference x in PCF using the dereference operator.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_30\n\nLANGUAGE: PCF\nCODE:\n```\n!x\n```\n\n----------------------------------------\n\nTITLE: Printing Movie Title in Scala\nDESCRIPTION: This snippet prints the title of a chosen movie using an itemId from a titles collection.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_67\n\nLANGUAGE: Scala\nCODE:\n```\nprintln(titles(itemId))\n```\n\n----------------------------------------\n\nTITLE: Custom MAP Calculation with K=2000\nDESCRIPTION: Custom implementation of Mean Average Precision calculation with a high K value (2000) to match RankingMetrics results. Uses a custom avgPrecisionK function to compute MAP across all recommendations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_82\n\nLANGUAGE: scala\nCODE:\n```\nval MAPK2000 = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) => \n  val actual = actualWithIds.map(_._2).toSeq\n  avgPrecisionK(actual, predicted, 2000)\n}.reduce(_ + _) / allRecs.count\nprintln(\"Mean Average Precision = \" + MAPK2000)\n```\n\n----------------------------------------\n\nTITLE: Stubbing SQL Queries with Pseudocode\nDESCRIPTION: Demonstrates how to use pseudocode as placeholders when designing complex SQL queries, allowing for incremental development.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_78\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT (<miracle for guys>) AS guy,\n       (<miracle for dolls>) AS doll\n  FROM People\n WHERE ?? (<join thingie for guys and dolls>)\n```\n\n----------------------------------------\n\nTITLE: Arithmetic Operation Rules in PCF\nDESCRIPTION: Defines rules for arithmetic operations in PCF's small-step operational semantics.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_11\n\nLANGUAGE: PCF\nCODE:\n```\np ⊗ q ⟶ n (if p ⊗ q = n)\n```\n\n----------------------------------------\n\nTITLE: Updating a row using auto-number ID\nDESCRIPTION: Example of updating a row in the Personnel table using the auto-number ID column, which can lead to data integrity issues.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\nUPDATE Personnel SET name = 'John Doe' WHERE id = 123\n```\n\n----------------------------------------\n\nTITLE: Computing Item Similarity with Cosine Similarity in Scala with Spark\nDESCRIPTION: This code computes the similarity between a given item and all other items using cosine similarity, then finds the top K most similar items.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_66\n\nLANGUAGE: Scala\nCODE:\n```\nval itemId = 567\nval itemFactor = model.productFeatures.lookup(itemId).head\nval itemVector = new DoubleMatrix(itemFactor)\n\nval sims = model.productFeatures.map{ case (id, factor) => \n  val factorVector = new DoubleMatrix(factor)\n  val sim = cosineSimilarity(factorVector, itemVector)\n  (id, sim)\n}\n\nval sortedSims = sims.top(K)(Ordering.by[(Int, Double), Double] { case (id, similarity) => similarity })\n\nprintln(sortedSims.take(10).mkString(\"\\n\"))\n```\n\n----------------------------------------\n\nTITLE: Executing Genetic Programming Algorithm in R\nDESCRIPTION: Runs the genetic programming algorithm with specified parameters including population size, fitness function, and stop condition.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_59\n\nLANGUAGE: R\nCODE:\n```\ngp=geneticProgramming(functionSet=FS,inputVariables=ST,\nconstantSet=cF1,populationSize=50,\nfitnessFunction=eval,\nstopCondition=makeTimeStopCondition(50),\nmutationFunction=mut,verbose=TRUE)\n```\n\n----------------------------------------\n\nTITLE: Simplified Notation for Function Definition\nDESCRIPTION: Shows the simplified notation for defining a function using the 'fun' keyword, which is equivalent to the formal notation using fun(x t).\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_4\n\nLANGUAGE: pseudo-code\nCODE:\n```\nfun x -> t\n```\n\n----------------------------------------\n\nTITLE: Starting Spark Shell with Increased Memory for Dimensionality Reduction\nDESCRIPTION: Command to start the Spark Scala shell with increased driver memory (2GB) to handle computationally expensive dimensionality reduction operations on image data.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_165\n\nLANGUAGE: shell\nCODE:\n```\n./SPARK_HOME/bin/spark-shell --driver-memory 2g\n```\n\n----------------------------------------\n\nTITLE: Finding Synonyms Using Trained Word2Vec Model in Scala\nDESCRIPTION: This code snippet shows how to use a trained Word2Vec model to find the top 20 synonyms for a given term. It demonstrates finding similar terms for 'hockey' and 'legislation' based on cosine similarity between word vectors.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_222\n\nLANGUAGE: Scala\nCODE:\n```\nword2vecModel.findSynonyms(\"hockey\", 20).foreach(println)\n\nword2vecModel.findSynonyms(\"legislation\", 20).foreach(println)\n```\n\n----------------------------------------\n\nTITLE: SQL Query Example - Poor Naming Convention\nDESCRIPTION: Example of poor SQL naming convention where table names are unnecessarily included in column names, making the query difficult to read and maintain.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT IPC.IPC_URN AS IPCURN,\n       IPC.IPCText, \n       IPC.IPCCode,\n       Offense.OffenseURN,\n       Offense.OffenseText,\n       Offense.OffenseCode\nFROM dbo_IPC AS IPC,\n     dbo_Offense AS Offense\nWHERE IPC.IPCCode = Offense.OffenseCode\n```\n\n----------------------------------------\n\nTITLE: Basic Streaming Application Output Example\nDESCRIPTION: Sample console output from a Spark Streaming application showing batch processing results including total purchases, unique users, total revenue, and most popular product information.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_227\n\nLANGUAGE: text\nCODE:\n```\n...\n14/11/15 21:27:30 INFO spark.SparkContext: Job finished: collect at Streaming.scala:125, took 0.071145 s\n== Batch start time: 2014/11/15 9:27 PM ==\nTotal purchases: 16\nUnique users: 10\nTotal revenue: 123.72\nMost popular product: iPad Cover with 6 purchases\n...\n```\n\n----------------------------------------\n\nTITLE: Poor Comment Example in Procedural Code\nDESCRIPTION: Shows an example of a poor comment in procedural code that doesn't provide meaningful information about the variable or operation being performed.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_46\n\nLANGUAGE: sql\nCODE:\n```\nscore := score + 1; -- increment score\n```\n\n----------------------------------------\n\nTITLE: Small-Step Operational Semantics Rules in PCF\nDESCRIPTION: Defines the small-step operational semantics for the abstract machine, showing how each instruction transforms the machine state. The machine state is represented as a tuple containing the accumulator, stack, environment, and code registers.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_20\n\nLANGUAGE: PCF\nCODE:\n```\n(a,s,e,((Mkclos i),c)) ⟶ (〈i,e〉,s,e,c)\n(a,s,e,(Push,c)) ⟶ (a,(a,s),e,c)\n(a,s,e,(Extend,c)) ⟶ (a,s,(e,a),c)\n(a,s,e,((Search n),c)) ⟶ (V,s,e,c)\n(a,s,e,(Pushenv,c)) ⟶ (a,(e,s),e,c)\n(a,(e',s),e,(Popenv,c)) ⟶ (a,s,e',c)\n(〈i,e'〉,(W,s),e,(Apply,c)) ⟶ (〈i,e'〉,s,(e', 〈i,e'〉, W), i c)\n(a,s,e,((Ldi n),c)) ⟶ (n,s,e,c)\n(n,(m,s),e,(Add,c)) ⟶ (n + m,s,e,c)\n(n,(m,s),e,(Sub,c)) ⟶ (n - m,s,e,c)\n(n,(m,s),e,(Mult,c)) ⟶ (n * m,s,e,c)\n(n,(m,s),e,(Div,c)) ⟶ (n / m,s,e,c)\n(0,s,e,((Test(i,j)),c)) ⟶ (0,s,e,i c)\n(n,s,e,((Test(i,j)),c)) ⟶ (n,s,e,j c)\n```\n\n----------------------------------------\n\nTITLE: Training Models with Spark MLlib\nDESCRIPTION: Demonstrates training of different classification models including Logistic Regression, SVM, Naive Bayes, and Decision Tree models using prepared training data.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_94\n\nLANGUAGE: scala\nCODE:\n```\nval lrModel = LogisticRegressionWithSGD.train(data, numIterations)\nval svmModel = SVMWithSGD.train(data, numIterations)\nval nbModel = NaiveBayes.train(nbData)\nval dtModel = DecisionTree.train(data, Algo.Classification, Entropy, maxTreeDepth)\n```\n\n----------------------------------------\n\nTITLE: Computing SVD on Image Matrix in Scala\nDESCRIPTION: This code computes the Singular Value Decomposition (SVD) on the image matrix using MLlib's RowMatrix functionality.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_187\n\nLANGUAGE: scala\nCODE:\n```\nval svd = matrix.computeSVD(10, computeU = true)\nprintln(s\"U dimension: (${svd.U.numRows}, ${svd.U.numCols})\")\nprintln(s\"S dimension: (${svd.s.size}, )\")\nprintln(s\"V dimension: (${svd.V.numRows}, ${svd.V.numCols})\")\n```\n\n----------------------------------------\n\nTITLE: Applying PCA for Dimensionality Reduction in Scala\nDESCRIPTION: This code creates a RowMatrix from the scaled vectors and computes the top K principal components using MLlib's PCA implementation.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_180\n\nLANGUAGE: Scala\nCODE:\n```\nimport org.apache.spark.mllib.linalg.Matrix\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\nval matrix = new RowMatrix(scaledVectors)\nval K = 10\nval pc = matrix.computePrincipalComponents(K)\n```\n\n----------------------------------------\n\nTITLE: KAG Installation Commands for Windows Developers\nDESCRIPTION: Installation instructions for developers using Windows to set up a Python environment, clone the KAG repository, and install the package in development mode.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/README_cn.md#2025-04-07_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n# 安装官方 Python 3.8.10 或更新版本，安装 Git。\n\n# 创建、激活 Python 虚拟环境：py -m venv kag-demo && kag-demo\\Scripts\\activate\n\n# 代码 clone：git clone https://github.com/OpenSPG/KAG.git\n\n# KAG 安装: cd KAG && pip install -e .\n```\n\n----------------------------------------\n\nTITLE: Object Field Access in PCF with References\nDESCRIPTION: Example of a term in PCF that demonstrates the difference in behavior between functional objects and objects with references. This illustrates how side effects are handled differently.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_40\n\nLANGUAGE: PCF\nCODE:\n```\nlet r = ref 0 in\nlet t = {a = ςs !r} in\nr := 1;\nt#a\n```\n\n----------------------------------------\n\nTITLE: Decision Tree Training Helper Function in Scala\nDESCRIPTION: Helper function to train decision trees with configurable maximum depth and impurity measures.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_112\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.mllib.tree.impurity.Impurity\nimport org.apache.spark.mllib.tree.impurity.Entropy\nimport org.apache.spark.mllib.tree.impurity.Gini\n\ndef trainDTWithParams(input: RDD[LabeledPoint], maxDepth: Int, impurity: Impurity) = {\n  DecisionTree.train(input, Algo.Classification, impurity, maxDepth)\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing R Help System Examples\nDESCRIPTION: Demonstrates various ways to access R's help system using example() and demo() functions.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_3\n\nLANGUAGE: R\nCODE:\n```\n> example(barplot)\n> demo(graphics)\n> demo()\n```\n\n----------------------------------------\n\nTITLE: Incorrect Multiple Update Approach in SQL\nDESCRIPTION: An example of incorrect SQL that doesn't produce the desired result when trying to update book prices. This approach updates all records twice, causing unintended changes.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_69\n\nLANGUAGE: SQL\nCODE:\n```\nUPDATE Books\nSET price = price * 0.90\nWHERE price >= 25.00;\n\nUPDATE Books\nSET price = price * 1.10\nWHERE price < 25.00;\n```\n\n----------------------------------------\n\nTITLE: Calculating evaluation metrics for generated answers\nDESCRIPTION: Commands to execute Python scripts for calculating summarization metrics and factual correctness of the generated answers.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython ./solver/summarization_metrics.py\npython ./solver/factual_correctness.py\n```\n\n----------------------------------------\n\nTITLE: Conditional Rules in PCF\nDESCRIPTION: Defines rules for conditional statements in PCF's small-step operational semantics.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_12\n\nLANGUAGE: PCF\nCODE:\n```\nifz 0 then t else u ⟶ t\nifz n then t else u ⟶ u (if n ≠ 0)\n```\n\n----------------------------------------\n\nTITLE: Using Simple CASE Expression in SQL\nDESCRIPTION: Demonstrates the simple form of the CASE expression which uses equality comparisons against a single operand. This approach is cleaner when comparing a single value against multiple possibilities.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_36\n\nLANGUAGE: sql\nCODE:\n```\nCASE state_code\n  WHEN 'TX' THEN 'Texas'\n  WHEN 'CA' THEN 'California'\n  WHEN 'NY' THEN 'New York'\n  ELSE 'Other'\nEND\n```\n\n----------------------------------------\n\nTITLE: Using Custom Functions in R\nDESCRIPTION: Demonstrates loading and using custom functions defined in an external file. Shows function calls with different argument passing methods and output formatting.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_13\n\nLANGUAGE: R\nCODE:\n```\nsource(\"functions.R\") # load the code\ncat(\"class of profit is:\",class(profit),\"\\n\") # function\nx=c(414.1,404.2,408.3,413.2,395.0)\ny=profit(x); cat(\"maximum profit:\",y,\"\\n\")\ncat(\"x is not changed:\",x,\"\\n\")\ncat(\"cost(x=\",x,\")=\",cost(x),\"\\n\")\ncat(\"sales(x=\",x,\")=\",sales(round(x)),\"\\n\")\nx=c(414,404); # sales for 2 bags:\ncat(\"sales(x=\",x,\")=\",sales(x),\"\\n\")\ncat(\"sales(x,A=1000,m=c(2,1.75))=\",sales(x,1000,m=c(2,1.75)),\"\\n\")\n# show 3! :\nx=3; cat(\"fact(\",x,\")=\",fact(x),\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Example of poor table design with auto-numbering\nDESCRIPTION: Example of a table design that relies on auto-numbering rather than natural keys, leading to potential data integrity issues.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE Personnel (id INT IDENTITY, ssn CHAR(11), name VARCHAR(30), ... )\n```\n\n----------------------------------------\n\nTITLE: Implementing Crossover Operators for Ordered Representations in R\nDESCRIPTION: This snippet defines two crossover operators for ordered representations in TSP: Partially Matched Crossover (PMX) which exchanges sections between parents through position-to-position swaps, and Order Crossover (OX) which preserves relative order of genes from both parents.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_79\n\nLANGUAGE: R\nCODE:\n```\n# partially matched crossover (PMX) operator:\n\n# m is a matrix with 2 parent x ordered solutions\n\npmx=function(m)\n\n{\n\nN=ncol(m)\n\np=sample(1:N,2) # two cutting points\n\nc=m # children\n\nfor(i in p[1]:p[2])\n\n{ # rearrange:\n\nc[1,which(c[1,]==m[2,i])]=c[1,i]\n\n# crossed section:\n\nc[1,i]=m[2,i]\n\n# rearrange:\n\nc[2,which(c[2,]==m[1,i])]=c[2,i]\n\n# crossed section:\n\nc[2,i]=m[1,i]\n\n}\n\nreturn(c)\n\n}\n\n# order crossover (OX) operator:\n\n# m is a matrix with 2 parent x ordered solutions\n\nox=function(m)\n\n{\n\nN=ncol(m)\n\np=sort(sample(1:N,2)) # two cutting points\n\nc=matrix(rep(NA,N*2),ncol=N)\n\n# keep selected section:\n\nc[,p[1]:p[2]]=m[,p[1]:p[2]]\n\n# rotate after cut 2 (p[2]):\n\nI=((p[2]+1):(p[2]+N))\n\nI=ifelse(I<=N,I,I-N)\n\na=m[,I]\n\n# fill remaining genes:\n\na1=setdiff(a[2,],c[1,p[1]:p[2]])\n\na2=setdiff(a[1,],c[2,p[1]:p[2]])\n\nI2=setdiff(I,p[1]:p[2])\n\nc[,I2]=rbind(a1,a2)\n\nreturn(c)\n\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Training and Test Data Splits\nDESCRIPTION: Splits the dataset into training (70%) and test (30%) sets using a holdout method for model evaluation, ensuring proper evaluation of the model's generalization capabilities.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_99\n\nLANGUAGE: R\nCODE:\n```\n# holdout split:\n\n# select training data (for fitting the model), 70%; and\n\n# test data (for estimating generalization capabilities), 30%.\n\nH=holdout(d[ALL,]$quality,ratio=0.7)\n\ncat(\"nr. training samples:\",length(H$tr),\"\\n\")\n\ncat(\"nr. test samples:\",length(H$ts),\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Multi-Objective Optimization Results Display in R\nDESCRIPTION: This code shows the output of running multi-objective optimization on binary, integer, and real value tasks. It displays the Pareto front solutions from the last generation with their respective objective function values.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_74\n\nLANGUAGE: R\nCODE:\n```\nsource(\"nsga2-test.R\")\n\nbinary task:\n\n11111111 f=(8,0.01)\n\n01111111 f=(7,1)\n\n11111111 f=(8,0.01)\n\n01111111 f=(7,1)\n\n11111111 f=(8,0.01)\n\n01111111 f=(7,1)\n\n11111111 f=(8,0.01)\n\n11111111 f=(8,0.01)\n\n11111111 f=(8,0.01)\n\n11111111 f=(8,0.01)\n\n11111111 f=(8,0.01)\n\n01111111 f=(7,1)\n```\n\n----------------------------------------\n\nTITLE: Proper table design with natural key\nDESCRIPTION: Example of a better table design using a natural key (SSN) instead of relying on auto-numbering.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE Personnel (ssn CHAR(11) PRIMARY KEY, name VARCHAR(30), ... )\n```\n\n----------------------------------------\n\nTITLE: Citation BibTeX for KAG Research Papers\nDESCRIPTION: BibTeX citation formats for referencing the KAG framework and related research papers in academic publications.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/README_cn.md#2025-04-07_snippet_4\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{liang2024kag,\n  title={KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation},\n  author={Liang, Lei and Sun, Mengshu and Gui, Zhengke and Zhu, Zhongshu and Jiang, Zhouyu and Zhong, Ling and Zhao, Peilong and Bo, Zhongpu and Yang, Jin and others},\n  journal={arXiv preprint arXiv:2409.13731},\n  year={2024}\n}\n\n@article{yikgfabric,\n  title={KGFabric: A Scalable Knowledge Graph Warehouse for Enterprise Data Interconnection},\n  author={Yi, Peng and Liang, Lei and Da Zhang, Yong Chen and Zhu, Jinye and Liu, Xiangyu and Tang, Kun and Chen, Jialin and Lin, Hao and Qiu, Leijie and Zhou, Jun}\n}\n```\n\n----------------------------------------\n\nTITLE: Feature Vector Creation for Decision Tree in PySpark\nDESCRIPTION: Function to extract features for decision tree model by converting all values to floats without binary encoding of categorical variables. Creates labeled points suitable for decision tree training.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_121\n\nLANGUAGE: python\nCODE:\n```\ndef extract_features_dt(record):\n  return np.array(map(float, record[2:14]))\ndata_dt = records.map(lambda r: LabeledPoint(extract_label(r), extract_features_dt(r)))\n```\n\n----------------------------------------\n\nTITLE: Term Construction with Function Application\nDESCRIPTION: Illustrates how to construct a term by applying a function to arguments using the α symbol for application.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/introduction_to_the_theory_of_programming_languages.txt#2025-04-07_snippet_2\n\nLANGUAGE: pseudo-code\nCODE:\n```\nα(t,u)\n```\n\n----------------------------------------\n\nTITLE: Matrix and Data Frame Operations in R\nDESCRIPTION: This snippet illustrates the creation and manipulation of matrices and data frames in R. It covers operations such as element access, row and column modifications, and data frame transformations.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_8\n\nLANGUAGE: R\nCODE:\n```\n> m=matrix(ncol=3,nrow=2); m[,]=0; print(m) # 3x2 matrix\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n\n> m[1,]=1:3; print(m) # change 1st row\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    0    0    0\n\n> m[,3]=1:2; print(m) # change 3rd column\n     [,1] [,2] [,3]\n[1,]    1    2    1\n[2,]    0    0    2\n\n> m[2,1]=3; print(m) # change m[2,1]\n     [,1] [,2] [,3]\n[1,]    1    2    1\n[2,]    3    0    2\n\n> print(nrow(m)) # number of rows\n[1] 2\n\n> print(ncol(m)) # number of columns\n[1] 3\n\n> m[nrow(m),ncol(m)]=5; print(m) # change last element\n     [,1] [,2] [,3]\n[1,]    1    2    1\n[2,]    3    0    5\n\n> m[nrow(m)-1,ncol(m)-1]=4; print(m) # change m[1,2]\n     [,1] [,2] [,3]\n[1,]    1    4    1\n[2,]    3    0    5\n\n> print(max(m)) # show maximum of m\n[1] 5\n\n> m=sqrt(m); print(m) # change m\n          [,1] [,2]     [,3]\n[1,] 1.000000    2 1.000000\n[2,] 1.732051    0 2.236068\n\n> m[1,]=c(1,1,2013); m[2,]=c(2,2,2013) # change m\n\n> d=data.frame(m) # create data.frame\n\n> names(d)=c(\"day\",\"month\",\"year\") # change names\n\n> d[1,]=c(2,1,2013); print(d) # change 1st row\n  day month year\n1   2     1 2013\n2   2     2 2013\n\n> d$day[2]=3; print(d) # change d[1,2]\n  day month year\n1   2     1 2013\n2   3     2 2013\n\n> d=rbind(d,c(4,3,2014)); print(d) # add row to d\n  day month year\n1   2     1 2013\n2   3     2 2013\n3   4     3 2014\n\n> # change 2nd column of d to factor, same as d[,2]=factor(...\n> d$month=factor(c(\"Jan\",\"Feb\",\"Mar\"))\n\n> print(summary(d)) # summary of d\n      day    month       year     \n Min.   :2.0   Feb:1   Min.   :2013  \n 1st Qu.:2.5   Jan:1   1st Qu.:2013  \n Median :3.0   Mar:1   Median :2013  \n Mean   :3.0           Mean   :2013  \n 3rd Qu.:3.5           3rd Qu.:2014  \n Max.   :4.0           Max.   :2014\n```\n\n----------------------------------------\n\nTITLE: Calculating Logistic Loss Function in Python\nDESCRIPTION: This snippet defines the logistic loss function used in logistic regression, where y is the actual target variable (1 for positive class, -1 for negative class).\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_85\n\nLANGUAGE: python\nCODE:\n```\nlog(1 + exp(-ywTx))\n```\n\n----------------------------------------\n\nTITLE: Loading Required R Packages for Wine Quality Classification\nDESCRIPTION: Loads the necessary R packages for the wine quality classification task: rminer for data mining functionality, kernlab for SVM implementation, and mco for multi-objective optimization.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_97\n\nLANGUAGE: R\nCODE:\n```\nlibrary(rminer) # load rminer package\n\nlibrary(kernlab) # load svm functions used by rminer\n\nlibrary(mco) # load mco package\n```\n\n----------------------------------------\n\nTITLE: SQL Comma Formatting\nDESCRIPTION: Illustrates proper comma placement in SQL queries for improved readability.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT col1,col2,col3,col4 FROM TableName\n```\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT col1, col2, col3, col4 FROM TableName\n```\n\n----------------------------------------\n\nTITLE: Naive Bayes Parameter Testing in Scala\nDESCRIPTION: Tests different lambda values for Naive Bayes additive smoothing and evaluates impact on model performance.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_114\n\nLANGUAGE: scala\nCODE:\n```\ndef trainNBWithParams(input: RDD[LabeledPoint], lambda: Double) = {\n  val nb = new NaiveBayes\n  nb.setLambda(lambda)\n  nb.run(input)\n}\nval nbResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n  val model = trainNBWithParams(dataNB, param)\n  val scoreAndLabels = dataNB.map { point =>\n    (model.predict(point.features), point.label)\n  }\n  val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n  (s\"$param lambda\", metrics.areaUnderROC)\n}\nnbResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\n```\n\n----------------------------------------\n\nTITLE: Committing Schema to Knowledge Graph\nDESCRIPTION: Command to commit the predefined schema to the knowledge graph using knext CLI.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nknext schema commit\n```\n\n----------------------------------------\n\nTITLE: Creating MLlib Vectors from Pixel Data in Scala\nDESCRIPTION: This code creates MLlib Vector instances from the extracted pixel data and caches the resulting RDD.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_178\n\nLANGUAGE: Scala\nCODE:\n```\nimport org.apache.spark.mllib.linalg.Vectors\nval vectors = pixels.map(p => Vectors.dense(p))\nvectors.setName(\"image-vectors\")\nvectors.cache\n```\n\n----------------------------------------\n\nTITLE: Combined Image Processing and Pixel Extraction in Scala\nDESCRIPTION: This function combines loading, processing, and pixel extraction for an image file.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_176\n\nLANGUAGE: Scala\nCODE:\n```\ndef extractPixels(path: String, width: Int, height: Int): Array[Double] = {\n  val raw = loadImageFromFile(path)\n  val processed = processImage(raw, width, height)\n  getPixelsFromImage(processed)\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Company Industry Affiliations\nDESCRIPTION: GQL query to retrieve the industry affiliations of companies in the knowledge graph.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nknext reasoner execute --dsl \"\nMATCH\n    (s:SupplyChain.Company)-[:belongToIndustry]->(o:SupplyChain.Industry)\nRETURN\n    s.name, o.name\n\"\n```\n\n----------------------------------------\n\nTITLE: Implementing One-Hot Encoding for Categorical Variables in Python\nDESCRIPTION: Code demonstrating how to create a one-hot (1-of-k) encoded binary feature vector for the occupation 'programmer' using NumPy arrays.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nK = len(all_occupations_dict)\nbinary_x = np.zeros(K)\nk_programmer = all_occupations_dict['programmer']\nbinary_x[k_programmer] = 1\nprint \"Binary feature vector: %s\" % binary_x\nprint \"Length of binary vector: %d\" % K\n```\n\n----------------------------------------\n\nTITLE: KAG Installation Commands for macOS/Linux Developers\nDESCRIPTION: Installation instructions for developers using macOS or Linux to set up a Python environment, clone the KAG repository, and install the package in development mode.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/README_cn.md#2025-04-07_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n# 安装 Python 虚拟环境：conda create -n kag-demo python=3.10 && conda activate kag-demo\n\n# 代码 clone：git clone https://github.com/OpenSPG/KAG.git\n\n# KAG 安装: cd KAG && pip install -e .\n```\n\n----------------------------------------\n\nTITLE: Implementing Squared Log Error Function in Python\nDESCRIPTION: Function to calculate squared log error for RMSLE calculation, useful for datasets with wide range in target variable.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_125\n\nLANGUAGE: python\nCODE:\n```\ndef squared_log_error(pred, actual):\n    return (np.log(pred + 1) - np.log(actual + 1))**2\n```\n\n----------------------------------------\n\nTITLE: Cleaning up project resources\nDESCRIPTION: Commands to delete checkpoint directories and the KAG project from the OpenSPG server to clean up resources after completion.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nrm -rf ./builder/ckpt\nrm -rf ./solver/ckpt\n```\n\n----------------------------------------\n\nTITLE: Implementing tournament Function for Solution Selection in R\nDESCRIPTION: The tournament function compares k randomly selected solutions from a population and returns the n best indexes based on lexicographic ordering. It evaluates each solution using the provided evaluation function and ranks them according to multiple objectives.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_67\n\nLANGUAGE: R\nCODE:\n```\ntournament=function(Population,evalFunc,k,n,m=2)\n\n{\n\npopSize=nrow(Population)\n\nPID=sample(1:popSize,k) # select k random tournament solutions\n\nE=matrix(nrow=k,ncol=m) # evaluations of tournament solutions\n\nfor(i in 1:k) # evaluate tournament\n\nE[i,]=evalFunc(Population[PID[i],])\n\n# return best n individuals:\n\nB=lexibest(E); i=1; res=PID[B] # best individual\n\nwhile(i<n) # other best individuals\n\n{\n\nE=E[-B,];PID=PID[-B] # all except B\n\nif(is.matrix(E)) B=lexibest(E)\n\nelse B=1 # only 1 row\n\nres=c(res,PID[B])\n\ni=i+1\n\n}\n\nreturn(res)\n\n}\n```\n\n----------------------------------------\n\nTITLE: Running Spark Example in Local Mode on EC2\nDESCRIPTION: Commands to run the SparkPi example application in local mode on an EC2 instance. This tests the Spark installation using the local compute resources of the master node.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n> cd spark\n> MASTER=local[2] ./bin/run-example SparkPi\n```\n\n----------------------------------------\n\nTITLE: Evaluating L1 Regularization in Spark Linear Regression\nDESCRIPTION: This code evaluates the impact of different L1 regularization levels on a linear regression model using Spark MLlib. It computes the RMSLE metric for various regularization parameter values and plots the results.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_143\n\nLANGUAGE: Python\nCODE:\n```\nparams = [0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\nmetrics = [evaluate(train_data, test_data, 10, 0.1, param, 'l1', False) for param in params]\nprint params\nprint metrics\nplot(params, metrics)\nfig = matplotlib.pyplot.gcf()\npyplot.xscale('log')\n```\n\n----------------------------------------\n\nTITLE: Examining User Data File Contents in Bash\nDESCRIPTION: Using the head command to view the first 5 lines of the u.user file, which contains user profile information.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/machine_learning_with_spark.txt#2025-04-07_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\n> head -5 u.user\n```\n\n----------------------------------------\n\nTITLE: Implementing Grid Search Methods in R\nDESCRIPTION: Implementation of standard grid search methods (gsearch and gsearch2) that reduce the solution space by implementing a regular hyper-dimensional search with a given step size. The first uses fsearch while the second uses dfsearch.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/modern_optimization_with_r.txt#2025-04-07_snippet_20\n\nLANGUAGE: r\nCODE:\n```\n### grid.R file ###\n\n# standard grid search method (uses fsearch)\n# step - vector with step size for each dimension D\n# lower - vector with lowest values for each dimension\n# upper - vector with highest values for each dimension\n# FUN - evaluation function\n# type - \"min\" or \"max\"\n# ... - extra parameters for FUN\ngsearch=function(step,lower,upper,FUN,type=\"min\",...)\n{ D=length(step) # dimension\ndomain=vector(\"list\",D) # domain values\nL=vector(length=D) # auxiliary vector\nfor(i in 1:D)\n{ domain[[i]]=seq(lower[i],upper[i],by=step[i])\nL[i]=length(domain[[i]])\n}\nLS=prod(L)\ns=matrix(ncol=D,nrow=LS) # set the search space\nfor(i in 1:D)\n{\nif(i==1) E=1 else E=E*L[i-1]\ns[,i]=rep(domain[[i]],length.out=LS,each=E)\n}\nfsearch(s,FUN,type,...) # best solution\n}\n\n# standard grid search method (uses dfsearch)\ngsearch2=function(step,lower,upper,FUN,type=\"min\",...)\n{ D=length(step) # dimension\ndomain=vector(\"list\",D) # domain values\nfor(i in 1:D) domain[[i]]=seq(lower[i],upper[i],by=step[i])\ndfsearch(domain=domain,FUN=FUN,type=type,...) # solution\n}\n\n# nested grid search method (uses fsearch)\n```\n\n----------------------------------------\n\nTITLE: Using CROSS JOIN in SQL Queries\nDESCRIPTION: Demonstrates an attempt to join male and female participants using a CROSS JOIN, which produces undesired multiple pairings.\nSOURCE: https://github.com/OpenSPG/KAG/blob/master/kag/examples/csqa/builder/data/joe_celko_s_sql_programming_style.txt#2025-04-07_snippet_80\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT g.name AS guy, d.name AS doll\n  FROM (SELECT name FROM People WHERE gender = 1) AS g,\n       (SELECT name FROM People WHERE gender = 2) AS d\n```"
  }
]
[
  {
    "owner": "google-research",
    "repo": "timesfm",
    "content": "TITLE: Visualizing TimesFM Predictions\nDESCRIPTION: Function to plot model predictions against ground truth data. Creates visualizations comparing historical data, ground truth, and model predictions with customizable plot parameters.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning_torch.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef plot_predictions(model: TimesFm, val_dataset: Dataset, save_path: Optional[str] = \"predictions.png\") -> None:\n  import matplotlib.pyplot as plt\n\n  model.eval()\n  x_context, x_padding, freq, x_future = val_dataset[0]\n  x_context = x_context.unsqueeze(0)\n  x_padding = x_padding.unsqueeze(0)\n  freq = freq.unsqueeze(0)\n  x_future = x_future.unsqueeze(0)\n\n  device = next(model.parameters()).device\n  x_context = x_context.to(device)\n  x_padding = x_padding.to(device)\n  freq = freq.to(device)\n  x_future = x_future.to(device)\n\n  with torch.no_grad():\n    predictions = model(x_context, x_padding.float(), freq)\n    predictions_mean = predictions[..., 0]\n    last_patch_pred = predictions_mean[:, -1, :]\n\n  context_vals = x_context[0].cpu().numpy()\n  future_vals = x_future[0].cpu().numpy()\n  pred_vals = last_patch_pred[0].cpu().numpy()\n\n  context_len = len(context_vals)\n  horizon_len = len(future_vals)\n\n  plt.figure(figsize=(12, 6))\n  plt.plot(range(context_len), context_vals, label=\"Historical Data\", color=\"blue\", linewidth=2)\n  plt.plot(range(context_len, context_len + horizon_len), future_vals, label=\"Ground Truth\", color=\"green\", linestyle=\"--\", linewidth=2)\n  plt.plot(range(context_len, context_len + horizon_len), pred_vals, label=\"Prediction\", color=\"red\", linewidth=2)\n  plt.xlabel(\"Time Step\")\n  plt.ylabel(\"Value\")\n  plt.title(\"TimesFM Predictions vs Ground Truth\")\n  plt.legend()\n  plt.grid(True)\n\n  if save_path:\n    plt.savefig(save_path)\n  plt.close()\n```\n\n----------------------------------------\n\nTITLE: Initializing TimesFM Model with JAX Backend\nDESCRIPTION: Loads the TimesFM 2.0 checkpoint using the JAX/PAX backend. Configures model parameters like batch size, horizon length, and context length.\nSOURCE: https://github.com/google-research/timesfm/blob/master/README.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport timesfm\n\ntfm = timesfm.TimesFm(\n      hparams=timesfm.TimesFmHparams(\n          backend=\"gpu\",\n          per_core_batch_size=32,\n          horizon_len=128,\n          num_layers=50,\n          context_len=2048,\n\n          use_positional_embedding=False,\n      ),\n      checkpoint=timesfm.TimesFmCheckpoint(\n          huggingface_repo_id=\"google/timesfm-2.0-500m-jax\"),\n  )\n```\n\n----------------------------------------\n\nTITLE: Loading TimesFM Base Model in Python\nDESCRIPTION: Initializes and loads the TimesFM model from a pre-trained checkpoint with specific configuration parameters. The model is configured with context length, horizon length, patch lengths, number of layers, dimensions, and backend type.\nSOURCE: https://github.com/google-research/timesfm/blob/master/peft/usage.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom timesfm import TimesFm, freq_map, data_loader\nfrom adapter.utils import load_adapter_checkpoint\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\n\n\ntfm = TimesFm(\n    context_len=512,\n    horizon_len=128,\n    input_patch_len=32,\n    output_patch_len=128,\n    num_layers=20,\n    model_dims=1280,\n    backend=\"cpu\",\n)\ntfm.load_from_checkpoint(repo_id=\"google/timesfm-1.0-200m\")\n```\n\n----------------------------------------\n\nTITLE: Implementing TimesFM Training Pipeline\nDESCRIPTION: Complete implementation of the TimesFM training pipeline including data preparation, model configuration, and finetuning process. Uses stock market data for demonstration with configurable training parameters.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning_torch.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_data(context_len: int, horizon_len: int, freq_type: int = 0) -> Tuple[Dataset, Dataset]:\n  df = yf.download(\"AAPL\", start=\"2010-01-01\", end=\"2019-01-01\")\n  time_series = df[\"Close\"].values\n\n  train_dataset, val_dataset = prepare_datasets(\n      series=time_series,\n      context_length=context_len,\n      horizon_length=horizon_len,\n      freq_type=freq_type,\n      train_split=0.8,\n  )\n\n  print(f\"Created datasets:\")\n  print(f\"- Training samples: {len(train_dataset)}\")\n  print(f\"- Validation samples: {len(val_dataset)}\")\n  print(f\"- Using frequency type: {freq_type}\")\n  return train_dataset, val_dataset\n\ndef single_gpu_example():\n  model, hparams, tfm_config = get_model(load_weights=True)\n  config = FinetuningConfig(batch_size=256,\n                            num_epochs=5,\n                            learning_rate=1e-4,\n                            use_wandb=True,\n                            freq_type=1,\n                            log_every_n_steps=10,\n                            val_check_interval=0.5,\n                            use_quantile_loss=True)\n\n  train_dataset, val_dataset = get_data(128, tfm_config.horizon_len, freq_type=config.freq_type)\n  finetuner = TimesFMFinetuner(model, config)\n\n  print(\"\\nStarting finetuning...\")\n  results = finetuner.finetune(train_dataset=train_dataset, val_dataset=val_dataset)\n\n  print(\"\\nFinetuning completed!\")\n  print(f\"Training history: {len(results['history']['train_loss'])} epochs\")\n\n  plot_predictions(model=model, val_dataset=val_dataset, save_path=\"timesfm_predictions.png\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Main Training Loop with Evaluation\nDESCRIPTION: Main training loop implementation with periodic evaluation, checkpoint saving, and early stopping logic. Includes progress tracking and loss monitoring.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfor epoch in range(NUM_EPOCHS):\n    print(f\"__________________Epoch: {epoch}__________________\", flush=True)\n    train_its = train_batches.as_numpy_iterator()\n    if patience >= PATIENCE:\n        print(\"Early stopping.\", flush=True)\n        break\n    for batch in tqdm(train_its):\n        train_losses = []\n        if patience >= PATIENCE:\n            print(\"Early stopping.\", flush=True)\n            break\n        tbatch = process_train_batch(batch)\n        tbatch = reshape_batch_for_pmap(tbatch, num_devices)\n        replicated_jax_states, step_fun_out = p_train_step(\n            replicated_jax_states, train_prng_seed, tbatch\n        )\n        train_losses.append(step_fun_out.loss[0])\n        if step_count % TRAIN_STEPS_PER_EVAL == 0:\n            print(\n                f\"Train loss at step {step_count}: {np.mean(train_losses)}\",\n                flush=True,\n            )\n            train_losses = []\n            print(\"Starting eval.\", flush=True)\n            val_its = val_batches.as_numpy_iterator()\n            eval_losses = []\n            for ev_batch in tqdm(val_its):\n                ebatch = process_eval_batch(ev_batch)\n                ebatch = reshape_batch_for_pmap(ebatch, num_devices)\n                _, step_fun_out = p_eval_step(\n                    replicated_jax_states, eval_prng_seed, ebatch\n                )\n                eval_losses.append(step_fun_out.loss[0])\n            mean_loss = np.mean(eval_losses)\n            print(f\"Eval loss at step {step_count}: {mean_loss}\", flush=True)\n            if mean_loss < best_eval_loss or np.isnan(mean_loss):\n                best_eval_loss = mean_loss\n                print(\"Saving checkpoint.\")\n                jax_state_for_saving = py_utils.maybe_unreplicate_for_fully_replicated(\n                    replicated_jax_states\n                )\n                checkpoints.save_checkpoint(\n                    jax_state_for_saving, CHECKPOINT_DIR, overwrite=True\n                )\n                patience = 0\n                del jax_state_for_saving\n                gc.collect()\n            else:\n                patience += 1\n                print(f\"patience: {patience}\")\n        step_count += 1\n```\n\n----------------------------------------\n\nTITLE: Basic TimesFM Forecasting with Monthly Data\nDESCRIPTION: Example of using TimesFM to generate forecasts on a monthly dataset using the forecast_on_df function. The input data contains time series observations with timestamps and values.\nSOURCE: https://github.com/google-research/timesfm/blob/master/README.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nforecast_df = tfm.forecast_on_df(\n    inputs=input_df,\n    freq=\"M\",  # monthly\n    value_name=\"y\",\n    num_jobs=-1,\n)\n```\n\n----------------------------------------\n\nTITLE: Forecasting with Covariates and Performance Benchmarking\nDESCRIPTION: Implements forecasting with TimesFM using covariates, comparing performance between standard TimesFM forecasts and forecasts enhanced with covariate information, and measuring execution time.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/covariates.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\n# Benchmark\nbatch_size = 128\ncontext_len = 120\nhorizon_len = 24\ninput_data = get_batched_data_fn(batch_size = 128)\nmetrics = defaultdict(list)\n\n\nfor i, example in enumerate(input_data()):\n  raw_forecast, _ = model.forecast(\n      inputs=example[\"inputs\"], freq=[0] * len(example[\"inputs\"])\n  )\n  start_time = time.time()\n  # Forecast with covariates\n  # Output: new forecast, forecast by the xreg\n  cov_forecast, ols_forecast = model.forecast_with_covariates(  \n      inputs=example[\"inputs\"],\n      dynamic_numerical_covariates={\n          \"gen_forecast\": example[\"gen_forecast\"],\n      },\n      dynamic_categorical_covariates={\n          \"week_day\": example[\"week_day\"],\n      },\n      static_numerical_covariates={},\n      static_categorical_covariates={\n          \"country\": example[\"country\"]\n      },\n      freq=[0] * len(example[\"inputs\"]),\n      xreg_mode=\"xreg + timesfm\",              # default\n      ridge=0.0,\n      force_on_cpu=False,\n      normalize_xreg_target_per_input=True,    # default\n  )\n  print(\n      f\"\\rFinished batch {i} linear in {time.time() - start_time} seconds\",\n      end=\"\",\n  )\n  metrics[\"eval_mae_timesfm\"].extend(\n      mae(raw_forecast[:, :horizon_len], example[\"outputs\"])\n  )\n  metrics[\"eval_mae_xreg_timesfm\"].extend(mae(cov_forecast, example[\"outputs\"]))\n  metrics[\"eval_mae_xreg\"].extend(mae(ols_forecast, example[\"outputs\"]))\n  metrics[\"eval_mse_timesfm\"].extend(\n      mse(raw_forecast[:, :horizon_len], example[\"outputs\"])\n  )\n  metrics[\"eval_mse_xreg_timesfm\"].extend(mse(cov_forecast, example[\"outputs\"]))\n  metrics[\"eval_mse_xreg\"].extend(mse(ols_forecast, example[\"outputs\"]))\n\nprint()\n\nfor k, v in metrics.items():\n  print(f\"{k}: {np.mean(v)}\")\n\n# My output:\n# eval_mae_timesfm: 6.762283045916956\n# eval_mae_xreg_timesfm: 5.39219617611074\n# eval_mae_xreg: 37.15275842572484\n# eval_mse_timesfm: 166.7771466306823\n# eval_mse_xreg_timesfm: 120.64757721021306\n# eval_mse_xreg: 1672.2116821201796\n```\n\n----------------------------------------\n\nTITLE: Loading Electricity Price Forecasting Dataset\nDESCRIPTION: Loads the Electricity Price Forecasting dataset from an online source and converts the timestamp column to datetime format for further processing.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/covariates.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE.csv')\ndf['ds'] = pd.to_datetime(df['ds'])\ndf\n```\n\n----------------------------------------\n\nTITLE: Initializing TimesFM Model with PyTorch Backend\nDESCRIPTION: Loads the TimesFM 2.0 checkpoint using the PyTorch backend. Sets similar parameters as the JAX version but uses a different HuggingFace repository.\nSOURCE: https://github.com/google-research/timesfm/blob/master/README.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport timesfm\n\ntfm = timesfm.TimesFm(\n      hparams=timesfm.TimesFmHparams(\n          backend=\"gpu\",\n          per_core_batch_size=32,\n          horizon_len=128,\n          num_layers=50,\n          use_positional_embedding=False,\n          context_len=2048,\n      ),\n      checkpoint=timesfm.TimesFmCheckpoint(\n          huggingface_repo_id=\"google/timesfm-2.0-500m-pytorch\"),\n  )\n```\n\n----------------------------------------\n\nTITLE: Creating TimeSeriesDataset Class for TimesFM\nDESCRIPTION: Implementation of a custom PyTorch Dataset class for time series data processing. Handles data preparation with sliding windows, context length, and horizon length specifications for TimesFM model input.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning_torch.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass TimeSeriesDataset(Dataset):\n  \"\"\"Dataset for time series data compatible with TimesFM.\"\"\"\n\n  def __init__(self,\n               series: np.ndarray,\n               context_length: int,\n               horizon_length: int,\n               freq_type: int = 0):\n    if freq_type not in [0, 1, 2]:\n      raise ValueError(\"freq_type must be 0, 1, or 2\")\n\n    self.series = series\n    self.context_length = context_length\n    self.horizon_length = horizon_length\n    self.freq_type = freq_type\n    self._prepare_samples()\n\n  def _prepare_samples(self) -> None:\n    self.samples = []\n    total_length = self.context_length + self.horizon_length\n\n    for start_idx in range(0, len(self.series) - total_length + 1):\n      end_idx = start_idx + self.context_length\n      x_context = self.series[start_idx:end_idx]\n      x_future = self.series[end_idx:end_idx + self.horizon_length]\n      self.samples.append((x_context, x_future))\n\n  def __len__(self) -> int:\n    return len(self.samples)\n\n  def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    x_context, x_future = self.samples[index]\n    x_context = torch.tensor(x_context, dtype=torch.float32)\n    x_future = torch.tensor(x_future, dtype=torch.float32)\n    input_padding = torch.zeros_like(x_context)\n    freq = torch.tensor([self.freq_type], dtype=torch.long)\n    return x_context, input_padding, freq, x_future\n```\n\n----------------------------------------\n\nTITLE: Loading TimesFM Pretrained Checkpoint from Hugging Face\nDESCRIPTION: Initializes a TimesFM model instance with specific hyperparameters and loads pretrained weights from the Hugging Face model repository, specifying backend type and architecture parameters.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntimesfm_backend = \"gpu\"  # @param\n\ntfm = timesfm.TimesFm(\n      hparams=timesfm.TimesFmHparams(\n          backend=timesfm_backend,\n          per_core_batch_size=32,\n          horizon_len=128,\n          num_layers=50,\n          # Se this to True for v1.0 checkpoints\n          use_positional_embedding=False,\n          # Note that we could set this to as high as 2048 but keeping it 512 here so that\n          # both v1.0 and 2.0 checkpoints work\n          context_len=512,\n      ),\n      checkpoint=timesfm.TimesFmCheckpoint(\n          huggingface_repo_id=\"google/timesfm-2.0-500m-jax\"),\n  )\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX Device Mesh and Parallelism Strategy\nDESCRIPTION: Sets up JAX device mesh configuration for model parallelism and prints information about available devices and their types for distributed training.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntask_p.model.ici_mesh_shape = [1, 1, 1]\ntask_p.model.mesh_axis_names = ['replica', 'data', 'mdl']\n\nDEVICES = np.array(jax.devices()).reshape([1, 1, 1])\nMESH = jax.sharding.Mesh(DEVICES, ['replica', 'data', 'mdl'])\n\nnum_devices = jax.local_device_count()\nprint(f'num_devices: {num_devices}')\nprint(f'device kind: {jax.local_devices()[0].device_kind}')\n```\n\n----------------------------------------\n\nTITLE: Evaluating TimesFM Model with MAE Metric on Test Data\nDESCRIPTION: Performs evaluation of the pretrained TimesFM model on the test dataset, calculating Mean Absolute Error (MAE) for each batch and reporting the overall average.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmae_losses = []\nfor batch in tqdm(test_batches.as_numpy_iterator()):\n    past = batch[0]\n    actuals = batch[3]\n    forecasts, _ = tfm.forecast(list(past), [0] * past.shape[0], normalize=True)\n    forecasts = forecasts[:, 0 : actuals.shape[1]]\n    mae_losses.append(np.abs(forecasts - actuals).mean())\n\nprint(f\"MAE: {np.mean(mae_losses)}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing JAX Training and Evaluation Steps\nDESCRIPTION: Defines training and evaluation step functions using JAX, with parallel mapping support. Sets up PRNG keys for randomization and implements pmapped versions of the training and evaluation functions.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\njax_task = task_p\n\ndef train_step(states, prng_key, inputs):\n  return trainer_lib.train_step_single_learner(\n      jax_task, states, prng_key, inputs\n  )\n\n\ndef eval_step(states, prng_key, inputs):\n  states = states.to_eval_state()\n  return trainer_lib.eval_step_single_learner(\n      jax_task, states, prng_key, inputs\n  )\n\nkey, train_key, eval_key = jax.random.split(key, 3)\ntrain_prng_seed = jax.random.split(train_key, num=jax.local_device_count())\neval_prng_seed = jax.random.split(eval_key, num=jax.local_device_count())\n\np_train_step = jax.pmap(train_step, axis_name='batch')\np_eval_step = jax.pmap(eval_step, axis_name='batch')\n```\n\n----------------------------------------\n\nTITLE: Evaluating TimesFM Model Performance with MAE Metric\nDESCRIPTION: Evaluates the TimesFM model on test data by iterating through test batches, generating forecasts, and computing Mean Absolute Error (MAE). For each batch, the model forecasts are compared with actual values to measure prediction accuracy.\nSOURCE: https://github.com/google-research/timesfm/blob/master/peft/usage.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmae_losses = []\nfor batch in tqdm(test_batches.as_numpy_iterator()):\n    past = batch[0]\n    actuals = batch[3]\n    _, forecasts = tfm.forecast(list(past), [0] * past.shape[0])\n    forecasts = forecasts[:, 0 : actuals.shape[1], 5]\n    mae_losses.append(np.abs(forecasts - actuals).mean())\n\nprint(f\"MAE: {np.mean(mae_losses)}\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Covariate Analysis\nDESCRIPTION: Imports necessary Python libraries for data manipulation, numerical operations, and data structuring when working with time series and covariates.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/covariates.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Array Inputs\nDESCRIPTION: Demonstrates how to use the TimesFM model for forecasting using numpy array inputs. Includes examples of setting different frequencies for input time series.\nSOURCE: https://github.com/google-research/timesfm/blob/master/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nforecast_input = [\n    np.sin(np.linspace(0, 20, 100)),\n    np.sin(np.linspace(0, 20, 200)),\n    np.sin(np.linspace(0, 20, 400)),\n]\nfrequency_input = [0, 1, 2]\n\npoint_forecast, experimental_quantile_forecast = tfm.forecast(\n    forecast_input,\n    freq=frequency_input,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Task Configuration for TimesFM Finetuning\nDESCRIPTION: Creates a Praxis/PAX task configuration that combines the model and learner for finetuning on time series data.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntask_p = tasks_lib.SingleTask(\n    name='ts-learn',\n    model=model,\n    train=tasks_lib.SingleTask.Train(\n        learner=build_learner(),\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Learner with Adam Optimizer and Cosine Schedule\nDESCRIPTION: Defines a learner configuration with Adam optimizer, cosine learning rate schedule, and variable exclusion for linear probing by freezing transformer layers during finetuning.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@pax_fiddle.auto_config\ndef build_learner() -> learners.Learner:\n  return pax_fiddle.Config(\n      learners.Learner,\n      name='learner',\n      loss_name='avg_qloss',\n      optimizer=optimizers.Adam(\n          epsilon=1e-7,\n          clip_threshold=1e2,\n          learning_rate=1e-2,\n          lr_schedule=pax_fiddle.Config(\n              schedules.Cosine,\n              initial_value=1e-3,\n              final_value=1e-4,\n              total_steps=40000,\n          ),\n          ema_decay=0.9999,\n      ),\n      # Linear probing i.e we hold the transformer layers fixed.\n      bprop_variable_exclusion=['.*/stacked_transformer_layer/.*'],\n  )\n```\n\n----------------------------------------\n\nTITLE: Batched Data Function for Time Series Forecasting\nDESCRIPTION: Creates a function that generates batches of time series data with associated covariates for forecasting tasks, separating data into context and horizon periods.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/covariates.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Data pipelining\ndef get_batched_data_fn(\n    batch_size: int = 128, \n    context_len: int = 120, \n    horizon_len: int = 24,\n):\n  examples = defaultdict(list)\n\n  num_examples = 0\n  for country in (\"FR\", \"BE\"):\n    sub_df = df[df[\"unique_id\"] == country]\n    for start in range(0, len(sub_df) - (context_len + horizon_len), horizon_len):\n      num_examples += 1\n      examples[\"country\"].append(country)\n      examples[\"inputs\"].append(sub_df[\"y\"][start:(context_end := start + context_len)].tolist())\n      examples[\"gen_forecast\"].append(sub_df[\"gen_forecast\"][start:context_end + horizon_len].tolist())\n      examples[\"week_day\"].append(sub_df[\"week_day\"][start:context_end + horizon_len].tolist())\n      examples[\"outputs\"].append(sub_df[\"y\"][context_end:(context_end + horizon_len)].tolist())\n  \n  def data_fn():\n    for i in range(1 + (num_examples - 1) // batch_size):\n      yield {k: v[(i * batch_size) : ((i + 1) * batch_size)] for k, v in examples.items()}\n  \n  return data_fn\n```\n\n----------------------------------------\n\nTITLE: Loading Adapter Checkpoint for TimesFM Model\nDESCRIPTION: Loads adapter weights into the TimesFM model using the specified checkpoint path. This implements adapter-based fine-tuning with LoRA (Low-Rank Adaptation) and DoRA (Weight-Decomposed Low-Rank Adaptation) techniques for efficient model adaptation.\nSOURCE: https://github.com/google-research/timesfm/blob/master/peft/usage.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nload_adapter_checkpoint(\n    model=tfm,\n    adapter_checkpoint_path=\"./checkpoints/run_20240716_163900_lyo4psz3\",\n    lora_rank=1,\n    lora_target_modules=\"all\",\n    use_dora=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Configuring Dataset for TimeSeriesdata Class\nDESCRIPTION: Loads a specific dataset (ettm1) from the ETT collection and configures the TimeSeriesdata loader with parameters for context length, prediction length, and data normalization, establishing train/validation/test splits.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndataset = \"ettm1\"\ndata_path = DATA_DICT[dataset][\"data_path\"]\nfreq = DATA_DICT[dataset][\"freq\"]\nint_freq = timesfm.freq_map(freq)\nboundaries = DATA_DICT[dataset][\"boundaries\"]\n\ndata_df = pd.read_csv(open(data_path, \"r\"))\n\n\nts_cols = [col for col in data_df.columns if col != \"date\"]\nnum_cov_cols = None\ncat_cov_cols = None\n\ncontext_len = 512\npred_len = 96\n\nnum_ts = len(ts_cols)\nbatch_size = 8\n\ndtl = data_loader.TimeSeriesdata(\n      data_path=data_path,\n      datetime_col=\"date\",\n      num_cov_cols=num_cov_cols,\n      cat_cov_cols=cat_cov_cols,\n      ts_cols=np.array(ts_cols),\n      train_range=[0, boundaries[0]],\n      val_range=[boundaries[0], boundaries[1]],\n      test_range=[boundaries[1], boundaries[2]],\n      hist_len=context_len,\n      pred_len=pred_len,\n      batch_size=num_ts,\n      freq=freq,\n      normalize=True,\n      epoch_len=None,\n      holiday=False,\n      permute=True,\n  )\n```\n\n----------------------------------------\n\nTITLE: Creating TensorFlow Dataset Batches for Training and Evaluation\nDESCRIPTION: Generates TensorFlow dataset iterators for training, validation, and testing with appropriate batch sizes and shift parameters for time series forecasting.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain_batches = dtl.tf_dataset(mode=\"train\", shift=1).batch(batch_size)\nval_batches = dtl.tf_dataset(mode=\"val\", shift=pred_len)\ntest_batches = dtl.tf_dataset(mode=\"test\", shift=pred_len)\n```\n\n----------------------------------------\n\nTITLE: Transferring Pretrained TimesFM Weights to Finetuning Model\nDESCRIPTION: Copies the pretrained TimesFM model parameters to the core layer of the finetuning model state, initializing with pretrained weights before training.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\njax_model_states.mdl_vars['params']['core_layer'] = tfm._train_state.mdl_vars['params']\njax_vars = jax_model_states.mdl_vars\ngc.collect()\n```\n\n----------------------------------------\n\nTITLE: Preparing Time Series Dataset for Evaluation\nDESCRIPTION: Sets up the data loading pipeline for a specific time series dataset (ettm1). Configures parameters such as context length, prediction length, and batch size, and initializes the TimeSeriesdata loader with appropriate data ranges and processing options.\nSOURCE: https://github.com/google-research/timesfm/blob/master/peft/usage.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndataset = \"ettm1\"\ndata_path = DATA_DICT[dataset][\"data_path\"]\nfreq = DATA_DICT[dataset][\"freq\"]\nint_freq = freq_map(freq)\nboundaries = DATA_DICT[dataset][\"boundaries\"]\n\ndata_df = pd.read_csv(open(data_path, \"r\"))\n\nts_cols = [col for col in data_df.columns if col != \"date\"]\nnum_cov_cols = None\ncat_cov_cols = None\n\ncontext_len = 512\npred_len = 96\n\nnum_ts = len(ts_cols)\nbatch_size = 16\n\ndtl = data_loader.TimeSeriesdata(\n    data_path=data_path,\n    datetime_col=\"date\",\n    num_cov_cols=num_cov_cols,\n    cat_cov_cols=cat_cov_cols,\n    ts_cols=np.array(ts_cols),\n    train_range=[0, boundaries[0]],\n    val_range=[boundaries[0], boundaries[1]],\n    test_range=[boundaries[1], boundaries[2]],\n    hist_len=context_len,\n    pred_len=pred_len,\n    batch_size=num_ts,\n    freq=\"15min\",\n    normalize=True,\n    epoch_len=None,\n    holiday=False,\n    permute=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Pandas DataFrame\nDESCRIPTION: Shows how to use the TimesFM model for forecasting using a pandas DataFrame input. The example sets the frequency to monthly.\nSOURCE: https://github.com/google-research/timesfm/blob/master/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\n# e.g. input_df is\n#       unique_id  ds          y\n# 0     T1         1975-12-31  697458.0\n```\n\n----------------------------------------\n\nTITLE: Computing MAE on Test Set\nDESCRIPTION: Evaluates model performance on test set using Mean Absolute Error (MAE) metric for forecasting accuracy.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nmae_losses = []\nfor batch in tqdm(test_batches.as_numpy_iterator()):\n    past = batch[0]\n    actuals = batch[3]\n    _, forecasts = tfm.forecast(list(past), [0] * past.shape[0])\n    forecasts = forecasts[:, 0 : actuals.shape[1], 5]\n    mae_losses.append(np.abs(forecasts - actuals).mean())\n\nprint(f\"MAE: {np.mean(mae_losses)}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring the Patched Decoder for TimesFM Finetuning\nDESCRIPTION: Creates a model configuration using PatchedDecoderFinetuneModel with the core layer template from the pretrained TimesFM model for finetuning.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmodel = pax_fiddle.Config(\n    patched_decoder.PatchedDecoderFinetuneModel,\n    name='patched_decoder_finetune',\n    core_layer_tpl=tfm.model_p,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining MSE and MAE Metrics for Forecast Evaluation\nDESCRIPTION: Implements Mean Squared Error (MSE) and Mean Absolute Error (MAE) functions to evaluate the accuracy of time series forecasts.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/covariates.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Define metrics\ndef mse(y_pred, y_true):\n  y_pred = np.array(y_pred)\n  y_true = np.array(y_true)\n  return np.mean(np.square(y_pred - y_true), axis=1, keepdims=True)\n\ndef mae(y_pred, y_true):\n  y_pred = np.array(y_pred)\n  y_true = np.array(y_true)\n  return np.mean(np.abs(y_pred - y_true), axis=1, keepdims=True)\n```\n\n----------------------------------------\n\nTITLE: Loading Best Checkpoint\nDESCRIPTION: Loads the best checkpoint based on validation loss and prepares the model for inference.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ntrain_state = checkpoints.restore_checkpoint(jax_model_states, CHECKPOINT_DIR)\nprint(train_state.step)\ntfm._train_state.mdl_vars['params'] = train_state.mdl_vars['params']['core_layer']\ntfm.jit_decode()\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Model State with Training Batch Processing\nDESCRIPTION: Initializes the JAX model state with batch processing functions for training and evaluation, defining data transformations from raw batches to model inputs.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\njax_task = task_p\nkey, init_key = jax.random.split(key)\n\n# To correctly prepare a batch of data for model initialization (now that shape\n# inference is merged), we take one devices*batch_size tensor tuple of data,\n# slice out just one batch, then run the prepare_input_batch function over it.\n\n\ndef process_train_batch(batch):\n    past_ts = batch[0].reshape(batch_size * num_ts, -1)\n    actual_ts = batch[3].reshape(batch_size * num_ts, -1)\n    return NestedMap(input_ts=past_ts, actual_ts=actual_ts)\n\n\ndef process_eval_batch(batch):\n    past_ts = batch[0]\n    actual_ts = batch[3]\n    return NestedMap(input_ts=past_ts, actual_ts=actual_ts)\n\n\njax_model_states, _ = trainer_lib.initialize_model_state(\n    jax_task,\n    init_key,\n    process_train_batch(tbatch),\n    checkpoint_type=checkpoint_types.CheckpointType.GDA,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Toy Data Examples for Covariate Formatting\nDESCRIPTION: Creates a small toy dataset to demonstrate the required format for covariates when using the forecast_with_covariates method, showing how different types of covariates should be structured.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/covariates.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntoy_input_pipeline = get_batched_data_fn(batch_size=2, context_len=5, horizon_len=2)\nprint(next(toy_input_pipeline()))\n```\n\n----------------------------------------\n\nTITLE: Processing Initial Training Batch and Checking Dimensions\nDESCRIPTION: Extracts and displays the first batch from the training dataset to verify data format and dimensions before model training.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfor tbatch in tqdm(train_batches.as_numpy_iterator()):\n    break\nprint(tbatch[0].shape)\n```\n\n----------------------------------------\n\nTITLE: Defining Dataset Configurations for Time Series Forecasting\nDESCRIPTION: Creates a dictionary mapping dataset names to their configurations including data boundaries, file paths, and frequency settings. This configuration is used for loading and processing various time series datasets including ETT, electricity, traffic and weather.\nSOURCE: https://github.com/google-research/timesfm/blob/master/peft/usage.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nDATA_DICT = {\n    \"ettm2\": {\n        \"boundaries\": [34560, 46080, 57600],\n        \"data_path\": \"../datasets/ETT-small/ETTm2.csv\",\n        \"freq\": \"15min\",\n    },\n    \"ettm1\": {\n        \"boundaries\": [34560, 46080, 57600],\n        \"data_path\": \"../datasets/ETT-small/ETTm1.csv\",\n        \"freq\": \"15min\",\n    },\n    \"etth2\": {\n        \"boundaries\": [8640, 11520, 14400],\n        \"data_path\": \"../datasets/ETT-small/ETTh2.csv\",\n        \"freq\": \"H\",\n    },\n    \"etth1\": {\n        \"boundaries\": [8640, 11520, 14400],\n        \"data_path\": \"../datasets/ETT-small/ETTh1.csv\",\n        \"freq\": \"H\",\n    },\n    \"elec\": {\n        \"boundaries\": [18413, 21044, 26304],\n        \"data_path\": \"../datasets/electricity/electricity.csv\",\n        \"freq\": \"H\",\n    },\n    \"traffic\": {\n        \"boundaries\": [12280, 14036, 17544],\n        \"data_path\": \"../datasets/traffic/traffic.csv\",\n        \"freq\": \"H\",\n    },\n    \"weather\": {\n        \"boundaries\": [36887, 42157, 52696],\n        \"data_path\": \"../datasets/weather/weather.csv\",\n        \"freq\": \"10min\",\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Defining ETT Dataset Configuration Dictionary\nDESCRIPTION: Creates a dictionary containing configuration details for various time series datasets including ETT and others, with specifications for data paths, time frequencies, and data split boundaries.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nDATA_DICT = {\n    \"ettm2\": {\n        \"boundaries\": [34560, 46080, 57600],\n        \"data_path\": \"../datasets/ETT-small/ETTm2.csv\",\n        \"freq\": \"15min\",\n    },\n    \"ettm1\": {\n        \"boundaries\": [34560, 46080, 57600],\n        \"data_path\": \"../datasets/ETT-small/ETTm1.csv\",\n        \"freq\": \"15min\",\n    },\n    \"etth2\": {\n        \"boundaries\": [8640, 11520, 14400],\n        \"data_path\": \"../datasets/ETT-small/ETTh2.csv\",\n        \"freq\": \"H\",\n    },\n    \"etth1\": {\n        \"boundaries\": [8640, 11520, 14400],\n        \"data_path\": \"../datasets/ETT-small/ETTh1.csv\",\n        \"freq\": \"H\",\n    },\n    \"elec\": {\n        \"boundaries\": [18413, 21044, 26304],\n        \"data_path\": \"../datasets/electricity/electricity.csv\",\n        \"freq\": \"H\",\n    },\n    \"traffic\": {\n        \"boundaries\": [12280, 14036, 17544],\n        \"data_path\": \"../datasets/traffic/traffic.csv\",\n        \"freq\": \"H\",\n    },\n    \"weather\": {\n        \"boundaries\": [36887, 42157, 52696],\n        \"data_path\": \"../datasets/weather/weather.csv\",\n        \"freq\": \"10min\",\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Training Configuration Parameters\nDESCRIPTION: Sets up training hyperparameters including number of epochs, patience for early stopping, evaluation frequency, and checkpoint directory.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nbest_eval_loss = 1e7\nstep_count = 0\npatience = 0\nNUM_EPOCHS = 100\nPATIENCE = 5\nTRAIN_STEPS_PER_EVAL = 1000\nCHECKPOINT_DIR='/home/senrajat_google_com/ettm1_finetune'\n```\n\n----------------------------------------\n\nTITLE: Replicating JAX Model States\nDESCRIPTION: Replicates model states and variables across devices for distributed training.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nreplicated_jax_states = trainer_lib.replicate_model_state(jax_model_states)\nreplicated_jax_vars = replicated_jax_states.mdl_vars\n```\n\n----------------------------------------\n\nTITLE: Loading TimesFM Model with JAX Backend\nDESCRIPTION: Code to import and initialize the TimesFM model using a 2.0-500m checkpoint from HuggingFace with GPU backend configuration.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/covariates.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport timesfm\ntimesfm_backend = \"gpu\"  # @param\n\nmodel = timesfm.TimesFm(\n      hparams=timesfm.TimesFmHparams(\n          backend=timesfm_backend,\n          per_core_batch_size=32,\n          horizon_len=128,\n          num_layers=50,\n          use_positional_embedding=False,\n          context_len=2048,\n      ),\n      checkpoint=timesfm.TimesFmCheckpoint(\n          huggingface_repo_id=\"google/timesfm-2.0-500m-jax\"),\n  )\n```\n\n----------------------------------------\n\nTITLE: Creating Test Dataset Batches\nDESCRIPTION: Generates TensorFlow dataset batches for testing using the previously configured data loader. The batches are created with a shift equal to the prediction length for proper testing evaluation.\nSOURCE: https://github.com/google-research/timesfm/blob/master/peft/usage.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntest_batches = dtl.tf_dataset(mode=\"test\", shift=pred_len)\n```\n\n----------------------------------------\n\nTITLE: Implementing Batch Reshape Function for Parallel Training\nDESCRIPTION: Helper function to reshape input batches for parallel processing across multiple devices using JAX's pmap.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef reshape_batch_for_pmap(batch, num_devices):\n  def _reshape(input_tensor):\n    bsize = input_tensor.shape[0]\n    residual_shape = list(input_tensor.shape[1:])\n    nbsize = bsize // num_devices\n    return jnp.reshape(input_tensor, [num_devices, nbsize] + residual_shape)\n\n  return jax.tree.map(_reshape, batch)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Praxis and JAX Configuration Shortcuts\nDESCRIPTION: Defines shortcuts and constants for Praxis/JAX configuration, initializes random key for reproducibility, and sets up type references for model components.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# PAX shortcuts\nNestedMap = py_utils.NestedMap\nWeightInit = base_layer.WeightInit\nWeightHParams = base_layer.WeightHParams\nInstantiableParams = py_utils.InstantiableParams\nJTensor = pytypes.JTensor\nNpTensor = pytypes.NpTensor\nWeightedScalars = pytypes.WeightedScalars\ninstantiate = base_hyperparams.instantiate\nLayerTpl = pax_fiddle.Config[base_layer.BaseLayer]\nAuxLossStruct = base_layer.AuxLossStruct\n\nAUX_LOSS = base_layer.AUX_LOSS\ntemplate_field = base_layer.template_field\n\n# Standard prng key names\nPARAMS = base_layer.PARAMS\nRANDOM = base_layer.RANDOM\n\nkey = jax.random.PRNGKey(seed=1234)\n```\n\n----------------------------------------\n\nTITLE: Running TimesFM Benchmark on ETT Dataset\nDESCRIPTION: Command-line example for running TimesFM on the benchmark using the ETTh1 dataset. It specifies the model path, backend, prediction length, context length, and dataset.\nSOURCE: https://github.com/google-research/timesfm/blob/master/experiments/long_horizon_benchmarks/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run python3 -m experiments.long_horizon_benchmarks.run_eval \\\n--model_path=google/timesfm-1.0-200m --backend=\"gpu\" \\\n--pred_len=96 --context_len=512 --dataset=etth1\n```\n\n----------------------------------------\n\nTITLE: Initializing TimesFM Model Configuration\nDESCRIPTION: Function to initialize and configure the TimesFM model with specified hyperparameters. Handles model creation, weight loading, and device selection based on CUDA availability.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning_torch.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef get_model(load_weights: bool = False):\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  repo_id = \"google/timesfm-2.0-500m-pytorch\"\n  hparams = TimesFmHparams(\n      backend=device,\n      per_core_batch_size=32,\n      horizon_len=128,\n      num_layers=50,\n      use_positional_embedding=False,\n      context_len=192,\n  )\n  tfm = TimesFm(hparams=hparams,\n                checkpoint=TimesFmCheckpoint(huggingface_repo_id=repo_id))\n\n  model = PatchedTimeSeriesDecoder(tfm._model_config)\n  if load_weights:\n    checkpoint_path = path.join(snapshot_download(repo_id), \"torch_model.ckpt\")\n    loaded_checkpoint = torch.load(checkpoint_path, weights_only=True)\n    model.load_state_dict(loaded_checkpoint)\n  return model, hparams, tfm._model_config\n```\n\n----------------------------------------\n\nTITLE: Running TimesFM on Extended Benchmarks\nDESCRIPTION: Command to execute TimesFM on the extended benchmarks. It specifies the model path and backend to use for the experiment.\nSOURCE: https://github.com/google-research/timesfm/blob/master/experiments/extended_benchmarks/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run python3 -m experiments.extended_benchmarks.run_timesfm --model_path=google/timesfm-1.0-200m(-pytorch) --backend=\"gpu\"\n```\n\n----------------------------------------\n\nTITLE: Importing TimesFM Core Libraries and Data Utilities\nDESCRIPTION: Imports the TimesFM library and related data handling utilities required for time series forecasting and model manipulation.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport timesfm\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom timesfm import patched_decoder\nfrom timesfm import data_loader\n```\n\n----------------------------------------\n\nTITLE: Importing JAX and Praxis Libraries for Model Finetuning\nDESCRIPTION: Imports JAX, Praxis, and PAX libraries required for model finetuning, including components for optimization, checkpointing, and parameter management.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nfrom jax import numpy as jnp\nfrom praxis import pax_fiddle\nfrom praxis import py_utils\nfrom praxis import pytypes\nfrom praxis import base_model\nfrom praxis import optimizers\nfrom praxis import schedules\nfrom praxis import base_hyperparams\nfrom praxis import base_layer\nfrom paxml import tasks_lib\nfrom paxml import trainer_lib\nfrom paxml import checkpoints\nfrom paxml import learners\nfrom paxml import partitioning\nfrom paxml import checkpoint_types\n```\n\n----------------------------------------\n\nTITLE: Running the Fine-Tuning Script with Predefined Parameters in Zsh\nDESCRIPTION: Example command to run the fine-tuning script with predefined hyperparameters by sourcing the finetune.sh shell script, which executes finetune.py with preset configurations.\nSOURCE: https://github.com/google-research/timesfm/blob/master/peft/README.md#2025-04-23_snippet_0\n\nLANGUAGE: zsh\nCODE:\n```\nsource finetune.sh\n```\n\n----------------------------------------\n\nTITLE: Running Amazon-Chronos Benchmark on ETT Dataset\nDESCRIPTION: Command-line example for running Amazon-Chronos on the benchmark using the ETTh1 dataset. It specifies the model path, backend, prediction length, context length, and dataset.\nSOURCE: https://github.com/google-research/timesfm/blob/master/experiments/long_horizon_benchmarks/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run python3 -m experiments.long_horizon_benchmarks.run_eval \\\n--model_path=amazon/chronos-t5-mini --backend=\"gpu\" \\\n--pred_len=96 --context_len=512 --dataset=etth1\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for TimesFM Benchmarks\nDESCRIPTION: Instructions for installing required packages using Poetry to run the TimesFM benchmarks. This includes adding Gluon-TS and Amazon-Chronos from GitHub repositories.\nSOURCE: https://github.com/google-research/timesfm/blob/master/experiments/long_horizon_benchmarks/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry add git+https://github.com/awslabs/gluon-ts.git\npoetry add git+https://github.com/amazon-science/chronos-forecasting.git\npoetry lock\npoetry install --only pax\n```\n\n----------------------------------------\n\nTITLE: Displaying Fine-Tuning Script Help Information in Zsh\nDESCRIPTION: Command to view the complete list of available options and their descriptions for the fine-tuning script by running finetune.py with the --help flag.\nSOURCE: https://github.com/google-research/timesfm/blob/master/peft/README.md#2025-04-23_snippet_1\n\nLANGUAGE: zsh\nCODE:\n```\npython3 finetune.py --help\n```\n\n----------------------------------------\n\nTITLE: Importing Visualization and Progress Tracking Libraries\nDESCRIPTION: Imports libraries for visualization (Matplotlib), interactive display (IPython), and progress tracking (tqdm) to monitor and display model training and evaluation results.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm import tqdm\nimport dataclasses\nimport IPython\nimport IPython.display\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rcParams['figure.figsize'] = (8, 6)\nmpl.rcParams['axes.grid'] = False\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for TimesFM Benchmarks\nDESCRIPTION: Commands for installing required packages to run TimesFM benchmarks. This includes adding the gluon-ts package from GitHub and installing either PAX or PyTorch dependencies.\nSOURCE: https://github.com/google-research/timesfm/blob/master/experiments/extended_benchmarks/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry add git+https://github.com/awslabs/gluon-ts.git\npoetry lock\npoetry install --only <pax or pytorch>\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for JAX/XLA in TimesFM\nDESCRIPTION: Sets environment variables to control JAX and XLA behavior, disabling preallocated memory and TensorStore use for PMAP operations.\nSOURCE: https://github.com/google-research/timesfm/blob/master/notebooks/finetuning.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\nos.environ['JAX_PMAP_USE_TENSORSTORE'] = 'false'\n```\n\n----------------------------------------\n\nTITLE: Installing JAX Dependencies for Covariate Support\nDESCRIPTION: Command to install the required JAX dependencies when using forecast_with_covariates functionality in the torch version of TimesFM.\nSOURCE: https://github.com/google-research/timesfm/blob/master/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install jax jaxlib\n```\n\n----------------------------------------\n\nTITLE: YAPF Formatting Configuration\nDESCRIPTION: Configuration settings for the YAPF code formatter, specifying Google-based style with custom indentation and spacing rules for maintaining consistent code formatting.\nSOURCE: https://github.com/google-research/timesfm/blob/master/README.md#2025-04-23_snippet_6\n\nLANGUAGE: ini\nCODE:\n```\n[style]\nbased_on_style = google\n# Add your custom style rules here\nindent_width = 2\nspaces_before_comment = 2\n```"
  }
]
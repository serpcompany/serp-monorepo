[
  {
    "owner": "stanfordnlp",
    "repo": "dspy",
    "content": "TITLE: Building ReAct Agent with Tools\nDESCRIPTION: Implements a ReAct agent that combines math evaluation and Wikipedia search tools for complex problem solving.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/index.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_math(expression: str):\n    return dspy.PythonInterpreter({}).execute(expression)\n\ndef search_wikipedia(query: str):\n    results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3)\n    return [x['text'] for x in results]\n\nreact = dspy.ReAct(\"question -> answer: float\", tools=[evaluate_math, search_wikipedia])\n\npred = react(question=\"What is 9362158 divided by the year of birth of David Gregory of Kinnairdy castle?\")\nprint(pred.answer)\n```\n\n----------------------------------------\n\nTITLE: Creating a Class-based DSPy Signature for Emotion Classification\nDESCRIPTION: This snippet illustrates how to define a more complex, class-based DSPy Signature for emotion classification. It uses typed fields and the Literal type to constrain the output to specific emotion categories.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/programming/signatures.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nclass Emotion(dspy.Signature):\n    \"\"\"Classify emotion.\"\"\"\n    \n    sentence: str = dspy.InputField()\n    sentiment: Literal['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'] = dspy.OutputField()\n\nsentence = \"i started feeling a little vulnerable when the giant spotlight started blinding me\"  # from dair-ai/emotion\n\nclassify = dspy.Predict(Emotion)\nclassify(sentence=sentence)\n```\n\n----------------------------------------\n\nTITLE: Building RAG System with Wikipedia Search\nDESCRIPTION: Implements a Retrieval-Augmented Generation system using Wikipedia search and ChainOfThought module for question answering.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/index.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef search_wikipedia(query: str) -> list[str]:\n    results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3)\n    return [x['text'] for x in results]\n\nrag = dspy.ChainOfThought('context, question -> response')\n\nquestion = \"What's the name of the castle that David Gregory inherited?\"\nrag(context=search_wikipedia(question), question=question)\n```\n\n----------------------------------------\n\nTITLE: Instantiating ReAct Module with BasicQA Signature in Python\nDESCRIPTION: This snippet demonstrates how to define a simple BasicQA signature and instantiate the ReAct module with it in DSPy. It sets up a question-answering task with a single input and output field.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/modules/react.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Define a simple signature for basic question answering\nclass BasicQA(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n\n# Pass signature to ReAct module\nreact_module = dspy.ReAct(BasicQA)\n```\n\n----------------------------------------\n\nTITLE: Configuring Language Models in DSPy\nDESCRIPTION: Sets up two language models for DSPy: a smaller Llama-3.1-8B-Instruct as the main model and GPT-4o as an optional teacher model. Configures DSPy to use the Llama model as the default.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\n\nlm = dspy.LM('<your_provider>/Llama-3.1-8B-Instruct', max_tokens=3000)\ngpt4o = dspy.LM('openai/gpt-4o', max_tokens=3000)\n\ndspy.configure(lm=lm)\n```\n\n----------------------------------------\n\nTITLE: Creating Multi-Stage Article Generation Pipeline\nDESCRIPTION: Builds a complex pipeline for article generation using multiple DSPy modules for outlining and section drafting.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/index.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass Outline(dspy.Signature):\n    \"\"\"Outline a thorough overview of a topic.\"\"\"\n    \n    topic: str = dspy.InputField()\n    title: str = dspy.OutputField()\n    sections: list[str] = dspy.OutputField()\n    section_subheadings: dict[str, list[str]] = dspy.OutputField(desc=\"mapping from section headings to subheadings\")\n\nclass DraftSection(dspy.Signature):\n    \"\"\"Draft a top-level section of an article.\"\"\"\n    \n    topic: str = dspy.InputField()\n    section_heading: str = dspy.InputField()\n    section_subheadings: list[str] = dspy.InputField()\n    content: str = dspy.OutputField(desc=\"markdown-formatted section\")\n\nclass DraftArticle(dspy.Module):\n    def __init__(self):\n        self.build_outline = dspy.ChainOfThought(Outline)\n        self.draft_section = dspy.ChainOfThought(DraftSection)\n\n    def forward(self, topic):\n        outline = self.build_outline(topic=topic)\n        sections = []\n        for heading, subheadings in outline.section_subheadings.items():\n            section, subheadings = f\"## {heading}\", [f\"### {subheading}\" for subheading in subheadings]\n            section = self.draft_section(topic=outline.title, section_heading=section, section_subheadings=subheadings)\n            sections.append(section.content)\n        return dspy.Prediction(title=outline.title, sections=sections)\n\ndraft_article = DraftArticle()\narticle = draft_article(topic=\"World Cup 2002\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Citation Faithfulness Checker with DSPy ChainOfThought\nDESCRIPTION: This example demonstrates a more complex class-based Signature for checking the faithfulness of text to provided context. It uses InputField and OutputField with descriptions to guide the language model's behavior.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/programming/signatures.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass CheckCitationFaithfulness(dspy.Signature):\n    \"\"\"Verify that the text is based on the provided context.\"\"\"\n\n    context: str = dspy.InputField(desc=\"facts here are assumed to be true\")\n    text: str = dspy.InputField()\n    faithfulness: bool = dspy.OutputField()\n    evidence: dict[str, list[str]] = dspy.OutputField(desc=\"Supporting evidence for claims\")\n\ncontext = \"The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\"\n\ntext = \"Lee scored 3 goals for Colchester United.\"\n\nfaithfulness = dspy.ChainOfThought(CheckCitationFaithfulness)\nfaithfulness(context=context, text=text)\n```\n\n----------------------------------------\n\nTITLE: Configuring DSPy Program Documentation Options in YAML\nDESCRIPTION: YAML configuration that specifies how the DSPy Program class documentation should be generated. Defines which members to include, source code visibility, heading structure, and inheritance handling. Configuration includes settings for method display, docstring formatting, and path representation.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/api/modules/Program.md#2025-04-07_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndspy.Program:\n    handler: python\n    options:\n        members:\n            - __call__\n            - batch\n            - deepcopy\n            - dump_state\n            - get_lm\n            - load\n            - load_state\n            - map_named_predictors\n            - named_parameters\n            - named_predictors\n            - named_sub_modules\n            - parameters\n            - predictors\n            - reset_copy\n            - save\n            - set_lm\n        show_source: true\n        show_root_heading: true\n        heading_level: 2\n        docstring_style: google\n        show_root_full_path: true\n        show_object_full_path: false\n        separate_signature: false\n        inherited_members: true\n```\n\n----------------------------------------\n\nTITLE: Concise implementation of multi-agent DSPy program\nDESCRIPTION: A condensed version of the main code, showcasing the core functionality of creating, optimizing, and aggregating ReAct agents in about 10 lines of DSPy code.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/multi_agent.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nagent = dspy.ReAct(\"question -> answer\", tools=[dspy.Retrieve(k=1)])\n\noptimizer = BootstrapFewShotWithRandomSearch(metric=dspy.evaluate.answer_exact_match)\noptimized_react = optimizer.compile(agent, trainset=trainset, valset=valset)\n\nclass Aggregator(dspy.Module):\n\tdef __init__(self):\n\t\tself.aggregate = dspy.ChainOfThought('context, question -> answer')\n\n\tdef forward(self, question):\n        preds = [agent(question=question) for agent in optimized_react.best_programs[:5]]\n\t\treturn self.aggregate(context=deduplicate(flatten([p.observations for p in preds])), question=question)\n\t\noptimized_aggregator = optimizer.compile(aggregator, trainset=trainset, valset=valset)\n\n# Use it!\noptimized_aggregator(question=\"How many storeys are in the castle that David Gregory inherited?\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Predict Class with Signature Processing in Python\nDESCRIPTION: The constructor for the Predict class that initializes attributes, processes signature inputs and outputs, generates instructions, and creates templates. It handles both string-based signatures and other signature types.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/modules/predict.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Predict(Parameter):\n    def __init__(self, signature, **config):\n        self.stage = random.randbytes(8).hex()\n        self.signature = signature\n        self.config = config\n        self.reset()\n\n        if isinstance(signature, str):\n            inputs, outputs = signature.split(\"->\") \n            inputs, outputs = inputs.split(\",\"), outputs.split(\",\")\n            inputs, outputs = [field.strip() for field in inputs], [field.strip() for field in outputs]\n\n            assert all(len(field.split()) == 1 for field in (inputs + outputs))\n\n            inputs_ = ', '.join([f\"`{field}`\" for field in inputs])\n            outputs_ = ', '.join([f\"`{field}`\" for field in outputs])\n\n            instructions = f\"\"\"Given the fields {inputs_}, produce the fields {outputs_}.\"\"\"\n\n            inputs = {k: InputField() for k in inputs}\n            outputs = {k: OutputField() for k in outputs}\n\n            for k, v in inputs.items():\n                v.finalize(k, infer_prefix(k))\n            \n            for k, v in outputs.items():\n                v.finalize(k, infer_prefix(k))\n\n            self.signature = dsp.Template(instructions, **inputs, **outputs)\n```\n\n----------------------------------------\n\nTITLE: Creating Sentiment Classification Module\nDESCRIPTION: Defines a custom classification signature for sentiment analysis with typed outputs and confidence scores.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/index.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nclass Classify(dspy.Signature):\n    \"\"\"Classify sentiment of a given sentence.\"\"\"\n    \n    sentence: str = dspy.InputField()\n    sentiment: Literal['positive', 'negative', 'neutral'] = dspy.OutputField()\n    confidence: float = dspy.OutputField()\n\nclassify = dspy.Predict(Classify)\nclassify(sentence=\"This book was super fun to read, though not the last chapter.\")\n```\n\n----------------------------------------\n\nTITLE: Configuring KNN Documentation Parameters in YAML\nDESCRIPTION: YAML configuration block that defines documentation generation parameters for the DSPy KNN class. Specifies handler type, member inclusion, source code visibility, heading formatting, and inheritance settings.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/api/optimizers/KNN.md#2025-04-07_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n::: dspy.KNN\n    handler: python\n    options:\n        members:\n            - __call__\n        show_source: true\n        show_root_heading: true\n        heading_level: 2\n        docstring_style: google\n        show_root_full_path: true\n        show_object_full_path: false\n        separate_signature: false\n        inherited_members: true\n```\n\n----------------------------------------\n\nTITLE: Defining Assessment Signature for AI Feedback\nDESCRIPTION: Implementation of a DSPy Signature class for automatic quality assessment of text outputs.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/evaluation/metrics.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass Assess(dspy.Signature):\n    \"\"\"Assess the quality of a tweet along the specified dimension.\"\"\"\n\n    assessed_text = dspy.InputField()\n    assessment_question = dspy.InputField()\n    assessment_answer: bool = dspy.OutputField()\n```\n\n----------------------------------------\n\nTITLE: Defining Breakdown Signature for Summary Evaluation in DSPy\nDESCRIPTION: Creates a signature class that breaks down a passage into key ideas and assigns importance grades (High, Medium, or Low) to each idea. This signature is used as part of the metric program for evaluating summary quality.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/summarization.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\n\nclass Breakdown(dspy.Signature):\n    \"\"\"\n    Given a passage, break down the passage into key ideas.\n    Enumerate every key idea in the passage and\n    assign it an importance grade\n    (High, Medium, or Low).\n    \"\"\"\n\n    passage = dspy.InputField()\n    key_ideas: str = dspy.OutputField(\n        desc=\"numbered list of one key idea per line,\"\n        \"followed by its importance grade, \"\n             \"e.g. 1. <Idea here>. High.\")\n    importance_grades: list[str] = dspy.OutputField(\n        desc='list of importance grades, '\n             'e.g. [\"High\", \"Medium\", \"Low\"].')\n```\n\n----------------------------------------\n\nTITLE: Implementing Metric Module for Summary Evaluation in DSPy\nDESCRIPTION: Creates a module that computes a weighted score for summary correctness using the Breakdown and SummaryCorrectness signatures. It weights key ideas based on importance and calculates a final score between 0 and 1.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/summarization.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Metric(dspy.Module):\n    \"\"\"\n    Compute a score for the correctness of a summary.\n    \"\"\"\n\n    def __init__(self):\n        self.breakdown = dspy.ChainOfThought(Breakdown)\n        self.assess = dspy.ChainOfThought(SummaryCorrectness)\n\n    def forward(self, example, pred, trace=None):\n        breakdown = self.breakdown(\n            passage=example.passage\n        )\n        key_ideas = breakdown.key_ideas\n        importance_grades = breakdown.importance_grades\n\n        scores = self.assess(\n            key_ideas=key_ideas,\n            summary=pred.summary,\n        )\n\n        try:\n            weight_map = {'High': 1.0, 'Medium': 0.7}\n            score = sum(\n                weight_map.get(g, 0.2) * int(b)\n                for g, b in zip(importance_grades, scores.binary_scores)\n            )\n            score /= sum(weight_map.get(g, 0.2) for g in importance_grades)\n\n        # pylint: disable=broad-except\n        except Exception:\n            score = float(scores.overall_score)\n\n        return score if trace is None else score >= 0.75\n```\n\n----------------------------------------\n\nTITLE: Initializing BootstrapFewShot Optimizer\nDESCRIPTION: Creates a BootstrapFewShot optimizer with configuration for the number of demonstrations, training rounds, and evaluation metric. This optimizer will improve the model by bootstrapping optimal few-shot examples.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/optimizers/bootstrap-fewshot.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.teleprompt import BootstrapFewShot\n\noptimizer = BootstrapFewShot(\n    metric=gsm8k_metric,\n    max_bootstrapped_demos=8,\n    max_labeled_demos=8,\n    max_rounds=10,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Hop Search Module in Python using DSPy\nDESCRIPTION: This code snippet defines a 'Hop' class that implements a multi-hop search module using DSPy. It generates queries, performs searches, and accumulates notes and titles over multiple hops. The module uses ChainOfThought components for query generation and note appending.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/programming/modules.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Hop(dspy.Module):\n    def __init__(self, num_docs=10, num_hops=4):\n        self.num_docs, self.num_hops = num_docs, num_hops\n        self.generate_query = dspy.ChainOfThought('claim, notes -> query')\n        self.append_notes = dspy.ChainOfThought('claim, notes, context -> new_notes: list[str], titles: list[str]')\n\n    def forward(self, claim: str) -> list[str]:\n        notes = []\n        titles = []\n\n        for _ in range(self.num_hops):\n            query = self.generate_query(claim=claim, notes=notes).query\n            context = search(query, k=self.num_docs)\n            prediction = self.append_notes(claim=claim, notes=notes, context=context)\n            notes.extend(prediction.new_notes)\n            titles.extend(prediction.titles)\n        \n        return dspy.Prediction(notes=notes, titles=list(set(titles)))\n```\n\n----------------------------------------\n\nTITLE: Evaluating MIPRO-Optimized Question Answering Program in Python\nDESCRIPTION: This code evaluates the MIPRO-optimized question answering program on both the training and validation sets, showing improvements in performance compared to the baseline.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/qa/hotpot/hotpotqa_with_MIPRO.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nbayesian_train_score = evaluate(compiled_program, devset=trainset)\nbayesian_eval_score = evaluate(compiled_program, devset=valset)\n```\n\n----------------------------------------\n\nTITLE: Evaluating CoT Baseline Model\nDESCRIPTION: Initializes and evaluates the baseline Chain of Thought model using the previously defined evaluator and GSM8K metric.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/optimizers/bootstrap-fewshot.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncot_baseline = CoT()\n\nevaluate(cot_baseline)\n```\n\n----------------------------------------\n\nTITLE: Bootstrapped Finetuning with Metric in Python\nDESCRIPTION: This snippet demonstrates bootstrapped finetuning using a metric. It creates an optimizer with BootstrapFinetune and compiles the student classifier using a teacher and training set.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\noptimizer = dspy.BootstrapFinetune(num_threads=16, metric=metric)\nclassify_ft = optimizer.compile(student_classify, teacher=teacher_classify, trainset=raw_data[:500])\n```\n\n----------------------------------------\n\nTITLE: Using ChainOfThoughtWithHint with a Basic QA Signature in Python\nDESCRIPTION: This code demonstrates how to define a BasicQA signature, instantiate a ChainOfThoughtWithHint module with it, and call the predictor with a question and hint. The example shows how to provide a hint to guide the reasoning process for answering a simple question about the color of the sky.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/modules/chain-of-thought-with-hint.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass BasicQA(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n\n#Pass signature to ChainOfThought module\ngenerate_answer = dspy.ChainOfThoughtWithHint(BasicQA)\n\n# Call the predictor on a particular input alongside a hint.\nquestion='What is the color of the sky?'\nhint = \"It's what you often see during a sunny day.\"\npred = generate_answer(question=question, hint=hint)\n\nprint(f\"Question: {question}\")\nprint(f\"Predicted Answer: {pred.answer}\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating the Summarization Program in DSPy\nDESCRIPTION: Creates an instance of the Summarize program and evaluates it using the custom metric function on the training dataset, displaying progress and results in a table format.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/summarization.md#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprogram = Summarize()\n\nevaluate = dspy.Evaluate(devset=trainset, metric=metric,\n                    display_progress=True,\n                    display_table=True, provide_traceback=True)\n\nres = evaluate(program, devset=trainset)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Defining Multi-Hop Retrieval Program in DSPy\nDESCRIPTION: This class implements a multi-hop retrieval program using DSPy modules. It generates queries and appends notes iteratively to find relevant titles for a given claim.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass Hop(dspy.Module):\n    def __init__(self, num_docs=10, num_hops=4):\n        self.num_docs, self.num_hops = num_docs, num_hops\n        self.generate_query = dspy.ChainOfThought('claim, notes -> query')\n        self.append_notes = dspy.ChainOfThought('claim, notes, context -> new_notes: list[str], titles: list[str]')\n\n    def forward(self, claim: str) -> list[str]:\n        notes = []\n        titles = []\n\n        for _ in range(self.num_hops):\n            query = self.generate_query(claim=claim, notes=notes).query\n            context = search(query, k=self.num_docs)\n            prediction = self.append_notes(claim=claim, notes=notes, context=context)\n            notes.extend(prediction.new_notes)\n            titles.extend(prediction.titles)\n        \n        return dspy.Prediction(notes=notes, titles=list(set(titles)))\n```\n\n----------------------------------------\n\nTITLE: Using DSPy Evaluate Utility\nDESCRIPTION: Implementation example using DSPy's built-in Evaluate utility for parallel evaluation with progress tracking and result display.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/evaluation/metrics.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.evaluate import Evaluate\n\n# Set up the evaluator, which can be re-used in your code.\nevaluator = Evaluate(devset=YOUR_DEVSET, num_threads=1, display_progress=True, display_table=5)\n\n# Launch evaluation.\nevaluator(YOUR_PROGRAM, metric=YOUR_METRIC)\n```\n\n----------------------------------------\n\nTITLE: Implementing SimplifiedBaleen Multi-Hop QA Pipeline\nDESCRIPTION: Builds a multi-hop question answering pipeline that generates search queries, retrieves passages, and produces answers through multiple iterations of information gathering.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/simplified-baleen.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dsp.utils import deduplicate\n\nclass SimplifiedBaleen(dspy.Module):\n    def __init__(self, passages_per_hop=3, max_hops=2):\n        super().__init__()\n\n        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n        self.max_hops = max_hops\n    \n    def forward(self, question):\n        context = []\n        \n        for hop in range(self.max_hops):\n            query = self.generate_query[hop](context=context, question=question).query\n            passages = self.retrieve(query).passages\n            context = deduplicate(context + passages)\n\n        pred = self.generate_answer(context=context, question=question)\n        return dspy.Prediction(context=context, answer=pred.answer)\n```\n\n----------------------------------------\n\nTITLE: Defining Validation Logic for DSPy Compilation\nDESCRIPTION: Implements a validation function that assesses the quality of predictions based on answer correctness, context relevance, and query characteristics.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/simplified-baleen.md#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef validate_context_and_answer_and_hops(example, pred, trace=None):\n    if not dspy.evaluate.answer_exact_match(example, pred): return False\n    if not dspy.evaluate.answer_passage_match(example, pred): return False\n\n    hops = [example.question] + [outputs.query for *_, outputs in trace if 'query' in outputs]\n\n    if max([len(h) for h in hops]) > 100: return False\n    if any(dspy.evaluate.answer_exact_match_str(hops[idx], hops[:idx], frac=0.8) for idx in range(2, len(hops))): return False\n\n    return True\n```\n\n----------------------------------------\n\nTITLE: Importing DSPy Bootstrap Optimizer\nDESCRIPTION: Code example showing how to import the BootstrapFewShotWithRandomSearch optimizer from DSPy's teleprompt module. This optimizer uses random search over generated demonstrations to optimize program performance.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/optimization/optimizers.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch\n```\n\n----------------------------------------\n\nTITLE: Initializing a DSPy ReAct Agent with ColBERT Retriever\nDESCRIPTION: Sets up a basic DSPy environment by configuring a GPT-4o-mini language model and a ColBERTv2 retriever using Wikipedia abstracts, then defines a simple ReAct agent with retrieval capabilities.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/observability/index.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\nfrom dspy.datasets import HotPotQA\n\nlm = dspy.LM('openai/gpt-4o-mini')\ncolbert = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\ndspy.configure(lm=lm, rm=colbert)\n\nagent = dspy.ReAct(\"question -> answer\", tools=[dspy.Retrieve(k=1)])\n```\n\n----------------------------------------\n\nTITLE: Compiling with BootstrapFinetune Teleprompter in Python\nDESCRIPTION: The compile method first performs bootstrapping with BootstrapFewShot teleprompter, then prepares data for fine-tuning by generating prompt-completion pairs, and finally performs model fine-tuning. It returns a compiled and fine-tuned predictor.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/optimizers/BootstrapFinetune.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef compile(self, student, *, teacher=None, trainset, valset=None, target='t5-large', bsize=12, accumsteps=1, lr=5e-5, epochs=1, bf16=False):\n```\n\n----------------------------------------\n\nTITLE: Compiling LongFormQA with Assertion-based Teacher\nDESCRIPTION: This pipeline uses assertions only in the teacher model during compilation. The LongFormQA student learns from a LongFormQAWithAssertions teacher that can self-correct, producing better bootstrapped examples while keeping the student model simple.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/longformqa/longformqa_assertions.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nlongformqa = LongFormQA()\nteleprompter = BootstrapFewShotWithRandomSearch(metric = answer_correctness, max_bootstrapped_demos=2, num_candidate_programs=6)\ncited_longformqa_teacher = teleprompter.compile(student=longformqa, teacher = assert_transform_module(LongFormQAWithAssertions().map_named_predictors(Retry), backtrack_handler), trainset=trainset, valset=devset[:25])\nevaluate(cited_longformqa_teacher)\n```\n\n----------------------------------------\n\nTITLE: Advanced Trace Validation for Query Hops\nDESCRIPTION: Advanced metric implementation that validates query hops using trace information during compilation optimization.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/evaluation/metrics.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef validate_hops(example, pred, trace=None):\n    hops = [example.question] + [outputs.query for *_, outputs in trace if 'query' in outputs]\n\n    if max([len(h) for h in hops]) > 100: return False\n    if any(dspy.evaluate.answer_exact_match_str(hops[idx], hops[:idx], frac=0.8) for idx in range(2, len(hops))): return False\n\n    return True\n```\n\n----------------------------------------\n\nTITLE: Defining DSPy Module\nDESCRIPTION: Creating and testing a ChainOfThought module for question answering\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/math/index.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodule = dspy.ChainOfThought(\"question -> answer\")\nmodule(question=example.question)\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading DSPy Programs with MLflow\nDESCRIPTION: Shows how to start an MLflow run, save a DSPy program as a model artifact, and load it back. Uses MLflow's DSPy integration for model persistence and versioning.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/agents/index.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Start an MLflow Run and save the program\nwith mlflow.start_run(run_name=\"optimized_rag\"):\n    model_info = mlflow.dspy.log_model(\n        optimized_react,\n        artifact_path=\"model\", # Any name to save the program in MLflow\n    )\n\n# Load the program back from MLflow\nloaded = mlflow.dspy.load_model(model_info.model_uri)\n```\n\n----------------------------------------\n\nTITLE: Implementing KNN Few-Shot Learning with DSPy in Python\nDESCRIPTION: This snippet sets up KNN few-shot learning using DSPy's KNNFewShot teleprompter. It compiles the BasicQABot with the training set for improved performance.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/knn.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.teleprompt import KNNFewShot\n\nknn_teleprompter = KNNFewShot(7, trainset)\ncompiled_knn = knn_teleprompter.compile(BasicQABot(), trainset=trainset)\n```\n\n----------------------------------------\n\nTITLE: Using ChainOfThought for Basic Question Answering\nDESCRIPTION: Example of how to use ChainOfThought with a basic question-answering signature. This demonstrates creating a predictor that can answer questions while showing its reasoning steps.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/modules/chain-of-thought.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#Define a simple signature for basic question answering\nclass BasicQA(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n\n#Pass signature to ChainOfThought module\ngenerate_answer = dspy.ChainOfThought(BasicQA)\n\n# Call the predictor on a particular input.\nquestion='What is the color of the sky?'\npred = generate_answer(question=question)\n\nprint(f\"Question: {question}\")\nprint(f\"Predicted Answer: {pred.answer}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing a zero-shot aggregator for multiple ReAct agents\nDESCRIPTION: Creates an Aggregator class that runs multiple optimized ReAct agents and combines their outputs to produce a final answer.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/multi_agent.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dsp.utils import flatten, deduplicate\n\n# the best-performing five ReAct programs from the optimization process\nAGENTS = [x[-1] for x in optimized_react.candidate_programs[:5]]\n\nclass Aggregator(dspy.Module):\n\tdef __init__(self, temperature=0.0):\n\t\tself.aggregate = dspy.ChainOfThought('context, question -> answer')\n\t\tself.temperature = temperature\n\n\tdef forward(self, question):\n\t\t# Run all five agents with high temperature, then extract and deduplicate their observed contexts\n\t\twith dspy.context(lm=gpt3.copy(temperature=self.temperature)):\n\t\t\tpreds = [agent(question=question) for agent in AGENTS]\n\t\t\tcontext = deduplicate(flatten([flatten(p.observations) for p in preds]))\n\n\t\t# Run the aggregation step to produce a final answer\n\t\treturn self.aggregate(context=context, question=question)\n\naggregator = Aggregator()\nevaluate(aggregator)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Entity Extractor with MIPROv2 in Python\nDESCRIPTION: This snippet demonstrates how to use DSPy's MIPROv2 optimizer to improve an entity extraction model. It automatically tunes the language model prompt and maximizes correctness on the training set.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/entity_extraction/index.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmipro_optimizer = dspy.MIPROv2(\n    metric=extraction_correctness_metric,\n    auto=\"medium\",\n)\noptimized_people_extractor = mipro_optimizer.compile(\n    people_extractor,\n    trainset=train_set,\n    max_bootstrapped_demos=4,\n    requires_permission_to_run=False,\n    minibatch=False\n)\n```\n\n----------------------------------------\n\nTITLE: Using BootstrapFinetune for RAG Model in Python\nDESCRIPTION: Example showing how to use the BootstrapFinetune teleprompter to compile a RAG (Retrieval-Augmented Generation) model. The example demonstrates defining the teleprompter with teacher settings and then compiling the student RAG model with a specified training set and target model.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/optimizers/BootstrapFinetune.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#Assume defined trainset\n#Assume defined RAG class\n...\n\n#Define teleprompter\nteleprompter = BootstrapFinetune(teacher_settings=dict({'lm': teacher}))\n\n# Compile!\ncompiled_rag = teleprompter.compile(student=RAG(), trainset=trainset, target='google/flan-t5-base')\n```\n\n----------------------------------------\n\nTITLE: Defining a BasicQA signature for question answering\nDESCRIPTION: Creates a DSPy Signature class for basic question answering that specifies input (question) and output (answer) fields with appropriate descriptions.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/intro.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass BasicQA(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG (Retrieval-Augmented Generation) Module\nDESCRIPTION: Creates an advanced DSPy module that combines query generation, retrieval, and answer generation for improved question answering.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/skycamp2023.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass RAG(dspy.Module):\n    def __init__(self, num_passages=3):\n        super().__init__()\n\n        self.retrieve = dspy.Retrieve(k=num_passages)\n        self.generate_query = dspy.ChainOfThought(\"question -> search_query\")\n        self.generate_answer = dspy.ChainOfThought(\"context, question -> answer\")\n    \n    def forward(self, question):\n        search_query = self.generate_query(question=question).search_query\n        passages = self.retrieve(search_query).passages\n\n        return self.generate_answer(context=passages, question=question)\n```\n\n----------------------------------------\n\nTITLE: Creating and using a Predictor with the BasicQA signature\nDESCRIPTION: Demonstrates how to create a DSPy Predictor using the BasicQA signature and how to call it with a question from the development set.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/intro.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Define the predictor.\ngenerate_answer = dspy.Predict(BasicQA)\n\n# Call the predictor on a particular input.\npred = generate_answer(question=dev_example.question)\n```\n\n----------------------------------------\n\nTITLE: Defining Metric Function for Summary Evaluation in DSPy\nDESCRIPTION: Creates a metric function that uses the Metric program to evaluate the quality of generated summaries by comparing the predicted score with the gold standard score from the dataset.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/summarization.md#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef metric(gold, pred, trace=None):\n    metric_program = Metric()\n    examp = dspy.Example(passage=gold.passage)\n    predicted = dspy.Example(summary=pred)\n    pred_score = metric_program(example=examp, pred=predicted)\n    gold_score = gold.score\n    # check if they are almost equal\n    return abs(float(gold_score) - float(pred_score)) < 0.2\n```\n\n----------------------------------------\n\nTITLE: Optimizing Multi-Hop Retrieval Program with MIPROv2 in DSPy\nDESCRIPTION: This code optimizes the Hop program using DSPy's MIPROv2. It sets up the optimization parameters and compiles the program to maximize the recall metric.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmodels = dict(prompt_model=gpt4o, teacher_settings=dict(lm=gpt4o))\ntp = dspy.MIPROv2(metric=top5_recall, auto=\"medium\", num_threads=16, **models)\n\nkwargs = dict(minibatch_size=40, minibatch_full_eval_steps=4, requires_permission_to_run=False)\noptimized = tp.compile(Hop(), trainset=trainset, max_bootstrapped_demos=4, max_labeled_demos=4, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Implementing Overall Metric Function in Python\nDESCRIPTION: Defines a composite metric function that evaluates generated tweets based on multiple criteria including hashtag usage, length limits, answer correctness, engagement and faithfulness.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/tweets/tweets_assertions.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef overall_metric(gold, pred, trace=None):\n    answer, context, tweet = gold.answer, pred.context, pred.generated_tweet\n    no_hashtags = has_no_hashtags(tweet)\n    within_length_limit = is_within_length_limit(tweet, 280)\n    correct = has_correct_answer(tweet, answer)\n    engaging = \"Does the assessed text make for a self-contained, engaging tweet? Say no if it is not engaging.\"\n    faithful = \"Is the assessed text grounded in the context? Say no if it includes significant facts not in the context.\"   \n    faithful = dspy.Predict(AssessTweet)(context=context, assessed_text=tweet, assessment_question=faithful)\n    engaging = dspy.Predict(AssessTweet)(context='N/A', assessed_text=tweet, assessment_question=engaging)\n    engaging, faithful = [m.assessment_answer.split()[0].lower() == 'yes' for m in [engaging, faithful]]\n    score = (correct + engaging + faithful + no_hashtags + within_length_limit) if correct and within_length_limit else 0\n    return score / 5.0\n```\n\n----------------------------------------\n\nTITLE: Testing Finetuned Classifier and Inspecting History in Python\nDESCRIPTION: This snippet tests the finetuned classifier on a specific input and inspects the history of the operation. It demonstrates how the model applies modules to get the right label with explicit reasoning.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nclassify_ft(text=\"why hasnt my card come in yet?\")\ndspy.inspect_history()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Long-Form QA Model on Multiple Metrics in Python\nDESCRIPTION: This function evaluates a long-form QA module on a development set using multiple metrics including answer correctness, citation recall, precision, and faithfulness. It calculates and prints average scores for each metric.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/longformqa/longformqa_assertions.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate(module):\n    correctness_values = []\n    recall_values = []\n    precision_values = []\n    citation_faithfulness_values = []\n    for i in range(len(devset)):\n        example = devset[i]\n        try:\n            pred = module(question=example.question)\n            correctness_values.append(answer_correctness(example, pred))            \n            citation_faithfulness_score, _ = citation_faithfulness(None, pred, None)\n            citation_faithfulness_values.append(citation_faithfulness_score)\n            recall = calculate_recall(example, pred)\n            precision = calculate_precision(example, pred)\n            recall_values.append(recall)\n            precision_values.append(precision)\n        except Exception as e:\n            print(f\"Failed generation with error: {e}\")\n\n    average_correctness = sum(correctness_values) / len(devset) if correctness_values else 0\n    average_recall = sum(recall_values) / len(devset) if recall_values else 0\n    average_precision = sum(precision_values) / len(devset) if precision_values else 0\n    average_citation_faithfulness = sum(citation_faithfulness_values) / len(devset) if citation_faithfulness_values else 0\n\n    print(f\"Average Correctness: {average_correctness}\")\n    print(f\"Average Recall: {average_recall}\")\n    print(f\"Average Precision: {average_precision}\")\n    print(f\"Average Citation Faithfulness: {average_citation_faithfulness}\")\n```\n\n----------------------------------------\n\nTITLE: Enabling LM Usage Tracking in DSPy\nDESCRIPTION: This code snippet demonstrates how to enable language model usage tracking in DSPy. It configures the settings to track usage and shows how to access the usage statistics from a prediction object.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/programming/modules.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndspy.settings.configure(track_usage=True)\n\nusage = prediction_instance.get_lm_usage()\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluation Metric and Evaluator in Python\nDESCRIPTION: This snippet defines an evaluation metric and creates an evaluator using DSPy. The metric checks if the predicted label exactly matches the true label, ignoring reasoning.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmetric = (lambda x, y, trace=None: x.label == y.label)\nevaluate = dspy.Evaluate(devset=devset, metric=metric, display_progress=True, display_table=5, num_threads=16)\n```\n\n----------------------------------------\n\nTITLE: Evaluating the unoptimized ReAct agent\nDESCRIPTION: Sets up an evaluator and runs it on the development set to assess the performance of the unoptimized ReAct agent.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/multi_agent.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconfig = dict(num_threads=8, display_progress=True, display_table=5)\nevaluate = Evaluate(devset=devset, metric=dspy.evaluate.answer_exact_match, **config)\n\nevaluate(agent)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Question Answering Program with MIPRO in Python\nDESCRIPTION: This snippet demonstrates how to use MIPRO to optimize the question answering program. It includes options for loading a pre-compiled program or compiling from scratch, and sets up the necessary parameters for optimization.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/qa/hotpot/hotpotqa_with_MIPRO.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cloudpickle as pickle\nfrom dspy.teleprompt import MIPROv2\n\nLOAD_PRECOMPILED_PROGRAM = True\ncompiled_program = program.deepcopy()\n\n# By default, we will load the precompiled program\nif LOAD_PRECOMPILED_PROGRAM:\n    # Load the data from the file\n    compiled_program.load(compiled_program_file_path)\n    with open(trial_logs_path, \"rb\") as f:\n        trial_logs = pickle.load(f)\n    compiled_program.trial_logs = trial_logs\n# Otherwise, if desired, the program can be compiled from scratch\nelse:\n    # Define hyperparameters:\n    N = 10 # The number of instructions and fewshot examples that we will generate and optimize over\n    batches = 30 # The number of optimization trials to be run (we will test out a new combination of instructions and fewshot examples in each trial)\n    temperature = 1.0 # The temperature configured for generating new instructions\n\n    # Compile\n    eval_kwargs = dict(num_threads=16, display_progress=True, display_table=0)\n    teleprompter = MIPROv2(prompt_model=prompt_model, task_model=task_model, metric=metric, num_candidates=N, init_temperature=temperature, verbose=True)\n    compiled_program = teleprompter.compile(program, trainset=trainset, valset=valset, num_batches=batches, max_bootstrapped_demos=1,max_labeled_demos=2, eval_kwargs=eval_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Defining Classification Module in DSPy\nDESCRIPTION: Code to define a DSPy ChainOfThought module for classification. This module takes text input and produces a label from the predefined CLASSES.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nclassify = dspy.ChainOfThought(f\"text -> label: Literal{CLASSES}\")\n```\n\n----------------------------------------\n\nTITLE: Using DSPy inspect_history for Debugging\nDESCRIPTION: Shows how to use the built-in inspect_history utility to examine recent LLM calls, which helps in understanding what interactions occurred during execution.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/observability/index.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Print out 5 LLM calls\ndspy.inspect_history(n=5)\n```\n\n----------------------------------------\n\nTITLE: Assigning Classifiers to Language Models in DSPy\nDESCRIPTION: Code to create separate instances of the classification module for the student and teacher models, assigning them to their respective language models.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nstudent_classify = classify.deepcopy()\nstudent_classify.set_lm(student_lm)\n\nteacher_classify = classify.deepcopy()\nteacher_classify.set_lm(teacher_lm)\n```\n\n----------------------------------------\n\nTITLE: Defining BasicQA Signature Class in Python using DSPy\nDESCRIPTION: This class defines the structure for a basic question-answering task using DSPy's Signature. It specifies input and output fields for questions and answers.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/knn.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass BasicQA(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating Multi-Hop Retrieval Program in DSPy\nDESCRIPTION: This snippet evaluates the initial Hop program using the defined evaluation metric and dataset.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nevaluate(Hop())\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Optimized Multi-Hop Retrieval Program in DSPy\nDESCRIPTION: This code demonstrates how to save the optimized Hop program to a JSON file and then load it back for use.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\noptimized.save(\"optimized_hop.json\")\n\nloaded_program = Hop()\nloaded_program.load(\"optimized_hop.json\")\n\nloaded_program(claim=\"The author of the 1960s unproduced script written for The Beatles, Up Against It, and Bernard-Marie Kolts are both playwrights.\").titles\n```\n\n----------------------------------------\n\nTITLE: Compiling Tweet Generator with Bootstrap Few-Shot Learning\nDESCRIPTION: Implementation of compilation process using DSPy's BootstrapFewShotWithRandomSearch optimizer for automatic few-shot demonstration generation and program optimization.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/tweets/tweets_assertions.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nteleprompter = BootstrapFewShotWithRandomSearch(metric = overall_metric, max_bootstrapped_demos=2, num_candidate_programs=6)\ncompiled_tweeter = teleprompter.compile(student = tweeter, teacher = tweeter, trainset=trainset, valset=devset[:25])\n\nfor metric in metrics:\n    evaluate = Evaluate(metric=metric, devset=devset, num_threads=1, display_progress=True, display_table=5)\n    evaluate(compiled_tweeter)\n```\n\n----------------------------------------\n\nTITLE: Querying a DSPy Agent with a Simple Question\nDESCRIPTION: Demonstrates how to run a prediction using a DSPy agent by providing a question and printing the generated answer, showing the basic usage pattern for agent interaction.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/observability/index.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprediction = agent(question=\"Which baseball team does Shohei Ohtani play for?\")\nprint(prediction.answer)\n```\n\n----------------------------------------\n\nTITLE: Loading HotPotQA Dataset\nDESCRIPTION: Loads and preprocesses the HotPotQA dataset with specific train, dev, and test splits.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/qa/hotpot/multihop_finetune.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndataset = HotPotQA(train_seed=1, train_size=200, eval_seed=2023, dev_size=1000, test_size=0)\ntrainset = [x.with_inputs('question') for x in dataset.train]\ndevset = [x.with_inputs('question') for x in dataset.dev]\ntestset = [x.with_inputs('question') for x in dataset.test]\n\nlen(trainset), len(devset), len(testset)\n```\n\n----------------------------------------\n\nTITLE: Saving Finetuned Program with MLflow in Python\nDESCRIPTION: This code demonstrates how to save a finetuned program using MLflow. It starts an MLflow run, logs the model, and shows how to load it back. This approach offers benefits like dependency management and experiment tracking.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Start an MLflow Run and save the program\nwith mlflow.start_run(run_name=\"optimized_classifier\"):\n    model_info = mlflow.dspy.log_model(\n        classify_ft,\n        artifact_path=\"model\", # Any name to save the program in MLflow\n    )\n\n# Load the program back from MLflow\nloaded = mlflow.dspy.load_model(model_info.model_uri)\n```\n\n----------------------------------------\n\nTITLE: Compiling MultiHop Instance with Teleprompter in Python\nDESCRIPTION: This code snippet compiles an instance of the MultiHop class using a teleprompter object. It uses training and development datasets for compilation.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/skycamp2023.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmultihop_compiled = teleprompter2.compile(MultiHop(), trainset=train, valset=dev)\n```\n\n----------------------------------------\n\nTITLE: Initializing YouRM Class - Python Constructor\nDESCRIPTION: Constructor definition for the YouRM class that handles web document retrieval through You.com APIs. Supports both Search and News endpoints with configurable parameters for search behavior and localization.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/YouRM.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nYouRM(\n    ydc_api_key: Optional[str] = None,\n    k: int = 3,\n    endpoint: Literal[\"search\", \"news\"] = \"search\",\n    num_web_results: Optional[int] = None,\n    safesearch: Optional[Literal[\"off\", \"moderate\", \"strict\"]] = None,\n    country: Optional[str] = None,\n    search_lang: Optional[str] = None,\n    ui_lang: Optional[str] = None,\n    spellcheck: Optional[bool] = None,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Answer Generation Signature in DSPy\nDESCRIPTION: Creates a DSPy signature for the answer generation component that takes context and question as inputs and produces an answer as output.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/simplified-baleen.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass GenerateAnswer(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n\n    context = dspy.InputField(desc=\"may contain relevant facts\")\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n```\n\n----------------------------------------\n\nTITLE: Saving Compiled Question Answering Program in Python\nDESCRIPTION: This snippet demonstrates how to save the compiled and optimized question answering program to disk for later use.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/qa/hotpot/hotpotqa_with_MIPRO.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncompiled_program.save(\"compiled_program.dspy\")\n```\n\n----------------------------------------\n\nTITLE: Compiling Optimized Program with BetterTogether\nDESCRIPTION: Demonstrates the process of compiling an optimized program using BetterTogether, including setting up the language model and retriever, and using a subset of the training data.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/finetune/_unpolished_finetune_demo.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Running BetterTogether on a small dataset\n\nlm = dspy.LM('gpt-4o-mini-2024-07-18')\nsmall_trainset = trainset[:50] # Use a small subset of the training data\n\nwith dspy.context(lm=lm, rm=retriever):\n  optimized_program = better_together.compile(\n    student=BasicMH(),\n    trainset=small_trainset,\n    strategy=\"p -> w -> p\",\n    valset_ratio=0.1\n  )\n```\n\n----------------------------------------\n\nTITLE: Multiple Choice Evaluation in Python\nDESCRIPTION: Script to calculate and print multiple choice question accuracy metrics.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/vlm/mmmu.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmc_correct = sum(outputs[i][2] for i in range(len(outputs)) if outputs[i][0][\"question_type\"] == \"multiple-choice\")\ntotal_mc = sum(1 for example in outputs if example[0][\"question_type\"] == \"multiple-choice\")\nprint(mc_correct, total_mc)\nprint(mc_correct / total_mc)\nprint(sum(outputs[i][1].get(\"answer\", None) is None for i in range(len(outputs))))\n\n# Note: Run above here\n```\n\n----------------------------------------\n\nTITLE: Implementing Top-5 Recall Evaluation Metric in Python\nDESCRIPTION: This function calculates the top-5 recall metric for the multi-hop retrieval task. It compares the predicted titles with the gold titles and returns the recall score.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef top5_recall(example, pred, trace=None):\n    gold_titles = example.titles\n    recall = sum(x in pred.titles[:5] for x in gold_titles) / len(gold_titles)\n\n    # If we're \"bootstrapping\" for optimization, return True if and only if the recall is perfect.\n    if trace is not None:\n        return recall >= 1.0\n    \n    # If we're just doing inference, just measure the recall.\n    return recall\n\nevaluate = dspy.Evaluate(devset=devset, metric=top5_recall, num_threads=16, display_progress=True, display_table=5)\n```\n\n----------------------------------------\n\nTITLE: Performing Single Query Retrieval with ColBERT in Python\nDESCRIPTION: Demonstrates how to perform a single query retrieval using the configured ColBERT retriever in DSPy.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/integrations/colbert/colbert_local.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npred = retrieved_docs(\n    \"What is the meaning of life?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Finetuned Classifier in Python\nDESCRIPTION: This code evaluates the finetuned 1B classifier using the previously defined evaluator. It applies the evaluate function to the classify_ft classifier.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nevaluate(classify_ft)\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow UI for Trace Visualization\nDESCRIPTION: Command to start the MLflow user interface on port 5000, allowing visualization and exploration of the traced executions through a web browser.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/observability/index.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui --port 5000\n```\n\n----------------------------------------\n\nTITLE: Evaluating Optimized DSPy Program in Python\nDESCRIPTION: This code evaluates the optimized entity extraction program on a test set to measure improvements and validate generalization to unseen data.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/entity_extraction/index.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nevaluate_correctness(optimized_people_extractor, devset=test_set)\n```\n\n----------------------------------------\n\nTITLE: Installing Fine-tuning Dependencies in Python\nDESCRIPTION: Commands to install necessary packages for fine-tuning, including torch, transformers, accelerate, trl, and peft. A specific version of transformers is used to address a known issue.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n> uv pip install -U torch transformers==4.48.3 accelerate trl peft\n```\n\n----------------------------------------\n\nTITLE: Loading and preparing the HotPotQA dataset for multi-hop question answering\nDESCRIPTION: Loads a sample from the HotPotQA dataset, dividing it into training and development sets. Specifies that the 'question' field will be the input, while other fields are labels or metadata.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/intro.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.datasets import HotPotQA\n\n# Load the dataset.\ndataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)\n\n# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\ntrainset = [x.with_inputs('question') for x in dataset.train]\ndevset = [x.with_inputs('question') for x in dataset.dev]\n\nlen(trainset), len(devset)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Search Agent with Thread-Safe Evaluator\nDESCRIPTION: Shows how to evaluate the optimized search agent using the thread-safe evaluator method provided by AvatarOptimizer.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/avatar_langchain_tools.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nteleprompter.thread_safe_evaluator(sqa_test, optimized_search_agent)\n```\n\n----------------------------------------\n\nTITLE: Inspecting MultiHop Prompt for Second-Hop Search Query in Python\nDESCRIPTION: This code demonstrates how to use the compiled MultiHop instance to process a specific question and then inspect the prompt history for the second-hop search query.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/skycamp2023.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmultihop_compiled(question=\"Who purchased the team Michael Schumacher raced for in the 1995 Monaco Grand Prix in 2000?\")\nllama.inspect_history(n=1, skip=2)\n```\n\n----------------------------------------\n\nTITLE: Initializing BootstrapFewShotWithRandomSearch Teleprompter in Python\nDESCRIPTION: Demonstrates how to initialize the BootstrapFewShotWithRandomSearch teleprompter with custom parameters including metric function, number of demonstrations, threads and candidate programs.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/optimizers/bfrs.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch\n\nteleprompter = BootstrapFewShotWithRandomSearch(\n    metric=gsm8k_metric, \n    max_bootstrapped_demos=8, \n    max_labeled_demos=8,\n    num_threads=10,\n    num_candidate_programs=10\n)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating DSPy Caching with LM Usage Tracking\nDESCRIPTION: This code snippet shows how to enable caching in DSPy along with LM usage tracking. It demonstrates that cached responses don't count towards usage statistics by making two identical calls to a program and comparing the usage output.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/programming/modules.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Enable caching\ndspy.settings.configure(\n    lm=dspy.LM(\"openai/gpt-4o-mini\", cache=True),\n    track_usage=True\n)\n\nprogram = MyProgram()\n\n# First call - will show usage statistics\noutput = program(question=\"What is the capital of Zambia?\")\nprint(output.get_lm_usage())  # Shows token usage\n\n# Second call - same question, will use cache\noutput = program(question=\"What is the capital of Zambia?\")\nprint(output.get_lm_usage())  # Shows empty dict: {}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI LM Client in DSPy\nDESCRIPTION: Sets up the OpenAI GPT-3.5-Turbo language model with a maximum token limit of 250 and configures it as the default language model for DSPy.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/optimizers/bootstrap-fewshot.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\n\nturbo = dspy.LM(model='openai/gpt-3.5-turbo', max_tokens=250)\ndspy.configure(lm=turbo)\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset for Summarization Task in DSPy\nDESCRIPTION: Loads a dataset from a JSONL file where each line contains a passage, a summary, and a score. The data is converted to DSPy Example objects for training and evaluation.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/summarization.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset = []\nwith open('src/summarization/programs/summarize/dataset.jsonl', 'r',\n          encoding='utf-8') as f:\n    for line in f:\n        data = json.loads(line)\n\n        passage = data.get(\"passage\", \"\")\n        summary = data.get(\"summary\", \"\")\n        score = data.get(\"score\", 0)\n\n        example = dspy.Example(passage=passage, summary=summary, score=score)\n        dataset.append(example)\n\ntrainset = [x.with_inputs(\"passage\") for x in dataset]\n```\n\n----------------------------------------\n\nTITLE: Implementing Chain of Thought Module\nDESCRIPTION: Creates a custom DSPy module using ChainOfThought for step-by-step reasoning in question answering.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/skycamp2023.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass CoT(dspy.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.generate_answer = dspy.ChainOfThought('question -> answer')\n    \n    def forward(self, question):\n        return self.generate_answer(question=question)\n```\n\n----------------------------------------\n\nTITLE: Configuring Language and Retrieval Models in DSPy\nDESCRIPTION: Sets up the language model (GPT-3.5-turbo) and retrieval model (ColBERTv2 with Wikipedia 2017 abstracts), then configures DSPy to use these models by default for subsequent operations.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/intro.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nturbo = dspy.OpenAI(model='gpt-3.5-turbo')\ncolbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n\ndspy.settings.configure(lm=turbo, rm=colbertv2_wiki17_abstracts)\n```\n\n----------------------------------------\n\nTITLE: Corrected Agent Response with Web Search\nDESCRIPTION: Shows the updated and correct response from the agent after implementing the web search capability, demonstrating successful debugging and improvement.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/observability/index.md#2025-04-07_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nLos Angeles Dodgers\n```\n\n----------------------------------------\n\nTITLE: Setting Up Student and Teacher Language Models in DSPy\nDESCRIPTION: Code to initialize the student (Llama-3.2-1B-Instruct) and teacher (GPT-4o-mini) language models using DSPy's LM class.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.clients.lm_local import LocalProvider\n\nstudent_lm_name = \"meta-llama/Llama-3.2-1B-Instruct\"\nstudent_lm = dspy.LM(model=f\"openai/local:{student_lm_name}\", provider=LocalProvider(), max_tokens=2000)\nteacher_lm = dspy.LM('openai/gpt-4o-mini', max_tokens=3000)\n```\n\n----------------------------------------\n\nTITLE: Implementing Tweet Evaluation Metrics\nDESCRIPTION: Defines evaluation functions for tweet constraints including hashtag checking, length limits, answer verification, engagement, and faithfulness\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/tweets/tweets_assertions.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef has_no_hashtags(text):\n    return len(re.findall(r\"#\\w+\", text)) == 0\n\ndef is_within_length_limit(text, length_limit=280):\n    return len(text) <= length_limit\n\ndef is_assessment_yes(assessment_answer):\n    return assessment_answer.split()[0].lower() == 'yes'\n\ndef has_correct_answer(text, answer):\n    return answer in text\n\nclass AssessTweet(dspy.Signature):\n    \"\"\"Assess the quality of a tweet along the specified dimension.\"\"\"\n    context = dspy.InputField(desc='ignore if N/A')\n    assessed_text = dspy.InputField()\n    assessment_question = dspy.InputField()\n    assessment_answer = dspy.OutputField(desc=\"Yes or No\")\n```\n\n----------------------------------------\n\nTITLE: Implementing BasicQABot Class in Python with DSPy\nDESCRIPTION: This class implements a basic question-answering bot using DSPy. It uses the BasicQA signature to generate answers for given questions.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/knn.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass BasicQABot(dspy.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.generate = dspy.Predict(BasicQA)\n\n    def forward(self,question):\n        prediction = self.generate(question = question)\n        return dspy.Prediction(answer = prediction.answer)\n```\n\n----------------------------------------\n\nTITLE: Configuring ColBERT as Reranker with DSPy in Python\nDESCRIPTION: Sets up ColBERT as a reranker using DSPy. It configures ColBERT with specific parameters for the reranking task.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/integrations/colbert/colbert_local.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncolbert_config = ColBERTConfig()\ncolbert_config.index_name = 'colbert-ir-index'\ncolbert_reranker = dspy.ColBERTv2RerankerLocal(\n    checkpoint='colbert-ir/colbertv2.0',colbert_config=colbert_config)\n```\n\n----------------------------------------\n\nTITLE: Compiling and Loading T5 Model\nDESCRIPTION: Configures and compiles the multi-hop QA program using T5-Large model, with options for pre-computing ensemble predictions.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/qa/hotpot/multihop_finetune.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nif not RECOMPILE_INTO_T5_FROM_SCRATCH:\n    t5_program = BasicMH()\n\n    ckpt_path = \"colbert-ir/dspy-Oct11-T5-Large-MH-3k-v1\"\n    LM = dspy.HFModel(checkpoint=ckpt_path, model='t5-large')\n\n    for p in t5_program.predictors():\n        p.lm = LM\n        p.activated = False\n```\n\n----------------------------------------\n\nTITLE: Testing Compiled KNN Model on Dev Set in Python\nDESCRIPTION: This code tests the compiled KNN model on a sample from the development set. It prints the question, expected answer, and the model's prediction for comparison.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/knn.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nexample = devset[0]\npred = compiled_knn(question = example.question)\nprint(\"Question: \", example.question)\nprint(\"Expected answer: \", example.answer)\nprint(\"Prediction: \", pred.answer)\n```\n\n----------------------------------------\n\nTITLE: Evaluating LongFormQA with Assertions using DSPy Retry Framework\nDESCRIPTION: This code evaluates the LongFormQAWithAssertions module by wrapping it with the Retry module which handles backtracking logic. It transforms the module using assert_transform_module to prepare and execute the assertion backtracking logic.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/longformqa/longformqa_assertions.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlongformqa_with_assertions = assert_transform_module(LongFormQAWithAssertions().map_named_predictors(Retry), backtrack_handler) \nevaluate(longformqa_with_assertions)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Optimized Multi-Hop Retrieval Program in DSPy\nDESCRIPTION: This snippet evaluates the optimized Hop program using the same evaluation metric and dataset as before.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nevaluate(optimized)\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Predict Module\nDESCRIPTION: Demonstrates the use of the dspy.Predict module for simple question answering.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/skycamp2023.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npredict = dspy.Predict('question -> answer')\n\npredict(question=\"What is the capital of Germany?\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating Bootstrapped Finetuned Classifier in Python\nDESCRIPTION: This snippet evaluates the bootstrapped finetuned classifier using the previously defined evaluate function. It demonstrates how to assess the performance of the optimized model.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nevaluate(classify_ft)\n```\n\n----------------------------------------\n\nTITLE: Testing Optimized Multi-Hop Retrieval Program in DSPy\nDESCRIPTION: This code tests the optimized program with a sample claim and retrieves relevant titles.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\noptimized(claim=\"The author of the 1960s unproduced script written for The Beatles, Up Against It, and Bernard-Marie Kolts are both playwrights.\").titles\n```\n\n----------------------------------------\n\nTITLE: Using DSPy Assertions in Python\nDESCRIPTION: This code snippet demonstrates the syntax for using dspy.Suggest assertions in DSPy. The Suggest mechanism provides a way to guide the program toward desired outcomes without halting execution, making it useful for enforcing soft constraints like citation requirements.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/longformqa/longformqa_assertions.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndspy.Suggest(validation_function(model_outputs): bool, instruction_message: str)\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM-based Evaluator for QA Responses\nDESCRIPTION: Creates an Evaluator signature and function to assess the quality of model-generated answers compared to reference answers. This provides a more flexible evaluation approach for open-ended QA tasks than rigid metrics.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/avatar_langchain_tools.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Evaluator(dspy.Signature):\n    \"\"\"Please act as an impartial judge and evaluate the quality of the responses provided by multiple AI assistants to the user question displayed below. You should choose the assistant that offers a better user experience by interacting with the user more effectively and efficiently, and providing a correct final response to the user's question.\n    \nRules:\n1. Avoid Position Biases: Ensure that the order in which the responses were presented does not influence your decision. Evaluate each response on its own merits.\n2. Length of Responses: Do not let the length of the responses affect your evaluation. Focus on the quality and relevance of the response. A good response is targeted and addresses the user's needs effectively, rather than simply being detailed.\n3. Objectivity: Be as objective as possible. Consider the user's perspective and overall experience with each assistant.\"\"\"\n    \n    question: str = dspy.InputField(\n        prefix=\"Question:\",\n        desc=\"question to ask\",\n    )\n    reference_answer: str = dspy.InputField(\n        prefix=\"Reference Answer:\",\n        desc=\"Answer to the question given by the model.\",\n    )\n    answer: str = dspy.InputField(\n        prefix=\"Answer:\",\n        desc=\"Answer to the question given by the model.\",\n    )\n    rationale: str = dspy.OutputField(\n        prefix=\"Rationale:\",\n        desc=\"Explanation of why the answer is correct or incorrect.\",\n    )\n    is_correct: bool = dspy.OutputField(\n        prefix=\"Correct:\",\n        desc=\"Whether the answer is correct.\",\n    )\n\n\nevaluator = dspy.TypedPredictor(Evaluator)\n\n\ndef metric(example, prediction, trace=None):\n    return int(\n        evaluator(\n            question=example.question,\n            answer=prediction.answer,\n            reference_answer=example.answer\n        ).is_correct\n    ) \n```\n\n----------------------------------------\n\nTITLE: Configuring DSPy Language Models\nDESCRIPTION: Setting up and configuring OpenAI language models in DSPy\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/math/index.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\n\ngpt4o_mini = dspy.LM('openai/gpt-4o-mini', max_tokens=2000)\ngpt4o = dspy.LM('openai/gpt-4o', max_tokens=2000)\ndspy.configure(lm=gpt4o_mini)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Optimized DSPy Program's Prompt in Python\nDESCRIPTION: This snippet shows how to inspect the history of interactions to view the augmented prompt with few-shot examples after optimization.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/entity_extraction/index.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndspy.inspect_history(n=1)\n```\n\n----------------------------------------\n\nTITLE: Running the Agent with MLflow Tracing Enabled\nDESCRIPTION: Executes the agent with the same query, but now with MLflow tracing enabled to capture detailed information about the execution flow.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/observability/index.md#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nagent(question=\"Which baseball team does Shohei Ohtani play for?\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating Optimized Module\nDESCRIPTION: Running evaluation on the optimized module\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/math/index.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nevaluate(optimized_module)\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow Dependencies\nDESCRIPTION: Commands for installing MLflow package for tracking and visualization\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/math/index.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n%pip install mlflow>=2.20\n```\n\n----------------------------------------\n\nTITLE: Evaluating ArxivQA Performance with Avatar Agent\nDESCRIPTION: Runs the evaluation on the ArxivQA test set using the custom multi-threaded executor and displays the average performance score.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/avatar_langchain_tools.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\naqa_score = multi_thread_executor(aqa_test, ArxivQASignature)\nprint(f\"Average Score on ArxivQA: {aqa_score:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow for DSPy Tracing\nDESCRIPTION: Sets up MLflow integration with DSPy for visualization and tracking of prompts and optimization progress. Includes installation command and configuration steps.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n%pip install mlflow>=2.20\n```\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui --port 5000\n```\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"DSPy\")\n```\n\nLANGUAGE: python\nCODE:\n```\nmlflow.dspy.autolog()\n```\n\n----------------------------------------\n\nTITLE: Tracking Evaluation Results with MLflow in Python\nDESCRIPTION: This snippet demonstrates how to track and visualize evaluation results using MLflow. It records the evaluation outputs and scores, logging metrics and creating a table of results.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nwith mlflow.start_run(run_name=\"classifier_evaluation\"):\n    evaluate_correctness = dspy.Evaluate(\n        devset=devset,\n        metric=extraction_correctness_metric,\n        num_threads=16,\n        display_progress=True,\n        # To record the outputs and detailed scores to MLflow\n        return_all_scores=True,\n        return_outputs=True,\n    )\n\n    # Evaluate the program as usual\n    aggregated_score, outputs, all_scores = evaluate_correctness(people_extractor)\n\n    # Log the aggregated score\n    mlflow.log_metric(\"exact_match\", aggregated_score)\n    # Log the detailed evaluation results as a table\n    mlflow.log_table(\n        {\n            \"Text\": [example.text for example in devset],\n            \"Expected\": [example.example_label for example in devset],\n            \"Predicted\": outputs,\n            \"Exact match\": all_scores,\n        },\n        artifact_file=\"eval_results.json\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Generating and Evaluating a Single Long-Form QA Example in Python\nDESCRIPTION: This code snippet shows how to generate a long-form answer for a single question from the development set, print the result, and evaluate its citation faithfulness. It demonstrates the use of the LongFormQA model on a specific example.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/longformqa/longformqa_assertions.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nquestion = devset[15].question\npred = longformqa(question)\ncitation_faithfulness_score, _ = citation_faithfulness(None, pred, None)\n\nprint(f\"Question: {question}\")\nprint(f\"Predicted Paragraph: {pred.paragraph}\")\nprint(f\"Citation Faithfulness: {citation_faithfulness_score}\")\n```\n\n----------------------------------------\n\nTITLE: Launching Finetuned Classifier in Python\nDESCRIPTION: This code launches the finetuned classifier, preparing it for evaluation. It uses the get_lm() method to access and launch the language model.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclassify_ft.get_lm().launch()\n```\n\n----------------------------------------\n\nTITLE: Inspecting Optimized Prompts in DSPy\nDESCRIPTION: This snippet inspects the last two prompts used in the optimized Hop program to understand what it has learned.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndspy.inspect_history(n=2)\n```\n\n----------------------------------------\n\nTITLE: Loading HotPotQA dataset for training and evaluation\nDESCRIPTION: Loads the HotPotQA dataset, splitting it into training, validation, and development sets for the QA task.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/multi_agent.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndataset = HotPotQA(train_seed=1, train_size=200, eval_seed=2023, dev_size=300, test_size=0)\ntrainset = [x.with_inputs('question') for x in dataset.train[0:150]]\nvalset = [x.with_inputs('question') for x in dataset.train[150:200]]\ndevset = [x.with_inputs('question') for x in dataset.dev]\n\n# show an example datapoint; it's just a question-answer pair\ntrainset[0]\n```\n\n----------------------------------------\n\nTITLE: Creating a ReAct agent for question answering\nDESCRIPTION: Initializes a DSPy ReAct agent that takes a question as input and outputs an answer using a ColBERTv2 retrieval tool.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/multi_agent.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nagent = dspy.ReAct(\"question -> answer\", tools=[dspy.Retrieve(k=1)])\n```\n\n----------------------------------------\n\nTITLE: Building Thread-safe Model Evaluator for Avatar Agents\nDESCRIPTION: Creates a custom multi-threaded evaluation function for Avatar agents. This approach is necessary because Avatar dynamically modifies its signature during execution, making standard DSPy evaluation tools incompatible.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/avatar_langchain_tools.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport tqdm\n\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef process_example(example, signature):\n    try:\n        avatar = Avatar(\n            signature,\n            tools=tools,\n            verbose=False,\n        )\n        prediction = avatar(**example.inputs().toDict())\n\n        return metric(example, prediction)\n    except Exception as e:\n        print(e)\n        return 0\n\n\ndef multi_thread_executor(test_set, signature, num_threads=60):\n    total_score = 0\n    total_examples = len(test_set)\n\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n        futures = [executor.submit(process_example, example, signature) for example in test_set]\n\n        for future in tqdm.tqdm(futures, total=total_examples, desc=\"Processing examples\"):\n            total_score += future.result()\n\n    avg_metric = total_score / total_examples\n    return avg_metric\n```\n\n----------------------------------------\n\nTITLE: Configuring ColBERT as Retriever with DSPy in Python\nDESCRIPTION: Sets up ColBERT as a retriever using DSPy. It configures ColBERT with specific parameters and initializes it with the sample passages.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/integrations/colbert/colbert_local.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\ncolbert_config = ColBERTConfig()\ncolbert_config.index_name = \"Colbert-RM\"\ncolbert_config.experiment = \"Colbert-Experiment\"\ncolbert_config.checkpoint = \"colbert-ir/colbertv2.0\"\ncolbert_retriever = dspy.ColBERTv2RetrieverLocal(\n    passages = passages,load_only=False,\n    colbert_config=colbert_config\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Language and Retrieval Models in DSPy\nDESCRIPTION: Sets up OpenAI's GPT-3.5-turbo as the language model and ColBERTv2 with Wiki17 abstracts as the retrieval model for the question answering system.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/simplified-baleen.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\n\nturbo = dspy.OpenAI(model='gpt-3.5-turbo')\ncolbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n\ndspy.settings.configure(lm=turbo, rm=colbertv2_wiki17_abstracts)\n```\n\n----------------------------------------\n\nTITLE: MMMU Module Testing with Modified Images in Python\nDESCRIPTION: Script to test MMMU module with modified image inputs and print results.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/vlm/mmmu.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmmmu = MMMUModule()\nprint(examples_no_image_1[0].inputs())\nprint(mmmu(**examples_no_image_1[0].inputs()))\n\nprint(examples_no_image_2[0].inputs())\nprint(mmmu(**examples_no_image_2[0].inputs()))\n```\n\n----------------------------------------\n\nTITLE: Configuring Language Model and Retriever\nDESCRIPTION: Configures Llama-2 language model and ColBERTv2 retriever with specific ports and endpoints.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/qa/hotpot/multihop_finetune.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nports = [7140, 7141, 7142, 7143, 7144, 7145]\nllamaChat = dspy.HFClientTGI(model=\"meta-llama/Llama-2-13b-chat-hf\", port=ports, max_tokens=150)\ncolbertv2 = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n\ndspy.settings.configure(rm=colbertv2, lm=llamaChat)\n```\n\n----------------------------------------\n\nTITLE: Printing HotPotQA Training Example in Python\nDESCRIPTION: This snippet prints a sample from the HotPotQA training set, displaying the question and its corresponding answer. It helps in understanding the dataset structure.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/knn.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_example = trainset[0]\nprint(train_example)\nprint(f\"Question: {train_example.question}\")\nprint(f\"Answer: {train_example.answer}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Tweet Generator with DSPy Signatures\nDESCRIPTION: Defines the tweet generation pipeline including search query generation and tweet creation with specific constraints\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/tweets/tweets_assertions.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass GenerateSearchQuery(dspy.Signature):\n    \"\"\"Write a simple search query that will help answer a complex question.\"\"\"\n    context = dspy.InputField(desc=\"may contain relevant facts\")\n    question = dspy.InputField()\n    query = dspy.OutputField()\n\nclass GenerateTweet(dspy.Signature):\n    \"\"\"Generate an engaging tweet that effectively answers a question staying faithful to the context, is less than 280 characters, and has no hashtags.\"\"\"\n    question = dspy.InputField()\n    context = dspy.InputField(desc=\"may contain relevant facts\")\n    tweet = dspy.OutputField()\n\nclass Tweeter(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.generate_tweet = dspy.ChainOfThought(GenerateTweet)\n\n    def forward(self, question, answer):\n        context = []\n        max_hops=2\n        passages_per_hop=3\n        generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n        retrieve = dspy.Retrieve(k=passages_per_hop)\n        for hop in range(max_hops):\n            query = generate_query[hop](context=context, question=question).query\n            passages = retrieve(query).passages\n            context = deduplicate(context + passages)\n        generated_tweet = self.generate_tweet(question=question, context=context).tweet\n        return dspy.Prediction(generated_tweet=generated_tweet, context=context)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing HotPotQA Dataset in Python using DSPy\nDESCRIPTION: This code loads the HotPotQA dataset using DSPy, setting up train and dev sets with specific sizes and seeds. It prepares the data for question answering tasks.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/knn.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.datasets import HotPotQA\n\n# Load the dataset.\ndataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)\n\ntrainset = [x.with_inputs('question') for x in dataset.train]\ndevset = [x.with_inputs('question') for x in dataset.dev]\n```\n\n----------------------------------------\n\nTITLE: Examining a development example with relevant Wikipedia titles\nDESCRIPTION: Displays a development example from the dataset, showing the question, its answer, and the relevant Wikipedia article titles that would help answer this multi-hop question.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/intro.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndev_example = devset[18]\nprint(f\"Question: {dev_example.question}\")\nprint(f\"Answer: {dev_example.answer}\")\nprint(f\"Relevant Wikipedia Titles: {dev_example.gold_titles}\")\n```\n\n----------------------------------------\n\nTITLE: Testing LongFormQA with Assertions on a Single Example\nDESCRIPTION: This code tests the LongFormQAWithAssertions module on a specific example from the development set. It prints the question, the generated paragraph with citations, and evaluates the citation faithfulness score to ensure the model is properly adhering to the defined constraints.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/longformqa/longformqa_assertions.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nquestion = devset[15].question\npred = longformqa_with_assertions(question)\ncitation_faithfulness_score, _ = citation_faithfulness(None, pred, None)\n\nprint(f\"Question: {question}\")\nprint(f\"Predicted Paragraph: {pred.paragraph}\")\nprint(f\"Citation Faithfulness: {citation_faithfulness_score}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Extrinsic Metrics for Long-Form QA Evaluation in Python\nDESCRIPTION: This snippet defines functions to calculate citation precision, recall, and answer inclusion for evaluating the quality of generated long-form answers. It extracts cited titles from paragraphs and compares them with gold standards.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/longformqa/longformqa_assertions.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef extract_cited_titles_from_paragraph(paragraph, context):\n    cited_indices = [int(m.group(1)) for m in re.finditer(r'\\[(\\d+)\\]\\.', paragraph)]\n    cited_indices = [index - 1 for index in cited_indices if index <= len(context)]\n    cited_titles = [context[index].split(' | ')[0] for index in cited_indices]\n    return cited_titles\n\ndef calculate_recall(example, pred, trace=None):\n    gold_titles = set(example['gold_titles'])\n    found_cited_titles = set(extract_cited_titles_from_paragraph(pred.paragraph, pred.context))\n    intersection = gold_titles.intersection(found_cited_titles)\n    recall = len(intersection) / len(gold_titles) if gold_titles else 0\n    return recall\n\ndef calculate_precision(example, pred, trace=None):\n    gold_titles = set(example['gold_titles'])\n    found_cited_titles = set(extract_cited_titles_from_paragraph(pred.paragraph, pred.context))\n    intersection = gold_titles.intersection(found_cited_titles)\n    precision = len(intersection) / len(found_cited_titles) if found_cited_titles else 0\n    return precision\n\ndef answer_correctness(example, pred, trace=None):\n    assert hasattr(example, 'answer'), \"Example does not have 'answer'.\"\n    normalized_context = normalize_text(pred.paragraph)\n    if isinstance(example.answer, str):\n        gold_answers = [example.answer]\n    elif isinstance(example.answer, list):\n        gold_answers = example.answer\n    else:\n        raise ValueError(\"'example.answer' is not string or list.\")\n    return 1 if any(normalize_text(answer) in normalized_context for answer in gold_answers) else 0\n```\n\n----------------------------------------\n\nTITLE: Evaluating Compiled MultiHop Instance on Development Set in Python\nDESCRIPTION: This code snippet evaluates the compiled MultiHop instance using the evaluate_hotpot function on a development dataset.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/skycamp2023.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nevaluate_hotpot(multihop_compiled, devset=dev)\n```\n\n----------------------------------------\n\nTITLE: Inspecting LM History in DSPy\nDESCRIPTION: Views the last three language model calls to analyze how the system generates queries and answers in the multi-hop process.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/simplified-baleen.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nturbo.inspect_history(n=3)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Baseline Question Answering Program in Python\nDESCRIPTION: This code snippet evaluates the baseline question answering program on both the training and validation sets, expecting performance around 35.4% on the training set and 38.2% on the validation set.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/qa/hotpot/hotpotqa_with_MIPRO.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbaseline_train_score = evaluate(program,devset=trainset)\nbaseline_eval_score = evaluate(program, devset=valset)\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Python Integration\nDESCRIPTION: Python code to set up MLflow tracking and experiment configuration\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/math/index.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"DSPy\")\n```\n\n----------------------------------------\n\nTITLE: Using WatsonDiscoveryRM for Information Retrieval in Python\nDESCRIPTION: Example code demonstrating how to instantiate and use the WatsonDiscoveryRM class to retrieve information from Watson Discovery. This snippet shows the complete workflow of creating the retriever and executing a search query.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/WatsonDiscovery.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\n\nretriever_model = WatsonDiscoveryRM(\n    apikey = \"Your API Key\",\n    url = \"URL of the Watson Discovery Service\",\n    version = \"2023-03-31\",\n    project_id = \"Project Id\",\n    collection_ids = [\"Collection ID\"],\n    k = 5\n)\n\nretrieval_response = retriever_model(\"Explore the significance of quantum computing\",k=5)\n\nfor result in retrieval_response:\n    print(\"Document:\", result.long_text, \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Language and Retrieval Models\nDESCRIPTION: Sets up the Llama2-13b-chat language model and ColBERTv2 retrieval model for use in DSPy.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/skycamp2023.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nllama = dspy.HFClientTGI(model=\"meta-llama/Llama-2-13b-chat-hf\", port=[7140, 7141, 7142, 7143], max_tokens=150)\ncolbertv2 = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n\ndspy.settings.configure(rm=colbertv2, lm=llama)\n```\n\n----------------------------------------\n\nTITLE: Compiling Optimized Agents with AvatarOptimizer\nDESCRIPTION: Demonstrates the compilation of arxiv and search agents using the teleprompter optimizer. Takes student models and training datasets as inputs to produce optimized versions.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/avatar_langchain_tools.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\noptimized_arxiv_agent = teleprompter.compile(\n    student=arxiv_agent,\n    trainset=aqa_train\n)\n\noptimized_search_agent = teleprompter.compile(\n    student=search_agent,\n    trainset=sqa_train\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring DSPy for Retrieval and Reranking with ColBERT in Python\nDESCRIPTION: Configures DSPy to use both ColBERT retriever and reranker, and initializes a RetrieveThenRerank object for combined retrieval and reranking.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/integrations/colbert/colbert_local.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndspy.settings.configure(rm=colbert_retriever,reranker=colbert_reranker)\n\nretrieve_rerank = dspy.RetrieveThenRerank(k=5)\n```\n\n----------------------------------------\n\nTITLE: Displaying Best Prompts Discovered by MIPRO in Python\nDESCRIPTION: This code snippet prints the best prompts discovered by MIPRO during optimization, showing how the instructions evolve as trials progress.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/qa/hotpot/hotpotqa_with_MIPRO.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nbest_score = 0\n\ndef get_signature(predictor):\n    if (hasattr(predictor, 'extended_signature')):\n        return predictor.extended_signature\n    elif (hasattr(predictor, 'signature')):\n        return predictor.signature\n\nprint(f\"Baseline program | Score: {best_score}:\")\nfor i,predictor in enumerate(program.predictors()):\n    print(f\"Prompt {i+1} Instruction: {get_signature(predictor).instructions}\")\nprint()\n\nprint(\"----------------\")\n\nfor trial_num in compiled_program.trial_logs:\n    program_score = compiled_program.trial_logs[trial_num][\"score\"]\n    program_pruned = compiled_program.trial_logs[trial_num][\"pruned\"]\n    if program_score > best_score and not program_pruned and compiled_program.trial_logs[trial_num][\"full_eval\"]:\n        best_score = program_score\n        best_program_so_far = compiled_program.trial_logs[trial_num][\"program\"]\n    if trial_num % 5 == 0:\n        print(f\"Best program after {trial_num} batches | Score: {best_score}:\")\n        for i,predictor in enumerate(best_program_so_far.predictors()):\n            print(f\"Prompt {i+1} Instruction: {get_signature(predictor).instructions}\")\n        print()\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Banking77 Dataset in Python\nDESCRIPTION: Code to load the Banking77 dataset, extract class names, and prepare a list of examples for training. It uses the Hugging Face datasets library and DSPy's DataLoader.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\nimport random\nfrom dspy.datasets import DataLoader\nfrom datasets import load_dataset\n\n# Load the Banking77 dataset.\nCLASSES = load_dataset(\"PolyAI/banking77\", split=\"train\", trust_remote_code=True).features['label'].names\nkwargs = dict(fields=(\"text\", \"label\"), input_keys=(\"text\",), split=\"train\", trust_remote_code=True)\n\n# Load the first 2000 examples from the dataset, and assign a hint to each *training* example.\nraw_data = [\n    dspy.Example(x, label=CLASSES[x.label]).with_inputs(\"text\")\n    for x in DataLoader().from_huggingface(dataset_name=\"PolyAI/banking77\", **kwargs)[:1000]\n]\n\nrandom.Random(0).shuffle(raw_data)\n```\n\n----------------------------------------\n\nTITLE: Validating Finetuned Classifier in Python\nDESCRIPTION: This snippet demonstrates how to validate a finetuned classifier by testing it on a single input. It uses the classify_ft function to classify a given text input.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclassify_ft(text=\"I didn't receive my money earlier and it says the transaction is still in progress. Can you fix it?\")\n```\n\n----------------------------------------\n\nTITLE: HotPotQA Task Setup\nDESCRIPTION: Sets up a multi-hop question answering task using HotPotQA dataset and defines the BasicMH module.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/finetune/_unpolished_finetune_demo.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass BasicMH(dspy.Module):\n    def __init__(self, passages_per_hop=3, num_hops=2):\n        super().__init__()\n        self.num_hops = 2\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n        self.generate_query = [dspy.ChainOfThought(\"context, question -> search_query\") for _ in range(self.num_hops)]\n        self.generate_answer = dspy.ChainOfThought(\"context, question -> answer\")\n    \n    def forward(self, question):\n        context = []\n        \n        for hop in range(self.num_hops):\n            search_query = self.generate_query[hop](context=context, question=question).search_query\n            passages = self.retrieve(search_query).passages\n            context = deduplicate(context + passages)\n\n        answer = self.generate_answer(context=context, question=question).copy(context=context)\n        return answer\n```\n\n----------------------------------------\n\nTITLE: Performing Retrieval and Reranking with ColBERT in Python\nDESCRIPTION: Demonstrates how to perform retrieval followed by reranking using the configured ColBERT retriever and reranker in DSPy.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/integrations/colbert/colbert_local.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npred = retrieve_rerank(\n    [\"What is the meaning of life?\",\"Meaning of pain?\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Saving DSPy Programs in MLflow Experiment using Python\nDESCRIPTION: This code demonstrates how to save a DSPy program in an MLflow experiment for better reproducibility, collaboration, and experiment tracking. It includes steps to save the program and load it back from MLflow.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/entity_extraction/index.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Start an MLflow Run and save the program\nwith mlflow.start_run(run_name=\"optimized_extractor\"):\n    model_info = mlflow.dspy.log_model(\n        optimized_people_extractor,\n        artifact_path=\"model\", # Any name to save the program in MLflow\n    )\n\n# Load the program back from MLflow\nloaded = mlflow.dspy.load_model(model_info.model_uri)\n```\n\n----------------------------------------\n\nTITLE: Importing and Using BestOfN Class in Python\nDESCRIPTION: This code snippet shows how to import and potentially use the BestOfN class from the dspy library. It includes a list of available methods and attributes that can be accessed on BestOfN instances.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/api/modules/BestOfN.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy import BestOfN\n\n# Available methods and attributes:\n# __call__\n# batch\n# deepcopy\n# dump_state\n# forward\n# get_lm\n# load\n# load_state\n# map_named_predictors\n# named_parameters\n# named_predictors\n# named_sub_modules\n# parameters\n# predictors\n# reset_copy\n# save\n# set_lm\n\n# Example usage (hypothetical):\n# best_of_n = BestOfN()\n# result = best_of_n(input_data)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Tools for Avatar Module in DSPy\nDESCRIPTION: Configures tool objects for Google Serper search and Arxiv API that can be used by the Avatar module. Each tool is defined with a name, description, and input/output types for proper integration.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/avatar_langchain_tools.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.predict.avatar import Tool, Avatar\nfrom langchain_community.utilities import GoogleSerperAPIWrapper, ArxivAPIWrapper\n\ntools = [\n    Tool(\n        tool=GoogleSerperAPIWrapper(),\n        name=\"WEB_SEARCH\",\n        desc=\"If you have a question, you can use this tool to search the web for the answer.\"\n    ),\n    Tool(\n        tool=ArxivAPIWrapper(),\n        name=\"ARXIV_SEARCH\",\n        desc=\"Pass the arxiv paper id to get the paper information.\",\n        input_type=\"Arxiv Paper ID\",\n    ),\n]\n```\n\n----------------------------------------\n\nTITLE: Compiling Basic LongFormQA with BootstrapFewShotWithRandomSearch\nDESCRIPTION: This code demonstrates how to use DSPy's BootstrapFewShotWithRandomSearch teleprompter to compile the basic LongFormQA module. It optimizes for answer correctness by bootstrapping few-shot demonstrations and searching for the best program configuration.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/longformqa/longformqa_assertions.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nlongformqa = LongFormQA()\nteleprompter = BootstrapFewShotWithRandomSearch(metric = answer_correctness, max_bootstrapped_demos=2, num_candidate_programs=6)\ncited_longformqa = teleprompter.compile(student = longformqa, teacher = longformqa, trainset=trainset, valset=devset[:25])\nevaluate(cited_longformqa)\n```\n\n----------------------------------------\n\nTITLE: Custom Provider Implementation\nDESCRIPTION: Implements a custom provider class for fine-tuning with a dummy fine-tune method.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/finetune/_unpolished_finetune_demo.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass CustomProvider(Provider):\n\n    def __init__(self):\n        super().__init__()\n        self.finetunable = True\n\n    @staticmethod\n    def finetune(\n        job: TrainingJob,\n        model: str,\n        train_data: List[Dict[str, Any]],\n        train_kwargs: Optional[Dict[str, Any]] = None,\n        data_format: Optional[DataFormat] = None,\n    ) -> str:\n\n        # Fake fine-tuning\n        print(\"Fake fine-tuning has started!!\")\n        time.sleep(15)\n        print(\"Done\")\n\n        # Return the new model name\n        model = \"ft:gpt-4o-mini-2024-07-18:stanford::AMDsC653\"\n        return model\n```\n\n----------------------------------------\n\nTITLE: Using MilvusRM for Document Retrieval in Python\nDESCRIPTION: Example code showing how to use the MilvusRM client to retrieve documents from a Milvus collection. It sets up the OpenAI API key, initializes the retriever with collection details, and performs a search query with specified parameters.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/MilvusRM.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.retrieve.milvus_rm import MilvusRM\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_API_KEY>\"\n\nretriever_model = MilvusRM(\n    collection_name=\"<YOUR_COLLECTION_NAME>\",\n    uri=\"<YOUR_MILVUS_URI>\",\n    token=\"<YOUR_MILVUS_TOKEN>\"  # ignore this if no token is required for Milvus connection\n    )\n\nresults = retriever_model(\"Explore the significance of quantum computing\", k=5)\n\nfor result in results:\n    print(\"Document:\", result.long_text, \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Freeing GPU Memory in Python\nDESCRIPTION: This code frees GPU memory by killing the finetuned language model. It's used to prepare for the next step of bootstrapped finetuning.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclassify_ft.get_lm().kill()\n```\n\n----------------------------------------\n\nTITLE: HFClientTGI Constructor Implementation\nDESCRIPTION: The implementation of the HFClientTGI constructor which initializes the client for communicating with the TGI server. It accepts parameters for model ID, port, URL, and additional HTTP request configuration.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/language_model_clients/lm_local_models/HFClientTGI.md#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass HFClientTGI(HFModel):\n    def __init__(self, model, port, url=\"http://future-hgx-1\", http_request_kwargs=None, **kwargs):\n```\n\n----------------------------------------\n\nTITLE: Using YouRM for Web Search - Python Example\nDESCRIPTION: Example code demonstrating how to initialize and use the YouRM class for web searching. Shows how to set up the retriever with an API key and perform a search query about US national parks.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/YouRM.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.retrieve.you_rm import YouRM\nimport os\n\n# The retriever obtains the API key from the `YDC_API_KEY` env var\nretriever_model = YouRM(endpoint=\"search\")\n\nresults = retriever_model(\"Tell me about national parks in the US\", k=5)\n\nfor result in results:\n    print(\"Document:\", result.long_text, \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Loading Datasets from Hugging Face using DSPy DataLoader\nDESCRIPTION: Initializes a DataLoader and uses it to load the SearchQA and ArxivQA datasets from Hugging Face. This prepares the raw data for processing in DSPy format.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/avatar_langchain_tools.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom random import sample\nfrom dspy.datasets import DataLoader\n\ndl = DataLoader()\n```\n\nLANGUAGE: python\nCODE:\n```\nsearchqa = dl.from_huggingface(\n    \"lucadiliello/searchqa\",\n    split=\"train\",\n    input_keys=(\"question\",),\n)\n\narxiv_qa = dl.from_huggingface(\n    \"taesiri/arxiv_qa\",\n    split=\"train\",\n    input_keys=(\"question\", \"paper_id\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Enhancing Agent with Web Search Capability\nDESCRIPTION: Improves the agent by adding a Tavily web search tool to access current information, addressing the outdated data issue identified during debugging.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/observability/index.md#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.predict.react import Tool\nfrom tavily import TavilyClient\n\nsearch_client = TavilyClient(api_key=\"<YOUR_TAVILY_API_KEY>\")\n\ndef web_search(query: str) -> list[str]:\n    \"\"\"Run a web search and return the content from the top 5 search results\"\"\"\n    response = search_client.search(query)\n    return [r[\"content\"] for r in response[\"results\"]]\n\nagent = dspy.ReAct(\"question -> answer\", tools=[Tool(web_search)])\n\nprediction = agent(question=\"Which baseball team does Shohei Ohtani play for?\")\nprint(agent.answer)\n```\n\n----------------------------------------\n\nTITLE: Creating Training and Development Data\nDESCRIPTION: Defines training and development datasets for question-answering tasks using DSPy Example objects.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/skycamp2023.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain = [('Who was the director of the 2009 movie featuring Peter Outerbridge as William Easton?', 'Kevin Greutert'),\n         ('The heir to the Du Pont family fortune sponsored what wrestling team?', 'Foxcatcher'),\n         ('In what year was the star of To Hell and Back born?', '1925'),\n         ('Which award did the first book of Gary Zukav receive?', 'U.S. National Book Award'),\n         ('What documentary about the Gilgo Beach Killer debuted on A&E?', 'The Killing Season'),\n         ('Which author is English: John Braine or Studs Terkel?', 'John Braine'),\n         ('Who produced the album that included a re-recording of \"Lithium\"?', 'Butch Vig')]\n\ntrain = [dspy.Example(question=question, answer=answer).with_inputs('question') for question, answer in train]\n\ndev = [('Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?', 'E. L. Doctorow'),\n       ('Right Back At It Again contains lyrics co-written by the singer born in what city?', 'Gainesville, Florida'),\n       ('What year was the party of the winner of the 1971 San Francisco mayoral election founded?', '1828'),\n       ('Anthony Dirrell is the brother of which super middleweight title holder?', 'Andre Dirrell'),\n       ('The sports nutrition business established by Oliver Cookson is based in which county in the UK?', 'Cheshire'),\n       ('Find the birth date of the actor who played roles in First Wives Club and Searching for the Elephant.', 'February 13, 1980'),\n       ('Kyle Moran was born in the town on what river?', 'Castletown River'),\n       (\"The actress who played the niece in the Priest film was born in what city, country?\", 'Surrey, England'),\n       ('Name the movie in which the daughter of Noel Harrison plays Violet Trefusis.', 'Portrait of a Marriage'),\n       ('What year was the father of the Princes in the Tower born?', '1442'),\n       ('What river is near the Crichton Collegiate Church?', 'the River Tyne'),\n       ('Who purchased the team Michael Schumacher raced for in the 1995 Monaco Grand Prix in 2000?', 'Renault'),\n       ('Andr Zucca was a French photographer who worked with a German propaganda magazine published by what Nazi organization?', 'the Wehrmacht')]\n\ndev = [dspy.Example(question=question, answer=answer).with_inputs('question') for question, answer in dev]\n```\n\n----------------------------------------\n\nTITLE: Configuring DSPy Settings and Initializing Retriever in Python\nDESCRIPTION: Configures DSPy settings to use the ColBERT retriever and initializes a Retrieve object for document retrieval.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/integrations/colbert/colbert_local.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndspy.settings.configure(rm=colbert_retriever)\n\nretrieved_docs = dspy.Retrieve(k=5)\n```\n\n----------------------------------------\n\nTITLE: Sample Output from DSPy Agent\nDESCRIPTION: Shows the (incorrect) response from the DSPy agent, which demonstrates the need for debugging as the information retrieved is outdated.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/observability/index.md#2025-04-07_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nShohei Ohtani plays for the Los Angeles Angels.\n```\n\n----------------------------------------\n\nTITLE: Initializing MilvusRM Constructor in Python\nDESCRIPTION: The constructor for the MilvusRM class, which configures the connection to a Milvus database and sets up retrieval parameters. It accepts collection name, connection details, and optional custom embedding function.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/MilvusRM.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nMilvusRM(\n    collection_name: str,\n    uri: Optional[str] = \"http://localhost:19530\",\n    token: Optional[str] = None,\n    db_name: Optional[str] = \"default\",\n    embedding_function: Optional[Callable] = None,\n    k: int = 3,\n)\n```\n\n----------------------------------------\n\nTITLE: Basic LM Fine-tuning Setup\nDESCRIPTION: Demonstrates basic fine-tuning of a language model using LM.finetune() with a dummy dataset.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/finetune/_unpolished_finetune_demo.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmessage = {\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"},\n    {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"},\n    {\"role\": \"assistant\", \"content\": \"384,400 kilometers\"},\n  ]\n}\ntraining_data = [message] * 20\n\ntrain_kwargs = {\n  \"n_epochs\": 1,\n}\n\njob = lm.finetune(\n  train_data=training_data,\n  train_kwargs=train_kwargs,\n  data_format=\"chat\",\n)\ntype(job)\n```\n\n----------------------------------------\n\nTITLE: Initializing Neo4jRM Class in Python\nDESCRIPTION: Constructor for the Neo4jRM class that sets up a connection to a Neo4j database with vector search capabilities. It requires credentials through environment variables and configures the vector index, text property, and embedding model parameters.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/Neo4jRM.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nNeo4jRM(\n    index_name: str,\n    text_node_property: str,\n    k: int = 5,\n    retrieval_query: str = None,\n    embedding_provider: str = \"openai\",\n    embedding_model: str = \"text-embedding-ada-002\",\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Responses Directly with HFClientTGI\nDESCRIPTION: Python code to generate responses by directly calling the _generate method of the TGI client. This allows for direct interaction with the TGI server without using DSPy's higher-level abstractions.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/language_model_clients/lm_local_models/HFClientTGI.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresponse = tgi_llama2._generate(prompt='What is the capital of Paris?')\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing HotPotQA Dataset\nDESCRIPTION: Loads the HotPotQA dataset, splits it into train and dev sets, and preprocesses the examples for use with DSPy.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/longformqa/longformqa_assertions.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0, keep_details=True)\ntrainset = [x.with_inputs('question') for x in dataset.train]\ndevset = [x.with_inputs('question') for x in dataset.dev]\n```\n\nLANGUAGE: python\nCODE:\n```\ntrain_example = trainset[0]\nprint(f\"Question: {train_example.question}\")\nprint(f\"Answer: {train_example.answer}\")\nprint(f\"Relevant Wikipedia Titles: {train_example.gold_titles}\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndev_example = devset[18]\nprint(f\"Question: {dev_example.question}\")\nprint(f\"Answer: {dev_example.answer}\")\nprint(f\"Relevant Wikipedia Titles: {dev_example.gold_titles}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing HFClientTGI in Python\nDESCRIPTION: Python code to initialize the HFClientTGI client with required parameters including model ID, port, and URL. This client connects to the TGI server running locally.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/language_model_clients/lm_local_models/HFClientTGI.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntgi_llama2 = dspy.HFClientTGI(model=\"meta-llama/Llama-2-7b-hf\", port=8080, url=\"http://localhost\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up DSPy Environment for LongFormQA\nDESCRIPTION: Installs and imports necessary libraries, sets up the DSPy environment with GPT-3.5 and ColBERTv2 retriever.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/longformqa/longformqa_assertions.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nimport regex as re\n\ntry: # When on google Colab, let's clone the notebook so we download the cache.\n    import google.colab  # noqa: F401\n    repo_path = 'dspy'\n    \n    !git -C $repo_path pull origin || git clone https://github.com/stanfordnlp/dspy $repo_path\nexcept:\n    repo_path = '.'\n\nif repo_path not in sys.path:\n    sys.path.append(repo_path)\n\n\nimport pkg_resources # Install the package if it's not installed\nif \"dspy-ai\" not in {pkg.key for pkg in pkg_resources.working_set}:\n    !pip install -U pip\n    !pip install dspy-ai\n    !pip install openai~=0.28.1\n    !pip install -e $repo_path\n\nimport dspy\nfrom dspy.predict import Retry\nfrom dspy.datasets import HotPotQA\n\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch\nfrom dsp.utils import normalize_text\nfrom dspy.primitives.assertions import assert_transform_module, backtrack_handler\n\n%cd dspy/examples/longformqa\n\nfrom utils import extract_text_by_citation, citations_check\n```\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n```\n\nLANGUAGE: python\nCODE:\n```\ncolbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\ndspy.settings.configure(rm=colbertv2_wiki17_abstracts)\nturbo = dspy.OpenAI(model='gpt-4o-mini', max_tokens=500)\ndspy.settings.configure(lm=turbo, trace=[], temperature=0.7)\n```\n\n----------------------------------------\n\nTITLE: Installing MATH Dataset Dependencies\nDESCRIPTION: Command to install the MATH benchmark dataset package\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/math/index.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n%pip install git+https://github.com/hendrycks/math.git\n```\n\n----------------------------------------\n\nTITLE: Configuring DSPy Environment and Dependencies\nDESCRIPTION: Sets up the DSPy environment, installs necessary packages, and configures paths. It also sets up caching for the notebook.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/skycamp2023.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\n\ntry: # When on google Colab, let's clone the notebook so we download the cache.\n    import google.colab  # noqa: F401\n    repo_path = 'dspy'\n    !git -C $repo_path pull origin || git clone https://github.com/stanfordnlp/dspy $repo_path\nexcept:\n    repo_path = '.'\n\nif repo_path not in sys.path:\n    sys.path.append(repo_path)\n\n# Set up the cache for this notebook\nos.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = os.path.join(repo_path, 'cache')\n\nimport pkg_resources # Install the package if it's not installed\nif \"dspy-ai\" not in {pkg.key for pkg in pkg_resources.working_set}:\n    !pip install -U pip\n    !pip install dspy-ai==2.1\n    # !pip install -e $repo_path\n\n!pip install transformers\n```\n\n----------------------------------------\n\nTITLE: Splitting Dataset with train_test_split in Python\nDESCRIPTION: Shows how to split a dataset into training and testing sets using an 80-20 split ratio. The code demonstrates the usage of train_test_split method from DataLoader class and prints the size of the resulting datasets.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/dataloaders/dataloaders_dolly.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsplits = dl.train_test_split(dataset=dolly_100_dataset, train_size=0.8)\ntrain_dataset = splits['train']\ntest_dataset = splits['test']\n\nprint(f\"Number of total examples in dataset: {len(dolly_100_dataset)}\")\nprint(f\"Number of examples in train set: {len(train_dataset)}\")\nprint(f\"Number of examples in test set: {len(test_dataset)}\")\n```\n\n----------------------------------------\n\nTITLE: Using FalkordbRM for Vector Search in Python\nDESCRIPTION: Example code demonstrating how to use the FalkordbRM class in a local environment. It shows setting up environment variables, initializing the retriever model, and performing a search query to retrieve relevant passages.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/FalkordbRM.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.retrieve.falkordb_rm import FalkordbRM\nimport os\n\n\nos.environ[\"FALKORDB_HOST\"] = 'localhost'\nos.environ[\"FALKORDB_PORT\"] = 6379\nos.environ[\"OPENAI_API_KEY\"] = 'sk-'\n\nretriever_model = FalkordbRM(\n    node_label=\"myIndex\",\n    text_node_property=\"text\",\n    embedding_node_property=\"embedding\"\n)\n\nresults = retriever_model(\"Explore the significance of quantum computing\", k=3)\n\nfor passage in results:\n    print(\"Document:\", passage, \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Importing DSPy Modules\nDESCRIPTION: Imports necessary DSPy modules for evaluation and few-shot learning.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/skycamp2023.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\nfrom dspy.evaluate import Evaluate\nfrom dspy.teleprompt import BootstrapFewShot, BootstrapFewShotWithRandomSearch\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Neo4jRM with Local Neo4j Database\nDESCRIPTION: A complete example showing how to set up environment variables, initialize the Neo4jRM retriever, and perform a vector search query against a local Neo4j database. The example prints the retrieved passages.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/Neo4jRM.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.retrieve.neo4j_rm import Neo4jRM\nimport os\n\nos.environ[\"NEO4J_URI\"] = 'bolt://localhost:7687'\nos.environ[\"NEO4J_USERNAME\"] = 'neo4j'\nos.environ[\"NEO4J_PASSWORD\"] = 'password'\nos.environ[\"OPENAI_API_KEY\"] = 'sk-'\n\nretriever_model = Neo4jRM(\n    index_name=\"vector\",\n    text_node_property=\"text\"\n)\n\nresults = retriever_model(\"Explore the significance of quantum computing\", k=3)\n\nfor passage in results:\n    print(\"Document:\", passage, \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Wikipedia Abstracts Dataset\nDESCRIPTION: Downloads and extracts a compressed file containing 5 million Wikipedia abstracts from 2017 to use as the retrieval corpus. The dataset is approximately 500MB compressed.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.utils import download\n\ndownload(\"https://huggingface.co/dspy/cache/resolve/main/wiki.abstracts.2017.tar.gz\")\n!tar -xzvf wiki.abstracts.2017.tar.gz\n```\n\n----------------------------------------\n\nTITLE: SnowflakeRM Quickstart Implementation Example in Python\nDESCRIPTION: Complete example demonstrating how to set up and use the SnowflakeRM class for retrieving passages from Snowflake through a Cortex Search endpoint, including connection establishment and query execution.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/SnowflakeRM.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.retrieve.snowflake_rm import SnowflakeRM\nfrom snowflake.snowpark import Session\nimport os\n\nconnection_parameters = {\n\n    \"account\": os.getenv('SNOWFLAKE_ACCOUNT'),\n    \"user\": os.getenv('SNOWFLAKE_USER'),\n    \"password\": os.getenv('SNOWFLAKE_PASSWORD'),\n    \"role\": os.getenv('SNOWFLAKE_ROLE'),\n    \"warehouse\": os.getenv('SNOWFLAKE_WAREHOUSE'),\n    \"database\": os.getenv('SNOWFLAKE_DATABASE'),\n    \"schema\": os.getenv('SNOWFLAKE_SCHEMA')}\n\n# Establish connection to Snowflake\nsnowpark = Session.builder.configs(connection_parameters).create()\n\nsnowflake_retriever = SnowflakeRM(snowflake_session=snowpark,\n    cortex_search_service=\"<YOUR_CORTEX_SEARCH_SERVICE_NAME>\",\n    snowflake_database=\"<YOUR_SNOWFLAKE_DATABASE_NAME>\",\n    snowflake_schema=\"<YOUR_SNOWFLAKE_SCHEMA_NAME>\",\n    auto_filter=True,\n    k = 5)\n\nresults = snowflake_retriever(\"Explore the meaning of life\",\n    response_columns=[\"<NAME_OF_INDEXED_COLUMN>\",\"<NAME_OF_ATTRIBUTE_COLUMN\"])\n\nfor result in results:\n    print(\"Document:\", result.long_text, \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for BM25 Retrieval\nDESCRIPTION: Installs the required Python packages for implementing BM25 retrieval, including the BM25S library, PyStemmer for stemming, and JAX for CPU operations.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n> pip install -U bm25s PyStemmer \"jax[cpu]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow for DSPy Tracing\nDESCRIPTION: Sets up MLflow tracing integration with DSPy by enabling autologging and creating an experiment to organize traces, providing a foundation for comprehensive observability.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/observability/index.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.dspy.autolog()\n\n# This is optional. Create an MLflow Experiment to store and organize your traces.\nmlflow.set_experiment(\"DSPy\")\n```\n\n----------------------------------------\n\nTITLE: Running TGI Server with HuggingFace Token for Meta Llama 2 Models\nDESCRIPTION: Docker command for running the TGI server with Meta Llama 2 models, which requires version 0.9.3+ of the docker image and passing a HuggingFace authentication token as an environment variable.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/language_model_clients/lm_local_models/HFClientTGI.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data -e HUGGING_FACE_HUB_TOKEN={your_token} ghcr.io/huggingface/text-generation-inference:0.9.3 --model-id $model --num-shard $num_shard\n```\n\n----------------------------------------\n\nTITLE: Initializing BetterTogether with Metric\nDESCRIPTION: Initializes the BetterTogether optimization framework with a specified metric for evaluation.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/finetune/_unpolished_finetune_demo.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# (1) The only required argument we require for BetterTogether is the metric\nbetter_together = dspy.BetterTogether(\n  metric=metric        # This is the only metric we require!\n                       # We could also consider not requiring it if BootstrapFewShotWithRandomSearch is modified.\n)\n```\n\n----------------------------------------\n\nTITLE: Installing SGLang for Local Inference in Python\nDESCRIPTION: Commands to install SGLang, which is used to run local inference servers. This includes upgrading pip, installing uv, and installing sglang with specific dependencies.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n> pip install --upgrade pip\n> pip install uv\n> uv pip install \"sglang[all]>=0.4.4.post3\" --find-links https://flashinfer.ai/whl/cu124/torch2.5/flashinfer-python\n```\n\n----------------------------------------\n\nTITLE: FalkordbRM Forward Method Signature in Python\nDESCRIPTION: The forward method of the FalkordbRM class that performs vector search using the specified query. It retrieves the top k passages matching the query based on embedding similarity and returns them as a dspy.Prediction object.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/FalkordbRM.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nforward(self, query: [str], k: Optional[int] = None) -> dspy.Prediction\n```\n\n----------------------------------------\n\nTITLE: Setting up DSPy environment with auto-reload and installation\nDESCRIPTION: Configures the Python environment with auto-reload, clones the DSPy repository if running on Google Colab, adds the repository to the system path, sets up caching, and installs required packages if not already installed.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/intro.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\n\ntry: # When on google Colab, let's clone the notebook so we download the cache.\n    import google.colab  # noqa: F401\n    repo_path = 'dspy'\n    !git -C $repo_path pull origin || git clone https://github.com/stanfordnlp/dspy $repo_path\nexcept:\n    repo_path = '.'\n\nif repo_path not in sys.path:\n    sys.path.append(repo_path)\n\n# Set up the cache for this notebook\nos.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = os.path.join(repo_path, 'cache')\n\nimport pkg_resources # Install the package if it's not installed\nif \"dspy-ai\" not in {pkg.key for pkg in pkg_resources.working_set}:\n    !pip install -U pip\n    !pip install dspy-ai==2.4.17\n    !pip install openai~=0.28.1\n    # !pip install -e $repo_path\n\nimport dspy\n```\n\n----------------------------------------\n\nTITLE: Instantiating QdrantRM with OpenAI Embeddings for Semantic Search\nDESCRIPTION: This example demonstrates how to set up a QdrantRM retriever with OpenAI embeddings. It configures a QdrantClient, creates an OpenAIVectorizer, initializes the QdrantRM retriever, and performs a simple retrieval operation.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/QdrantRM.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom qdrant_client import QdrantClient\n\nimport dspy\nfrom dsp.modules.sentence_vectorizer import OpenAIVectorizer\nfrom dspy.retrieve.qdrant_rm import QdrantRM\n\nos.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_API_KEY>\"\n\nclient = QdrantClient(url=\"http://localhost:6333/\")\nvectorizer = OpenAIVectorizer(model=\"text-embedding-3-small\")\n\nqdrant_retriever = QdrantRM(\n    qdrant_client=client,\n    qdrant_collection_name=\"{collection_name}\",\n    vectorizer=vectorizer,\n    document_field=\"text\",\n)\n\ndspy.settings.configure(rm=qdrant_retriever)\nretrieve = dspy.Retrieve()\n\nretrieve(\"Some computer programs.\")\n```\n\n----------------------------------------\n\nTITLE: Initializing WatsonDiscoveryRM Class in Python\nDESCRIPTION: Constructor for the WatsonDiscoveryRM class that sets up connection parameters for Watson Discovery service. It requires authentication credentials, service URLs, and collection information to establish the retrieval model.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/WatsonDiscovery.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass WatsonDiscoveryRM:\n    def __init__(\n        self,\n        apikey: str,\n        url:str,\n        version:str,\n        project_id:str,\n        collection_ids:list=[],\n        k: int = 7,\n    ):\n```\n\n----------------------------------------\n\nTITLE: Initializing DSPy Environment and Imports\nDESCRIPTION: Sets up the Python environment with autoreload and imports necessary DSPy modules for question answering tasks.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/qa/hotpot/multihop_finetune.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n\nimport dspy\nfrom dspy.evaluate import Evaluate\nfrom dspy.datasets.hotpotqa import HotPotQA\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch, BootstrapFinetune\n```\n\n----------------------------------------\n\nTITLE: Initializing SnowflakeRM Constructor in Python\nDESCRIPTION: Constructor signature for the SnowflakeRM class which requires a Snowflake Snowpark session, Cortex Search service name, database and schema names, auto-filtering option, and optional number of results to retrieve.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/SnowflakeRM.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nSnowflakeRM(\n     snowflake_session: object,\n     cortex_search_service: str,\n     snowflake_database: str,\n     snowflake_schema: dict,\n     auto_filter:bool,\n     k: int = 3,\n)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Dataset Example\nDESCRIPTION: Code to display a sample from the training dataset\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/math/index.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nexample = dataset.train[0]\nprint(\"Question:\", example.question)\nprint(\"Answer:\", example.answer)\n```\n\n----------------------------------------\n\nTITLE: Inspecting QWen Language Model History in Python\nDESCRIPTION: Simple script to inspect the history of a QWen language model instance.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/vlm/mmmu.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nqwen_lm.inspect_history()\n```\n\n----------------------------------------\n\nTITLE: Initializing DSPy Environment\nDESCRIPTION: Sets up the DSPy environment by enabling autoreload and importing required libraries.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/finetune/_unpolished_finetune_demo.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n\nimport os # noqa\n\n# os.environ[\"DSPY_CACHEDIR\"] =\n# os.environ[\"DSP_CACHEDIR\"] =\n# os.environ[\"OPENAI_API_KEY\"] =\n\n# Import the library\nimport dspy\n```\n\n----------------------------------------\n\nTITLE: MMMU Model Evaluation in Python\nDESCRIPTION: Script to evaluate MMMU model performance and output scores.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/vlm/mmmu.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nscores, outputs = evaluate_mmmu(mmmu)\n# lm.inspect_history()\n```\n\n----------------------------------------\n\nTITLE: Loading MATH Dataset\nDESCRIPTION: Loading and initializing the MATH algebra dataset\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/math/index.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.datasets import MATH\n\ndataset = MATH(subset='algebra')\nprint(len(dataset.train), len(dataset.dev))\n```\n\n----------------------------------------\n\nTITLE: Image Processing with Black Squares in Python\nDESCRIPTION: Function to replace images with black squares for testing, including verification of image replacement.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/vlm/mmmu.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport PIL\ndef set_image_to_black_square(example, key):\n    example_copy = example.copy()\n    example_copy[key] = PIL.Image.open(\"black_image_300x300.png\")\n    return example_copy.with_inputs(*example.inputs().keys())\n\nprint(updated_devset[0][\"image_1\"])\nprint(updated_devset[0][\"image_2\"])\nexamples_no_image_1 = list(map(lambda x: set_image_to_black_square(x, \"image_1\"), updated_valset))\nprint(examples_no_image_1[0][\"image_1\"] == PIL.Image.open(\"black_image_300x300.png\"))\nprint(examples_no_image_1[0][\"image_2\"] == PIL.Image.open(\"black_image_300x300.png\"))\nexamples_no_image_2 = list(map(lambda x: set_image_to_black_square(x, \"image_2\"), updated_valset))\nprint(examples_no_image_2[0][\"image_1\"] == PIL.Image.open(\"black_image_300x300.png\"))\nprint(examples_no_image_2[0][\"image_2\"] == PIL.Image.open(\"black_image_300x300.png\"))\n\nexamples_no_actual_image = list(map(lambda x: set_image_to_black_square(x, \"image_1\"), updated_valset))\nexamples_no_actual_image = list(map(lambda x: set_image_to_black_square(x, \"image_2\"), examples_no_actual_image))\nprint(examples_no_actual_image[0][\"image_1\"] == PIL.Image.open(\"black_image_300x300.png\"))\nprint(examples_no_actual_image[0][\"image_2\"] == PIL.Image.open(\"black_image_300x300.png\"))\n```\n\n----------------------------------------\n\nTITLE: JPG Conversion Utility in Python\nDESCRIPTION: Function to convert images to JPG format with verification of conversion.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/vlm/mmmu.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport io\nfrom PIL import Image\n\ndef convert_to_jpg(example):\n    example_copy = example.copy()\n    for key in ['image_1', 'image_2']:\n        if key in example_copy and isinstance(example_copy[key], Image.Image):\n            img = example[key].convert('RGB')\n            buffer = io.BytesIO()\n            img.save(buffer, format='JPEG')\n            buffer.seek(0)\n            example_copy[key] = Image.open(buffer)\n    \n    return example_copy.with_inputs(*example.inputs().keys())\n\nexamples_jpg = list(map(convert_to_jpg, updated_valset))\n\nprint(\"Original image format:\", updated_valset[0]['image_1'].format)\nprint(\"Converted image format:\", examples_jpg[0]['image_1'].format)\n```\n\n----------------------------------------\n\nTITLE: Adding DSPy Package to System Path in Python\nDESCRIPTION: Adds the parent directory to the system path to allow importing of the DSPy package. This step is necessary if DSPy is not installed as a package.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/integrations/colbert/colbert_local.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nsys.path.append(\"../../..\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation Page Layout with HTML Comments\nDESCRIPTION: HTML comment block containing YAML-style configuration for documentation page layout, including sidebar position and navigation visibility settings, along with a notebook include directive.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/intro-tutorialX.md#2025-04-07_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!-- ---\nsidebar_position: 1\nhide:\n  - navigation\n  # - toc\n---\n\n::: notebook\nintro-tutorial.ipynb\n::: -->\n```\n\n----------------------------------------\n\nTITLE: Color Image URL Generator in Python\nDESCRIPTION: Functions to generate placeholder image URLs with specific colors.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/vlm/mmmu.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncolors = {\n    \"White\": \"FFFFFF\",\n    \"Red\": \"FF0000\",\n    \"Green\": \"00FF00\",\n    \"Blue\": \"0000FF\",\n    \"Yellow\": \"FFFF00\",\n    \"Cyan\": \"00FFFF\",\n    \"Magenta\": \"FF00FF\",\n    \"Gray\": \"808080\",\n    \"Orange\": \"FFA500\",\n    \"Purple\": \"800080\"\n}\ndef get_color_image_url(color, file_extension=\"png\"):\n    return f\"https://placehold.co/300/{colors[color]}/{colors[color]}.{file_extension}\"\n```\n\n----------------------------------------\n\nTITLE: Cloning the Text-Generation-Inference Repository\nDESCRIPTION: Command to clone the Hugging Face Text-Generation-Inference repository from GitHub to set up the TGI server locally.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/language_model_clients/lm_local_models/HFClientTGI.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/text-generation-inference.git\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation for ColBERTv2 Class in Python\nDESCRIPTION: This code snippet sets up the documentation configuration for the ColBERTv2 class. It specifies which members to include, formatting options, and other display preferences for the generated documentation.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/api/tools/ColBERTv2.md#2025-04-07_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n::: dspy.ColBERTv2\n    handler: python\n    options:\n        members:\n            - __call__\n        show_source: true\n        show_root_heading: true\n        heading_level: 2\n        docstring_style: google\n        show_root_full_path: true\n        show_object_full_path: false\n        separate_signature: false\n        inherited_members: true\n```\n\n----------------------------------------\n\nTITLE: AI Feedback Metric Implementation for Tweet Validation\nDESCRIPTION: Complex metric implementation using AI feedback to assess tweet quality across multiple dimensions including correctness and engagement.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/evaluation/metrics.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef metric(gold, pred, trace=None):\n    question, answer, tweet = gold.question, gold.answer, pred.output\n\n    engaging = \"Does the assessed text make for a self-contained, engaging tweet?\"\n    correct = f\"The text should answer `{question}` with `{answer}`. Does the assessed text contain this answer?\"\n    \n    correct =  dspy.Predict(Assess)(assessed_text=tweet, assessment_question=correct)\n    engaging = dspy.Predict(Assess)(assessed_text=tweet, assessment_question=engaging)\n\n    correct, engaging = [m.assessment_answer for m in [correct, engaging]]\n    score = (correct + engaging) if correct and (len(tweet) <= 280) else 0\n\n    if trace is not None: return score >= 2\n    return score / 2.0\n```\n\n----------------------------------------\n\nTITLE: Configuring documentation options for dspy.InferRules in Python\nDESCRIPTION: This code snippet sets up the documentation configuration for the dspy.InferRules class. It specifies which members to include, how to display the source code, heading levels, and other formatting options for the generated documentation.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/api/optimizers/InferRules.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: dspy.InferRules\n    handler: python\n    options:\n        members:\n            - compile\n            - evaluate_program\n            - format_examples\n            - get_params\n            - get_predictor_demos\n            - induce_natural_language_rules\n            - update_program_instructions\n        show_source: true\n        show_root_heading: true\n        heading_level: 2\n        docstring_style: google\n        show_root_full_path: true\n        show_object_full_path: false\n        separate_signature: false\n        inherited_members: true\n```\n\n----------------------------------------\n\nTITLE: Defining SummaryCorrectness Signature for Quality Assessment in DSPy\nDESCRIPTION: Creates a signature class that compares a generated summary to the key ideas in the original passage. It assigns binary scores for each key idea and computes an overall score, forming the second part of the metric program.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/summarization.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass SummaryCorrectness(dspy.Signature):\n    \"\"\"\n    Compare a system generated summary to the key ideas in the passage.\n    For every key idea supplied,\n    assign a binary score based on whether the summary contains it.\n    And compute an overall score based on the binary scores.\n    \"\"\"\n\n    key_ideas: str = dspy.InputField(\n        desc=\"key ideas in the passage \"\n             \"for evaluating the summary\")\n    summary: str = dspy.InputField()\n    binary_scores: list[bool] = dspy.OutputField(\n        desc=\"list of binary scores for each key idea, \"\n             \"e.g. [True, False, True]\")\n    overall_score: float = dspy.OutputField(\n        desc=\"overall score for the summary out of 1.0\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Mkdocs Documentation for COPRO Class in Python\nDESCRIPTION: This code snippet sets up the documentation configuration for the COPRO class in DSPy. It specifies which methods to include, how to display the documentation, and various formatting options for the generated documentation.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/api/optimizers/COPRO.md#2025-04-07_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n::: dspy.COPRO\n    handler: python\n    options:\n        members:\n            - compile\n            - get_params\n        show_source: true\n        show_root_heading: true\n        heading_level: 2\n        docstring_style: google\n        show_root_full_path: true\n        show_object_full_path: false\n        separate_signature: false\n        inherited_members: true\n```\n\n----------------------------------------\n\nTITLE: Loading OpenAI API Key from JSON File in Python\nDESCRIPTION: This code reads an OpenAI API key from a JSON file named 'creds.json'. It's used to authenticate API requests to OpenAI's services.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/knn.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith open(\"creds.json\", \"r\") as creds:\n    api_key = json.loads(creds.read())[\"openai_key\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation Generation for DSPy inspect_history Module\nDESCRIPTION: This code snippet configures the documentation generation for the inspect_history module in DSPy. It sets various options such as showing source code, specifying heading levels, and handling inherited members. The configuration is likely used by a documentation generation tool to produce comprehensive API documentation.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/api/utils/inspect_history.md#2025-04-07_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n::: dspy.inspect_history\n    handler: python\n    options:\n        show_source: true\n        show_root_heading: true\n        heading_level: 2\n        docstring_style: google\n        show_root_full_path: true\n        show_object_full_path: false\n        separate_signature: false\n        inherited_members: true\n```\n\n----------------------------------------\n\nTITLE: Using ColBERT Reranker as Standalone Model in Python\nDESCRIPTION: Shows how to use the ColBERT reranker as a standalone model to score a set of passages for a given query. It also demonstrates how to display the results in a tabular format.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/integrations/colbert/colbert_local.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport tabulate\n\nscores_arr = colbert_reranker(\n    \"What is the meaning of life and pain?\",\n    # Pass a subset of passages\n    passages[:10]\n)\n\ntabulate_data = []\nfor idx in np.argsort(scores_arr)[::-1]:\n    # print(f\"Passage = {passages[idx]} --> Score = {scores_arr[idx]}\")\n    tabulate_data.append([passages[idx],scores_arr[idx]])\n\ntable = tabulate.tabulate(tabulate_data,tablefmt=\"html\",headers={'sentence','score'})\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation for DSPy enable_logging Function\nDESCRIPTION: This code snippet defines the documentation configuration for the enable_logging function in DSPy. It specifies various options for rendering the function's documentation, including source code display, heading levels, docstring style, and inheritance handling.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/api/utils/enable_logging.md#2025-04-07_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n::: dspy.enable_logging\n    handler: python\n    options:\n        show_source: true\n        show_root_heading: true\n        heading_level: 2\n        docstring_style: google\n        show_root_full_path: true\n        show_object_full_path: false\n        separate_signature: false\n        inherited_members: true\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML Frontmatter for DSPy Homepage\nDESCRIPTION: This YAML frontmatter block configures the homepage for the DSPy project documentation. It specifies the template to use and hides navigation and table of contents elements for a cleaner presentation.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/index_.md#2025-04-07_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntemplate: home.html\nhide:\n  - navigation\n  - toc\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring DSPy Load Module Documentation\nDESCRIPTION: YAML configuration block for documenting the dspy.load module. Specifies documentation generation settings including source code display, heading styles, docstring handling, and path display options.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/api/utils/load.md#2025-04-07_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n::: dspy.load\n    handler: python\n    options:\n        show_source: true\n        show_root_heading: true\n        heading_level: 2\n        docstring_style: google\n        show_root_full_path: true\n        show_object_full_path: false\n        separate_signature: false\n        inherited_members: true\n```\n\n----------------------------------------\n\nTITLE: Configuring MkDocs Documentation for DSPy History Module\nDESCRIPTION: This configuration block specifies how the DSPy History module documentation should be rendered. It sets options for displaying source code, heading style, docstring format, and inheritance relationships.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/api/primitives/History.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n::: dspy.History\n    handler: python\n    options:\n        show_source: true\n        show_root_heading: true\n        heading_level: 2\n        docstring_style: google\n        show_root_full_path: true\n        show_object_full_path: false\n        separate_signature: false\n        inherited_members: true\n```\n\n----------------------------------------\n\nTITLE: Setting up DSPy with GPT-3.5 and ColBERTv2 for QA\nDESCRIPTION: Configures the language model (GPT-3.5) and the retrieval model (ColBERTv2 over Wikipedia) for use in DSPy.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/multi_agent.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\nfrom dspy.evaluate import Evaluate\nfrom dspy.datasets.hotpotqa import HotPotQA\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch\n\ngpt3 = dspy.OpenAI('gpt-3.5-turbo-0125', max_tokens=1000)\ncolbert = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\ndspy.configure(lm=gpt3, rm=colbert)\n```\n\n----------------------------------------\n\nTITLE: Loading GSM8K Dataset from Hugging Face\nDESCRIPTION: Loads the GSM8K mathematical reasoning dataset from Hugging Face and splits it into training and development sets. A random sample of 50 examples from the test set is used for evaluation.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/optimizers/bootstrap-fewshot.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport random\nfrom dspy.datasets import DataLoader\n\ndl = DataLoader()\n\ngsm8k = dl.from_huggingface(\n    \"gsm8k\",\n    \"main\",\n    input_keys = (\"question\",),\n)\n\ntrainset, devset = gsm8k['train'], random.sample(gsm8k['test'], 50)\n```\n\n----------------------------------------\n\nTITLE: Implementing Math Reasoning with ChainOfThought Module\nDESCRIPTION: Demonstrates using DSPy's ChainOfThought module to solve mathematical probability problems with step-by-step reasoning.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/index.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmath = dspy.ChainOfThought(\"question -> answer: float\")\nmath(question=\"Two dice are tossed. What is the probability that the sum equals two?\")\n```\n\n----------------------------------------\n\nTITLE: Initializing BootstrapFinetune Teleprompter in Python\nDESCRIPTION: Constructor for the BootstrapFinetune class that extends the Teleprompter base class. It sets up parameters for bootstrapping and fine-tuning, including metric function, teacher settings, and multitask options.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/optimizers/BootstrapFinetune.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass BootstrapFinetune(Teleprompter):\n    def __init__(self, metric=None, teacher_settings={}, multitask=True):\n```\n\n----------------------------------------\n\nTITLE: Evaluating Metrics on Tweet Generator\nDESCRIPTION: Code to evaluate multiple metrics on the tweet generator including hashtags, correctness, length, engagement, and faithfulness metrics.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/tweets/tweets_assertions.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmetrics = [no_hashtags_metric, is_correct_metric, within_length_metric, engaging_metric, faithful_metric, overall_metric]\n\nfor metric in metrics:\n    evaluate = Evaluate(metric=metric, devset=devset, num_threads=1, display_progress=True, display_table=5)\n    evaluate(tweeter)\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation Generation for BootstrapFewShotWithRandomSearch in Python\nDESCRIPTION: This code snippet configures the documentation generation settings for the BootstrapFewShotWithRandomSearch class. It specifies which members to include, how to display the source code, and various formatting options.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/api/optimizers/BootstrapFewShotWithRandomSearch.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: dspy.BootstrapFewShotWithRandomSearch\n    handler: python\n    options:\n        members:\n            - compile\n            - get_params\n        show_source: true\n        show_root_heading: true\n        heading_level: 2\n        docstring_style: google\n        show_root_full_path: true\n        show_object_full_path: false\n        separate_signature: false\n        inherited_members: true\n```\n\n----------------------------------------\n\nTITLE: Initializing AvatarOptimizer in Python\nDESCRIPTION: Creates an instance of AvatarOptimizer with specific parameters for metric optimization. Configures settings for iteration limits and input sampling for the comparator module.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/avatar_langchain_tools.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.teleprompt import AvatarOptimizer\n\nteleprompter = AvatarOptimizer(\n    metric=metric,\n    max_iters=1,\n    max_negative_inputs=10,\n    max_positive_inputs=10,\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Arxiv Agent with Thread-Safe Evaluator\nDESCRIPTION: Shows how to evaluate the optimized arxiv agent using the thread-safe evaluator method provided by AvatarOptimizer.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/avatar_langchain_tools.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nteleprompter.thread_safe_evaluator(aqa_test, optimized_arxiv_agent)\n```\n\n----------------------------------------\n\nTITLE: Testing BasicQABot with a Sample Question in Python\nDESCRIPTION: This code creates an instance of BasicQABot and tests it with a sample historical question. It demonstrates how to use the bot and retrieve its answer.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/knn.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nqa_bot = BasicQABot()\npred = qa_bot.forward(\"In the 10th Century A.D. Ealhswith had a son called thelweard by which English king?\")\npred.answer\n```\n\n----------------------------------------\n\nTITLE: MkDocs Configuration for dspy.Parallel Documentation\nDESCRIPTION: MkDocs directive configuration for documenting the dspy.Parallel class. The configuration specifies which methods to include, formatting options, and documentation style preferences.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/api/modules/Parallel.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n::: dspy.Parallel\n    handler: python\n    options:\n        members:\n            - __call__\n            - forward\n        show_source: true\n        show_root_heading: true\n        heading_level: 2\n        docstring_style: google\n        show_root_full_path: true\n        show_object_full_path: false\n        separate_signature: false\n        inherited_members: true\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Dataset for Code Plagiarism Detection\nDESCRIPTION: This code loads a TSV dataset containing code samples and plagiarism information, then converts it into DSPy Example objects for further processing. It specifies input fields for the model.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/multi-input-output/beginner-multi-input-output.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\"https://raw.githubusercontent.com/williambrach/LLM-plagiarism-check/main/data/train.tsv\", sep=\"\\t\")\ndf.head()\n\ndef create_example(row: pd.Series) -> dspy.Example:\n    return dspy.Example(\n        code_sample_1=row[\"sample_1\"],\n        code_sample_2=row[\"sample_2\"],\n        plagiarized=\"Yes\" if row[\"plagiarized\"] else \"No\",\n        explanation=row[\"reason\"],\n    ).with_inputs(\"code_sample_1\", \"code_sample_2\")\n\n\nexamples = []\nfor _, row in df.iterrows():\n    example = create_example(row)\n    examples.append(example)\n```\n\n----------------------------------------\n\nTITLE: Defining Search Query Generation Signature in DSPy\nDESCRIPTION: Creates a DSPy signature for generating search queries that help find missing information needed to answer complex questions.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/simplified-baleen.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass GenerateSearchQuery(dspy.Signature):\n    \"\"\"Write a simple search query that will help answer a complex question.\"\"\"\n\n    context = dspy.InputField(desc=\"may contain relevant facts\")\n    question = dspy.InputField()\n    query = dspy.OutputField()\n```\n\n----------------------------------------\n\nTITLE: Indexing Corpus with BM25 Retriever\nDESCRIPTION: Creates a BM25 index for the Wikipedia corpus with English stemming and stopword removal. Configures the BM25 retriever with specific k1 and b parameters for tuning the ranking algorithm.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport bm25s\nimport Stemmer\n\nstemmer = Stemmer.Stemmer(\"english\")\ncorpus_tokens = bm25s.tokenize(corpus, stopwords=\"en\", stemmer=stemmer)\n\nretriever = bm25s.BM25(k1=0.9, b=0.4)\nretriever.index(corpus_tokens)\n```\n\n----------------------------------------\n\nTITLE: Configuring DSPy Settings with OpenAI Model\nDESCRIPTION: Configures DSPy settings to use the GPT-4o-mini model from OpenAI with specific parameters like temperature and token limits. This setup is used for all subsequent language model interactions.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/avatar_langchain_tools.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport dspy\n\ndspy.settings.configure(\n    lm=dspy.OpenAI(\n        model=\"gpt-4o-mini\",\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        max_tokens=4000,\n        temperature=0,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing MIPRO Optimization Progress in Python\nDESCRIPTION: This snippet creates a plot to visualize the optimization progress over trials, showing how performance increases over time until it saturates.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/qa/hotpot/hotpotqa_with_MIPRO.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\ntrial_logs = compiled_program.trial_logs\n\n# Extracting trial numbers, scores, and pruning status\ntrial_numbers = list(trial_logs.keys())\nscores = [trial_logs[trial]['score'] for trial in trial_numbers]\npruning_status = [trial_logs[trial]['pruned'] for trial in trial_numbers]\n\n# Plot setup\nplt.figure(figsize=(5, 3))\n\n# Plotting each point\nfor trial_number, score, pruned in zip(trial_numbers, scores, pruning_status):\n    if pruned:\n        plt.scatter(trial_number, score, color='grey', label='Pruned Batch' if 'Pruned Batch' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n    else:\n        plt.scatter(trial_number, score, color='green', label='Successful Batch' if 'Successful Batch' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n\nplt.xlabel('Batch Number')\nplt.ylabel('Score')\nplt.title('Batch Scores')\nplt.grid(True)\nplt.legend()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Evaluation for GSM8K\nDESCRIPTION: Creates an evaluator for the GSM8K dataset using the gsm8k_metric. The evaluator will assess model performance on the development set with multithreading and progress display.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/optimizers/bootstrap-fewshot.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.evaluate import Evaluate\n\nevaluate = Evaluate(devset=devset[:], metric=gsm8k_metric, num_threads=NUM_THREADS, display_progress=True, display_table=False)\n```\n\n----------------------------------------\n\nTITLE: Creating Avatar Agents for Different QA Tasks\nDESCRIPTION: Instantiates two Avatar agents: one for general search QA tasks and one for Arxiv-specific QA tasks. Each agent is configured with the appropriate signature and tools, with verbose logging enabled.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/avatar_langchain_tools.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\narxiv_agent = Avatar(\n    tools=tools,\n    signature=ArxivQASignature,\n    verbose=True,\n)\n\nsearch_agent = Avatar(\n    tools=tools,\n    signature=SearchQASignature,\n    verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling Experimental Features in DSPy\nDESCRIPTION: Enables experimental features in DSPy, which are required for using LM.finetune(), BSFT, and BetterTogether.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/finetune/_unpolished_finetune_demo.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Using LM.finetune(), BSFT, and BetterTogether requires this flag\ndspy.settings.experimental = True\n```\n\n----------------------------------------\n\nTITLE: Creating Unlabeled Training Set in Python\nDESCRIPTION: Code to create an unlabeled training set by sampling 500 queries from the Banking77 dataset. This set will be used for bootstrapped fine-tuning.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nunlabeled_trainset = [dspy.Example(text=x.text).with_inputs(\"text\") for x in raw_data[:500]]\n\nunlabeled_trainset[0]\n```\n\n----------------------------------------\n\nTITLE: Installing DSPy Package via pip\nDESCRIPTION: Command to install the latest version of DSPy Python package using pip package manager.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/agents/index.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U dspy\n```\n\n----------------------------------------\n\nTITLE: Preparing Dev Set for Evaluation in Python\nDESCRIPTION: This code prepares a small development set for evaluation by selecting a subset of the raw data. It demonstrates how to create a dev set and access its first item.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndevset = raw_data[500:600]\ndevset[0]\n```\n\n----------------------------------------\n\nTITLE: Evaluating Teacher Classifier in Python\nDESCRIPTION: This code evaluates the teacher classifier for comparison with the finetuned model. It uses the same evaluate function to assess the performance of the teacher_classify model.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nevaluate(teacher_classify)\n```\n\n----------------------------------------\n\nTITLE: Displaying Resource Tables in Markdown\nDESCRIPTION: This snippet shows how to create tables in Markdown to list various DSPy resources, including blogs, videos, and podcasts. It demonstrates the use of Markdown table syntax to organize and present information in a structured format.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/community/community-resources.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| **Name** | **Link** |\n|---|---|\n| **Why I bet on DSPy** | [Blog](https://blog.isaacbmiller.com/posts/dspy) |\n| **Not Your Average Prompt Engineering** | [Blog](https://jina.ai/news/dspy-not-your-average-prompt-engineering/) |\n| **Why I'm excited about DSPy** | [Blog](https://substack.stephen.so/p/why-im-excited-about-dspy) |\n| **Achieving GPT-4 Performance at Lower Cost** | [Link](https://gradient.ai/blog/achieving-gpt-4-level-performance-at-lower-cost-using-dspy) |\n```\n\n----------------------------------------\n\nTITLE: Initializing Sentiment Classification with Inline DSPy Signature in Python\nDESCRIPTION: This snippet demonstrates how to use an inline DSPy Signature to create a sentiment classification task. It defines a simple signature that takes a sentence as input and returns a boolean sentiment.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/programming/signatures.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsentence = \"it's a charming and often affecting journey.\"  # example from the SST-2 dataset.\n\nclassify = dspy.Predict('sentence -> sentiment: bool')  # we'll see an example with Literal[] later\nclassify(sentence=sentence).sentiment\n```\n\n----------------------------------------\n\nTITLE: Compiling the QA Pipeline with Bootstrap Few-Shot Learning\nDESCRIPTION: Optimizes the SimplifiedBaleen pipeline using DSPy's BootstrapFewShot teleprompter to improve performance on multi-hop question answering tasks.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/simplified-baleen.md#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.teleprompt import BootstrapFewShot\n\nteleprompter = BootstrapFewShot(metric=validate_context_and_answer_and_hops)\ncompiled_baleen = teleprompter.compile(SimplifiedBaleen(), teacher=SimplifiedBaleen(passages_per_hop=2), trainset=trainset)\n```\n\n----------------------------------------\n\nTITLE: Configuring Language Model for DSPy Summarization\nDESCRIPTION: Sets up the GPT-4o-mini model from OpenAI with a maximum token limit of 250 as the language model for the DSPy summarization system.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/summarization.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlm = dspy.LM(\"openai/gpt-4o-mini\", max_tokens=250)\ndspy.settings.configure(lm=lm)\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation Settings for LabeledFewShot Class\nDESCRIPTION: YAML configuration block that defines documentation generation parameters for the LabeledFewShot class in DSPy. Specifies which members to include, display options for headings and signatures, and inheritance settings.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/api/optimizers/LabeledFewShot.md#2025-04-07_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndspy.LabeledFewShot:\n    handler: python\n    options:\n        members:\n            - compile\n            - get_params\n        show_source: true\n        show_root_heading: true\n        heading_level: 2\n        docstring_style: google\n        show_root_full_path: true\n        show_object_full_path: false\n        separate_signature: false\n        inherited_members: true\n```\n\n----------------------------------------\n\nTITLE: Navigating to the Repository Directory\nDESCRIPTION: Command to change into the cloned Text-Generation-Inference repository directory.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/language_model_clients/lm_local_models/HFClientTGI.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd text-generation-inference\n```\n\n----------------------------------------\n\nTITLE: Configuring BetterTogether with Weight and Prompt Optimizers\nDESCRIPTION: Sets up BetterTogether with custom weight and prompt optimizers, including BootstrapFinetune and BootstrapFewShotWithRandomSearch.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/finetune/_unpolished_finetune_demo.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# (2) We can also pass the weight and prompt optimizers we initialized\ntrain_kwargs = {\n  \"n_epochs\": 1,\n}\nadapter = dspy.ChatAdapter()\n\nweight_optimizer = dspy.BootstrapFinetune(\n  metric=metric,               # Can be left empty, leads to no filtering\n  multitask=True,              # We can also handle False!\n  train_kwargs=train_kwargs,   # Can be left empty\n  adapter=adapter,             # Can be left empty, leads to adapters inferred from the LM\n  exclude_demos=True,          # We are dropping the demos for fine-tuning \n  num_threads = 1,             # Can be left empty\n)\n\nprompt_optimizer = dspy.BootstrapFewShotWithRandomSearch(\n    metric=metric,\n    max_bootstrapped_demos=3,\n    max_labeled_demos=3,\n    num_candidate_programs=6,\n    num_threads=6\n)\n\n# Initialize BetterTogether\nbetter_together = dspy.BetterTogether(\n  metric=metric,\n  weight_optimizer=weight_optimizer,   # Can be left empty\n  prompt_optimizer=prompt_optimizer,   # Can be left empty\n  seed=2023,                           # Can be left empty\n)\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Compiled Model in Python\nDESCRIPTION: Demonstrates how to save the compiled model to a JSON file and optionally load it back for future use.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/optimizers/bfrs.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncot_compiled.save('turbo_gsm8k.json')\n\n# Loading:\n# cot = CoT()\n# cot.load('turbo_gsm8k.json')\n```\n\n----------------------------------------\n\nTITLE: Implementing and Running a DSPy Program with LM Usage Tracking\nDESCRIPTION: This code snippet defines a simple DSPy program that makes multiple language model calls, configures DSPy with tracking enabled, runs the program, and prints the usage statistics. It demonstrates how to set up and use LM usage tracking in a complete example.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/programming/modules.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\n\n# Configure DSPy with tracking enabled\ndspy.settings.configure(\n    lm=dspy.LM(\"openai/gpt-4o-mini\", cache=False),\n    track_usage=True\n)\n\n# Define a simple program that makes multiple LM calls\nclass MyProgram(dspy.Module):\n    def __init__(self):\n        self.predict1 = dspy.ChainOfThought(\"question -> answer\")\n        self.predict2 = dspy.ChainOfThought(\"question, answer -> score\")\n\n    def __call__(self, question: str) -> str:\n        answer = self.predict1(question=question)\n        score = self.predict2(question=question, answer=answer)\n        return score\n\n# Run the program and check usage\nprogram = MyProgram()\noutput = program(question=\"What is the capital of France?\")\nprint(output.get_lm_usage())\n```\n\n----------------------------------------\n\nTITLE: Optimizing ReAct agent with BootstrapFewShotWithRandomSearch\nDESCRIPTION: Uses DSPy's BootstrapFewShotWithRandomSearch optimizer to create and optimize examples of the ReAct program, improving its performance.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/multi_agent.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconfig = dict(max_bootstrapped_demos=2, max_labeled_demos=0, num_candidate_programs=20, num_threads=8)\ntp = BootstrapFewShotWithRandomSearch(metric=dspy.evaluate.answer_exact_match, **config)\noptimized_react = tp.compile(agent, trainset=trainset, valset=valset)\n\nevaluate(optimized_react)\n```\n\n----------------------------------------\n\nTITLE: Detailed MIPRO Program Compilation with Configuration Options\nDESCRIPTION: Complete implementation of program compilation including options to load pre-compiled programs or compile from scratch. Includes hyperparameter configuration and detailed compilation setup.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/nli/scone/scone_with_MIPRO.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cloudpickle as pickle\nfrom dspy.teleprompt import MIPROv2\n\nLOAD_PRECOMPILED_PROGRAM = False\ncompiled_program = program.deepcopy()\n\nif LOAD_PRECOMPILED_PROGRAM:\n    compiled_program.load(compiled_program_file_path)\n    with open(trial_logs_file_path, \"rb\") as f:\n        trial_logs = pickle.load(f)\n    compiled_program.trial_logs = trial_logs\nelse:\n    N = 10\n    batches = 30\n    temperature = 1.0\n\n    eval_kwargs = dict(num_threads=16, display_progress=True, display_table=0)\n    teleprompter = MIPROv2(prompt_model=prompt_model, task_model=task_model, metric=metric, num_candidates=N, init_temperature=temperature, verbose=True)\n    print(trainset[:10])\n    compiled_program = teleprompter.compile(program, trainset=trainset, valset=valset, num_batches=batches, max_bootstrapped_demos=1,max_labeled_demos=2, eval_kwargs=eval_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Using ReAct Module for Question Answering in Python\nDESCRIPTION: This code snippet shows how to use the instantiated ReAct module to answer a specific question. It demonstrates the process of calling the module with an input question and retrieving the final predicted answer.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/modules/react.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Call the ReAct module on a particular input\nquestion = 'Aside from the Apple Remote, what other devices can control the program Apple Remote was originally designed to interact with?'\nresult = react_module(question=question)\n\nprint(f\"Question: {question}\")\nprint(f\"Final Predicted Answer (after ReAct process): {result.answer}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing LongFormQA with Assertions in Python\nDESCRIPTION: This class extends the basic LongFormQA implementation by adding DSPy assertions that enforce citation requirements and faithfulness checks. It uses dspy.Suggest to guide the model to include citations every 1-2 sentences and ensure all cited content is faithful to the source context.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/longformqa/longformqa_assertions.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass LongFormQAWithAssertions(dspy.Module):\n    def __init__(self, passages_per_hop=3, max_hops=2):\n        super().__init__()\n        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n        self.generate_cited_paragraph = dspy.ChainOfThought(GenerateCitedParagraph)\n        self.max_hops = max_hops\n    \n    def forward(self, question):\n        context = []\n        for hop in range(self.max_hops):\n            query = self.generate_query[hop](context=context, question=question).query\n            passages = self.retrieve(query).passages\n            context = deduplicate(context + passages)\n        pred = self.generate_cited_paragraph(context=context, question=question)\n        pred = dspy.Prediction(context=context, paragraph=pred.paragraph)\n        dspy.Suggest(citations_check(pred.paragraph), \"Make sure every 1-2 sentences has citations. If any 1-2 sentences lack citations, add them in 'text... [x].' format.\", target_module=self.generate_cited_paragraph)\n        _, unfaithful_outputs = citation_faithfulness(None, pred, None)\n        if unfaithful_outputs:\n            unfaithful_pairs = [(output['text'], output['context']) for output in unfaithful_outputs]\n            for _, context in unfaithful_pairs:\n                dspy.Suggest(len(unfaithful_pairs) == 0, f\"Make sure your output is based on the following context: '{context}'.\", target_module=self.generate_cited_paragraph)\n        else:\n            return pred\n        return pred\n```\n\n----------------------------------------\n\nTITLE: Exploring a training example from HotPotQA dataset\nDESCRIPTION: Displays a training example from the dataset, showing the question and its corresponding answer.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/intro.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_example = trainset[0]\nprint(f\"Question: {train_example.question}\")\nprint(f\"Answer: {train_example.answer}\")\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow for Enhanced Tracing\nDESCRIPTION: Command to install MLflow version 2.18.0 or newer, which is required for using the tracing capabilities with DSPy.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/observability/index.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install -U mlflow>=2.18.0\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing HoVer Dataset in Python\nDESCRIPTION: This snippet loads the HoVer dataset from Hugging Face, preprocesses it to select 3-hop examples, and splits it into train, dev, and test sets. It uses DSPy's DataLoader and custom filtering logic.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport random\nfrom dspy.datasets import DataLoader\n\nkwargs = dict(fields=(\"claim\", \"supporting_facts\", \"hpqa_id\", \"num_hops\"), input_keys=(\"claim\",))\nhover = DataLoader().from_huggingface(dataset_name=\"hover-nlp/hover\", split=\"train\", trust_remote_code=True, **kwargs)\n\nhpqa_ids = set()\nhover = [\n    dspy.Example(claim=x.claim, titles=list(set([y[\"key\"] for y in x.supporting_facts]))).with_inputs(\"claim\")\n    for x in hover\n    if x[\"num_hops\"] == 3 and x[\"hpqa_id\"] not in hpqa_ids and not hpqa_ids.add(x[\"hpqa_id\"])\n]\n\nrandom.Random(0).shuffle(hover)\ntrainset, devset, testset = hover[:200], hover[200:500], hover[650:]\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation for dspy.ChainOfThought module\nDESCRIPTION: This YAML configuration block specifies documentation parameters for the dspy.ChainOfThought module, including which members to document, source code visibility, heading formatting, and inheritance options.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/api/modules/ChainOfThought.md#2025-04-07_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n::: dspy.ChainOfThought\n    handler: python\n    options:\n        members:\n            - __call__\n            - batch\n            - deepcopy\n            - dump_state\n            - forward\n            - get_lm\n            - load\n            - load_state\n            - map_named_predictors\n            - named_parameters\n            - named_predictors\n            - named_sub_modules\n            - parameters\n            - predictors\n            - reset_copy\n            - save\n            - set_lm\n        show_source: true\n        show_root_heading: true\n        heading_level: 2\n        docstring_style: google\n        show_root_full_path: true\n        show_object_full_path: false\n        separate_signature: false\n        inherited_members: true\n```\n\n----------------------------------------\n\nTITLE: Initializing DSPy Environment and Dependencies\nDESCRIPTION: Sets up the DSPy environment with required imports, installations and configurations including OpenAI API integration\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/tweets/tweets_assertions.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nimport os\nimport regex as re\n\ntry:\n    import google.colab\n    repo_path = 'dspy'\n    !git -C $repo_path pull origin || git clone https://github.com/stanfordnlp/dspy $repo_path\nexcept:\n    repo_path = '.'\n\nif repo_path not in sys.path:\n    sys.path.append(repo_path)\n```\n\n----------------------------------------\n\nTITLE: Forward Method Definition for SnowflakeRM in Python\nDESCRIPTION: Definition of the forward method for the SnowflakeRM class, which queries the Cortex Search service to retrieve relevant results based on a query or list of queries, with optional filtering and result count customization.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/SnowflakeRM.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self,query_or_queries: Union[str, list[str]],response_columns:list[str],filters:dict = None, k: Optional[int] = None)-> dspy.Prediction:\n```\n\n----------------------------------------\n\nTITLE: Setting Up Module Evaluation\nDESCRIPTION: Configuring and running evaluation on the module using dev dataset\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/math/index.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nTHREADS = 24\nkwargs = dict(num_threads=THREADS, display_progress=True, display_table=5)\nevaluate = dspy.Evaluate(devset=dataset.dev, metric=dataset.metric, **kwargs)\n\nevaluate(module)\n```\n\n----------------------------------------\n\nTITLE: Implementing TweeterWithAssertions Class\nDESCRIPTION: Defines a DSPy module that implements tweet generation with assertions for quality control, checking for hashtags, length limits, answer correctness, engagement and faithfulness.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/tweets/tweets_assertions.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass TweeterWithAssertions(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.generate_tweet = dspy.ChainOfThought(GenerateTweet)\n\n    def forward(self, question, answer):\n        context = []\n        max_hops=2\n        passages_per_hop=3\n        generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n        retrieve = dspy.Retrieve(k=passages_per_hop)\n        for hop in range(max_hops):\n            query = generate_query[hop](context=context, question=question).query\n            passages = retrieve(query).passages\n            context = deduplicate(context + passages)\n        generated_tweet = self.generate_tweet(question=question, context=context).tweet\n        dspy.Suggest(has_no_hashtags(generated_tweet), \"Please revise the tweet to remove hashtag phrases following it.\", target_module=self.generate_tweet)\n        dspy.Suggest(is_within_length_limit(generated_tweet, 280), f\"Please ensure the tweet is within {280} characters.\", target_module=self.generate_tweet)\n        dspy.Suggest(has_correct_answer(generated_tweet, answer), \"The tweet does not include the correct answer to the question. Please revise accordingly.\", target_module=self.generate_tweet)\n        engaging_question = \"Does the assessed text make for a self-contained, engaging tweet? Say no if it is not engaging.\"\n        engaging_assessment = dspy.Predict(AssessTweet)(context=context, assessed_text=generated_tweet, assessment_question=engaging_question)\n        dspy.Suggest(is_assessment_yes(engaging_assessment.assessment_answer), \"The text is not engaging enough. Please revise to make it more captivating.\", target_module=self.generate_tweet)\n        faithful_question = \"Is the assessed text grounded in the context? Say no if it includes significant facts not in the context.\"\n        faithful_assessment = dspy.Predict(AssessTweet)(context='N/A', assessed_text=generated_tweet, assessment_question=faithful_question)\n        dspy.Suggest(is_assessment_yes(faithful_assessment.assessment_answer), \"The text contains unfaithful elements or significant facts not in the context. Please revise for accuracy.\", target_module=self.generate_tweet)\n        return dspy.Prediction(generated_tweet=generated_tweet, context=context)\n```\n\n----------------------------------------\n\nTITLE: Tracking LM Call Costs in DSPy Python Program\nDESCRIPTION: This code calculates the total cost of all language model calls made by the DSPy extractor program, using cost information provided by LiteLLM for certain providers.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/entity_extraction/index.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncost = sum([x['cost'] for x in lm.history if x['cost'] is not None])  # cost in USD, as calculated by LiteLLM for certain providers\ncost\n```\n\n----------------------------------------\n\nTITLE: Implementing LongFormQA Module with Citations\nDESCRIPTION: Defines the LongFormQA module that extends the Multi-Hop QA program to generate long-form paragraphs with citations.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/longformqa/longformqa_assertions.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dsp.utils import deduplicate\n\nclass GenerateSearchQuery(dspy.Signature):\n    \"\"\"Write a simple search query that will help answer a complex question.\"\"\"\n    context = dspy.InputField(desc=\"may contain relevant facts\")\n    question = dspy.InputField()\n    query = dspy.OutputField()\n\nclass GenerateCitedParagraph(dspy.Signature):\n    \"\"\"Generate a paragraph with citations.\"\"\"\n    context = dspy.InputField(desc=\"may contain relevant facts\")\n    question = dspy.InputField()\n    paragraph = dspy.OutputField(desc=\"includes citations\")\n\nclass LongFormQA(dspy.Module):\n    def __init__(self, passages_per_hop=3, max_hops=2):\n        super().__init__()\n        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n        self.generate_cited_paragraph = dspy.ChainOfThought(GenerateCitedParagraph)\n        self.max_hops = max_hops\n    \n    def forward(self, question):\n        context = []\n        for hop in range(self.max_hops):\n            query = self.generate_query[hop](context=context, question=question).query\n            passages = self.retrieve(query).passages\n            context = deduplicate(context + passages)\n        pred = self.generate_cited_paragraph(context=context, question=question)\n        pred = dspy.Prediction(context=context, paragraph=pred.paragraph)\n        return pred\n```\n\n----------------------------------------\n\nTITLE: Implementing Information Extraction Module\nDESCRIPTION: Creates a structured information extraction module that parses text to extract titles, headings, and named entities.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/index.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass ExtractInfo(dspy.Signature):\n    \"\"\"Extract structured information from text.\"\"\"\n    \n    text: str = dspy.InputField()\n    title: str = dspy.OutputField()\n    headings: list[str] = dspy.OutputField()\n    entities: list[dict[str, str]] = dspy.OutputField(desc=\"a list of entities and their metadata\")\n\nmodule = dspy.Predict(ExtractInfo)\n\ntext = \"Apple Inc. announced its latest iPhone 14 today.\" \\\n    \"The CEO, Tim Cook, highlighted its new features in a press release.\"\nresponse = module(text=text)\n\nprint(response.title)\nprint(response.headings)\nprint(response.entities)\n```\n\n----------------------------------------\n\nTITLE: Initializing FalkordbRM Class Constructor in Python\nDESCRIPTION: The constructor for the FalkordbRM class that initializes a vector search retrieval model using Falkordb. It requires node label, text and embedding node properties, and accepts optional parameters for retrieval configuration and embedding generation.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/FalkordbRM.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nFalkordbRM(\n    node_label: str,\n    text_node_property: str,\n    embedding_node_property: str,\n    k: int = 5,\n    retrieval_query: str,\n    embedding_provider: str = \"openai\",\n    embedding_model: str = \"text-embedding-ada-002\",\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluation Results Comparison in Python\nDESCRIPTION: Script to compare evaluation results across different image configurations.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/vlm/mmmu.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnormal = evaluate_mmmu(mmmu, devset=updated_valset)\nno_image_1 = evaluate_mmmu(mmmu, devset=examples_no_image_1)\nno_image_2 = evaluate_mmmu(mmmu, devset=examples_no_image_2)\nno_actual_image = evaluate_mmmu(mmmu, devset=examples_no_actual_image)\nprint(\"Testing on MMMU validation set (N=\", len(updated_valset), \")\")\nprint(\"Score with both images:\", normal)\nprint(\"Score with image_1 set to black square:\", no_image_1)\nprint(\"Score with image_2 set to black square:\", no_image_2)\nprint(\"Score with both images set to black squares:\", no_actual_image)\n```\n\n----------------------------------------\n\nTITLE: Basic Evaluation Loop Implementation in Python\nDESCRIPTION: Simple evaluation loop that processes examples from a development set and calculates metric scores.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/evaluation/metrics.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nscores = []\nfor x in devset:\n    pred = program(**x.inputs())\n    score = metric(x, pred)\n    scores.append(score)\n```\n\n----------------------------------------\n\nTITLE: Evaluating KNN Model on HotPotQA Dataset in Python with DSPy\nDESCRIPTION: This code sets up and runs an evaluation of the compiled KNN model on the HotPotQA dev set. It uses the answer_exact_match metric to assess the model's performance.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/knn.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.evaluate.evaluate import Evaluate\n\n# Set up the `evaluate_on_hotpotqa` function. We'll use this many times below.\nevaluate_on_hotpotqa = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n\n# Evaluate the `compiled_knn` program with the `answer_exact_match` metric.\nmetric = dspy.evaluate.answer_exact_match\n\n    \nevaluate_on_hotpotqa(compiled_knn, metric)\n```\n\n----------------------------------------\n\nTITLE: Importing DSPy and JSON Libraries in Python\nDESCRIPTION: This snippet imports the necessary libraries: dspy for machine learning tasks and json for handling JSON data.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/knn.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\nimport json\n```\n\n----------------------------------------\n\nTITLE: Initializing MIPRO Optimizer and Compiling Program\nDESCRIPTION: Basic setup for MIPRO optimizer and program compilation showing the core functionality. Demonstrates initialization of MIPROv2 optimizer and compilation of a program with training and validation sets.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/nli/scone/scone_with_MIPRO.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nteleprompter = MIPROv2(prompt_model=prompt_model, task_model=task_model, metric=metric, num_candidates=N, init_temperature=temperature)\ncompiled_program = teleprompter.compile(program, trainset=trainset, valset=valset, num_batches=batches, max_bootstrapped_demos=1,max_labeled_demos=2, eval_kwargs=eval_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Defining QA Signatures for Different Dataset Types\nDESCRIPTION: Creates two signature classes for question answering: a general SearchQASignature and an ArxivQASignature that includes paper_id as an additional input field. These signatures define the input/output structure for the QA models.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/avatar_langchain_tools.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass SearchQASignature(dspy.Signature):\n    \"\"\"You will be given a question. Your task is to answer the question.\"\"\"\n    \n    question: str = dspy.InputField(\n        prefix=\"Question:\",\n        desc=\"question to ask\",\n        format=lambda x: x.strip(),\n    )\n    answer: str = dspy.OutputField(\n        prefix=\"Answer:\",\n        desc=\"answer to the question\",\n    )\n\nclass ArxivQASignature(dspy.Signature):\n    \"\"\"You will be given a question and an Arxiv Paper ID. Your task is to answer the question.\"\"\"\n    \n    question: str = dspy.InputField(\n        prefix=\"Question:\",\n        desc=\"question to ask\",\n        format=lambda x: x.strip(),\n    )\n    paper_id: str = dspy.InputField(\n        prefix=\"Paper ID:\",\n        desc=\"Arxiv Paper ID\",\n    )\n    answer: str = dspy.OutputField(\n        prefix=\"Answer:\",\n        desc=\"answer to the question\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Sample Passages for ColBERT Retrieval in Python\nDESCRIPTION: Creates a list of sample passages (proverbs and sayings) to be used for demonstration of ColBERT's retrieval capabilities.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/integrations/colbert/colbert_local.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npassages =  [\"It's a piece of cake.\", \"Don't put off until tomorrow what you can do today.\", 'To kill two birds with one stone.', 'Actions speak louder than words.', 'Honesty is the best policy.', 'If you want something done right, do it yourself.', 'The best things in life are free.', \"Don't count your chickens before they hatch.\", 'She sells seashells by the seashore.', 'Practice makes perfect.', \"Where there's a will, there's a way.\", 'Absence makes the heart grow fonder.', 'When the going gets tough, the tough get going.', 'A journey of a thousand miles begins with a single step.', \"You can't have your cake and eat it too.\", \"If you can't beat them, join them.\", 'Keep your friends close and your enemies closer.', \"Don't put all your eggs in one basket.\", \"All's fair in love and war.\", 'Every dog has its day.', 'All good things must come to an end.', 'Once bitten, twice shy.', \"The apple doesn't fall far from the tree.\", 'A penny saved is a penny earned.', \"Don't bite the hand that feeds you.\", 'You reap what you sow.', 'An apple a day keeps the doctor away.', \"One man's trash is another man's treasure.\", 'The squeaky wheel gets the grease.', 'A picture is worth a thousand words.', 'Fortune favors the bold.', 'Practice what you preach.', 'A watched pot never boils.', 'No pain, no gain.', \"You can't make an omelet without breaking eggs.\", \"There's no place like home.\", 'Ask and you shall receive.', 'Let sleeping dogs lie.', 'If the shoe fits, wear it.', 'Every cloud has a silver lining.', 'Look before you leap.', 'The more, the merrier.', 'The grass is always greener on the other side.', 'Beauty is only skin deep.', \"Two wrongs don't make a right.\", 'Beauty is in the eye of the beholder.', 'Necessity is the mother of invention.', 'Out of sight, out of mind.', 'Patience is a virtue.', 'Curiosity killed the cat.', \"If at first you don't succeed, try, try again.\", \"Beggars can't be choosers.\", 'Too many cooks spoil the broth.', 'Easy come, easy go.', \"Don't cry over spilled milk.\", \"There's no such thing as a free lunch.\", 'A bird in the hand is worth two in the bush.', 'Good things come to those who wait.', 'The quick brown fox jumps over the lazy dog.', 'It takes two to tango.', 'A friend in need is a friend indeed.', 'Like father, like son.', 'Let bygones be bygones.', 'Kill two birds with one stone.', 'A penny for your thoughts.', 'I am the master of my fate, I am the captain of my soul.', 'The pen is mightier than the sword.', 'When in Rome, do as the Romans do.', \"Rome wasn't built in a day.\", \"You can't judge a book by its cover.\", \"It's raining cats and dogs.\", 'Make hay while the sun shines.', \"It's better to be safe than sorry.\", 'The early bird catches the worm.', 'To be or not to be, that is the question.', 'Better late than never.']\n```\n\n----------------------------------------\n\nTITLE: Optimizing DSPy Module\nDESCRIPTION: Configuring and running the MIPROv2 optimizer on the module\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/math/index.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nkwargs = dict(num_threads=THREADS, teacher_settings=dict(lm=gpt4o), prompt_model=gpt4o_mini)\noptimizer = dspy.MIPROv2(metric=dataset.metric, auto=\"medium\", **kwargs)\n\nkwargs = dict(requires_permission_to_run=False, max_bootstrapped_demos=4, max_labeled_demos=4)\noptimized_module = optimizer.compile(module, trainset=dataset.train, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Executing the Uncompiled QA Pipeline\nDESCRIPTION: Runs the SimplifiedBaleen pipeline in zero-shot mode to demonstrate its capability to answer a complex question through multi-hop reasoning.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/simplified-baleen.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Ask any question you like to this simple RAG program.\nmy_question = \"How many storeys are in the castle that David Gregory inherited?\"\n\n# Get the prediction. This contains `pred.context` and `pred.answer`.\nuncompiled_baleen = SimplifiedBaleen()  # uncompiled (i.e., zero-shot) program\npred = uncompiled_baleen(my_question)\n\n# Print the contexts and the answer.\nprint(f\"Question: {my_question}\")\nprint(f\"Predicted Answer: {pred.answer}\")\nprint(f\"Retrieved Contexts (truncated): {[c[:200] + '...' for c in pred.context]}\")\n```\n\n----------------------------------------\n\nTITLE: Compiling and Evaluating RAG Module\nDESCRIPTION: Compiles the RAG module using BootstrapFewShotWithRandomSearch and evaluates its performance on the development set.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/skycamp2023.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nteleprompter2 = BootstrapFewShotWithRandomSearch(metric=metric_EM, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=NUM_THREADS)\nrag_compiled = teleprompter2.compile(RAG(), trainset=train, valset=dev)\n\nevaluate_hotpot(rag_compiled)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-hop Question Answering Module\nDESCRIPTION: Defines a BasicMH class that implements a two-hop question answering system using retrieval and generation steps.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/qa/hotpot/multihop_finetune.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dsp.utils.utils import deduplicate\n\nclass BasicMH(dspy.Module):\n    def __init__(self, passages_per_hop=3):\n        super().__init__()\n\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n        self.generate_query = [dspy.ChainOfThought(\"context, question -> search_query\") for _ in range(2)]\n        self.generate_answer = dspy.ChainOfThought(\"context, question -> answer\")\n    \n    def forward(self, question):\n        context = []\n        \n        for hop in range(2):\n            search_query = self.generate_query[hop](context=context, question=question).search_query\n            passages = self.retrieve(search_query).passages\n            context = deduplicate(context + passages)\n\n        return self.generate_answer(context=context, question=question).copy(context=context)\n```\n\n----------------------------------------\n\nTITLE: Configuring DSPy with HFClientTGI and Using a QA Module\nDESCRIPTION: Example of configuring DSPy with the TGI client as the default language model and using a Chain-of-Thought QA module. This approach is recommended for integrating the TGI client with DSPy programs.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/language_model_clients/lm_local_models/HFClientTGI.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndspy.configure(lm=tgi_llama2)\n\n#Example DSPy CoT QA program\nqa = dspy.ChainOfThought('question -> answer')\n\nresponse = qa(question=\"What is the capital of Paris?\") #Prompted to tgi_llama2\nprint(response.answer)\n```\n\n----------------------------------------\n\nTITLE: Running Evaluation on Long-Form QA Model in Python\nDESCRIPTION: This snippet demonstrates how to instantiate a LongFormQA model and run the evaluation function on it. It's a simple two-line code that creates the model and calls the evaluate function.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/longformqa/longformqa_assertions.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlongformqa = LongFormQA()\nevaluate(longformqa)\n```\n\n----------------------------------------\n\nTITLE: Initializing Multi-Hop QA Task with DSPy and HotPotQA\nDESCRIPTION: Sets up the DSPy environment, defines a BasicMH class for multi-hop question answering, prepares the HotPotQA dataset, and initializes evaluation metrics and retriever model.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/finetune/_unpolished_finetune_demo.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\nfrom dspy.datasets import HotPotQA\nfrom dspy.evaluate import Evaluate # noqa\nfrom dsp.utils.utils import deduplicate # noqa\n\n\n# We are setting the experimental flag to True to make use of the fine-tuning\n# features that are still in development.\ndspy.settings.configure(experimental=True)\n\n# Define the program\nclass BasicMH(dspy.Module):\n    def __init__(self, passages_per_hop=3, num_hops=2):\n        super().__init__()\n        self.num_hops = 2\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n        self.generate_query = [dspy.ChainOfThought(\"context, question -> search_query\") for _ in range(self.num_hops)]\n        self.generate_answer = dspy.ChainOfThought(\"context, question -> answer\")\n    \n    def forward(self, question):\n        context = []\n        \n        for hop in range(self.num_hops):\n            search_query = self.generate_query[hop](context=context, question=question).search_query\n            passages = self.retrieve(search_query).passages\n            context = deduplicate(context + passages)\n\n        answer = self.generate_answer(context=context, question=question).copy(context=context)\n        return answer\n\n# Prepare the dataset\nTRAIN_SIZE = 1000\nDEV_SIZE = 500\ndataset = HotPotQA(train_seed=1, eval_seed=2023, test_size=0, only_hard_examples=True)\ntrainset = [x.with_inputs('question') for x in dataset.train][:TRAIN_SIZE]\ndevset = [x.with_inputs('question') for x in dataset.dev][:DEV_SIZE]\n\n# Prepare the metric and evaluator\nNUM_THREADS = 12\nmetric = dspy.evaluate.answer_exact_match\nevaluate = Evaluate(devset=devset, metric=metric, num_threads=NUM_THREADS, display_progress=True)\n\n# Prepare the retriever model\nCOLBERT_V2_ENDPOINT = \"http://20.102.90.50:2017/wiki17_abstracts\"\nretriever = dspy.ColBERTv2(url=COLBERT_V2_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Initializing DatabricksRM Class Constructor\nDESCRIPTION: Constructor definition for DatabricksRM class that enables querying Databricks Mosaic AI Vector Search indexes. Takes parameters for index configuration, authentication, and retrieval settings.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/DatabricksRM.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDatabricksRM(\n    databricks_index_name: str,\n    databricks_endpoint: Optional[str] = None,\n    databricks_token: Optional[str] = None,\n    columns: Optional[List[str]] = None,\n    filters_json: Optional[str] = None,\n    k: int = 3,\n    docs_id_column_name: str = \"id\",\n    text_column_name: str = \"text\",\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Compiled Program to Disk\nDESCRIPTION: Demonstrates how to save the compiled and optimized program to disk for later use.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/nli/scone/scone_with_MIPRO.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncompiled_program.save(\"compiled_program.dspy\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Simple Answer Validation Metric in Python\nDESCRIPTION: Basic metric function that compares example answer with predicted answer using case-insensitive matching. Returns a boolean value indicating exact match.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/evaluation/metrics.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef validate_answer(example, pred, trace=None):\n    return example.answer.lower() == pred.answer.lower()\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Optimized Model\nDESCRIPTION: Demonstrates how to save the compiled model to a JSON file and provides commented code for loading it back. This allows for reusing the optimized model without retraining.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/optimizers/bootstrap-fewshot.md#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncot_compiled.save('turbo_gsm8k.json')\n\n# Loading:\n# cot = CoT()\n# cot.load('turbo_gsm8k.json')\n```\n\n----------------------------------------\n\nTITLE: Implementing Summarize Module Using ChainOfThought in DSPy\nDESCRIPTION: Creates a module that uses the SummarizeSignature with DSPy's ChainOfThought to generate summaries from passages. This is the main summarization program that will be evaluated and optimized.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/summarization.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass Summarize(dspy.Module):\n    def __init__(self):\n        self.summarize = dspy.ChainOfThought(SummarizeSignature)\n\n    def forward(self, passage: str):\n        summary = self.summarize(\n            passage=passage\n        )\n        return summary\n```\n\n----------------------------------------\n\nTITLE: Implementing ChainOfThought Class in DSPy\nDESCRIPTION: The constructor for ChainOfThought class that extends the Predict functionality with chain-of-thought reasoning. It prepends a reasoning or rationale field to the output signature to enable step-by-step reasoning before generating the final output.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/modules/chain-of-thought.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass ChainOfThought(Predict):\n    def __init__(self, signature, rationale_type=None, activated=True, **config):\n        super().__init__(signature, **config)\n\n        self.activated = activated\n\n        self.signature = signature = ensure_signature(signature)\n        *_keys, last_key = signature.output_fields.keys()\n\n        prefix = \"Reasoning: Let's think step by step in order to\"\n\n        if isinstance(dspy.settings.lm, dspy.LM):\n            desc = \"${reasoning}\"\n        elif dspy.settings.experimental:\n            desc = \"${produce the output fields}. We ...\"\n        else:\n            # For dspy <2.5\n            desc = f\"${{produce the {last_key}}}. We ...\"\n\n        rationale_type = rationale_type or dspy.OutputField(prefix=prefix, desc=desc)\n\n        # Add \"rationale\" field to the output signature.\n        if isinstance(dspy.settings.lm, dspy.LM) or dspy.settings.experimental:\n            extended_signature = signature.prepend(\"reasoning\", rationale_type, type_=str)\n        else:\n            # For dspy <2.5\n            extended_signature = signature.prepend(\"rationale\", rationale_type, type_=str)\n        \n        self._predict = dspy.Predict(extended_signature, **config)\n        self._predict.extended_signature = extended_signature\n```\n\n----------------------------------------\n\nTITLE: Evaluating SearchQA Performance with Avatar Agent\nDESCRIPTION: Runs the evaluation on the SearchQA test set using the custom multi-threaded executor and displays the average performance score.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/avatar_langchain_tools.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsqa_score = multi_thread_executor(sqa_test, SearchQASignature)\nprint(f\"Average Score on SearchQA: {sqa_score:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Compiling and Evaluating Chain of Thought Module\nDESCRIPTION: Compiles the CoT module using BootstrapFewShot and evaluates its performance on the development set.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/skycamp2023.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmetric_EM = dspy.evaluate.answer_exact_match\n\nteleprompter = BootstrapFewShot(metric=metric_EM, max_bootstrapped_demos=2)\ncot_compiled = teleprompter.compile(CoT(), trainset=train)\n\nNUM_THREADS = 32\nevaluate_hotpot = Evaluate(devset=dev, metric=metric_EM, num_threads=NUM_THREADS, display_progress=True, display_table=15)\n\nevaluate_hotpot(cot_compiled)\n```\n\n----------------------------------------\n\nTITLE: Neo4jRM Forward Method for Vector Search Retrieval\nDESCRIPTION: The forward method performs vector search in Neo4j using the provided query. It converts the query to embeddings and returns the top k matching passages from the database based on vector similarity.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/retrieval_models_clients/Neo4jRM.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nPrediction(\n    passages=['Passage 1 Lorem Ipsum awesome', 'Passage 2 Lorem Ipsum Youppidoo', 'Passage 3 Lorem Ipsum Yassssss']\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing MultiHop Class for Multi-Hop Retrieval and Reasoning in Python\nDESCRIPTION: This code defines a MultiHop class that inherits from dspy.Module. It implements a multi-hop retrieval and reasoning system with methods for generating queries, retrieving passages, and producing answers. The class is designed to work with the DSPy framework.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/skycamp2023.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom dsp.utils.utils import deduplicate\n\nclass MultiHop(dspy.Module):\n    def __init__(self, num_passages=3):\n        super().__init__()\n\n        self.retrieve = dspy.Retrieve(k=num_passages)\n        self.generate_query = dspy.ChainOfThought(\"question -> search_query\")\n\n        # TODO: Define a dspy.ChainOfThought module with the signature 'context, question -> search_query'.\n        self.generate_query_from_context = None\n\n        self.generate_answer = dspy.ChainOfThought(\"context, question -> answer\")\n    \n    def forward(self, question):\n        passages = []\n        \n        search_query = self.generate_query(question=question).search_query\n        passages += self.retrieve(search_query).passages\n\n        # TODO: Replace `None` with a call to self.generate_query_from_context to generate a search query.\n        # Note: In DSPy, always pass keyword arguments (e.g., context=..., question=...) to the modules to avoid ambiguity.\n        # Note 2: Don't forget to access the field .search_query to extract that from the output of the module.\n        # Note 3: Check the following notebook for a completed example: https://github.com/stanfordnlp/dspy/blob/main/skycamp2023_completed.ipynb.\n        search_query2 = None\n\n        # TODO: Replace `None` with a call to self.retrieve to retrieve passages. Append them to the list `passages`.\n        passages += None\n\n        return self.generate_answer(context=deduplicate(passages), question=question)\n```\n\n----------------------------------------\n\nTITLE: Performing Bootstrapped Fine-tuning in DSPy\nDESCRIPTION: Code to set up and execute bootstrapped fine-tuning using DSPy's BootstrapFinetune optimizer. This process uses the teacher model to create a training dataset for the student model.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndspy.settings.experimental = True  # fine-tuning is an experimental feature, so we set a flag to enable it\n\noptimizer = dspy.BootstrapFinetune(num_threads=16)  # if you *do* have labels, pass metric=your_metric here!\nclassify_ft = optimizer.compile(student_classify, teacher=teacher_classify, trainset=unlabeled_trainset)\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Optimized DSPy Programs in Python\nDESCRIPTION: This snippet demonstrates how to save an optimized DSPy program to a file and load it back for future use, enabling reuse of optimized systems without re-optimization.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/entity_extraction/index.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\noptimized_people_extractor.save(\"optimized_extractor.json\")\n\nloaded_people_extractor = dspy.ChainOfThought(PeopleExtraction)\nloaded_people_extractor.load(\"optimized_extractor.json\")\n\nloaded_people_extractor(tokens=[\"Italy\", \"recalled\", \"Marcello\", \"Cuttitta\"]).extracted_people\n```\n\n----------------------------------------\n\nTITLE: Displaying HoVer Dataset Example in Python\nDESCRIPTION: This code snippet prints an example from the HoVer dataset, showing the claim and the pages that must be retrieved for fact-checking.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nexample = trainset[0]\n\nprint(\"Claim:\", example.claim)\nprint(\"Pages that must be retrieved:\", example.titles)\n```\n\n----------------------------------------\n\nTITLE: Output Analysis Counter in Python\nDESCRIPTION: Script that counts and analyzes model outputs using Counter, checking for answer distributions and non-letter responses.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/vlm/mmmu.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import Counter\nc = Counter([outputs[i][1].get(\"answer\", \"nothing returned\") for i in range(len(outputs))])\nnon_letters = sum([1 for output in outputs if output[1].get(\"answer\", \"nothing returned\") not in [\"A\", \"B\", \"C\", \"D\"]])\nprint(c)\nprint(non_letters)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI and ColBERT Models\nDESCRIPTION: Configures OpenAI API key and sets up ColBERTv2 model for wiki abstracts retrieval along with GPT-4 mini model\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/tweets/tweets_assertions.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ncolbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\ndspy.settings.configure(rm=colbertv2_wiki17_abstracts)\nturbo = dspy.OpenAI(model='gpt-4o-mini', max_tokens=500)\ndspy.settings.configure(lm=turbo, trace=[], temperature=0.7)\n```\n\n----------------------------------------\n\nTITLE: Loading and Configuring HotPotQA Dataset in Python\nDESCRIPTION: This snippet loads the HotPotQA dataset, sets the train and evaluation sizes, and prepares the training and validation sets. It also configures evaluation metrics and parameters.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/qa/hotpot/hotpotqa_with_MIPRO.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Load and configure the datasets.\nTRAIN_SIZE = 500\nEVAL_SIZE = 500\n\nhotpot_dataset = HotPotQA(train_seed=1, eval_seed=2023, test_size=0)\ntrainset = [x.with_inputs('question') for x in hotpot_dataset.train][:TRAIN_SIZE]\nvalset = [x.with_inputs('question') for x in hotpot_dataset.dev][:EVAL_SIZE]\n\n# Set up metrics\nNUM_THREADS = 10\n\nmetric = dspy.evaluate.answer_exact_match\n\nkwargs = dict(num_threads=NUM_THREADS, display_progress=True)\nevaluate = Evaluate(devset=valset, metric=metric, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Compiling CoT Module with Teleprompter in Python\nDESCRIPTION: Shows how to compile a Chain of Thought module using the teleprompter with training and validation datasets.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/optimizers/bfrs.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncot_compiled = teleprompter.compile(CoT(), trainset=trainset, valset=devset)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for ColBERT Debugging in Python\nDESCRIPTION: Sets an environment variable to enable verbose logging when loading ColBERT's torch extension. This is useful for debugging purposes.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/integrations/colbert/colbert_local.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n# You can set this environment variable for debugging purposes\nos.environ['COLBERT_LOAD_TORCH_EXTENSION_VERBOSE'] = \"True\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation for dspy.KNNFewShot in Python\nDESCRIPTION: This code snippet configures the documentation generation for the dspy.KNNFewShot class. It specifies which members to include, how to display the source code, and various formatting options for the generated documentation.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/api/optimizers/KNNFewShot.md#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n::: dspy.KNNFewShot\n    handler: python\n    options:\n        members:\n            - compile\n            - get_params\n        show_source: true\n        show_root_heading: true\n        heading_level: 2\n        docstring_style: google\n        show_root_full_path: true\n        show_object_full_path: false\n        separate_signature: false\n        inherited_members: true\n```\n\n----------------------------------------\n\nTITLE: Inspecting Language Model History in Python using DSPy\nDESCRIPTION: This line inspects the history of the language model, showing the last interaction. It's useful for debugging and understanding the model's behavior.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/knn.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nlm.inspect_history(1)\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Summarization with ChainOfThought in DSPy\nDESCRIPTION: This example shows how to use DSPy's ChainOfThought module for text summarization. It defines a signature that takes a document as input and produces a summary, demonstrating the module's ability to provide reasoning alongside the output.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/programming/signatures.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Example from the XSum dataset.\ndocument = \"\"\"The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\"\"\"\n\nsummarize = dspy.ChainOfThought('document -> summary')\nresponse = summarize(document=document)\n\nprint(response.summary)\n\nprint(\"Reasoning:\", response.reasoning)\n```\n\n----------------------------------------\n\nTITLE: Context and Answer Validation Metric in Python\nDESCRIPTION: Complex metric that validates both answer accuracy and context presence. Returns float for evaluation/optimization and boolean for bootstrapping.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/evaluation/metrics.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef validate_context_and_answer(example, pred, trace=None):\n    # check the gold label and the predicted answer are the same\n    answer_match = example.answer.lower() == pred.answer.lower()\n\n    # check the predicted answer comes from one of the retrieved contexts\n    context_match = any((pred.answer.lower() in c) for c in pred.context)\n\n    if trace is None: # if we're doing evaluation or optimization\n        return (answer_match + context_match) / 2.0\n    else: # if we're doing bootstrapping, i.e. self-generating good demonstrations of each step\n        return answer_match and context_match\n```\n\n----------------------------------------\n\nTITLE: Implementing Citation Faithfulness Check in Python with DSPy\nDESCRIPTION: This snippet defines a DSPy signature for checking citation faithfulness and implements a function to verify the accuracy of citations in generated text against provided context. It uses ChainOfThought for each citation check.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/longformqa/longformqa_assertions.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass CheckCitationFaithfulness(dspy.Signature):\n    \"\"\"Verify that the text is based on the provided context.\"\"\"\n    context = dspy.InputField(desc=\"may contain relevant facts\")\n    text = dspy.InputField(desc=\"between 1 to 2 sentences\")\n    faithfulness = dspy.OutputField(desc=\"boolean indicating if text is faithful to context\")\n\ndef citation_faithfulness(example, pred, trace):\n    paragraph, context = pred.paragraph, pred.context\n    citation_dict = extract_text_by_citation(paragraph)\n    if not citation_dict:\n        return False, None\n    context_dict = {str(i): context[i].split(' | ')[1] for i in range(len(context))}\n    faithfulness_results = []\n    unfaithful_citations = []\n    check_citation_faithfulness = dspy.ChainOfThought(CheckCitationFaithfulness)\n    for citation_num, texts in citation_dict.items():\n        if citation_num not in context_dict:\n            continue\n        current_context = context_dict[citation_num]\n        for text in texts:\n            try:\n                result = check_citation_faithfulness(context=current_context, text=text)\n                is_faithful = result.faithfulness.lower() == 'true'\n                faithfulness_results.append(is_faithful)\n                if not is_faithful:\n                    unfaithful_citations.append({'paragraph': paragraph, 'text': text, 'context': current_context})\n            except ValueError as e:\n                faithfulness_results.append(False)\n                unfaithful_citations.append({'paragraph': paragraph, 'text': text, 'error': str(e)})\n    final_faithfulness = all(faithfulness_results)\n    if not faithfulness_results:\n        return False, None\n    return final_faithfulness, unfaithful_citations\n```\n\n----------------------------------------\n\nTITLE: Printing Optimized Program Predictor Models\nDESCRIPTION: Iterates through the predictors in the optimized program and prints the model used for each predictor.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/finetune/_unpolished_finetune_demo.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfor p in optimized_program.predictors():\n  print(p.lm.model)\n```\n\n----------------------------------------\n\nTITLE: Optimizing the aggregator with BootstrapFewShotWithRandomSearch\nDESCRIPTION: Applies the BootstrapFewShotWithRandomSearch optimizer to the aggregator, further improving its performance on the QA task.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/multi_agent.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nkwargs = dict(max_bootstrapped_demos=2, max_labeled_demos=6, num_candidate_programs=10, num_threads=8)\ntp = BootstrapFewShotWithRandomSearch(metric=dspy.evaluate.answer_exact_match, **kwargs)\noptimized_aggregator = tp.compile(aggregator, trainset=trainset, valset=valset)\n\noptimized_aggregator2 = optimized_aggregator.deepcopy()\noptimized_aggregator2.temperature = 0.7\n\nevaluate(optimized_aggregator2)\n```\n\n----------------------------------------\n\nTITLE: Importing ColBERT Configuration in Python\nDESCRIPTION: Imports the ColBERTConfig class from the colbert.infra.config module. This is essential for configuring ColBERT for retrieval and reranking tasks.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/integrations/colbert/colbert_local.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom colbert.infra.config import ColBERTConfig\n```\n\n----------------------------------------\n\nTITLE: Sample Output from inspect_history\nDESCRIPTION: Displays a truncated log of an LLM interaction, showing the agent's thinking process and final action, which reveals limitations in the information retrieved.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/observability/index.md#2025-04-07_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n[2024-12-01T10:23:29.144257]\n\nSystem message:\n\nYour input fields are:\n1. `question` (str)\n\n...\n\nResponse:\n\n[[ ## Thought_5 ## ]]\nThe search results continue to be unhelpful and do not provide the current team for Shohei Ohtani in Major League Baseball. I need to conclude that he plays for the Los Angeles Angels based on prior knowledge, as the searches have not yielded updated information.\n\n[[ ## Action_5 ## ]]\nFinish[Los Angeles Angels] \n\n[[ ## completed ## ]]\n```\n\n----------------------------------------\n\nTITLE: Running the TGI Server with Docker\nDESCRIPTION: Docker command to run the Text Generation Inference server. It requires specifying the model, number of shards, and a volume for persistent storage. This makes the server accessible at http://localhost:8080.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/language_model_clients/lm_local_models/HFClientTGI.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmodel=meta-llama/Llama-2-7b-hf # set to the specific Hugging Face model ID you wish to use.\nnum_shard=2 # set to the number of shards you wish to use.\nvolume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\n\ndocker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:0.9 --model-id $model --num-shard $num_shard\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for API Keys in DSPy\nDESCRIPTION: Sets up environment variables for the Serper API and OpenAI API using Jupyter's magic commands. These keys are required for the search functionality and language model access.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/avatar_langchain_tools.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%env SERPER_API_KEY=YOUR_SERPER_API_KEY\n%env OPENAI_API_KEY=YOUR_OPENAI_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow UI Server\nDESCRIPTION: Command to start the MLflow user interface on port 5000\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/math/index.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui --port 5000\n```\n\n----------------------------------------\n\nTITLE: Defining SummarizeSignature for Passage Summarization in DSPy\nDESCRIPTION: Creates a signature class for summarization that takes a passage as input and produces a concise summary as output. This forms the basic structure for the summarization program.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/summarization.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass SummarizeSignature(dspy.Signature):\n    \"\"\"\n    Given a passage, generate a summary.\n    \"\"\"\n\n    passage = dspy.InputField(desc=\"a passage to summarize\")\n    summary: str = dspy.OutputField(\n        desc=\"a concise summary of the passage\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Environment and Importing Dependencies for DSPy Project\nDESCRIPTION: This snippet sets up the environment by importing necessary libraries, loading environment variables, and defining constants for various API configurations including Azure OpenAI, OpenAI, and Ollama.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/multi-input-output/beginner-multi-input-output.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport re\n\nimport dspy\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom dspy.evaluate import Evaluate\nfrom dspy.teleprompt import (\n    BootstrapFewShot,\n    BootstrapFewShotWithRandomSearch,\n    KNNFewShot,\n    MIPROv2,\n)\n\n# load your environment variables from .env file\nload_dotenv()\n\n# azure-openai model deployment\nAZURE_OPENAI_KEY = os.getenv(\"AZURE_OPENAI_KEY\")\nAZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\nAZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\nAZURE_OPENAI_VERSION = os.getenv(\"AZURE_OPENAI_VERSION\")\n\n# openai model deployment\n\nOPENAI_MODEL = os.getenv(\"OPENAI_MODEL\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# ollama deployment\nOLLAMA_URL = os.getenv(\"OLLAMA_URL\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing HotPotQA Dataset\nDESCRIPTION: Loads the HotPotQA dataset with specified train and dev set sizes, and configures the input field for DSPy processing.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/old/simplified-baleen.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.datasets import HotPotQA\n\n# Load the dataset.\ndataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)\n\n# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\ntrainset = [x.with_inputs('question') for x in dataset.train]\ndevset = [x.with_inputs('question') for x in dataset.dev]\n\nlen(trainset), len(devset)\n```\n\n----------------------------------------\n\nTITLE: Launching Fine-tuned Model in DSPy\nDESCRIPTION: Code to explicitly launch the fine-tuned local model for use.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/classification_finetuning/index.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclassify_ft.get_lm().launch()\n```\n\n----------------------------------------\n\nTITLE: BootstrapFinetune Configuration\nDESCRIPTION: Shows different ways to configure and initialize BootstrapFinetune optimizer with various parameters.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/finetune/_unpolished_finetune_demo.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_kwargs = {\n  \"n_epochs\": 1,\n}\nadapter = dspy.ChatAdapter()\n\nweight_optimizer = dspy.BootstrapFinetune(\n  metric=metric,               # Can be left empty, leads to no filtering\n  multitask=True,              # We can also handle False!\n  train_kwargs=train_kwargs,   # Can be left empty\n  adapter=adapter,             # Can be left empty, leads to adapters inferred from the LM\n  exclude_demos=False,         # Can be left empty\n  num_threads = 1,             # Can be left empty\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Wikipedia Corpus from JSONL\nDESCRIPTION: Loads the Wikipedia abstracts from the JSONL file into a list, combining each article's title and text content. The code formats each entry as \"title | text\".\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ujson\ncorpus = []\n\nwith open(\"wiki.abstracts.2017.jsonl\") as f:\n    for line in f:\n        line = ujson.loads(line)\n        corpus.append(f\"{line['title']} | {' '.join(line['text'])}\")\n\nlen(corpus)\n```\n\n----------------------------------------\n\nTITLE: Creating Training and Testing Datasets with Random Sampling\nDESCRIPTION: Prepares training (200 examples) and testing (100 examples) subsets from the loaded datasets. It formats the data as DSPy Examples with appropriate input and output fields for each dataset type.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/agents/avatar_langchain_tools.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\n# Set a random seed for reproducibility\nrandom.seed(42)\n\n\nsqa_train = [\n    dspy.Example(question=example.question, answer=\",\".join(example.answers)).with_inputs(\"question\")\n    for example in sample(searchqa, 200)\n]\nsqa_test = [\n    dspy.Example(question=example.question, answer=\",\".join(example.answers)).with_inputs(\"question\")\n    for example in sample(searchqa, 100)\n]\n\naqa_train = [\n    dspy.Example(question=example.question, paper_id=example.paper_id, answer=example.answer).with_inputs(\"question\", \"paper_id\")\n    for example in sample(arxiv_qa, 200)\n]\naqa_test = [\n    dspy.Example(question=example.question, paper_id=example.paper_id, answer=example.answer).with_inputs(\"question\", \"paper_id\")\n    for example in sample(arxiv_qa, 100)\n]\n```\n\n----------------------------------------\n\nTITLE: Inspecting Optimization History\nDESCRIPTION: Displaying the prompt changes after optimization\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/math/index.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndspy.inspect_history()\n```\n\n----------------------------------------\n\nTITLE: MLflow Evaluation Tracking\nDESCRIPTION: Recording and tracking evaluation results using MLflow\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/math/index.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nwith mlflow.start_run(run_name=\"math_evaluation\"):\n    kwargs = dict(num_threads=THREADS, display_progress=True, return_all_scores=True, return_outputs=True)\n    evaluate = dspy.Evaluate(devset=dataset.dev, metric=dataset.metric, **kwargs)\n\n    aggregated_score, outputs, all_scores = evaluate(module)\n\n    mlflow.log_metric(\"correctness\", aggregated_score)\n    mlflow.log_table(\n        {\n            \"Question\": [example.question for example in dataset.dev],\n            \"Gold Answer\": [example.answer for example in dataset.dev],\n            \"Predicted Answer\": outputs,\n            \"Correctness\": all_scores,\n        },\n        artifact_file=\"eval_results.json\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Chain of Thought Module\nDESCRIPTION: Defines a Chain of Thought module that takes a question as input and produces an answer. The module leverages DSPy's ChainOfThought class to apply the reasoning pattern to the basic question-answer signature.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/optimizers/bootstrap-fewshot.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass CoT(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.prog = dspy.ChainOfThought(\"question -> answer\")\n\n    def forward(self, question):\n        return self.prog(question=question)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-modal Image Classification with DSPy Signature\nDESCRIPTION: This snippet shows how to create a DSPy Signature for multi-modal tasks, specifically for classifying dog breeds from images. It uses dspy.Image as an input type and demonstrates how to handle image URLs.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/learn/programming/signatures.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass DogPictureSignature(dspy.Signature):\n    \"\"\"Output the dog breed of the dog in the image.\"\"\"\n    image_1: dspy.Image = dspy.InputField(desc=\"An image of a dog\")\n    answer: str = dspy.OutputField(desc=\"The dog breed of the dog in the image\")\n\nimage_url = \"https://picsum.photos/id/237/200/300\"\nclassify = dspy.Predict(DogPictureSignature)\nclassify(image_1=dspy.Image.from_url(image_url))\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Logging Callback in DSPy\nDESCRIPTION: Creates a custom callback class that extends BaseCallback to log intermediate steps of a ReAct agent, demonstrating how to implement specialized observability solutions.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/observability/index.md#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\nfrom dspy.utils.callback import BaseCallback\n\n# 1. Define a custom callback class that extends BaseCallback class\nclass AgentLoggingCallback(BaseCallback):\n\n    # 2. Implement on_module_end handler to run a custom logging code.\n    def on_module_end(self, call_id, outputs, exception):\n        step = \"Reasoning\" if self._is_reasoning_output(outputs) else \"Acting\"\n        print(f\"== {step} Step ===\")\n        for k, v in outputs.items():\n            print(f\"  {k}: {v}\")\n        print(\"\\n\")\n\n    def _is_reasoning_output(self, outputs):\n        return any(k.startswith(\"Thought\") for k in outputs.keys())\n\n# 3. Set the callback to DSPy setting so it will be applied to program execution\ndspy.configure(callbacks=[AgentLoggingCallback()])\n```\n\n----------------------------------------\n\nTITLE: Output from Custom Logging Callback\nDESCRIPTION: Displays sample output from the custom callback, showing the agent's reasoning and acting steps in a structured format tailored to debugging specific aspects of the execution.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/observability/index.md#2025-04-07_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n== Reasoning Step ===\n  Thought_1: I need to find the current team that Shohei Ohtani plays for in Major League Baseball.\n  Action_1: Search[Shohei Ohtani current team 2023]\n\n== Acting Step ===\n  passages: [\"Shohei Ohtani ...\"]\n\n...\n```\n\n----------------------------------------\n\nTITLE: Markdown Configuration for PythonInterpreter Documentation\nDESCRIPTION: Configuration directives for the documentation generator that specify which PythonInterpreter members to display and how to format the documentation. The configuration includes options for showing source code, heading structure, and method selection.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/api/tools/PythonInterpreter.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n::: dspy.PythonInterpreter\n    handler: python\n    options:\n        members:\n            - __call__\n            - execute\n            - shutdown\n        show_source: true\n        show_root_heading: true\n        heading_level: 2\n        docstring_style: google\n        show_root_full_path: true\n        show_object_full_path: false\n        separate_signature: false\n        inherited_members: true\n```\n\n----------------------------------------\n\nTITLE: Compiling LongFormQA with Assertions in Both Student and Teacher\nDESCRIPTION: This advanced pipeline uses assertions in both student and teacher models during compilation. This ensures both models can self-correct, with the teacher providing good examples and the student able to correct itself when it still makes mistakes during inference.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/longformqa/longformqa_assertions.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nlongformqa = LongFormQA()\nteleprompter = BootstrapFewShotWithRandomSearch(metric = answer_correctness, max_bootstrapped_demos=2, num_candidate_programs=6)\ncited_longformqa_student_teacher = teleprompter.compile(student=assert_transform_module(LongFormQAWithAssertions().map_named_predictors(Retry), backtrack_handler), teacher = assert_transform_module(LongFormQAWithAssertions().map_named_predictors(Retry), backtrack_handler), trainset=trainset, valset=devset[:25])\nevaluate(cited_longformqa_student_teacher)\n```\n\n----------------------------------------\n\nTITLE: Implementing BM25 Search Function in Python\nDESCRIPTION: This function performs a search using BM25 indexing. It tokenizes the query, retrieves results, and returns a dictionary of document titles and their scores.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/multihop_search/index.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef search(query: str, k: int) -> list[str]:\n    tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n    results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n    run = {corpus[doc]: float(score) for doc, score in zip(results[0], scores[0])}\n    return run\n```\n\n----------------------------------------\n\nTITLE: Configuring DSPy with OpenAI GPT-4 Model in Python\nDESCRIPTION: This snippet sets up the DSPy environment to use OpenAI's GPT-4 model. It configures the model type, maximum tokens, and applies the API key for authentication.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/knn.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlm = dspy.OpenAI(model='gpt-4', api_key=api_key, model_type='chat', max_tokens = 500)\ndspy.settings.configure(lm=lm)\n```\n\n----------------------------------------\n\nTITLE: Performing Multiple Query Retrieval with ColBERT in Python\nDESCRIPTION: Shows how to perform retrieval for multiple queries simultaneously using the ColBERT retriever in DSPy.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/integrations/colbert/colbert_local.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmultiple_pred = retrieved_docs(\n    [\"What is the meaning of life?\",\"Meaning of pain?\"],by_prob=False\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Optimization Trial Scores\nDESCRIPTION: Creates a visualization of optimization trial scores using matplotlib, showing both successful and pruned batches over time.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/nli/scone/scone_with_MIPRO.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\ntrial_logs = compiled_program.trial_logs\n\ntrial_numbers = list(trial_logs.keys())\nscores = [trial_logs[trial]['score'] for trial in trial_numbers]\npruning_status = [trial_logs[trial]['pruned'] for trial in trial_numbers]\n\nplt.figure(figsize=(5, 3))\n\nfor trial_number, score, pruned in zip(trial_numbers, scores, pruning_status):\n    if pruned:\n        plt.scatter(trial_number, score, color='grey', label='Pruned Batch' if 'Pruned Batch' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n    else:\n        plt.scatter(trial_number, score, color='green', label='Successful Batch' if 'Successful Batch' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n\nplt.xlabel('Batch Number')\nplt.ylabel('Score')\nplt.title('Batch Scores')\nplt.grid(True)\nplt.legend()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Customizing ChainOfThought with Custom Rationale\nDESCRIPTION: Example showing how to customize the ChainOfThought module by providing a custom rationale type. This allows for more control over the format and prompt for the reasoning steps.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/modules/chain-of-thought.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#define a custom rationale\nrationale_type = dspy.OutputField(\n            prefix=\"Reasoning: Let's think step by step in order to\",\n            desc=\"${produce the answer}. We ...\",\n        )\n#Pass signature to ChainOfThought module\ngenerate_answer = dspy.ChainOfThought(BasicQA, rationale_type=rationale_type)\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow Tracing\nDESCRIPTION: Enables automatic logging of DSPy operations in MLflow\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/tutorials/math/index.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmlflow.dspy.autolog()\n```\n\n----------------------------------------\n\nTITLE: Displaying Reranking Results in HTML Table Format in Python\nDESCRIPTION: Renders the reranking results as an HTML table in a Jupyter notebook environment using IPython's display functionality.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/integrations/colbert/colbert_local.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import HTML, display\ndisplay(HTML(table))\n```\n\n----------------------------------------\n\nTITLE: Using DSPy Predict for Question Answering in Python\nDESCRIPTION: An example demonstrating how to use the Predict class with a custom signature for basic question answering. The example defines a BasicQA signature, creates a predictor, and uses it to generate an answer to a specific question.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/deep-dive/modules/predict.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#Define a simple signature for basic question answering\nclass BasicQA(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n\n#Pass signature to Predict module\ngenerate_answer = dspy.Predict(BasicQA)\n\n# Call the predictor on a particular input.\nquestion='What is the color of the sky?'\npred = generate_answer(question=question)\n\nprint(f\"Question: {question}\")\nprint(f\"Predicted Answer: {pred.answer}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring DSPy with Azure OpenAI Client\nDESCRIPTION: This snippet demonstrates how to set up DSPy with an Azure OpenAI client. It also includes commented-out code for alternative configurations using OpenAI and Ollama clients, showcasing DSPy's flexibility with different language model providers.\nSOURCE: https://github.com/stanfordnlp/dspy/blob/main/examples/outdated_v2.4_examples/multi-input-output/beginner-multi-input-output.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# client for AzureOpenAI\nlm = dspy.AzureOpenAI(\n    api_base=AZURE_OPENAI_ENDPOINT,\n    api_version=AZURE_OPENAI_VERSION,\n    deployment_id=AZURE_OPENAI_DEPLOYMENT,\n    api_key=AZURE_OPENAI_KEY,\n)\n\n# client for OpenAI\n# lm = dspy.OpenAI(\n#     api_key=OPENAI_API_KEY,\n#     model=OPENAI_MODEL,\n# )\n\n# client for Ollama\n# model_name = \"llama3.1\"\n# lm = dspy.OllamaLocal(base_url=OLLAMA_URL, model=model_name)\n\ndspy.settings.configure(lm=lm)\n```"
  }
]
[
  {
    "owner": "dlt-hub",
    "repo": "dlt",
    "content": "TITLE: Loading Chess Player Data with dlt Pipeline\nDESCRIPTION: A Python example that demonstrates creating a dlt pipeline to fetch chess player data from Chess.com API and load it into a DuckDB destination. The code creates a pipeline, fetches data for two chess players, and then runs the pipeline to process and load the data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/README.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.helpers import requests\n\n# Create a dlt pipeline that will load\n# chess player data to the DuckDB destination\npipeline = dlt.pipeline(\n    pipeline_name='chess_pipeline',\n    destination='duckdb',\n    dataset_name='player_data'\n)\n\n# Grab some player data from Chess.com API\ndata = []\nfor player in ['magnuscarlsen', 'rpragchess']:\n    response = requests.get(f'https://api.chess.com/pub/player/{player}')\n    response.raise_for_status()\n    data.append(response.json())\n\n# Extract, normalize, and load the data\npipeline.run(data, table_name='player')\n```\n\n----------------------------------------\n\nTITLE: GitHub API Credentials Configuration (TOML)\nDESCRIPTION: Configures the GitHub API access token in the secrets file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/create-a-pipeline.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[sources]\napi_secret_key = '<api key value>'\n```\n\n----------------------------------------\n\nTITLE: Interactive Data Transformation with GitHub Reactions\nDESCRIPTION: Example showing how to fetch GitHub reactions data from an issues table and count reaction types using both DataFrame and Arrow table approaches.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/transformations/python.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"github_pipeline\",\n    destination=\"duckdb\",\n    dataset_name=\"github_reactions\",\n    dev_mode=True\n)\n\n# get a dataframe of all reactions from the dataset\nreactions = pipeline.dataset().issues.select(\"reactions__+1\", \"reactions__-1\", \"reactions__laugh\", \"reactions__hooray\", \"reactions__rocket\").df()\n\n# calculate and print out the sum of all reactions\ncounts = reactions.sum(0).sort_values(0, ascending=False)\nprint(counts)\n\n# alternatively, you can fetch the data as an arrow table\nreactions = pipeline.dataset().issues.select(\"reactions__+1\", \"reactions__-1\", \"reactions__laugh\", \"reactions__hooray\", \"reactions__rocket\").arrow()\n# ... do transformations on the arrow table\n```\n\n----------------------------------------\n\nTITLE: Initializing Google Analytics Source Function in Python\nDESCRIPTION: Main source function that configures and returns Google Analytics data resources. Accepts credentials, property ID, queries configuration, start date and pagination parameters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_analytics.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source(max_table_nesting=2)\ndef google_analytics(\n    credentials: Union[ GcpOAuthCredentials, GcpServiceAccountCredentials ] = dlt.secrets.value,\n    property_id: int = dlt.config.value,\n    queries: List[DictStrAny] = dlt.config.value,\n    start_date: Optional[str] = START_DATE_STRING,\n    rows_per_page: int = 1000,\n) -> List[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Stripe Credentials\nDESCRIPTION: TOML configuration for storing Stripe API credentials securely in secrets.toml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/stripe.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# put your secret values and credentials here. do not share this file and do not push it to github\n[sources.stripe_analytics]\nstripe_secret_key = \"stripe_secret_key\"# please set me up!\n```\n\n----------------------------------------\n\nTITLE: Complex Nested Schema Evolution in DLT\nDESCRIPTION: Demonstrates handling of deeply nested data structures with multiple levels of nesting and array relationships.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema-evolution.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndata = [{\n    \"organization\": \"Tech Innovations Inc.\",\n    \"CEO\": \"Alice Smith\",\n    \"address\": {'main_block': 'r&d'},\n    \"Inventory\": [\n        {\"name\": \"Plasma ray\", \"inventory nr\": \"AR2411\"},\n        {\"name\": \"Self-aware Roomba\", \"inventory nr\": \"AR268\"},\n        {\n            \"name\": \"Type-inferrer\", \"inventory nr\": \"AR3621\",\n            \"details\": {\n                \"category\": \"Computing Devices\",\n                \"id\": 369,\n                \"specifications\": [{\n                    \"processor\": \"Quantum Core\",\n                    \"memory\": \"512PB\"\n                }]\n            }\n        }\n    ]\n}]\n\n# Run `dlt` pipeline\ndlt.pipeline(\"organizations_pipeline\", destination=\"duckdb\").run(data, table_name=\"org\")\n```\n\n----------------------------------------\n\nTITLE: Running a DLT Pipeline with Transformer Function in Python\nDESCRIPTION: This snippet demonstrates an alternative way to run the DLT pipeline using a transformer function. It applies the fetch_average_price function as a transformer to the tracked data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/user_agent_device_data_enrichment.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# using fetch_average_price as a transformer function\nload_info = pipeline.run(\n    tracked_data | fetch_average_price,\n    table_name=\"tracked_data\"\n)\n```\n\n----------------------------------------\n\nTITLE: Running the Data Enrichment Pipeline Script in Shell\nDESCRIPTION: This command executes the Python script containing the data enrichment pipeline. It assumes the script is named device_enrichment_pipeline.py.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/user_agent_device_data_enrichment.md#2025-04-14_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npython device_enrichment_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Setting Schema Contract for a DLT Resource in Python\nDESCRIPTION: This snippet demonstrates how to set a schema contract for a dlt resource. It allows new tables to be created but freezes the columns of existing tables.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema-contracts.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(schema_contract={\"tables\": \"evolve\", \"columns\": \"freeze\"})\ndef items():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Fetching Paginated GitHub Issues with dlt RESTClient in Python\nDESCRIPTION: This snippet demonstrates how to create a simple pipeline that reads paginated issues from the dlt GitHub repository using the RESTClient. It sets up the client, defines a resource function to fetch and yield pages of issues, and runs the pipeline to load the data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/http/overview.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.helpers.rest_client import RESTClient\n\ngithub_client = RESTClient(base_url=\"https://api.github.com\")  # (1)\n\n@dlt.resource\ndef get_issues():\n    for page in github_client.paginate(                        # (2)\n        \"/repos/dlt-hub/dlt/issues\",                           # (3)\n        params={                                               # (4)\n            \"per_page\": 100,\n            \"sort\": \"updated\",\n            \"direction\": \"desc\",\n        },\n    ):\n        yield page                                             # (5)\n\n\npipeline = dlt.pipeline(\n    pipeline_name=\"github_issues\",\n    destination=\"duckdb\",\n    dataset_name=\"github_data\",\n)\nload_info = pipeline.run(get_issues)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Working with Multiple Schemas in dlt Pipeline\nDESCRIPTION: Demonstrates extracting data from multiple sources with different schemas in a dlt pipeline. Shows how schemas are assigned and managed automatically when extracting data from different sources.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/technical/general_usage.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\np = dlt.pipeline(dataset_name=\"spotify_data_1\")\np.extract(spotify(\"me\"))  # gets schema \"spotify\" from spotify source, that schema becomes default schema\np.extract(echonest(\"me\").with_resources(\"mel\"))  # get schema \"echonest\", all tables belonging to resource \"mel\" will be placed in that schema\np.extract([label1, label2, label3], name=\"labels\")  # will use default schema \"spotitfy\" for table \"labels\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Loading with dlt in Python\nDESCRIPTION: This snippet shows how to configure a table for incremental loading using dlt. It sets up the 'family' table to load incrementally based on the 'updated' column, only loading new or modified data in subsequent runs.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/sql-database.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.sql_database import sql_database\n\ndef load_tables_family_and_genome():\n\n    source = sql_database().with_resources(\"family\", \"genome\")\n\n    # only load rows whose \"updated\" value is greater than the last pipeline run\n    source.family.apply_hints(incremental=dlt.sources.incremental(\"updated\"))\n\n    pipeline = dlt.pipeline(\n        pipeline_name=\"sql_to_duckdb_pipeline\",\n        destination=\"duckdb\",\n        dataset_name=\"sql_to_duckdb_pipeline_data\"\n    )\n\n    load_info = pipeline.run(source)\n\n    print(load_info)\n\n\n\nif __name__ == '__main__':\n    load_tables_family_and_genome()\n```\n\n----------------------------------------\n\nTITLE: Modifying Queries with ReadableRelation in Python\nDESCRIPTION: Shows how to refine data retrieval by limiting the number of records, selecting specific columns, or chaining these operations. It includes examples of using limit(), head(), and select() methods.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/dataset.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Get the first 50 items as a PyArrow table\narrow_table = items_relation.limit(50).arrow()\n\n# Using head() to get the first 5 records\ndf = items_relation.head().df()\n\n# Select only 'col1' and 'col2' columns\nitems_list = items_relation.select(\"col1\", \"col2\").fetchall()\n\n# Alternate notation with brackets\nitems_list = items_relation[[\"col1\", \"col2\"]].fetchall()\n\n# Only get one column\nitems_list = items_relation[\"col1\"].fetchall()\n\n# Select columns and limit the number of records\narrow_table = items_relation.select(\"col1\", \"col2\").limit(50).arrow()\n```\n\n----------------------------------------\n\nTITLE: Loading All HubSpot Data\nDESCRIPTION: Loads all data from contacts, companies, deals, products, tickets, and quotes into the specified destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/hubspot.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nload_data = hubspot()\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Loading Data from REST API with dlt\nDESCRIPTION: Example of using dlt to extract data from a REST API with pagination and authentication support. The code demonstrates setting up a pipeline to load data into DuckDB.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/intro.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.rest_api import rest_api_source\n\nsource = rest_api_source({\n    \"client\": {\n        \"base_url\": \"https://api.example.com/\",\n        \"auth\": {\n            \"token\": dlt.secrets[\"your_api_token\"],\n        },\n        \"paginator\": {\n            \"type\": \"json_link\",\n            \"next_url_path\": \"paging.next\",\n        },\n    },\n    \"resources\": [\"posts\", \"comments\"],\n})\n\npipeline = dlt.pipeline(\n    pipeline_name=\"rest_api_example\",\n    destination=\"duckdb\",\n    dataset_name=\"rest_api_data\",\n)\n\nload_info = pipeline.run(source)\n\n# print load info and posts table as dataframe\nprint(load_info)\nprint(pipeline.dataset().posts.df())\n```\n\n----------------------------------------\n\nTITLE: Loading Data from a Generator Function in DLT Pipeline\nDESCRIPTION: This example shows how to create a dlt pipeline that loads data produced by a generator function. It demonstrates pipeline creation with a BigQuery destination and how to run the pipeline with a generator.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/pipeline.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\ndef generate_rows(nr):\n    for i in range(nr):\n        yield {'id':1}\n\npipeline = dlt.pipeline(destination='bigquery', dataset_name='sql_database_data')\n\ninfo = pipeline.run(generate_rows(10))\n\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Setting Config and Secrets in Code\nDESCRIPTION: Example showing how to programmatically set configuration values and secrets.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/advanced.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndlt.config[\"sheet_id\"] = \"23029402349032049\"\ndlt.secrets[\"destination.postgres.credentials\"] = BaseHook.get_connection('postgres_dsn').extra\n```\n\n----------------------------------------\n\nTITLE: Loading Pandas DataFrame to Snowflake\nDESCRIPTION: Example showing how to load a Pandas DataFrame containing order data into a Snowflake table using DLT pipeline. Creates a sample dataframe with order information including timestamps and numerical data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/arrow-pandas.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.common import pendulum\nimport pandas as pd\n\n\ndf = pd.DataFrame({\n    \"order_id\": [1, 2, 3],\n    \"customer_id\": [1, 2, 3],\n    \"ordered_at\": [pendulum.DateTime(2021, 1, 1, 4, 5, 6), pendulum.DateTime(2021, 1, 3, 4, 5, 6), pendulum.DateTime(2021, 1, 6, 4, 5, 6)],\n    \"order_amount\": [100.0, 200.0, 300.0],\n})\n\npipeline = dlt.pipeline(\"orders_pipeline\", destination=\"snowflake\")\n\npipeline.run(df, table_name=\"orders\")\n```\n\n----------------------------------------\n\nTITLE: Resource Selection and Data Access\nDESCRIPTION: Demonstrates how to access individual resources and control their selection status for loading.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/source.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# resources are accessible as attributes of a source\nfor c in source.companies:  # enumerate all data in the companies resource\n    print(c)\n\n# check if deals are selected to load\nprint(source.deals.selected)\n# deselect the deals\nsource.deals.selected = False\n```\n\n----------------------------------------\n\nTITLE: Truncating DLT Pipeline Tables and Resetting Resource State\nDESCRIPTION: This example shows how to use the 'drop_data' refresh mode in a dlt pipeline. It truncates tables and resets resource state for selected resources without modifying the schema.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/pipeline.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\npipeline = dlt.pipeline(\"airtable_demo\", destination=\"duckdb\")\npipeline.run(airtable_emojis().with_resources(\"ðŸ“† Schedule\"), refresh=\"drop_data\")\n```\n\n----------------------------------------\n\nTITLE: Loading JSON with DuckDB to Arrow\nDESCRIPTION: Shows how to load JSON data into an Arrow table using DuckDB, bypassing the default DLT JSON normalizer.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/arrow-pandas.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\n\nconn = duckdb.connect()\ntable = conn.execute(\"SELECT * FROM read_json_auto('./json_file_path')\").fetch_arrow_table()\n```\n\n----------------------------------------\n\nTITLE: Executing DLT Pipeline for All Asana Data\nDESCRIPTION: Runs the pipeline to load all available Asana data and prints loading information.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/asana.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nload_info = pipeline.run(load_data)\n# print the information on data that was loaded\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Resource Decomposition for Parallel Loading\nDESCRIPTION: Demonstrates how to break down a large source into individual resources for parallel loading.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/source.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# get a list of resources' names\nresource_list = sql_source().resources.keys()\n\n# now we are able to make a pipeline for each resource\nfor res in resource_list:\n    pipeline.run(sql_source().with_resources(res))\n```\n\n----------------------------------------\n\nTITLE: Executing Currency Enrichment Pipeline Script\nDESCRIPTION: This command runs the Python script that contains the currency enrichment pipeline. It executes the pipeline defined in the 'currency_enrichment_pipeline.py' file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/currency_conversion_data_enrichment.md#2025-04-14_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\npython currency_enrichment_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Modifying Schema in Source Function Body\nDESCRIPTION: Python code that demonstrates how to modify a schema within a source function using dlt.current.source_schema(). This example removes the ISO timestamp detector and adds a custom timestamp detector.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema.md#2025-04-14_snippet_15\n\nLANGUAGE: py\nCODE:\n```\n@dlt.source\ndef textual(nesting_level: int):\n    # get the source schema from the `current` context\n    schema = dlt.current.source_schema()\n    # remove date detector\n    schema.remove_type_detection(\"iso_timestamp\")\n    # convert UNIX timestamp (float, within a year from NOW) into timestamp\n    schema.add_type_detection(\"timestamp\")\n\n    return dlt.resource([])\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Database Source Arguments in Python\nDESCRIPTION: Detailed parameter list for initializing SQL database source in DLT. Includes credential handling, schema configuration, backend selection, and various customization options through callbacks. Supports multiple backend types and reflection levels for schema handling.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/configuration.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncredentials = Union[ConnectionStringCredentials, Engine, str]\nschema = Optional[str]\nmetadata = Optional[MetaData]\ntable_names = Optional[List[str]]\nchunk_size = int\nbackend = TableBackend\ndetect_precision_hints = bool\nreflection_level = ReflectionLevel\ndefer_table_reflect = bool\ntable_adapter_callback = Callable\nbackend_kwargs = **kwargs\ninclude_views = bool\ntype_adapter_callback = Optional[Callable]\nquery_adapter_callback = Optional[Callable[Select, Table], Select]\nresolve_foreign_keys = bool\nengine_adapter_callback = Callable[[Engine], Engine]\n```\n\n----------------------------------------\n\nTITLE: Initial Schema Inference with Nested Data in DLT Pipeline\nDESCRIPTION: Demonstrates how DLT infers schema from nested data structures including dictionaries and arrays, showing the base schema creation process.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema-evolution.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndata = [{\n    \"organization\": \"Tech Innovations Inc.\",\n    \"address\": {\n        'building': 'r&d',\n        \"room\": 7890,\n    },\n    \"Inventory\": [\n        {\"name\": \"Plasma ray\", \"inventory nr\": 2411},\n        {\"name\": \"Self-aware Roomba\", \"inventory nr\": 268},\n        {\"name\": \"Type-inferrer\", \"inventory nr\": 3621}\n    ]\n}]\n\n# Run `dlt` pipeline\ndlt.pipeline(\"organizations_pipeline\", destination=\"duckdb\").run(data, table_name=\"org\")\n```\n\n----------------------------------------\n\nTITLE: Implementing PII Pseudonymization with Python and dlt\nDESCRIPTION: Complete example showing how to create a data source with PII data and implement deterministic pseudonymization using SHA-256 hashing. The code demonstrates both direct execution and pipeline integration approaches, with a custom mapping function to handle the pseudonymization process.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/customising-pipelines/pseudonymizing_columns.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nimport hashlib\n\n@dlt.source\ndef dummy_source(prefix: str = None):\n    @dlt.resource\n    def dummy_data():\n        for _ in range(3):\n            yield {'id': _, 'name': f'Jane Washington {_}'}\n    return dummy_data(),\n\ndef pseudonymize_name(doc):\n    '''\n    Pseudonymization is a deterministic type of PII-obscuring.\n    Its role is to allow identifying users by their hash,\n    without revealing the underlying info.\n    '''\n    # add a constant salt to generate\n    salt = 'WI@N57%zZrmk#88c'\n    salted_string = doc['name'] + salt\n    sh = hashlib.sha256()\n    sh.update(salted_string.encode())\n    hashed_string = sh.digest().hex()\n    doc['name'] = hashed_string\n    return doc\n\n# run it as is\nfor row in dummy_source().dummy_data.add_map(pseudonymize_name):\n    print(row)\n\n#{'id': 0, 'name': '96259edb2b28b48bebce8278c550e99fbdc4a3fac8189e6b90f183ecff01c442'}\n#{'id': 1, 'name': '92d3972b625cbd21f28782fb5c89552ce1aa09281892a2ab32aee8feeb3544a1'}\n#{'id': 2, 'name': '443679926a7cff506a3b5d5d094dc7734861352b9e0791af5d39db5a7356d11a'}\n\n# Or create an instance of the data source, modify the resource and run the source.\n\n# 1. Create an instance of the source so you can edit it.\nsource_instance = dummy_source()\n# 2. Modify this source instance's resource\ndata_resource = source_instance.dummy_data.add_map(pseudonymize_name)\n# 3. Inspect your result\nfor row in source_instance:\n    print(row)\n\npipeline = dlt.pipeline(pipeline_name='example', destination='bigquery', dataset_name='normalized_data')\nload_info = pipeline.run(source_instance)\n```\n\n----------------------------------------\n\nTITLE: Creating Aggregated Sales Table with DLT SQL Client\nDESCRIPTION: Demonstrates how to create a new aggregated_sales table using DLT's SQL client with DuckDB dialect. The transformation aggregates sales data by category and region, calculating total and average sales.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/transformations/sql.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(destination=\"duckdb\", dataset_name=\"crm\")\n\n# NOTE: this is the duckdb sql dialect, other destinations may use different expressions\nwith pipeline.sql_client() as client:\n    client.execute_sql(\n        \"\"\" CREATE OR REPLACE TABLE aggregated_sales AS\n            SELECT \n                category,\n                region,\n                SUM(amount) AS total_sales,\n                AVG(amount) AS average_sales\n            FROM \n                sales\n            GROUP BY \n                category, \n                region;\n    \"\"\")\n```\n\n----------------------------------------\n\nTITLE: Loading Python Data Structures with dlt\nDESCRIPTION: Example of using dlt to load data directly from Python generators and data structures into DuckDB.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/intro.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\n@dlt.resource(table_name=\"foo_data\")\ndef foo():\n    for i in range(10):\n        yield {\"id\": i, \"name\": f\"This is item {i}\"}\n\npipeline = dlt.pipeline(\n    pipeline_name=\"python_data_example\",\n    destination=\"duckdb\",\n)\n\nload_info = pipeline.run(foo)\n\n# print load info and the \"foo_data\" table as dataframe\nprint(load_info)\nprint(pipeline.dataset().foo_data.df())\n```\n\n----------------------------------------\n\nTITLE: Executing DLT Pipeline with dlt.run Function in Python\nDESCRIPTION: The dlt.run function is used to execute a data pipeline. It handles data extraction, normalization, and loading. The function takes parameters such as data sources, destination, dataset name, and optional configurations like schema and credentials.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/technical/general_usage.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndlt.run(data, destination, dataset_name, table_name, write_disposition, schema, credentials)\n```\n\n----------------------------------------\n\nTITLE: Implementing dlt Pipeline Functions for HubSpot Data Integration\nDESCRIPTION: Python implementation file (pipeline.py) containing multiple functions for loading different types of HubSpot data to BigQuery using dlt. This includes loading CRM data, property history, custom properties, and web analytics events.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-orchestra.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport dlt\n\nfrom hubspot import hubspot, hubspot_events_for_objects, THubspotObjectType\nfrom setup_logger import build_logger\n\nlogger = build_logger(\"debug.log\")\n\ndef run_pipeline() -> None:\n    \"\"\"\n    This function loads all resources from HubSpot CRM\n\n    Returns:\n        None\n    \"\"\"\n    logger.info(\"Starting to run dlt Pipeline\")\n    # Create a DLT pipeline object with the pipeline name, dataset name, and destination database type\n    # Add dev_mode=(True or False) if you need your pipeline to create the dataset in your destination\n    p = dlt.pipeline(\n        pipeline_name=\"hubspot\",\n        dataset_name=\"dlt_hubspot\",\n        destination='bigquery',\n    )\n\n    data = hubspot()\n    data.companies.bind(props=['hs_time_in_opportunity', 'hs_analytics_first_visit_timestamp'])\n\n    # Run the pipeline with the HubSpot source connector\n    info = p.run(data)\n\n    # Print information about the pipeline run\n    print(info)\n    logger.info(\"Completed Loading Data from Hubspot to BigQuery\")\n\n\ndef load_crm_data_with_history() -> None:\n    \"\"\"\n    Loads all HubSpot CRM resources and property change history for each entity.\n    The history entries are loaded to a tables per resource `{resource_name}_property_history`, e.g. `contacts_property_history`\n\n    Returns:\n        None\n    \"\"\"\n\n    # Create a DLT pipeline object with the pipeline name, dataset name, and destination database type\n    # Add dev_mode=(True or False) if you need your pipeline to create the dataset in your destination\n    p = dlt.pipeline(\n        pipeline_name=\"hubspot\",\n        dataset_name=\"hubspot_dataset\",\n        destination='bigquery',\n    )\n\n    # Configure the source with `include_history` to enable property history load, history is disabled by default\n    data = hubspot(include_history=True)\n    data.contacts.bind()\n    # Run the pipeline with the HubSpot source connector\n    info = p.run(data)\n\n    # Print information about the pipeline run\n    print(info)\n\n\ndef load_crm_objects_with_custom_properties() -> None:\n    \"\"\"\n    Loads CRM objects, reading only properties defined by the user.\n    \"\"\"\n\n    # Create a DLT pipeline object with the pipeline name,\n    # dataset name, properties to read and destination database\n    # type Add dev_mode=(True or False) if you need your\n    # pipeline to create the dataset in your destination\n    p = dlt.pipeline(\n        pipeline_name=\"hubspot\",\n        dataset_name=\"hubspot_dataset\",\n        destination='bigquery',\n    )\n\n    source = hubspot()\n\n    # By default, all the custom properties of a CRM object are extracted,\n    # ignoring those driven by Hubspot (prefixed with `hs_`).\n\n    # To read fields in addition to the custom ones:\n    # source.contacts.bind(props=[\"date_of_birth\", \"degree\"])\n\n    # To read only two particular fields:\n    source.contacts.bind(props=[\"date_of_birth\", \"degree\"], include_custom_props=False)\n\n    # Run the pipeline with the HubSpot source connector\n    info = p.run(source)\n\n    # Print information about the pipeline run\n    print(info)\n\n\ndef load_web_analytics_events(\n    object_type: THubspotObjectType, object_ids: List[str]\n) -> None:\n    \"\"\"\n    This function loads web analytics events for a list objects in `object_ids` of type `object_type`\n\n    Returns:\n        None\n    \"\"\"\n\n    # Create a DLT pipeline object with the pipeline name, dataset name, and destination database type\n    p = dlt.pipeline(\n        pipeline_name=\"hubspot\",\n        dataset_name=\"hubspot_dataset\",\n        destination='bigquery',\n        dev_mode=False,\n    )\n\n    # you can get many resources by calling this function for various object types\n    resource = hubspot_events_for_objects(object_type, object_ids)\n    # and load them together passing resources in the list\n    info = p.run([resource])\n\n    # Print information about the pipeline run\n    print(info)\n```\n\n----------------------------------------\n\nTITLE: Connecting to Remote Database over SSH in DLT\nDESCRIPTION: This script demonstrates how to establish an SSH tunnel, create a SQLAlchemy engine, and use it to configure and run a DLT pipeline for accessing a remote database securely. It includes credential management and SSH tunnel setup.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/configuration.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom sshtunnel import SSHTunnelForwarder\nfrom sqlalchemy import create_engine\n\nfrom dlt.sources.sql_database import sql_table\nimport dlt\n\nssh_creds = dlt.secrets[\"ssh\"]\ndb_creds = dlt.secrets[\"destination.sqlalchemy.credentials\"]\n\nwith SSHTunnelForwarder(\n    (ssh_creds[\"server_ip_address\"], 22),\n    ssh_username=ssh_creds[\"username\"],\n    ssh_pkey=ssh_creds[\"private_key_path\"],\n    ssh_private_key_password=ssh_creds.get(\"private_key_password\"),\n    remote_bind_address=(\"127.0.0.1\", 5432),\n) as tunnel:\n    engine = create_engine(\n        f\"postgresql://{db_creds['username']}:{db_creds['password']}\"\n        f\"@127.0.0.1:{tunnel.local_bind_port}/{db_creds['database']}\"\n    )\n\n    # Access database table as a dlt resource\n    table_resource = sql_table(engine, table=\"employees\", schema=\"public\")\n\n    # Define and run the pipeline\n    pipeline = dlt.pipeline(\n        pipeline_name=\"remote_db_pipeline_2\",\n        destination=\"duckdb\",\n        dataset_name=\"remote_dataset\",\n    )\n\n    print(pipeline.run(table_resource))\n```\n\n----------------------------------------\n\nTITLE: Project Configuration YAML\nDESCRIPTION: Example DLT project configuration defining profiles, sources, destinations, and pipelines\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nprofiles:\n  dev: {}\n\nsources:\n  arrow:\n    type: sources.arrow.source\n\ndestinations:\n  duckdb:\n    type: duckdb\n\ndatasets: {}\n\npipelines:\n  my_pipeline:\n    source: arrow\n    destination: duckdb\n    dataset_name: my_pipeline_dataset\n```\n\n----------------------------------------\n\nTITLE: Retrieving Data from Pipeline in Python\nDESCRIPTION: Demonstrates how to retrieve data from a dlt pipeline and load it into a Pandas DataFrame or PyArrow Table. This snippet shows the basic steps of accessing a dataset, getting a table relation, and fetching data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/dataset.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Assuming you have a Pipeline object named 'pipeline'\n# and you have loaded data to a table named 'items' in the destination\n\n# Step 1: Get the readable dataset from the pipeline\ndataset = pipeline.dataset()\n\n# Step 2: Access a table as a ReadableRelation\nitems_relation = dataset.items  # Or dataset[\"items\"]\n\n# Step 3: Fetch the entire table as a Pandas DataFrame\ndf = items_relation.df()\n\n# Alternatively, fetch as a PyArrow Table\narrow_table = items_relation.arrow()\n```\n\n----------------------------------------\n\nTITLE: GitHub Events Pipeline Implementation\nDESCRIPTION: Python implementation of a pipeline that fetches GitHub events and dispatches them to different DuckDB tables based on event type. Uses incremental loading and handles pagination of the GitHub API.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/dispatch-to-multiple-tables.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.helpers import requests\n\n@dlt.resource(\n    primary_key=\"id\",\n    table_name=lambda i: i[\"type\"],\n    write_disposition=\"append\",\n)\ndef repo_events(last_created_at=dlt.sources.incremental(\"created_at\")):\n    url = \"https://api.github.com/repos/dlt-hub/dlt/events?per_page=100\"\n\n    while True:\n        response = requests.get(url)\n        response.raise_for_status()\n        yield response.json()\n\n        # Stop requesting pages if the last element was already older than\n        # the initial value.\n        # Note: incremental will skip those items anyway, we just do not\n        # want to use the API limits.\n        if last_created_at.start_out_of_range:\n            break\n\n        # Get the next page.\n        if \"next\" not in response.links:\n            break\n        url = response.links[\"next\"][\"url\"]\n\n\npipeline = dlt.pipeline(\n    pipeline_name=\"github_events\",\n    destination=\"duckdb\",\n    dataset_name=\"github_events_data\",\n)\nload_info = pipeline.run(repo_events)\nrow_counts = pipeline.last_trace.last_normalize_info\n\nprint(row_counts)\nprint(\"------\")\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster dlt Assets\nDESCRIPTION: Python code defining Dagster assets using the dlt_assets decorator, integrating the GitHub source and DuckDB destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-dagster.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dagster import AssetExecutionContext\nfrom dagster_embedded_elt.dlt import DagsterDltResource, dlt_assets\nfrom .github_pipeline import github_source\n\n@dlt_assets(\n    dlt_source=github_source(),\n    dlt_pipeline=dlt.pipeline(\n        pipeline_name=\"github_issues\",\n        dataset_name=\"github\",\n        destination=\"duckdb\",\n        progress=\"log\",\n    ),\n    name=\"github\",\n    group_name=\"github\",\n)\ndef dagster_github_assets(context: AssetExecutionContext, dlt: DagsterDltResource):\n    yield from dlt.run(context=context)\n```\n\n----------------------------------------\n\nTITLE: Selectively Refreshing DLT Pipeline Resources\nDESCRIPTION: This snippet demonstrates how to selectively refresh specific resources in a dlt pipeline using the 'drop_resources' mode. It drops tables and resets state for the specified resources only.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/pipeline.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\npipeline = dlt.pipeline(\"airtable_demo\", destination=\"duckdb\")\npipeline.run(airtable_emojis().with_resources(\"ðŸ“† Schedule\"), refresh=\"drop_resources\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Nested Table References in Python\nDESCRIPTION: Example demonstrating how to model custom relations between root and nested tables using nested_hints in @dlt.resource. Shows setup of compound primary keys and custom table references.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsource = data_source()\n# remove iso time detector\nsource.schema.remove_type_detection(\"iso_timestamp\")\n# convert UNIX timestamp (float, within a year from NOW) into timestamp\nsource.schema.add_type_detection(\"timestamp\")\n```\n\n----------------------------------------\n\nTITLE: Creating Dummy Data Source with DLT Decorators\nDESCRIPTION: Defines a source function that generates sample data with three columns (id, name, country_code) using DLT decorators.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/customising-pipelines/removing_columns.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\n# This function creates a dummy data source.\n@dlt.source\ndef dummy_source():\n    @dlt.resource(write_disposition=\"replace\")\n    def dummy_data():\n        for i in range(3):\n            yield {\"id\": i, \"name\": f\"Jane Washington {i}\", \"country_code\": 40 + i}\n\n    return dummy_data()\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Analytics and BigQuery Credentials in Python\nDESCRIPTION: This Python snippet demonstrates how to set up credentials for Google Analytics source and BigQuery destination using environment variables and dlt.secrets. It shows different methods of assigning sensitive information in a dlt pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport dlt\n\n# Do not set up the secrets directly in the code!\n# What you can do is reassign env variables\nos.environ[\"SOURCES__GOOGLE_ANALYTICS__CREDENTIALS__CLIENT_EMAIL\"] = os.environ.get(\"SHEETS_CLIENT_EMAIL\")\nos.environ[\"SOURCES__GOOGLE_ANALYTICS__CREDENTIALS__PRIVATE_KEY\"] = os.environ.get(\"ANALYTICS_PRIVATE_KEY\")\nos.environ[\"SOURCES__GOOGLE_ANALYTICS__CREDENTIALS__PROJECT_ID\"] = os.environ.get(\"ANALYTICS_PROJECT_ID\")\n\nos.environ[\"DESTINATION__CREDENTIALS__CLIENT_EMAIL\"] = os.environ.get(\"BIGQUERY_CLIENT_EMAIL\")\nos.environ[\"DESTINATION__CREDENTIALS__PRIVATE_KEY\"] = os.environ.get(\"BIGQUERY_PRIVATE_KEY\")\nos.environ[\"DESTINATION__CREDENTIALS__PROJECT_ID\"] = os.environ.get(\"BIGQUERY_PROJECT_ID\")\n\n# Or set them to the dlt.secrets\ndlt.secrets[\"sources.credentials.client_email\"] = os.environ.get(\"SHEETS_CLIENT_EMAIL\")\ndlt.secrets[\"sources.credentials.private_key\"] = os.environ.get(\"SHEETS_PRIVATE_KEY\")\ndlt.secrets[\"sources.credentials.project_id\"] = os.environ.get(\"SHEETS_PROJECT_ID\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Stripe Source Function\nDESCRIPTION: Python function decorator implementing the main Stripe data source with configurable endpoints and date range.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/stripe.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef stripe_source(\n    endpoints: Tuple[str, ...] = STRIPE_ENDPOINTS,\n    stripe_secret_key: str = dlt.secrets.value,\n    start_date: Optional[DateTime] = None,\n    end_date: Optional[DateTime] = None,\n) -> Iterable[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Loading All Tables from SQL Database with DLT\nDESCRIPTION: This snippet demonstrates how to load all tables from a SQL database using the sql_database source in a DLT pipeline. It defines the pipeline, fetches all tables, and runs the pipeline with a replace disposition.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/configuration.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.sql_database import sql_database\n\ndef load_entire_database() -> None:\n    # Define the pipeline\n    pipeline = dlt.pipeline(\n        pipeline_name=\"rfam\",\n        destination='synapse',\n        dataset_name=\"rfam_data\"\n    )\n\n    # Fetch all the tables from the database\n    source = sql_database()\n\n    # Run the pipeline\n    info = pipeline.run(source, write_disposition=\"replace\")\n\n    # Print load info\n    print(info)\n```\n\n----------------------------------------\n\nTITLE: Loading Data from Filesystem with dlt\nDESCRIPTION: Example of using dlt to extract data from cloud storage (S3 in this case). Demonstrates loading CSV files into DuckDB.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/intro.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.filesystem import filesystem\n\nresource = filesystem(\n    bucket_url=\"s3://example-bucket\",\n    file_glob=\"*.csv\"\n)\n\npipeline = dlt.pipeline(\n    pipeline_name=\"filesystem_example\",\n    destination=\"duckdb\",\n    dataset_name=\"filesystem_data\",\n)\n\nload_info = pipeline.run(resource)\n\n# print load info and the \"example\" table as dataframe\nprint(load_info)\nprint(pipeline.dataset().example.df())\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Loading with sql_table Resource\nDESCRIPTION: Example showing how to set up incremental loading for a SQL table using a timestamp column as cursor. The code demonstrates configuration with initial value setting and pipeline setup.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/advanced.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.sql_database import sql_table\nfrom dlt.common.pendulum import pendulum\n\n# Example: Incrementally loading a table based on a timestamp column\ntable = sql_table(\n   table='family',\n   incremental=dlt.sources.incremental(\n       'last_modified',  # Cursor column name\n       initial_value=pendulum.DateTime(2024, 1, 1, 0, 0, 0)  # Initial cursor value\n   )\n)\n\npipeline = dlt.pipeline(destination=\"duckdb\")\nextract_info = pipeline.extract(table, write_disposition=\"merge\")\nprint(extract_info)\n```\n\n----------------------------------------\n\nTITLE: Loading Tables from SQL Database with dlt\nDESCRIPTION: Python function that creates a pipeline to extract specific tables (family and genome) from a SQL database source and load them into DuckDB. The function configures the source, creates a pipeline, and executes it.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/sql-database.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.sql_database import sql_database\n\ndef load_tables_family_and_genome():\n\n    # Create a dlt source that will load tables \"family\" and \"genome\"\n    source = sql_database().with_resources(\"family\", \"genome\")\n\n    # Create a dlt pipeline object\n    pipeline = dlt.pipeline(\n        pipeline_name=\"sql_to_duckdb_pipeline\", # Custom name for the pipeline\n        destination=\"duckdb\", # dlt destination to which the data will be loaded\n        dataset_name=\"sql_to_duckdb_pipeline_data\" # Custom name for the dataset created in the destination\n    )\n\n    # Run the pipeline\n    load_info = pipeline.run(source)\n\n    # Pretty print load information\n    print(load_info)\n\nif __name__ == '__main__':\n    load_tables_family_and_genome()\n```\n\n----------------------------------------\n\nTITLE: Schema Evolution Handling in DLT Pipeline\nDESCRIPTION: Shows how DLT handles various schema changes including adding columns, changing data types, removing columns, and renaming fields.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema-evolution.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndata = [{\n    \"organization\": \"Tech Innovations Inc.\",\n    \"CEO\": \"Alice Smith\",\n    \"address\": {\n        'main_block': 'r&d',\n    },\n    \"Inventory\": [\n        {\"name\": \"Plasma ray\", \"inventory nr\": \"AR2411\"},\n        {\"name\": \"Self-aware Roomba\", \"inventory nr\": \"AR268\"},\n        {\"name\": \"Type-inferrer\", \"inventory nr\": \"AR3621\"}\n    ]\n}]\n\n# Run `dlt` pipeline\ndlt.pipeline(\"organizations_pipeline\", destination=\"duckdb\").run(data, table_name=\"org\")\n```\n\n----------------------------------------\n\nTITLE: Simple DLT Pipeline Example - Python\nDESCRIPTION: Basic example showing how to create a DLT pipeline that loads simple user data into a DuckDB destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination-tables.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\ndata = [\n    {'id': 1, 'name': 'Alice'},\n    {'id': 2, 'name': 'Bob'}\n]\n\npipeline = dlt.pipeline(\n    pipeline_name='quick_start',\n    destination='duckdb',\n    dataset_name='mydata'\n)\nload_info = pipeline.run(data, table_name=\"users\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a Basic DLT Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to create a simple dlt pipeline that loads a list of objects into a DuckDB table named 'three'. It shows pipeline instantiation with destination and dataset name, and how to run the pipeline with data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/pipeline.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\npipeline = dlt.pipeline(destination=\"duckdb\", dataset_name=\"sequence\")\n\ninfo = pipeline.run([{'id':1}, {'id':2}, {'id':3}], table_name=\"three\")\n\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Configuring Naming Convention in TOML\nDESCRIPTION: This snippet demonstrates how to set a global naming convention for all pipelines using a TOML configuration file. It specifies the 'sql_ci_v1' naming convention.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/naming-convention.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[schema]\nnaming=\"sql_ci_v1\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Players Games Resource\nDESCRIPTION: Python function decorated with @dlt.resource that fetches player games for a specified time period. It uses state management to track already loaded archives and avoid duplicates.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/chess.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(write_disposition=\"append\")\ndef players_games(\n    players: List[str], start_month: str = None, end_month: str = None\n) -> Iterator[TDataItems]:\n    # gets a list of already checked (loaded) archives.\n    checked_archives = dlt.current.resource_state().setdefault(\"archives\", [])\n    yield {}  # return your retrieved data here\n```\n\n----------------------------------------\n\nTITLE: GitHub Reactions Source Implementation\nDESCRIPTION: Python implementation of the GitHub reactions source that fetches issues and pull requests with their associated reactions using GraphQL.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/github.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef github_reactions(\n    owner: str,\n    name: str,\n    access_token: str = dlt.secrets.value,\n    items_per_page: int = 100,\n    max_items: int = None,\n    max_item_age_seconds: float = None,\n) -> Sequence[DltResource]:\n\n   return dlt.resource(\n      _get_reactions_data(\n         \"issues\",\n         owner,\n         name,\n         access_token,\n         items_per_page,\n         max_items,\n         max_item_age_seconds,\n      ),\n      name=\"issues\",\n      write_disposition=\"replace\",\n   )\n```\n\n----------------------------------------\n\nTITLE: Loading All Range Names from Google Sheets\nDESCRIPTION: Shows how to load all named ranges from a Google Spreadsheet by enabling named range retrieval.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nload_data = google_spreadsheet(\n        \"https://docs.google.com/spreadsheets/d/1HhWHjqouQnnCIZAFa2rL6vT91YRN8aIhts22SUUR580/edit#gid=0\", # Spreadsheet URL\n        get_sheets=False,\n        get_named_ranges=True,\n)\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Implementing Device Price Fetching Function\nDESCRIPTION: Python function that retrieves average device prices using SerpAPI and Google Shopping, with caching functionality to minimize API calls.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/user_agent_device_data_enrichment.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\n# Uncomment transformer function if it is to be used as a transformer,\n# otherwise, it is being used with the `add_map` functionality.\n\n# @dlt.transformer(data_from=tracked_data)\ndef fetch_average_price(user_tracked_data):\n    \"\"\"\n    Fetches the average price of a device from an external API and\n    updates the user_data dictionary.\n\n    This function retrieves the average price of a device specified in the\n    user_data dictionary by making an API request. The price data is cached\n    in the device_info state to reduce API calls. If the data for the device\n    is older than 180 days, a new API request is made.\n\n    Args:\n        user_tracked_data (dict): A dictionary containing user data, including\n        the device name.\n\n    Returns:\n        dict: The updated user_data dictionary with added device price and\n        updated timestamp.\n    \"\"\"\n\n    # Retrieve the API key from dlt secrets\n    api_key: str = dlt.secrets.get(\"sources.api_key\")\n\n    # Get the current resource state for device information\n    device_info = dlt.current.resource_state().setdefault(\"devices\", {})\n\n    # Current timestamp for checking the last update\n    current_timestamp = datetime.datetime.now()\n\n    # Print the current device information\n    # print(device_info) # if you need to check state\n\n    # Extract the device name from user data\n    device = user_tracked_data['device_name']\n    device_data = device_info.get(device, {})\n\n    # Calculate the time since the last update\n    last_updated = (\n        current_timestamp -\n        device_data.get('timestamp', datetime.datetime.min)\n    )\n    # Check if the device is not in state or data is older than 180 days\n    if device not in device_info or last_updated > datetime.timedelta(days=180):\n        try:\n            # Make an API request to fetch device prices\n            response = requests.get(\"https://serpapi.com/search\", params={\n                \"engine\": \"google_shopping\", \"q\": device,\n                \"api_key\": api_key, \"num\": 10\n            })\n        except requests.RequestException as e:\n            print(f\"Request failed: {e}\")\n            return None\n\n        if response.status_code != 200:\n            print(f\"Failed to retrieve data: {response.status_code}\")\n            return None\n\n        # Process the response to extract prices\n        results = response.json().get(\"shopping_results\", [])\n        prices = []\n        for r in results:\n            if r.get(\"price\"):\n                # Split the price string and convert each part to float\n                price = r.get(\"price\")\n                price_parts = price.replace('$', '').replace(',', '').split()\n                for part in price_parts:\n                    try:\n                        prices.append(float(part))\n                    except ValueError:\n                        pass  # Ignore parts that can't be converted to float\n\n        # Calculate the average price and update the device_info\n        device_price = round(sum(prices) / len(prices), 2) if prices else None\n        device_info[device] = {\n            'timestamp': current_timestamp,\n            'price': device_price\n        }\n\n        # Add the device price and timestamp to the user data\n        user_tracked_data['device_price_USD'] = device_price\n        user_tracked_data['price_updated_at'] = current_timestamp\n\n    else:\n        # Use cached price data if available and not outdated\n        user_tracked_data['device_price_USD'] = device_data.get('price')\n        user_tracked_data['price_updated_at'] = device_data.get('timestamp')\n\n    return user_tracked_data\n```\n\n----------------------------------------\n\nTITLE: Table Schema Definition in YAML\nDESCRIPTION: Example YAML schema showing the structure of customer and nested purchase tables with their columns, data types, and relationships.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema.md#2025-04-14_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ntables:\n  customers:\n    columns:\n      id:\n        nullable: false\n        primary_key: true\n        data_type: bigint\n      name:\n        data_type: text\n        nullable: true\n      city:\n        data_type: text\n        nullable: true\n      _dlt_id:\n        data_type: text\n        nullable: false\n        unique: true\n        row_key: true\n      _dlt_load_id:\n        data_type: text\n        nullable: false\n    write_disposition: merge\n    resource: customers\n  customers__purchases:\n    columns:\n      customer_id:\n        data_type: bigint\n        primary_key: true\n        nullable: false\n      id:\n        nullable: false\n        primary_key: true\n        data_type: bigint\n      name:\n        data_type: text\n        nullable: true\n      price:\n        data_type: decimal\n        nullable: true\n      _dlt_root_id:\n        data_type: text\n        nullable: false\n        root_key: true\n      _dlt_id:\n        data_type: text\n        nullable: false\n        unique: true\n        row_key: true\n      _dlt_load_id:\n        data_type: text\n        nullable: false\n    references:\n    - referenced_table: customers\n      columns:\n      - customer_id\n      referenced_columns:\n      - id\n    write_disposition: merge\n    resource: customers\n```\n\n----------------------------------------\n\nTITLE: Using Ibis Connection with DLT Dataset in Python\nDESCRIPTION: Python code demonstrating how to obtain an Ibis connection from a DLT dataset, list tables, retrieve a specific table, and execute a query. This snippet shows the basic workflow of integrating Ibis with DLT.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/ibis-backend.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# get the dataset from the pipeline\ndataset = pipeline.dataset()\ndataset_name = pipeline.dataset_name\n\n# get the native ibis connection from the dataset\nibis_connection = dataset.ibis()\n\n# list all tables in the dataset\n# NOTE: You need to provide the dataset name to ibis, in ibis datasets are named databases\nprint(ibis_connection.list_tables(database=dataset_name))\n\n# get the items table\ntable = ibis_connection.table(\"items\", database=dataset_name)\n\n# print the first 10 rows\nprint(table.limit(10).execute())\n\n# Visit the ibis docs to learn more about the available methods\n```\n\n----------------------------------------\n\nTITLE: Loading Multiple Zendesk Sources\nDESCRIPTION: Example of loading data from multiple Zendesk sources (Support, Chat, Talk) in a single pipeline\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/zendesk.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Zendesk support source function\ndata_support = zendesk_support(load_all=True)\n# Zendesk chat source function\ndata_chat = zendesk_chat()\n# Zendesk talk source function\ndata_talk = zendesk_talk()\n# Run pipeline with all 3 sources\ninfo = pipeline.run([data_support, data_chat, data_talk])\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a DLT Pipeline with add_map in Python\nDESCRIPTION: This snippet shows how to create a DLT pipeline, use the add_map function to apply a transformation, and run the pipeline. It uses DuckDB as the destination and applies the fetch_average_price function to enrich the data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/user_agent_device_data_enrichment.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create the pipeline\npipeline = dlt.pipeline(\n    pipeline_name=\"data_enrichment_one\",\n    destination=\"duckdb\",\n    dataset_name=\"user_device_enrichment\",\n)\n\n# Run the pipeline with the transformed source\nload_info = pipeline.run(tracked_data.add_map(fetch_average_price))\n\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Implementing Jira Search Source\nDESCRIPTION: Python decorator and function definition for creating a Jira search source with JQL support.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/jira.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef jira_search(\n     subdomain: str = dlt.secrets.value,\n     email: str = dlt.secrets.value,\n     api_token: str = dlt.secrets.value,\n) -> Iterable[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Defining Google Sheets Source Function with DLT in Python\nDESCRIPTION: This snippet defines a source function 'google_sheets' using the DLT library to extract data from specified tabs in a Google Sheets spreadsheet. It handles credential parsing, tab name processing, and uses the Google Sheets API to fetch data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/advanced.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef google_sheets(\n    spreadsheet_id=dlt.config.value,\n    tab_names=dlt.config.value,\n    credentials=dlt.secrets.value,\n    only_strings=False\n):\n    # Allow both a dictionary and a string to be passed as credentials\n    if isinstance(credentials, str):\n        credentials = json.loads(credentials)\n    # Allow both a list and a comma-delimited string to be passed as tabs\n    if isinstance(tab_names, str):\n      tab_names = tab_names.split(\",\")\n    sheets = build('sheets', 'v4', credentials=ServiceAccountCredentials.from_service_account_info(credentials))\n    tabs = []\n    for tab_name in tab_names:\n        data = _get_sheet(sheets, spreadsheet_id, tab_name)\n        tabs.append(dlt.resource(data, name=tab_name))\n    return tabs\n```\n\n----------------------------------------\n\nTITLE: Initializing a New DLT Project with REST API Source\nDESCRIPTION: Command to create a new DLT project with REST API source and DuckDB destination, generating the necessary project structure and configuration files.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/rest-api.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init rest_api duckdb\n```\n\n----------------------------------------\n\nTITLE: Creating XML File Transformer in dlt\nDESCRIPTION: Custom standalone transformer that reads data from XML files using the xmltodict library. The example shows how to set up a pipeline that fetches an XML file from S3 storage and processes it with the custom transformer.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/advanced.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.common.storages.fsspec_filesystem import FileItemDict\nfrom dlt.common.typing import TDataItems\nfrom dlt.sources.filesystem import filesystem\n\nBUCKET_URL = \"s3://my_bucket/data\"\n\n# Define a standalone transformer to read data from an XML file.\n@dlt.transformer(standalone=True)\ndef read_xml(items: Iterator[FileItemDict]) -> Iterator[TDataItems]:\n    # Import the required xmltodict library.\n    import xmltodict\n\n    # Iterate through each file item.\n    for file_obj in items:\n        # Open the file object.\n        with file_obj.open() as file:\n            # Parse the file to dict records.\n            yield xmltodict.parse(file.read())\n\n# Set up the pipeline to fetch a specific XML file from a filesystem (bucket).\nexample_xml = filesystem(\n    bucket_url=BUCKET_URL, file_glob=\"../directory/example.xml\"\n) | read_xml()   # Pass the data through the transformer\n\npipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\", dataset_name=\"example_xml_data\")\n# Execute the pipeline and load the extracted data into the \"duckdb\" destination.\nload_info = pipeline.run(example_xml.with_name(\"example_xml_data\"))\n\n# Print the loading information.\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Running dbt with a dlt Pipeline Example\nDESCRIPTION: This example demonstrates how to run a dlt pipeline to load Pipedrive data and then use the dbt runner to transform that data. It shows how to set up the pipeline, create a virtual environment for dbt, initialize the dbt package, and run the models.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/transformations/dbt/dbt.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Load all Pipedrive endpoints to the pipedrive_raw dataset\npipeline = dlt.pipeline(\n    pipeline_name='pipedrive',\n    destination='bigquery',\n    dataset_name='pipedrive_raw'\n)\n\nload_info = pipeline.run(pipedrive_source())\nprint(load_info)\n\n# Create a transformation on a new dataset called 'pipedrive_dbt'\n# We created a local dbt package\n# and added pipedrive_raw to its sources.yml\n# The destination for the transformation is passed in the pipeline\npipeline = dlt.pipeline(\n    pipeline_name='pipedrive',\n    destination='bigquery',\n    dataset_name='pipedrive_dbt'\n)\n\n# Make or restore venv for dbt, using the latest dbt version\n# NOTE: If you have dbt installed in your current environment, just skip this line\n#       and the `venv` argument to dlt.dbt.package()\nvenv = dlt.dbt.get_venv(pipeline)\n\n# Get runner, optionally pass the venv\ndbt = dlt.dbt.package(\n    pipeline,\n    \"pipedrive/dbt_pipedrive/pipedrive\",\n    venv=venv\n)\n\n# Run the models and collect any info\n# If running fails, the error will be raised with a full stack trace\nmodels = dbt.run_all()\n\n# On success, print the outcome\nfor m in models:\n    print(\n        f\"Model {m.model_name} materialized\" +\n        f\" in {m.time}\" +\n        f\" with status {m.status}\" +\n        f\" and message {m.message}\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Loading Shopify Orders in Weekly Chunks\nDESCRIPTION: Implements a chunked loading strategy for Shopify orders using weekly date ranges. This approach helps manage large data loads and enables parallel processing for faster initial loads while maintaining incremental updates.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/shopify.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Load all orders from 2023-01-01 to now\nmin_start_date = current_start_date = pendulum.DateTime(2023, 1, 1)\nmax_end_date = pendulum.now()\n# Create a list of time ranges of 1 week each, we'll use this to load the data in chunks\nranges: List[Tuple[pendulum.DateTime, pendulum.DateTime]] = []\nwhile current_start_date < max_end_date:\n     end_date = min(current_start_date.add(weeks=1), max_end_date)\n     ranges.append((current_start_date, end_date))\n     current_start_date = end_date\n\nfor start_date, end_date in ranges:\n     print(f\"Load orders between {start_date} and {end_date}\")\n     # Create the source with start and end date set according to the current time range to filter\n     # created_at_min lets us set a cutoff to exclude orders created before the initial date of (2023-01-01)\n     # even if they were updated after that date\n     load_data = shopify_source(\n         start_date=start_date, end_date=end_date, created_at_min=min_start_date\n     ).with_resources(\"orders\")\n\n     load_info = pipeline.run(load_data)\n     print(load_info)\n# Continue loading new data incrementally starting at the end of the last range\n# created_at_min still filters out items created before 2023-01-01\nload_info = pipeline.run(\n     shopify_source(\n         start_date=max_end_date, created_at_min=min_start_date\n     ).with_resources(\"orders\")\n)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Performing Full Load with GitHub Issues API in Python\nDESCRIPTION: Example showing how to perform a full load operation by fetching GitHub issues data and loading it into BigQuery with replace disposition. The code iterates through different reaction types and pages of issues.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/full-loading.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\np = dlt.pipeline(destination=\"bigquery\", dataset_name=\"github\")\nissues = []\nreactions = [\"%2B1\", \"-1\", \"smile\", \"tada\", \"thinking_face\", \"heart\", \"rocket\", \"eyes\"]\nfor reaction in reactions:\n    for page_no in range(1, 3):\n      page = requests.get(f\"https://api.github.com/repos/{REPO_NAME}/issues?state=all&sort=reactions-{reaction}&per_page=100&page={page_no}\", headers=headers)\n      print(f\"Got page for {reaction} page {page_no}, requests left\", page.headers[\"x-ratelimit-remaining\"])\n      issues.extend(page.json())\np.run(issues, write_disposition=\"replace\", primary_key=\"id\", table_name=\"issues\")\n```\n\n----------------------------------------\n\nTITLE: Incremental Loading of GitHub Repository Events in Python\nDESCRIPTION: Fetches and processes GitHub repository events data incrementally. It loads all data during the first run and only new data in subsequent runs. This snippet also demonstrates how to use an optional access token for API authentication.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/github.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nload_data = github_repo_events(\n    \"duckdb\", \"duckdb\", access_token=os.getenv(\"ACCESS_TOKEN_ENV_VAR\")\n)\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Reading and Writing Pipeline State in a Resource\nDESCRIPTION: This snippet demonstrates how to use the resource-scoped state to create a list of chess game archives and prevent requesting duplicates. It shows how to read and write state, and use it to control the flow of data processing.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/state.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(write_disposition=\"append\")\ndef players_games(chess_url, player, start_month=None, end_month=None):\n    # create or request a list of archives from resource-scoped state\n    checked_archives = dlt.current.resource_state().setdefault(\"archives\", [])\n    # get a list of archives for a particular player\n    archives = _get_players_archives(chess_url, player)\n    for url in archives:\n        if url in checked_archives:\n            print(f\"skipping archive {url}\")\n            continue\n        else:\n            print(f\"getting archive {url}\")\n            checked_archives.append(url)\n        # get the filtered archive\n        r = requests.get(url)\n        r.raise_for_status()\n        yield r.json().get(\"games\", [])\n```\n\n----------------------------------------\n\nTITLE: Setting Up Snowflake Database User and Permissions\nDESCRIPTION: SQL commands to create a new database, user, role, and assign necessary permissions for dlt to work with Snowflake. This grants appropriate access levels for creating schemas and tables.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n-- create database with standard settings\nCREATE DATABASE dlt_data;\n-- create new user - set your password here\nCREATE USER loader WITH PASSWORD='<password>';\n-- we assign all permissions to a role\nCREATE ROLE DLT_LOADER_ROLE;\nGRANT ROLE DLT_LOADER_ROLE TO USER loader;\n-- give database access to new role\nGRANT USAGE ON DATABASE dlt_data TO DLT_LOADER_ROLE;\n-- allow `dlt` to create new schemas\nGRANT CREATE SCHEMA ON DATABASE dlt_data TO DLT_LOADER_ROLE;\n-- allow access to a warehouse named COMPUTE_WH\nGRANT USAGE ON WAREHOUSE COMPUTE_WH TO DLT_LOADER_ROLE;\n-- grant access to all future schemas and tables in the database\nGRANT ALL PRIVILEGES ON FUTURE SCHEMAS IN DATABASE dlt_data TO DLT_LOADER_ROLE;\nGRANT ALL PRIVILEGES ON FUTURE TABLES IN DATABASE dlt_data TO DLT_LOADER_ROLE;\n```\n\n----------------------------------------\n\nTITLE: Defining Companies Resource in Python\nDESCRIPTION: Python function that defines the 'companies' resource, fetching data from the HubSpot API and loading it to the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/hubspot.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(name=\"companies\", write_disposition=\"replace\")\ndef companies(\n   api_key: str = API_KEY,\n   include_history: bool = False,\n   props: Sequence[str] = DEFAULT_COMPANY_PROPS,\n   include_custom_props: bool = True,\n) -> Iterator[TDataItems]:\n   \"\"\"Hubspot companies resource\"\"\"\n   yield from crm_objects(\n      \"company\",\n      api_key,\n      include_history=include_history,\n      props=props,\n      include_custom_props=include_custom_props,\n   )\n```\n\n----------------------------------------\n\nTITLE: Creating dlt Pipeline with Snowflake and GCS Staging\nDESCRIPTION: This Python code creates a dlt pipeline that loads chess player data to Snowflake using GCS as the staging destination. It shows how to activate the staging location.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_17\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name='chess_pipeline',\n    destination='snowflake',\n    staging='filesystem', # add this to activate the staging location\n    dataset_name='player_data'\n)\n```\n\n----------------------------------------\n\nTITLE: DuckDB Pipeline Configuration Examples\nDESCRIPTION: Various examples of configuring DuckDB pipelines with different database paths and settings\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/duckdb.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\n\ndb = duckdb.connect()\np = dlt.pipeline(\n  pipeline_name=\"chess\",\n  destination=dlt.destinations.duckdb(db),\n  dataset_name=\"chess_data\",\n  dev_mode=False,\n)\n\n# Or if you would like to use an in-memory duckdb instance\ndb = duckdb.connect(\":memory:\")\np = pipeline_one = dlt.pipeline(\n  pipeline_name=\"in_memory_pipeline\",\n  destination=dlt.destinations.duckdb(db),\n  dataset_name=\"chess_data\",\n)\n\nprint(db.sql(\"DESCRIBE;\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing Mux Source in Python\nDESCRIPTION: Python function defining the main Mux data source that yields assets and views resources.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mux.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef mux_source() -> Iterable[DltResource]:\n    yield assets_resource\n    yield views_resource\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Stripe Source\nDESCRIPTION: Python function decorator implementing incremental data loading from Stripe API endpoints.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/stripe.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef incremental_stripe_source(\n    endpoints: Tuple[str, ...] = INCREMENTAL_ENDPOINTS,\n    stripe_secret_key: str = dlt.secrets.value,\n    initial_start_date: Optional[DateTime] = None,\n    end_date: Optional[DateTime] = None,\n) -> Iterable[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Manual Testing Script for Webhook in Python\nDESCRIPTION: This Python script demonstrates how to manually test the webhook by sending a POST request to the cloud function's trigger URL. It includes a sample message payload and handles the response.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-gcp-cloud-function-as-webhook.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nwebhook_url = 'please set me up!' # Your cloud function Trigger URL\nmessage = {\n    'text': 'Hello, Slack!',\n    'user': 'dlthub',\n    'channel': 'dlthub'\n}\n\nresponse = requests.post(webhook_url, json=message)\nif response.status_code == 200:\n  print('Message sent successfully.')\nelse:\n  print('Failed to send message. Error:', response.text)\n```\n\n----------------------------------------\n\nTITLE: Defining Source Pipeline in Python using DLT\nDESCRIPTION: Creates a source pipeline configuration for replicating data from PostgreSQL. It sets the pipeline name, destination, dataset name, and enables dev mode.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/pg_replication.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Defining source pipeline\nsrc_pl = dlt.pipeline(\n    pipeline_name=\"source_pipeline\",\n    destination=\"postgres\",\n    dataset_name=\"source_dataset\",\n    dev_mode=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Applying Column Hints in DLT Resource (Python)\nDESCRIPTION: Demonstrates how to apply hints for nested columns in DLT, either when fetching source data or during resource definition. This allows specifying certain columns as 'json' to prevent normalization.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/frequently-asked-questions.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmy_source.resource3.apply_hints(\n    columns={\n        \"column_name\": {\n            \"data_type\": \"json\"\n        }\n    }\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(columns={\"column_name\": {\"data_type\": \"json\"}})\ndef my_resource():\n    # Function body goes here\n    pass\n```\n\n----------------------------------------\n\nTITLE: Initializing Filesystem Source and Loading Parquet Files in Python\nDESCRIPTION: This snippet demonstrates how to initialize a filesystem resource, create a pipeline, and load Parquet files incrementally using dlt. It sets up the filesystem source, applies incremental loading, and runs the pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/basic.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.filesystem import filesystem, read_parquet\n\nfilesystem_resource = filesystem(\n  bucket_url=\"file://Users/admin/Documents/parquet_files\",\n  file_glob=\"**/*.parquet\"\n)\nfilesystem_pipe = filesystem_resource | read_parquet()\nfilesystem_pipe.apply_hints(incremental=dlt.sources.incremental(\"modification_date\"))\n\n# We load the data into the table_name table\npipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\nload_info = pipeline.run(filesystem_pipe.with_name(\"table_name\"))\nprint(load_info)\nprint(pipeline.last_trace.last_normalize_info)\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Loading by Record Update Time\nDESCRIPTION: Example of incremental loading based on record update timestamp.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/basic.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.filesystem import filesystem, read_csv\n\n# We consider all CSV files\nall_files = filesystem(bucket_url=\"s3://bucket_name\", file_glob=\"directory/*.csv\")\n\n# But filter out only updated records\nfilesystem_pipe = (all_files | read_csv())\nfilesystem_pipe.apply_hints(incremental=dlt.sources.incremental(\"updated_at\"))\npipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\nload_info = pipeline.run(filesystem_pipe)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Triggering dbt Cloud Jobs with Helper Function\nDESCRIPTION: This snippet shows how to use the run_dbt_cloud_job() helper function to trigger and monitor dbt Cloud job runs. It supports various configuration options including custom git SHA and schema overrides.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/transformations/dbt/dbt_cloud.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.helpers.dbt_cloud import run_dbt_cloud_job\n\n# Trigger a job run with default configuration\nstatus = run_dbt_cloud_job()\n\n# Trigger a job run with additional data\nadditional_data = {\n    \"git_sha\": \"abcd1234\",\n    \"schema_override\": \"custom_schema\",\n    # ... other parameters\n}\nstatus = run_dbt_cloud_job(job_id=1234, data=additional_data, wait_for_outcome=True)\n```\n\n----------------------------------------\n\nTITLE: Defining Replication Resource for Postgres (Python)\nDESCRIPTION: Python code defining the replication_resource function which is the main resource for Postgres replication. It uses dlt decorators and specifies various parameters for customizing the replication process.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/pg_replication.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(\n    name=lambda args: args[\"slot_name\"] + \"_\" + args[\"pub_name\"],\n    standalone=True,\n)\ndef replication_resource(\n    slot_name: str,\n    pub_name: str,\n    credentials: ConnectionStringCredentials = dlt.secrets.value,\n    include_columns: Optional[Dict[str, Sequence[str]]] = None,\n    columns: Optional[Dict[str, TTableSchemaColumns]] = None,\n    target_batch_size: int = 1000,\n    flush_slot: bool = True,\n) -> Iterable[Union[TDataItem, DataItemWithMeta]]:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Pipeline Creation and Execution\nDESCRIPTION: Implementation of the dlt pipeline that combines the tracked data resource with URL parsing functionality.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/url-parser-data-enrichment.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create the pipeline\npipeline = dlt.pipeline(\n    pipeline_name=\"data_enrichment_three\",\n    destination=\"duckdb\",\n    dataset_name=\"user_device_enrichment\",\n)\n\n# Run the pipeline with the transformed source\nload_info = pipeline.run(tracked_data.add_map(url_parser))\n\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Accessing ReadableDataset and Tables in Python\nDESCRIPTION: Shows how to access a ReadableDataset from a pipeline and retrieve tables as ReadableRelation objects. It also demonstrates how to print row counts of all tables in the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/dataset.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Get the readable dataset from the pipeline\ndataset = pipeline.dataset()\n\n# print the row counts of all tables in the destination as dataframe\nprint(dataset.row_counts().df())\n\n# Using attribute access\nitems_relation = dataset.items\n\n# Using item access\nitems_relation = dataset[\"items\"]\n```\n\n----------------------------------------\n\nTITLE: Loading Kinesis Stream Data from Last Hour in Python\nDESCRIPTION: This code demonstrates how to load messages from a Kinesis stream for the last hour. It configures the kinesis_stream resource with specific parameters and runs the pipeline to process the data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/amazon_kinesis.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# The resource below will take its name from the stream name,\n# it can be used multiple times. By default, it assumes that data is JSON and parses it,\n# here we disable that to just get bytes in data elements of the message.\nkinesis_stream_data = kinesis_stream(\n    \"kinesis_source_name\",\n    parse_json=False,\n    initial_at_timestamp=pendulum.now().subtract(hours=1),\n)\ninfo = pipeline.run(kinesis_stream_data)\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Performing Full Refresh on Selected Resources in dlt\nDESCRIPTION: This snippet demonstrates how to force a full refresh on specific resources only, using the with_resources() method to select which tables should be replaced instead of incrementally updated.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/source.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\np.run(tables.with_resources(\"users\"), write_disposition=\"replace\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Merge Write Disposition in DLT Pipeline\nDESCRIPTION: Example showing how to set up a DLT pipeline with merge write disposition, which creates staging tables for atomic transactions. The pipeline includes a users resource with primary key and outputs data to DuckDB.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination-tables.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\n@dlt.resource(primary_key=\"id\", write_disposition=\"merge\")\ndef users():\n    yield [\n        {'id': 1, 'name': 'Alice 2'},\n        {'id': 2, 'name': 'Bob 2'}\n    ]\n\npipeline = dlt.pipeline(\n    pipeline_name='quick_start',\n    destination='duckdb',\n    dataset_name='mydata'\n)\n\nload_info = pipeline.run(users)\n```\n\n----------------------------------------\n\nTITLE: Modifying Schema Programmatically in Python\nDESCRIPTION: Demonstrates how to extract data with a default schema, modify the schema attributes programmatically, and then apply those changes. This example changes a column type from integer to text before running the pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/technical/working_with_schemas.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# extract some to \"table\" resource using default schema\np = dlt.pipeline(destination=redshift)\np.extract([1,2,3,4], name=\"table\")\n# get live schema\nschema = p.default_schema\n# we want the list data to be text, not integer\nschema.tables[\"table\"][\"columns\"][\"value\"] = schema_utils.new_column(\"value\", \"text\")\n# `run` will apply schema changes and run the normalizer and loader for already extracted data\np.run()\n```\n\n----------------------------------------\n\nTITLE: Accessing Data in Destination Instead of Pipeline State\nDESCRIPTION: This example shows how to load recent comments made by a given user_id by accessing the user_comments table in the destination. It demonstrates querying the destination to find the maximum comment id for a user and using it to filter results.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/state.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\n@dlt.resource(name=\"user_comments\")\ndef comments(user_id: str):\n    current_pipeline = dlt.current.pipeline()\n    # find the last comment id for the given user_id by looking in the destination\n    max_id: int = 0\n    # on the first pipeline run, the user_comments table does not yet exist so do not check at all\n    # alternatively, catch DatabaseUndefinedRelation which is raised when an unknown table is selected\n    if not current_pipeline.first_run:\n        # get user comments table from pipeline dataset\n        user_comments = current_pipeline.dataset().user_comments\n        # get last user comment id with ibis expression, ibis-extras need to be installed\n        max_id_df = user_comments.filter(user_comments.user_id == user_id).select(user_comments[\"_id\"].max()).df()\n        # if there are no comments for the user, max_id will be None, so we replace it with 0\n        max_id = max_id_df[0][0] if len(max_id_df.index) else 0\n\n    # use max_id to filter our results (we simulate an API query)\n    yield from [\n        {\"_id\": i, \"value\": letter, \"user_id\": user_id}\n        for i, letter in zip([1, 2, 3], [\"A\", \"B\", \"C\"])\n        if i > max_id\n    ]\n```\n\n----------------------------------------\n\nTITLE: Implementing a DLT Pipeline with a Custom Presto Destination\nDESCRIPTION: Example script demonstrating how to use a custom 'presto' destination with the DLT framework. The script loads GitHub repository events data into a Presto database, showing how to configure the pipeline with the custom destination and execute an incremental data load.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/create-new-destination.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\nfrom github import github_repo_events\nfrom presto import presto  # importing destination factory\n\ndef load_airflow_events() -> None:\n    \"\"\"Loads airflow events. Shows incremental loading. Forces anonymous access token.\"\"\"\n    pipeline = dlt.pipeline(\n        \"github_events\", destination=presto(), dataset_name=\"airflow_events\"\n    )\n    data = github_repo_events(\"apache\", \"airflow\", access_token=\"\")\n    print(pipeline.run(data))\n\nif __name__ == \"__main__\":\n    load_airflow_events()\n```\n\n----------------------------------------\n\nTITLE: Importing and Using Destination Factory in Python\nDESCRIPTION: Shows how to import a destination factory directly and use it in pipeline creation.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.destinations import filesystem\npipeline = dlt.pipeline(pipeline_name=\"pipeline_name\", destination=filesystem)\n```\n\n----------------------------------------\n\nTITLE: Initializing Chess.com API Source Instance\nDESCRIPTION: Python code to create an instance of the Chess.com API source, specifying player usernames and date range for data retrieval.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/chess.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Loads games for Nov 2022\nsource_instance = chess_source(\n    [\"magnuscarlsen\", \"vincentkeymer\", \"dommarajugukesh\", \"rpragchess\"],\n    start_month=\"2022/11\",\n    end_month=\"2022/11\",\n)\n```\n\n----------------------------------------\n\nTITLE: Processing Kinesis Messages Without Pipeline in Python\nDESCRIPTION: This snippet shows how to read Kinesis messages and process them without using a DLT pipeline. It demonstrates state management, iterating over messages, and saving state after each message for transactional loading.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/amazon_kinesis.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.common.configuration.container import Container\nfrom dlt.common.pipeline import StateInjectableContext\n\nSTATE_FILE = \"kinesis_source_name.state.json\"\n\n# Load the state if it exists.\nif os.path.exists(STATE_FILE):\n    with open(STATE_FILE, \"rb\") as rf:\n        state = json.typed_loadb(rf.read())\nelse:\n    # Provide new state.\n    state = {}\n\nwith Container().injectable_context(\n    StateInjectableContext(state=state)\n) as managed_state:\n    # dlt resources/source is just an iterator.\n    for message in kinesis_stream_data:\n        # Here you can send the message somewhere.\n        print(message)\n        # Save state after each message to have full transaction load.\n        # DynamoDB is also OK.\n        with open(STATE_FILE, \"wb\") as wf:\n            json.typed_dump(managed_state.state, wf)\n        print(managed_state.state)\n```\n\n----------------------------------------\n\nTITLE: Accessing and Selecting Source Resources\nDESCRIPTION: Shows how to access resources from a source and selectively load specific resources using the pipeline.run method.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/source.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom hubspot import hubspot\n\nsource = hubspot()\n# \"resources\" is a dictionary with all resources available, the key is the resource name\nprint(source.resources.keys())  # print names of all resources\n# print resources that are selected to load\nprint(source.resources.selected.keys())\n# load only \"companies\" and \"deals\" using the \"with_resources\" convenience method\npipeline.run(source.with_resources(\"companies\", \"deals\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing Merge Loading Strategy with DLT in Python\nDESCRIPTION: This code demonstrates creating a DLT pipeline that merges records from an SQL source into BigQuery based on a timestamp field 'last_modified_at' and 'id' field as the primary key. It configures incremental loading with a specific initial value and uses 'merge' write disposition to update existing records and insert new ones.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add-incremental-configuration.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef load_merge_table_resource() -> None:\n    \"\"\"Merge (update/insert) records based on last_modified_at timestamp and ID.\"\"\"\n    pipeline = dlt.pipeline(\n        pipeline_name=\"mysql_database\",\n        destination='bigquery',\n        dataset_name=\"dlt_contacts\",\n    )\n\n    # Merge records, 'contact' table, based on ID and last_modified_at timestamp\n    source = sql_database().with_resources(\"contact\")\n    source.contact.apply_hints(incremental=dlt.sources.incremental(\n        \"last_modified_at\", initial_value=datetime.datetime(2024, 4, 1, 0, 0, 0)),\n        primary_key=\"id\")\n\n    # Run the pipeline\n    info = pipeline.run(source, write_disposition=\"merge\")\n\n    # Print the info\n    print(info)\n\nload_merge_table_resource()\n```\n\n----------------------------------------\n\nTITLE: Implementing Event Ingestion Webhook with dlt in Python for GCP Cloud Function\nDESCRIPTION: This code implements a Google Cloud Function that receives webhook data, processes it using dlt, and loads it into BigQuery. It extracts JSON data from the incoming request and passes it to a dlt pipeline configured for BigQuery.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-gcp-cloud-function-as-webhook.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nimport time\nfrom google.cloud import bigquery\nfrom dlt.common import json\n\ndef your_webhook(request):\n    # Extract relevant data from the request payload\n    data = request.get_json()\n\n    Event = [data]\n\n    pipeline = dlt.pipeline(\n        pipeline_name='platform_to_bigquery',\n        destination='bigquery',\n        dataset_name='webhooks',\n    )\n\n    pipeline.run(Event, table_name='webhook') #table_name can be customized\n    return 'Event received and processed successfully.'\n```\n\n----------------------------------------\n\nTITLE: Initializing BigQuery Pipeline Project\nDESCRIPTION: Command to create a new dlt project with BigQuery configuration\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/bigquery.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init chess bigquery\n```\n\n----------------------------------------\n\nTITLE: Implementing Players Profiles Resource\nDESCRIPTION: Python function decorated with @dlt.resource that fetches player profiles for a list of usernames. It uses @dlt.defer for parallel execution in a thread pool.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/chess.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(write_disposition=\"replace\")\ndef players_profiles(players: List[str]) -> Iterator[TDataItem]:\n\n    @dlt.defer\n    def _get_profile(username: str) -> TDataItem:\n        return _get_path_with_retry(f\"player/{username}\")\n    \n    for username in players:\n        yield _get_profile(username)\n```\n\n----------------------------------------\n\nTITLE: Configuring a dlt Pipeline for BigQuery Destination in Python\nDESCRIPTION: Python code that shows how to modify a dlt pipeline configuration to use BigQuery as the destination instead of the default local DuckDB. The code creates a pipeline for chess.com data and runs it with the new destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/share-a-dataset.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\nif __name__ == \"__main__\":\n    pipeline = dlt.pipeline(\n        pipeline_name=\"chess_pipeline\",\n        destination='bigquery',\n        dataset_name=\"games_data\"\n    )\n    # get data for a few famous players\n    data = chess_source(\n        data=['magnuscarlsen', 'rpragchess'],\n        start_month=\"2022/11\",\n        end_month=\"2022/12\"\n    )\n    load_info = pipeline.run(data)\n```\n\n----------------------------------------\n\nTITLE: URL Parser Function Implementation\nDESCRIPTION: Function that sends URLs to the URL Parse API and returns parsed data containing URL components and metadata.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/url-parser-data-enrichment.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef url_parser(record):\n    \"\"\"\n    Send a URL to a parsing service and return the parsed data.\n\n    This function sends a URL to a specified API endpoint for URL parsing.\n\n    Parameters:\n    url (str): The URL to be parsed.\n\n    Returns:\n    dict: Parsed URL data in JSON format if the request is successful.\n    None: If the request fails (e.g., an invalid URL or server error).\n    \"\"\"\n    # Define the API endpoint URL for the URL parsing service\n    api_url = \"https://api.urlparse.com/v1/query\"\n    url = record['page_referer']\n    # Send a POST request to the API with the URL to be parsed\n    response = requests.post(api_url, json={\"url\": url})\n\n    # Check if the response from the API is successful (HTTP status code 200)\n    if response.status_code == 200:\n        # If successful, return the parsed data in JSON format\n        return response.json()\n    else:\n        # If the request failed, print an error message with the status code and return None\n        print(f\"Request for {url} failed with status code: {response.status_code}\")\n        return None\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline for Weaviate\nDESCRIPTION: Defining a dlt pipeline that will load data into a Weaviate destination with a specified dataset name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/weaviate.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"movies\",\n    destination=\"weaviate\",\n    dataset_name=\"MoviesDataset\",\n)\n```\n\n----------------------------------------\n\nTITLE: Reading Data from ReadableRelation in Python\nDESCRIPTION: Demonstrates various methods to read data from a ReadableRelation, including fetching as a Pandas DataFrame, PyArrow Table, or a list of Python tuples. It also shows how to iterate over data in chunks.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/dataset.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = items_relation.df()\n\narrow_table = items_relation.arrow()\n\nitems_list = items_relation.fetchall()\n\nfor df_chunk in items_relation.iter_df(chunk_size=500):\n    # Process each DataFrame chunk\n    pass\n\nfor arrow_chunk in items_relation.iter_arrow(chunk_size=500):\n    # Process each PyArrow chunk\n    pass\n\nfor items_chunk in items_relation.iter_fetch(chunk_size=500):\n    # Process each chunk of tuples\n    pass\n```\n\n----------------------------------------\n\nTITLE: Retrieving Complete Schema of DLT Source (Python)\nDESCRIPTION: Shows how to retrieve the complete schema of a DLT source by executing both extract and normalization steps on a small sample size. This process uses the 'add_limit' function to restrict the sample size.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/frequently-asked-questions.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initialize a new DLT pipeline with specified properties\np = dlt.pipeline(\n    pipeline_name=\"my_pipeline\",\n    destination=\"duckdb\",\n    dataset_name=\"my_dataset\"\n)\n\n# Extract data using the predefined source `my_source`\np.extract(my_source.add_limit(10))\n\n# Normalize the data structure for consistency\np.normalize()\n\n# Print the default schema of the pipeline in pretty YAML format for review\nprint(p.default_schema.to_pretty_yaml())\n```\n\n----------------------------------------\n\nTITLE: Defining Amazon Kinesis Stream Resource in Python\nDESCRIPTION: Python code defining the kinesis_stream resource for reading messages from an Amazon Kinesis stream. It includes parameters for stream configuration, credentials, incremental loading, and message parsing.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/amazon_kinesis.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(\n    name=lambda args: args[\"stream_name\"],\n    primary_key=\"_kinesis_msg_id\",\n    standalone=True,\n)\ndef kinesis_stream(\n    stream_name: str = dlt.config.value,\n    credentials: AwsCredentials = dlt.secrets.value,\n    last_msg: Optional[dlt.sources.incremental[StrStr]] = dlt.sources.incremental(\n        \"_kinesis\", last_value_func=max_sequence_by_shard\n    ),\n    initial_at_timestamp: TAnyDateTime = 0.0,\n    max_number_of_messages: int = None,\n    milliseconds_behind_latest: int = 1000,\n    parse_json: bool = True,\n    chunk_size: int = 1000,\n) -> Iterable[TDataItem]:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring an Airflow-integrated Log Progress Monitor in DLT Pipeline\nDESCRIPTION: Sets up a DLT pipeline with a custom log progress monitor that logs to the Airflow task logger every 60 seconds. This example demonstrates integration with Airflow's logging system.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/pipeline.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.operators.python import get_current_context  # noqa\n\n# log each minute to Airflow task logger\nti = get_current_context()[\"ti\"]\npipeline = dlt.pipeline(\n    pipeline_name=\"chess_pipeline\",\n    destination='duckdb',\n    dataset_name=\"chess_players_games_data\",\n    progress=dlt.progress.log(60, ti.log)\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Tasks Resource-Transformer for Asana\nDESCRIPTION: Python function that incrementally loads tasks for Asana projects, using a merge write disposition and tracking modifications with the incremental decorator.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/asana.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@dlt.transformer(data_from=projects, write_disposition=\"merge\", primary_key=\"gid\")\ndef tasks(\n    project_array: List[TDataItem],\n    access_token: str = dlt.secrets.value,\n    modified_at: dlt.sources.incremental[str] = dlt.sources.incremental(\n        \"modified_at\", initial_value=START_DATE_STRING\n    ),\n    fields: Iterable[str] = TASK_FIELDS,\n) -> Iterable[TDataItem]:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Creating and Running DLT Pipeline\nDESCRIPTION: Complete example of creating and running a DLT pipeline with filesystem source and CSV transformer.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/basic.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.filesystem import filesystem, read_csv\n\nfilesystem_pipe = filesystem(bucket_url=\"file://Users/admin/Documents/csv_files\", file_glob=\"*.csv\") | read_csv()\npipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\ninfo = pipeline.run(filesystem_pipe)\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Configuring Notion Source and Filesystem Destination using TOML\nDESCRIPTION: This snippet shows how to set up configurations and secrets for a Notion source and filesystem destination using TOML files.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n# we can set up a lot in config.toml\n# config.toml\n[runtime]\nlog_level=\"INFO\"\n\n[destination.filesystem]\nbucket_url = \"s3://[your_bucket_name]\"\n\n[normalize.data_writer]\ndisable_compression=true\n\n# but credentials should go to secrets.toml!\n# secrets.toml\n[source.notion]\napi_key = \"api_key\"\n\n[destination.filesystem.credentials]\naws_access_key_id = \"ABCDEFGHIJKLMNOPQRST\" # copy the access key here\naws_secret_access_key = \"1234567890_access_key\" # copy the secret access key here\n```\n\n----------------------------------------\n\nTITLE: Loading All Sheets and Range Names from Google Spreadsheet\nDESCRIPTION: Shows how to load both all sheets and all named ranges from a Google Spreadsheet.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nload_data = google_spreadsheet(\n        \"https://docs.google.com/spreadsheets/d/1HhWHjqouQnnCIZAFa2rL6vT91YRN8aIhts22SUUR580/edit#gid=0\", # Spreadsheet URL\n        get_sheets=True,\n        get_named_ranges=True,\n)\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Loading Multiple Shopify Resources with Date Filter\nDESCRIPTION: Demonstrates how to load multiple Shopify resources (products, orders, customers) with a specific start date filter. Shows the basic pattern for resource selection and data loading.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/shopify.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Add your desired resources to the list...\nresources = [\"products\", \"orders\", \"customers\"]\n\nload_data = shopify_source(start_date=\"2023-01-01\").with_resources(*resources)\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Filters in YAML Schema\nDESCRIPTION: Demonstrates how to define include and exclude filters for tables using regular expressions. This example shows filtering child tables and columns that start with 'parse_data' while including specific exceptions.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/technical/working_with_schemas.md#2025-04-14_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nevent_user:\n    columns: {}\n    write_disposition: append\n    filters:\n      excludes:\n      - re:^parse_data\n      includes:\n      - re:^parse_data__(intent|entities|message_id$|text$)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Google Analytics Metadata Resource in Python\nDESCRIPTION: Resource function that fetches all metrics and dimensions metadata from a Google Analytics project using the provided client and property ID.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_analytics.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(selected=False)\ndef get_metadata(client: Resource, property_id: int) -> Iterator[Metadata]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Loading Specific Resources from Chess.com API\nDESCRIPTION: Python code to run the Chess.com API pipeline, loading data only from specific resources (players_games and players_profiles) for the specified players and time period.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/chess.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ninfo = pipeline.run(source_instance.with_resources(\"players_games\", \"players_profiles\"))\n# print the information on data that was loaded\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource-level Schema Contract in DLT\nDESCRIPTION: Defines a resource with customized schema contract settings. This example will silently ignore new subtables, allow new columns to be added to existing tables, and raise an error if a data type variant is discovered.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema-contracts.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(schema_contract={\"tables\": \"discard_row\", \"columns\": \"evolve\", \"data_type\": \"freeze\"})\ndef items():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Loading All GitHub Repository Data in Python\nDESCRIPTION: Extracts all data from a specified GitHub repository, including issues, pull requests, comments, and reactions. This code snippet demonstrates how to load the complete dataset for a given repository.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/github.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nload_data = github_reactions(\"duckdb\", \"duckdb\")\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Loading Data in Chunks with Python\nDESCRIPTION: This snippet demonstrates how to load 1 million rows of data into a pipeline using 10,000 row chunks. It utilizes the 'run' method of a pipeline object and the 'iter_arrow' method of a relation object.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/dataset.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nother_pipeline.run(limited_items_relation.iter_arrow(chunk_size=10_000), table_name=\"limited_items\")\n```\n\n----------------------------------------\n\nTITLE: Configuring DLT Pipeline for Kinesis Data in Python\nDESCRIPTION: This snippet shows how to configure a DLT pipeline by specifying the pipeline name, destination, and dataset. It sets up the basic structure for processing Kinesis data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/amazon_kinesis.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"kinesis_pipeline\",  # Use a custom name if desired\n    destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"kinesis\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Additional Data Load Example - Python\nDESCRIPTION: Example showing how to load additional data into an existing pipeline with load package tracking.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination-tables.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata = [\n    {\n        'id': 3,\n        'name': 'Charlie',\n        'pets': []\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Defining Delta Destination in Python\nDESCRIPTION: Python code to define a Delta destination when creating a dlt pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/delta.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\"loads_delta\", destination=\"delta\")\n```\n\n----------------------------------------\n\nTITLE: Defining Players Archives Resource\nDESCRIPTION: Python function decorated with @dlt.resource that retrieves URLs to game archives for specified players. This resource is not selected by default in pipeline runs.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/chess.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(write_disposition=\"replace\", selected=False)\ndef players_archives(players: List[str]) -> Iterator[List[TDataItem]]:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Initializing dlt Pipeline with ClickHouse Destination and Staging\nDESCRIPTION: This snippet demonstrates how to create a dlt pipeline with ClickHouse as the destination and filesystem staging enabled. It sets up a pipeline for chess data with specific configuration parameters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/clickhouse.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n  pipeline_name='chess_pipeline',\n  destination='clickhouse',\n  staging='filesystem',  # add this to activate staging\n  dataset_name='chess_data'\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline with Direct SQLAlchemy Engine\nDESCRIPTION: Python code to create a dlt pipeline by directly passing an SQLAlchemy engine instance to the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/sqlalchemy.md#2025-04-14_snippet_4\n\nLANGUAGE: py\nCODE:\n```\nimport sqlalchemy as sa\nimport dlt\n\nengine = sa.create_engine('sqlite:///chess_data.db')\n\npipeline = dlt.pipeline(\n    pipeline_name='chess',\n    destination=dlt.destinations.sqlalchemy(engine),\n    dataset_name='main'\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Selected Google Ads Resources in Python\nDESCRIPTION: Example demonstrating how to load specific resources (customers and campaigns) from Google Ads using the configured pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_ads.md#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndata_selected = google_ads().with_resources(\"customers\", \"campaigns\")\ninfo = pipeline.run(data=[data_default])\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Using Regular Expressions for Column Hints in YAML\nDESCRIPTION: YAML configuration that demonstrates how to use regular expressions (SimpleRegex) to apply hints to columns matching certain patterns, specifically adding a partition hint to columns ending with '_timestamp'.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema.md#2025-04-14_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nsettings:\n    partition:\n      - re:_timestamp$\n```\n\n----------------------------------------\n\nTITLE: Custom SQL Database Source Configuration in TOML\nDESCRIPTION: This TOML configuration shows how to set up a custom SQL database source with a renamed configuration. It includes settings for credentials, schema, backend, chunk size, and incremental loading for a specific table.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/advanced.md#2025-04-14_snippet_12\n\nLANGUAGE: toml\nCODE:\n```\n[sources.my_db]\ncredentials=\"mssql+pyodbc://loader.database.windows.net/dlt_data?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server\"\nschema=\"data\"\nbackend=\"pandas\"\nchunk_size=1000\n\n[sources.my_db.chat_message.incremental]\ncursor_path=\"updated_at\"\n```\n\n----------------------------------------\n\nTITLE: Defining HubSpot Source in Python\nDESCRIPTION: Python function that defines the HubSpot source, including parameters for API key, history inclusion, and custom properties.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/hubspot.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source(name=\"hubspot\")\ndef hubspot(\n    api_key: str = dlt.secrets.value,\n    include_history: bool = False,\n    include_custom_props: bool = False,\n) -> Sequence[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Database Credentials with Connection String\nDESCRIPTION: This TOML configuration demonstrates an alternative method to set up SQL database credentials using a connection string.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/setup.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[sources.sql_database.credentials]\ncredentials=\"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\"\n```\n\n----------------------------------------\n\nTITLE: Configuring DLT Profiles in YAML\nDESCRIPTION: Example of defining multiple profiles in dlt.yml file with different environment configurations for sources, runtime, and destinations.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/core-concepts/profiles.md#2025-04-14_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprofiles:\n  # profiles allow you to configure different settings for different environments\n  dev:\n    sources:\n      my_arrow_source:\n        row_count: 100\n    runtime:\n      log_level: DEBUG\n  prod:\n    sources:\n      my_arrow_source:\n        row_count: 200\n    runtime:\n      log_level: INFO\n    destinations:\n      my_duckdb_destination:\n        credentials: my_data_prod.duckdb\n```\n\n----------------------------------------\n\nTITLE: Setting BigQuery Credentials via Environment Variables\nDESCRIPTION: This snippet shows how to set BigQuery credentials using environment variables. It includes the client email, private key, and project ID for BigQuery authentication.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n# BigQuery credentials\nexport DESTINATION__BIGQUERY__CREDENTIALS__CLIENT_EMAIL=\"<client_email>\"\nexport DESTINATION__BIGQUERY__CREDENTIALS__PRIVATE_KEY=\"<private_key>\"\nexport DESTINATION__BIGQUERY__CREDENTIALS__PROJECT_ID=\"<project_id>\"\n```\n\n----------------------------------------\n\nTITLE: Fetching PokÃ©mon Data with Explicit Pagination Parameters in Python\nDESCRIPTION: This example shows how to use the RESTClient with explicitly specified pagination parameters. It sets up a client for the PokÃ©API, defines a custom paginator, specifies a data selector, and creates a resource function to fetch and yield pages of PokÃ©mon data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/http/overview.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.helpers.rest_client import RESTClient\nfrom dlt.sources.helpers.rest_client.paginators import JSONLinkPaginator\n\ngithub_client = RESTClient(\n    base_url=\"https://pokeapi.co/api/v2\",\n    paginator=JSONLinkPaginator(next_url_path=\"next\"),   # (1)\n    data_selector=\"results\",                             # (2)\n)\n\n@dlt.resource\ndef get_pokemons():\n    for page in github_client.paginate(\n        \"/pokemon\",\n        params={\n            \"limit\": 100,                                    # (3)\n        },\n    ):\n        yield page\n\npipeline = dlt.pipeline(\n    pipeline_name=\"get_pokemons\",\n    destination=\"duckdb\",\n    dataset_name=\"github_data\",\n)\nload_info = pipeline.run(get_pokemons)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Analytics OAuth Credentials\nDESCRIPTION: TOML configuration for storing Google Analytics OAuth credentials in the secrets.toml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_analytics.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[sources.google_analytics.credentials]\nclient_id = \"client_id\" # please set me up!\nclient_secret = \"client_secret\" # please set me up!\nrefresh_token = \"refresh_token\" # please set me up!\nproject_id = \"project_id\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Loading Standalone Table from SQL Database with DLT\nDESCRIPTION: This snippet illustrates how to load a single table from a SQL database using the sql_table resource in a DLT pipeline. It defines the pipeline, fetches a specific table, and runs the pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/configuration.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.sql_database import sql_table\n\ndef load_select_tables_from_database() -> None:\n    # Define the pipeline\n    pipeline = dlt.pipeline(\n        pipeline_name=\"rfam\",\n        destination=\"duckdb\",\n        dataset_name=\"rfam_data\"\n    )\n\n    # Fetch the table \"family\"\n    table = sql_table(table=\"family\")\n\n    # Run the pipeline\n    info = pipeline.run(table)\n\n    # Print load info\n    print(info)\n```\n\n----------------------------------------\n\nTITLE: Pseudonymizing Data Before Load (Python)\nDESCRIPTION: Shows how to transform data by pseudonymizing personally identifiable information (PII) before loading it to the destination using a sql_database source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/usage.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nimport hashlib\nfrom dlt.sources.sql_database import sql_database\n\ndef pseudonymize_name(doc):\n    '''\n    Pseudonymization is a deterministic type of PII-obscuring.\n    Its role is to allow identifying users by their hash,\n    without revealing the underlying info.\n    '''\n    # add a constant salt to generate\n    salt = 'WI@N57%zZrmk#88c'\n    salted_string = doc['rfam_acc'] + salt\n    sh = hashlib.sha256()\n    sh.update(salted_string.encode())\n    hashed_string = sh.digest().hex()\n    doc['rfam_acc'] = hashed_string\n    return doc\n\npipeline = dlt.pipeline(\n    # Configure the pipeline\n)\n# using sql_database source to load family table and pseudonymize the column \"rfam_acc\"\nsource = sql_database().with_resources(\"family\")\n# modify this source instance's resource\nsource.family.add_map(pseudonymize_name)\n# Run the pipeline. For a large db this may take a while\ninfo = pipeline.run(source, write_disposition=\"replace\")\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: GitHub API Source Function (Python)\nDESCRIPTION: Defines the source function that handles GitHub API authentication.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/create-a-pipeline.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef github_api_source(api_secret_key: str = dlt.secrets.value):\n    return github_api_resource(api_secret_key=api_secret_key)\n```\n\n----------------------------------------\n\nTITLE: Incrementally Loading a Specific MongoDB Collection in Python\nDESCRIPTION: This snippet demonstrates how to incrementally load data from a specific MongoDB collection ('movies') using DLT. It configures incremental loading based on the 'lastupdated' field, starting from a specific date.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mongodb.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nload_data = mongodb_collection(\n    collection=\"movies\",\n    incremental=dlt.sources.incremental(\n        \"lastupdated\", initial_value=pendulum.DateTime(2020, 9, 10, 0, 0, 0)\n  ))\n\nload_info = pipeline.run(load_data, write_disposition=\"merge\")\n```\n\n----------------------------------------\n\nTITLE: Creating Pipeline with Staging Configuration\nDESCRIPTION: Python code showing how to create a DLT pipeline with filesystem staging and Redshift destination, including optional Parquet format specification.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/staging.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name='chess_pipeline',\n    destination='redshift',\n    staging='filesystem',\n    dataset_name='player_data'\n)\n\ninfo = pipeline.run(chess_source(), loader_file_format=\"parquet\")\n```\n\n----------------------------------------\n\nTITLE: Renaming Columns with Special Characters in Python using DLT\nDESCRIPTION: This code snippet demonstrates how to create a dummy source with special characters, define a function to replace umlauts, and apply it to modify the resource output. It uses the DLT library for data processing.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/customising-pipelines/renaming_columns.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\n# create a dummy source with umlauts (special characters) in key names (um)\n@dlt.source\ndef dummy_source(prefix: str = None):\n    @dlt.resource\n    def dummy_data():\n        for _ in range(100):\n            yield {f'Objekt_{_}': {'GrÃ¶ÃŸe': _, 'Ã„quivalenzprÃ¼fung': True}}\n    return dummy_data(),\n\ndef replace_umlauts_in_dict_keys(d):\n    \"\"\"\n    Replaces umlauts in dictionary keys with standard characters.\n    \"\"\"\n    umlaut_map =  {'Ã¤': 'ae', 'Ã¶': 'oe', 'Ã¼': 'ue', 'ÃŸ': 'ss', 'Ã„': 'Ae', 'Ã–': 'Oe', 'Ãœ': 'Ue'}\n    result = {}\n    for k, v in d.items():\n        new_key = ''.join(umlaut_map.get(c, c) for c in k)\n        if isinstance(v, dict):\n            result[new_key] = replace_umlauts_in_dict_keys(v)\n        else:\n            result[new_key] = v\n    return result\n\n# We can add the map function to the resource\n\n# 1. Create an instance of the source so you can edit it.\nsource_instance = dummy_source()\n\n# 2. Modify this source instance's resource\nsource_instance.dummy_data().add_map(replace_umlauts_in_dict_keys)\n\n# 3. Inspect your result\nfor row in source_instance:\n    print(row)\n\n# {'Objekt_0': {'Groesse': 0, 'Aequivalenzpruefung': True}}\n# ...\n```\n\n----------------------------------------\n\nTITLE: Loading Messages from Specific Slack Channels with DLT in Python\nDESCRIPTION: This code snippet shows how to load only messages from selected Slack channels using DLT. It demonstrates the use of the 'with_resources' method to specify which resources to load from the selected channels.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/slack.md#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# To load data from selected channels.\nselected_channels=[\"general\", \"random\"] # Enter the channel names here.\n\nsource = slack_source(\n    page_size=20,\n    selected_channels=selected_channels,\n    start_date=datetime.datetime(2023, 9, 1),\n    end_date=datetime.datetime(2023, 9, 8),\n)\n# It loads only messages from the channel \"general\".\nload_info = pipeline.run(source.with_resources(\"general\"))\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Setting Static Staging Dataset Name in Python\nDESCRIPTION: Demonstrates how to configure a fixed staging dataset name using the destination factory in Python. Creates a postgres destination with a static staging dataset name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/staging.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\ndest_ = dlt.destinations.postgres(staging_dataset_name_layout=\"_dlt_staging\")\n```\n\n----------------------------------------\n\nTITLE: Executing Basic SQL Query with DLT Pipeline\nDESCRIPTION: Demonstrates how to execute a SQL query using the DLT pipeline SQL client. Shows connection setup and fetching results using a parameterized query.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/sql-client.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(destination=\"bigquery\", dataset_name=\"crm\")\nwith pipeline.sql_client() as client:\n    with client.execute_query(\n        \"SELECT id, name, email FROM customers WHERE id = %s\",\n        10\n    ) as cursor:\n        # get all data from the cursor as a list of tuples\n        print(cursor.fetchall())\n```\n\n----------------------------------------\n\nTITLE: Passing Connection Credentials Directly in DLT Script\nDESCRIPTION: This code snippet shows how to explicitly pass connection credentials inside the source for a DLT pipeline. It uses ConnectionStringCredentials to provide the database connection string.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/configuration.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.credentials import ConnectionStringCredentials\nfrom dlt.sources.sql_database import sql_database\n\ncredentials = ConnectionStringCredentials(\n    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\"\n)\n\nsource = sql_database(credentials).with_resources(\"family\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Destinations in dlt.yml\nDESCRIPTION: Shows how to define a destination (in this case, DuckDB) in the dlt.yml manifest file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/projects.md#2025-04-14_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndestinations:\n    duckdb:\n        type: duckdb\n```\n\n----------------------------------------\n\nTITLE: Initializing DLT Pipeline Configuration for Personio\nDESCRIPTION: Configures a DLT pipeline with custom name, destination, and dataset settings for Personio data extraction. Specifies core pipeline parameters including the destination database and dataset naming.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/personio.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n   pipeline_name=\"personio\",  # Use a custom name if desired\n   destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n   dataset_name=\"personio_data\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Importing DLT Credential Types\nDESCRIPTION: Example of importing various credential types available in DLT.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.credentials import ConnectionStringCredentials\nfrom dlt.sources.credentials import OAuth2Credentials\nfrom dlt.sources.credentials import GcpServiceAccountCredentials, GcpOAuthCredentials\nfrom dlt.sources.credentials import AwsCredentials\nfrom dlt.sources.credentials import AzureCredentials\n```\n\n----------------------------------------\n\nTITLE: Incremental Loading Configuration\nDESCRIPTION: Example of configuring incremental data loading from Zendesk sources with custom start dates\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/zendesk.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n     pipeline_name=\"dlt_zendesk_pipeline\",  # Use a custom name if desired\n     destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n     dev_mode=False,\n     dataset_name=\"sample_zendesk_data\"  # Use a custom name if desired\n)\ndata = zendesk_support(load_all=True, start_date=START_DATE)\ndata_chat = zendesk_chat(start_date=START_DATE)\ndata_talk = zendesk_talk(start_date=START_DATE)\ninfo = pipeline.run(data=[data, data_chat, data_talk])\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Loading with GitHub API\nDESCRIPTION: Example demonstrating how to set up incremental loading from the GitHub API using dlt. This configuration uses the 'since' parameter with an incremental type and cursor path to fetch only new or updated data based on the 'updated_at' field.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/rest-api.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.rest_api import rest_api_source\n\npipeline = dlt.pipeline(\n    pipeline_name=\"rest_api_github\",\n    destination=\"duckdb\",\n    dataset_name=\"rest_api_data\",\n)\n\ngithub_source = rest_api_source({\n    \"client\": {\n        \"base_url\": \"https://api.github.com/repos/dlt-hub/dlt/\",\n    },\n    \"resource_defaults\": {\n        \"primary_key\": \"id\",\n        \"write_disposition\": \"merge\",\n        \"endpoint\": {\n            \"params\": {\n                \"per_page\": 100,\n            },\n        },\n    },\n    \"resources\": [\n        {\n            \"name\": \"issues\",\n            \"endpoint\": {\n                \"path\": \"issues\",\n                \"params\": {\n                    \"sort\": \"updated\",\n                    \"direction\": \"desc\",\n                    \"state\": \"open\",\n                    \"since\": {\n                        \"type\": \"incremental\",\n                        \"cursor_path\": \"updated_at\",\n                        \"initial_value\": \"2024-01-25T11:21:28Z\",\n                    },\n                },\n            },\n        },\n    ],\n})\n\nload_info = pipeline.run(github_source)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Sending Slack Notifications for Database Table Updates in Python\nDESCRIPTION: This code demonstrates how to send Slack notifications when database tables are updated using dlt's slack integration. It iterates through load_info packages, schema updates, and columns, sending detailed notifications about each change to a specified Slack webhook URL.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/running-in-production/alerting.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Import the send_slack_message function from the dlt library\nfrom dlt.common.runtime.slack import send_slack_message\n\n# Define the URL for your Slack webhook\nhook = \"https://hooks.slack.com/services/xxx/xxx/xxx\"\n\n# Iterate over each package in the load_info object\nfor package in load_info.load_packages:\n    # Iterate over each table in the schema_update of the current package\n    for table_name, table in package.schema_update.items():\n        # Iterate over each column in the current table\n        for column_name, column in table[\"columns\"].items():\n            # Send a message to the Slack channel with the table\n            # and column update information\n            send_slack_message(\n                hook,\n                message=(\n                    f\"\\tTable updated: {table_name}: \"\n                    f\"Column changed: {column_name}: \"\n                    f\"{column['data_type']}\"\n                )\n            )\n```\n\n----------------------------------------\n\nTITLE: Loading Slack Data from Selected Channels with Date Range in Python\nDESCRIPTION: This snippet demonstrates how to load data from selected Slack channels within a specified date range using DLT. It allows you to specify channel names, page size, and date parameters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/slack.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# To load data from selected channels.\nselected_channels=[\"general\", \"random\"] # Enter the channel names here.\n\nsource = slack_source(\n    page_size=20,\n    selected_channels=selected_channels,\n    start_date=datetime.datetime(2023, 9, 1),\n    end_date=datetime.datetime(2023, 9, 8),\n)\n# It loads data starting from 1st September 2023 to 8th September 2023 from the channels: \"general\" and \"random\".\nload_info = pipeline.run(source)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Configuring Development Mode in DLT Pipeline\nDESCRIPTION: Demonstrates how to enable development mode in a DLT pipeline, which creates versioned datasets with datetime-based suffixes for each pipeline run. The example shows loading data into a users table.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination-tables.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\ndata = [\n    {'id': 1, 'name': 'Alice'},\n    {'id': 2, 'name': 'Bob'}\n]\n\npipeline = dlt.pipeline(\n    pipeline_name='quick_start',\n    destination='duckdb',\n    dataset_name='mydata',\n    dev_mode=True # <-- add this line\n)\nload_info = pipeline.run(data, table_name=\"users\")\n```\n\n----------------------------------------\n\nTITLE: Nested Data Pipeline Example - Python\nDESCRIPTION: Example demonstrating how DLT handles nested data structures by creating multiple related tables.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination-tables.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\ndata = [\n    {\n        'id': 1,\n        'name': 'Alice',\n        'pets': [\n            {'id': 1, 'name': 'Fluffy', 'type': 'cat'},\n            {'id': 2, 'name': 'Spot', 'type': 'dog'}\n        ]\n    },\n    {\n        'id': 2,\n        'name': 'Bob',\n        'pets': [\n            {'id': 3, 'name': 'Fido', 'type': 'dog'}\n        ]\n    }\n]\n\npipeline = dlt.pipeline(\n    pipeline_name='quick_start',\n    destination='duckdb',\n    dataset_name='mydata'\n)\nload_info = pipeline.run(data, table_name=\"users\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Key Pair Authentication for Snowflake\nDESCRIPTION: TOML configuration for using key pair authentication with Snowflake, which requires a base64-encoded DER format private key and optional passphrase.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[destination.snowflake.credentials]\ndatabase = \"dlt_data\"\nusername = \"loader\"\nhost = \"kgiotue-wn98412\"\nprivate_key = \"LS0tLS1CRUdJTiBFTkNSWVBURUQgUFJJ....Qo=\"\nprivate_key_passphrase=\"passphrase\"\n```\n\n----------------------------------------\n\nTITLE: Initializing a dlt Pipeline Project\nDESCRIPTION: Creates a new pipeline project using Pipedrive as the source and BigQuery as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add-a-verified-source.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndlt init pipedrive bigquery\n```\n\n----------------------------------------\n\nTITLE: Defining dlt Pipeline in Python\nDESCRIPTION: Python code defining a dlt pipeline for fetching GitHub issues, including a resource and source definition.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-dagster.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n...\n@dlt.resource(\n    table_name=\"issues\",\n    write_disposition=\"merge\",\n    primary_key=\"id\",\n)\ndef get_issues(\n        updated_at=dlt.sources.incremental(\"updated_at\", initial_value=\"1970-01-01T00:00:00Z\")\n):\n    url = (\n        f\"{BASE_URL}?since={updated_at.last_value}&per_page=100&sort=updated\"\n        \"&direction=desc&state=open\"\n    )\n    yield pagination(url)\n\n@dlt.source\ndef github_source():\n    return get_issues()\n```\n\n----------------------------------------\n\nTITLE: Subsequent Incremental Loading with Change Tracking\nDESCRIPTION: Python code for running incremental loads after the initial setup, using the stored tracking version.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/ms-sql.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt_plus.sources.mssql import create_change_tracking_table\n\nincremental_resource = create_change_tracking_table(\n    credentials=engine,\n    table=table_name,\n    schema=schema_name,\n)\npipeline.run(incremental_resource)\n```\n\n----------------------------------------\n\nTITLE: Email Messages Transformer\nDESCRIPTION: Transformer resource for retrieving email content and metadata using message UIDs\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/inbox.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@dlt.transformer(name=\"messages\", primary_key=\"message_uid\")\ndef get_messages(\n    items: TDataItems,\n    include_body: bool = True,\n) -> TDataItem:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Quality Contract in YAML for dlt+\nDESCRIPTION: Demonstrates how to define a quality contract for a customers table with specific column constraints. The example shows validation rules for a 'category' column including data type enforcement, nullability checks, and value range validation.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/quality/data-quality.md#2025-04-14_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nengine_version: 10\nname: scd_type_3\ntables:\n  customers:\n    columns:\n      category:\n        data_type: bigint\n        nullable: false\n        quality_contracts:\n          expect_column_max_to_be_between:\n            min_value: 1\n            max_value: 100\n```\n\n----------------------------------------\n\nTITLE: Loading All Integrated Notion Databases\nDESCRIPTION: Python code to load data from all integrated Notion databases using the notion_databases source and run the pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/notion.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nload_data = notion_databases()\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Loading SQL Tables with Merge Write Disposition in Python using dlt\nDESCRIPTION: This snippet demonstrates how to load data from SQL tables using dlt with merge write disposition. It specifies different primary keys for each table using the apply_hints method.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/sql-database.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.sql_database import sql_database\n\ndef load_tables_family_and_genome():\n\n    source = sql_database().with_resources(\"family\", \"genome\")\n\n    # specify different loading strategy for each resource using apply_hints\n    source.family.apply_hints(write_disposition=\"merge\", primary_key=\"rfam_id\") # merge table \"family\" on column \"rfam_id\"\n    source.genome.apply_hints(write_disposition=\"merge\", primary_key=\"upid\") # merge table \"genome\" on column \"upid\"\n\n    pipeline = dlt.pipeline(\n        pipeline_name=\"sql_to_duckdb_pipeline\",\n        destination=\"duckdb\",\n        dataset_name=\"sql_to_duckdb_pipeline_data\"\n    )\n\n    load_info = pipeline.run(source)\n\n    print(load_info)\n\nif __name__ == '__main__':\n    load_tables_family_and_genome()\n```\n\n----------------------------------------\n\nTITLE: Refreshing DLT Pipeline Data with Drop Sources Mode\nDESCRIPTION: This example shows how to use the 'refresh' argument with 'drop_sources' mode when running a dlt pipeline. This mode drops all tables and resets the pipeline state for the specified source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/pipeline.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\npipeline = dlt.pipeline(\"airtable_demo\", destination=\"duckdb\")\npipeline.run(airtable_emojis(), refresh=\"drop_sources\")\n```\n\n----------------------------------------\n\nTITLE: Loading Custom and Additional Properties\nDESCRIPTION: Loads all custom properties along with specific additional HubSpot system properties for contacts.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/hubspot.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nload_data = hubspot()\nload_data.contacts.bind(props=[\"hs_content_membership_email\", \"hs_content_membership_email_confirmed\"])\nload_info = pipeline.run(load_data.with_resources(\"contacts\"))\n```\n\n----------------------------------------\n\nTITLE: Loading Specific Workable Endpoints\nDESCRIPTION: Demonstrates loading data from specific endpoints (candidates and members) using resource filtering. The candidates endpoint uses incremental merge mode while members uses replace mode.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/workable.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nload_info = pipeline.run(load_data.with_resources(\"candidates\", \"members\"))\n# print the information on data that was loaded\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Stripe Pipeline\nDESCRIPTION: Example of creating a custom pipeline configuration with specific destination and dataset name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/stripe.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"stripe_pipeline\",  # Use a custom name if desired\n    destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"stripe_dataset\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Pipeline Configuration Example\nDESCRIPTION: Example of configuring a custom pipeline with name, destination, and dataset specifications.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/airtable.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n     pipeline_name=\"airtable\",  # Use a custom name if desired\n     destination=\"duckdb\",      # Choose the appropriate destination (e.g., duckdb, redshift, post)\n     dataset_name=\"airtable_data\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Select Tables from SQL Database with DLT\nDESCRIPTION: This code shows how to load specific tables from a SQL database using the sql_database source in a DLT pipeline. It demonstrates two methods: using table_names parameter and using with_resources method.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/configuration.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.sql_database import sql_database\n\ndef load_select_tables_from_database() -> None:\n    # Define the pipeline\n    pipeline = dlt.pipeline(\n        pipeline_name=\"rfam\",\n        destination=\"postgres\",\n        dataset_name=\"rfam_data\"\n    )\n\n    # Fetch tables \"family\" and \"clan\"\n    source = sql_database(table_names=['family', 'clan'])\n    # or\n    # source = sql_database().with_resources(\"family\", \"clan\")\n\n    # Run the pipeline\n    info = pipeline.run(source)\n\n    # Print load info\n    print(info)\n```\n\n----------------------------------------\n\nTITLE: Python Synapse Pipeline Configuration\nDESCRIPTION: Python code examples for configuring Synapse pipelines with different authentication methods\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/synapse.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name='chess',\n    destination=dlt.destinations.synapse(\n        credentials='synapse://loader:your_loader_password@your_synapse_workspace_name.azuresynapse.net/yourpool'\n    ),\n    dataset_name='chess_data'\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nconn_str = (\n    f\"DRIVER={{ODBC Driver 18 for SQL Server}};\"\n    f\"SERVER={SERVER_NAME};\"\n    f\"DATABASE={DATABASE_NAME};\"\n    f\"UID={SERVICE_PRINCIPAL_ID}@{TENANT_ID};\"\n    f\"PWD={SERVICE_PRINCIPAL_SECRETS};\"\n    f\"Authentication=ActiveDirectoryServicePrincipal\"\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom sqlalchemy.engine import URL\n\nconnection_url = URL.create(\n    \"mssql+pyodbc\",\n    query={\"odbc_connect\": conn_str}\n)\n```\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name='chess',\n    destination=dlt.destinations.synapse(\n        credentials=connection_url.render_as_string(hide_password=False)\n    ),\n    dataset_name='chess_data'\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.destinations.adapters import synapse_adapter\n\ninfo = pipeline.run(\n    synapse_adapter(\n        data=my_resource,\n        table_index_type=\"clustered_columnstore_index\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Dynamic Filesystem Configuration in Python\nDESCRIPTION: Python code for dynamically configuring the filesystem destination with custom layout and placeholders at pipeline initialization time.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimport pendulum\n\nimport dlt\nfrom dlt.destinations import filesystem\n\npipeline = dlt.pipeline(\n    pipeline_name=\"data_things\",\n    destination=filesystem(\n        layout=\"{table_name}/{test_placeholder}/{timestamp}/{load_id}.{file_id}.{ext}\",\n        current_datetime=pendulum.now(),\n        extra_placeholders={\n            \"test_placeholder\": \"test_value\",\n        }\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Delta Tables with Helper Functions\nDESCRIPTION: Python code demonstrating how to use the get_delta_tables helper function to access and manipulate Delta tables.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/delta.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.common.libs.deltalake import get_delta_tables\n\n...\n\n# get dictionary of DeltaTable objects\ndelta_tables = get_delta_tables(pipeline)\n\n# execute operations on DeltaTable objects\ndelta_tables[\"my_delta_table\"].optimize.compact()\ndelta_tables[\"another_delta_table\"].optimize.z_order([\"col_a\", \"col_b\"])\n# delta_tables[\"my_delta_table\"].vacuum()\n# etc.\n```\n\n----------------------------------------\n\nTITLE: Loading Data Incrementally\nDESCRIPTION: Example of incremental data loading from Stripe Invoice endpoint with date range specification.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/stripe.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsource_incremental = incremental_stripe_source(\n    endpoints=(\"Invoice\", ),\n    initial_start_date=pendulum.DateTime(2022, 1, 1),\n    end_date=pendulum.DateTime(2022, 12, 31),\n)\nload_info = pipeline.run(source_incremental)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Creating dlt Pipeline with Snowflake and S3 Staging\nDESCRIPTION: This Python code creates a dlt pipeline that loads chess player data to Snowflake using S3 as the staging destination. It demonstrates how to activate the staging location.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_15\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name='chess_pipeline',\n    destination='snowflake',\n    staging='filesystem', # add this to activate the staging location\n    dataset_name='player_data'\n)\n```\n\n----------------------------------------\n\nTITLE: Catching and Handling DataValidationError in Python\nDESCRIPTION: This code snippet shows how to catch and handle a DataValidationError exception when running a dlt pipeline with a schema contract in freeze mode.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema-contracts.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntry:\n  pipeline.run()\nexcept PipelineStepFailed as pip_ex:\n  if pip_ex.step == \"normalize\":\n    if isinstance(pip_ex.__context__.__context__, DataValidationError):\n      ...\n  if pip_ex.step == \"extract\":\n    if isinstance(pip_ex.__context__, DataValidationError):\n      ...\n```\n\n----------------------------------------\n\nTITLE: Direct Load Example with Databricks Notebook\nDESCRIPTION: Example showing how to load data directly from a Databricks Notebook using DLT with Pokemon API as data source. Demonstrates configuration of catalog and volume name with notebook context.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/databricks.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.destinations import databricks\nfrom dlt.sources.rest_api import rest_api_source\n\n# Fully qualified Databricks managed volume (recommended for production)\n# - dlt assumes the named volume already exists\nstaging_volume_name = \"dlt_ci.dlt_tests_shared.static_volume\"\n\nbricks = databricks(credentials={\"catalog\": \"dlt_ci\"}, staging_volume_name=staging_volume_name)\n\npokemon_source = rest_api_source(\n    {\n        \"client\": {\"base_url\": \"https://pokeapi.co/api/v2/\"},\n        \"resource_defaults\": {\"endpoint\": {\"params\": {\"limit\": 1000}}},\n        \"resources\": [\"pokemon\"],\n    }\n)\n\npipeline = dlt.pipeline(\n    pipeline_name=\"rest_api_example\",\n    dataset_name=\"rest_api_data\",\n    destination=bricks,\n)\n\nload_info = pipeline.run(pokemon_source)\nprint(load_info)\nprint(pipeline.dataset().pokemon.df())\n```\n\n----------------------------------------\n\nTITLE: Loading Limited GitHub Issues Data in Python\nDESCRIPTION: Extracts a limited number of issues (in this case, 100) from a specified GitHub repository. This snippet shows how to restrict the data extraction to a specific number of items and resource type.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/github.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nload_data = github_reactions(\"duckdb\", \"duckdb\", max_items=100)\nload_info = pipeline.run(load_data.with_resources(\"issues\"))\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Configuring GitHub Data Pipeline in Python\nDESCRIPTION: Sets up a DLT pipeline for GitHub data, specifying the pipeline name, destination (e.g., DuckDB), and dataset name. This configuration is the first step in creating a custom GitHub data pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/github.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"github_pipeline\",  # Use a custom name if desired\n    destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"github_reaction_data\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Running DLT Pipeline\nDESCRIPTION: Sets up and executes a DLT pipeline that loads the transformed data into BigQuery, specifying pipeline name, destination, and dataset details.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/customising-pipelines/removing_columns.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Integrating with a dlt pipeline\npipeline = dlt.pipeline(\n    pipeline_name='example',\n    destination='bigquery',\n    dataset_name='filtered_data'\n)\n# Run the pipeline with the transformed source\nload_info = pipeline.run(data_source)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Configuring dlt+ Project in YAML\nDESCRIPTION: YAML configuration for a dlt+ project that sets up a Pokemon API data pipeline with local storage.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/ai.md#2025-04-14_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  pokemon_api:\n    type: dlt.sources.rest_api.rest_api\n    client:\n      base_url: https://pokeapi.co/api/v2\n    resource_defaults:\n      endpoint:\n        params:\n          limit: 1000\n    resources:\n      - pokemon\n      - berry\n      - location\n\ndestinations:\n  pokemon_local:\n    type: filesystem\n    bucket_url: pokemon_data\n\npipelines:\n  pokemon:\n    source: pokemon_api\n    destination: pokemon_local\n    dataset_name: pokemon_dataset\n\ndatasets:\n  pokemon_dataset:\n    destination:\n      - pokemon_local\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Write Disposition in a Resource\nDESCRIPTION: Example of configuring the merge write disposition, primary key, and merge key in a dlt resource definition.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/lancedb.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(\n  primary_key=[\"doc_id\", \"chunk_id\"],\n  merge_key=[\"doc_id\"],\n  write_disposition={\"disposition\": \"merge\", \"strategy\": \"upsert\"},\n)\ndef my_rag_docs(\n  data: List[DictStrAny],\n) -> Generator[List[DictStrAny], None, None]:\n    yield data\n```\n\n----------------------------------------\n\nTITLE: Loading MongoDB Collection Using Apache Arrow in DLT Pipeline (Python)\nDESCRIPTION: This snippet shows how to load a selected MongoDB collection using Apache Arrow for data conversion in a DLT pipeline. It loads the 'movies' collection with the 'data_item_format' set to 'arrow'.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mongodb.md#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Load collection \"movies\", using Apache Arrow for conversion\nmovies = mongodb_collection(\n   collection=\"movies\",\n   data_item_format=\"arrow\",\n)\n\n# Run the pipeline\ninfo = pipeline.run(movies)\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Writing Custom SQL Queries with query_adapter_callback (Python)\nDESCRIPTION: Shows how to fully rewrite automatically generated queries using an extended version of query_adapter_callback in a sql_database source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/usage.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport sqlalchemy as sa\n\ndef query_adapter_callback(\n      query, table, incremental=None, engine=None\n  ) -> TextClause:\n\n      if incremental and incremental.start_value is not None:\n          t_query = sa.text(\n              f\"SELECT *, 1 as add_int, 'const' as add_text FROM {table.fullname} WHERE\"\n              f\" {incremental.cursor_path} > :start_value\"\n          ).bindparams(**{\"start_value\": incremental.start_value})\n      else:\n          t_query = sa.text(f\"SELECT *, 1 as add_int, 'const' as add_text FROM {table.fullname}\")\n\n      return t_query\n```\n\n----------------------------------------\n\nTITLE: Configuring Shared Google Credentials in TOML\nDESCRIPTION: This snippet demonstrates how to set up shared Google credentials for both source and destination in a TOML file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[credentials]\nclient_email = \"<client_email_both_for_destination_and_source>\"\nprivate_key = \"<private_key_both_for_destination_and_source>\"\nproject_id = \"<project_id_both_for_destination_and_source>\"\n```\n\n----------------------------------------\n\nTITLE: Loading Complete Workable Dataset\nDESCRIPTION: Executes a full data load from all Workable endpoints. The candidates endpoint uses incremental loading with merge mode, while other endpoints use replace mode.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/workable.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nload_data = workable_source()\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Loading by File Modification Date\nDESCRIPTION: Example of incremental loading based on file modification dates.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/basic.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.filesystem import filesystem, read_csv\n\n# This configuration will only consider new CSV files\nnew_files = filesystem(bucket_url=\"s3://bucket_name\", file_glob=\"directory/*.csv\")\n# Add incremental on modification time\nnew_files.apply_hints(incremental=dlt.sources.incremental(\"modification_date\"))\n\npipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\nload_info = pipeline.run((new_files | read_csv()).with_name(\"csv_files\"))\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Load Selected Tables Example\nDESCRIPTION: Code example demonstrating how to load specific tables from an Airtable base.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/airtable.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nbase_id = \"Please set me up!\"     # The ID of the base.\ntable_names = [\"Table1\", \"Table2\"] # A list of table IDs or table names to load.\n\nairtables = airtable_source(\n   base_id = base_id,\n   table_names = table_names\n)\nload_info = pipeline.run(airtables, write_disposition = \"replace\")\n```\n\n----------------------------------------\n\nTITLE: Creating Excel File Transformer in dlt\nDESCRIPTION: Custom standalone transformer that reads data from Excel files. This example sets up a pipeline that fetches an Excel file from S3 storage and processes it using a custom transformer that extracts data from a specific sheet.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/advanced.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Iterator\n\nimport dlt\nfrom dlt.common.storages.fsspec_filesystem import FileItemDict\nfrom dlt.common.typing import TDataItems\nfrom dlt.sources.filesystem import filesystem\n\nBUCKET_URL = \"s3://my_bucket/data\"\n\n# Define a standalone transformer to read data from an Excel file.\n@dlt.transformer(standalone=True)\ndef read_excel(\n    items: Iterator[FileItemDict], sheet_name: str\n) -> Iterator[TDataItems]:\n    # Import the required pandas library.\n    import pandas as pd\n\n    # Iterate through each file item.\n    for file_obj in items:\n        # Open the file object.\n        with file_obj.open() as file:\n            # Read from the Excel file and yield its content as dictionary records.\n            yield pd.read_excel(file, sheet_name).to_dict(orient=\"records\")\n\n# Set up the pipeline to fetch a specific Excel file from a filesystem (bucket).\nexample_xls = filesystem(\n    bucket_url=BUCKET_URL, file_glob=\"../directory/example.xlsx\"\n) | read_excel(\"example_table\")   # Pass the data through the transformer to read the \"example_table\" sheet.\n\npipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\", dataset_name=\"example_xls_data\")\n# Execute the pipeline and load the extracted data into the \"duckdb\" destination.\nload_info = pipeline.run(example_xls.with_name(\"example_xls_data\"))\n# Print the loading information.\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Using BigQuery Adapter for Resource Configuration\nDESCRIPTION: Example demonstrating how to use bigquery_adapter to apply column and table level hints for BigQuery-specific features.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/bigquery.md#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.destinations.adapters import bigquery_adapter\n\n\n@dlt.resource(\n    columns=[\n        {\"name\": \"event_date\", \"data_type\": \"date\"},\n        {\"name\": \"user_id\", \"data_type\": \"bigint\"},\n        # Other columns.\n    ]\n)\ndef event_data():\n    yield from [\n        {\"event_date\": datetime.date.today() + datetime.timedelta(days=i)} for i in range(100)\n    ]\n\n\n# Apply column options.\nbigquery_adapter(\n    event_data, partition=\"event_date\", cluster=[\"event_date\", \"user_id\"]\n)\n\n# Apply table level options.\nbigquery_adapter(event_data, table_description=\"Dummy event data.\")\n\n# Load data in \"streaming insert\" mode (only available with\n# write_disposition=\"append\").\nbigquery_adapter(event_data, insert_api=\"streaming\")\n```\n\n----------------------------------------\n\nTITLE: Loading Workable Data from Specific Date\nDESCRIPTION: Loads data from a specified start date with optional dependent endpoints. Uses pendulum for date handling and includes a load_details parameter for loading related data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/workable.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nload_data = workable_source(start_date=pendulum.DateTime(2022, 1, 1), load_details=True)\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Initializing dlt Pipeline for Postgres Replication (Shell)\nDESCRIPTION: Shell commands to initialize a dlt pipeline for Postgres replication with DuckDB as the destination. It also shows how to initialize the sql_database source if needed for initial load.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/pg_replication.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ndlt init pg_replication duckdb\n```\n\nLANGUAGE: sh\nCODE:\n```\ndlt init sql_database duckdb\n```\n\n----------------------------------------\n\nTITLE: Shopify Source Implementation\nDESCRIPTION: Python implementation of the main Shopify data source that provides access to products, orders, and customers data\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/shopify.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source()\ndef shopify_source(\n    private_app_password: str = dlt.secrets.value,\n    api_version: str = API_VERSION,\n    shop_url: str = dlt.config.value,\n    start_date: TAnyDateTime = FIRST_DAY_OF_MILLENNIUM,\n    end_date: Optional[TAnyDateTime] = None,\n    created_at_min: TAnyDateTime = FIRST_DAY_OF_MILLENNIUM,\n    items_per_page: int = ITEMS_PER_PAGE,\n    order_status: TOrderStatus = \"any\",\n) -> Iterable[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Basic Custom Destination Pipeline Implementation\nDESCRIPTION: Simple example of creating a custom destination function with DLT decorator and running a pipeline with basic data items.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/destination.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.common.typing import TDataItems\nfrom dlt.common.schema import TTableSchema\n\n@dlt.destination(batch_size=10)\ndef my_destination(items: TDataItems, table: TTableSchema) -> None:\n    print(table[\"name\"])\n    print(items)\n\npipeline = dlt.pipeline(\"custom_destination_pipeline\", destination=my_destination)\npipeline.run([1, 2, 3], table_name=\"items\")\n```\n\n----------------------------------------\n\nTITLE: Azure Source Configuration with DLT\nDESCRIPTION: Implementation of a DLT source function using Azure credentials with SAS token generation and credential format conversion.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef azure_readers(\n    bucket_url: str = dlt.config.value,\n    credentials: AzureCredentials = dlt.secrets.value,\n):\n    ...\n    # Generate a SAS token\n    credentials.create_sas_token()\n    print(credentials.azure_storage_sas_token)\n\n    # Convert credentials to adlfs format\n    adlfs_credentials = credentials.to_adlfs_credentials()\n    print(adlfs_credentials[\"account_name\"])\n\n    # to_native_credentials() is not yet implemented\n    ...\n```\n\n----------------------------------------\n\nTITLE: Load Phase Implementation in dlt\nDESCRIPTION: Shows how to execute the load phase independently, which handles schema migrations and loads normalized data into the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/explainers/how-dlt-works.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npipeline.load()\n```\n\n----------------------------------------\n\nTITLE: Expanding Shorthand Schema Contract in Python\nDESCRIPTION: This example shows how a shorthand schema contract is expanded to its full form, applying the 'freeze' mode to all schema entities.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema-contracts.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n{\"tables\": \"freeze\", \"columns\": \"freeze\", \"data_type\": \"freeze\"}\n```\n\n----------------------------------------\n\nTITLE: Example Test Implementation\nDESCRIPTION: Sample test code demonstrating how to write tests using dlt project API, including pipeline testing and data verification.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/quality/tests.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt_plus.project import Project\nfrom dlt_plus.project.entity_factory import EntityFactory\nfrom dlt_plus.project.pipeline_manager import PipelineManager\nfrom dlt_plus_tests.fixtures import auto_test_access_profile as auto_test_access_profile\nfrom dlt_plus_tests.utils import assert_load_info, load_table_counts\n\ndef test_events_to_data_lake(dpt_project_config: Project) -> None:\n    \"\"\"Make sure we dispatch the events to tables properly\"\"\"\n    factory = EntityFactory(dpt_project_config)\n    github_events = factory.get_source(\"events\")\n    events_to_lake = factory.get_pipeline(\"events_to_lake\")\n    info = events_to_lake.run(github_events())\n    assert_load_info(info)\n\n    # Did I load my test data?\n    assert load_table_counts(\n        events_to_lake, *events_to_lake.default_schema.data_table_names()\n    ) == {\n        \"issues_event\": 604,\n    }\n\ndef test_t_layer(dpt_project_config: Project) -> None:\n    \"\"\"Make sure that our generated dbt package creates expected reports in the data warehouse\"\"\"\n    pipeline_manager = PipelineManager(dpt_project_config)\n    info = pipeline_manager.run_pipeline(\"events_to_lake\")\n    assert_load_info(info)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom JQL Queries\nDESCRIPTION: Python code example showing how to use custom JQL queries with the Jira search source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/jira.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Define the JQL queries as follows\nqueries = [\n          \"created >= -30d order by created DESC\",\n          'created >= -30d AND project = DEV AND issuetype = Epic AND status = \"In Progress\" order by created DESC',\n          ]\n# Run the pipeline\nload_info = pipeline.run(jira_search().issues(jql_queries=queries))\n# Print Load information\nprint(f\"Load Information: {load_info}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Postgres Credentials in dlt\nDESCRIPTION: Example TOML configuration for Postgres credentials in the dlt secrets file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/postgres.md#2025-04-14_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[destination.postgres.credentials]\n\ndatabase = \"dlt_data\"\nusername = \"loader\"\npassword = \"<password>\" # replace with your password\nhost = \"localhost\" # or the IP address location of your database\nport = 5432\nconnect_timeout = 15\n```\n\n----------------------------------------\n\nTITLE: Copying Files Locally in dlt\nDESCRIPTION: Example demonstrating how to copy files from cloud storage to a local folder. This involves adding a mapping step to the filesystem resource that downloads each file before loading the file listing to the database.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/advanced.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport dlt\nfrom dlt.common.storages.fsspec_filesystem import FileItemDict\nfrom dlt.sources.filesystem import filesystem\n\ndef _copy(item: FileItemDict) -> FileItemDict:\n    # Instantiate fsspec and copy file\n    dest_file = os.path.join(\"./local_folder\", item[\"file_name\"])\n    # Create destination folder\n    os.makedirs(os.path.dirname(dest_file), exist_ok=True)\n    # Download file\n    item.fsspec.download(item[\"file_url\"], dest_file)\n    # Return file item unchanged\n    return item\n\nBUCKET_URL = \"gs://ci-test-bucket/\"\n\n# Use recursive glob pattern and add file copy step\ndownloader = filesystem(BUCKET_URL, file_glob=\"**\").add_map(_copy)\n\n# NOTE: You do not need to load any data to execute extract; below, we obtain\n# a list of files in a bucket and also copy them locally\nlisting = list(downloader)\nprint(listing)\n# Download to table \"listing\"\npipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\nload_info = pipeline.run(\n    downloader.with_name(\"listing\"), write_disposition=\"replace\"\n)\n# Pretty print the information on data that was loaded\nprint(load_info)\nprint(listing)\nprint(pipeline.last_trace.last_normalize_info)\n```\n\n----------------------------------------\n\nTITLE: Using Postgres Adapter for Geometric Data in dlt\nDESCRIPTION: Python code example demonstrating the use of the postgres_adapter for handling geometric data types in dlt resources.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/postgres.md#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.destinations.impl.postgres.postgres_adapter import postgres_adapter\n\n# Sample data with various geometry types\ndata_wkt = [\n  {\"type\": \"Point_wkt\", \"geom\": \"POINT (1 1)\"},\n  {\"type\": \"Point_wkt\", \"geom\": \"Polygon([(0, 0), (1, 0), (1, 1), (0, 1), (0, 0)])\"},\n  ]\n\ndata_wkb_hex = [\n  {\"type\": \"Point_wkb_hex\", \"geom\": \"0101000000000000000000F03F000000000000F03F\"},\n  {\"type\": \"Point_wkb_hex\", \"geom\": \"01020000000300000000000000000000000000000000000000000000000000F03F000000000000F03F00000000000000400000000000000040\"},\n]\n\n\n\n# Apply postgres_adapter to the 'geom' column with default SRID 4326\nresource_wkt = postgres_adapter(data_wkt, geometry=\"geom\")\nresource_wkb_hex = postgres_adapter(data_wkb_hex, geometry=\"geom\")\n\n# If you need a different SRID\nresource_wkt = postgres_adapter(data_wkt, geometry=\"geom\", srid=3242)\n```\n\n----------------------------------------\n\nTITLE: Type Hints in Google Sheets Source\nDESCRIPTION: Example showing how to use type hints in source definitions to enable automatic type parsing and credential handling.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/advanced.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef google_sheets(\n    spreadsheet_id: str = dlt.config.value,\n    tab_names: List[str] = dlt.config.value,\n    credentials: GcpServiceAccountCredentials = dlt.secrets.value,\n    only_strings: bool = False\n):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Mixed Type Schema Data Example in Python\nDESCRIPTION: Example showing how schema handles mixed data types, demonstrating automatic variant column creation when introducing string IDs alongside existing integer IDs.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata = [\n  {\"id\": 1, \"human_name\": \"Alice\"},\n  {\"id\": \"idx-nr-456\", \"human_name\": \"Bob\"}\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring ConnectorX Backend for SQL Database\nDESCRIPTION: Implementation example of ConnectorX backend with PostgreSQL, showing performance optimization settings and custom connection configuration\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/configuration.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"This example is taken from the benchmarking tests for ConnectorX performed on the UNSW_Flow dataset (~2mln rows, 25+ columns). Full code here: https://github.com/dlt-hub/sql_database_benchmarking\"\"\"\nimport os\nimport dlt\nfrom dlt.destinations import filesystem\nfrom dlt.sources.sql_database import sql_table\n\nunsw_table = sql_table(\n    \"postgresql://loader:loader@localhost:5432/dlt_data\",\n    \"unsw_flow_7\",\n    \"speed_test\",\n    # this is ignored by connectorx\n    chunk_size=100000,\n    backend=\"connectorx\",\n    # keep source data types\n    reflection_level=\"full_with_precision\",\n    # just to demonstrate how to set up a separate connection string for connectorx\n    backend_kwargs={\"conn\": \"postgresql://loader:loader@localhost:5432/dlt_data\"}\n)\n\npipeline = dlt.pipeline(\n    pipeline_name=\"unsw_download\",\n    destination=filesystem(os.path.abspath(\"../_storage/unsw\")),\n    progress=\"log\",\n    dev_mode=True,\n)\n\ninfo = pipeline.run(\n    unsw_table,\n    dataset_name=\"speed_test\",\n    table_name=\"unsw_flow\",\n    loader_file_format=\"parquet\",\n)\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Loading All Sheets from Google Spreadsheet\nDESCRIPTION: Illustrates how to load all sheets from a Google Spreadsheet by enabling sheet retrieval.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nload_data = google_spreadsheet(\n        \"https://docs.google.com/spreadsheets/d/1HhWHjqouQnnCIZAFa2rL6vT91YRN8aIhts22SUUR580/edit#gid=0\", # Spreadsheet URL\n        get_sheets=True,\n        get_named_ranges=False,\n)\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Implementing Tracked Data Resource\nDESCRIPTION: Python resource function that generates sample user tracking data including user IDs, device names, and page referrers.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/user_agent_device_data_enrichment.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(write_disposition=\"append\")\ndef tracked_data():\n    \"\"\"\n    A generator function that yields a series of dictionaries, each representing\n    user tracking data.\n\n    This function is decorated with `dlt.resource` to integrate into the DLT (Data\n    Loading Tool) pipeline. The `write_disposition` parameter is set to \"append\" to\n    ensure that data from this generator is appended to the existing data in the\n    destination table.\n\n    Yields:\n        dict: A dictionary with keys 'user_id', 'device_name', and 'page_referer',\n        representing the user's tracking data including their device and the page\n        they were referred from.\n    \"\"\"\n\n    # Sample data representing tracked user data\n    sample_data = [\n        {\"user_id\": 1, \"device_name\": \"Sony Experia XZ\", \"page_referer\":\n        \"https://b2venture.lightning.force.com/\"},\n        {\"user_id\": 2, \"device_name\": \"Samsung Galaxy S23 Ultra 5G\",\n        \"page_referer\": \"https://techcrunch.com/2023/07/20/can-dlthub-solve-the-python-library-problem-for-ai-dig-ventures-thinks-so/\"},\n        {\"user_id\": 3, \"device_name\": \"Apple iPhone 14 Pro Max\",\n        \"page_referer\": \"https://dlthub.com/success-stories/freelancers-perspective/\"},\n        {\"user_id\": 4, \"device_name\": \"OnePlus 11R\",\n        \"page_referer\": \"https://www.reddit.com/r/dataengineering/comments/173kp9o/ideas_for_data_validation_on_data_ingestion/\"},\n        {\"user_id\": 5, \"device_name\": \"Google Pixel 7 Pro\", \"page_referer\": \"https://pypi.org/\"},\n    ]\n\n    # Yielding each user's data as a dictionary\n    for user_data in sample_data:\n        yield user_data\n```\n\n----------------------------------------\n\nTITLE: Executing Parameterized SQL Queries with DLT\nDESCRIPTION: Shows how to execute a parameterized SQL query to select specific customer data using the DLT SQL client. The example includes error handling and demonstrates how to access the returned data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/transformations/sql.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    with pipeline.sql_client() as client:\n        res = client.execute_sql(\n            \"SELECT id, name, email FROM customers WHERE id = %s\",\n            10\n        )\n        # Prints column values of the first row\n        print(res[0])\nexcept Exception:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Naming Convention in TOML\nDESCRIPTION: Example of how to set a custom naming convention (sql_ci_v1) in the global dlt config.toml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/naming-convention.md#2025-04-14_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[schema]\nnaming=\"sql_ci_v1\"\n```\n\n----------------------------------------\n\nTITLE: Creating Endpoint Resources in Freshdesk Source\nDESCRIPTION: Code that creates and yields dlt resources for each Freshdesk endpoint. Each resource is configured with incremental loading, merge write disposition, and 'id' as the primary key.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/freshdesk.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source()\ndef freshdesk_source(\n    #args as defined above\n) -> Iterable[DltResource]:\n    for endpoint in ENDPOINTS:\n        yield dlt.resource(\n            incremental_resource,\n            name=endpoint,\n            write_disposition=\"merge\",\n            primary_key=\"id\",\n        )(endpoint=endpoint)\n```\n\n----------------------------------------\n\nTITLE: Schema Validation with Not Null Constraints\nDESCRIPTION: Example of implementing schema validation using not null constraints to verify column removal in DLT pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema-evolution.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata = [{\n    \"organization\": \"Tech Innovations Inc.\",\n    \"address\": {\n        'building': 'r&d'\n    },\n    \"Inventory\": [\n        {\"name\": \"Plasma ray\", \"inventory nr\": 2411},\n        {\"name\": \"Self-aware Roomba\", \"inventory nr\": 268},\n        {\"name\": \"Type-inferrer\", \"inventory nr\": 3621}\n    ]\n}]\n\npipeline = dlt.pipeline(\"organizations_pipeline\", destination=\"duckdb\")\n# Adding not null constraint\npipeline.run(data, table_name=\"org\", columns={\"room\": {\"data_type\": \"bigint\", \"nullable\": False}})\n```\n\n----------------------------------------\n\nTITLE: Grouping Multiple Data Sources with dlt_assets in Dagster\nDESCRIPTION: This snippet demonstrates how to organize multiple Google Analytics data sources in Dagster using the group_name parameter in the @dlt_assets decorator. It creates two separate asset functions that load data from Google Analytics into BigQuery, grouped for better visualization in the Dagster UI.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-dagster.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dagster_embedded_elt.dlt import dlt_assets\nfrom dlt_sources.google_analytics import google_analytics\n\n# Define assets for the first Google Analytics source\n@dlt_assets(\n    dlt_source=google_analytics(),\n    dlt_pipeline=dlt.pipeline(\n      pipeline_name=\"google_analytics_pipeline_1\",\n      destination=\"bigquery\",\n      dataset_name=\"google_analytics_data_1\"\n    ),\n    group_name='Google_Analytics'\n)\ndef google_analytics_assets_1(context, dlt):\n    yield from dlt.run(context=context)\n\n# Define assets for the second Google Analytics source\n@dlt_assets(\n    dlt_source=google_analytics(),\n    dlt_pipeline=dlt.pipeline(\n      pipeline_name=\"google_analytics_pipeline_2\",\n      destination=\"bigquery\",\n      dataset_name=\"google_analytics_data_2\"\n    ),\n    group_name='Google_Analytics'\n)\ndef google_analytics_assets_2(context, dlt):\n    yield from dlt.run(context=context)\n```\n\n----------------------------------------\n\nTITLE: Applying Data Type Hints to MongoDB Collections\nDESCRIPTION: Python code that demonstrates using apply_hints to programmatically set data types across multiple collections from a MongoDB source, specifically applying the JSON data type to a specified column.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema.md#2025-04-14_snippet_12\n\nLANGUAGE: py\nCODE:\n```\nall_collections = [\"collection1\", \"collection2\", \"collection3\"]  # replace with your actual collection names\nsource_data = mongodb().with_resources(*all_collections)\n\nfor col in all_collections:\n    source_data.resources[col].apply_hints(columns={\"column_name\": {\"data_type\": \"json\"}})\n\npipeline = dlt.pipeline(\n    pipeline_name=\"mongodb_pipeline\",\n    destination=\"duckdb\",\n    dataset_name=\"mongodb_data\"\n)\nload_info = pipeline.run(source_data)\n```\n\n----------------------------------------\n\nTITLE: Defining Channels Resource in Python\nDESCRIPTION: Python function defining the channels resource for the Slack pipeline. It yields all channel data as a dlt resource with a primary key and replace write disposition.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/slack.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(name=\"channels\", primary_key=\"id\", write_disposition=\"replace\")\ndef channels_resource() -> Iterable[TDataItem]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Directly Specifying Column Data Types with @dlt.resource\nDESCRIPTION: Python code that uses the @dlt.resource decorator to directly define data types and properties for columns, bypassing the need for external schema files. This example creates a nullable boolean column.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema.md#2025-04-14_snippet_11\n\nLANGUAGE: py\nCODE:\n```\n@dlt.resource(name='my_table', columns={\"my_column\": {\"data_type\": \"bool\", \"nullable\": True}})\ndef my_resource():\n    for i in range(10):\n        yield {'my_column': i % 2 == 0}\n```\n\n----------------------------------------\n\nTITLE: OAuth2Credentials Usage Example\nDESCRIPTION: Example showing how to use OAuth2Credentials for handling OAuth 2.0 authentication.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\noauth_credentials = OAuth2Credentials(\n    client_id=\"CLIENT_ID\",\n    client_secret=\"CLIENT_SECRET\",  # type: ignore\n    refresh_token=\"REFRESH_TOKEN\",  # type: ignore\n    scopes=[\"scope1\", \"scope2\"]\n)\n\n# Authorize the client\noauth_credentials.auth()\n\n# Add additional scopes\noauth_credentials.add_scopes([\"scope3\", \"scope4\"])\n```\n\n----------------------------------------\n\nTITLE: Performing Full Refresh on All Resources in dlt\nDESCRIPTION: This snippet shows how to change the write disposition to 'replace' for all resources in a source, forcing a full refresh of the data rather than incremental loading.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/source.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\np.run(merge_source(), write_disposition=\"replace\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Additional Snowflake Destination Options in TOML\nDESCRIPTION: This TOML configuration snippet shows how to set additional Snowflake destination options, including stage name, keeping staged files, creating indexes, and using vectorized scanner.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_20\n\nLANGUAGE: toml\nCODE:\n```\n[destination.snowflake]\n# Use an existing named stage instead of the default. Default uses the implicit table stage per table\nstage_name=\"DLT_STAGE\"\n# Whether to keep or delete the staged files after COPY INTO succeeds\nkeep_staged_files=true\n# Add UNIQUE and PRIMARY KEY hints to tables\ncreate_indexes=true\n# Enable vectorized scanner when using the Parquet format\nuse_vectorized_scanner=true\n```\n\n----------------------------------------\n\nTITLE: Tracked Data Resource Implementation\nDESCRIPTION: Python resource implementation that generates sample tracking data including user ID, device name, and page referrer information.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/url-parser-data-enrichment.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(write_disposition=\"append\")\ndef tracked_data():\n    \"\"\"\n    A generator function that yields a series of dictionaries, each representing\n    user tracking data.\n\n    This function is decorated with `dlt.resource` to integrate into the DLT (Data\n    Loading Tool) pipeline. The `write_disposition` parameter is set to \"append\" to\n    ensure that data from this generator is appended to the existing data in the\n    destination table.\n\n    Yields:\n        dict: A dictionary with keys 'user_id', 'device_name', and 'page_referer',\n        representing the user's tracking data including their device and the page\n        they were referred from.\n    \"\"\"\n\n    # Sample data representing tracked user data\n    sample_data = [\n    {\n            \"user_id\": 1,\n            \"device_name\": \"Sony Experia XZ\",\n            \"page_referer\": \"https://b2venture.lightning.force.com/\"\n    },\n        \"\"\"\n        Data for other users\n        \"\"\"\n    ]\n\n    # Yielding each user's data as a dictionary\n    for user_data in sample_data:\n        yield user_data\n```\n\n----------------------------------------\n\nTITLE: Loading Messages from Specific Email in Python\nDESCRIPTION: This snippet demonstrates how to load messages from a specific email address starting from a certain date. It uses the inbox_source function to retrieve messages, configures message properties, and executes the pipeline to load data into a table.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/inbox.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Retrieve messages from the specified email address.\nmessages = inbox_source(filter_emails=(\"mycreditcard@bank.com\",)).messages\n# Configure messages to exclude body and name the result \"my_inbox\".\nmessages = messages(include_body=False).with_name(\"my_inbox\")\n# Execute the pipeline and load messages to the \"my_inbox\" table.\nload_info = pipeline.run(messages)\n# Print the loading details.\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Repository Events Resource Implementation\nDESCRIPTION: Python implementation of the repository events resource with incremental loading and dynamic table naming.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/github.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndlt.resource(primary_key=\"id\", table_name=lambda i: i[\"type\"])  # type: ignore\ndef repo_events(\n    last_created_at: dlt.sources.incremental[str] = dlt.sources.incremental(\n        \"created_at\", initial_value=\"1970-01-01T00:00:00Z\", last_value_func=max\n    )\n) -> Iterator[TDataItems]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Soft Deletes with SQL Table Nullability Adapter\nDESCRIPTION: Implementation for configuring soft deletes by setting up a nullability adapter for the SQL table. This ensures destination schema accepts NULL values for replicated columns when records are marked as deleted.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/ms-sql.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt_plus.sources.mssql import remove_nullability_adapter\n\ntable = sql_table(\n    table_adapter_callback=remove_nullability_adapter,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading SQL Data with Replace Strategy\nDESCRIPTION: Modified Python function that implements the \"replace\" write disposition when loading data, which replaces existing data in the destination rather than appending. This prevents duplication of data when the pipeline is run multiple times.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/sql-database.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.sql_database import sql_database\n\ndef load_tables_family_and_genome():\n\n    source = sql_database().with_resources(\"family\", \"genome\")\n\n    pipeline = dlt.pipeline(\n        pipeline_name=\"sql_to_duckdb_pipeline\",\n        destination=\"duckdb\",\n        dataset_name=\"sql_to_duckdb_pipeline_data\"\n    )\n\n    load_info = pipeline.run(source, write_disposition=\"replace\") # Set write_disposition to load the data with \"replace\"\n\n    print(load_info)\n\nif __name__ == '__main__':\n    load_tables_family_and_genome()\n```\n\n----------------------------------------\n\nTITLE: Load Entire Base Example\nDESCRIPTION: Code example showing how to load all data from an Airtable base.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/airtable.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nbase_id = \"Please set me up!\"     # The ID of the base.\n\nairtables = airtable_source(base_id=base_id)\nload_info = pipeline.run(airtables, write_disposition=\"replace\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Weaviate Destination with Cloud Service\nDESCRIPTION: Configuration for Weaviate destination in the dlt secrets file using Weaviate Cloud Services and OpenAI API for embeddings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/weaviate.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[destination.weaviate.credentials]\nurl = \"https://your-weaviate-url\"\napi_key = \"your-weaviate-api-key\"\n\n[destination.weaviate.credentials.additional_headers]\nX-OpenAI-Api-Key = \"your-openai-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Applying qdrant_adapter to Multiple Resources\nDESCRIPTION: Example showing how to apply the qdrant_adapter to specific resources within a source, specifying different embedding fields for each.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/qdrant.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nproducts_tables = sql_database().with_resources(\"products\", \"customers\")\n\npipeline = dlt.pipeline(\n        pipeline_name=\"postgres_to_qdrant_pipeline\",\n        destination=\"qdrant\",\n    )\n\n# apply adapter to the needed resources\nqdrant_adapter(products_tables.products, embed=\"description\")\nqdrant_adapter(products_tables.customers, embed=\"bio\")\n\ninfo = pipeline.run(products_tables)\n```\n\n----------------------------------------\n\nTITLE: Printing Schema in a Pipeline Context\nDESCRIPTION: Python code that creates a pipeline, runs it with a source, and then prints the default schema in YAML format, showing how to access and display schema details after pipeline execution.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema.md#2025-04-14_snippet_14\n\nLANGUAGE: py\nCODE:\n```\n# Create a pipeline\npipeline = dlt.pipeline(\n               pipeline_name=\"chess_pipeline\",\n               destination='duckdb',\n               dataset_name=\"games_data\")\n\n# Run the pipeline\nload_info = pipeline.run(source)\n\n# Print the default schema in a pretty YAML format\nprint(pipeline.default_schema.to_pretty_yaml())\n```\n\n----------------------------------------\n\nTITLE: Schema Change Notifications with Slack Integration\nDESCRIPTION: Implementation of Slack notifications for schema changes using DLT's load_info tracking and webhook integration.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema-evolution.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.common.runtime.slack import send_slack_message\n\nhook = \"https://hooks.slack.com/services/xxx/xxx/xxx\"\n\nfor package in load_info.load_packages:\n    for table_name, table in package.schema_update.items():\n        for column_name, column in table[\"columns\"].items():\n            send_slack_message(\n                hook,\n                message=(\n                    f\"\\tTable updated: {table_name}: \"\n                    f\"Column changed: {column_name}: \"\n                    f\"{column['data_type']}\"\n                )\n            )\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using dbt Cloud API Client in Python\nDESCRIPTION: This snippet demonstrates how to initialize the DBTCloudClientV2 class and use it to trigger a job run and retrieve job run status in dbt Cloud. It requires an API token and account ID for authentication.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/transformations/dbt/dbt_cloud.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.helpers.dbt_cloud import DBTCloudClientV2\n\n# Initialize the client\nclient = DBTCloudClientV2(api_token=\"YOUR_API_TOKEN\", account_id=\"YOUR_ACCOUNT_ID\")\n\n# Example: Trigger a job run\njob_run_id = client.trigger_job_run(job_id=1234, data={\"cause\": \"Triggered via API\"})\nprint(f\"Job run triggered successfully. Run ID: {job_run_id}\")\n\n# Example: Get run status\nrun_status = client.get_run_status(run_id=job_run_id)\nprint(f\"Job run status: {run_status['status_humanized']}\")\n```\n\n----------------------------------------\n\nTITLE: Using Snowflake Plus in Python Pipeline\nDESCRIPTION: Python code example showing how to use Snowflake Plus destination in a dlt pipeline\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/snowflake_plus.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\npipeline = dlt.pipeline(\n    pipeline_name=\"my_snowflake_plus_pipeline\",\n    destination=\"snowflake_plus\",\n    dataset_name=\"my_dataset\"\n)\n\n@dlt.resource\ndef my_iceberg_table():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring a Custom Freshdesk Pipeline\nDESCRIPTION: Example configuration for a custom Freshdesk pipeline, specifying the pipeline name, destination, and dataset name. Used as the foundation for creating your own data pipelines.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/freshdesk.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"freshdesk_pipeline\",  # Use a custom name if desired\n    destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"freshdesk_data\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Profiles in dlt.yml\nDESCRIPTION: Demonstrates how to set up different profiles (dev and prod) in the dlt.yml manifest file, with different destination configurations for each profile.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/projects.md#2025-04-14_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nprofiles:\n  dev: # Using \"dev\" profile will write to local filesystem\n    destinations:\n      delta_lake:\n        type: delta\n        bucket_url: delta_lake\n  prod: # Using \"prod\" profile will write to s3 bucket\n    destinations:\n      delta_lake:\n        type: delta\n        bucket_url: s3://dlt-ci-test-bucket/dlt_example_project/\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Config Provider in Python\nDESCRIPTION: This snippet demonstrates how to create and register a custom configuration provider in dlt using a JSON file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.common.configuration.providers import CustomLoaderDocProvider\n\n# Create a function that loads a dict\ndef load_config():\n    with open(\"config.json\", \"rb\") as f:\n        return json.load(f)\n\n\n# Create the custom provider\nprovider = CustomLoaderDocProvider(\"my_json_provider\", load_config)\n\n# Register provider\ndlt.config.register_provider(provider)\n```\n\n----------------------------------------\n\nTITLE: Incrementally Loading MongoDB Collection with Append-Only Disposition in Python\nDESCRIPTION: This snippet shows how to incrementally load a MongoDB collection with an append-only disposition using hints in DLT. It's suitable for tables where new rows are added but existing rows aren't updated.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mongodb.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Suitable for tables where new rows are added, but existing rows aren't updated.\n# Load data from the 'listingsAndReviews' collection in MongoDB, using 'last_scraped' for incremental addition.\nairbnb = mongodb().with_resources(\"listingsAndReviews\")\n\nairbnb.listingsAndReviews.apply_hints(\n    incremental=dlt.sources.incremental(\"last_scraped\")\n)\ninfo = pipeline.run(airbnb, write_disposition=\"append\")\n```\n\n----------------------------------------\n\nTITLE: Using Standalone Filesystem Resource in dlt\nDESCRIPTION: Example showing how to use the standalone filesystem resource to list files in cloud storage or local filesystem. This allows customization of file readers or managing files using fsspec.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/advanced.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.filesystem import filesystem\n\npipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\nfiles = filesystem(bucket_url=\"s3://my_bucket/data\", file_glob=\"csv_folder/*.csv\")\npipeline.run(files)\n```\n\n----------------------------------------\n\nTITLE: Loading Specific Freshdesk Resources\nDESCRIPTION: Code example showing how to load data only from specific Freshdesk endpoints (agents, contacts, and tickets). This demonstrates resource filtering in the pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/freshdesk.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nload_data = freshdesk_source().with_resources(\"agents\", \"contacts\", \"tickets\")\n# Run the pipeline\nload_info = pipeline.run(load_data)\n# Print the pipeline run information\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Tagging for Snowflake in TOML\nDESCRIPTION: This TOML configuration snippet demonstrates how to set up query tagging for Snowflake, including source, resource, table, load_id, and pipeline_name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_23\n\nLANGUAGE: toml\nCODE:\n```\n[destination.snowflake]\nquery_tag='{{\"source\":\"{source}\", \"resource\":\"{resource}\", \"table\": \"{table}\", \"load_id\":\"{load_id}\", \"pipeline_name\":\"{pipeline_name}\"}}'\n```\n\n----------------------------------------\n\nTITLE: Complete DLT Pipeline with Relationship Definitions\nDESCRIPTION: Full example showing resource definitions with primary keys and relationship mapping using table_reference_adapter.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/transformations/dbt-transformations.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt_plus.dbt_generator.utils import table_reference_adapter\n\n\n# Example countries table\n@dlt.resource(name=\"countries\", primary_key=\"id\", write_disposition=\"merge\")\ndef countries():\n    yield from [\n        {\"id\": 1, \"name\": \"USA\"},\n        {\"id\": 2, \"name\": \"Germany\"},\n    ]\n\n\n# Example companies table\n@dlt.resource(name=\"companies\", primary_key=\"id\", write_disposition=\"merge\")\ndef companies():\n    yield from [\n        {\"id\": 1, \"name\": \"GiggleTech\", \"country_id\": 2},\n        {\"id\": 2, \"name\": \"HappyHacks\", \"country_id\": 1},\n    ]\n\n\n# Example customers table which references company\n@dlt.resource(name=\"customers\", primary_key=\"id\", write_disposition=\"merge\")\ndef customers():\n    yield from [\n        {\"id\": 1, \"name\": \"Andrea\", \"company_id\": 1},\n        {\"id\": 2, \"name\": \"Violetta\", \"company_id\": 2},\n        {\"id\": 3, \"name\": \"Marcin\", \"company_id\": 1},\n    ]\n\n\n# Example orders table which references customer\n@dlt.resource(name=\"orders\", primary_key=\"id\", write_disposition=\"merge\")\ndef orders():\n    yield from [\n        {\"id\": 1, \"date\": \"1-2-2020\", \"customer_id\": 1},\n        {\"id\": 2, \"date\": \"14-2-2020\", \"customer_id\": 2},\n        {\"id\": 3, \"date\": \"18-2-2020\", \"customer_id\": 1},\n        {\"id\": 4, \"date\": \"1-3-2020\", \"customer_id\": 3},\n        {\"id\": 5, \"date\": \"2-3-2020\", \"customer_id\": 3},\n    ]\n\n# Run your pipeline\np = dlt.pipeline(pipeline_name=\"example_shop\", destination=\"duckdb\")\np.run([customers(), companies(), orders(), countries()])\n\n# Define relationships in your schema\ntable_reference_adapter(\n    p,\n    \"companies\",\n    references=[\n        {\n            \"referenced_table\": \"countries\",\n            \"columns\": [\"country_id\"],\n            \"referenced_columns\": [\"id\"],\n        }\n    ],\n)\n\ntable_reference_adapter(\n    p,\n    \"customers\",\n    references=[\n        {\n            \"referenced_table\": \"companies\",\n            \"columns\": [\"company_id\"],\n            \"referenced_columns\": [\"id\"],\n        }\n    ],\n)\n\ntable_reference_adapter(\n    p,\n    \"orders\",\n    references=[\n        {\n            \"referenced_table\": \"customers\",\n            \"columns\": [\"customer_id\"],\n            \"referenced_columns\": [\"id\"],\n        }\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring a Customized TQDM Progress Monitor in DLT Pipeline\nDESCRIPTION: Creates a DLT pipeline with a customized tqdm progress bar that displays in yellow. This example shows how to modify the appearance of progress monitors through configuration parameters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/pipeline.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# set tqdm bar color to yellow\npipeline = dlt.pipeline(\n    pipeline_name=\"chess_pipeline\",\n    destination='duckdb',\n    dataset_name=\"chess_players_games_data\",\n    progress=dlt.progress.tqdm(colour=\"yellow\")\n)\n```\n\n----------------------------------------\n\nTITLE: Transforming Google Analytics Metrics Data in Python\nDESCRIPTION: Transformer function that processes metadata to extract metrics information and populate a metrics table. Uses the get_metadata resource as input.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_analytics.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@dlt.transformer(data_from=get_metadata, write_disposition=\"replace\", name=\"metrics\")\ndef metrics_table(metadata: Metadata) -> Iterator[TDataItem]:\n    for metric in metadata.metrics:\n        yield to_dict(metric)\n```\n\n----------------------------------------\n\nTITLE: Comprehensive SQL Database and Table Configuration in TOML\nDESCRIPTION: This TOML configuration combines settings for both the SQL database connection and specific table parameters. It includes credentials, schema, backend, chunk size, and incremental loading settings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/advanced.md#2025-04-14_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[sources.sql_database]\ncredentials=\"mssql+pyodbc://loader.database.windows.net/dlt_data?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server\"\nschema=\"data\"\nbackend=\"pandas\"\nchunk_size=1000\n\n[sources.sql_database.chat_message.incremental]\ncursor_path=\"updated_at\"\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Timestamp Column in dlt Resource\nDESCRIPTION: Example of configuring timestamp data type with custom precision and timezone settings in a dlt resource, which maps to specific Snowflake timestamp types.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_11\n\nLANGUAGE: py\nCODE:\n```\n@dlt.resource(\n    columns={\"event_tstamp\": {\"data_type\": \"timestamp\", \"precision\": 3, \"timezone\": False}},\n    primary_key=\"event_id\",\n)\ndef events():\n    yield [{\"event_id\": 1, \"event_tstamp\": \"2024-07-30T10:00:00.123\"}]\n\npipeline = dlt.pipeline(destination=\"snowflake\")\npipeline.run(events())\n```\n\n----------------------------------------\n\nTITLE: Adding Computed Columns and Custom Incremental Clauses (Python)\nDESCRIPTION: Demonstrates how to add computed columns to table definitions and use them for incremental loading in a sql_database source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/usage.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef add_max_timestamp(table):\n    computed_max_timestamp = sa.sql.type_coerce(\n        sa.func.greatest(table.c.created_at, table.c.updated_at),\n        sqltypes.DateTime,\n    ).label(\"max_timestamp\")\n    subquery = sa.select(*table.c, computed_max_timestamp).subquery()\n    return subquery\n```\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.sql_database import sql_table\n\nread_table = sql_table(\n    table=\"chat_message\",\n    table_adapter_callback=add_max_timestamp,\n    incremental=dlt.sources.incremental(\"max_timestamp\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Combining File and Record Level Incremental Loading\nDESCRIPTION: Example showing combined incremental loading using both file modification date and record update time.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/basic.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.filesystem import filesystem, read_csv\n\n# This configuration will only consider modified CSV files\nnew_files = filesystem(bucket_url=\"s3://bucket_name\", file_glob=\"directory/*.csv\")\nnew_files.apply_hints(incremental=dlt.sources.incremental(\"modification_date\"))\n\n# And in each modified file, we filter out only updated records\nfilesystem_pipe = (new_files | read_csv())\nfilesystem_pipe.apply_hints(incremental=dlt.sources.incremental(\"updated_at\"))\npipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\nload_info = pipeline.run(filesystem_pipe)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Database Connection in secrets.toml\nDESCRIPTION: TOML configuration for connecting to the MySQL RFam database. This configuration specifies the database driver, credentials, and connection details required by the dlt pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/sql-database.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[sources.sql_database.credentials]\ndrivername = \"mysql+pymysql\" # database+dialect\ndatabase = \"Rfam\"\npassword = \"\"\nusername = \"rfamro\"\nhost = \"mysql-rfam-public.ebi.ac.uk\"\nport = 4497\n```\n\n----------------------------------------\n\nTITLE: Initializing Replication in Python using DLT\nDESCRIPTION: Configures PostgreSQL for replication using the init_replication function. It sets up the replication slot, publication, and specifies the schema and table to replicate. The reset parameter allows for re-initialization.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/pg_replication.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# requires the Postgres user to have the REPLICATION attribute assigned\ninit_replication(  \n    slot_name=slot_name,\n    pub_name=pub_name,\n    schema_name=src_pl.dataset_name,\n    table_names=\"my_source_table\",\n    reset=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a dlt Pipeline with Redshift Destination and S3 Staging\nDESCRIPTION: Python code example to create a dlt pipeline that loads chess player data to a Redshift destination with staging on S3.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/redshift.md#2025-04-14_snippet_8\n\nLANGUAGE: py\nCODE:\n```\n# Create a dlt pipeline that will load\n# chess player data to the Redshift destination\n# via staging on S3\npipeline = dlt.pipeline(\n    pipeline_name='chess_pipeline',\n    destination='redshift',\n    staging='filesystem', # add this to activate the staging location\n    dataset_name='player_data'\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Google Analytics Data from Specific Date in Python\nDESCRIPTION: Example of loading Google Analytics data from a specified start date using the configured pipeline, with support for incremental loading.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_analytics.md#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nload_data = google_analytics(start_date='2023-01-01')\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Loading All Freshdesk Endpoints\nDESCRIPTION: Code example for loading data from all available Freshdesk endpoints as specified in settings.py. Runs the pipeline and prints execution information.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/freshdesk.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nload_data = freshdesk_source()\n# Run the pipeline\nload_info = pipeline.run(load_data)\n# Print the pipeline run information\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Defining a dlt Pipeline with Qdrant Destination\nDESCRIPTION: Python code setting up a dlt pipeline with Qdrant as the destination and a specified dataset name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/qdrant.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"movies\",\n    destination=\"qdrant\",\n    dataset_name=\"MoviesDataset\",\n)\n```\n\n----------------------------------------\n\nTITLE: Schema Settings Configuration in YAML\nDESCRIPTION: YAML configuration example showing how to set up data type autodetectors in the schema settings. Defines the order and types of data detection functions to be applied.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema.md#2025-04-14_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nsettings:\n  detections:\n    - timestamp\n    - iso_timestamp\n    - iso_date\n    - large_integer\n    - hexbytes_to_text\n    - wei_to_double\n```\n\n----------------------------------------\n\nTITLE: ConnectionStringCredentials Usage Example\nDESCRIPTION: Detailed example of using ConnectionStringCredentials class to manage database connection information.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncredentials = ConnectionStringCredentials()\n\n# Set the necessary attributes\ncredentials.drivername = \"postgresql\"\ncredentials.database = \"my_database\"\ncredentials.username = \"my_user\"\ncredentials.password = \"my_password\"  # type: ignore\ncredentials.host = \"localhost\"\ncredentials.port = 5432\n\n# Convert credentials to a connection string\nconnection_string = credentials.to_native_representation()\n\n# Parse a connection string and update credentials\nnative_value = \"postgresql://my_user:my_password@localhost:5432/my_database\"\ncredentials.parse_native_representation(native_value)\n\n# Get a URL representation of the connection\nurl_representation = credentials.to_url()\n```\n\n----------------------------------------\n\nTITLE: Setting Max Identifier Length in DuckDB Destination Configuration\nDESCRIPTION: This code snippet demonstrates how to configure the max_identifier_length parameter when initializing a DuckDB destination in a DLT pipeline to prevent 'File name too long' errors. The parameter truncates all identifiers (tables, columns, filenames) to the specified maximum length.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.destinations import duckdb as duckdb_destination\n\npipeline = dlt.pipeline(\n    pipeline_name=\"your_pipeline_name\",\n    destination=duckdb_destination(\n        max_identifier_length=200,  # Adjust the length as needed\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Renaming MongoDB Collection in DLT Pipeline (Python)\nDESCRIPTION: This snippet demonstrates how to load a selected MongoDB collection and rename it in the destination using DLT. It applies a hint to rename the table 'collection_1' to 'loaded_data_1' in the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mongodb.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Create the MongoDB source and select the \"collection_1\" collection\nsource = mongodb().with_resources(\"collection_1\")\n\n# Apply the hint to rename the table in the destination\nsource.resources[\"collection_1\"].apply_hints(table_name=\"loaded_data_1\")\n\n# Run the pipeline\ninfo = pipeline.run(source, write_disposition=\"replace\")\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Loading Slack Data from Specified Date Range with DLT in Python\nDESCRIPTION: This code snippet shows how to load Slack resources from a specified start date to end date using DLT. It uses the slack_source function and sets page size and date range parameters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/slack.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsource = slack_source(page_size=1000, start_date=datetime.datetime(2023, 9, 1), end_date=datetime.datetime(2023, 9, 8))\n\n# Enable below to load only 'access_logs', available for paid accounts only.\n# source.access_logs.selected = True\n\n# It loads data starting from 1st September 2023 to 8th September 2023.\nload_info = pipeline.run(source)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Union Type Credentials Example\nDESCRIPTION: Example showing how to implement and use multiple credential types with Union types in DLT.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef zen_source(credentials: Union[ZenApiKeyCredentials, ZenEmailCredentials, str] = dlt.secrets.value, some_option: bool = False):\n  # Depending on what the user provides in config, ZenApiKeyCredentials or ZenEmailCredentials will be injected into the `credentials` argument. Both classes implement `auth` so you can always call it.\n  credentials.auth() # type: ignore[union-attr]\n  return dlt.resource([credentials], name=\"credentials\")\n\n# Pass native value\nos.environ[\"CREDENTIALS\"] = \"email:mx:pwd\"\nassert list(zen_source())[0].email == \"mx\"\n\n# Pass explicit native value\nassert list(zen_source(\"secret:ðŸ”‘:secret\"))[0].api_secret == \"secret\"\n# Pass explicit dict\nassert list(zen_source(credentials={\"email\": \"emx\", \"password\": \"pass\"}))[0].email == \"emx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Pipeline\nDESCRIPTION: Python code example showing how to configure a custom pipeline with specific name and destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/jira.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"jira_pipeline\",  # Use a custom name if desired\n    destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"jira\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Currency Conversion Function in Python\nDESCRIPTION: Function that converts amounts between currencies using ExchangeRate-API with rate caching functionality\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/currency_conversion_data_enrichment.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef converted_amount(record):\n    base_currency = \"USD\"\n    target_currency = \"EUR\"\n\n    api_key: str = dlt.secrets.get(\"sources.api_key\")\n\n    rates_state = dlt.current.resource_state().setdefault(\"rates\", {})\n    currency_pair_key = f\"{base_currency}-{target_currency}\"\n    currency_pair_state = rates_state.setdefault(currency_pair_key, {\n        \"last_update\": datetime.datetime.min,\n        \"rate\": None\n    })\n\n    if (currency_pair_state.get(\"rate\") is None or\n        (datetime.datetime.utcnow() - currency_pair_state[\"last_update\"] >= datetime.timedelta(hours=12))):\n        url = f\"https://v6.exchangerate-api.com/v6/{api_key}/pair/{base_currency}/{target_currency}\"\n        response = requests.get(url)\n        if response.status_code == 200:\n            data = response.json()\n            currency_pair_state.update({\n                \"rate\": data.get(\"conversion_rate\"),\n                \"last_update\": datetime.datetime.fromtimestamp(data.get(\"time_last_update_unix\"))\n            })\n            print(f\"The latest rate of {data.get('conversion_rate')} for the currency pair {currency_pair_key} is fetched and updated.\")\n        else:\n            raise Exception(f\"Error fetching the exchange rate: HTTP {response.status_code}\")\n\n    amount = record['device_price_usd']\n    rate = currency_pair_state[\"rate\"]\n    yield {\n        \"actual_amount\": amount,\n        \"base_currency\": base_currency,\n        \"converted_amount\": round(amount * rate, 2),\n        \"target_currency\": target_currency,\n        \"rate\": rate,\n        \"rate_last_updated\": currency_pair_state[\"last_update\"],\n    }\n```\n\n----------------------------------------\n\nTITLE: Using LanceDB Adapter with Multiple Embed Fields\nDESCRIPTION: Demonstrates how to use the LanceDB adapter to specify multiple fields for embedding.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/lancedb.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlancedb_adapter(\n  resource,\n  embed=[\"title\", \"description\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Analytics Property and Queries\nDESCRIPTION: TOML configuration for specifying Google Analytics property ID and query parameters in the config.toml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_analytics.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[sources.google_analytics]\nproperty_id = \"213025502\" #  this is an example property id, please use yours\nqueries = [\n    {\"resource_name\"= \"sample_analytics_data1\", \"dimensions\"= [\"browser\", \"city\"], \"metrics\"= [\"totalUsers\", \"transactions\"]},\n    {\"resource_name\"= \"sample_analytics_data2\", \"dimensions\"= [\"browser\", \"city\", \"dateHour\"], \"metrics\"= [\"totalUsers\"]}\n]\n```\n\n----------------------------------------\n\nTITLE: AWS Source Configuration with DLT\nDESCRIPTION: Implementation of a DLT source function using AWS credentials with conversion to different credential formats.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef aws_readers(\n    bucket_url: str = dlt.config.value,\n    credentials: AwsCredentials = dlt.secrets.value,\n):\n    ...\n    # Convert credentials to s3fs format\n    s3fs_credentials = credentials.to_s3fs_credentials()\n    print(s3fs_credentials[\"key\"])\n\n    # Get AWS credentials from botocore session\n    aws_credentials = credentials.to_native_credentials()\n    print(aws_credentials.access_key)\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Loading with sql_database Source\nDESCRIPTION: Implementation of incremental loading using the sql_database source with resource hints. Shows how to apply incremental loading settings to specific tables.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/advanced.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.sql_database import sql_database\n\nsource = sql_database().with_resources(\"family\")\n# Using the \"last_modified\" field as an incremental field using initial value of midnight January 1, 2024\nsource.family.apply_hints(incremental=dlt.sources.incremental(\"updated\", initial_value=pendulum.DateTime(2022, 1, 1, 0, 0, 0)))\n\n# Running the pipeline\npipeline = dlt.pipeline(destination=\"duckdb\")\nload_info = pipeline.run(source, write_disposition=\"merge\")\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF Attachments in Python\nDESCRIPTION: This snippet demonstrates how to use the pdf_to_text transformer to extract text from PDF attachments in emails. It filters emails by sender and MIME type, processes the attachments, and loads the extracted text into a database table.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/inbox.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfilter_emails = [\"mycreditcard@bank.com\", \"community@dlthub.com.\"] # Email senders\nattachments = inbox_source(\n     filter_emails=filter_emails, filter_by_mime_type=[\"application/pdf\"]\n).attachments\n\n# Process attachments through PDF parser and save to 'my_pages' table.\nload_info = pipeline.run((attachments | pdf_to_text).with_name(\"my_pages\"))\n# Display loaded data details.\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Unique Visitors Transformer Implementation\nDESCRIPTION: Python transformer function for processing and extracting unique visitor information\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/matomo.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@dlt.transformer(\n    data_from=get_last_visits,\n    write_disposition=\"merge\",\n    name=\"visitors\",\n    primary_key=\"visitorId\",\n)\ndef get_unique_visitors(\n    visits: List[DictStrAny], client: MatomoAPIClient, site_id: int\n) -> Iterator[TDataItem]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Using BigQuery Adapter with Partitioned Tables in Dagster dlt_asset\nDESCRIPTION: This code demonstrates how to configure bigquery_adapter with @dlt_asset for partitioned tables in Google Analytics data extraction. It sets up a data pipeline that retrieves Google Analytics data with date-based partitioning, handling metadata, dimensions, and metrics while ensuring incremental loading based on the date field.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-dagster.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom google.analytics import BetaAnalyticsDataClient\nfrom dlt.destinations.adapters import bigquery_adapter\nfrom dagster import dlt_asset\n\n@dlt_asset\ndef google_analytics_asset(context):\n    # Configuration (replace with your actual values or parameters)\n    queries = [\n        {\"dimensions\": [\"dimension1\"], \"metrics\": [\"metric1\"], \"resource_name\": \"resource1\"}\n    ]\n    property_id = \"your_property_id\"\n    start_date = \"2024-01-01\"\n    rows_per_page = 1000\n    credentials = your_credentials\n\n    # Initialize Google Analytics client\n    client = BetaAnalyticsDataClient(credentials=credentials.to_native_credentials())\n\n    # Fetch metadata\n    metadata = get_metadata(client=client, property_id=property_id)\n    resource_list = [metadata | metrics_table, metadata | dimensions_table]\n\n    # Configure and add resources to the list\n    for query in queries:\n        dimensions = query[\"dimensions\"]\n        if \"date\" not in dimensions:\n            dimensions.append(\"date\") # type: ignore[attr-defined]\n\n        resource_name: str = query[\"resource_name\"] # type: ignore[assignment]\n        resource_list.append(\n            bigquery_adapter(\n                dlt.resource(data, name=resource_name, write_disposition=\"append\")(\n                    client=client,\n                    rows_per_page=rows_per_page,\n                    property_id=property_id,\n                    dimensions=dimensions,\n                    metrics=query[\"metrics\"],\n                    resource_name=resource_name,\n                    start_date=start_date,\n                    last_date=dlt.sources.incremental(\"date\"),\n                ),\n                partition=\"date\"\n            )\n        )\n\n    return resource_list\n```\n\n----------------------------------------\n\nTITLE: Initializing DLT Pipeline with Azure Synapse and Blob Storage Staging\nDESCRIPTION: Creates a DLT pipeline that loads chess player data to Synapse destination using Azure Blob Storage as a staging location. The pipeline configuration includes essential parameters for naming and staging setup.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/synapse.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name='chess_pipeline',\n    destination='synapse',\n    staging='filesystem', # add this to activate the staging location\n    dataset_name='player_data'\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Sources in dlt.yml\nDESCRIPTION: Demonstrates how to declare data sources in the dlt.yml manifest file, including a REST API source and a custom GitHub source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/projects.md#2025-04-14_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  pokemon:\n    type: rest_api\n    client:\n      base_url: https://pokeapi.co/api/v2/\n    resource_defaults:\n      endpoint:\n        params:\n          limit: 1000\n    resources:\n      - pokemon\n      - berry\n\n  github:\n    type: github.source\n```\n\n----------------------------------------\n\nTITLE: Loading Specific Contact Properties\nDESCRIPTION: Configures the pipeline to load only specific contact properties while excluding custom properties.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/hubspot.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nload_data = hubspot()\nload_data.contacts.bind(props=[\"date_of_birth\", \"degree\"], include_custom_props=False)\nload_info = pipeline.run(load_data.with_resources(\"contacts\"))\n```\n\n----------------------------------------\n\nTITLE: Airtable Source Definition\nDESCRIPTION: Python function definition for the Airtable source that retrieves tables from a base.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/airtable.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef airtable_source(\n    base_id: str = dlt.config.value,\n    table_names: Optional[List[str]] = None,\n    access_token: str = dlt.secrets.value,\n) -> Iterable[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Replace Write Disposition for REST API Source\nDESCRIPTION: Python code showing how to set the write_disposition parameter to 'replace' in the REST API source configuration to replace existing data instead of appending it.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/rest-api.md#2025-04-14_snippet_9\n\nLANGUAGE: py\nCODE:\n```\n...\npokemon_source = rest_api_source(\n    {\n        \"client\": {\n            \"base_url\": \"https://pokeapi.co/api/v2/\",\n        },\n        \"resource_defaults\": {\n            \"endpoint\": {\n                \"params\": {\n                    \"limit\": 1000,\n                },\n            },\n            \"write_disposition\": \"replace\", # Setting the write disposition to `replace`\n        },\n        \"resources\": [\n            \"pokemon\",\n            \"berry\",\n            \"location\",\n        ],\n    }\n)\n...\n```\n\n----------------------------------------\n\nTITLE: Defining Spreadsheet Info Resource in Python\nDESCRIPTION: This Python snippet defines the spreadsheet_info resource, which loads information about the sheets and range names into the destination as a table. It uses merge loading with the spreadsheet_id as the merge key.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndlt.resource(\n     metadata_table,\n     write_disposition=\"merge\",\n     name=\"spreadsheet_info\",\n     merge_key=\"spreadsheet_id\",\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Replication with Snapshot in Python using DLT\nDESCRIPTION: Sets up replication and creates a snapshot of the data for initial load. The persist_snapshots parameter enables snapshot creation, and the function returns resources for the initial data load.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/pg_replication.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsnapshot = init_replication(  # requires the Postgres user to have the REPLICATION attribute assigned\n     slot_name=slot_name,\n     pub_name=pub_name,\n     schema_name=src_pl.dataset_name,\n     table_names=\"my_source_table\",\n     persist_snapshots=True,  # persist snapshot table(s) and let function return resource(s) for initial load\n     reset=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Working with SQLite In-Memory Databases\nDESCRIPTION: Example showing how to use SQLite in-memory databases with dlt by maintaining a persistent connection throughout the pipeline run.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/sqlalchemy.md#2025-04-14_snippet_5\n\nLANGUAGE: py\nCODE:\n```\nimport dlt\nimport sqlalchemy as sa\n\n# Create the SQLite engine\nengine = sa.create_engine('sqlite:///:memory:')\n\n# Configure the destination instance and create pipeline\npipeline = dlt.pipeline('my_pipeline', destination=dlt.destinations.sqlalchemy(engine), dataset_name='main')\n\n# Run the pipeline with some data\npipeline.run([1,2,3], table_name='my_table')\n\n# The engine is still open and you can query the database\nwith engine.connect() as conn:\n    result = conn.execute(sa.text('SELECT * FROM my_table'))\n    print(result.fetchall())\n```\n\n----------------------------------------\n\nTITLE: Configuring Upsert Merge Strategy for Delta Table in Python\nDESCRIPTION: Example of configuring the upsert merge strategy for a Delta table in Python (experimental feature).\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/delta-iceberg.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(\n    write_disposition={\"disposition\": \"merge\", \"strategy\": \"upsert\"},\n    primary_key=\"my_primary_key\",\n    table_format=\"delta\"\n)\ndef my_upsert_resource():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Google Sheets Pipeline in Airflow DAG\nDESCRIPTION: Demonstrates how to properly set up an Airflow DAG for Google Sheets data pipeline, including configuration for daily runs and BigQuery destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.helpers.airflow_helper import PipelineTasksGroup\n\n@dag(\n    schedule_interval='@daily',\n    start_date=pendulum.DateTime(2023, 2, 1),\n    catchup=False,\n    max_active_runs=1,\n    default_args=default_task_args\n)\ndef get_named_ranges():\n    tasks = PipelineTasksGroup(\"get_named_ranges\", use_data_folder=False, wipe_local_data=True)\n\n    # Import your source from pipeline script\n    from google_sheets import google_spreadsheet\n\n    pipeline = dlt.pipeline(\n        pipeline_name=\"get_named_ranges\",\n        dataset_name=\"named_ranges_data\",\n        destination='bigquery',\n    )\n\n    # Do not use decompose to run `google_spreadsheet` in single task\n    tasks.add_run(pipeline, google_spreadsheet(\"1HhWHjqouQnnCIZAFa2rL6vT91YRN8aIhts22SUUR580\"), decompose=\"none\", trigger_rule=\"all_done\", retries=0, provide_context=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Plus in dlt.yml\nDESCRIPTION: YAML configuration for setting up Snowflake Plus destination with Iceberg support\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/snowflake_plus.md#2025-04-14_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ndestinations:\n  snowflake:\n    type: snowflake_plus\n```\n\nLANGUAGE: yaml\nCODE:\n```\ndestinations:\n  snowflake:\n    type: snowflake_plus\n    external_volume: \"<external_volume_name>\"\n    force_iceberg: true\n```\n\n----------------------------------------\n\nTITLE: Implementing Employees Resource with Incremental Loading\nDESCRIPTION: Python function to define the 'employees' resource with incremental loading capability. It uses dlt.sources.incremental for efficient data updates based on the 'last_modified_at' field.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/personio.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(primary_key=\"id\", write_disposition=\"merge\")\ndef employees(\n    updated_at: dlt.sources.incremental[\n        pendulum.DateTime\n    ] = dlt.sources.incremental(\n        \"last_modified_at\", initial_value=None, allow_external_schedulers=True\n    ),\n    items_per_page: int = ITEMS_PER_PAGE,\n) -> Iterable[TDataItem]:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring a Basic Log Progress Monitor in DLT Pipeline\nDESCRIPTION: Creates a DLT pipeline that loads chess data and dumps progress to stdout every 10 seconds using the default log progress monitor. This is particularly useful in production environments.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/pipeline.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# create a pipeline loading chess data that dumps\n# progress to stdout every 10 seconds (the default)\npipeline = dlt.pipeline(\n    pipeline_name=\"chess_pipeline\",\n    destination='duckdb',\n    dataset_name=\"chess_players_games_data\",\n    progress=\"log\"\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL SSL Connections\nDESCRIPTION: Examples of setting up MySQL connections with different SSL security configurations using mysql and pymysql dialects.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/troubleshooting.md#2025-04-14_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\nsources.sql_database.credentials=\"mysql+pymysql://root:<pass>@<host>:3306/mysql?ssl_ca=\"\n```\n\nLANGUAGE: toml\nCODE:\n```\nsources.sql_database.credentials=\"mysql+pymysql://root:<pass>@<host>:3306/mysql?ssl_ca=server-ca.pem&ssl_check_hostname=false\"\n```\n\nLANGUAGE: toml\nCODE:\n```\nsources.sql_database.credentials=\"mysql+pymysql://root:<pass>@35.203.96.191:3306/mysql?ssl_ca=&ssl_cert=client-cert.pem&ssl_key=client-key.pem\"\n```\n\n----------------------------------------\n\nTITLE: Loading Matomo Live Visits and Recent Data in Python\nDESCRIPTION: This example demonstrates how to load data on live visits and visitors from Matomo, retrieving only data from today. It uses the matomo_visits() function with specific parameters to limit the data range and include live event visitors.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/matomo.md#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nload_data = matomo_visits(initial_load_past_days=1, get_live_event_visitors=True)\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: GCP OAuth Credentials Example\nDESCRIPTION: Example of using GcpOAuthCredentials in a Google Analytics source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.credentials import GcpOAuthCredentials\n\n@dlt.source\ndef google_analytics(\n    property_id: str = dlt.config.value,\n    credentials: GcpOAuthCredentials = dlt.secrets.value,\n):\n    # Authenticate and get access token\n    credentials.auth(scopes=[\"scope1\", \"scope2\"])\n\n    # Retrieve native credentials for Google clients\n    # For example, build the service object for Google Analytics API.\n    client = BetaAnalyticsDataClient(credentials=credentials.to_native_credentials())\n\n    # Get a string representation of the credentials\n    # Returns a string representation of the credentials in the format client_id@project_id.\n    credentials_str = str(credentials)\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Write Disposition in Python\nDESCRIPTION: Python code to configure merge write disposition with upsert strategy for a resource using a decorator.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/delta.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(\n    primary_key=\"id\",  # merge_key also works; primary_key and merge_key may be used together\n    write_disposition={\"disposition\": \"merge\", \"strategy\": \"upsert\"},\n)\ndef my_resource():\n    yield [\n        {\"id\": 1, \"foo\": \"foo\"},\n        {\"id\": 2, \"foo\": \"bar\"}\n    ]\n...\n\npipeline = dlt.pipeline(\"loads_delta\", destination=\"delta\")\n```\n\n----------------------------------------\n\nTITLE: Combined Loading Pipeline\nDESCRIPTION: Example of combining both regular and incremental loading in a single pipeline run.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/stripe.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsource_single = stripe_source(\n    endpoints=(\"Plan\", \"Charge\"),\n    start_date=pendulum.DateTime(2022, 12, 31),\n)\nsource_incremental = incremental_stripe_source(\n    endpoints=(\"Invoice\", ),\n)\nload_info = pipeline.run(data=[source_single, source_incremental])\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Loading Complete Google Analytics Data in Python\nDESCRIPTION: Example of loading all Google Analytics data using the configured pipeline, supporting incremental loading in subsequent runs.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_analytics.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nload_data = google_analytics()\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Loading Shopify Partner Transactions via GraphQL\nDESCRIPTION: Implements a GraphQL query to fetch transactions from the Shopify Partner API with pagination support. The query is configured to fetch 10 transactions per page and includes cursor-based pagination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/shopify.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Construct query to load transactions 100 per page, the `$after` variable is used to paginate\nquery = \"\"\"query Transactions($after: String) {\ntransactions(after: $after, first: 10) {\n    edges {\n        cursor\n        node {\n            id\n        }\n    }\n}\n}\n\"\"\"\n\n# Configure the resource with the query and JSON paths to extract the data and pagination cursor\nresource = shopify_partner_query(\n    query,\n    # JSON path pointing to the data item in the results\n    data_items_path=\"data.transactions.edges[*].node\",\n    # JSON path pointing to the highest page cursor in the results\n    pagination_cursor_path=\"data.transactions.edges[-1].cursor\",\n    # The variable name used for pagination\n    pagination_variable_name=\"after\",\n)\n\nload_info = pipeline.run(resource)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Complex Data Transformation with Arrow Compute\nDESCRIPTION: Advanced example showing how to perform complex transformations using PyArrow compute functions, including filtering, hashing, and column modifications.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/transformations/python.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.compute as pc\n\npipeline = dlt.pipeline(\n    pipeline_name=\"users_pipeline\",\n    destination=\"duckdb\",\n    dataset_name=\"users_raw\",\n    dev_mode=True\n)\n\n# NOTE: this resource will work like a regular resource and support write_disposition, primary_key, etc.\n# NOTE: For selecting only users above 18, we could also use the filter method on the relation with ibis expressions\n@dlt.resource(table_name=\"users_clean\")\ndef users_clean():\n    users = pipeline.dataset().users\n    for arrow_table in users.iter_arrow(chunk_size=1000):\n\n        # we want to filter out users under 18\n        age_filter = pc.greater_equal(arrow_table[\"age\"], 18)\n        arrow_table = arrow_table.filter(age_filter)\n\n        # we want to hash the email column\n        arrow_table = arrow_table.append_column(\"email_hash\", pc.sha256(arrow_table[\"email\"]))\n\n        # we want to remove the email column and name column\n        arrow_table = arrow_table.drop([\"email\", \"name\"])\n\n        # yield the transformed arrow table\n        yield arrow_table\n\n\npipeline.run(users_clean())\n```\n\n----------------------------------------\n\nTITLE: Manual Config and Secrets Access\nDESCRIPTION: Example demonstrating how to manually access configuration values and secrets using dictionary-like syntax.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/advanced.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsource_instance = google_sheets(\n    dlt.config[\"sheet_id\"],\n    dlt.config[\"my_section.tabs\"],\n    dlt.secrets[\"my_section.gcp_credentials\"]\n)\n\nsource_instance.run(destination=\"bigquery\")\n```\n\n----------------------------------------\n\nTITLE: Initializing a dlt Project with Chess Source and Redshift Destination\nDESCRIPTION: Command to initialize a new dlt project with chess as the source and Redshift as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/redshift.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init chess redshift\n```\n\n----------------------------------------\n\nTITLE: Enabling creation of unique indexes in MS SQL\nDESCRIPTION: Configuration to enable the creation of UNIQUE indexes on columns with the 'unique' hint in MS SQL Server.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/mssql.md#2025-04-14_snippet_11\n\nLANGUAGE: toml\nCODE:\n```\n[destination.mssql]\ncreate_indexes=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Loading with Change Tracking\nDESCRIPTION: Python code to set up incremental loading using Change Tracking with dlt+.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/ms-sql.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt_plus.sources.mssql import create_change_tracking_table\n\n# Optional: Configure engine isolation level\n# use it if you create an Engine implicitly\ndef configure_engine_isolation_level(engine):\n    return engine.execution_options(isolation_level=\"SERIALIZABLE\")\n\nincremental_resource = create_change_tracking_table(\n    credentials=engine,\n    table=table_name,\n    schema=schema_name,\n    initial_tracking_version=tracking_version,\n    engine_adapter_callback=configure_engine_isolation_level,\n)\n\npipeline.run(incremental_resource)\n```\n\n----------------------------------------\n\nTITLE: Scheduling a dlt Pipeline with Prefect\nDESCRIPTION: This code shows how to create and schedule a Prefect deployment for daily execution of a dlt pipeline. The .serve method automatically creates a deployment that runs daily at midnight using cron scheduling.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-prefect.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    slack_pipeline.serve(\"slack_pipeline\", cron=\"0 0 * * *\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Table and Column Selection in TOML for DLT\nDESCRIPTION: This TOML configuration demonstrates how to select specific tables and columns for the SQL database source in DLT. It allows for managing table and column selections outside of Python scripts.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/configuration.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n# to select tables names\n[sources.sql_database]\ntable_names = [\n    \"Table_Name_1\",  \n]\n\n# to select specific columns from table \"Table_Name_1\"\n[sources.sql_database.Table_Name_1]\nincluded_columns = [\n    \"Column_Name_1\",\n    \"Column_Name_2\"\n]\n```\n\n----------------------------------------\n\nTITLE: Defining Cache Configuration in YAML for dlt\nDESCRIPTION: This YAML snippet demonstrates how to configure a cache in the dlt.yml file. It defines input and output datasets and tables for a GitHub events processing workflow.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/core-concepts/cache.md#2025-04-14_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncaches:\n  github_events_cache:\n    inputs:\n      - dataset: github_events_dataset\n        tables:\n          items: items\n    outputs:\n      - dataset: github_reports_dataset\n        tables:\n          items: items\n          items_aggregated: items_aggregated\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Filesystem Destination in TOML\nDESCRIPTION: Basic configuration for setting up a local filesystem destination with an absolute path using the file:// schema.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_17\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\nbucket_url = \"file:///absolute/path\"  # three / for an absolute path\n```\n\n----------------------------------------\n\nTITLE: Defining Messages Resource in Python\nDESCRIPTION: Python function for fetching messages from a specific Slack channel. It uses incremental loading to retrieve messages within a specified date range.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/slack.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef get_messages_resource(\n    channel_data: Dict[str, Any],\n    created_at: dlt.sources.incremental[DateTime] = dlt.sources.incremental(\n        \"ts\",\n        initial_value=START_DATE,\n        end_value=END_DATE,\n        allow_external_schedulers=True,\n    ),\n) -> Iterable[TDataItem]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Multiple Topics Extraction\nDESCRIPTION: Example of extracting data from multiple Kafka topics using the consumer resource.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/kafka.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntopics = [\"topic1\", \"topic2\", \"topic3\"]\n\nresource = kafka_consumer(topics)\npipeline.run(resource, write_disposition=\"replace\")\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Database Credentials in TOML\nDESCRIPTION: This TOML configuration sets up the credentials for connecting to a SQL database. It includes driver name, database name, username, host, and port.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/setup.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[sources.sql_database.credentials]\ndrivername = \"mysql+pymysql\" # driver name for the database\ndatabase = \"Rfam\" # database name\nusername = \"rfamro\" # username associated with the database\nhost = \"mysql-rfam-public.ebi.ac.uk\" # host address\nport = \"4497\" # port required for connection\n```\n\n----------------------------------------\n\nTITLE: Using Ibis Expressions with ReadableRelation in Python\nDESCRIPTION: Demonstrates how to use ibis expressions to modify queries on ReadableRelation objects. It includes examples of joining tables, filtering data, and performing various operations using ibis.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/dataset.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# now that ibis is installed, we can get a dataset with ibis relations\ndataset = pipeline.dataset()\n\n# get two relations\nitems_relation = dataset[\"items\"]\norder_relation = dataset[\"orders\"]\n\n# join them using an ibis expression\njoined_relation = items_relation.join(order_relation, items_relation.id == order_relation.item_id)\n\n# now we can use the ibis expression to filter the data\nfiltered_relation = joined_relation.filter(order_relation.status == \"completed\")\n\n# we can inspect the query that will be used to read the data\nprint(filtered_relation.query)\n\n# and finally fetch the data as a pandas dataframe, the same way we would do with a normal relation\ndf = filtered_relation.df()\n\n# a few more examples\n\n# filter for rows where the id is in the list of ids\nitems_relation.filter(items_relation.id.isin([1, 2, 3])).df()\n\n# limit and offset\nitems_relation.limit(10, offset=5).arrow()\n\n# mutate columns by adding a new colums that always is 10 times the value of the id column\nitems_relation.mutate(new_id=items_relation.id * 10).df()\n\n# sort asc and desc\nimport ibis\nitems_relation.order_by(ibis.desc(\"id\"), ibis.asc(\"price\")).limit(10)\n\n# group by and aggregate\nitems_relation.group_by(\"item_group\").having(items_table.count() >= 1000).aggregate(sum_id=items_table.id.sum()).df()\n\n# subqueries\nitems_relation.filter(items_table.category.isin(beverage_categories.name)).df()\n```\n\n----------------------------------------\n\nTITLE: Running a Pipeline with Weaviate Adapter\nDESCRIPTION: Executing the pipeline to load data into Weaviate, specifying which field to vectorize for similarity search.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/weaviate.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ninfo = pipeline.run(\n    weaviate_adapter(\n        movies,\n        vectorize=\"title\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Replicating Changes to Destination in Python using DLT\nDESCRIPTION: Creates a resource for tracking changes in the source table and runs the destination pipeline to replicate these changes.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/pg_replication.md#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Create a resource that generates items for each change in the source table\nchanges = replication_resource(slot_name, pub_name)\n\n# Run the pipeline as\ndest_pl.run(changes)\n```\n\n----------------------------------------\n\nTITLE: Applying Data Type Hints to Multiple Resources in Python\nDESCRIPTION: This Python snippet demonstrates how to apply data type hints to multiple resources in the pipeline, specifically setting 'total_amount' as double and 'date' as timestamp.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfor resource in resources:\n    resource.apply_hints(columns={\n        \"total_amount\": {\"data_type\": \"double\"},\n        \"date\": {\"data_type\": \"timestamp\"},\n    })\n```\n\n----------------------------------------\n\nTITLE: Time-Based Message Extraction\nDESCRIPTION: Example of extracting Kafka messages starting from a specific timestamp using pendulum DateTime.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/kafka.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nresource = kafka_consumer(\"topic\", start_from=pendulum.DateTime(2023, 12, 15))\npipeline.run(resource)\n```\n\n----------------------------------------\n\nTITLE: Running the MotherDuck Pipeline\nDESCRIPTION: Command to execute the MotherDuck pipeline script.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/motherduck.md#2025-04-14_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npython3 chess_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Setting Native Table Format for a Resource\nDESCRIPTION: Python code to set the native table format for a specific resource, overriding the default Delta format.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/delta.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(\n  table_format=\"native\"\n)\ndef my_resource():\n    ...\n\npipeline = dlt.pipeline(\"loads_delta\", destination=\"delta\")\n```\n\n----------------------------------------\n\nTITLE: Generating Fact Table Command\nDESCRIPTION: Shell command to generate a fact table model within an existing dbt project.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/transformations/dbt-transformations.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ndlt dbt generate example_shop --fact orders\n```\n\n----------------------------------------\n\nTITLE: GitHub Issues API Resource (Python)\nDESCRIPTION: Implements the resource function that fetches paginated GitHub issues data using REST client.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/create-a-pipeline.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.helpers.rest_client import paginate\nfrom dlt.sources.helpers.rest_client.auth import BearerTokenAuth\nfrom dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator\n\n@dlt.resource(write_disposition=\"replace\")\ndef github_api_resource(api_secret_key: str = dlt.secrets.value):\n    url = \"https://api.github.com/repos/dlt-hub/dlt/issues\"\n\n    for page in paginate(\n        url,\n        auth=BearerTokenAuth(api_secret_key), # type: ignore\n        paginator=HeaderLinkPaginator(),\n        params={\"state\": \"open\"}\n    ):\n        yield page\n```\n\n----------------------------------------\n\nTITLE: Creating a Prefect Flow for dlt Slack Pipeline\nDESCRIPTION: This example demonstrates how to create a Prefect flow function that orchestrates the execution of individual tasks in a dlt pipeline. The flow accepts parameters for channels and date range, then calls the get_users task.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-prefect.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@flow\ndef slack_pipeline(\n    channels=None, \n    start_date=pendulum.now().subtract(days=1).date()\n) -> None:\n    get_users()\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Chess.com API Pipeline\nDESCRIPTION: Python code to configure a custom pipeline for the Chess.com API, specifying the pipeline name, destination, and dataset name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/chess.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"chess_pipeline\", # Use a custom name if desired\n    destination=\"duckdb\", # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"chess_players_games_data\", # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Current Change Tracking Version in Python\nDESCRIPTION: Python code to get the current Change Tracking version from MS SQL Server using SQLAlchemy.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/ms-sql.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt_plus.sources.mssql import get_current_change_tracking_version\nfrom sqlalchemy import create_engine\n\nconnection_url = \"mssql+pyodbc://username:password@your_server:port/YourDatabaseName?driver=ODBC+Driver+18+for+SQL+Server&TrustServerCertificate=yes\"\n\nengine = create_engine(connection_url)\n\ntracking_version = get_current_change_tracking_version(engine)\n```\n\n----------------------------------------\n\nTITLE: Opening dlt Credentials File\nDESCRIPTION: Command to open the secrets.toml configuration file\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/bigquery.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nopen .dlt/secrets.toml\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON with Simple Decoder in Python DLT Pipeline\nDESCRIPTION: This code snippet demonstrates how to parse JSON data within a DLT pipeline using a simple decoder function. It updates the item with parsed JSON data if possible.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/amazon_kinesis.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef _maybe_parse_json(item: TDataItem) -> TDataItem:\n    try:\n        item.update(json.loadb(item[\"data\"]))\n    except Exception:\n        pass\n    return item\n\ninfo = pipeline.run(kinesis_stream_data.add_map(_maybe_parse_json))\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Configuring Parallelized Extraction\nDESCRIPTION: Shows how to enable parallel extraction of tables using threading for improved performance.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/advanced.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.sql_database import sql_database, sql_table\n\ndatabase = sql_database().parallelize()\ntable = sql_table().parallelize()\n```\n\n----------------------------------------\n\nTITLE: Configuring Orchestra Pipeline YAML for dlt Integration\nDESCRIPTION: YAML configuration for setting up a Python task in Orchestra to run a dlt pipeline. This defines the execution environment, dependencies, Python version, and command to execute the pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-orchestra.md#2025-04-14_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nversion: v1\nname: 'Archive: Python2'\npipeline:\n  32be8199-4e28-437e-9a3a-2b1a10107bd9:\n    tasks:\n      64569afa-ed14-43d4-8c33-64e6b8309593:\n        integration: PYTHON\n        integration_job: PYTHON_EXECUTE_SCRIPT\n        parameters:\n          command: python -m run_dlt_pipelines\n          package_manager: PIP\n          python_version: '3.12'\n          build_command: pip install -r requirements.txt\n        depends_on: []\n        condition: null\n        name: Run DLT\n        tags: []\n        connection: orchestra_python_96778\n        operation_metadata: null\n        treat_failure_as_warning: null\n        configuration: null\n```\n\n----------------------------------------\n\nTITLE: Synapse Credentials Configuration\nDESCRIPTION: TOML configuration examples for setting up Synapse connection credentials\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/synapse.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[destination.synapse.credentials]\ndatabase = \"yourpool\"\nusername = \"loader\"\npassword = \"your_loader_password\"\nhost = \"your_synapse_workspace_name.sql.azuresynapse.net\"\n```\n\nLANGUAGE: toml\nCODE:\n```\ndestination.synapse.credentials = \"synapse://loader:your_loader_password@your_synapse_workspace_name.azuresynapse.net/yourpool\"\n```\n\n----------------------------------------\n\nTITLE: Fetching New Messages from Incremental Kinesis Streams in Python\nDESCRIPTION: This snippet shows how to fetch only new messages from incremental Kinesis streams. It runs the pipeline and checks the message counts to determine if new data was received.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/amazon_kinesis.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Running pipeline will get only new messages.\ninfo = pipeline.run(kinesis_stream_data)\nmessage_counts = pipeline.last_trace.last_normalize_info.row_counts\nif \"kinesis_source_name\" not in message_counts:\n    print(\"No messages in kinesis\")\nelse:\n    print(pipeline.last_trace.last_normalize_info)\n```\n\n----------------------------------------\n\nTITLE: Applying Data Type Hints to a Single Resource in Python\nDESCRIPTION: This Python snippet shows how to apply data type hints to a single resource (Sheet1) in the pipeline, setting 'total_amount' as double and 'date' as timestamp.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nsource.Sheet1.apply_hints(columns={\n    \"total_amount\": {\"data_type\": \"double\"},\n    \"date\": {\"data_type\": \"timestamp\"},\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring DLT Pipeline for MongoDB in Python\nDESCRIPTION: This snippet demonstrates how to configure a DLT pipeline by specifying the pipeline name, destination, and dataset name. It sets up a pipeline to load data from MongoDB into DuckDB.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mongodb.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"mongodb_pipeline\",  # Use a custom name if desired\n    destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"mongodb_data\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Running Scrapy Pipeline with Custom Settings in Python\nDESCRIPTION: Python code to run the Scrapy pipeline with customized settings, including depth limit and middleware configuration.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/scrapy.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrun_pipeline(\n    pipeline,\n    MySpider,\n    # you can pass scrapy settings overrides here\n    scrapy_settings={\n        # How many sub pages to scrape\n        # https://docs.scrapy.org/en/latest/topics/settings.html#depth-limit\n        \"DEPTH_LIMIT\": 100,\n        \"SPIDER_MIDDLEWARES\": {\n            \"scrapy.spidermiddlewares.depth.DepthMiddleware\": 200,\n            \"scrapy.spidermiddlewares.httperror.HttpErrorMiddleware\": 300,\n        },\n        \"HTTPERROR_ALLOW_ALL\": False,\n    },\n    write_disposition=\"append\",\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Data with Table Rename in Google Sheets\nDESCRIPTION: Shows how to load data from a Google Spreadsheet while specifying a custom table name for the loaded data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nload_data = google_spreadsheet(\n    \"https://docs.google.com/spreadsheets/d/43lkHjqouQnnCIZAFa2rL6vT91YRN8aIhts22SUUR580/edit#gid=0\", # Spreadsheet URL\n     range_names=[\"Sheet 1!A1:B10\"],\n     get_named_ranges=False,\n)\n\nload_data.resources[\"Sheet 1!A1:B10\"].apply_hints(table_name=\"loaded_data_1\")\n\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Accessing Delta Tables Using Helper Functions in Python\nDESCRIPTION: Example of using the get_delta_tables helper function to access and perform operations on Delta tables.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/delta-iceberg.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.common.libs.deltalake import get_delta_tables\n\n# get dictionary of DeltaTable objects\ndelta_tables = get_delta_tables(pipeline)\n\n# execute operations on DeltaTable objects\ndelta_tables[\"my_delta_table\"].optimize.compact()\ndelta_tables[\"another_delta_table\"].optimize.z_order([\"col_a\", \"col_b\"])\n# delta_tables[\"my_delta_table\"].vacuum()\n# etc.\n```\n\n----------------------------------------\n\nTITLE: Defining Range Names Resource in Python\nDESCRIPTION: This Python snippet defines the range_names resource, which processes each range name provided by the source function and loads its data into separate tables in the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndlt.resource(\n     process_range(data, headers=headers, data_types=data_types),\n     name=name,\n     write_disposition=\"replace\",\n)\n```\n\n----------------------------------------\n\nTITLE: REST API Source Configuration Structure\nDESCRIPTION: Python code snippet showing the basic structure of the REST API source configuration object, including client settings, resource defaults, and resource definitions.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/rest-api.md#2025-04-14_snippet_8\n\nLANGUAGE: py\nCODE:\n```\nconfig: RESTAPIConfig = {\n    \"client\": {\n        # ...\n    },\n    \"resource_defaults\": {\n        # ...\n    },\n    \"resources\": [\n        # ...\n    ],\n}\n```\n\n----------------------------------------\n\nTITLE: Advanced Usage of ReadableRelation in Python\nDESCRIPTION: Shows advanced usage of ReadableRelation, including using custom SQL queries to create relations and loading a ReadableRelation into a pipeline table.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/dataset.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Join 'items' and 'other_items' tables\ncustom_relation = dataset(\"SELECT * FROM items JOIN other_items ON items.id = other_items.id\")\narrow_table = custom_relation.arrow()\n\n# Create a readable relation with a limit of 1m rows\nlimited_items_relation = dataset.items.limit(1_000_000)\n\n# Create a new pipeline\nother_pipeline = dlt.pipeline(pipeline_name=\"other_pipeline\", destination=\"duckdb\")\n```\n\n----------------------------------------\n\nTITLE: Setting Table Format in Python Pipeline Run\nDESCRIPTION: Example of setting the table format to Delta when running a pipeline in Python.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/delta-iceberg.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipeline.run(my_resource, table_format=\"delta\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Issues Resource\nDESCRIPTION: Python decorator and function definition for creating a resource to handle Jira issues using JQL queries.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/jira.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(write_disposition=\"replace\")\ndef issues(jql_queries: List[str]) -> Iterable[TDataItem]:\n   api_path = \"rest/api/3/search\"\n   return {}  # return the retrieved values here\n```\n\n----------------------------------------\n\nTITLE: Loading Custom Matomo Report Data with Queries in Python\nDESCRIPTION: This example shows how to load custom data from Matomo reports using specific queries. It includes parameters for the resource name, methods, date, period, and extra parameters like the custom report ID.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/matomo.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nqueries = [\n    {\n        \"resource_name\": \"custom_report_name\",\n        \"methods\": [\"CustomReports.getCustomReport\"],\n        \"date\": \"2023-01-01\",\n        \"period\": \"day\",\n        \"extra_params\": {\"idCustomReport\": 1}, # ID of the report\n    },\n]\n\nsite_id = 1 # ID of the site for which reports are being loaded\n\nload_data = matomo_reports(queries=queries, site_id=site_id)\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Creating Loader User SQL Setup\nDESCRIPTION: SQL commands to create and configure a loader user with appropriate permissions in Synapse\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/synapse.md#2025-04-14_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n-- on master database, using a SQL admin account\n\nCREATE LOGIN loader WITH PASSWORD = 'your_loader_password';\n```\n\nLANGUAGE: sql\nCODE:\n```\n-- on yourpool database\n\nCREATE USER loader FOR LOGIN loader;\n\n-- DDL permissions\nGRANT CREATE SCHEMA ON DATABASE :: yourpool TO loader;\nGRANT CREATE TABLE ON DATABASE :: yourpool TO loader;\nGRANT CREATE VIEW ON DATABASE :: yourpool TO loader;\n\n-- DML permissions\nGRANT ADMINISTER DATABASE BULK OPERATIONS TO loader;\n```\n\n----------------------------------------\n\nTITLE: Defining Chess.com API Source in Python\nDESCRIPTION: Python function decorated with @dlt.source that defines the Chess.com API source. It returns a sequence of DltResource objects for player profiles, archives, games, and online status.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/chess.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source(name=\"chess\")\ndef source(\n    players: List[str], start_month: str = None, end_month: str = None\n) -> Sequence[DltResource]:\n    return (\n        players_profiles(players),\n        players_archives(players),\n        players_games(players, start_month=start_month, end_month=end_month),\n        players_online_status(players),\n    )\n```\n\n----------------------------------------\n\nTITLE: Loading Messages from Multiple Emails in Python\nDESCRIPTION: This snippet shows how to load messages from multiple email addresses using the inbox_source function. It filters messages based on specified email addresses.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/inbox.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmessages = inbox_source(\n     filter_emails=(\"mycreditcard@bank.com\", \"community@dlthub.com.\")\n).messages\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Format for Postgres Destination in dlt\nDESCRIPTION: TOML and Python examples for configuring custom CSV format settings for the Postgres destination in dlt.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/postgres.md#2025-04-14_snippet_12\n\nLANGUAGE: toml\nCODE:\n```\n[destination.postgres.csv_format]\ndelimiter=\"|\"\ninclude_header=false\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.destinations import postgres\nfrom dlt.common.data_writers.configuration import CsvFormatConfiguration\n\ncsv_format = CsvFormatConfiguration(delimiter=\"|\", include_header=False)\n\ndest_ = postgres(csv_format=csv_format)\n```\n\n----------------------------------------\n\nTITLE: Using SQLAlchemy Engine as Credentials in DLT\nDESCRIPTION: This example demonstrates how to use a SQLAlchemy Engine instance instead of credentials for connecting to a SQL database in a DLT pipeline. It creates an engine and passes it to the sql_table resource.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/configuration.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.sql_database import sql_table\nfrom sqlalchemy import create_engine\n\nengine = create_engine(\"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\")\ntable = sql_table(engine, table=\"chat_message\", schema=\"data\")\n```\n\n----------------------------------------\n\nTITLE: Initializing DLT Project for Databricks\nDESCRIPTION: Command to initialize a new DLT project with Databricks configuration\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/databricks.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ndlt init chess databricks\n```\n\n----------------------------------------\n\nTITLE: Enabling Iceberg Tables for Athena\nDESCRIPTION: Configuration to force all tables to use Iceberg format and control their layout in the S3 bucket.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/athena.md#2025-04-14_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[destination.athena]\nforce_iceberg=true\ntable_location_layout=\"{dataset_name}/{table_name}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Sheets Source in TOML\nDESCRIPTION: This snippet shows the configuration for the Google Sheets source in the config.toml file. It includes placeholders for the spreadsheet URL or ID and range names.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[sources.google_sheets]\nspreadsheet_url_or_id = \"Please set me up!\"\nrange_names = [\"Please set me up!\"]\n```\n\n----------------------------------------\n\nTITLE: Applying LanceDB Adapter to Specific Resources\nDESCRIPTION: Example showing how to apply the LanceDB adapter to specific resources in a pipeline with multiple tables.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/lancedb.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nproducts_tables = sql_database().with_resources(\"products\", \"customers\")\n\npipeline = dlt.pipeline(\n        pipeline_name=\"postgres_to_lancedb_pipeline\",\n        destination=\"lancedb\",\n    )\n\n# Apply adapter to the needed resources\nlancedb_adapter(products_tables.products, embed=\"description\")\nlancedb_adapter(products_tables.customers, embed=\"bio\")\n\ninfo = pipeline.run(products_tables)\n```\n\n----------------------------------------\n\nTITLE: Loading Data with Replace Mode\nDESCRIPTION: Example of loading Stripe data using replace mode for specific endpoints and date range.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/stripe.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsource_single = stripe_source(\n    endpoints=(\"Plan\", \"Charge\"),\n    start_date=pendulum.DateTime(2022, 1, 1),\n    end_date=pendulum.DateTime(2022, 12, 31),\n)\nload_info = pipeline.run(source_single)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Running Amazon Kinesis Pipeline\nDESCRIPTION: Command to execute the Amazon Kinesis pipeline script.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/amazon_kinesis.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython kinesis_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Defining Personio Source in Python\nDESCRIPTION: Python function decorated with @dlt.source to define the Personio data source. It initializes the source with API credentials and returns multiple data resources.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/personio.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source(name=\"personio\")\ndef personio_source(\n    client_id: str = dlt.secrets.value,\n    client_secret: str = dlt.secrets.value,\n    items_per_page: int = ITEMS_PER_PAGE,\n) -> Iterable[DltResource]:\n    ...\n    return (\n    employees,\n    absence_types,\n    absences,\n    attendances,\n    projects,\n    document_categories,\n    employees_absences_balance,\n    custom_reports_list,\n    custom_reports,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Snapshot Data to Destination in Python using DLT\nDESCRIPTION: Runs the destination pipeline to load the snapshot data created during replication initialization.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/pg_replication.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndest_pl.run(snapshot)\n```\n\n----------------------------------------\n\nTITLE: DLT Resource Decorator Pipeline Example - Python\nDESCRIPTION: Alternative implementation using the @dlt.resource decorator to define a data resource function.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination-tables.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource\ndef users():\n    yield [\n        {'id': 1, 'name': 'Alice'},\n        {'id': 2, 'name': 'Bob'}\n    ]\n\npipeline = dlt.pipeline(\n    pipeline_name='quick_start',\n    destination='duckdb',\n    dataset_name='mydata'\n)\nload_info = pipeline.run(users)\n```\n\n----------------------------------------\n\nTITLE: Creating Postgres User with Replication Privileges (SQL)\nDESCRIPTION: SQL commands to create a Postgres user with necessary replication privileges. The user is granted LOGIN and REPLICATION attributes, as well as CREATE privilege on the database.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/pg_replication.md#2025-04-14_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE ROLE replication_user WITH LOGIN REPLICATION;\n```\n\nLANGUAGE: sql\nCODE:\n```\nGRANT CREATE ON DATABASE dlt_data TO replication_user;\n```\n\n----------------------------------------\n\nTITLE: Recommended Filesystem Layout Configuration\nDESCRIPTION: The recommended layout structure for filesystem destination that balances efficiency, compatibility, flexibility, and performance.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_31\n\nLANGUAGE: toml\nCODE:\n```\nlayout=\"{table_name}/{load_id}.{file_id}.{ext}\"\n```\n\n----------------------------------------\n\nTITLE: Cleaning Files After Loading in dlt\nDESCRIPTION: Example showing how to get an fsspec client from the filesystem resource after files have been extracted. This can be used for operations like deleting processed files.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/advanced.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.filesystem import filesystem, read_csv\nfrom dlt.sources.filesystem.helpers import fsspec_from_resource\n\n# Get filesystem source.\ngs_resource = filesystem(\"gs://ci-test-bucket/\")\n# Extract files.\npipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\npipeline.run(gs_resource | read_csv())\n# Get fs client.\nfs_client = fsspec_from_resource(gs_resource)\n# Do any operation.\nfs_client.ls(\"ci-test-bucket/standard_source/samples\")\n```\n\n----------------------------------------\n\nTITLE: Loading Custom Resources\nDESCRIPTION: Python code example demonstrating how to load specific resources from the Jira source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/jira.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Run the pipeline\nload_info = pipeline.run(jira().with_resources(\"issues\", \"users\"))\nprint(f\"Load Information: {load_info}\")\n```\n\n----------------------------------------\n\nTITLE: Running the Google Sheets Pipeline\nDESCRIPTION: This command executes the Google Sheets pipeline script to extract data from the configured Google Sheets source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\npython google_sheets_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Triggering a Full Refresh in dlt Pipeline\nDESCRIPTION: Code to trigger a full refresh in the dlt pipeline, which drops the destination table, deletes destination data, and resets the tracking version state.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/ms-sql.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipeline.run(initial_resource, refresh=\"drop_resources\")\n```\n\n----------------------------------------\n\nTITLE: Initializing DLT Pipeline with Schema Paths\nDESCRIPTION: Configuration of a DLT pipeline with import and export schema paths for schema management.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/adjust-a-schema.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndlt.pipeline(\n    import_schema_path=\"schemas/import\",\n    export_schema_path=\"schemas/export\",\n    pipeline_name=\"chess_pipeline\",\n    destination='duckdb',\n    dataset_name=\"games_data\"\n)\n```\n\n----------------------------------------\n\nTITLE: Azure Blob Storage Configuration Examples\nDESCRIPTION: Configuration examples for setting up Azure Blob Storage as a staging destination for Databricks, showing TOML, environment variables, and code-based methods.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/databricks.md#2025-04-14_snippet_10\n\nLANGUAGE: toml\nCODE:\n```\n# secrets.toml\n[destination.filesystem]\nbucket_url = \"abfss://container_name@storage_account_name.dfs.core.windows.net/path\"\n\n[destination.filesystem.credentials]\nazure_storage_account_name=\"XXX\"\nazure_storage_account_key=\"XXX\"\n```\n\nLANGUAGE: shell\nCODE:\n```\nexport DESTINATIONS__FILESYSTEM__BUCKET_URL=\"abfss://container_name@storage_account_name.dfs.core.windows.net/path\"\nexport DESTINATIONS__FILESYSTEM__CREDENTIALS__AZURE_STORAGE_ACCOUNT_NAME=\"XXX\"\nexport DESTINATIONS__FILESYSTEM__CREDENTIALS__AZURE_STORAGE_ACCOUNT_KEY=\"XXX\"\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Do not set up the secrets directly in the code!\n# What you can do is reassign env variables.\nos.environ[\"DESTINATIONS__FILESYSTEM__BUCKET_URL\"] = \"abfss://container_name@storage_account_name.dfs.core.windows.net/path\"\nos.environ[\"DESTINATIONS__FILESYSTEM__CREDENTIALS__AZURE_STORAGE_ACCOUNT_NAME\"] = os.environ.get(\"AZURE_STORAGE_ACCOUNT_NAME\")\nos.environ[\"DESTINATIONS__FILESYSTEM__CREDENTIALS__AZURE_STORAGE_ACCOUNT_KEY\"] = os.environ.get(\"AZURE_STORAGE_ACCOUNT_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Creating main.py for Google Cloud Function\nDESCRIPTION: Python code for the main.py file that defines the Cloud Function entry point. It imports and calls the load_databases function from the notion_pipeline module.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-google-cloud-functions.md#2025-04-14_snippet_1\n\nLANGUAGE: py\nCODE:\n```\nfrom notion_pipeline import load_databases\n\ndef pipeline_notion(request):\n  load_databases()\n  return \"Pipeline run successfully!\"\n```\n\n----------------------------------------\n\nTITLE: Loading Matomo Reports and Visits Data in Python\nDESCRIPTION: This snippet shows how to load both report data and visit data from Matomo using the matomo_reports() and matomo_visits() functions, and then run them through the pipeline together.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/matomo.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndata_reports = matomo_reports()\ndata_events = matomo_visits()\nload_info = pipeline.run([data_reports, data_events])\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Implementing Customers Resource for Google Ads Data in Python\nDESCRIPTION: Defines a resource function that retrieves customer data from Google Ads. The function takes a client resource and customer ID as parameters and yields customer data as dictionaries.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_ads.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(write_disposition=\"replace\")\ndef customers(\n    client: Resource, customer_id: str = dlt.secrets.value\n) -> Iterator[TDataItem]:\n    \"\"\"\n    Fetches customer data from the Google Ads service and\n    yields each customer as a dictionary.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Applying Hints for Merge Operations\nDESCRIPTION: Example of applying hints to control write disposition and merge behavior in pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/basic.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.filesystem import filesystem, read_csv\n\nfilesystem_pipe = filesystem(bucket_url=\"file://Users/admin/Documents/csv_files\", file_glob=\"*.csv\") | read_csv()\n# Tell dlt to merge on date\nfilesystem_pipe.apply_hints(write_disposition=\"merge\", merge_key=\"date\")\n\n# We load the data into the table_name table\npipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\nload_info = pipeline.run(filesystem_pipe.with_name(\"table_name\"))\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Simple Data Transformation and Persistence\nDESCRIPTION: Demonstrates how to create a new table from an existing user table by selecting specific columns and excluding private information using Arrow tables.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/transformations/python.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"users_pipeline\",\n    destination=\"duckdb\",\n    dataset_name=\"users_raw\",\n    dev_mode=True\n)\n\n# get user relation with only a few columns selected, but omitting email and name\nusers = pipeline.dataset().users.select(\"age\", \"amount_spent\", \"country\")\n\n# load the data into a new table called users_clean in the same dataset\npipeline.run(users.iter_arrow(chunk_size=1000), table_name=\"users_clean\")\n```\n\n----------------------------------------\n\nTITLE: Run Pipeline\nDESCRIPTION: Command to execute the Airtable pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/airtable.md#2025-04-14_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npython airtable_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Using Merge Write Disposition with Weaviate\nDESCRIPTION: Loading data with the 'merge' write disposition and a primary key to update existing objects or create new ones in Weaviate.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/weaviate.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ninfo = pipeline.run(\n    weaviate_adapter(\n        movies,\n        vectorize=\"title\",\n    ),\n    primary_key=\"document_id\",\n    write_disposition=\"merge\"\n)\n```\n\n----------------------------------------\n\nTITLE: Running DBT Project with Logging\nDESCRIPTION: Commands showing how to run the generated dbt project with different logging configurations.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/transformations/dbt-transformations.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython run_<pipeline_name>_dbt.py\nRUNTIME__LOG_LEVEL=INFO python run_<pipeline_name>_dbt.py\n```\n\n----------------------------------------\n\nTITLE: Defining Events Resource for HubSpot Objects in Python\nDESCRIPTION: Python function that defines a resource for loading web analytics events for specific HubSpot objects.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/hubspot.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource\ndef hubspot_events_for_objects(\n     object_type: THubspotObjectType,\n     object_ids: List[str],\n     api_key: str = dlt.secrets.value,\n     start_date: DateTime = START_DATE,\n) -> DltResource:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Loading Data from Explicit Range Names in Google Sheets\nDESCRIPTION: Demonstrates how to load data from specific named ranges in a Google Spreadsheet while disabling sheet and named range retrieval.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nload_data = google_spreadsheet(\n        \"https://docs.google.com/spreadsheets/d/1HhWHjqouQnnCIZAFa2rL6vT91YRN8aIhts22SUUR580/edit#gid=0\", # Spreadsheet URL\n        range_names=[\"range_name1\", \"range_name2\"], # Range names\n        get_sheets=False,\n        get_named_ranges=False,\n)\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Configuring Pandas Backend for SQL Database\nDESCRIPTION: Example showing Pandas backend configuration with custom decimal handling and chunk size settings\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/configuration.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nimport sqlalchemy as sa\nfrom dlt.sources.sql_database import sql_database\n\npipeline = dlt.pipeline(\n    pipeline_name=\"rfam_cx\", destination=\"postgres\", dataset_name=\"rfam_data_pandas_2\"\n)\n\ndef _double_as_decimal_adapter(table: sa.Table) -> sa.Table:\n    \"\"\"Emits decimals instead of floats.\"\"\"\n    for column in table.columns.values():\n        if isinstance(column.type, sa.Float):\n            column.type.asdecimal = True\n    return table\n\nsql_alchemy_source = sql_database(\n    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam?&binary_prefix=true\",\n    backend=\"pandas\",\n    table_adapter_callback=_double_as_decimal_adapter,\n    chunk_size=100000,\n    backend_kwargs={\"coerce_float\": False, \"dtype_backend\": \"numpy_nullable\"},\n).with_resources(\"family\", \"genome\")\n\ninfo = pipeline.run(sql_alchemy_source)\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Setting File Format via Resource Decorator\nDESCRIPTION: Specify the file format directly in the resource decorator for a particular resource function.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/file-formats/_set_the_format.mdx#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(file_format=\"{props.file_type}\")\\ndef generate_rows(nr):\\n    pass\n```\n\n----------------------------------------\n\nTITLE: Installing Pipeline Dependencies\nDESCRIPTION: Command to install all necessary dependencies for the Slack pipeline using pip and the requirements.txt file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/slack.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Partitioning in Python\nDESCRIPTION: Set up partitioning for a resource using Python decorators.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/iceberg.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(\n  columns={\"foo\": {\"partition\": True}}\n)\ndef my_resource():\n    ...\n\npipeline = dlt.pipeline(\"loads_iceberg\", destination=\"iceberg\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Separate Google Credentials in TOML\nDESCRIPTION: This snippet shows how to set up separate Google credentials for source and destination in a TOML file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_11\n\nLANGUAGE: toml\nCODE:\n```\n# Google Sheet credentials\n[sources.credentials]\nclient_email = \"<client_email from services.json>\"\nprivate_key = \"<private_key from services.json>\"\nproject_id = \"<project_id from services json>\"\n\n# BigQuery credentials\n[destination.credentials]\nclient_email = \"<client_email from services.json>\"\nprivate_key = \"<private_key from services.json>\"\nproject_id = \"<project_id from services json>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring ClickHouse Credentials in TOML\nDESCRIPTION: TOML configuration for ClickHouse credentials including database connection details and security settings\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/clickhouse.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[destination.clickhouse.credentials]\ndatabase = \"dlt\"\nusername = \"dlt\"\npassword = \"Dlt*12345789234567\"\nhost = \"localhost\"\nport = 9000\nhttp_port = 8443\nsecure = 1\n```\n\n----------------------------------------\n\nTITLE: Complete MS SQL Replication Script with dlt+\nDESCRIPTION: Full Python script demonstrating initial load and incremental loading for MS SQL replication using dlt+.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/ms-sql.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\nfrom sqlalchemy import create_engine\n\nfrom dlt.sources.sql_database import sql_table\nfrom dlt_plus.sources.mssql import (\n    create_change_tracking_table,\n    get_current_change_tracking_version,\n)\n\n\ndef single_table_initial_load(connection_url: str, schema_name: str, table_name: str) -> None:\n    \"\"\"Performs an initial full load and sets up tracking version and incremental loads\"\"\"\n    # Create a new pipeline\n    pipeline = dlt.pipeline(\n        pipeline_name=f\"{schema_name}_{table_name}_sync\",\n        destination=\"duckdb\",\n        dataset_name=schema_name,\n    )\n\n    # Explicit database connection\n    engine = create_engine(connection_url, isolation_level=\"SNAPSHOT\")\n\n    # Initial full load\n    initial_resource = sql_table(\n        credentials=engine,\n        schema=schema_name,\n        table=table_name,\n        reflection_level=\"full\",\n        write_disposition=\"merge\",\n    )\n\n    # Get the current tracking version before you run the pipeline to make sure\n    # you do not miss any records\n    tracking_version = get_current_change_tracking_version(engine)\n    print(f\"will track from: {tracking_version}\")  # noqa\n\n    # Run the pipeline for the initial load\n    # NOTE: we always drop data and state from the destination on initial load\n    print(pipeline.run(initial_resource, refresh=\"drop_resources\"))  # noqa\n\n    # Incremental loading resource\n    incremental_resource = create_change_tracking_table(\n        credentials=engine,\n        table=table_name,\n        schema=schema_name,\n        initial_tracking_version=tracking_version,\n    )\n\n    # Run the pipeline for incremental load\n    print(pipeline.run(incremental_resource))  # noqa\n\n\ndef single_table_incremental_load(connection_url: str, schema_name: str, table_name: str) -> None:\n    \"\"\"Continues loading incrementally\"\"\"\n    # Make sure you use the same pipeline and dataset names in order to continue incremental\n    # loading.\n    pipeline = dlt.pipeline(\n        pipeline_name=f\"{schema_name}_{table_name}_sync\",\n        destination=\"duckdb\",\n        dataset_name=schema_name,\n    )\n\n    engine = create_engine(connection_url, isolation_level=\"SNAPSHOT\")\n    # We do not need to pass the tracking version anymore\n    incremental_resource = create_change_tracking_table(\n        credentials=engine,\n        table=table_name,\n        schema=schema_name,\n    )\n    print(pipeline.run(incremental_resource))  # noqa\n\n\nif __name__ == \"__main__\":\n    # Change Tracking already enabled here\n    test_db = \"my_database83ed099d2d98a3ccfa4beae006eea44c\"\n    # A test run with a local mssql instance\n    connection_url = (\n        f\"mssql+pyodbc://sa:Strong%21Passw0rd@localhost:1433/{test_db}\"\n        \"?driver=ODBC+Driver+18+for+SQL+Server&TrustServerCertificate=yes\"\n    )\n    single_table_initial_load(\n        connection_url,\n        \"my_dlt_source\",\n        \"app_user\",\n    )\n    single_table_incremental_load(\n        connection_url,\n        \"my_dlt_source\",\n        \"app_user\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-Side Encryption with AWS KMS in dlt Pipeline\nDESCRIPTION: This code demonstrates how to encrypt specific fields in a nested data structure using AWS KMS before loading data with dlt. It uses the AWS Encryption SDK to encrypt the 'security_key' field for each child record.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/transformations/encryption.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport boto3\nimport dlt\nimport aws_encryption_sdk\nfrom aws_encryption_sdk import CommitmentPolicy\nfrom aws_encryption_sdk.key_providers.kms import KMSMasterKey\n\n# Define the KMS Key ARN\nKMS_KEY_ARN = (\n    \"arn:aws:kms:<region>:<number>:key/<key>\"\n)\n\n# Create a boto3 client for AWS KMS\nkms_client = boto3.client(\n    \"kms\",\n    region_name=\"<region-name>\",\n    aws_access_key_id=\"your aws access key\",\n    aws_secret_access_key=\"your aws secret key\",\n)\n\n# Create the KMS Master Key\nmaster_key = KMSMasterKey(key_id=KMS_KEY_ARN, client=kms_client)\n\n# Instantiate the AWS Encryption SDK client\nclient = aws_encryption_sdk.EncryptionSDKClient(\n    commitment_policy=CommitmentPolicy.REQUIRE_ENCRYPT_REQUIRE_DECRYPT\n)\n\n#Encryption function\ndef encryption_func(record):\n    \"\"\"\n    Encrypts the 'security_key' for each child in a given record.\n    \"\"\"\n    for child in record.get(\"children\", []):\n        # Convert the security key to bytes for encryption\n        key_to_encrypt = str(child[\"security_key\"]).encode(\"utf-8\")\n        try:\n            # Encrypt the security key using the provided master key\n            ciphertext, _ = client.encrypt(\n                source=key_to_encrypt, key_provider=master_key\n            )\n            # Replace the plain key with the encrypted data in hex format\n            child[\"security_key\"] = ciphertext.hex()\n        except Exception as e:\n            print(\n                f\"Failed to encrypt security key for child_id {child['child_id']}: {e}\"\n            )\n            raise\n    return record\n\n\n# Define the raw data structure\nraw_data = [\n    {\n        \"parent_id\": 1,\n        \"parent_name\": \"Alice\",\n        \"children\": [\n            {\"child_id\": 1, \"child_name\": \"Child 1\", \"security_key\": 12345},\n            {\"child_id\": 2, \"child_name\": \"Child 2\", \"security_key\": 67891},\n        ],\n    },\n    {\n        \"parent_id\": 2,\n        \"parent_name\": \"Bob\",\n        \"children\": [\n            {\"child_id\": 3, \"child_name\": \"Child 3\", \"security_key\": 999111}\n        ],\n    },\n]\n\n#dlt resource\n@dlt.resource(name=\"data_test\", write_disposition={\"disposition\": \"replace\"})\ndef data_source():\n    yield from raw_data\n\n\nif __name__ == \"__main__\":\n    # Apply the encryption transformation using add_map\n    data_encrypted = data_source().add_map(encryption_func)\n\n    # Configure and run the pipeline\n    pipeline = dlt.pipeline(\n        pipeline_name=\"pipeline\",\n        destination=\"duckdb\",\n        dataset_name=\"dataset\",\n    )\n    load_info = pipeline.run(data_encrypted)\n    print(load_info)\n```\n\n----------------------------------------\n\nTITLE: Pipeline Dependencies Installation\nDESCRIPTION: Command to install required dependencies for the pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_ads.md#2025-04-14_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Matomo Reports Source Implementation\nDESCRIPTION: Python function for executing and loading Matomo reports with configurable parameters\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/matomo.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source(max_table_nesting=2)\ndef matomo_reports(\n    api_token: str = dlt.secrets.value,\n    url: str = dlt.config.value,\n    queries: List[DictStrAny] = dlt.config.value,\n    site_id: int = dlt.config.value,\n) -> Iterable[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Initializing DLT Pipeline Configuration for Asana\nDESCRIPTION: Sets up a DLT pipeline with custom name, destination database, and dataset name. Configures the basic pipeline structure for loading Asana data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/asana.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"asana_pipeline\",  # Use a custom name if desired\n    destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"asana_dataset\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Nested JSON with Arrow\nDESCRIPTION: Example of loading nested JSON data using PyArrow and BigQuery schema autodetection\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/bigquery.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.json as paj\n\nimport dlt\nfrom dlt.destinations.adapters import bigquery_adapter\n\n@dlt.resource(name=\"cve\")\ndef load_cve():\n  with open(\"cve.json\", 'rb') as f:\n    # autodetect arrow schema and yield arrow table\n    yield paj.read_json(f)\n\npipeline = dlt.pipeline(\"load_json_struct\", destination=\"bigquery\")\npipeline.run(\n  bigquery_adapter(load_cve(), autodetect_schema=True)\n)\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Configuration in Python\nDESCRIPTION: Example of setting configuration values using environment variables in Python code\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PIPELINE_NAME__SOURCES__SOURCE_MODULE_NAME__SOURCE_FUNCTION_NAME__ARGUMENT_NAME\"] = \"some_value\"\n```\n\n----------------------------------------\n\nTITLE: Configuring IAM Role for S3 Staging Authentication\nDESCRIPTION: Configuration for using an IAM role to authorize access to S3 for staging without forwarding AWS credentials.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/redshift.md#2025-04-14_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\n[destination]\nstaging_iam_role=\"arn:aws:iam::...\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Athena Iceberg Table Partitioning in DLT Pipeline\nDESCRIPTION: This code snippet demonstrates how to use the athena_adapter to partition an Iceberg table in Athena. It creates a resource function that yields data, then applies partitioning hints using the athena_adapter. The table is partitioned by category and month of the created_at date.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/athena.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import date\n\nimport dlt\nfrom dlt.destinations.adapters import athena_partition, athena_adapter\n\ndata_items = [\n    (1, \"A\", date(2021, 1, 1)),\n    (2, \"A\", date(2021, 1, 2)),\n    (3, \"A\", date(2021, 1, 3)),\n    (4, \"A\", date(2021, 2, 1)),\n    (5, \"A\", date(2021, 2, 2)),\n    (6, \"B\", date(2021, 1, 1)),\n    (7, \"B\", date(2021, 1, 2)),\n    (8, \"B\", date(2021, 1, 3)),\n    (9, \"B\", date(2021, 2, 1)),\n    (10, \"B\", date(2021, 3, 2)),\n]\n\n@dlt.resource(table_format=\"iceberg\")\ndef partitioned_data():\n    yield [{\"id\": i, \"category\": c, \"created_at\": d} for i, c, d in data_items]\n\n# Add partitioning hints to the table\nathena_adapter(\n    partitioned_data,\n    partition=[\n        # Partition per category and month\n        \"category\",\n        athena_partition.month(\"created_at\"),\n    ],\n)\n\n\npipeline = dlt.pipeline(\"athena_example\")\npipeline.run(partitioned_data)\n```\n\n----------------------------------------\n\nTITLE: Removing Nullability from SQL Table Schema in Python\nDESCRIPTION: This snippet demonstrates how to remove nullability information from a reflected SQL table schema using dlt. It uses the sql_table function with a custom adapter callback to remove nullability.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/advanced.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.sql_database import sql_table, remove_nullability_adapter\n\nread_table = sql_table(\n    table=\"chat_message\",\n    reflection_level=\"full_with_precision\",\n    table_adapter_callback=remove_nullability_adapter,\n)\nprint(read_table.compute_table_schema())\n```\n\n----------------------------------------\n\nTITLE: Loading Trace Information to Destination in Python\nDESCRIPTION: This Python code snippet demonstrates how to retrieve the trace of the last pipeline run and load it into a '_trace' table in the destination. This allows for detailed analysis of the extract, normalize, and load steps.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/running-in-production/monitoring.md#2025-04-14_snippet_1\n\nLANGUAGE: py\nCODE:\n```\n# Create a pipeline with the specified name, destination, and dataset\n# Run the pipeline\n\n# Get the trace of the last run of the pipeline\n# The trace contains timing information on extract, normalize, and load steps\ntrace = pipeline.last_trace\n\n# Load the trace information into a table named \"_trace\" in the destination\npipeline.run([trace], table_name=\"_trace\")\n```\n\n----------------------------------------\n\nTITLE: Configuring DLT Pipeline Working Directory in Python\nDESCRIPTION: This snippet demonstrates how to set a custom working directory for a dlt pipeline. It shows how to use the 'pipelines_dir' argument to store pipeline artifacts in a specific location, useful for managing different environments.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/pipeline.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.common.pipeline import get_dlt_pipelines_dir\n\ndev_pipelines_dir = os.path.join(get_dlt_pipelines_dir(), \"dev\")\npipeline = dlt.pipeline(destination=\"duckdb\", dataset_name=\"sequence\", pipelines_dir=dev_pipelines_dir)\n```\n\n----------------------------------------\n\nTITLE: Matomo Visits Source Implementation\nDESCRIPTION: Python function for loading visit data with customizable timeframes and parameters\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/matomo.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source()\ndef matomo_visits(\n    api_token: str = dlt.secrets.value,\n    url: str = dlt.config.value,\n    live_events_site_id: int = dlt.config.value,\n    initial_load_past_days: int = 10,\n    visit_timeout_seconds: int = 1800,\n    visit_max_duration_seconds: int = 3600,\n    get_live_event_visitors: bool = False,\n) -> List[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Setting Naming Convention for Snowflake Destination in Python\nDESCRIPTION: This snippet demonstrates how to set the preferred naming convention for a Snowflake destination to 'sql_cs' to enable case-sensitive mode. It uses the dlt library to configure the Snowflake destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nsnow_ = dlt.destinations.snowflake(naming_convention=\"sql_cs_v1\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Column Removal Function\nDESCRIPTION: Defines a function that removes specified columns from a dictionary document. Takes a document and list of columns to remove as parameters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/customising-pipelines/removing_columns.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict, List, Optional\n\ndef remove_columns(doc: Dict, remove_columns: Optional[List[str]] = None) -> Dict:\n    if remove_columns is None:\n        remove_columns = []\n\n    # Iterating over the list of columns to be removed\n    for column_name in remove_columns:\n        # Removing the column if it exists in the document\n        if column_name in doc:\n            del doc[column_name]\n\n    return doc\n```\n\n----------------------------------------\n\nTITLE: Configuring Named Destination Instance in Python\nDESCRIPTION: Example of creating a named destination instance with explicit configuration parameters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.destinations import filesystem\n# create destination with explicit parameters\nfs = filesystem(bucket_url=\"az://dlt-azure-bucket\", name=\"production_az_bucket\")\n# pass destination instance to pipeline\npipeline = dlt.pipeline(pipeline_name=\"pipeline_name\", destination=fs)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Row Counts for dlt Pipeline Tables\nDESCRIPTION: This command displays the names of tables loaded by a dlt pipeline and the number of rows in each table. It provides a quick overview of the data loaded into the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/running-in-production/monitoring.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> trace\n```\n\n----------------------------------------\n\nTITLE: Defining Sample Data Resource in Python\nDESCRIPTION: Implementation of a DLT resource that yields sample enriched data containing user information and device prices\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/currency_conversion_data_enrichment.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource()\ndef enriched_data_part_two():\n    data_enrichment_part_one = [\n        {\n             \"user_id\": 1,\n             \"device_name\": \"Sony Experia XZ\",\n             \"page_referer\": \"https://b2venture.lightning.force.com/\",\n             \"device_price_usd\": 313.01,\n             \"price_updated_at\": \"2024-01-15 04:08:45.088499+00:00\"\n        },\n    ]\n    \"\"\"\n    Similar data for the other users.\n    \"\"\"\n    for user_data in data_enrichment_part_one:\n        yield user_data\n```\n\n----------------------------------------\n\nTITLE: Defining Slack Source in Python\nDESCRIPTION: Python function defining the Slack source for dlt. It sets up parameters for authentication, date range, and channel selection, and returns an iterable of DltResource objects.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/slack.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source(name=\"slack\", max_table_nesting=2)\ndef slack_source(\n    page_size: int = MAX_PAGE_SIZE,\n    access_token: str = dlt.secrets.value,\n    start_date: Optional[TAnyDateTime] = START_DATE,\n    end_date: Optional[TAnyDateTime] = None,\n    selected_channels: Optional[List[str]] = dlt.config.value,\n) -> Iterable[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Custom Retry Condition Implementation\nDESCRIPTION: Implementation of a custom retry condition based on response content rather than HTTP status codes\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/http/requests.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.helpers import requests\n\ndef retry_if_error_key(response: Optional[requests.Response], exception: Optional[BaseException]) -> bool:\n    \"\"\"Decide whether to retry the request based on whether\n    the json response contains an `error` key\n    \"\"\"\n    if response is None:\n        # Fall back on the default exception predicate.\n        return False\n    data = response.json()\n    return 'error' in data\n\nhttp_client = Client(\n    retry_condition=retry_if_error_key\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Command to install all the necessary dependencies listed in requirements.txt, which includes dlt with Snowflake extras.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Defining Access Logs Resource in Python\nDESCRIPTION: Python function defining the access logs resource for the Slack pipeline. It retrieves access logs with specific configuration for primary key and write disposition.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/slack.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(\n    name=\"access_logs\",\n    selected=False,\n    primary_key=\"user_id\",\n    write_disposition=\"append\",\n)\ndef logs_resource() -> Iterable[TDataItem]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Setting Shared Google Credentials as Environment Variables\nDESCRIPTION: This snippet shows how to set shared Google credentials as environment variables for dlt configuration.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport CREDENTIALS__CLIENT_EMAIL=\"<client_email_both_for_destination_and_source>\"\nexport CREDENTIALS__PRIVATE_KEY=\"<private_key_both_for_destination_and_source>\"\nexport CREDENTIALS__PROJECT_ID=\"<project_id_both_for_destination_and_source>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Redshift Credentials in TOML File\nDESCRIPTION: Configuration example for setting up Redshift credentials in the .dlt/secrets.toml file, including database, password, username, host, port, and connection timeout.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/redshift.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[destination.redshift.credentials]\ndatabase = \"please set me up!\" # Copy your database name here\npassword = \"please set me up!\" # Keep your Redshift db instance password here\nusername = \"please set me up!\" # Keep your Redshift db instance username here\nhost = \"please set me up!\" # Copy your Redshift host from cluster endpoint here\nport = 5439\nconnect_timeout = 15 # Enter the timeout value\n```\n\n----------------------------------------\n\nTITLE: Defining the Freshdesk Source in Python\nDESCRIPTION: Python function definition for the Freshdesk source that retrieves data from specified endpoints. It supports pagination, incremental loading, and configurable parameters for domain and authentication.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/freshdesk.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source()\ndef freshdesk_source(\n    endpoints: Optional[List[str]] = None,\n    per_page: int = 100,\n    domain: str = dlt.secrets.value,\n    api_secret_key: str = dlt.secrets.value,\n) -> Iterable[DltResource]:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Running dlt Pipeline for Postgres Replication (Shell)\nDESCRIPTION: Shell commands to install dependencies, run the Postgres replication pipeline, and verify the loaded data using dlt CLI commands.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/pg_replication.md#2025-04-14_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\nLANGUAGE: sh\nCODE:\n```\npython pg_replication_pipeline.py\n```\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Manual Config Type Casting\nDESCRIPTION: Example showing how to request and cast configuration values to specific types.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/advanced.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncredentials = dlt.secrets.get(\"my_section.gcp_credentials\", GcpServiceAccountCredentials)\n```\n\n----------------------------------------\n\nTITLE: Implementing Employees Absences Balance Transformer\nDESCRIPTION: Python transformer function to process employee absence balances. It uses the @dlt.transformer decorator and @dlt.defer for parallel execution in a thread pool.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/personio.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dlt.transformer(\n    data_from=employees,\n    write_disposition=\"merge\",\n    primary_key=[\"employee_id\", \"id\"],\n)\n@dlt.defer\ndef employees_absences_balance(employees_item: TDataItem) -> Iterable[TDataItem]:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Loading Specific Collections from MongoDB Database in Python\nDESCRIPTION: This snippet demonstrates how to load specific collections from a MongoDB database using DLT. It selects 'collection_1' and 'collection_2' for loading.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mongodb.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nload_data = mongodb().with_resources(\"collection_1\", \"collection_2\")\nload_info = pipeline.run(load_data, write_disposition=\"replace\")\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: MongoDB Source Function Definition\nDESCRIPTION: Python function definition for MongoDB source that loads data from multiple collections.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mongodb.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef mongodb(\n    connection_url: str = dlt.secrets.value,\n    database: Optional[str] = dlt.config.value,\n    collection_names: Optional[List[str]] = dlt.config.value,\n    incremental: Optional[dlt.sources.incremental] = None,  # type: ignore[type-arg]\n    write_disposition: Optional[str] = dlt.config.value,\n) -> Iterable[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Configuring LanceDB Destination in TOML\nDESCRIPTION: TOML configuration for setting up LanceDB as a destination in dlt. Includes settings for the database URI, embedding model provider, and API keys.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/lancedb.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[destination.lancedb]\nlance_uri = \".lancedb\"\nembedding_model_provider = \"ollama\"\nembedding_model = \"mxbai-embed-large\"\nembedding_model_provider_host = \"http://localhost:11434\"  # Optional: custom endpoint for providers that support it\n\n[destination.lancedb.credentials]\napi_key = \"api_key\" # API key to connect to LanceDB Cloud. Leave out if you are using LanceDB OSS.\nembedding_model_provider_api_key = \"embedding_model_provider_api_key\" # Not needed for providers that don't need authentication (ollama, sentence-transformers).\n```\n\n----------------------------------------\n\nTITLE: Initializing DLT Project (Shell)\nDESCRIPTION: Initializes a new DLT project with GitHub API source and DuckDB destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/create-a-pipeline.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init github_api duckdb\n```\n\n----------------------------------------\n\nTITLE: Installing dlt with BigQuery Dependencies\nDESCRIPTION: Command to install dlt library with BigQuery support\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/bigquery.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[bigquery]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring PyArrow Backend for SQL Database\nDESCRIPTION: Example of setting up a DLT pipeline using PyArrow backend with custom decimal type handling and UTC timezone configuration\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/configuration.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nimport sqlalchemy as sa\nfrom dlt.sources.sql_database import sql_database\n\npipeline = dlt.pipeline(\n    pipeline_name=\"rfam_cx\", destination=\"postgres\", dataset_name=\"rfam_data_arrow\"\n)\n\ndef _double_as_decimal_adapter(table: sa.Table) -> sa.Table:\n    \"\"\"Emits decimals instead of floats.\"\"\"\n    for column in table.columns.values():\n        if isinstance(column.type, sa.Float):\n            column.type.asdecimal = False\n    return table\n\nsql_alchemy_source = sql_database(\n    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam?&binary_prefix=true\",\n    backend=\"pyarrow\",\n    backend_kwargs={\"tz\": \"UTC\"},\n    table_adapter_callback=_double_as_decimal_adapter\n).with_resources(\"family\", \"genome\")\n\ninfo = pipeline.run(sql_alchemy_source)\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Response Processing for REST API in Python\nDESCRIPTION: This example shows how to implement custom response processing functions for a REST API source. It includes functions to set encoding and modify response content, demonstrating advanced usage of response_actions.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/rest_api/advanced.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom requests.models import Response\nfrom dlt.common import json\n\ndef set_encoding(response, *args, **kwargs):\n    # Sets the encoding in case it's not correctly detected\n    response.encoding = 'windows-1252'\n    return response\n\n\ndef add_and_remove_fields(response: Response, *args, **kwargs) -> Response:\n    payload = response.json()\n    for record in payload[\"data\"]:\n        record[\"custom_field\"] = \"foobar\"\n        record.pop(\"email\", None)\n    modified_content: bytes = json.dumps(payload).encode(\"utf-8\")\n    response._content = modified_content\n    return response\n\n\nsource_config = {\n    \"client\": {\n        # ...\n    },\n    \"resources\": [\n        {\n            \"name\": \"issues\",\n            \"endpoint\": {\n                \"path\": \"issues\",\n                \"response_actions\": [\n                    set_encoding,\n                    {\n                        \"status_code\": 200,\n                        \"content\": \"some text\",\n                        \"action\": add_and_remove_fields,\n                    },\n                ],\n            },\n        },\n    ],\n}\n```\n\n----------------------------------------\n\nTITLE: Initial Schema Data Example in Python\nDESCRIPTION: Example showing initial data structure with integer ID type that will be used to demonstrate schema evolution.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndata = [\n  {\"id\": 1, \"human_name\": \"Alice\"}\n]\n```\n\n----------------------------------------\n\nTITLE: Using Weaviate Adapter with Multiple Fields\nDESCRIPTION: Example of using the weaviate_adapter function with multiple fields to vectorize and custom tokenization settings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/weaviate.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nweaviate_adapter(\n    resource,\n    vectorize=[\"title\", \"description\"],\n    tokenization={\"title\": \"word\", \"description\": \"whitespace\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring MS SQL credentials in TOML format\nDESCRIPTION: Example configuration for MS SQL Server credentials in the .dlt/secrets.toml file, including database connection parameters and ODBC settings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/mssql.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[destination.mssql.credentials]\ndatabase = \"dlt_data\"\nusername = \"loader\"\npassword = \"<password>\"\nhost = \"loader.database.windows.net\"\nport = 1433\nconnect_timeout = 15\n[destination.mssql.credentials.query]\n# trust self-signed SSL certificates\nTrustServerCertificate=\"yes\"\n# require SSL connection\nEncrypt=\"yes\"\n# send large string as VARCHAR, not legacy TEXT\nLongAsMax=\"yes\"\n```\n\n----------------------------------------\n\nTITLE: Defining Notion Databases Source in Python\nDESCRIPTION: Python function definition for the notion_databases source, which retrieves data from Notion databases. It accepts optional database IDs and the API key as parameters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/notion.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef notion_databases(\n    database_ids: Optional[List[Dict[str, str]]] = None,\n    api_key: str = dlt.secrets.value,\n) -> Iterator[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Partitioning and Sort in Python\nDESCRIPTION: Python code example showing how to configure Apache Iceberg table partitions and local sort properties using DLT decorators.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/dremio.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.common.schema import TColumnSchema\n\n@dlt.resource(\n    table_name=\"my_table\",\n    columns=dict(\n        foo=TColumnSchema(partition=True),\n        bar=TColumnSchema(partition=True),\n        baz=TColumnSchema(sort=True),\n    ),\n)\ndef my_table_resource():\n  ...\n```\n\n----------------------------------------\n\nTITLE: Setting Table Format in Python Resource Definition\nDESCRIPTION: Example of setting the table format to Delta when defining a resource in Python.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/delta-iceberg.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(table_format=\"delta\")\ndef my_delta_resource():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Handling PipelineStepFailed Exception in Python\nDESCRIPTION: The run, extract, normalize, and load methods raise PipelineStepFailed exception when a step in the pipeline fails. This allows for proper error handling in the pipeline execution process.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/technical/general_usage.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    dlt.run(data, destination, dataset_name)\nexcept PipelineStepFailed as e:\n    print(f\"Pipeline step failed: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Setting HOME Environment Variable for Troubleshooting\nDESCRIPTION: Python code to set the HOME environment variable to resolve connection issues with MotherDuck.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/motherduck.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"HOME\"] = \"/tmp\"\n```\n\n----------------------------------------\n\nTITLE: Airtable Resource Definition\nDESCRIPTION: Python function definition for retrieving data from a single Airtable table.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/airtable.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pyairtable\n\ndef airtable_resource(\n    api: pyairtable.Api,\n    base_id: str,\n    table: Dict[str, Any],\n) -> DltResource:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Applying Column-wise Filtering in SQL Database Source (Python)\nDESCRIPTION: Demonstrates how to use query_adapter_callback to filter data based on specific columns before extraction in a sql_database source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/usage.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.sql_database import sql_database\n\ndef query_adapter_callback(query, table):\n    if table.name == \"orders\":\n        # Only select rows where the column customer_id has value 1\n        return query.where(table.c.customer_id==1)\n    # Use the original query for other tables\n    return query\n\nsource = sql_database(\n    query_adapter_callback=query_adapter_callback\n).with_resources(\"orders\")\n```\n\n----------------------------------------\n\nTITLE: Explicit Credentials Configuration in Python\nDESCRIPTION: Shows how to pass explicit credentials when creating a destination instance.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.destinations import redshift\nfrom dlt.common.configuration.specs import ConnectionStringCredentials\n\nredshift_dest = redshift(credentials=ConnectionStringCredentials(\"postgresql://admin:admin@localhost/postgres\"))\n```\n\n----------------------------------------\n\nTITLE: Example SQL Query for Incremental Loading\nDESCRIPTION: Demonstrates the SQL query generated for incremental loading with inclusive filtering on the start value.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/advanced.md#2025-04-14_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM family\nWHERE last_modified >= :start_value\nORDER BY last_modified ASC\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Write Disposition in Pipeline Run\nDESCRIPTION: Example of configuring the merge write disposition, primary key, and merge key when running a dlt pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/lancedb.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipeline.run(\n  lancedb_adapter(\n    my_new_rag_docs,\n    merge_key=\"doc_id\"\n  ),\n  write_disposition={\"disposition\": \"merge\", \"strategy\": \"upsert\"},\n  primary_key=[\"doc_id\", \"chunk_id\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Setting File Format via Pipeline Run Command\nDESCRIPTION: Configure the loader file format directly in the pipeline.run() method using the loader_file_format parameter.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/file-formats/_set_the_format.mdx#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ninfo = pipeline.run(some_source(), loader_file_format=\"{props.file_type}\")\n```\n\n----------------------------------------\n\nTITLE: Applying Weaviate Adapter to Specific Resources\nDESCRIPTION: Example showing how to correctly apply the weaviate_adapter to individual resources in a database source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/weaviate.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nproducts_tables = sql_database().with_resources(\"products\", \"customers\")\n\npipeline = dlt.pipeline(\n        pipeline_name=\"postgres_to_weaviate_pipeline\",\n        destination=\"weaviate\",\n    )\n\n# Apply adapter to the needed resources\nweaviate_adapter(products_tables.products, vectorize=\"description\")\nweaviate_adapter(products_tables.customers, vectorize=\"bio\")\n\ninfo = pipeline.run(products_tables)\n```\n\n----------------------------------------\n\nTITLE: Setting Separate Google Credentials as Environment Variables\nDESCRIPTION: This snippet demonstrates how to set separate Google credentials as environment variables for source and destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n# Google Sheet credentials\nexport SOURCES__CREDENTIALS__CLIENT_EMAIL=\"<client_email>\"\nexport SOURCES__CREDENTIALS__PRIVATE_KEY=\"<private_key>\"\nexport SOURCES__CREDENTIALS__PROJECT_ID=\"<project_id>\"\n\n# BigQuery credentials\nexport DESTINATION__CREDENTIALS__CLIENT_EMAIL=\"<client_email>\"\nexport DESTINATION__CREDENTIALS__PRIVATE_KEY=\"<private_key>\"\nexport DESTINATION__CREDENTIALS__PROJECT_ID=\"<project_id>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Plus in config.toml\nDESCRIPTION: TOML configuration for Snowflake Plus destination settings\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/snowflake_plus.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[destination.snowflake]\nexternal_volume = \"<external_volume_name>\"\nforce_iceberg = true\n```\n\n----------------------------------------\n\nTITLE: Defining Asana Source in Python\nDESCRIPTION: Python function decorated with @dlt.source that returns all available Asana resources as a list of DltResource objects including workspaces, projects, sections, tags, tasks, stories, teams, and users.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/asana.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef asana_source(access_token: str = dlt.secrets.value) -> Any:\n    return [\n      workspaces, projects, sections, tags, tasks, stories, teams, users,\n    ]\n```\n\n----------------------------------------\n\nTITLE: Loading Data from Multiple Google Spreadsheets\nDESCRIPTION: Demonstrates how to load data from multiple Google Spreadsheets in a single pipeline run.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nload_data1 = google_spreadsheet(\n        \"https://docs.google.com/spreadsheets/d/43lkHjqouQnnCIZAFa2rL6vT91YRN8aIhts22SUUR580/edit#gid=0\", # Spreadsheet URL\n        range_names=[\"Sheet 1!A1:B10\"],\n        get_named_ranges=False,\n)\n\nload_data2 = google_spreadsheet(\n        \"https://docs.google.com/spreadsheets/d/3jo4HjqouQnnCIZAFa2rL6vT91YRN8aIhts22SKKO390/edit#gid=0\", # Spreadsheet URL\n        range_names=[\"Sheet 1!B1:C10\"],\n        get_named_ranges=True,\n)\nload_info = pipeline.run([load_data1, load_data2])\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Importing Destinations in dlt\nDESCRIPTION: Shows how to import built-in destination modules in dlt, which can be passed directly to run or pipeline methods. These destinations require extras to be installed.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/technical/general_usage.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.destinations import bigquery\nfrom dlt.destinations import redshift\n```\n\n----------------------------------------\n\nTITLE: Creating dlt Pipeline with Snowflake and Azure Staging\nDESCRIPTION: This Python code creates a dlt pipeline that loads chess player data to Snowflake using Azure Blob Storage as the staging destination. It demonstrates how to activate the staging location.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_19\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name='chess_pipeline',\n    destination='snowflake',\n    staging='filesystem', # add this to activate the staging location\n    dataset_name='player_data'\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Talk Resource Handler\nDESCRIPTION: Function for handling Zendesk Talk API endpoint data retrieval with pagination support\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/zendesk.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef talk_resource(\n    zendesk_client: ZendeskAPIClient,\n    talk_endpoint_name: str,\n    talk_endpoint: str,\n    pagination_type: PaginationType,\n) -> Iterator[TDataItem]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Setting Sentry DSN via Environment Variables for dlt\nDESCRIPTION: This snippet demonstrates how to enable Sentry tracing for dlt pipelines by setting the DSN through environment variables instead of the configuration file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/running-in-production/tracing.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nRUNTIME__SENTRY_DSN=\"https:///<...>\"\n```\n\n----------------------------------------\n\nTITLE: Setting up BigQuery Pipeline with GCS Staging\nDESCRIPTION: Example of creating a dlt pipeline that loads chess player data to BigQuery using GCS bucket as staging location.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/bigquery.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name='chess_pipeline',\n    destination='bigquery',\n    staging='filesystem', # Add this to activate the staging location.\n    dataset_name='player_data'\n)\n```\n\n----------------------------------------\n\nTITLE: Message UIDs Resource\nDESCRIPTION: Resource definition for collecting email message UIDs with incremental loading support\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/inbox.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(name=\"uids\")\ndef get_messages_uids(\n    initial_message_num: Optional[\n        dlt.sources.incremental[int]\n    ] = dlt.sources.incremental(\"message_uid\", initial_value=1),\n) -> TDataItem:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Applying Column Hints with Regular Expressions in Python\nDESCRIPTION: Python code that updates existing hints in a schema with new hints using regular expressions, specifically adding a partition hint to columns ending with '_timestamp'.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema.md#2025-04-14_snippet_8\n\nLANGUAGE: py\nCODE:\n```\nfrom dlt.common.schema.typing import TSimpleRegex\n  \nsource = data_source()\n# this will update existing hints with the hints passed\nsource.schema.merge_hints({\"partition\": [TSimpleRegex(\"re:_timestamp$\")]})\n```\n\n----------------------------------------\n\nTITLE: Loading Matomo Report Data in Python\nDESCRIPTION: This snippet demonstrates how to load data from Matomo reports using the matomo_reports() function and run it through the pipeline. The site_id is assumed to be defined in the .dlt/config.toml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/matomo.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndata_reports = matomo_reports()\nload_info = pipeline.run(data_reports)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Last Visits Resource Implementation\nDESCRIPTION: Python resource function for retrieving detailed visit data within specified timeframes\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/matomo.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(\n    name=\"visits\", write_disposition=\"append\", primary_key=\"idVisit\", selected=True\n)\ndef get_last_visits(\n    client: MatomoAPIClient,\n    site_id: int,\n    last_date: dlt.sources.incremental[float],\n    visit_timeout_seconds: int = 1800,\n    visit_max_duration_seconds: int = 3600,\n    rows_per_page: int = 2000,\n) -> Iterator[TDataItem]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Using Transformer Resources with Filesystem\nDESCRIPTION: Example of applying CSV transformer to filesystem source using pipe notation.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/basic.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.filesystem import filesystem, read_csv\n\nfilesystem_pipe = filesystem(\n  bucket_url=\"file://Users/admin/Documents/csv_files\",\n  file_glob=\"*.csv\"\n) | read_csv()\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running a REST API Source for Pokemon Data\nDESCRIPTION: Python code that defines and executes a pipeline to extract data from the Pokemon API using the REST API source, demonstrating URL configuration and resource selection.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/rest-api.md#2025-04-14_snippet_7\n\nLANGUAGE: py\nCODE:\n```\nimport dlt\nfrom dlt.sources.rest_api import rest_api_source\n\ndef load_pokemon() -> None:\n    pipeline = dlt.pipeline(\n        pipeline_name=\"rest_api_pokemon\",\n        destination=\"duckdb\",\n        dataset_name=\"rest_api_data\",\n    )\n\n    pokemon_source = rest_api_source(\n        {\n            \"client\": {\n                \"base_url\": \"https://pokeapi.co/api/v2/\"\n            },\n            \"resource_defaults\": {\n                \"endpoint\": {\n                    \"params\": {\n                        \"limit\": 1000,\n                    },\n                },\n            },\n            \"resources\": [\n                \"pokemon\",\n                \"berry\",\n                \"location\",\n            ],\n        }\n    )\n\n    ...\n\n    load_info = pipeline.run(pokemon_source)\n    print(load_info)\n```\n\n----------------------------------------\n\nTITLE: Configuring SQLAlchemy Database Credentials in TOML\nDESCRIPTION: Example configuration for database connection credentials in the dlt secrets.toml file, showing both direct parameter specification and connection string approaches.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/sqlalchemy.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[destination.sqlalchemy.credentials]\ndatabase = \"dlt_data\"\nusername = \"loader\"\npassword = \"<password>\"\nhost = \"localhost\"\nport = 3306\ndriver_name = \"mysql\"\n```\n\nLANGUAGE: toml\nCODE:\n```\n[destination.sqlalchemy]\ncredentials = \"mysql://loader:<password>@localhost:3306/dlt_data\"\n```\n\n----------------------------------------\n\nTITLE: Defining Default Workable API Endpoints in Python\nDESCRIPTION: Python code defining the default endpoints and details for the Workable API.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/workable.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nDEFAULT_ENDPOINTS = (\"members\", \"recruiters\", \"stages\", \"requisitions\", \"jobs\", \"custom_attributes\",\"events\")\n\nDEFAULT_DETAILS = {\n    \"candidates\": (\"activities\", \"offer\"),\n    \"jobs\": (\"activities\", \"application_form\", \"questions\", \"stages\", \"custom_attributes\", \"members\", \"recruiters\" ),\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Data for Qdrant Loading\nDESCRIPTION: Python code defining a simple movies dataset to be loaded into Qdrant with vector embeddings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/qdrant.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.destinations.adapters import qdrant_adapter\n\nmovies = [\n    {\n        \"title\": \"Blade Runner\",\n        \"year\": 1982,\n    },\n    {\n        \"title\": \"Ghost in the Shell\",\n        \"year\": 1995,\n    },\n    {\n        \"title\": \"The Matrix\",\n        \"year\": 1999,\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Adding New Columns with table_adapter_callback (Python)\nDESCRIPTION: Demonstrates how to explicitly type additional columns added with table_adapter_callback in a sql_database source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/usage.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sqlalchemy.sql import sqltypes\n\ndef add_new_columns(table) -> None:\n    required_columns = [\n        (\"add_int\", sqltypes.BigInteger, {\"nullable\": True}),\n        (\"add_text\", sqltypes.Text, {\"default\": None, \"nullable\": True}),\n    ]\n    for col_name, col_type, col_kwargs in required_columns:\n        if col_name not in table.c:\n            table.append_column(sa.Column(col_name, col_type, **col_kwargs))\n```\n\n----------------------------------------\n\nTITLE: Implementing Candidates Resource Function in Python\nDESCRIPTION: Python function that defines the resource for retrieving candidate data from the Workable API.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/workable.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(name=\"candidates\", write_disposition=\"merge\", primary_key=\"id\")\ndef candidates_resource(\n    updated_at: Optional[Any] = dlt.sources.incremental(\n        \"updated_at\", initial_value=workable.start_date_iso\n    )\n) -> Iterable[TDataItem]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Using sql_table with Custom Adapters (Python)\nDESCRIPTION: Shows how to use sql_table with custom table and query adapters for more granular control over data extraction.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/usage.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.sql_database import sql_table\n\ntable = sql_table(\n  table=\"chat_channel\",\n  table_adapter_callback=add_new_columns,\n  query_adapter_callback=query_adapter_callback,\n  incremental=dlt.sources.incremental(\"updated_at\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Loading PyArrow Table to Pipeline\nDESCRIPTION: Demonstrates converting a Pandas DataFrame to a PyArrow Table and loading it into the pipeline. Uses the same data structure as the previous example.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/arrow-pandas.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\n\n# Create dataframe and pipeline same as above\n...\n\ntable = pa.Table.from_pandas(df)\npipeline.run(table, table_name=\"orders\")\n```\n\n----------------------------------------\n\nTITLE: Basic Usage Example of DLT Requests\nDESCRIPTION: Demonstrates making a GET request with the dlt requests wrapper using authentication headers\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/http/requests.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresponse = requests.get(\n    'https://example.com/api/contacts',\n    headers={'Authorization': API_KEY}\n)\ndata = response.json()\n...\n```\n\n----------------------------------------\n\nTITLE: AWS Credentials Initialization\nDESCRIPTION: Example of initializing AWS credentials object.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\naws_credentials = AwsCredentials()\n```\n\n----------------------------------------\n\nTITLE: Configuring Sentry DSN in TOML for dlt Pipeline Tracing\nDESCRIPTION: This snippet shows how to enable Sentry tracing for dlt pipelines by configuring the DSN in the config.toml file. The configuration is added under the runtime section.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/running-in-production/tracing.md#2025-04-14_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[runtime]\n\nsentry_dsn=\"https:///<...>\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Slack Pipeline with DuckDB Destination\nDESCRIPTION: Command to initialize a dlt pipeline using Slack as the source and DuckDB as the destination. This sets up the necessary files and configuration for the pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/slack.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init slack duckdb\n```\n\n----------------------------------------\n\nTITLE: Configuring MotherDuck Credentials in TOML\nDESCRIPTION: TOML configuration for setting up MotherDuck credentials, including database name and service token.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/motherduck.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[destination.motherduck.credentials]\ndatabase = \"dlt_data_3\"\npassword = \"<your token here>\"\n```\n\n----------------------------------------\n\nTITLE: Using DocCardList Component in Markdown/JSX\nDESCRIPTION: Renders the DocCardList component to display a list of available destinations cards automatically generated from the directory structure.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/index.md#2025-04-14_snippet_2\n\nLANGUAGE: jsx\nCODE:\n```\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Implementing Change Tracking SQL Query for Incremental Loading\nDESCRIPTION: The SQL query that powers incremental loading by joining CHANGETABLE function with the source table to fetch changed records. It retrieves column data, tracks change versions, and identifies deleted records.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/ms-sql.md#2025-04-14_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n    [Columns],\n    ct.SYS_CHANGE_VERSION AS _dlt_sys_change_version,\n    CASE WHEN ct.SYS_CHANGE_OPERATION = 'D' THEN 'DELETE' ELSE NULL END AS _dlt_deleted\nFROM\n    CHANGETABLE(CHANGES [YourSchemaName].[YourTableName], @last_version) AS ct\n    LEFT JOIN [YourSchemaName].[YourTableName] AS t\n        ON ct.[PrimaryKey] = t.[PrimaryKey]\nWHERE\n    ct.SYS_CHANGE_VERSION > @last_version\nORDER BY\n    ct.SYS_CHANGE_VERSION ASC\n```\n\n----------------------------------------\n\nTITLE: OAuth Authentication Configuration\nDESCRIPTION: OAuth 2.0 authentication configuration for BigQuery in secrets.toml\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/bigquery.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[destination.bigquery]\nlocation = \"US\"\n\n[destination.bigquery.credentials]\nproject_id=\"project_id\"  # please set me up!\nclient_id = \"client_id\"  # please set me up!\nclient_secret = \"client_secret\"  # please set me up!\nrefresh_token = \"refresh_token\"  # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Initial Full Load of MS SQL Table with dlt\nDESCRIPTION: Python code to perform an initial full load of an MS SQL table using dlt's sql_table resource.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/ms-sql.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.sql_database import sql_table\n\n# Initial full load\ninitial_resource = sql_table(\n    credentials=engine,\n    schema=schema_name,\n    table=table_name,\n    reflection_level=\"full\",\n    write_disposition=\"merge\",\n)\n\npipeline = dlt.pipeline(\n    pipeline_name='sql_server_sync_pipeline',\n    destination='your_destination',\n    dataset_name='destination_dataset',\n)\n\n# Run the pipeline for initial load\npipeline.run(initial_resource)\n```\n\n----------------------------------------\n\nTITLE: Shopify Partner Query Resource Implementation\nDESCRIPTION: Python implementation of the GraphQL query resource for Shopify Partner API\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/shopify.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource\ndef shopify_partner_query(\n    query: str,\n    data_items_path: jp.TJsonPath,\n    pagination_cursor_path: jp.TJsonPath,\n    pagination_variable_name: str = \"after\",\n    variables: Optional[Dict[str, Any]] = None,\n    access_token: str = dlt.secrets.value,\n    organization_id: str = dlt.config.value,\n    api_version: str = API_VERSION,\n) -> Iterable[TDataItem]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Setting SQL Database Configuration as Environment Variables\nDESCRIPTION: This bash script shows how to set SQL database configuration parameters as environment variables. It includes settings for credentials, backend, chunk size, and incremental loading.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/advanced.md#2025-04-14_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nSOURCES__SQL_DATABASE__CREDENTIALS=\"mssql+pyodbc://loader.database.windows.net/dlt_data?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server\"\nSOURCES__SQL_DATABASE__BACKEND=pandas\nSOURCES__SQL_DATABASE__CHUNK_SIZE=1000\nSOURCES__SQL_DATABASE__CHAT_MESSAGE__INCREMENTAL__CURSOR_PATH=updated_at\n```\n\n----------------------------------------\n\nTITLE: AWS S3 Destination Configuration in secrets.toml\nDESCRIPTION: Configuration for AWS S3 bucket in the secrets.toml file, including bucket URL and AWS credentials setup.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\nbucket_url = \"s3://[your_bucket_name]\" # replace with your bucket name,\n\n[destination.filesystem.credentials]\naws_access_key_id = \"please set me up!\" # copy the access key here\naws_secret_access_key = \"please set me up!\" # copy the secret access key here\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake GCS Stage in TOML\nDESCRIPTION: This snippet demonstrates how to configure a Snowflake GCS stage in the TOML configuration file by setting the stage name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_16\n\nLANGUAGE: toml\nCODE:\n```\n[destination]\nstage_name=\"PUBLIC.my_gcs_stage\"\n```\n\n----------------------------------------\n\nTITLE: Limiting Data Load with Pipeline\nDESCRIPTION: Examples of limiting data extraction by number of items or time duration using add_limit method.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/source.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pipedrive import pipedrive_source\n\npipeline = dlt.pipeline(pipeline_name='pipedrive', destination='duckdb', dataset_name='pipedrive_data')\nload_info = pipeline.run(pipedrive_source().add_limit(10))\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Applying Column Removal to Source\nDESCRIPTION: Shows how to configure and apply the column removal function to the data source using source instance modification.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/customising-pipelines/removing_columns.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Example columns to remove:\nremove_columns_list = [\"country_code\"]\n\n# Create an instance of the source so you can edit it.\nsource_instance = dummy_source()\n\n# Modify this source instance's resource\nsource_instance.dummy_data.add_map(\n    lambda doc: remove_columns(doc, remove_columns_list)\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Replace Strategy in TOML\nDESCRIPTION: Configuration example showing how to set the replace strategy in the destination configuration using TOML format. This example sets the strategy to staging-optimized.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/full-loading.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[destination]\n# Set the optimized replace strategy\nreplace_strategy = \"staging-optimized\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Case Sensitivity in BigQuery\nDESCRIPTION: Configuration settings for handling case sensitivity in BigQuery identifiers using TOML configuration.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/bigquery.md#2025-04-14_snippet_11\n\nLANGUAGE: toml\nCODE:\n```\n[destination.bigquery]\nhas_case_sensitive_identifiers=false\n```\n\nLANGUAGE: toml\nCODE:\n```\n[destination.bigquery]\nshould_set_case_sensitivity_on_new_dataset=true\n```\n\n----------------------------------------\n\nTITLE: Configuring HubSpot API Credentials in TOML\nDESCRIPTION: Example of how to store the HubSpot API key in the secrets.toml file for secure access.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/hubspot.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# put your secret values and credentials here\n# do not share this file and do not push it to GitHub\n[sources.hubspot]\n api_key = \"api_key\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Excluding Unnecessary Columns Before Load (Python)\nDESCRIPTION: Demonstrates how to transform data by removing unnecessary columns before loading it to the destination using a sql_database source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/usage.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.sql_database import sql_database\n\ndef remove_columns(doc):\n    del doc[\"rfam_id\"]\n    return doc\n\npipeline = dlt.pipeline(\n    # Configure the pipeline\n)\n# using sql_database source to load family table and remove the column \"rfam_id\"\nsource = sql_database().with_resources(\"family\")\n# modify this source instance's resource\nsource.family.add_map(remove_columns)\n# Run the pipeline. For a large db this may take a while\ninfo = pipeline.run(source, write_disposition=\"replace\")\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Configuring Weaviate Modules in DLT\nDESCRIPTION: Demonstrates how to configure Weaviate modules, specifically text2vec-openai and generative-openai, in the config.toml file for the Weaviate destination in DLT.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/weaviate.md#2025-04-14_snippet_14\n\nLANGUAGE: toml\nCODE:\n```\n[destination.weaviate]\nmodule_config={text2vec-openai = {}, generative-openai = {}}\n```\n\n----------------------------------------\n\nTITLE: Running DLT Pipeline with Different Profiles\nDESCRIPTION: Commands demonstrating how to run a pipeline using different profile configurations.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/core-concepts/profiles.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline --profile dev my_pipeline run\ndlt pipeline --profile tests my_pipeline run\n```\n\n----------------------------------------\n\nTITLE: Rendering DocCardList Component in Markdown\nDESCRIPTION: This snippet renders the DocCardList component, which will display a list of documentation cards for the code examples.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/examples/index.md#2025-04-14_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Defining Destination Pipeline in Python using DLT\nDESCRIPTION: Sets up a destination pipeline configuration for replicating data to DuckDB. It specifies the pipeline name, destination, dataset name, and enables dev mode.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/pg_replication.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndest_pl = dlt.pipeline(\n    pipeline_name=\"pg_replication_pipeline\",\n    destination='duckdb',\n    dataset_name=\"replicate_single_table\",\n    dev_mode=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Late Destination Access Example in Python\nDESCRIPTION: Demonstrates how to use late destination access in multi-stage pipelines.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndata = pipeline.extract(source_1)\nnormalized = pipeline.normalize(data)\n# store normalized data to disk\nwith open(\"/tmp/normalized.json\", \"wb\") as f: normalized.write(f)\n\n# no destination access until here\nwith open(\"/tmp/normalized.json\", \"rb\") as f:\n    load_info = pipeline.load(f)\n```\n\n----------------------------------------\n\nTITLE: Loading Custom Notion Databases\nDESCRIPTION: Python code to load data from specific Notion databases by providing custom database IDs and optional names.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/notion.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nselected_database_ids = [{\"id\": \"0517dae9409845cba7d\",\"use_name\":\"db_one\"}, {\"id\": \"d8ee2d159ac34cfc\"}]\nload_data = notion_databases(database_ids=selected_database_ids)\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Advanced Dynamic Configuration with Callbacks in Python\nDESCRIPTION: Advanced Python configuration for filesystem destination using callback functions for both extra placeholders and current datetime determination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nimport pendulum\n\nimport dlt\nfrom dlt.destinations import filesystem\n\ndef placeholder_callback(schema_name: str, table_name: str, load_id: str, file_id: str, ext: str) -> str:\n    # Custom logic here\n    return \"custom_value\"\n\ndef get_current_datetime() -> pendulum.DateTime:\n    return pendulum.now()\n\npipeline = dlt.pipeline(\n    pipeline_name=\"data_things\",\n    destination=filesystem(\n        layout=\"{table_name}/{placeholder_x}/{timestamp}/{load_id}.{file_id}.{ext}\",\n        current_datetime=get_current_datetime,\n        extra_placeholders={\n            \"placeholder_x\": placeholder_callback\n        }\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Database Connection Using ConnectionStringCredentials\nDESCRIPTION: Example of using ConnectionStringCredentials class to handle database connection strings in a DLT source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.credentials import ConnectionStringCredentials\n\n@dlt.source\ndef query(sql: str, dsn: ConnectionStringCredentials = dlt.secrets.value):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Setting Write Disposition in Pipeline Run\nDESCRIPTION: Configure write disposition for the entire pipeline run in Python.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/iceberg.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npipeline.run(write_disposition={\"disposition\": \"merge\", \"strategy\": \"delete-insert\"})\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Branch Naming Pattern in Git for dlt Contributions\nDESCRIPTION: Shows the required naming convention for branches when contributing to dlt. The pattern follows a category/ticket-id-description format, with lowercase letters and dashes instead of underscores.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/CONTRIBUTING.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n{category}/{ticket-id}-description-of-the-branch\n# example:\nfeat/4922-add-avro-support\n```\n\n----------------------------------------\n\nTITLE: Generating Transformation Scaffolding with CLI\nDESCRIPTION: This shell command generates transformation scaffolding based on the dlt pipeline. It creates transformation files inside the ./transformations folder.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/transformations/setup.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndlt transformation <transformation-name> render-t-layer\n```\n\n----------------------------------------\n\nTITLE: Printing Resource Names in Python\nDESCRIPTION: This Python snippet demonstrates how to print the names of all available resources in the pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nprint(source.resources.keys())\n```\n\n----------------------------------------\n\nTITLE: Implementing Assets Resource in Python\nDESCRIPTION: Python function for fetching video assets metadata from Mux API with configurable parameters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mux.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nDEFAULT_LIMIT = 100\n\n@dlt.resource(write_disposition=\"merge\")\ndef assets_resource(\n    mux_api_access_token: str = dlt.secrets.value,\n    mux_api_secret_key: str = dlt.secrets.value,\n    limit: int = DEFAULT_LIMIT,\n) -> Iterable[TDataItem]:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Custom Message Processing\nDESCRIPTION: Example of implementing a custom message processor for Kafka messages with specific field mapping.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/kafka.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef custom_msg_processor(msg: confluent_kafka.Message) -> Dict[str, Any]:\n    return {\n        \"_kafka\": {\n            \"topic\": msg.topic(),  # required field\n            \"key\": msg.key().decode(\"utf-8\"),\n            \"partition\": msg.partition(),\n        },\n        \"data\": msg.value().decode(\"utf-8\"),\n    }\n\nresource = kafka_consumer(\"topic\", msg_processor=custom_msg_processor)\npipeline.run(resource)\n```\n\n----------------------------------------\n\nTITLE: Starting Local Development Server for Docusaurus Website\nDESCRIPTION: This command starts a local development web server for the Docusaurus website, allowing live updates while authoring content.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/README.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ npm run start\n```\n\n----------------------------------------\n\nTITLE: Google Cloud Secret Manager Integration with DLT\nDESCRIPTION: Python implementation for retrieving secrets from Google Cloud Secret Manager and using them in a DLT pipeline with GitHub API example.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add_credentials.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport json as json_lib\n\nimport dlt\nfrom dlt.sources.helpers import requests\nfrom dlt.common.configuration.inject import with_config\nfrom dlt.common.configuration.specs import GcpServiceAccountCredentials\nfrom google.cloud import secretmanager\n\n@with_config(sections=(\"google_secrets\",))\ndef get_secret_dict(secret_id: str, credentials: GcpServiceAccountCredentials = dlt.secrets.value) -> dict:\n    client = secretmanager.SecretManagerServiceClient(credentials=credentials.to_native_credentials())\n    name = f\"projects/{credentials.project_id}/secrets/{secret_id}/versions/latest\"\n    response = client.access_secret_version(request={\"name\": name})\n    secret_string = response.payload.data.decode(\"UTF-8\")\n    secret_dict = json_lib.loads(secret_string)\n    return secret_dict\n\nsecret_data = get_secret_dict(\"temp-secret\")\n\nurl = \"https://api.github.com/orgs/dlt-hub/repos\"\nheaders = {\n    \"Authorization\": f\"token {secret_data['api_token']}\",\n    \"Accept\": \"application/vnd.github+json\",\n}\n\nresponse = requests.get(url, headers=headers)\n\npipeline = dlt.pipeline(\n    pipeline_name=\"quick_start\", destination=\"duckdb\", dataset_name=\"mydata\"\n)\nload_info = pipeline.run(response.json())\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Setting Preferred Data Types in YAML\nDESCRIPTION: YAML configuration that defines preferred data types for columns, using both direct column name matches and regular expressions to set timestamp data types for specific columns.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema.md#2025-04-14_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nsettings:\n  preferred_types:\n    re:timestamp: timestamp\n    inserted_at: timestamp\n    created_at: timestamp\n    updated_at: timestamp\n```\n\n----------------------------------------\n\nTITLE: Initializing dlt+ Project Package\nDESCRIPTION: Python module initialization file that sets up core functionality for a dlt+ project package, including catalog access, context management, and entity creation.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/data-access.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\\nA demonstration package that sends GitHub events to Delta Lake, aggregates, and shares via Snowflake\\n\\n>>> import dlt_package_template\\n>>>\\n>>> print(dlt_package_template.catalog())  # list datasets\\n>>> print(dlt_package_template.catalog().dataset_name) # lists tables in dataset\\n>>> df_ = dlt_package_template.catalog().dataset_name.table_name.df()  # reads table\\n\"\"\"\\n\\nimport os\\nimport dlt as dlt\\nfrom dlt_plus.project import Catalog, EntityFactory, ProjectRunContext, Project, PipelineManager\\n\\ndef access_profile() -> str:\\n    \"\"\"Implement this function to select profile assigned to users that import this Python package\\n    into their own scripts or other modules.\\n    \"\"\"\\n    return \"access\"\\n\\n\\ndef context() -> ProjectRunContext:\\n    \"\"\"Returns the context of this package, including run directory,\\n    data directory and project config\\n    \"\"\"\\n    from dlt_plus.project.run_context import ensure_project\\n    return ensure_project(run_dir=os.path.dirname(__file__), profile=access_profile())\\n\\n\\ndef config() -> Project:\\n    \"\"\"Returns project configuration and getters of entities like sources, destinations\\n    and pipelines\"\"\"\\n    return context().project\\n\\n\\ndef entities() -> EntityFactory:\\n    \"\"\"Returns methods to create entities in this package likes sources, pipelines etc.\"\"\"\\n    return EntityFactory(config())\\n\\n\\ndef runner() -> PipelineManager:\\n    return PipelineManager(config())\\n\\n\\ndef catalog() -> Catalog:\\n    \"\"\"Returns a catalogue with available datasets, which can be read and written to\"\"\"\\n    return Catalog(context())\n```\n\n----------------------------------------\n\nTITLE: Configuring Freshdesk credentials in secrets.toml\nDESCRIPTION: Configuration template for storing Freshdesk API credentials securely in the secrets.toml file. You need to provide your Freshdesk domain and API secret key.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/freshdesk.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# Put your secret values and credentials here\n# Github access token (must be classic for reactions source)\n[sources.freshdesk]\ndomain = \"please set me up!\" # Enter the Freshdesk domain here\napi_secret_key = \"please set me up!\" # Enter the Freshdesk API key here\n```\n\n----------------------------------------\n\nTITLE: Disabling Custom Properties Loading\nDESCRIPTION: Demonstrates how to disable loading of custom properties when encountering request length limitations.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/hubspot.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ninfo = p.run(hubspot(include_custom_props=False))\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Definitions Object\nDESCRIPTION: Python code to create a Dagster Definitions object, including assets and resources for the dlt pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-dagster.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport assets\nfrom dagster import Definitions, load_assets_from_modules\nfrom dagster_embedded_elt.dlt import DagsterDltResource\n\ndlt_resource = DagsterDltResource()\nall_assets = load_assets_from_modules([assets])\n\ndefs = Definitions(\n    assets=all_assets,\n    resources={\n        \"dlt\": dlt_resource,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Full Custom Destination Function Signature\nDESCRIPTION: Complete signature of the destination decorator with all available configuration options and function parameters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/destination.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@dlt.destination(\n    batch_size=10,\n    loader_file_format=\"jsonl\",\n    name=\"my_custom_destination\",\n    naming_convention=\"direct\",\n    max_table_nesting=0,\n    skip_dlt_columns_and_tables=True,\n    max_parallel_load_jobs=5,\n    loader_parallelism_strategy=\"table-sequential\",\n)\ndef my_destination(items: TDataItems, table: TTableSchema) -> None:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Custom Destination with Secrets Configuration\nDESCRIPTION: Example of implementing a custom destination function with API key configuration using secrets management.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/destination.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dlt.destination(batch_size=10, loader_file_format=\"jsonl\", name=\"my_destination\")\ndef my_destination(items: TDataItems, table: TTableSchema, api_key: str = dlt.secrets.value) -> None:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Loading Jobs Data with Dependencies\nDESCRIPTION: Loads data from the jobs endpoint along with related endpoints like activities and application form. Includes date filtering and handles dependent data loading.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/workable.md#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nload_data = workable_source(start_date=pendulum.DateTime(2022, 2, 1), load_details=True)\n# Set the load_details as True to load all the dependent endpoints.\nload_info = pipeline.run(load_data.with_resources(\"jobs\",\"jobs_activities\",\"jobs_application_form\"))\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Using Merge Write Disposition with Qdrant\nDESCRIPTION: Example of using 'merge' write disposition with a primary key to update existing records or insert new ones in Qdrant.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/qdrant.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ninfo = pipeline.run(\n    qdrant_adapter(\n        movies,\n        embed=\"title\",\n    ),\n    primary_key=\"document_id\",\n    write_disposition=\"merge\"\n)\n```\n\n----------------------------------------\n\nTITLE: Google Cloud Storage Configuration in secrets.toml\nDESCRIPTION: Configuration for using Google Cloud Storage as the filesystem destination in dlt, specifying the bucket URL and GCP credentials.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_10\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\nbucket_url = \"gs://[your_bucket_name]\" # replace with your bucket name,\n\n[destination.filesystem.credentials]\nproject_id = \"project_id\" # please set me up!\nprivate_key = \"private_key\" # please set me up!\nclient_email = \"client_email\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Creating Modal Image Configuration\nDESCRIPTION: Python code defining a Modal Image with required dependencies for running dlt pipeline\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-modal.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodal_image\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Write Disposition in dlt.yml\nDESCRIPTION: YAML configuration for setting up the merge write disposition with upsert strategy for a source in dlt.yml.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/delta.md#2025-04-14_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  my_source:\n    type: sources.my_source\n    with_args:\n      write_disposition:\n        disposition: merge\n        strategy: upsert\n```\n\n----------------------------------------\n\nTITLE: Running Transformations with CLI\nDESCRIPTION: This shell command executes the defined transformation. It populates the local cache, applies the transformations, and flushes the transformed tables to the specified destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/transformations/setup.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ndlt transformation <transformation_name> run\n```\n\n----------------------------------------\n\nTITLE: Customizing Pipeline Configuration in Python\nDESCRIPTION: Example code showing how to configure and run custom pipelines for different Mux data loading scenarios.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mux.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"mux_pipeline\", # Use a custom name if desired\n    destination=\"bigquery\", # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"mux_dataset\" # Use a custom name if desired\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nload_info = pipeline.run(mux_source().with_resources(\"assets_resource\"))\nprint(load_info)\n```\n\nLANGUAGE: python\nCODE:\n```\nload_info = pipeline.run(mux_source().with_resources(\"views_resource\"))\nprint(load_info)\n```\n\nLANGUAGE: python\nCODE:\n```\nload_info = pipeline.run(mux_source())\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Shopify Products Resource Implementation\nDESCRIPTION: Python implementation of the products resource with incremental loading support\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/shopify.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(primary_key=\"id\", write_disposition=\"merge\")\ndef products(\n    updated_at: dlt.sources.incremental[\n        pendulum.DateTime\n    ] = dlt.sources.incremental(\n        \"updated_at\",\n        initial_value=START_DATE,\n        end_value=END_DATE,\n        allow_external_schedulers=True,\n    ),\n    created_at_min: pendulum.DateTime = created_at_min_obj,\n    items_per_page: int = ITEMS_PER_PAGE,\n) -> Iterable[TDataItem]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Jira Source Function\nDESCRIPTION: Python decorator and function definition for creating a Jira data source with authentication parameters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/jira.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef jira(\n     subdomain: str = dlt.secrets.value,\n     email: str = dlt.secrets.value,\n     api_token: str = dlt.secrets.value,\n) -> Iterable[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Email Attachments Transformer\nDESCRIPTION: Transformer resource for extracting and processing email attachments with file hash-based deduplication\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/inbox.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dlt.transformer(\n    name=\"attachments\",\n    primary_key=\"file_hash\",\n)\ndef get_attachments(\n    items: TDataItems,\n) -> Iterable[List[FileItem]]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Retrieving dbt Cloud Run Status\nDESCRIPTION: This snippet demonstrates how to use the get_dbt_cloud_run_status() function to retrieve information about a specific dbt Cloud job run. It supports waiting until the run completes with the wait_for_outcome parameter.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/transformations/dbt/dbt_cloud.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.helpers.dbt_cloud import get_dbt_cloud_run_status\n\n# Retrieve status for a specific run\nstatus = get_dbt_cloud_run_status(run_id=1234, wait_for_outcome=True)\n```\n\n----------------------------------------\n\nTITLE: Building Static Content for Docusaurus Website\nDESCRIPTION: This command generates static content for the Docusaurus website into the 'build' directory, which can be served using any static content hosting service.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/README.md#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ npm run build\n```\n\n----------------------------------------\n\nTITLE: Configuring MS SQL to handle long strings properly\nDESCRIPTION: Connection string configuration that enables proper handling of long strings (>8k) and avoids collation errors in MS SQL Server.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/mssql.md#2025-04-14_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\ndestination.mssql.credentials=\"mssql://loader:loader@localhost/dlt_data?LongAsMax=yes\"\n```\n\n----------------------------------------\n\nTITLE: Using AWS Profile for Credentials in secrets.toml\nDESCRIPTION: Configuration that uses a named AWS profile instead of explicit credentials, useful when credentials are stored in ~/.aws/credentials.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem.credentials]\nprofile_name=\"dlt-ci-user\"\n```\n\n----------------------------------------\n\nTITLE: Incrementally Loading Specific Collections from MongoDB in Python\nDESCRIPTION: This snippet shows how to incrementally load specific collections from MongoDB using DLT. It uses the 'date' field for incremental loading of 'collection_1'.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mongodb.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nload_data = mongodb(incremental=dlt.sources.incremental(\"date\")).with_resources(\"collection_1\")\nload_info = pipeline.run(load_data, write_disposition=\"merge\")\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Inbox Source Definition\nDESCRIPTION: Python function that defines the main inbox source with configuration parameters for email collection\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/inbox.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef inbox_source(\n    host: str = dlt.secrets.value,\n    email_account: str = dlt.secrets.value,\n    password: str = dlt.secrets.value,\n    folder: str = \"INBOX\",\n    gmail_group: Optional[str] = GMAIL_GROUP,\n    start_date: pendulum.DateTime = START_DATE,\n    filter_emails: Sequence[str] = None,\n    filter_by_mime_type: Sequence[str] = None,\n    chunksize: int = CHUNK_SIZE,\n) -> Sequence[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Loading Web Analytics Events\nDESCRIPTION: Loads web analytics events for specific HubSpot objects using their IDs, with support for incremental loading.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/hubspot.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresource = hubspot_events_for_objects(\"company\", [\"7086461639\", \"7086464459\"])\n# Here, object type: company, and object IDs: 7086461639 and 7086464459\nload_info = pipeline.run([resource])\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Enabling Change Tracking on MS SQL Table\nDESCRIPTION: SQL command to enable Change Tracking on a specific table with column update tracking.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/ms-sql.md#2025-04-14_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE [YourSchemaName].[YourTableName]\nENABLE CHANGE_TRACKING\nWITH (TRACK_COLUMNS_UPDATED = ON);\n```\n\n----------------------------------------\n\nTITLE: Creating an Iceberg Table Resource in dlt\nDESCRIPTION: Python code example showing how to define a dlt resource that will be stored as an Iceberg table in Athena.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/athena.md#2025-04-14_snippet_10\n\nLANGUAGE: py\nCODE:\n```\n@dlt.resource(table_format=\"iceberg\")\ndef data() -> Iterable[TDataItem]:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Write Disposition in Python\nDESCRIPTION: Set up merge write disposition for a resource using Python decorators.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/iceberg.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(\n    primary_key=\"id\",  # merge_key also works; primary_key and merge_key may be used together\n    write_disposition={\"disposition\": \"merge\", \"strategy\": \"delete-insert\"},\n)\ndef my_resource():\n    yield [\n        {\"id\": 1, \"foo\": \"foo\"},\n        {\"id\": 2, \"foo\": \"bar\"}\n    ]\n...\n\npipeline = dlt.pipeline(\"loads_iceberg\", destination=\"iceberg\")\n```\n\n----------------------------------------\n\nTITLE: Configuring MotherDuck Connection String\nDESCRIPTION: Alternative TOML configuration using a connection string for MotherDuck credentials.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/motherduck.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[destination]\nmotherduck.credentials=\"md:dlt_data_3?motherduck_token=<my service token>\"\n```\n\n----------------------------------------\n\nTITLE: Declaring Datasets in dlt.yml\nDESCRIPTION: Demonstrates how to declare datasets in the dlt.yml manifest file, specifying the destination where the dataset resides.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/projects.md#2025-04-14_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndatasets:\n  github_events_dataset:\n    destination:\n      - duckdb\n```\n\n----------------------------------------\n\nTITLE: Initializing Workable dlt Pipeline with DuckDB\nDESCRIPTION: Command to initialize a dlt pipeline for Workable with DuckDB as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/workable.md#2025-04-14_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndlt init workable duckdb\n```\n\n----------------------------------------\n\nTITLE: Partial Credentials Configuration in Python\nDESCRIPTION: Demonstrates passing partial credentials with remaining values pulled from config providers.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.destinations import redshift\nfrom dlt.common.configuration.specs import ConnectionStringCredentials\n\nredshift_dest = redshift(credentials=ConnectionStringCredentials(\"postgresql://admin@localhost/postgres\"))\n```\n\n----------------------------------------\n\nTITLE: Configuring DLT Pipeline for Scrapy in Python\nDESCRIPTION: Python code to configure the dlt pipeline with a custom name, destination, and dataset for Scrapy scraping.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/scrapy.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"scrapy_pipeline\",  # Use a custom name if desired\n    destination=\"duckdb\",  # Choose the appropriate destination (e.g., bigquery, redshift)\n    dataset_name=\"scrapy_data\",  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Google Ads Source Function in Python\nDESCRIPTION: Defines a source function that initializes a Google Ads client with credentials and development token. The function returns a list of resources including customers, campaigns, change events, and customer clients.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_ads.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source()\ndef google_ads(\n    credentials: Union[\n        GcpOAuthCredentials, GcpServiceAccountCredentials\n    ] = dlt.secrets.value,\n    impersonated_email: str = dlt.secrets.value,\n    dev_token: str = dlt.secrets.value,\n)  -> List[DltResource]:\n   \"\"\"\n   Initializes a client with the provided credentials and development token to\n   load default tables from Google Ads into the database. This function returns\n   various resources such as customers, campaigns, change events, and customer\n   clients.\n   \"\"\"\n```\n\n----------------------------------------\n\nTITLE: DLT Requests Configuration in TOML\nDESCRIPTION: Configuration options for customizing retry behavior, timeouts, and backoff settings in config.toml\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/http/requests.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[runtime]\nrequest_max_attempts = 10  # Stop after 10 retry attempts instead of 5\nrequest_backoff_factor = 1.5  # Multiplier applied to the exponential delays. Default is 1\nrequest_timeout = 120  # Timeout in seconds\nrequest_max_retry_delay = 30  # Cap exponential delay to 30 seconds\n```\n\n----------------------------------------\n\nTITLE: Loading Personio Employee Data\nDESCRIPTION: Executes a pipeline run to specifically load employee data from Personio using the employees resource.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/personio.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nload_data = personio_source().with_resources(\"employees\")\nprint(pipeline.run(load_data))\n```\n\n----------------------------------------\n\nTITLE: Accessing DLT Project Entities via Python API\nDESCRIPTION: Python code demonstrating how to interact with dlt+ project entities programmatically. Shows how to access pipeline and transformation objects using the entities manager.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/projects.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport dlt_plus\n\nentities = dlt_plus.current.entities()\npipeline = entities.get_pipeline(\"bronze_pipe\")\ntransformation = entities.get_transformation(\"stressed_transformation\")\n```\n\n----------------------------------------\n\nTITLE: Using CSV Loader for Fast Data Loading in dlt\nDESCRIPTION: Python code snippet showing how to use the CSV loader file format for fast data loading with Arrow tables in dlt.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/postgres.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ninfo = pipeline.run(arrow_table, loader_file_format=\"csv\")\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS S3 Credentials in TOML Files\nDESCRIPTION: This snippet shows how to configure AWS S3 credentials in dlt's secrets.toml and config.toml files. It includes settings for access key ID, secret access key, and bucket URL.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/basic.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n# secrets.toml\n[sources.filesystem.credentials]\naws_access_key_id=\"Please set me up!\"\naws_secret_access_key=\"Please set me up!\"\n\n# config.toml\n[sources.filesystem]\nbucket_url=\"s3://<bucket_name>/<path_to_files>/\"\n```\n\n----------------------------------------\n\nTITLE: Named Destination Configuration in TOML\nDESCRIPTION: Demonstrates TOML configuration for a named destination instance.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[destination.production_az_bucket]\nbucket_url = \"az://dlt-azure-bucket\"\n[destination.production_az_bucket.credentials]\nazure_storage_account_name = \"dltdata\"\nazure_storage_account_key = \"storage key\"\n```\n\n----------------------------------------\n\nTITLE: Custom Configuration Spec Example\nDESCRIPTION: Example of creating a custom configuration specification for Google Sheets integration.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.config import configspec, with_config\n\n@configspec\nclass GoogleSheetsConfiguration(BaseConfiguration):\n  tab_names: List[str] = None  # mandatory\n  credentials: GcpServiceAccountCredentials = None # mandatory secret\n  only_strings: Optional[bool] = False\n```\n\n----------------------------------------\n\nTITLE: Setting Source-Specific Naming Convention in TOML\nDESCRIPTION: This example shows how to configure a naming convention for a specific source (Zendesk) using a TOML configuration file. It sets the 'sql_cs_v1' naming convention for the Zendesk source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/naming-convention.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[sources.zendesk]\nconfig=\"prop\"\n[sources.zendesk.schema]\nnaming=\"sql_cs_v1\"\n[sources.zendesk.credentials]\npassword=\"pass\"\n```\n\n----------------------------------------\n\nTITLE: Running the dlt Pipeline with LanceDB Adapter\nDESCRIPTION: Example of running the dlt pipeline using the LanceDB adapter to specify which field to embed.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/lancedb.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninfo = pipeline.run(\n  lancedb_adapter(\n    movies,\n    embed=\"title\",\n  )\n)\n```\n\n----------------------------------------\n\nTITLE: Deploying Google Cloud Function for dlt Pipeline\nDESCRIPTION: gcloud command to deploy the Cloud Function. It specifies the function name, runtime, trigger type, authentication settings, source directory, and timeout.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-google-cloud-functions.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ngcloud functions deploy pipeline_notion --runtime python310 \\\n  --trigger-http --allow-unauthenticated --source . --timeout 300\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Scrapy Spider in Python\nDESCRIPTION: Example of a custom Scrapy spider class that scrapes quotes from a specific website.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/scrapy.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MySpider(Spider):\n    def parse(self, response: Response, **kwargs: Any) -> Any:\n        # Iterate through each \"next\" page link found\n        for next_page in response.css(\"li.next a::attr(href)\"):\n            if next_page:\n                yield response.follow(next_page.get(), self.parse)\n\n        # Iterate through each quote block found on the page\n        for quote in response.css(\"div.quote\"):\n            # Extract the quote details\n            result = {\n                \"quote\": {\n                    \"text\": quote.css(\"span.text::text\").get(),\n                    \"author\": quote.css(\"small.author::text\").get(),\n                    \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n                },\n            }\n            yield result\n```\n\n----------------------------------------\n\nTITLE: Deploying GCP Cloud Run Job for dlt Pipeline\nDESCRIPTION: gcloud command to deploy the dlt pipeline as a Cloud Run job. It specifies resources, region, and project details.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-google-cloud-run.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ngcloud run jobs deploy notion-pipeline-job \\\n    --source . \\\n    --tasks 1 \\\n    --max-retries 5 \\\n    --cpu 4 \\\n    --memory 4Gi \\\n    --region us-central1 \\\n    --project dlthub-sandbox\n```\n\n----------------------------------------\n\nTITLE: Initializing SQL Database Source with Configuration in Python\nDESCRIPTION: This Python snippet demonstrates how to initialize a SQL database source using the configured settings. It uses the sql_database function without arguments, relying on the TOML configuration.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/advanced.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndatabase = sql_database()\n```\n\n----------------------------------------\n\nTITLE: Enabling Case-Sensitive Identifiers in Redshift Configuration\nDESCRIPTION: Configuration to enable case-sensitive identifiers in Redshift, allowing for case-sensitive naming conventions.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/redshift.md#2025-04-14_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[destination.redshift]\nhas_case_sensitive_identifiers=true\n```\n\n----------------------------------------\n\nTITLE: Verifying Pipeline Execution with DLT CLI in Shell\nDESCRIPTION: This command uses the DLT CLI to show the details of a specific pipeline execution. It helps verify that the data was loaded as expected. Replace <pipeline_name> with the actual name of your pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/user_agent_device_data_enrichment.md#2025-04-14_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Specifying Google Sheets URL in TOML Configuration\nDESCRIPTION: This snippet demonstrates how to specify the full Google Sheets URL in the config.toml file for the spreadsheet_identifier.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\nspreadsheet_identifier = \"https://docs.google.com/spreadsheets/d/1VTtCiYgxjAwcIw7UM1_BSaxC3rzIpr0HwXZwd2OlPD4/edit?usp=sharing\"\n```\n\n----------------------------------------\n\nTITLE: Streaming Insert Resource Configuration\nDESCRIPTION: Python code showing how to configure a resource for streaming inserts\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/bigquery.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(write_disposition=\"append\")\ndef streamed_resource():\n    yield {\"field1\": 1, \"field2\": 2}\n\nstreamed_resource.apply_hints(additional_table_hints={\"x-insert-api\": \"streaming\"})\n```\n\n----------------------------------------\n\nTITLE: Configuring Caches in dlt.yml\nDESCRIPTION: Shows how to set up caches for transformations in the dlt.yml manifest file, specifying input and output tables.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/projects.md#2025-04-14_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ncaches:\n  github_events_cache:\n    inputs:\n      - dataset: github_events_dataset\n        tables:\n          events: events\n    outputs:\n      - dataset: github_events_dataset\n        tables:\n          events: events\n          events_aggregated: events_aggregated\n```\n\n----------------------------------------\n\nTITLE: Configuring Partitioning for Delta Table in Python\nDESCRIPTION: Example of configuring partitioning for a Delta table using column hints in Python.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/delta-iceberg.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(\n  table_format=\"delta\",\n  columns={\"foo\": {\"partition\": True}}\n)\ndef my_delta_resource():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Kafka Credentials Configuration\nDESCRIPTION: TOML configuration for Kafka authentication credentials including bootstrap servers, security protocols, and authentication details.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/kafka.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[sources.kafka.credentials]\nbootstrap_servers=\"web.address.gcp.confluent.cloud:9092\"\ngroup_id=\"test_group\"\nsecurity_protocol=\"SASL_SSL\"\nsasl_mechanisms=\"PLAIN\"\nsasl_username=\"example_username\"\nsasl_password=\"example_secret\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Naming Convention in TOML\nDESCRIPTION: This snippet illustrates how to use a custom naming convention by specifying the full Python import path in a TOML configuration file. It points to a custom naming convention module.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/naming-convention.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[schema]\nnaming=\"tests.common.cases.normalizers.sql_upper\"\n```\n\n----------------------------------------\n\nTITLE: Initializing DLT Project with Dremio\nDESCRIPTION: Command to initialize a new DLT project with chess as the source and Dremio as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/dremio.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init chess dremio\n```\n\n----------------------------------------\n\nTITLE: Accessing Iceberg Tables in Python\nDESCRIPTION: Use helper functions to access and manipulate Iceberg tables in Python.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/iceberg.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.common.libs.pyiceberg import get_iceberg_tables\n\n...\n\n# get dictionary of Table objects\ndelta_tables = get_iceberg_tables(pipeline)\n\n# execute operations on Table objects\niceberg_tables[\"my_iceberg_table\"].optimize.compact()\niceberg_tables[\"another_iceberg_table\"].optimize.z_order([\"col_a\", \"col_b\"])\n# iceberg_tables[\"my_iceberg_table\"].vacuum()\n```\n\n----------------------------------------\n\nTITLE: Forcing Iceberg Format for All Tables\nDESCRIPTION: Configuration to force all tables to be created in Iceberg format regardless of individual resource settings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/athena.md#2025-04-14_snippet_11\n\nLANGUAGE: toml\nCODE:\n```\n[destination.athena]\nforce_iceberg = true\n```\n\n----------------------------------------\n\nTITLE: Custom DLT Requests Client Configuration\nDESCRIPTION: Example of creating a custom requests client instance with specific retry status codes and exceptions\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/http/requests.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.helpers import requests\n\nhttp_client = requests.Client(\n    status_codes=(403, 500, 502, 503),\n    exceptions=(requests.ConnectionError, requests.ChunkedEncodingError)\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Synapse Destination Options in TOML\nDESCRIPTION: Defines additional configuration options for Synapse destination including table index type, index creation settings, staging authentication, and connection parameters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/synapse.md#2025-04-14_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[destination.synapse]\ndefault_table_index_type = \"heap\"\ncreate_indexes = \"false\"\nstaging_use_msi = \"false\"\n\n[destination.synapse.credentials]\nport = \"1433\"\nconnect_timeout = 15\n```\n\n----------------------------------------\n\nTITLE: Configuring Notion API Credentials in TOML\nDESCRIPTION: TOML configuration for storing the Notion API key securely in the secrets.toml file. This file should be kept private and not shared or pushed to version control.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/notion.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# Put your secret values and credentials here\n# Note: Do not share this file and do not push it to GitHub!\n[source.notion]\napi_key = \"set me up!\" # Notion API token (e.g. secret_XXX...)\n```\n\n----------------------------------------\n\nTITLE: Setting Schema Import/Export Paths in Python\nDESCRIPTION: Shows how to configure schema import and export directories when initializing a dlt pipeline. This enables automated schema management with user-editable import schemas and system-generated export schemas.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/technical/working_with_schemas.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndlt.pipeline(import_schema_path=\"schemas/import\", export_schema_path=\"schemas/export\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Write Disposition with Pokemon API\nDESCRIPTION: Example showing how to configure a REST API source with merge write disposition for the Pokemon resource. This setup specifies a primary key for matching new data with existing data in the destination table.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/rest-api.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n...\npokemon_source = rest_api_source(\n    {\n        \"client\": {\n            \"base_url\": \"https://pokeapi.co/api/v2/\",\n        },\n        \"resource_defaults\": {\n            \"endpoint\": {\n                \"params\": {\n                    \"limit\": 1000,\n                },\n            },\n            # For the `berry` and `location` resources, we keep\n            # the `replace` write disposition\n            \"write_disposition\": \"replace\",\n        },\n        \"resources\": [\n            # We create a specific configuration for the `pokemon` resource\n            # using a dictionary instead of a string to configure\n            # the primary key and write disposition\n            {\n                \"name\": \"pokemon\",\n                \"primary_key\": \"name\",\n                \"write_disposition\": \"merge\",\n            },\n            # The `berry` and `location` resources will use the default\n            \"berry\",\n            \"location\",\n        ],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Timestamp Column in dlt Resource\nDESCRIPTION: Python code example demonstrating how to configure a timestamp column with precision and timezone settings in a dlt resource.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/postgres.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(\n    columns={\"event_tstamp\": {\"data_type\": \"timestamp\", \"precision\": 3, \"timezone\": False}},\n    primary_key=\"event_id\",\n)\ndef events():\n    yield [{\"event_id\": 1, \"event_tstamp\": \"2024-07-30T10:00:00.123\"}]\n\npipeline = dlt.pipeline(destination=\"postgres\")\npipeline.run(events())\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline with Custom Dataset Name in Python\nDESCRIPTION: This Python code snippet demonstrates how to create a dlt pipeline with a custom dataset name. This is relevant when dataset name normalization is disabled.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/naming-convention.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\npipeline = dlt.pipeline(dataset_name=\"MyCamelCaseName\")\n```\n\n----------------------------------------\n\nTITLE: Creating Custom SQL Database Source with Renamed Configuration in Python\nDESCRIPTION: This Python snippet demonstrates how to create a custom SQL database source with a renamed configuration. It uses the clone method to create a new source with a custom name and section.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/advanced.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.sql_database import sql_database\n\nmy_db = sql_database.clone(name=\"my_db\", section=\"my_db\")(table_names=[\"chat_message\"])\nprint(my_db.name)\n```\n\n----------------------------------------\n\nTITLE: Defining Sample Data for Weaviate\nDESCRIPTION: Creating a simple data structure of movies that will be loaded into Weaviate for vector search purposes.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/weaviate.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.destinations.adapters import weaviate_adapter\n\nmovies = [\n    {\n        \"title\": \"Blade Runner\",\n        \"year\": 1982,\n    },\n    {\n        \"title\": \"Ghost in the Shell\",\n        \"year\": 1995,\n    },\n    {\n        \"title\": \"The Matrix\",\n        \"year\": 1999,\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Configure GitHub Credentials in TOML\nDESCRIPTION: TOML configuration for storing GitHub access token in secrets file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/github.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# Put your secret values and credentials here\n# GitHub access token (must be classic for reactions source)\n[sources.github]\naccess_token=\"please set me up!\" # use GitHub access token here\n```\n\n----------------------------------------\n\nTITLE: Configuring Dremio with SqlAlchemy Connection String\nDESCRIPTION: Alternative TOML configuration using SqlAlchemy-style connection string for Dremio credentials.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/dremio.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[destination.dremio]\nstaging_data_source=\"s3_staging\"\ncredentials=\"grpc://<username>:<password>@<host>:<port>/<data_source>\"\n```\n\n----------------------------------------\n\nTITLE: Type Adaptation for Snowflake Timestamps\nDESCRIPTION: Example of customizing type reflection for Snowflake timestamps using a type adapter callback function.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/advanced.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nimport sqlalchemy as sa\nfrom dlt.sources.sql_database import sql_database, sql_table\nfrom snowflake.sqlalchemy import TIMESTAMP_NTZ\n\ndef type_adapter_callback(sql_type):\n    if isinstance(sql_type, TIMESTAMP_NTZ):  # Snowflake does not inherit from sa.DateTime\n        return sa.DateTime(timezone=True)\n    return sql_type  # Use default detection for other types\n\nsource = sql_database(\n    \"snowflake://user:password@account/database?&warehouse=WH_123\",\n    reflection_level=\"full\",\n    type_adapter_callback=type_adapter_callback,\n    backend=\"pyarrow\"\n)\n\ndlt.pipeline(\"demo\").run(source)\n```\n\n----------------------------------------\n\nTITLE: Creating Postgres Database for dlt\nDESCRIPTION: SQL command to create a new database named 'dlt_data' in Postgres for use with dlt.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/postgres.md#2025-04-14_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE DATABASE dlt_data;\n```\n\n----------------------------------------\n\nTITLE: Configuring BigQuery Credentials in secrets.toml\nDESCRIPTION: TOML configuration that shows how to set up BigQuery credentials in the secrets.toml file. It includes the location configuration and the required credential fields: project_id, private_key, and client_email.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/share-a-dataset.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[destination.bigquery]\nlocation = \"US\"\n\n[destination.bigquery.credentials]\nproject_id = \"project_id\" # please set me up!\nprivate_key = \"private_key\" # please set me up!\nclient_email = \"client_email\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Configuring Backward Compatible Behavior in TOML\nDESCRIPTION: Example of how to explicitly enable backward compatible behavior for path normalization in the config.toml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/naming-convention.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[schema]\nuse_break_path_on_normalize=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Partitioning in dlt.yml\nDESCRIPTION: YAML configuration for setting up partitioning for a source in dlt.yml.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/delta.md#2025-04-14_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  my_source:\n    type: sources.my_source\n    with_args:\n      columns:\n        foo:\n          partition: True\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Analytics Service Account Credentials\nDESCRIPTION: TOML configuration for storing Google Analytics service account credentials in the secrets.toml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_analytics.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[sources.google_analytics.credentials]\nproject_id = \"project_id\" # please set me up!\nclient_email = \"client_email\" # please set me up!\nprivate_key = \"private_key\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Configuring Redshift Credentials\nDESCRIPTION: TOML configuration for setting up Redshift destination credentials.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/staging.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\ndestination.redshift.credentials=\"redshift://loader:<password>@localhost/dlt_data?connect_timeout=15\"\n```\n\n----------------------------------------\n\nTITLE: Executing Chess.com API Pipeline\nDESCRIPTION: Python code to run the Chess.com API pipeline, loading data from all resources for the specified players and time period.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/chess.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ninfo = pipeline.run(source_instance)\n# print the information on data that was loaded\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Defining Iceberg Destination in Python\nDESCRIPTION: Create a dlt pipeline with Iceberg destination using Python code.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/iceberg.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\"loads_iceberg\", destination=\"iceberg\")\n```\n\n----------------------------------------\n\nTITLE: Configuring OAuth Authentication for Snowflake\nDESCRIPTION: TOML configuration for using OAuth authentication with Snowflake, which requires specifying 'oauth' as the authenticator and providing a token.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[destination.snowflake.credentials]\ndatabase = \"dlt_data\"\nusername = \"loader\"\nauthenticator=\"oauth\"\ntoken=\"...\"\n```\n\n----------------------------------------\n\nTITLE: Service Account Configuration\nDESCRIPTION: Basic BigQuery service account credentials configuration in secrets.toml\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/bigquery.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[destination.bigquery]\nlocation = \"US\"\n\n[destination.bigquery.credentials]\nproject_id = \"project_id\" # please set me up!\nprivate_key = \"private_key\" # please set me up!\nclient_email = \"client_email\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: JSON Column Configuration in YAML\nDESCRIPTION: Schema configuration for handling nested data as JSON columns instead of flattened structure.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/adjust-a-schema.md#2025-04-14_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nplayers_games:\n  columns:\n    end_time:\n      nullable: true\n      data_type: timestamp\n    white:\n      nullable: false\n      data_type: json\n    black:\n      nullable: false\n      data_type: json\n```\n\n----------------------------------------\n\nTITLE: Configuring Delta Destination in dlt.yml\nDESCRIPTION: YAML configuration for setting up the Delta destination in the dlt.yml file, specifying the bucket URL.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/delta.md#2025-04-14_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndestinations:\n  delta_destination:\n    type: delta\n    bucket_url: \"s3://your_bucket\" # replace with bucket url\n```\n\n----------------------------------------\n\nTITLE: Defining Primary Key in DLT Resource\nDESCRIPTION: Example of defining a primary key for a DLT resource to enable relationship mapping.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/transformations/dbt-transformations.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\n@dlt.resource(name=\"customers\", primary_key=\"id\")\ndef customers():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Setting Row Group Size for Parquet Files\nDESCRIPTION: Example of configuring the buffer_max_items parameter to control the size of row groups in parquet files, which can optimize memory usage and query performance.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/file-formats/parquet.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[extract.data_writer]\nbuffer_max_items=10e6\n```\n\n----------------------------------------\n\nTITLE: Additional fsspec Configuration in secrets.toml\nDESCRIPTION: Configuration to pass additional arguments to fsspec through kwargs and client_kwargs in the toml config.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_9\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem.kwargs]\nuse_ssl=true\nauto_mkdir=true\n\n[destination.filesystem.client_kwargs]\nverify=\"public.crt\"\n```\n\n----------------------------------------\n\nTITLE: Checking Pipeline Status\nDESCRIPTION: Command to verify pipeline execution status and data loading\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/zendesk.md#2025-04-14_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Passing Explicit Postgres Credentials in dlt Pipeline\nDESCRIPTION: Python code example showing how to pass explicit Postgres credentials when creating a dlt pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/postgres.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n  pipeline_name='chess',\n  destination=dlt.destinations.postgres(\"postgresql://loader:<password>@localhost/dlt_data\"),\n  dataset_name='chess_data' #your destination schema name\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring dlt in Python Code\nDESCRIPTION: This snippet shows how to configure dlt directly in Python code, including setting environment variables and using third-party credential suppliers.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport dlt\nimport botocore.session\nfrom dlt.common.credentials import AwsCredentials\n\n# you can freely set up configuration directly in the code\n\n# via env vars\nos.environ[\"RUNTIME__LOG_LEVEL\"] = \"INFO\"\nos.environ[\"DESTINATION__FILESYSTEM__BUCKET_URL\"] = \"s3://[your_bucket_name]\"\nos.environ[\"NORMALIZE__DATA_WRITER__DISABLE_COMPRESSION\"] = \"true\"\n\n# or even directly to the dlt.config\ndlt.config[\"runtime.log_level\"] = \"INFO\"\ndlt.config[\"destination.filesystem.bucket_url\"] = \"s3://[your_bucket_name]\"\ndlt.config[\"normalize.data_writer.disable_compression\"] = \"true\"\n\n# but please, do not set up the secrets in the code!\n# what you can do is reassign env variables:\nos.environ[\"SOURCE__NOTION__API_KEY\"] = os.environ.get(\"NOTION_KEY\")\n\n# or use a third-party credentials supplier\ncredentials = AwsCredentials()\nsession = botocore.session.get_session()\ncredentials.parse_native_representation(session)\ndlt.secrets[\"destination.filesystem.credentials\"] = credentials\n```\n\n----------------------------------------\n\nTITLE: Setting Write Disposition in Pipeline Run\nDESCRIPTION: Python code to set the write disposition for the entire pipeline run.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/delta.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npipeline.run(write_disposition={\"disposition\": \"merge\", \"strategy\": \"upsert\"})\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Table Parameters in TOML\nDESCRIPTION: This TOML configuration sets specific parameters for a SQL table named 'chat_message'. It specifies the backend, chunk size, and incremental loading settings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/advanced.md#2025-04-14_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\n[sources.sql_database.chat_message]\nbackend=\"pandas\"\nchunk_size=1000\n\n[sources.sql_database.chat_message.incremental]\ncursor_path=\"updated_at\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Production Profile in TOML\nDESCRIPTION: This TOML configuration sets up the 'prod' profile with specific settings for runtime, destination, and data source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_16\n\nLANGUAGE: toml\nCODE:\n```\n[runtime]\nlog_level = \"INFO\"\n\n[destination.my_duckdb_destination]\ncredentials = \"my_data_prod.duckdb\"\n\n[sources.my_arrow_source]\nrow_count = 200\n```\n\n----------------------------------------\n\nTITLE: Running a Pipeline with Qdrant Vector Embeddings\nDESCRIPTION: Python code executing a dlt pipeline that loads data into Qdrant, creating vector embeddings from the 'title' field.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/qdrant.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ninfo = pipeline.run(\n    qdrant_adapter(\n        movies,\n        embed=\"title\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Running DLT Commands with Profiles\nDESCRIPTION: Examples of using the --profile flag with DLT CLI commands to specify different execution environments.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/core-concepts/profiles.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt project --profile dev my_pipeline run\ndlt dataset --profile prod my_duckdb_destination_dataset row-counts\n```\n\n----------------------------------------\n\nTITLE: Configuring Delta Storage Options in TOML\nDESCRIPTION: Example of configuring Delta storage options in a TOML configuration file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/delta-iceberg.md#2025-04-14_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\ndeltalake_storage_options = '{\"AWS_S3_LOCKING_PROVIDER\": \"dynamodb\", \"DELTA_DYNAMO_TABLE_NAME\": \"custom_table_name\"}'\n```\n\n----------------------------------------\n\nTITLE: S3 Storage Configuration Examples\nDESCRIPTION: Multiple configuration methods for setting up S3 as a staging destination for Databricks, including TOML, environment variables, and code-based approaches.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/databricks.md#2025-04-14_snippet_9\n\nLANGUAGE: toml\nCODE:\n```\n# secrets.toml\n[destination.filesystem]\nbucket_url = \"s3://your-bucket-name\"\n\n[destination.filesystem.credentials]\naws_access_key_id=\"XXX\"\naws_secret_access_key=\"XXX\"\n```\n\nLANGUAGE: shell\nCODE:\n```\nexport DESTINATIONS__FILESYSTEM__BUCKET_URL=\"s3://your-bucket-name\"\nexport DESTINATIONS__FILESYSTEM__CREDENTIALS__AWS_ACCESS_KEY_ID=\"XXX\"\nexport DESTINATIONS__FILESYSTEM__CREDENTIALS__AWS_SECRET_ACCESS_KEY=\"XXX\"\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Do not set up the secrets directly in the code!\n# What you can do is reassign env variables.\nos.environ[\"DESTINATIONS__FILESYSTEM__BUCKET_URL\"] = \"s3://your-bucket-name\"\nos.environ[\"DESTINATIONS__FILESYSTEM__CREDENTIALS__AWS_ACCESS_KEY_ID\"] = os.environ.get(\"AWS_ACCESS_KEY_ID\")\nos.environ[\"DESTINATIONS__FILESYSTEM__CREDENTIALS__AWS_SECRET_ACCESS_KEY\"] = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Postgres Replication Credentials (TOML)\nDESCRIPTION: TOML configuration for setting up Postgres replication credentials in the secrets.toml file. It shows two alternative ways to provide the credentials.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/pg_replication.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[sources.pg_replication.credentials]\ndrivername = \"postgresql\" # please set me up!\ndatabase = \"database\" # please set me up!\npassword = \"password\" # please set me up!\nusername = \"username\" # please set me up!\nhost = \"host\" # please set me up!\nport = 0 # please set me up!\n```\n\nLANGUAGE: toml\nCODE:\n```\nsources.pg_replication.credentials=\"postgresql://username@password.host:port/database\"\n```\n\n----------------------------------------\n\nTITLE: Defining Cache Configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a cache for GitHub events data in the dlt.yml file. It specifies the input and output datasets and tables for the cache.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/transformations/setup.md#2025-04-14_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncaches:\n  github_events_cache:\n    inputs:\n      - dataset: github_events_dataset\n        tables:\n          items: items\n    outputs:\n      - dataset: github_events_dataset\n        tables:\n          items: items\n          items_aggregated: items_aggregated\n```\n\n----------------------------------------\n\nTITLE: Obtaining Load Package Information in Python\nDESCRIPTION: This Python code snippet shows how to retrieve and print information about the load package after running a pipeline. It provides details about the data loading process, including timing and performance metrics.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/running-in-production/monitoring.md#2025-04-14_snippet_3\n\nLANGUAGE: py\nCODE:\n```\ninfo = pipeline.run(source, table_name=\"table_name\", write_disposition='append')\n\nprint(info.load_packages[0])\n```\n\n----------------------------------------\n\nTITLE: Multi-Project Configuration\nDESCRIPTION: Configuration for using different project IDs for credentials and destination\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/bigquery.md#2025-04-14_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\n[destination.bigquery]\nproject_id = \"project_id_destination\"\n\n[destination.bigquery.credentials]\nproject_id = \"project_id_credentials\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Filesystem Staging Credentials\nDESCRIPTION: TOML configuration for setting up S3 bucket access credentials for filesystem staging destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/staging.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\nbucket_url = \"s3://[your_bucket_name]\"\n\n[destination.filesystem.credentials]\naws_access_key_id = \"please set me up!\"\naws_secret_access_key = \"please set me up!\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Google Analytics Pipeline in Python\nDESCRIPTION: Example configuration for creating a custom Google Analytics data pipeline using DLT, specifying pipeline name, destination and dataset name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_analytics.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"google_analytics\",  # Use a custom name if desired\n    destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"GA4_data\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Installing DLT with DuckDB Support in Shell\nDESCRIPTION: This command installs the DLT library with DuckDB support using pip. It's a prerequisite for running the data enrichment pipeline with DuckDB as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/user_agent_device_data_enrichment.md#2025-04-14_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npip install \"dlt[duckdb]\"\n```\n\n----------------------------------------\n\nTITLE: Secrets Configuration in TOML\nDESCRIPTION: TOML configuration example for storing destination secrets like API keys.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/destination.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[destination.my_destination]\napi_key=\"<my-api-key>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Cloud Credentials in secrets.toml\nDESCRIPTION: This snippet shows how to set up dbt Cloud API authentication credentials in a secrets.toml file. It includes required fields for API token and account ID, plus optional job_id and run_id configurations.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/transformations/dbt/dbt_cloud.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[dbt_cloud]\napi_token = \"set me up!\" # required for authentication\naccount_id = \"set me up!\" # required for both helper functions\njob_id = \"set me up!\" # optional only for the run_dbt_cloud_job function (you can pass this explicitly as an argument to the function)\nrun_id = \"set me up!\" # optional for the get_dbt_cloud_run_status function (you can pass this explicitly as an argument to the function)\n```\n\n----------------------------------------\n\nTITLE: Setting MotherDuck Token as Environment Variable\nDESCRIPTION: Command to set the MotherDuck token as an environment variable for authentication.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/motherduck.md#2025-04-14_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nexport motherduck_token='<token>'\n```\n\n----------------------------------------\n\nTITLE: Configuring Case-Sensitive Identifiers for MSSQL Destination in Python\nDESCRIPTION: This code shows how to configure an MSSQL destination to accept case-sensitive identifiers. It sets the 'has_case_sensitive_identifiers' flag to True and uses a case-sensitive naming convention.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.destinations import mssql\ndest_ = mssql(has_case_sensitive_identifiers=True, naming_convention=\"sql_cs_v1\")\n```\n\n----------------------------------------\n\nTITLE: Setting Separate Google Credentials in Python Code\nDESCRIPTION: This snippet shows how to set separate Google credentials by reassigning environment variables and using dlt.secrets in Python code.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nimport os\n\n# Do not set up the secrets directly in the code!\n# What you can do is reassign env variables.\nos.environ[\"DESTINATION__CREDENTIALS__CLIENT_EMAIL\"] = os.environ.get(\"BIGQUERY_CLIENT_EMAIL\")\nos.environ[\"DESTINATION__CREDENTIALS__PRIVATE_KEY\"] = os.environ.get(\"BIGQUERY_PRIVATE_KEY\")\nos.environ[\"DESTINATION__CREDENTIALS__PROJECT_ID\"] = os.environ.get(\"BIGQUERY_PROJECT_ID\")\n\n# Or set them to the dlt.secrets.\ndlt.secrets[\"sources.credentials.client_email\"] = os.environ.get(\"SHEETS_CLIENT_EMAIL\")\ndlt.secrets[\"sources.credentials.private_key\"] = os.environ.get(\"SHEETS_PRIVATE_KEY\")\ndlt.secrets[\"sources.credentials.project_id\"] = os.environ.get(\"SHEETS_PROJECT_ID\")\n```\n\n----------------------------------------\n\nTITLE: Replicating Selected Columns in Python using DLT\nDESCRIPTION: Initializes replication for specific columns of a table using the include_columns parameter. This allows for selective replication of data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/pg_replication.md#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# requires the Postgres user to have the REPLICATION attribute assigned\ninitial_load = init_replication(  \n    slot_name=slot_name,\n    pub_name=pub_name,\n    schema_name=src_pl.dataset_name,\n    table_names=\"my_source_table\",\n    include_columns={\n        \"my_source_table\": (\"column1\", \"column2\")\n    },\n    reset=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Case-Insensitive Naming in DLT for Weaviate\nDESCRIPTION: Shows how to configure an alternative naming convention in the config.toml file to use case-insensitive naming for Weaviate destination in DLT.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/weaviate.md#2025-04-14_snippet_13\n\nLANGUAGE: toml\nCODE:\n```\n[schema]\nnaming=\"dlt.destinations.impl.weaviate.ci_naming\"\n```\n\n----------------------------------------\n\nTITLE: Declaring Destination Using Factory Type in Python\nDESCRIPTION: Demonstrates declaring a destination using the full destination factory type path.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(pipeline_name=\"pipeline_name\", destination=\"dlt.destinations.filesystem\")\n```\n\n----------------------------------------\n\nTITLE: Configuring OAuth Credentials for Google Sheets in TOML\nDESCRIPTION: This snippet demonstrates the format for storing Google Sheets OAuth credentials in the secrets.toml file. It includes placeholders for client_id, client_secret, refresh_token, and project_id.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[sources.google_sheets.credentials] ##CHECK IT\nclient_id = \"client_id\" # please set me up!\nclient_secret = \"client_secret\" # please set me up!\nrefresh_token = \"refresh_token\" # please set me up!\nproject_id = \"project_id\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Azure Credentials Setup\nDESCRIPTION: Basic example of setting Azure storage credentials directly.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_16\n\nLANGUAGE: python\nCODE:\n```\naz_credentials = AzureCredentials()\n# Set the necessary attributes\naz_credentials.azure_storage_account_name = \"ACCOUNT_NAME\"\naz_credentials.azure_storage_account_key = \"ACCOUNT_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Allowing self-signed SSL certificates in MS SQL connection\nDESCRIPTION: Configuration to allow self-signed SSL certificates when connecting to MS SQL Server to avoid certificate verification failures.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/mssql.md#2025-04-14_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\ndestination.mssql.credentials=\"mssql://loader:loader@localhost/dlt_data?TrustServerCertificate=yes\"\n```\n\n----------------------------------------\n\nTITLE: Using Connection String with Key Pair Authentication\nDESCRIPTION: Example of providing Snowflake key pair authentication credentials via connection string, with both private key and passphrase as URL parameters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\n# Keep it at the top of your TOML file, before any section starts\ndestination.snowflake.credentials=\"snowflake://loader:<password>@kgiotue-wn98412/dlt_data?private_key=<base64 encoded pem>&private_key_passphrase=<url encoded passphrase>\"\n```\n\n----------------------------------------\n\nTITLE: Defining Stripe API Endpoints\nDESCRIPTION: Python code defining the default and incremental endpoints for the Stripe API integration.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/stripe.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# The most popular Stripe API's endpoints\nSTRIPE_ENDPOINTS = (\"Subscription\", \"Account\", \"Coupon\", \"Customer\", \"Product\", \"Price\")\n# Possible incremental endpoints\n# The incremental endpoints default to Stripe API endpoints with uneditable data.\nINCREMENTAL_ENDPOINTS = (\"Event\", \"Invoice\", \"BalanceTransaction\")\n```\n\n----------------------------------------\n\nTITLE: Setting AWS Credentials Directly\nDESCRIPTION: Basic example of setting AWS credentials by directly assigning values to credential attributes.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\naws_credentials.aws_access_key_id = \"ACCESS_KEY_ID\"\naws_credentials.aws_secret_access_key = \"SECRET_ACCESS_KEY\"\naws_credentials.region_name = \"us-east-1\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Standalone Weaviate with Contextionary Vectorizer in DLT\nDESCRIPTION: Shows the configuration for running Weaviate fully standalone using the contextionary vectorizer, which can be used offline without external APIs for vectorization.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/weaviate.md#2025-04-14_snippet_15\n\nLANGUAGE: toml\nCODE:\n```\n[destination.weaviate]\nvectorizer=\"text2vec-contextionary\"\nmodule_config={text2vec-contextionary = { vectorizeClassName = false, vectorizePropertyName = true}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Credentials in secrets.toml\nDESCRIPTION: Example configuration for Snowflake credentials in the .dlt/secrets.toml file, showing required parameters like database, username, password, host, warehouse, and role.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[destination.snowflake.credentials]\ndatabase = \"dlt_data\"\npassword = \"<password>\"\nusername = \"loader\"\nhost = \"kgiotue-wn98412\"\nwarehouse = \"COMPUTE_WH\"\nrole = \"DLT_LOADER_ROLE\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Destinations in YAML\nDESCRIPTION: Example configuration for iceberg and duckdb destinations using relative paths in the dlt project manifest file. This ensures files are stored in profile-separated data directories.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/projects.md#2025-04-14_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\ndestinations:\n  iceberg:\n    bucket_url: lake\n  my_duckdb:\n    type: duckdb\n```\n\n----------------------------------------\n\nTITLE: Apply Column Hints Example\nDESCRIPTION: Code example showing how to apply data type hints to specific columns when loading data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/airtable.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nbase_id = \"Please set me up!\"       # The ID of the base.\ntable_names = [\"Table1\", \"Table2\"]   # A list of table IDs or table names to load.\nresource_name = \"Please set me up!\" # The table name we want to apply hints.\nfield_name = \"Please set me up!\"    # The table field name for which we want to apply hints.\n\nairtables = airtable_source(\n     base_id=\"Please set me up!\",\n     table_names=[\"Table1\", \"Table2\"],\n)\n\nairtables.resources[resource_name].apply_hints(\n     primary_key=field_name,\n     columns={field_name: {\"data_type\": \"text\"}},\n)\nload_info = pipeline.run(airtables, write_disposition=\"replace\")\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Column Hints in YAML\nDESCRIPTION: YAML configuration that sets default hints for columns added by the JSON normalizer, including row keys, parent keys, not null constraints, unique constraints, and root keys.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema.md#2025-04-14_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nsettings:\n  default_hints:\n    row_key:\n      - _dlt_id\n    parent_key:\n      - _dlt_parent_id\n    not_null:\n      - _dlt_id\n      - _dlt_root_id\n      - _dlt_parent_id\n      - _dlt_list_idx\n      - _dlt_load_id\n    unique:\n      - _dlt_id\n    root_key:\n      - _dlt_root_id\n```\n\n----------------------------------------\n\nTITLE: Setting Global Schema Contract on Pipeline Run\nDESCRIPTION: Sets a global schema contract at pipeline run time that applies to all resources. This configuration raises an error on any encountered schema change by setting all contract aspects to 'freeze'.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema-contracts.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipeline.run(my_source, schema_contract=\"freeze\")\n```\n\n----------------------------------------\n\nTITLE: Defining Users Resource in Python\nDESCRIPTION: Python function defining the users resource for the Slack pipeline. It yields all user data as a dlt resource with a primary key and replace write disposition.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/slack.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(name=\"users\", primary_key=\"id\", write_disposition=\"replace\")\ndef users_resource() -> Iterable[TDataItem]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Loading Historical Data from Selected Resources\nDESCRIPTION: Loads historical data from specific HubSpot resources (contacts and companies) including their time history.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/hubspot.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nload_data = hubspot(include_history=True).with_resources(\"companies\",\"contacts\")\nload_info = pipeline.run(load_data)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Initializing Filesystem Source with dlt CLI\nDESCRIPTION: This command initializes a new dlt pipeline with filesystem as the source and DuckDB as the destination. It creates a new directory with necessary files and configuration settings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/basic.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init filesystem duckdb\n```\n\n----------------------------------------\n\nTITLE: DLT Project Configuration Example\nDESCRIPTION: Sample YAML configuration showing transformation and pipeline setup in a dlt project manifest file. Defines a stressed transformation using Arrow engine and a bronze pipeline with filesystem destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/projects.md#2025-04-14_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ntransformations:\n  stressed_transformation:\n    engine: arrow\n    cache: stressed_cache\npipelines:\n  bronze_pipe:\n    destination: filesystem\n    dataset_name: bronze\n```\n\n----------------------------------------\n\nTITLE: GCP Service Account Credentials Example\nDESCRIPTION: Example of using GcpServiceAccountCredentials in a Google Analytics source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.credentials import GcpServiceAccountCredentials\nfrom google.analytics import BetaAnalyticsDataClient\n\n@dlt.source\ndef google_analytics(\n    property_id: str = dlt.config.value,\n    credentials: GcpServiceAccountCredentials = dlt.secrets.value,\n):\n    # Retrieve native credentials for Google clients\n    # For example, build the service object for Google Analytics PI.\n    client = BetaAnalyticsDataClient(credentials=credentials.to_native_credentials())\n\n    # Get a string representation of the credentials\n    # Returns a string representation of the credentials in the format client_email@project_id.\n    credentials_str = str(credentials)\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Amazon Kinesis Credentials in TOML\nDESCRIPTION: TOML configuration for storing Amazon Kinesis credentials securely in the secrets.toml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/amazon_kinesis.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# Put your secret values and credentials here.\n# Note: Do not share this file and do not push it to GitHub!\n[sources.kinesis.credentials]\naws_access_key_id=\"AKIA********\"\naws_secret_access_key=\"K+o5mj********\"\nregion_name=\"please set me up!\" # aws region name\n```\n\n----------------------------------------\n\nTITLE: Initializing dlt Project with Postgres Pipeline\nDESCRIPTION: Command to initialize a dlt project with a pipeline that loads data to Postgres.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/postgres.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init chess postgres\n```\n\n----------------------------------------\n\nTITLE: Setting Native Table Format in Python\nDESCRIPTION: Configure a resource to use native table format instead of Iceberg.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/iceberg.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(\n  table_format=\"native\"\n)\ndef my_resource():\n    ...\n\npipeline = dlt.pipeline(\"loads_iceberg\", destination=\"iceberg\")\n```\n\n----------------------------------------\n\nTITLE: Creating a dlt Pipeline for LanceDB\nDESCRIPTION: Python code to create a dlt pipeline that uses LanceDB as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/lancedb.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n  pipeline_name=\"movies\",\n  destination=\"lancedb\",\n  dataset_name=\"MoviesDataset\",\n)\n```\n\n----------------------------------------\n\nTITLE: AWS Credentials from Botocore Session\nDESCRIPTION: Example showing how to import and parse AWS credentials from an external botocore session.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport botocore.session\n\naws_credentials = AwsCredentials()\nsession = botocore.session.get_session()\naws_credentials.parse_native_representation(session)\nprint(aws_credentials.aws_access_key_id)\n```\n\n----------------------------------------\n\nTITLE: Creating Scrapy Pipeline Runner in Python\nDESCRIPTION: Python code to create a pipeline runner for Scrapy using the 'create_pipeline_runner' function, setting batch size and item limit.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/scrapy.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nscraping_host = create_pipeline_runner(pipeline, MySpider, batch_size=10)\nscraping_host.pipeline_runner.scraping_resource.add_limit(2)\nscraping_host.run(dataset_name=\"quotes\", write_disposition=\"append\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Filesystem Source in Python\nDESCRIPTION: Basic initialization of a filesystem source with direct parameter specification for accessing CSV files.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/basic.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.filesystem import filesystem\n\nfilesystem_source = filesystem(\n  bucket_url=\"file://Users/admin/Documents/csv_files\",\n  file_glob=\"*.csv\"\n)\n```\n\n----------------------------------------\n\nTITLE: Shopify Pipeline Configuration\nDESCRIPTION: TOML configuration for Shopify pipeline settings including shop URL and organization ID\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/shopify.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[sources.shopify_dlt]\nshop_url = \"Please set me up!\"\norganization_id = \"Please set me up!\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Telemetry Endpoint in dlt\nDESCRIPTION: Configuration snippet to redirect telemetry data to a custom endpoint using the global config.toml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/telemetry.md#2025-04-14_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[runtime]\ndlthub_telemetry_endpoint=\"telemetry-tracker.services4745.workers.dev\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Project Configuration with Profile\nDESCRIPTION: This command shows the merged configuration settings for the 'prod' profile in a DLT project.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\ndlt project --profile prod config show\n```\n\n----------------------------------------\n\nTITLE: Azure Blob Storage SAS Token Credentials\nDESCRIPTION: Configuration for Azure Blob Storage using SAS token or storage account key authentication.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_14\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem.credentials]\n# The storage account name is always required\nazure_storage_account_name = \"account_name\" # please set me up!\n# You can set either account_key or sas_token, only one is needed\nazure_storage_account_key = \"account_key\" # please set me up!\nazure_storage_sas_token = \"sas_token\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Advanced Filesystem Layout Configuration in TOML\nDESCRIPTION: Advanced TOML configuration for filesystem destination layout with custom placeholders, current datetime specification, and additional arguments for automatic directory creation.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_28\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\nlayout = \"{table_name}/{test_placeholder}/{YYYY}-{MM}-{DD}/{ddd}/{mm}/{load_id}.{file_id}.{ext}\"\nextra_placeholders = { \"test_placeholder\" = \"test_value\" }\ncurrent_datetime=\"2024-04-14T00:00:00\"\n# for automatic directory creation in the local filesystem\nkwargs = '{\"auto_mkdir\": true}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake S3 Stage in TOML\nDESCRIPTION: This snippet shows how to configure a Snowflake S3 stage in the TOML configuration file. It sets the stage name to prevent dlt from forwarding S3 bucket credentials on every command.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_14\n\nLANGUAGE: toml\nCODE:\n```\n[destination]\nstage_name=\"PUBLIC.my_s3_stage\"\n```\n\n----------------------------------------\n\nTITLE: Matomo Pipeline Configuration\nDESCRIPTION: TOML configuration for Matomo pipeline settings including URL and site IDs\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/matomo.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[sources.matomo]\nurl = \"Please set me up !\" # please set me up!\nqueries = [\"a\", \"b\", \"c\"] # please set me up!\nsite_id = 0 # please set me up!\nlive_events_site_id = 0 # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Defining Transformations in YAML\nDESCRIPTION: This YAML snippet shows how to define transformations in the dlt.yml file. It specifies the transformation engine (dbt in this case) and the cache it will run on.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/transformations/setup.md#2025-04-14_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ntransformations:\n  github_events_transformations:\n    engine: dbt\n    cache: github_events_cache\n```\n\n----------------------------------------\n\nTITLE: Azure Blob Storage Concurrency Configuration\nDESCRIPTION: Configuration to adjust the maximum concurrency for blob uploads in Azure Blob Storage to optimize memory usage and connection count.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_16\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem.kwargs]\nmax_concurrency=3\n```\n\n----------------------------------------\n\nTITLE: Azure Blob Storage Configuration with Full URL\nDESCRIPTION: Configuration for Azure Blob Storage using the abfss:// URL scheme with container name, storage account name, and path.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_12\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\nbucket_url = \"abfss://<container_name>@<storage_account_name>.dfs.core.windows.net/path\"\n```\n\n----------------------------------------\n\nTITLE: Using Random Iceberg Table Location\nDESCRIPTION: Configuration to add a random location tag to Iceberg tables to prevent conflicts when tables are deleted and recreated.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/athena.md#2025-04-14_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\n[destination.athena]\ntable_location_layout=\"{dataset_name}/{table_name}_{location_tag}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Zendesk Credentials in TOML\nDESCRIPTION: Configuration template for storing Zendesk authentication credentials in secrets.toml file. Supports multiple authentication methods including email/password, API token, and OAuth token.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/zendesk.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n#Zendesk support credentials\n[sources.zendesk.credentials]\nsubdomain = \"subdomain\" # Zendesk subdomain\nemail = \"set me up\" # Email used to login to Zendesk\npassword = \"set me up\" # Password for Zendesk account\ntoken = \"set me up\" # For API token auth\noauth_token = \"set me up\" # Use Zendesk support OAuth token or Zendesk chat OAuth token\n```\n\n----------------------------------------\n\nTITLE: Enabling Constraints in SQLAlchemy Destination\nDESCRIPTION: Configuration to enable primary key and unique constraints in the SQLAlchemy destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/sqlalchemy.md#2025-04-14_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[destination.sqlalchemy]\ncreate_unique_indexes=true\ncreate_primary_keys=true\n```\n\n----------------------------------------\n\nTITLE: Generating OAuth Refresh Token for Google Sheets\nDESCRIPTION: Command to run a Python script that generates a refresh token for OAuth authentication with Google Sheets API.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npython google_sheets/setup_script_gcp_oauth.py\n```\n\n----------------------------------------\n\nTITLE: Implementing Prefect Task Decorator for Slack User Collection in dlt\nDESCRIPTION: This snippet shows how to transform a standard dlt function into a Prefect task using the @task decorator. The function is responsible for extracting Slack users data as part of a larger pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-prefect.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@task\ndef get_users() -> None:\n    \"\"\"Execute a pipeline that will load the Slack users list.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Slack Data Pipeline with DLT in Python\nDESCRIPTION: This snippet demonstrates how to configure a DLT pipeline for Slack data. It specifies the pipeline name, destination (e.g., DuckDB), and dataset name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/slack.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"slack\",  # Use a custom name if desired\n    destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"slack_data\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Modal Function for Pipeline Execution\nDESCRIPTION: Python code setting up a Modal Function to run the SQL pipeline in a containerized environment with scheduling capabilities\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-modal.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodal_function\n```\n\n----------------------------------------\n\nTITLE: Preventing Staging Files Truncation\nDESCRIPTION: TOML configuration to prevent automatic truncation of previously loaded files in staging storage.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/staging.md#2025-04-14_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[destination.redshift]\ntruncate_tables_on_staging_destination_before_load=false\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Settings using TOML Configuration\nDESCRIPTION: Shows how to configure CSV writer settings such as delimiter, header inclusion, and quoting style using TOML configuration. This example changes the delimiter to a pipe, removes headers, and quotes all values.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/file-formats/csv.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[normalize.data_writer]\ndelimiter=\"|\"\ninclude_header=false\nquoting=\"quote_all\"\n```\n\n----------------------------------------\n\nTITLE: AWS IAM Policy for S3 Bucket Access\nDESCRIPTION: Sample IAM policy that provides the minimum permissions required for dlt to access an S3 bucket, including list, get, put, and delete operations.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"DltBucketAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:DeleteObject\",\n                \"s3:GetObjectAttributes\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::dlt-ci-test-bucket/*\",\n                \"arn:aws:s3:::dlt-ci-test-bucket\"\n            ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying AWS Region in secrets.toml\nDESCRIPTION: Configuration to set the AWS region for the S3 bucket access.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem.credentials]\nregion_name=\"eu-central-1\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Pipeline Output with dlt CLI\nDESCRIPTION: This command uses the dlt CLI to display information about a specific pipeline. It helps ensure that everything loaded as expected. Replace <pipeline_name> with the actual name of your pipeline, such as 'data_enrichment_two'.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/currency_conversion_data_enrichment.md#2025-04-14_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Staged Files Cleanup Configuration\nDESCRIPTION: TOML configuration to enable immediate deletion of staged files after loading into Databricks.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/databricks.md#2025-04-14_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[destination.databricks]\nkeep_staged_files = false\n```\n\n----------------------------------------\n\nTITLE: Dataset Head Query\nDESCRIPTION: CLI commands to view the first rows of a dataset table\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ndlt dataset my_pipeline_dataset head items\n```\n\nLANGUAGE: sh\nCODE:\n```\ndlt dataset duckdb_dataset head items --limit 50\n```\n\n----------------------------------------\n\nTITLE: AWS Configuration Files\nDESCRIPTION: TOML configuration files for AWS credentials and bucket URL settings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_15\n\nLANGUAGE: toml\nCODE:\n```\n[sources.aws_readers.credentials]\naws_access_key_id = \"key_id\"\naws_secret_access_key = \"access_key\"\nregion_name = \"region\"\n```\n\nLANGUAGE: toml\nCODE:\n```\n[sources.aws_readers]\nbucket_url = \"bucket_url\"\n```\n\n----------------------------------------\n\nTITLE: Defining an Incomplete Table Schema in YAML\nDESCRIPTION: This YAML snippet defines a table schema for Ethereum blocks with an incomplete column 'number'. Such a table is considered new by dlt.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema-contracts.md#2025-04-14_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nblocks:\n  description: Ethereum blocks\n  write_disposition: append\n  columns:\n    number:\n      nullable: false\n      primary_key: true\n      name: number\n```\n\n----------------------------------------\n\nTITLE: Normalizing JSON Data for Weaviate in DLT\nDESCRIPTION: Demonstrates how clashing property names in JSON data are normalized when using case-insensitive naming convention in Weaviate destination. The example shows the input and output JSON structures.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/weaviate.md#2025-04-14_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\"camelCase\": 1, \"CamelCase\": 2}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"camelcase\": 2}\n```\n\n----------------------------------------\n\nTITLE: Configuring Change Tracking with Delete Handling Options\nDESCRIPTION: Implementation of change tracking table creation with configuration options for handling deletes, including the hard_delete parameter which determines whether deleted records are permanently removed from the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/ms-sql.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt_plus.sources.mssql import create_change_tracking_table\n\nincremental_resource = create_change_tracking_table(\n    credentials=engine,\n    table=table_name,\n    schema=schema_name,\n    hard_delete=True,\n)\npipeline.run(incremental_resource)\n```\n\n----------------------------------------\n\nTITLE: DLT MongoDB Secrets Configuration\nDESCRIPTION: TOML configuration for storing MongoDB connection credentials securely.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mongodb.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n# put your secret values and credentials here\n# do not share this file and do not push it to github\n[sources.mongodb]\nconnection_url = \"mongodb connection_url\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Implementing Workable Source Function in Python\nDESCRIPTION: Python function that serves as the main source for extracting data from Workable API endpoints.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/workable.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source(name=\"workable\")\ndef workable_source(\n    access_token: str = dlt.secrets.value,\n    subdomain: str = dlt.config.value,\n    start_date: Optional[DateTime] = None,\n    load_details: bool = False,\n) -> Iterable[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Matomo Pipeline in Python\nDESCRIPTION: This snippet shows how to configure a DLT pipeline for Matomo data. It specifies the pipeline name, destination (DuckDB in this case), and dataset name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/matomo.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"matomo\",  # Use a custom name if desired\n    destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"matomo_data\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: ConnectorX MS SQL Server Configuration\nDESCRIPTION: Special connection string configuration to resolve ConnectorX issues with MS SQL Server by including both Encrypt parameters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/troubleshooting.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\nsources.sql_database.credentials=\"mssql://user:password@server:1433/database?driver=ODBC+Driver+17+for+SQL+Server&Encrypt=yes&encrypt=true\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Redshift Credentials with Connection String\nDESCRIPTION: Example of configuring Redshift credentials using a database connection string compatible with psycopg2 or SQLAlchemy.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/redshift.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n# Keep it at the top of your TOML file, before any section starts\ndestination.redshift.credentials=\"redshift://loader:<password>@localhost/dlt_data?connect_timeout=15\"\n```\n\n----------------------------------------\n\nTITLE: Using qdrant_adapter with Multiple Embedding Fields\nDESCRIPTION: Example of using the qdrant_adapter function with multiple fields for vector embedding generation.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/qdrant.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nqdrant_adapter(\n    resource,\n    embed=[\"title\", \"description\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Automatic Staging Dataset Cleanup\nDESCRIPTION: TOML configuration to enable automatic truncation of staging dataset tables after load completion.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/staging.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[load]\ntruncate_staging_dataset=true\n```\n\n----------------------------------------\n\nTITLE: Installing DLT CLI Dependencies\nDESCRIPTION: Command to install additional CLI dependencies required for deployment\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-github-actions.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[cli]\"\n```\n\n----------------------------------------\n\nTITLE: Managing Custom Pagination State in DLT (Python)\nDESCRIPTION: Demonstrates how to manage custom pagination state directly in Python using DLT's resource state. This allows for creating custom pagination logic based on specific requirements.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/frequently-asked-questions.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstate = dlt.current.resource_state()\nstate[\"your_custom_key\"] = \"your_value\"\n```\n\n----------------------------------------\n\nTITLE: Adding Another Source to dlt Project\nDESCRIPTION: Adds another verified source (chess) with DuckDB as the destination to an existing project.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add-a-verified-source.md#2025-04-14_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ndlt init chess duckdb\n```\n\n----------------------------------------\n\nTITLE: DLT Source Function Definition\nDESCRIPTION: Example of defining a DLT source function with credentials injection\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# pipedrive.py\n\n@dlt.source\ndef deals(api_key: str = dlt.secrets.value):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Format Settings in TOML\nDESCRIPTION: This TOML configuration snippet demonstrates how to set non-default CSV format settings for Snowflake, including delimiter, header inclusion, and error handling.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_21\n\nLANGUAGE: toml\nCODE:\n```\n[destination.snowflake.csv_format]\ndelimiter=\"|\"\ninclude_header=false\non_error_continue=true\n```\n\n----------------------------------------\n\nTITLE: Initializing dlt Notion Source with BigQuery Target\nDESCRIPTION: Command to initialize a dlt pipeline using the Notion verified source and BigQuery as the target. This creates necessary configuration files and folders.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-google-cloud-run.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init notion bigquery\n```\n\n----------------------------------------\n\nTITLE: Generating Zendesk Support OAuth Token with cURL\nDESCRIPTION: This cURL command creates an OAuth access token for Zendesk Support API. It requires the subdomain, email address, password, and client ID. The scope is set to 'read' but can be customized as needed.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/zendesk.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://{subdomain}.zendesk.com/api/v2/oauth/tokens.json \\\n-X POST \\\n-v -u {email_address}:{password} \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"token\": {\n    \"client_id\": 223443,\n    \"scopes\": [\n      \"read\"\n    ]\n  }\n}'\n```\n\n----------------------------------------\n\nTITLE: Defining Pipelines in dlt.yml\nDESCRIPTION: Illustrates how to configure a pipeline in the dlt.yml manifest file, specifying the source, destination, and dataset name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/projects.md#2025-04-14_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ngithub_pipeline:\n  source: github\n  destination: duckdb\n  dataset_name: github_events_dataset\n```\n\n----------------------------------------\n\nTITLE: Initializing DLT+ Project\nDESCRIPTION: CLI command to initialize a new DLT+ project with Arrow source and DuckDB destination\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt project init arrow duckdb\n```\n\n----------------------------------------\n\nTITLE: Deploying Pipeline with GitHub Actions\nDESCRIPTION: Command to create a GitHub workflow that runs the pipeline every 30 minutes\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-github-actions.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndlt deploy chess_pipeline.py github-action --schedule \"*/30 * * * *\"\n```\n\n----------------------------------------\n\nTITLE: Shopify API Credentials Configuration\nDESCRIPTION: TOML configuration for storing Shopify API credentials and access tokens\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/shopify.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n#shopify\n[sources.shopify_dlt]\nprivate_app_password=\"Please set me up!\" #Admin API access token\naccess_token=\"Please set me up!\" #Partner API access token\n```\n\n----------------------------------------\n\nTITLE: Displaying dlt+ Project Directory Structure\nDESCRIPTION: This code snippet shows the general directory structure of a dlt+ project, including the .dlt folder for settings, _data folder for local storage, sources folder for code, and the main dlt.yml manifest file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/core-concepts/project.md#2025-04-14_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n.\nâ”œâ”€â”€ .dlt/                 # your dlt settings including profile settings\nâ”‚   â”œâ”€â”€ config.toml\nâ”‚   â”œâ”€â”€ dev.secrets.toml\nâ”‚   â””â”€â”€ secrets.toml\nâ”œâ”€â”€ _data/             # local storage for your project, excluded from git\nâ”œâ”€â”€ sources/              # your sources, contains the code for the arrow source\nâ”‚   â””â”€â”€ arrow.py\nâ”œâ”€â”€ .gitignore\nâ”œâ”€â”€ requirements.txt\nâ””â”€â”€ dlt.yml               # the main project manifest\n```\n\n----------------------------------------\n\nTITLE: Configuring MS SQL with SQLAlchemy connection string\nDESCRIPTION: Alternative method to configure MS SQL Server connection using a SQLAlchemy-style connection string in the secrets.toml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/mssql.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\ndestination.mssql.credentials=\"mssql://loader:<password>@loader.database.windows.net/dlt_data?TrustServerCertificate=yes&Encrypt=yes&LongAsMax=yes\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Notion Pipeline in Python\nDESCRIPTION: Python code to configure a custom Notion pipeline, specifying the pipeline name, destination, and dataset name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/notion.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n   pipeline_name=\"notion\",  # Use a custom name if desired\n   destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n   dataset_name=\"notion_database\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Table Engine for Specific Resources\nDESCRIPTION: Python code example showing how to set a specific table engine for individual resources using clickhouse_adapter\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/clickhouse.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.destinations.adapters import clickhouse_adapter\n\n@dlt.resource()\ndef my_resource():\n    ...\n\nclickhouse_adapter(my_resource, table_engine_type=\"merge_tree\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Security Profiles\nDESCRIPTION: YAML configuration for setting up access controls and data contracts for different datasets within the dlt+ project.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/data-access.md#2025-04-14_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nprofiles:\\n    access:\\n        datasets:\\n            github_events_dataset:\\n                # no new tables, no column changes\\n                contract: freeze\\n\\n            reports_dataset:\\n                # allow new tables but no column changes\\n                contract:\\n                    tables: evolve\\n                    columns: freeze\\n                    data_type: freeze\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for dlt Configuration\nDESCRIPTION: This snippet demonstrates how to set environment variables for configuring dlt sources and destinations.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Environment variables are set up the same way both for configs and secrets\nexport RUNTIME__LOG_LEVEL=\"INFO\"\nexport DESTINATION__FILESYSTEM__BUCKET_URL=\"s3://[your_bucket_name]\"\nexport NORMALIZE__DATA_WRITER__DISABLE_COMPRESSION=\"true\"\nexport SOURCE__NOTION__API_KEY=\"api_key\"\nexport DESTINATION__FILESYSTEM__CREDENTIALS__AWS_ACCESS_KEY_ID=\"ABCDEFGHIJKLMNOPQRST\"\nexport DESTINATION__FILESYSTEM__CREDENTIALS__AWS_SECRET_ACCESS_KEY=\"1234567890_access_key\"\n```\n\n----------------------------------------\n\nTITLE: Data Filtering Example\nDESCRIPTION: Shows how to add filters to resources to limit the data based on specific conditions.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/source.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsource.deals.add_filter(lambda deal: deal[\"created_at\"] > yesterday)\n```\n\n----------------------------------------\n\nTITLE: Configuring Project Settings in dlt.yml\nDESCRIPTION: Shows how to override default project settings in the dlt.yml manifest file, including project name, data directory, and default profile.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/projects.md#2025-04-14_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nproject:\n  name: test_project\n  data_dir: \"{env.DLT_DATA_DIR}/{current_profile}\"\n  allow_undefined_entities: false\n  default_profile: tests\n  local_dir: \"{data_dir}/local\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Google Sources and Destinations in TOML\nDESCRIPTION: This snippet demonstrates how to configure credentials for multiple Google sources and destinations using full paths in a TOML file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_14\n\nLANGUAGE: toml\nCODE:\n```\n# Google Sheet credentials\n[sources.google_sheets.credentials]\nclient_email = \"<client_email from services.json>\"\nprivate_key = \"<private_key from services.json>\"\nproject_id = \"<project_id from services.json>\"\n\n# Google Analytics credentials\n[sources.google_analytics.credentials]\nclient_email = \"<client_email from services.json>\"\nprivate_key = \"<private_key from services.json>\"\nproject_id = \"<project_id from services.json>\"\n\n# BigQuery credentials\n[destination.bigquery.credentials]\nclient_email = \"<client_email from services.json>\"\nprivate_key = \"<private_key from services.json>\"\nproject_id = \"<project_id from services.json>\"\n```\n\n----------------------------------------\n\nTITLE: Airtable Source Configuration\nDESCRIPTION: TOML configuration for specifying Airtable base and table settings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/airtable.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[sources.airtable]\nbase_id = \"Please set me up!\"       # The ID of the base.\ntable_names = [\"Table1\",\"Table2\"]   # A list of table IDs or table names to load.\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Command to install required dependencies from requirements.txt\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/bigquery.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Passing MS SQL credentials directly to pipeline\nDESCRIPTION: Example of passing MS SQL Server credentials directly to the dlt pipeline instance instead of using secrets.toml.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/mssql.md#2025-04-14_snippet_9\n\nLANGUAGE: py\nCODE:\n```\npipeline = dlt.pipeline(\n  pipeline_name='chess',\n  destination=dlt.destinations.mssql(\"mssql://loader:<password>@loader.database.windows.net/dlt_data?connect_timeout=15\"),\n  dataset_name='chess_data')\n```\n\n----------------------------------------\n\nTITLE: SQL Database Configuration\nDESCRIPTION: TOML configuration for SQL database source credentials\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[sources.sql_db_1]\ntable_names = [\"family\", \"clan\"]\n\n[sources.sql_db_1.credentials]\ndrivername = \"mysql+pymysql\"\ndatabase = \"Rfam\"\nusername = \"rfamro\"\nhost = \"mysql-rfam-public.ebi.ac.uk\"\nport = 4497\n```\n\n----------------------------------------\n\nTITLE: Limiting Processed Items in Scrapy Pipeline with Python\nDESCRIPTION: Python code demonstrating how to limit the number of items processed by the Scrapy pipeline using the 'on_before_start' function.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/scrapy.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef on_before_start(res: DltResource) -> None:\n    res.add_limit(2)\n\nrun_pipeline(\n    pipeline,\n    MySpider,\n    batch_size=10,\n    scrapy_settings={\n        \"DEPTH_LIMIT\": 100,\n        \"SPIDER_MIDDLEWARES\": {\n            \"scrapy.spidermiddlewares.depth.DepthMiddleware\": 200,\n            \"scrapy.spidermiddlewares.httperror.HttpErrorMiddleware\": 300,\n        }\n    },\n    on_before_start=on_before_start,\n    write_disposition=\"append\",\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing DLT Pipeline Configuration for Shopify\nDESCRIPTION: Sets up a basic DLT pipeline configuration specifying the pipeline name, destination database, and dataset name. This is the initial setup required before loading any data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/shopify.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"shopify\",  # Use a custom name if desired\n    destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"shopify_data\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Delta Storage Options\nDESCRIPTION: TOML configuration for setting up Delta storage options in the destination configuration.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/delta.md#2025-04-14_snippet_12\n\nLANGUAGE: toml\nCODE:\n```\n[destination.delta]\ndeltalake_storage_options = '{\"AWS_S3_LOCKING_PROVIDER\": \"dynamodb\", \"DELTA_DYNAMO_TABLE_NAME\": \"custom_table_name\"}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Mux Credentials in TOML\nDESCRIPTION: TOML configuration file structure for storing Mux API credentials securely.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mux.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# Put your secret values and credentials here. Do not share this file and do not push it to GitHub\n[sources.mux]\nmux_api_access_token = \"please set me up\" # Mux API access token\nmux_api_secret_key = \"please set me up!\" # Mux API secret key\n```\n\n----------------------------------------\n\nTITLE: Defining Absence Types Resource in Python\nDESCRIPTION: Python function to define the 'absence_types' resource. It uses a replace write disposition to completely replace existing data on each load.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/personio.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(primary_key=\"id\", write_disposition=\"replace\")\ndef absence_types(items_per_page: int = ITEMS_PER_PAGE) -> Iterable[TDataItem]:\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: Pipeline Execution and Data Review\nDESCRIPTION: Shell commands to execute the pipeline script and inspect the created tables and data\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/dispatch-to-multiple-tables.md#2025-04-14_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython github_events_dispatch.py\n```\n\nLANGUAGE: shell\nCODE:\n```\ndlt pipeline -v github_events info\ndlt pipeline github_events trace\n```\n\nLANGUAGE: shell\nCODE:\n```\ndlt pipeline -v github_events show\n```\n\n----------------------------------------\n\nTITLE: Filtering Files by Size\nDESCRIPTION: Example of filtering files based on file size.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/basic.md#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.filesystem import filesystem, read_csv\n\nMAX_SIZE_IN_BYTES = 10\n\n# Filter files accessing size_in_bytes field\nfiltered_files = filesystem(bucket_url=\"s3://bucket_name\", file_glob=\"directory/*.csv\")\nfiltered_files.add_filter(lambda item: item[\"size_in_bytes\"] < MAX_SIZE_IN_BYTES)\n\nfilesystem_pipe = (filtered_files | read_csv())\npipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\nload_info = pipeline.run(filesystem_pipe)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Data Load Time for dlt Pipeline Tables\nDESCRIPTION: This command provides information about the data loading time for each table in a dlt pipeline. It offers insights into the performance of the data loading process.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/running-in-production/monitoring.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> load-package\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Multiple Google Sources and Destinations\nDESCRIPTION: This snippet shows how to set environment variables for configuring multiple Google sources and destinations.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n# Google Sheet credentials\nexport SOURCES__GOOGLE_SHEETS__CREDENTIALS__CLIENT_EMAIL=\"<client_email>\"\nexport SOURCES__GOOGLE_SHEETS__CREDENTIALS__PRIVATE_KEY=\"<private_key>\"\nexport SOURCES__GOOGLE_SHEETS__CREDENTIALS__PROJECT_ID=\"<project_id>\"\n\n# Google Analytics credentials\nexport SOURCES__GOOGLE_ANALYTICS__CREDENTIALS__CLIENT_EMAIL=\"<client_email>\"\nexport SOURCES__GOOGLE_ANALYTICS__CREDENTIALS__PRIVATE_KEY=\"<private_key>\"\nexport SOURCES__GOOGLE_ANALYTICS__CREDENTIALS__PROJECT_ID=\"<project_id>\"\n```\n\n----------------------------------------\n\nTITLE: Creating Asana Data Source\nDESCRIPTION: Initializes the Asana data source to fetch data from all available fields.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/asana.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nload_data = asana_source()\n```\n\n----------------------------------------\n\nTITLE: Service Account Credentials Configuration\nDESCRIPTION: TOML configuration for Google Ads service account authentication setup.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_ads.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[sources.google_ads]\ndev_token = \"please set me up!\"\ncustomer_id = \"please set me up!\"\nimpersonated_email = \"please set me up\"\n```\n\n----------------------------------------\n\nTITLE: Running Modal Pipeline Once\nDESCRIPTION: CLI command to execute the pipeline one time using Modal\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-modal.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nmodal run sql_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Using Connection String for Snowflake Credentials\nDESCRIPTION: Example of providing Snowflake credentials as a connection string in the TOML file, which should be placed at the top of the file before any section starts.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n# Keep it at the top of your TOML file, before any section starts\ndestination.snowflake.credentials=\"snowflake://loader:<password>@kgiotue-wn98412/dlt_data?warehouse=COMPUTE_WH&role=DLT_LOADER_ROLE\"\n```\n\n----------------------------------------\n\nTITLE: Setting Encoding for All Responses in REST API Source Configuration\nDESCRIPTION: This snippet demonstrates how to configure a REST API source to set the correct encoding for all responses. It shows a simpler use case of response_actions with a single custom function.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/rest_api/advanced.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef set_encoding(response, *args, **kwargs):\n    # Sets the encoding in case it's not correctly detected\n    response.encoding = 'windows-1252'\n    return response\n\nsource_config = {\n    \"client\": {\n        # ...\n    },\n    \"resources\": [\n        {\n            \"name\": \"issues\",\n            \"endpoint\": {\n                \"path\": \"issues\",\n                \"response_actions\": [\n                    set_encoding,\n                ],\n            },\n        },\n    ],\n}\n```\n\n----------------------------------------\n\nTITLE: Explicit Credential Injection in Pipedrive Source\nDESCRIPTION: Example showing how to explicitly pass credentials to a Pipedrive source instead of using automatic injection. Demonstrates optional nature of the injection mechanism.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/advanced.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source(name=\"pipedrive\")\ndef pipedrive_source(\n    pipedrive_api_key: str = dlt.secrets.value,\n    since_timestamp: Optional[Union[pendulum.DateTime, str]] = \"1970-01-01 00:00:00\",\n) -> Iterator[DltResource]:\n  ...\n\nmy_key = os.environ[\"MY_PIPEDRIVE_KEY\"]\nmy_source = pipedrive_source(pipedrive_api_key=my_key)\n```\n\n----------------------------------------\n\nTITLE: Initializing dlt Project with Delta Destination\nDESCRIPTION: Shell command to initialize a dlt project with a SQL database source and Delta destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/delta.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt project init sql_database delta\n```\n\n----------------------------------------\n\nTITLE: Configuring Local DLT Credentials in TOML\nDESCRIPTION: Example of credentials configuration in the .dlt/secrets.toml file for Pipedrive source and BigQuery destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add_credentials.md#2025-04-14_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[sources.pipedrive]\npipedrive_api_key = \"pipedrive_api_key\" # please set me up!\n\n[destination.bigquery]\nlocation = \"US\"\n\n[destination.bigquery.credentials]\nproject_id = \"project_id\" # please set me up!\nprivate_key = \"private_key\" # please set me up!\nclient_email = \"client_email\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Default Credentials Configuration\nDESCRIPTION: Minimal configuration for using default Google credentials\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/bigquery.md#2025-04-14_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[destination.bigquery]\nlocation = \"US\"\n```\n\n----------------------------------------\n\nTITLE: Enabling case-sensitive identifiers for MS SQL\nDESCRIPTION: Configuration to enable case-sensitive identifiers for MS SQL Server databases that use case-sensitive collation.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/mssql.md#2025-04-14_snippet_10\n\nLANGUAGE: toml\nCODE:\n```\n[destination.mssql]\nhas_case_sensitive_identifiers=true\n```\n\n----------------------------------------\n\nTITLE: Installing DLT with Databricks Dependencies\nDESCRIPTION: Command to install the DLT library with Databricks-specific dependencies\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/databricks.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[databricks]\"\n```\n\n----------------------------------------\n\nTITLE: Initializing DLT Pipeline Configuration\nDESCRIPTION: Sets up a basic DLT pipeline configuration specifying the pipeline name, destination database, and dataset name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/hubspot.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"hubspot\",  # Use a custom name if desired\n    destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"hubspot_data\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Verifying Pipeline Execution\nDESCRIPTION: Command to display information about the executed pipeline, showing details of the data loaded from Notion.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/notion.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Configuring Athena Workgroup\nDESCRIPTION: Configuration to specify a custom Athena workgroup in the secrets.toml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/athena.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[destination.athena]\nathena_work_group=\"my_workgroup\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Jira Credentials\nDESCRIPTION: TOML configuration file structure for storing Jira API credentials securely.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/jira.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# put your secret values and credentials here. Please do not share this file, and do not push it to GitHub\n[sources.jira]\nsubdomain = \"set me up!\" # please set me up!\nemail = \"set me up!\" # please set me up!\napi_token = \"set me up!\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Normalize Phase Implementation in dlt\nDESCRIPTION: Demonstrates how to run the normalize phase separately, which processes extracted data to compute schemas and normalize data structures.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/explainers/how-dlt-works.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipeline.normalize()\n```\n\n----------------------------------------\n\nTITLE: Configuring Workable API Credentials in TOML\nDESCRIPTION: TOML configuration for storing Workable API access token securely.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/workable.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[sources.workable]\naccess_token = \"access_token\" # Your Workable token copied above\n```\n\n----------------------------------------\n\nTITLE: Configuring Athena Destination in secrets.toml\nDESCRIPTION: Basic configuration for Athena destination in the secrets.toml file, including S3 bucket settings and AWS credentials.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/athena.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\nbucket_url = \"s3://[your_bucket_name]\" # replace with your bucket name,\n\n[destination.filesystem.credentials]\naws_access_key_id = \"please set me up!\" # copy the access key here\naws_secret_access_key = \"please set me up!\" # copy the secret access key here\n\n[destination.athena]\nquery_result_bucket=\"s3://[results_bucket_name]\" # replace with your query results bucket name\n\n[destination.athena.credentials]\naws_access_key_id=\"please set me up!\" # same as credentials for filesystem\naws_secret_access_key=\"please set me up!\" # same as credentials for filesystem\nregion_name=\"please set me up!\" # set your AWS region, for example \"eu-central-1\" for Frankfurt\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Profile for Credentials\nDESCRIPTION: TOML configuration for using named AWS profile instead of explicit credentials.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/dremio.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem.credentials]\nprofile_name=\"dlt-ci-user\"\n```\n\n----------------------------------------\n\nTITLE: Initializing dlt Project\nDESCRIPTION: Shell command to initialize a new dlt project with Snowflake Plus destination\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/snowflake_plus.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndlt project init sql_database snowflake_plus\n```\n\n----------------------------------------\n\nTITLE: Databricks OAuth2 Credentials Configuration in TOML\nDESCRIPTION: TOML configuration for setting up Databricks credentials using OAuth2 authentication method\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/databricks.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[destination.databricks.credentials]\nserver_hostname = \"MY_DATABRICKS.azuredatabricks.net\"\nhttp_path = \"/sql/1.0/warehouses/12345\"\ncatalog = \"my_catalog\"\nclient_id = \"XXX\"\nclient_secret = \"XXX\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Pipeline Settings via TOML\nDESCRIPTION: Example of pipeline configuration options that can only be set via config providers like config.toml. These settings control destination restoration and dataset organization behavior.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/technical/general_usage.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# Enables the `run` method of the `Pipeline` object to restore the pipeline state and schemas from the destination\nrestore_from_destination=true\n# Stores all schemas in single dataset. When False, each schema will get a separate dataset with `{dataset_name}_{schema_name}\nuse_single_dataset=true\n```\n\n----------------------------------------\n\nTITLE: Using Replace Write Disposition with Weaviate\nDESCRIPTION: Loading data with the 'replace' write disposition to completely replace existing data in Weaviate with new data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/weaviate.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ninfo = pipeline.run(\n    weaviate_adapter(\n        movies,\n        vectorize=\"title\",\n    ),\n    write_disposition=\"replace\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Presto Destination in secrets.toml\nDESCRIPTION: Example TOML configuration for a Presto destination in DLT. This configuration would typically contain connection parameters like host, port, username, password, and other database-specific settings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/create-new-destination.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[destination.presto]\n\n```\n\n----------------------------------------\n\nTITLE: Filtering Files by Name\nDESCRIPTION: Example of filtering files based on filename patterns.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/basic.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.sources.filesystem import filesystem, read_csv\n\n# Filter files accessing file_name field\nfiltered_files = filesystem(bucket_url=\"s3://bucket_name\", file_glob=\"directory/*.csv\")\nfiltered_files.add_filter(lambda item: (\"London\" in item[\"file_name\"]) or (\"Berlin\" in item[\"file_name\"]))\n\nfilesystem_pipe = (filtered_files | read_csv())\npipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\nload_info = pipeline.run(filesystem_pipe)\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Installing dlt with LanceDB Support\nDESCRIPTION: Command to install dlt with LanceDB integration using pip. This installs dlt and lancedb, but additional SDKs may be required for specific model providers.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/lancedb.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[lancedb]\"\n```\n\n----------------------------------------\n\nTITLE: Alternative SQL Database Connection Configuration\nDESCRIPTION: Alternative way to configure SQL database connection using a connection string in the TOML file. This single-line approach defines all connection parameters in a standard URI format.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/sql-database.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\nsources.sql_database.credentials=\"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Weaviate Destination in config.toml\nDESCRIPTION: TOML configuration for the Weaviate destination, specifying the vectorizer and module configuration for text2vec-contextionary.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/dlt/destinations/impl/weaviate/README.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[destination.weaviate]\nvectorizer=\"text2vec-contextionary\"\nmodule_config={text2vec-contextionary = { vectorizeClassName = false, vectorizePropertyName = true}}\n```\n\n----------------------------------------\n\nTITLE: Incremental Loading by Timestamp in SQL with dlt in Python\nDESCRIPTION: Illustrates how to implement an incremental loading strategy that appends only records created after a specific timestamp. It uses dlt.sources.incremental(\"created_at\") with an initial_value parameter to define a starting point for data loading.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add-incremental-configuration.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef load_incremental_timestamp_table_resource() -> None:\n    \"\"\"Load a table incrementally based on created_at timestamp.\"\"\"\n    pipeline = dlt.pipeline(\n        pipeline_name=\"mysql_databasecdc\",\n        destination='bigquery',\n        dataset_name=\"dlt_contacts\",\n    )\n\n    # Load table \"contact\", incrementally starting at a given timestamp\n    source = sql_database().with_resources(\"contact\")\n    source.contact.apply_hints(incremental=dlt.sources.incremental(\n        \"created_at\", initial_value=datetime.datetime(2024, 4, 1, 0, 0, 0)))\n\n    # Run the pipeline\n    info = pipeline.run(source, write_disposition=\"append\")\n\n    # Print the info\n    print(info)\n\nload_incremental_timestamp_table_resource()\n```\n\n----------------------------------------\n\nTITLE: DuckDB Profile Configuration for dbt\nDESCRIPTION: A YAML configuration example for setting up a DuckDB profile to be used with dbt. It disables anonymous usage stats tracking and configures a DuckDB connection with necessary extensions.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/transformations/dbt/dbt.md#2025-04-14_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nconfig:\n  # Do not track usage, do not create .user.yml\n  send_anonymous_usage_stats: False\n\nduckdb_dlt_dbt_test:\n  target: analytics\n  outputs:\n    analytics:\n      type: duckdb\n      # Schema: \"{{ var('destination_dataset_name', var('source_dataset_name')) }}\"\n      path: \"duckdb_dlt_dbt_test.duckdb\"\n      extensions:\n        - httpfs\n        - parquet\n```\n\n----------------------------------------\n\nTITLE: Specifying Google Sheets Range Names\nDESCRIPTION: Example of how to specify range names for Google Sheets, including named ranges, sheet names, and explicit cell ranges.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nrange_names = [\"Range_1\",\"Range_2\",\"Sheet1!A1:D10\"]\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline Without Dataset Name\nDESCRIPTION: Defining a dlt pipeline without specifying a dataset name to avoid prefixing class names in Weaviate.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/weaviate.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"movies\",\n    destination=\"weaviate\",\n)\n```\n\n----------------------------------------\n\nTITLE: Schema Directory Structure\nDESCRIPTION: Shows the folder structure created for schema management.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/adjust-a-schema.md#2025-04-14_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nschemas\n    |---import/\n    |---export/\n```\n\n----------------------------------------\n\nTITLE: Configuring Postgres Connection String in dlt\nDESCRIPTION: Alternative TOML configuration using a connection string for Postgres credentials in dlt.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/postgres.md#2025-04-14_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\ndestination.postgres.credentials=\"postgresql://loader:<password>@localhost/dlt_data?connect_timeout=15\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Account Credentials for Google Sheets in TOML\nDESCRIPTION: This snippet shows the format for storing Google Sheets service account credentials in the secrets.toml file. It includes placeholders for project_id, client_email, and private_key.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[sources.google_sheets.credentials] ##CHECK IT\nproject_id = \"project_id\" # please set me up!\nclient_email = \"client_email\" # please set me up!\nprivate_key = \"private_key\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Basic Pipeline Configuration\nDESCRIPTION: Example of configuring a DLT pipeline with custom name, destination, and dataset.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/kafka.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n     pipeline_name=\"kafka\",     # Use a custom name if desired\n     destination=\"duckdb\",      # Choose the appropriate destination (e.g., duckdb, redshift, post)\n     dataset_name=\"kafka_data\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring ClickHouse Destination with Google Cloud Storage HMAC Credentials\nDESCRIPTION: This TOML configuration snippet shows how to set up the ClickHouse destination with Google Cloud Storage (GCS) as the staging area. It includes settings for the bucket URL, HMAC credentials, and the GCS endpoint URL.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/clickhouse.md#2025-04-14_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\nbucket_url = \"s3://my_awesome_bucket\"\n\n[destination.filesystem.credentials]\naws_access_key_id = \"JFJ$$*f2058024835jFffsadf\"\naws_secret_access_key = \"DFJdwslf2hf57)%$02jaflsedjfasoi\"\nproject_id = \"my-awesome-project\"\nendpoint_url = \"https://storage.googleapis.com\"\n```\n\n----------------------------------------\n\nTITLE: Pokemon Pipeline Configuration Dictionary\nDESCRIPTION: Python configuration dictionary that defines the REST API endpoints and settings for the Pokemon pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/openapi-generator.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"client\": {\n        \"base_url\": base_url,\n        \"paginator\": {\n            ...\n        },\n    },\n    \"resources\": [\n        {\n            \"name\": \"pokemon_list\",\n            \"table_name\": \"pokemon\",\n            \"endpoint\": {\n                \"data_selector\": \"results\",\n                \"path\": \"/api/v2/pokemon/\",\n            },\n        },\n        {\n            \"name\": \"pokemon_read\",\n            \"table_name\": \"pokemon\",\n            \"primary_key\": \"name\",\n            \"write_disposition\": \"merge\",\n            \"endpoint\": {\n                \"data_selector\": \"$\",\n                \"path\": \"/api/v2/pokemon/{name}/\",\n                \"params\": {\n                    \"name\": {\n                        \"type\": \"resolve\",\n                        \"resource\": \"pokemon_list\",\n                        \"field\": \"name\",\n                    },\n                },\n            },\n        },\n    ],\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Project Directory (Shell)\nDESCRIPTION: Creates a new directory for the DLT project and navigates into it.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/create-a-pipeline.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nmkdir github_api_duckdb && cd github_api_duckdb\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Amazon Kinesis Pipeline\nDESCRIPTION: Command to install the required dependencies for running the Amazon Kinesis pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/amazon_kinesis.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Updating a Verified Source\nDESCRIPTION: Updates an existing verified source to the newest version.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add-a-verified-source.md#2025-04-14_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ndlt init pipedrive bigquery\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS S3 Credentials for Delta in secrets.toml\nDESCRIPTION: TOML configuration for setting up AWS S3 credentials for the Delta destination in the secrets.toml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/delta.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[destination.delta.credentials]\naws_access_key_id=\"Please set me up!\"\naws_secret_access_key=\"Please set me up!\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Vectorized Scanner for Parquet in Snowflake\nDESCRIPTION: Configuration to enable the vectorized scanner feature for improved performance when loading Parquet files into Snowflake.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_12\n\nLANGUAGE: toml\nCODE:\n```\n[destination.snowflake]\nuse_vectorized_scanner=true\n```\n\n----------------------------------------\n\nTITLE: Loading All Personio Data\nDESCRIPTION: Executes a pipeline run to load data from all supported Personio endpoints using the default source configuration.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/personio.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nload_data = personio_source()\nprint(pipeline.run(load_data))\n```\n\n----------------------------------------\n\nTITLE: Configuring Synapse Connection String in TOML\nDESCRIPTION: Demonstrates how to specify Synapse connection parameters including port and timeout directly in the connection string format.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/synapse.md#2025-04-14_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\ndestination.synapse.credentials = \"synapse://loader:your_loader_password@your_synapse_workspace_name.azuresynapse.net:1433/yourpool?connect_timeout=15\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Development Profile in TOML\nDESCRIPTION: This TOML configuration sets up the 'dev' profile with specific settings for runtime, destination, and data source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_15\n\nLANGUAGE: toml\nCODE:\n```\n[runtime]\nlog_level = \"WARNING\"\n\n[destination.my_duckdb_destination]\ncredentials = \"my_data.duckdb\"\n\n[sources.my_arrow_source]\nrow_count = 100\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Writer Settings in TOML\nDESCRIPTION: Example of how to configure parquet writer settings in a TOML configuration file, showing default values for flavor, version, data page size, and timestamp timezone.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/file-formats/parquet.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[normalize.data_writer]\n# the default values\nflavor=\"spark\"\nversion=\"2.4\"\ndata_page_size=1048576\ntimestamp_timezone=\"Europe/Berlin\"\n```\n\n----------------------------------------\n\nTITLE: Setting CSV Format Configuration in Python\nDESCRIPTION: This Python code shows how to explicitly set CSV format configuration for Snowflake, including delimiter, header inclusion, and error handling.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.destinations import snowflake\nfrom dlt.common.data_writers.configuration import CsvFormatConfiguration\n\ncsv_format = CsvFormatConfiguration(delimiter=\"|\", include_header=False, on_error_continue=True)\n\ndest_ = snowflake(csv_format=csv_format)\n```\n\n----------------------------------------\n\nTITLE: Partitioned Column Configuration\nDESCRIPTION: YAML schema configuration showing how to add partitioning hints for better query performance.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/adjust-a-schema.md#2025-04-14_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nplayers_games:\n  columns:\n    end_time:\n      nullable: false\n      data_type: timestamp\n      partition: true\n    white:\n      nullable: false\n      data_type: json\n    black:\n      nullable: false\n      data_type: json\n```\n\n----------------------------------------\n\nTITLE: Running Kafka Pipeline\nDESCRIPTION: Command to execute the Kafka pipeline script.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/kafka.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython kafka_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Runtime Settings in dlt.yml\nDESCRIPTION: Demonstrates how to set runtime configurations, such as log level, in the dlt.yml manifest file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/projects.md#2025-04-14_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nruntime:\n  log_level: WARNING\n```\n\n----------------------------------------\n\nTITLE: Source Renaming Example\nDESCRIPTION: Demonstrates how to rename a source and configure custom sections for credentials.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/source.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.sql_database import sql_database\n\nmy_db = sql_database.clone(name=\"my_db\", section=\"my_db\")(table_names=[\"table_1\"])\nprint(my_db.name)\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Settings using Environment Variables\nDESCRIPTION: Demonstrates how to set CSV writer options using environment variables. This approach is an alternative to TOML configuration, allowing the same customizations for delimiter, header inclusion, and quoting behavior.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/file-formats/csv.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nNORMALIZE__DATA_WRITER__DELIMITER=|\nNORMALIZE__DATA_WRITER__INCLUDE_HEADER=False\nNORMALIZE__DATA_WRITER__QUOTING=quote_all\n```\n\n----------------------------------------\n\nTITLE: Configuring Slack OAuth Token in secrets.toml\nDESCRIPTION: TOML configuration for storing the Slack OAuth token securely in the secrets.toml file. This token is used for authenticating with the Slack API.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/slack.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[sources.slack]\naccess_token = \"Please set me up!\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Running DLT Pipeline with Production Profile\nDESCRIPTION: This command runs the 'my_pipeline' pipeline using the 'prod' profile settings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline --profile prod my_pipeline run\n```\n\n----------------------------------------\n\nTITLE: Declaring Destination Using Shorthand Type in Python\nDESCRIPTION: Shows how to declare a filesystem destination using the shorthand type syntax when creating a pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(pipeline_name=\"pipeline_name\", destination=\"filesystem\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Personio API Credentials in TOML\nDESCRIPTION: TOML configuration for storing Personio API credentials securely in the secrets.toml file. This includes the client_id and client_secret required for API authentication.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/personio.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[sources.personio]\nclient_id = \"papi-*****\" # please set me up!\nclient_secret = \"papi-*****\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Initializing dlt Pipeline for Notion with BigQuery Target\nDESCRIPTION: Command to initialize a dlt pipeline using the Notion verified source and BigQuery as the target. This creates necessary files and folders in the current directory.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-google-cloud-functions.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init notion bigquery\n```\n\n----------------------------------------\n\nTITLE: Configuring Weaviate Destination with Local Instance\nDESCRIPTION: Configuration for a local Weaviate instance in the dlt secrets file, with OpenAI API for embeddings but default connection settings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/weaviate.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[destination.weaviate.credentials.additional_headers]\nX-OpenAI-Api-Key = \"your-openai-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Initializing DLT Pipeline Configuration for Workable\nDESCRIPTION: Sets up the basic pipeline configuration including pipeline name, destination database, and dataset name. These parameters define how and where the data will be stored.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/workable.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n        pipeline_name=\"workable\",  # Use a custom name if desired\n        destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n        dataset_name=\"workable_data\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment (Unix)\nDESCRIPTION: Commands to create and activate a Python virtual environment on Unix-based systems.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/installation.md#2025-04-14_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nuv venv --python 3.10\nsource .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Initializing Amazon Kinesis Pipeline with DuckDB Destination\nDESCRIPTION: Command to initialize a dlt pipeline using Amazon Kinesis as the source and DuckDB as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/amazon_kinesis.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init kinesis duckdb\n```\n\n----------------------------------------\n\nTITLE: Disabling Staged File Retention in Snowflake\nDESCRIPTION: Configuration to disable keeping staged files in Snowflake after data is loaded, which can help manage storage usage.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_10\n\nLANGUAGE: toml\nCODE:\n```\n[destination.snowflake]\nkeep_staged_files = false\n```\n\n----------------------------------------\n\nTITLE: Disabling File Compression Configuration\nDESCRIPTION: Configuration to disable compression in the data writer for plain text file output.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_26\n\nLANGUAGE: toml\nCODE:\n```\n[normalize.data_writer]\ndisable_compression=true\n```\n\n----------------------------------------\n\nTITLE: Explicitly setting the ODBC driver name for MS SQL\nDESCRIPTION: Configuration to explicitly specify which ODBC driver version to use for connecting to MS SQL Server.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/mssql.md#2025-04-14_snippet_12\n\nLANGUAGE: toml\nCODE:\n```\n[destination.mssql.credentials]\ndriver=\"ODBC Driver 18 for SQL Server\"\n```\n\n----------------------------------------\n\nTITLE: Extracting Redshift Host from Cluster Endpoint\nDESCRIPTION: Example showing how to extract the host parameter from a Redshift cluster endpoint string.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/redshift.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n# If the endpoint is:\nredshift-cluster-1.cv3cmsy7t4il.us-east-1.redshift.amazonaws.com:5439/your_database_name\n# Then the host is:\nredshift-cluster-1.cv3cmsy7t4il.us-east-1.redshift.amazonaws.com\n```\n\n----------------------------------------\n\nTITLE: Configuring Partitioning in YAML\nDESCRIPTION: Set up partitioning for a source or resource in the dlt.yml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/iceberg.md#2025-04-14_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  my_source:\n    type: sources.my_source\n    with_args:\n      columns:\n        foo:\n          partition: True\n```\n\n----------------------------------------\n\nTITLE: Running dlt Pipeline with Profile\nDESCRIPTION: Shows how to run a dlt pipeline using a specific profile, such as 'prod', allowing for execution in different environments. This is done using the 'dlt project' command with the '--profile' flag.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/production/runners.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt project --profile prod my_pipeline run\n```\n\n----------------------------------------\n\nTITLE: Defining Pipeline Without Dataset Prefix\nDESCRIPTION: Example of creating a pipeline without specifying a dataset name to avoid the dataset prefix in Qdrant collection names.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/qdrant.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"movies\",\n    destination=\"qdrant\",\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying dlt+ Project Structure\nDESCRIPTION: Shows the general file and folder structure of a dlt+ Project, including configuration files, source code directories, and the main dlt.yml manifest file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/projects.md#2025-04-14_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nâ”œâ”€â”€ .dlt/                 # folder containing dlt configurations and profile settings\nâ”‚   â”œâ”€â”€ config.toml\nâ”‚   â”œâ”€â”€ dev.secrets.toml  # credentials for access profile 'dev'\nâ”‚   â””â”€â”€ secrets.toml\nâ”œâ”€â”€ _data/                # local storage for your project, excluded from git\nâ”œâ”€â”€ sources/              # modules containing the source code for sources\nâ”‚   â””â”€â”€ github.py         # source code for a github source\nâ”œâ”€â”€ transformations/      # modules containing the source code for transformations\nâ”œâ”€â”€ .gitignore\nâ””â”€â”€ dlt.yml               # the main project manifest\n```\n\n----------------------------------------\n\nTITLE: Historical Data Loading with Weekly Ranges\nDESCRIPTION: Implementation of historical data loading using weekly date ranges with transition to incremental loading\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/zendesk.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Load ranges of dates between January 1st, 2023, and today\nmin_start_date = pendulum.DateTime(year=2023, month=1, day=1).in_timezone(\"UTC\")\nmax_end_date = pendulum.today()\n# Generate tuples of date ranges, each with 1 week in between.\nranges = make_date_ranges(min_start_date, max_end_date, datetime.timedelta(weeks=1))\n\n# Run the pipeline in a loop for each 1-week range\nfor start, end in ranges:\n    print(f\"Loading tickets between {start} and {end}\")\n    data = zendesk_support(start_date=start, end_date=end).with_resources(\"tickets\")\n    info = pipeline.run(data=data)\n    print(info)\n\n# Backloading is done, now we continue loading with incremental state, starting where the backloading left off\nprint(f\"Loading with incremental state, starting at {end}\")\ndata = zendesk_support(start_date=end).with_resources(\"tickets\")\ninfo = pipeline.run(data)\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Configure Continue MCP Server with JSON\nDESCRIPTION: JSON configuration for the dlt MCP server in the Continue experimental settings. This defines the transport type and command arguments for launching the server.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/llm-tooling/mcp-server.md#2025-04-14_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"experimental\": {\n    \"modelContextProtocolServers\": [\n      {\n        \"transport\": {\n          \"type\": \"stdio\",\n          \"command\": \"uv\",\n          \"args\": [\n            \"tool\",\n            \"run\",\n            \"--with\",\n            \"dlt-plus[mcp]==0.9.0\",\n            \"dlt\",\n            \"mcp\",\n            \"run\"\n          ]\n        }\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running the BigQuery Pipeline from Command Line\nDESCRIPTION: Shell command to run the Python script containing the dlt pipeline configured for BigQuery. This simple command executes the pipeline after all the necessary configurations have been set up.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/share-a-dataset.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython chess_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Creating ClickHouse Database and User\nDESCRIPTION: SQL commands to create a new ClickHouse database, user, and grant necessary permissions\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/clickhouse.md#2025-04-14_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE DATABASE IF NOT EXISTS dlt;\nCREATE USER dlt IDENTIFIED WITH sha256_password BY 'Dlt*12345789234567';\nGRANT CREATE, ALTER, SELECT, DELETE, DROP, TRUNCATE, OPTIMIZE, SHOW, INSERT, dictGet ON dlt.* TO dlt;\nGRANT SELECT ON INFORMATION_SCHEMA.COLUMNS TO dlt;\nGRANT CREATE TEMPORARY TABLE, S3 ON *.* TO dlt;\n```\n\n----------------------------------------\n\nTITLE: Running Postgres Docker Container for Testing dlt\nDESCRIPTION: Commands for setting up a Postgres instance using Docker Compose for testing dlt. The provided docker-compose file is pre-configured to work with the dlt tests.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/CONTRIBUTING.md#2025-04-14_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd tests/load/postgres/\ndocker-compose up --build -d\n```\n\n----------------------------------------\n\nTITLE: OAuth Credentials Configuration\nDESCRIPTION: TOML configuration for Google Ads OAuth authentication setup.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_ads.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[sources.google_ads.credentials]\nclient_id = \"client_id\" # please set me up!\nclient_secret = \"client_secret\" # please set me up!\nrefresh_token = \"refresh_token\" # please set me up!\nproject_id = \"project_id\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Write Disposition in YAML\nDESCRIPTION: Set up merge write disposition for a source or resource in the dlt.yml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/iceberg.md#2025-04-14_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  my_source:\n    type: sources.my_source\n    with_args:\n      write_disposition:\n        disposition: merge\n        strategy: delete-insert\n```\n\n----------------------------------------\n\nTITLE: Implementing Zendesk Talk Source\nDESCRIPTION: Python function definition for retrieving Zendesk Talk data with configurable date ranges and authentication\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/zendesk.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source(max_table_nesting=2)\ndef zendesk_talk(\n    credentials: TZendeskCredentials = dlt.secrets.value,\n    start_date: Optional[TAnyDateTime] = START_DATE,\n    end_date: Optional[TAnyDateTime] = None,\n) -> Iterable[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Example Databricks Credentials Configuration\nDESCRIPTION: Example TOML configuration for Databricks credentials setup\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/databricks.md#2025-04-14_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[destination.databricks.credentials]\nserver_hostname = \"MY_DATABRICKS.azuredatabricks.net\"\nhttp_path = \"/sql/1.0/warehouses/12345\"\nclient_id = \"XXX\"\nclient_secret = \"XXX\"\ncatalog = \"my_catalog\"\n```\n\n----------------------------------------\n\nTITLE: Installing Pipeline Dependencies\nDESCRIPTION: Command to install required dependencies for the Workable pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/workable.md#2025-04-14_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing DLT Kafka Pipeline\nDESCRIPTION: Command to initialize a new DLT pipeline with Kafka as the source and DuckDB as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/kafka.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init kafka duckdb\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Compose for Kestra with Environment Variables\nDESCRIPTION: YAML configuration snippet for Docker Compose that sets up the Kestra service to use environment variables from a .env file, which is necessary for storing credentials and configuration settings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-kestra.md#2025-04-14_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nkestra:\n    image: kestra/kestra:develop-full\n    env_file:\n        - .env\n```\n\n----------------------------------------\n\nTITLE: Launching DLT Data Browser\nDESCRIPTION: Command to start the DLT browser application for visualizing and interacting with the data loaded by the REST API pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/rest-api.md#2025-04-14_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline rest_api_pokemon show\n```\n\n----------------------------------------\n\nTITLE: Setting Directory Creation with TOML\nDESCRIPTION: Configuration for enabling automatic directory creation in local filesystem destination using kwargs.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_18\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\nkwargs = '{\"auto_mkdir\": true}'\n```\n\n----------------------------------------\n\nTITLE: Initializing a dlt Project with Snowflake\nDESCRIPTION: Command to initialize a new dlt project that loads data to Snowflake using the chess example.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init chess snowflake\n```\n\n----------------------------------------\n\nTITLE: Verifying License Installation\nDESCRIPTION: Command to verify that the dlt+ license is properly installed and valid.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/installation.md#2025-04-14_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\ndlt license show\n```\n\n----------------------------------------\n\nTITLE: Initializing Freshdesk dlt pipeline from CLI\nDESCRIPTION: Command to initialize a new dlt pipeline with Freshdesk as the source and DuckDB as the destination. This creates the necessary directory structure and configuration files.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/freshdesk.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init freshdesk duckdb\n```\n\n----------------------------------------\n\nTITLE: Extract Phase Implementation in dlt\nDESCRIPTION: Shows how to execute the extract phase independently, which pulls data from sources to local storage in a load package.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/explainers/how-dlt-works.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipeline.extract(data)\n```\n\n----------------------------------------\n\nTITLE: Initializing a dlt Project with SQLAlchemy\nDESCRIPTION: Commands to set up a new dlt project using SQLAlchemy as the destination, specifically for a chess-related data pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/sqlalchemy.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init chess sqlalchemy\n```\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[sqlalchemy]\"\n```\n\nLANGUAGE: sh\nCODE:\n```\npip install mysqlclient\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom CSV Format for Snowflake\nDESCRIPTION: Configuration for customizing CSV format settings when importing external CSV files into Snowflake, including delimiter, header options, and error handling.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_13\n\nLANGUAGE: toml\nCODE:\n```\n[destination.snowflake.csv_format]\ndelimiter=\"|\"\ninclude_header=false\non_error_continue=true\n```\n\n----------------------------------------\n\nTITLE: Loading Specific Asana Resources\nDESCRIPTION: Executes the pipeline to load only workspace and project data from Asana, demonstrating selective resource loading.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/asana.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nload_info = pipeline.run(load_data.with_resources(\"workspaces\", \"projects\"))\n# print the information on data that was loaded\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Chess.com API Pipeline\nDESCRIPTION: Command to install all required dependencies for the Chess.com API pipeline from the requirements.txt file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/chess.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Initializing dlt Project with MotherDuck\nDESCRIPTION: Command to initialize a dlt project with a pipeline that loads to MotherDuck.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/motherduck.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndlt init chess motherduck\n```\n\n----------------------------------------\n\nTITLE: Database Credentials Configuration in Dictionary Form\nDESCRIPTION: TOML configuration example showing database credentials in dictionary format.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[dsn]\ndatabase=\"dlt_data\"\npassword=\"loader\"\nusername=\"loader\"\nhost=\"localhost\"\n```\n\n----------------------------------------\n\nTITLE: Claude Desktop MCP Server Configuration\nDESCRIPTION: JSON configuration for setting up the dlt MCP server in Claude Desktop. This defines the server name, command, and arguments needed to integrate with the Claude Desktop application.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/llm-tooling/mcp-server.md#2025-04-14_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"mcpServers\": {\n    \"dlt\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"tool\",\n        \"run\",\n        \"--with\",\n        \"dlt-plus[mcp]==0.9.0\",\n        \"dlt\",\n        \"mcp\",\n        \"run\"\n      ]\n    } \n  }\n}\n```\n\n----------------------------------------\n\nTITLE: S3 Compatible Storage Configuration in secrets.toml\nDESCRIPTION: Configuration for using S3 compatible storage services like MinIO, Cloudflare R2, or Google Cloud Storage by specifying an endpoint URL.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\nbucket_url = \"s3://[your_bucket_name]\" # replace with your bucket name,\n\n[destination.filesystem.credentials]\naws_access_key_id = \"please set me up!\" # copy the access key here\naws_secret_access_key = \"please set me up!\" # copy the secret access key here\nendpoint_url = \"https://<account_id>.r2.cloudflarestorage.com\" # copy your endpoint URL here\n```\n\n----------------------------------------\n\nTITLE: Required Arguments in Slack Source\nDESCRIPTION: Example showing required arguments that cannot be injected and must be explicitly specified when calling the source.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/advanced.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef slack_data(channels_list: List[str], api_key: str = dlt.secrets.value):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Changing Default AWS Data Catalog\nDESCRIPTION: Configuration to specify a different AWS data catalog name for Athena operations.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/athena.md#2025-04-14_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[destination.athena]\naws_data_catalog=\"awsdatacatalog\"\n```\n\n----------------------------------------\n\nTITLE: YAML Schema Data Type Example\nDESCRIPTION: Example of column definition in YAML schema showing timestamp data type configuration.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/adjust-a-schema.md#2025-04-14_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nplayers_games:\n  columns:\n    end_time:\n      nullable: true\n      data_type: timestamp\n```\n\n----------------------------------------\n\nTITLE: Installing Optional Dependencies for Efficient Data Loading\nDESCRIPTION: This command installs additional dependencies (pyarrow, numpy, pandas) for more efficient data loading using pyarrow.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/setup.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npip install pyarrow numpy pandas\n```\n\n----------------------------------------\n\nTITLE: Configuring Dev Mode for DLT Pipeline in Python\nDESCRIPTION: Dev mode in DLT pipeline allows for non-destructive full refresh. It wipes out the working directory, prefixes the dataset name with a timestamp, and prevents pipeline restoration from the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/technical/general_usage.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndlt.pipeline(dev_mode=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring Staging Dataset Name Pattern in TOML\nDESCRIPTION: Sets the naming pattern for staging datasets with a prefix format. Used to customize how staging dataset names are generated from the main dataset name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/staging.md#2025-04-14_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[destination.postgres]\nstaging_dataset_name_layout=\"staging_%s\"\n```\n\n----------------------------------------\n\nTITLE: Running Jira Pipeline\nDESCRIPTION: Shell command to execute the Jira data pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/jira.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython jira_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Command to install project dependencies in editable mode for development.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-dagster.md#2025-04-14_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Integration for dlt Telemetry\nDESCRIPTION: Configuration snippet to send telemetry data to a Segment account by specifying the endpoint and write key in config.toml.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/telemetry.md#2025-04-14_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\n[runtime]\ndlthub_telemetry_endpoint=\"https://api.segment.io/v1/track\"\ndlthub_telemetry_segment_write_key=\"<write_key>\"\n```\n\n----------------------------------------\n\nTITLE: Installing DLT with DuckDB Support\nDESCRIPTION: Command to install the DLT package with DuckDB integration support\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/dispatch-to-multiple-tables.md#2025-04-14_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install \"dlt[duckdb]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring License in YAML\nDESCRIPTION: YAML configuration for setting the dlt+ license key in project manifest file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/installation.md#2025-04-14_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nruntime:\n  license: { env.MY_ENV_CONTAINING_LICENSE_KEY }\n```\n\n----------------------------------------\n\nTITLE: Initializing dlt Project with Iceberg Destination\nDESCRIPTION: Initialize a dlt project with an Iceberg destination using the command line interface.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/iceberg.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n# replace sql_database with the source of your choice\ndlt project init sql_database iceberg\n```\n\n----------------------------------------\n\nTITLE: Google Analytics Credentials Configuration in TOML\nDESCRIPTION: TOML configuration example for Google Analytics credentials.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[sources.google_analytics.credentials]\nclient_id = \"client_id\" # please set me up!\nclient_secret = \"client_secret\" # please set me up!\nrefresh_token = \"refresh_token\" # please set me up!\nproject_id = \"project_id\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Alternative Installation of dlt with Independent Dependencies\nDESCRIPTION: Alternative installation approach that installs dlt and s3fs separately to avoid pip backtracking issues.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npip install dlt\npip install s3fs\n```\n\n----------------------------------------\n\nTITLE: Initializing a dlt Project for SQL Database Pipeline\nDESCRIPTION: Command to create a new dlt project structure for extracting data from a SQL database and loading it into DuckDB. This generates the necessary configuration files and pipeline script.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/sql-database.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init sql_database duckdb\n```\n\n----------------------------------------\n\nTITLE: Initializing DuckDB Pipeline Project\nDESCRIPTION: Commands to set up a new DLT project with DuckDB as the destination\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/duckdb.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init chess duckdb\n```\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\nLANGUAGE: sh\nCODE:\n```\npython3 chess_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Azure Configuration Files\nDESCRIPTION: TOML configuration files for Azure storage credentials and bucket URL settings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_18\n\nLANGUAGE: toml\nCODE:\n```\n[sources.azure_readers.credentials]\nazure_storage_account_name = \"account_name\"\nazure_storage_account_key = \"account_key\"\n```\n\nLANGUAGE: toml\nCODE:\n```\n[sources.azure_readers]\nbucket_url = \"bucket_url\"\n```\n\n----------------------------------------\n\nTITLE: Default Destination Configuration in TOML\nDESCRIPTION: Shows the default TOML configuration layout for a filesystem destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\nbucket_url = \"az://dlt-azure-bucket\"\n[destination.filesystem.credentials]\nazure_storage_account_name = \"dltdata\"\nazure_storage_account_key = \"storage key\"\n```\n\n----------------------------------------\n\nTITLE: Specifying ODBC driver in SQLAlchemy connection string\nDESCRIPTION: Example of specifying the ODBC driver in a SQLAlchemy connection string for MS SQL Server, using plus signs to replace spaces.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/mssql.md#2025-04-14_snippet_13\n\nLANGUAGE: toml\nCODE:\n```\ndestination.mssql.credentials=\"mssql://loader:<password>@loader.database.windows.net/dlt_data?driver=ODBC+Driver+18+for+SQL+Server\"\n```\n\n----------------------------------------\n\nTITLE: Alternative ClickHouse Connection String\nDESCRIPTION: Alternative format for specifying ClickHouse credentials using a connection string\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/clickhouse.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\ndestination.clickhouse.credentials=\"clickhouse://dlt:Dlt*12345789234567@localhost:9000/dlt?secure=1\"\n```\n\n----------------------------------------\n\nTITLE: Pipeline Execution\nDESCRIPTION: Command to run the Google Ads pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_ads.md#2025-04-14_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npython google_ads_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: DLT MongoDB Pipeline Configuration\nDESCRIPTION: TOML configuration for MongoDB pipeline settings including database and collection names.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mongodb.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[your_pipeline_name]  # Set your pipeline name here!\ndatabase = \"defaultDB\"  # Database name (Optional), default database is loaded if not provided.\ncollection_names = [\"collection_1\", \"collection_2\"] # Collection names (Optional), all collections are loaded if not provided.\n```\n\n----------------------------------------\n\nTITLE: Setting Up Postgres User on RDS (SQL)\nDESCRIPTION: SQL command to grant replication privileges to a user on Amazon RDS Postgres instance. This is used instead of the WITH LOGIN REPLICATION clause which doesn't work on RDS.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/pg_replication.md#2025-04-14_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nGRANT rds_replication TO replication_user;\n```\n\n----------------------------------------\n\nTITLE: Azure Blob Storage with Custom Host Configuration\nDESCRIPTION: Configuration for Azure Blob Storage with a custom host for the storage account.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_13\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem.credentials]\n# The storage account name is always required\nazure_account_host = \"<storage_account_name>.<host_base>\"\n```\n\n----------------------------------------\n\nTITLE: Pinning DLT Profile\nDESCRIPTION: Command to pin a specific profile as the default for all subsequent operations.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/core-concepts/profiles.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ndlt profile prod pin\n```\n\n----------------------------------------\n\nTITLE: Setting Directory Creation with Environment Variable\nDESCRIPTION: Shell command to enable automatic directory creation using environment variable.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_19\n\nLANGUAGE: sh\nCODE:\n```\nexport DESTINATION__FILESYSTEM__KWARGS = '{\"auto_mkdir\": true/false}'\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Embedded ELT Package\nDESCRIPTION: Command to install Dagster and the embedded ELT package using pip.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-dagster.md#2025-04-14_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster dagster-embedded-elt\n```\n\n----------------------------------------\n\nTITLE: Importing Documentation Components in JSX\nDESCRIPTION: Imports Admonition component from theme and a link component from a markdown file for use in documentation pages.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/_source-info-header.md#2025-04-14_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\nimport Admonition from \"@theme/Admonition\";\nimport Link from '../../_book-onboarding-call.md';\n```\n\n----------------------------------------\n\nTITLE: Configuring dlt MCP in Continue YAML Format\nDESCRIPTION: YAML configuration for setting up the dlt MCP server as a Continue assistant. This defines the server name, command, and arguments needed to properly launch the MCP server.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/llm-tooling/mcp-server.md#2025-04-14_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# local_dlt.yaml\nname: dlt MCP  # can change\nversion: 0.0.1  # can change\nschema: v1\nmcpServers:\n  - name: dlt  # can change\n    command: uv\n    args:\n      - tool\n      - run\n      - --with\n      - dlt-plus==0.9.0\n      - --with\n      - sqlglot\n      - --with\n      - pyarrow\n      - --with\n      - pandas\n      - --with\n      - duckdb\n      - --with\n      - mcp\n      - dlt\n      - mcp\n      - run\n```\n\n----------------------------------------\n\nTITLE: Disabling dlt Telemetry While Initializing a Pipeline\nDESCRIPTION: Command to permanently disable telemetry and then initialize the chess pipeline with duckdb destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/telemetry.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt --disable-telemetry init chess duckdb\n```\n\n----------------------------------------\n\nTITLE: Setting License Environment Variable\nDESCRIPTION: Command to set the dlt+ license key as an environment variable.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/installation.md#2025-04-14_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nexport RUNTIME__LICENSE=\"eyJhbGciOiJSUz...vKSjbEc===\"\n```\n\n----------------------------------------\n\nTITLE: Installing Snowflake Plus Package\nDESCRIPTION: Command to install the dlt-plus package with Snowflake support using pip\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/snowflake_plus.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt-plus[snowflake]\"\n```\n\n----------------------------------------\n\nTITLE: SQL Database Credentials Configuration\nDESCRIPTION: Examples of different ways to configure SQL database credentials in TOML format\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[sources.sql_database]\ncredentials=\"snowflake://user:password@service-account/database?warehouse=warehouse_name&role=role\"\n```\n\nLANGUAGE: toml\nCODE:\n```\n[sources.sql_database.credentials]\ndrivername=\"snowflake\"\nusername=\"user\"\npassword=\"password\"\ndatabase=\"database\"\nhost=\"service-account\"\nwarehouse=\"warehouse_name\"\nrole=\"role\"\n```\n\n----------------------------------------\n\nTITLE: Using dlt Init with a Specific Branch\nDESCRIPTION: Initializes a dlt project using a specific branch from the verified-sources repository.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add-a-verified-source.md#2025-04-14_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\ndlt init source destination --branch <branch_name>\n```\n\n----------------------------------------\n\nTITLE: Enabling Change Tracking on MS SQL Database\nDESCRIPTION: SQL command to enable Change Tracking on a database with a 7-day retention period and auto-cleanup.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/ms-sql.md#2025-04-14_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nALTER DATABASE [YourDatabaseName]\nSET CHANGE_TRACKING = ON\n(CHANGE_RETENTION = 7 DAYS, AUTO_CLEANUP = ON);\n```\n\n----------------------------------------\n\nTITLE: Initialize DLT Pipeline\nDESCRIPTION: Command to initialize a new DLT pipeline with Airtable as source and DuckDB as destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/airtable.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init airtable duckdb\n```\n\n----------------------------------------\n\nTITLE: Installing dlt with Microsoft SQL Server dependencies\nDESCRIPTION: Command to install the dlt library with the required dependencies for connecting to Microsoft SQL Server.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/mssql.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[mssql]\"\n```\n\n----------------------------------------\n\nTITLE: Installing dlt with DuckDB support\nDESCRIPTION: Command to install dlt with additional DuckDB support using uv.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/installation.md#2025-04-14_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nuv pip install \"dlt[duckdb]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring File-Based Qdrant Storage in dlt\nDESCRIPTION: TOML configuration for using Qdrant with local file-based storage instead of a server connection.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/qdrant.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[destination.qdrant]\nqd_path = \"db.qdrant\"\n```\n\n----------------------------------------\n\nTITLE: Windows and POSIX Path Configurations\nDESCRIPTION: Examples of configuring various path types including Windows UNC, POSIX absolute, and relative paths.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_20\n\nLANGUAGE: toml\nCODE:\n```\n[destination.unc_destination]\nbucket_url = 'C:\\a\\b\\c'\n\n[destination.unc_destination]\nbucket_url = '\\\\localhost\\c$\\a\\b\\c'  # UNC equivalent of C:\\a\\b\\c\n\n[destination.posix_destination]\nbucket_url = '/var/local/data'  # absolute POSIX style path\n\n[destination.relative_destination]\nbucket_url = '_storage/data'  # relative POSIX style path\n```\n\n----------------------------------------\n\nTITLE: Configuring Dremio Credentials with Full Settings\nDESCRIPTION: TOML configuration for setting up Dremio and filesystem credentials including AWS access keys and Dremio connection details.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/dremio.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\nbucket_url = \"s3://[your_bucket_name]\" # replace with your bucket name,\n\n[destination.filesystem.credentials]\naws_access_key_id = \"please set me up!\" # copy the access key here\naws_secret_access_key = \"please set me up!\" # copy the secret access key here\n\n[destination.dremio]\nstaging_data_source = \"<staging-data-source>\" # the name of the \"Object Storage\" data source in Dremio containing the s3 bucket\n\n[destination.dremio.credentials]\nusername = \"<username>\"  # the Dremio username\npassword = \"<password or pat token>\"  # Dremio password or PAT token\ndatabase = \"<database>\" # the name of the \"data source\" set up in Dremio where you want to load your data\nhost = \"localhost\" # the Dremio hostname\nport = 32010 # the Dremio Arrow Flight grpc port\ndrivername=\"grpc\" # either 'grpc' or 'grpc+tls'\n```\n\n----------------------------------------\n\nTITLE: Connecting to MS SQL using Windows authentication\nDESCRIPTION: Connection string configuration for connecting to MS SQL Server using Windows authentication.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/mssql.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\ndestination.mssql.credentials=\"mssql://loader.database.windows.net/dlt_data?trusted_connection=yes\"\n```\n\n----------------------------------------\n\nTITLE: Creating Catalog Integration in Snowflake\nDESCRIPTION: SQL command to create a catalog integration for Snowflake Open Catalog synchronization\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/snowflake_plus.md#2025-04-14_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nCREATE OR REPLACE CATALOG INTEGRATION my_open_catalog_int\n    CATALOG_SOURCE = POLARIS\n    TABLE_FORMAT = ICEBERG\n    REST_CONFIG = (\n      CATALOG_URI = 'https://<orgname>-<my-snowflake-open-catalog-account-name>.snowflakecomputing.com/polaris/api/catalog'\n      CATALOG_NAME = 'myOpenCatalogExternalCatalogName'\n    )\n    REST_AUTHENTICATION = (\n      TYPE = OAUTH\n      OAUTH_CLIENT_ID = 'myClientId'\n      OAUTH_CLIENT_SECRET = 'myClientSecret'\n      OAUTH_ALLOWED_SCOPES = ('PRINCIPAL_ROLE:ALL')\n    )\n    ENABLED = TRUE;\n```\n\n----------------------------------------\n\nTITLE: Schema Path Configuration in TOML\nDESCRIPTION: Alternative way to set schema paths using TOML configuration file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/adjust-a-schema.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\nexport_schema_path=\"schemas/export\"\nimport_schema_path=\"schemas/import\"\n```\n\n----------------------------------------\n\nTITLE: Installing Pipeline Dependencies\nDESCRIPTION: Command to install required Python dependencies for the Zendesk pipeline\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/zendesk.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: GitHub Repository Events Source Implementation\nDESCRIPTION: Python implementation of the GitHub repository events source with incremental loading capability.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/github.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source(max_table_nesting=2)\ndef github_repo_events(\n    owner: str, name: str, access_token: str = None\n) -> DltResource:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Verifying Pipeline Execution\nDESCRIPTION: This command shows the status and details of the executed pipeline, replacing <pipeline_name> with the actual name of the pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Initializing Personio Pipeline with DLT CLI\nDESCRIPTION: Command to initialize a new dlt pipeline for Personio with DuckDB as the destination. This creates a new directory with necessary files and configurations.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/personio.md#2025-04-14_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndlt init personio duckdb\n```\n\n----------------------------------------\n\nTITLE: Creating and activating virtual environment on Unix-based systems\nDESCRIPTION: Commands to create a new virtual environment using uv and activate it on Ubuntu or macOS.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/installation.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nuv venv --python 3.10\nsource .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Testing Python Example with Name-Main Pattern\nDESCRIPTION: A code pattern recommendation for testing examples by including a '__name__ == \"__main__\"' clause that will be used for automated testing.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/examples/CONTRIBUTING.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nif __name__ = \"__main__\"\n```\n\n----------------------------------------\n\nTITLE: Installing MS SQL dependencies from requirements file\nDESCRIPTION: Command to install the necessary dependencies for MS SQL from the requirements.txt file generated by the init command.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/mssql.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Cursor IDE MCP Server Configuration\nDESCRIPTION: JSON configuration for setting up the dlt MCP server in Cursor IDE. This uses the same format as Claude Desktop to define the server configuration for Agent Mode.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/llm-tooling/mcp-server.md#2025-04-14_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"mcpServers\": {\n    \"dlt\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"tool\",\n        \"run\",\n        \"--with\",\n        \"dlt-plus[mcp]==0.9.0\",\n        \"dlt\",\n        \"mcp\",\n        \"run\"\n      ]\n    } \n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Amazon Kinesis Stream Name in TOML\nDESCRIPTION: Optional TOML configuration for specifying the Amazon Kinesis stream name in the config.toml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/amazon_kinesis.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[sources.kinesis]\nstream_name = \"please set me up!\" # Stream name (Optional).\n```\n\n----------------------------------------\n\nTITLE: Configuring Claude Desktop for MCP in JSON\nDESCRIPTION: JSON configuration for Claude Desktop to enable Model Context Protocol integration with a dlt+ project.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/ai.md#2025-04-14_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"mcpServers\": {\n    \"dlt+ project\": {\n      \"command\": \"</path/to/your/project/.venv/bin/dlt>\",\n      \"args\": [\n        \"project\",\n        \"--project\",\n        \"<path/to/your/project>\",\n        \"mcp\"\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling All Tracing in dlt via TOML Configuration\nDESCRIPTION: This snippet shows how to completely disable all pipeline tracing in dlt, including both anonymous telemetry and Sentry tracing, by setting a configuration option in config.toml.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/running-in-production/tracing.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\nenable_runtime_trace=false\n```\n\n----------------------------------------\n\nTITLE: Defining a Data Source in Python\nDESCRIPTION: Example of defining a data source for movies in Python, which will be used in the dlt pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/lancedb.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.destinations.adapters import lancedb_adapter\n\n\nmovies = [\n  {\n    \"id\": 1,\n    \"title\": \"Blade Runner\",\n    \"year\": 1982,\n  },\n  {\n    \"id\": 2,\n    \"title\": \"Ghost in the Shell\",\n    \"year\": 1995,\n  },\n  {\n    \"id\": 3,\n    \"title\": \"The Matrix\",\n    \"year\": 1999,\n  },\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Project Dependencies\nDESCRIPTION: TOML configuration file specifying project metadata, Python version requirements, and dependencies for the dlt+ project package.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/data-access.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[project]\\nname = \"dlt_example_project\"\\nversion = \"0.0.1\"\\ndescription = \"Description\"\\nrequires-python = \">=3.9,<3.13\"\\n\\ndependencies = [\\n    \"dlt>=1.7.0\",\\n    \"dlt-plus==0.7.0\"\\n]\\n\\n[project.entry-points.dlt_package]\\ndlt-project = \"dlt_example_project\"\n```\n\n----------------------------------------\n\nTITLE: Defining Transformations in dlt.yml\nDESCRIPTION: Illustrates how to configure transformations in the dlt.yml manifest file, specifying the engine and cache to use.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/projects.md#2025-04-14_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ntransformations:\n  github_events_transformations:\n    engine: arrow\n    cache: github_events_cache\n```\n\n----------------------------------------\n\nTITLE: Windows Extended Paths Configuration\nDESCRIPTION: Configuration for Windows extended paths to handle paths longer than 255 characters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_22\n\nLANGUAGE: toml\nCODE:\n```\n[destination.regular_extended]\nbucket_url = '\\\\?\\C:\\a\\b\\c'\n\n[destination.unc_extended]\nbucket_url='\\\\?\\UNC\\localhost\\c$\\a\\b\\c'\n```\n\n----------------------------------------\n\nTITLE: ClickHouse Configuration Options\nDESCRIPTION: Additional configuration options for ClickHouse destination including dataset separators and table engine settings\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/clickhouse.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[destination.clickhouse]\ndataset_table_separator = \"___\"\ntable_engine_type = \"merge_tree\"\ndataset_sentinel_table_name = \"dlt_sentinel_table\"\nstaging_use_https = true\n```\n\n----------------------------------------\n\nTITLE: Setting Case Sensitivity for MSSQL Destination in TOML Configuration\nDESCRIPTION: This TOML configuration snippet demonstrates how to set the 'has_case_sensitive_identifiers' flag for an MSSQL destination. This indicates to DLT that the destination has been configured for case-sensitive identifiers.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination.md#2025-04-14_snippet_12\n\nLANGUAGE: toml\nCODE:\n```\n[destination.mssql]\nhas_case_sensitive_identifiers=true\n```\n\n----------------------------------------\n\nTITLE: Run GitHub Pipeline\nDESCRIPTION: Command to execute the GitHub data pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/github.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython github_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Setting Additional Snowflake Connection Options\nDESCRIPTION: Example of configuring additional connection options for Snowflake, such as timezone and session keep-alive settings, which are passed to the Snowflake Python Connector.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_9\n\nLANGUAGE: toml\nCODE:\n```\n[destination.snowflake.credentials]\ndatabase = \"dlt_data\"\nauthenticator=\"oauth\"\n[destination.snowflake.credentials.query]\ntimezone=\"UTC\"\n# keep session alive beyond 4 hours\nclient_session_keep_alive=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Scrapy Source in TOML\nDESCRIPTION: TOML configuration for the Scrapy source, specifying start URLs for scraping.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/scrapy.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# put your configuration values here\n[sources.scraping]\nstart_urls = [\"URL to be scraped\"] # please set me up!\nstart_urls_file = \"/path/to/urls.txt\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Service Account Credentials Details\nDESCRIPTION: Additional service account credential configuration in TOML format.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_ads.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[sources.google_ads.credentials]\nproject_id = \"project_id\" # please set me up!\nclient_email = \"client_email\" # please set me up!\nprivate_key = \"private_key\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Initializing Shopify Pipeline\nDESCRIPTION: Shell command to initialize a new Shopify pipeline with DuckDB as the destination\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/shopify.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init shopify_dlt duckdb\n```\n\n----------------------------------------\n\nTITLE: Installing dlt+ using pip\nDESCRIPTION: Command to install the dlt+ library using pip. Requires Python versions 3.9 to 3.12. Note that a license is required to run dlt+.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/intro.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install dlt-plus\n```\n\n----------------------------------------\n\nTITLE: Setting File Format via Environment Variable\nDESCRIPTION: Set the loader file format using an environment variable NORMALIZE__LOADER_FILE_FORMAT.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/file-formats/_set_the_format.mdx#2025-04-14_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n\"export\" NORMALIZE__LOADER_FILE_FORMAT=\"{props.file_type}\"\n```\n\n----------------------------------------\n\nTITLE: Creating and activating virtual environment on Windows\nDESCRIPTION: Commands to create a new virtual environment using uv and activate it on Windows.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/installation.md#2025-04-14_snippet_5\n\nLANGUAGE: bat\nCODE:\n```\nC:\\> uv venv --python 3.10\nC:\\> .\\venv\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Installing dlt-plus-tests Package\nDESCRIPTION: Command to install the dlt-plus-tests package from the dltHub PyPI registry.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/quality/tests.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install --index-url https://pypi.dlthub.com --no-deps  dlt-plus-tests\n```\n\n----------------------------------------\n\nTITLE: OAuth Token Generation Script Execution\nDESCRIPTION: Command to run the OAuth setup script for generating refresh token.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_ads.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npython google_ads/setup_script_gcp_oauth.py\n```\n\n----------------------------------------\n\nTITLE: DLT Project Directory Structure\nDESCRIPTION: Example of DLT project directory structure showing configuration and secrets files for different profiles.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/core-concepts/profiles.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n.\nâ”œâ”€â”€ .dlt/                 # your dlt settings including profile settings\nâ”‚   â”œâ”€â”€ config.toml\nâ”‚   â”œâ”€â”€ dev.secrets.toml\nâ”‚   â””â”€â”€ tests.secrets.toml\n```\n\n----------------------------------------\n\nTITLE: Installing Google Cloud Secret Manager\nDESCRIPTION: Command to install the Google Cloud Secret Manager library using pip.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add_credentials.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install google-cloud-secret-manager\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Names for DLT Credentials\nDESCRIPTION: Shell commands showing the environment variable naming convention for DLT credentials configuration.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add_credentials.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nSOURCES__PIPEDRIVE__PIPEDRIVE_API_KEY\nDESTINATION__BIGQUERY__CREDENTIALS__PROJECT_ID\nDESTINATION__BIGQUERY__CREDENTIALS__PRIVATE_KEY\nDESTINATION__BIGQUERY__CREDENTIALS__CLIENT_EMAIL\nDESTINATION__BIGQUERY__LOCATION\n```\n\n----------------------------------------\n\nTITLE: Initializing Notion dlt Pipeline with DuckDB Destination\nDESCRIPTION: Command to initialize a dlt pipeline for Notion with DuckDB as the destination. This sets up the necessary files and configuration for the pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/notion.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init notion duckdb\n```\n\n----------------------------------------\n\nTITLE: Building Docusaurus Website for Netlify Deployment\nDESCRIPTION: This command builds the Docusaurus website for deployment on Netlify, placing the build in the 'build/docs' folder.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/README.md#2025-04-14_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm run build:netlify\n```\n\n----------------------------------------\n\nTITLE: Running Python Pipeline\nDESCRIPTION: Command to verify pipeline functionality by executing the pipeline script\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-github-actions.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npython3 chess_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Asana Credentials in secrets.toml\nDESCRIPTION: TOML configuration for storing the Asana API access token in the dlt secrets file. This token is required to authenticate with the Asana API.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/asana.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[sources.asana_dlt]\naccess_token = \"access_token\"\n```\n\n----------------------------------------\n\nTITLE: Extended Path Configuration for Windows\nDESCRIPTION: TOML configuration example for handling extremely long file paths in Windows.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/basic.md#2025-04-14_snippet_14\n\nLANGUAGE: toml\nCODE:\n```\n[sources.filesystem]\nbucket_url = '\\\\?\\C:\\a\\b\\c'\n```\n\n----------------------------------------\n\nTITLE: Running dlt Pipeline from Command Line\nDESCRIPTION: Demonstrates how to run a dlt pipeline directly from the command line using the 'dlt pipeline' command. This allows for faster production deployment of pipelines.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/production/runners.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline my_pipeline run\n```\n\n----------------------------------------\n\nTITLE: Configuring Iceberg Destination in YAML\nDESCRIPTION: Define the Iceberg destination configuration in the dlt.yml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/iceberg.md#2025-04-14_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndestinations:\n  iceberg_destination:\n    type: iceberg\n    bucket_url: \"s3://your_bucket\" # replace with bucket url\n```\n\n----------------------------------------\n\nTITLE: Viewing and Printing Schema in YAML Format\nDESCRIPTION: Simple Python command to display the default schema in a structured YAML format, making it easy to review schema details.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema.md#2025-04-14_snippet_13\n\nLANGUAGE: py\nCODE:\n```\npipeline.default_schema.to_pretty_yaml()\n```\n\n----------------------------------------\n\nTITLE: Databricks OAuth2 Environment Variables Setup\nDESCRIPTION: Environment variable configuration for Databricks OAuth2 authentication\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/databricks.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nexport DESTINATIONS__DATABRICKS__CREDENTIALS__SERVER_HOSTNAME=\"MY_DATABRICKS.azuredatabricks.net\"\nexport DESTINATIONS__DATABRICKS__CREDENTIALS__HTTP_PATH=\"/sql/1.0/warehouses/12345\"\nexport DESTINATIONS__DATABRICKS__CREDENTIALS__CATALOG=\"my_catalog\"\nexport DESTINATIONS__DATABRICKS__CREDENTIALS__CLIENT_ID=\"XXX\"\nexport DESTINATIONS__DATABRICKS__CREDENTIALS__CLIENT_SECRET=\"XXX\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Pipeline Execution\nDESCRIPTION: Command to display the status and results of the Slack pipeline after execution, using the dlt CLI tool.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/slack.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Running Zendesk Pipeline\nDESCRIPTION: Command to execute the Zendesk data pipeline script\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/zendesk.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython zendesk_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Show Pipeline Status\nDESCRIPTION: Command to display the pipeline execution status.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/airtable.md#2025-04-14_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Inspecting Transformed Data\nDESCRIPTION: Demonstrates how to inspect the transformed data after column removal, showing the resulting structure without the removed column.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/customising-pipelines/removing_columns.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor row in source_instance:\n    print(row)\n#{'id': 0, 'name': 'Jane Washington 0'}\n#{'id': 1, 'name': 'Jane Washington 1'}\n#{'id': 2, 'name': 'Jane Washington 2'}\n```\n\n----------------------------------------\n\nTITLE: Adding Project Components\nDESCRIPTION: CLI commands to add source, destination, and pipeline to a project\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\ndlt source my_arrow_source add arrow\n```\n\nLANGUAGE: sh\nCODE:\n```\ndlt destination my_duckdb_destination add duckdb\n```\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline my_pipeline add my_arrow_source my_duckdb_destination\n```\n\nLANGUAGE: sh\nCODE:\n```\ndlt source sql_db_1 add sql_database\n```\n\n----------------------------------------\n\nTITLE: Deploying Modal Pipeline\nDESCRIPTION: CLI command to deploy the pipeline for continuous execution on Modal\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-modal.md#2025-04-14_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nmodal deploy sql_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Example dlt Pipeline Load Telemetry Message\nDESCRIPTION: JSON example of a telemetry message sent when a pipeline load step is completed, showing metrics like execution time and anonymized identifiers.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/telemetry.md#2025-04-14_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"anonymousId\": \"570816b273a41d16caacc26a797204d9\",\n  \"context\": {\n    \"ci_run\": false,\n    \"cpu\": 3,\n    \"exec_info\": [],\n    \"library\": {\n      \"name\": \"dlt\",\n      \"version\": \"0.2.0a26\"\n    },\n    \"os\": {\n      \"name\": \"Darwin\",\n      \"version\": \"21.6.0\"\n    },\n    \"python\": \"3.10.10\"\n  },\n  \"event\": \"pipeline_load\",\n  \"properties\": {\n    \"destination_name\": \"duckdb\",\n    \"destination_fingerprint\": \"\",\n    \"pipeline_name_hash\": \"OpVShb3cX7qQAmOZSbV8\",\n    \"dataset_name_hash\": \"Hqk0a3Ov5AD55KjSg2rC\",\n    \"default_schema_name_hash\": \"Hqk0a3Ov5AD55KjSg2rC\",\n    \"elapsed\": 2.234885,\n    \"event_category\": \"pipeline\",\n    \"event_name\": \"load\",\n    \"success\": true,\n    \"transaction_id\": \"39c3b69c858836c36b9b7c6e046eb391\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Printing Pipeline Execution Results\nDESCRIPTION: Simple Python code to print the execution information after running a dlt pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/qdrant.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Creating Entry Point for dlt Pipeline Execution\nDESCRIPTION: A simple Python entry point file (run_pipelines.py) that imports and executes the main dlt pipeline function. This file serves as the single point of entry for running the pipeline in Orchestra.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-orchestra.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pipeline.py import run_pipeline\nimport os\n\nrun_pipeline()\n```\n\n----------------------------------------\n\nTITLE: Inspecting Datasets for Different Profiles\nDESCRIPTION: These commands display the row counts for the 'my_duckdb_destination_dataset' dataset using both 'dev' and 'prod' profiles after loading data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\ndlt dataset --profile dev my_duckdb_destination_dataset row-counts\ndlt dataset --profile prod my_duckdb_destination_dataset row-counts\n```\n\n----------------------------------------\n\nTITLE: Setting Filesystem Credentials as Environment Variables\nDESCRIPTION: This example demonstrates how to set filesystem credentials as environment variables. It shows the correct naming convention for the variables, replacing dots with double underscores.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/basic.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nexport SOURCES__FILESYSTEM__CREDENTIALS__AWS_ACCESS_KEY_ID = \"Please set me up!\"\nexport SOURCES__FILESYSTEM__CREDENTIALS__AWS_SECRET_ACCESS_KEY = \"Please set me up!\"\n```\n\n----------------------------------------\n\nTITLE: Importing Essential Test Fixtures\nDESCRIPTION: Python code showing how to import essential test fixtures in conftest.py for dlt+ testing.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/quality/tests.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt_plus_tests.fixtures import (\n    auto_preserve_environ as auto_preserve_environ,\n    drop_pipeline as drop_pipeline,\n    autouse_test_storage as autouse_test_storage,\n)\n```\n\n----------------------------------------\n\nTITLE: Alternative Destination Implementation Methods\nDESCRIPTION: Different approaches to implementing and referencing custom destination functions in a DLT pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/destination.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dlt.destination(batch_size=10)\ndef local_destination_func(items: TDataItems, table: TTableSchema) -> None:\n    ...\n\n# Reference function directly\np = dlt.pipeline(\"my_pipe\", destination=local_destination_func)\n\n# With explicit credentials\n@dlt.destination(batch_size=10, loader_file_format=\"jsonl\", name=\"my_destination\")\ndef my_destination(items: TDataItems, table: TTableSchema, api_key: str = dlt.secrets.value) -> None:\n    ...\n\np = dlt.pipeline(\"my_pipe\", destination=my_destination(api_key=os.getenv(\"API_KEY\")))\n```\n\n----------------------------------------\n\nTITLE: Showing Pipeline Status\nDESCRIPTION: Shell command to display the status of a pipeline execution.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/jira.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Setting Database Connection Credentials\nDESCRIPTION: Environment variable configuration for database connection credentials using Modal secrets\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-modal.md#2025-04-14_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nSOURCES__SQL_DATABASE__CREDENTIALS=mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\n```\n\n----------------------------------------\n\nTITLE: Starting Weaviate Container with Docker Compose\nDESCRIPTION: Command to start the Weaviate container using Docker Compose with a specific compose file located in the .github folder.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/dlt/destinations/impl/weaviate/README.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose -f .github/weaviate-compose.yml up -d\n```\n\n----------------------------------------\n\nTITLE: Using AWS Profile for Authentication\nDESCRIPTION: Alternative configuration in secrets.toml for using existing AWS profiles instead of hardcoded credentials.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/athena.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem.credentials]\nprofile_name=\"dlt-ci-user\"\n\n[destination.athena.credentials]\nprofile_name=\"dlt-ci-user\"\n```\n\n----------------------------------------\n\nTITLE: Running the Asana Pipeline\nDESCRIPTION: Command to execute the Asana data pipeline script that loads data from Asana to the configured destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/asana.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython asana_dlt_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Google Sheets Pipeline\nDESCRIPTION: This command installs the necessary dependencies for running the Google Sheets pipeline using pip and the requirements.txt file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Generating OAuth Refresh Token\nDESCRIPTION: Command to run a setup script for generating the OAuth refresh token for Google Analytics authentication.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_analytics.md#2025-04-14_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npython google_analytics/setup_script_gcp_oauth.py\n```\n\n----------------------------------------\n\nTITLE: Creating Database in Redshift SQL\nDESCRIPTION: SQL command to create a new database named 'dlt_ci' in Redshift. This is part of the loader account setup process.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/dlt/destinations/impl/redshift/README.md#2025-04-14_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE DATABASE dlt_ci\n```\n\n----------------------------------------\n\nTITLE: Initializing DLT Pipeline for Stripe\nDESCRIPTION: Command to initialize a new DLT pipeline for Stripe analytics with DuckDB as destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/stripe.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init stripe_analytics duckdb\n```\n\n----------------------------------------\n\nTITLE: Configuring Load Workers in TOML\nDESCRIPTION: TOML configuration to set the number of load workers, which can help with connection issues.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/motherduck.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[load]\nworkers=3\n```\n\n----------------------------------------\n\nTITLE: Initializing dlt SQL Database Pipeline\nDESCRIPTION: CLI command to initialize a new dlt project for SQL database source with DuckDB destination\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-modal.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init sql_database duckdb\n```\n\n----------------------------------------\n\nTITLE: Generating DBT Project Command\nDESCRIPTION: Shell command to generate a baseline dbt project from a DLT pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/transformations/dbt-transformations.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndlt dbt generate <pipeline-name>\n```\n\n----------------------------------------\n\nTITLE: Installing Streamlit for Data Exploration\nDESCRIPTION: Command to install Streamlit, which is used by DLT to provide a browser-based interface for exploring the loaded data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/rest-api.md#2025-04-14_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npip install streamlit\n```\n\n----------------------------------------\n\nTITLE: Importing React Components for Documentation in JSX\nDESCRIPTION: Imports React components used for documentation, including DocCardList for displaying a card list of destinations and a custom Link component for booking an onboarding call.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/index.md#2025-04-14_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\nimport Link from '../../_book-onboarding-call.md';\n```\n\n----------------------------------------\n\nTITLE: Setting Up Dagster Project\nDESCRIPTION: Commands to create a new Dagster project directory and scaffold a project named 'github-issues'.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-dagster.md#2025-04-14_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmkdir dagster_github_issues\ncd dagster_github_issues\ndagster project scaffold --name github-issues\n```\n\n----------------------------------------\n\nTITLE: Installing dlt using Pixi\nDESCRIPTION: Command to install dlt using the Pixi package manager.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/installation.md#2025-04-14_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\npixi add dlt\n```\n\n----------------------------------------\n\nTITLE: Google Secrets Configuration in TOML\nDESCRIPTION: TOML configuration for Google Cloud credentials in DLT secrets file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add_credentials.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[google_secrets.credentials]\n\"project_id\" = \"<project_id>\"\n\"private_key\" = \"-----BEGIN PRIVATE KEY-----\\n....\\n-----END PRIVATE KEY-----\\n\"\n\"client_email\" = \"....gserviceaccount.com\"\n```\n\n----------------------------------------\n\nTITLE: Database Credentials in Native Connection String Form\nDESCRIPTION: TOML configuration example showing database credentials as a connection string.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\ndsn=\"postgres://loader:loader@localhost:5432/dlt_data\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Asana Pipeline with dlt CLI\nDESCRIPTION: Command to initialize a new pipeline with Asana as the source and DuckDB as the destination using the dlt command-line interface.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/asana.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init asana_dlt duckdb\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for DLT Documentation Tools\nDESCRIPTION: Command to install required dependencies for the documentation tools using pip. This should be run from the directory containing the requirements.txt file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/tools/README.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Initialize DLT Pipeline with Shell Command\nDESCRIPTION: Command to initialize a new DLT pipeline with GitHub as source and DuckDB as destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/github.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init github duckdb\n```\n\n----------------------------------------\n\nTITLE: Configuring Credentials for dlt Pipeline in Orchestra\nDESCRIPTION: Example JSON configuration for setting up environment variables and credentials in Orchestra for a dlt pipeline. This configuration includes destination credentials for BigQuery and source credentials for HubSpot.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-orchestra.md#2025-04-14_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{\n  \"DESTINATION__BIGQUERY__LOCATION\": \"*******st2\",\n  \"DESTINATION__BIGQUERY__CREDENTIALS__CLIENT_EMAIL\": \"*******com\",\n  \"DESTINATION__BIGQUERY__CREDENTIALS__PROJECT_ID\": \"*******114\",\n  \"DESTINATION__BIGQUERY__CREDENTIALS__PRIVATE_KEY\": \"*******--\\n\",\n  \"SOURCES__HUBSPOT__API_KEY\": \"*******1e4\"\n}\n```\n\n----------------------------------------\n\nTITLE: Mixed Form Database Credentials Configuration\nDESCRIPTION: TOML configuration example showing credentials in mixed form with just password.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\ndsn.password=\"loader\"\n```\n\n----------------------------------------\n\nTITLE: Launching Streamlit App for DLT Pipeline\nDESCRIPTION: Command to launch the Streamlit app for a specific DLT pipeline. Replace {pipeline_name} with the actual name of your pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/streamlit.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline {pipeline_name} show\n```\n\n----------------------------------------\n\nTITLE: Setting dbt Cloud Environment Variables\nDESCRIPTION: This snippet demonstrates how to set dbt Cloud credentials as environment variables. The variables follow a specific naming convention where sections are separated by double underscores and names are capitalized.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/transformations/dbt/dbt_cloud.md#2025-04-14_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nDBT_CLOUD__API_TOKEN\nDBT_CLOUD__ACCOUNT_ID\nDBT_CLOUD__JOB_ID\n```\n\n----------------------------------------\n\nTITLE: Enabling PostGIS Extension in Postgres\nDESCRIPTION: SQL command to enable the PostGIS extension in a Postgres database for spatial data support.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/postgres.md#2025-04-14_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTENSION postgis;\n```\n\n----------------------------------------\n\nTITLE: Pushing to GitHub Repository\nDESCRIPTION: Git command to push the committed changes to GitHub\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-github-actions.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ngit push origin\n```\n\n----------------------------------------\n\nTITLE: Pipeline Execution Output for GitHub Events Loading\nDESCRIPTION: Console output showing the execution of a GitHub events pipeline, including API request status and load completion details with timing information\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/create-new-destination.md#2025-04-14_snippet_4\n\nLANGUAGE: text\nCODE:\n```\npython github_pipeline.py\ngot page https://api.github.com/repos/apache/airflow/events?per_page=100, requests left: 59\ngot page https://api.github.com/repositories/33884891/events?per_page=100&page=2, requests left: 58\ngot page https://api.github.com/repositories/33884891/events?per_page=100&page=3, requests left: 57\nPipeline github_events completed in 4.56 seconds\n1 load package(s) were loaded to destination presto and into dataset airflow_events\nThe presto destination used postgres://loader:***@localhost:5432/dlt_data location to store data\nLoad package 1690628947.953597 is LOADED and contains no failed jobs\n```\n\n----------------------------------------\n\nTITLE: Configuring the INSERT File Format\nDESCRIPTION: A component usage showing how to configure the SQL INSERT file format in the application.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/file-formats/insert-format.md#2025-04-14_snippet_1\n\nLANGUAGE: mdx\nCODE:\n```\n<SetTheFormat file_type=\"insert_values\"/>\n```\n\n----------------------------------------\n\nTITLE: Running dlt MCP Server with UV Package Manager\nDESCRIPTION: The command to run the dlt MCP server using the UV package manager. This creates an isolated environment with the required dependencies and launches the server.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/llm-tooling/mcp-server.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nuv tool run --with \"dlt-plus[mcp]==0.9.0\" dlt mcp run\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Google Analytics Pipeline\nDESCRIPTION: Command to install required dependencies for the Google Analytics pipeline from the requirements.txt file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_analytics.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Database-Specific Driver for MySQL\nDESCRIPTION: Command to install the PyMySQL package, which is the required database driver for connecting to MySQL databases through SQLAlchemy in the dlt pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/sql-database.md#2025-04-14_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npip install pymysql\n```\n\n----------------------------------------\n\nTITLE: Creating Project Directory\nDESCRIPTION: Shell commands to create and navigate to a new project directory\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nmkdir tutorial && cd tutorial\n```\n\n----------------------------------------\n\nTITLE: Running Shopify Pipeline\nDESCRIPTION: Shell command to execute the Shopify data pipeline\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/shopify.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npython shopify_dlt_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Basic Pipeline Configuration\nDESCRIPTION: Example of configuring a DLT pipeline with custom name, destination, and dataset specifications\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/zendesk.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"dlt_zendesk_pipeline\",  # Use a custom name if desired\n    destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"sample_zendesk_data\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Installing dlt library using uv\nDESCRIPTION: Command to install or upgrade to the newest version of dlt using the uv package manager.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/installation.md#2025-04-14_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nuv pip install -U dlt\n```\n\n----------------------------------------\n\nTITLE: Configuring PyAthena Connection Properties\nDESCRIPTION: Example of setting additional PyAthena connection properties in the secrets.toml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/athena.md#2025-04-14_snippet_9\n\nLANGUAGE: toml\nCODE:\n```\n[destination.athena.conn_properties]\npoll_interval=2\n```\n\n----------------------------------------\n\nTITLE: Installing dlt with Filesystem Dependencies\nDESCRIPTION: Command to install dlt with necessary filesystem dependencies, which includes s3fs and botocore packages for working with cloud storage.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[filesystem]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Command to install the necessary dependencies for the Postgres project from the requirements.txt file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/postgres.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Initializing dlt Pipeline for Google Sheets with DuckDB\nDESCRIPTION: Command to initialize a dlt pipeline using Google Sheets as the source and DuckDB as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ndlt init google_sheets duckdb\n```\n\n----------------------------------------\n\nTITLE: Loading Data from SQL Database with dlt\nDESCRIPTION: Example of using dlt to extract data from a SQL database (MySQL in this case). Shows setup of pipeline to load data into DuckDB.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/intro.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.sql_database import sql_database\n\nsource = sql_database(\n    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\"\n)\n\npipeline = dlt.pipeline(\n    pipeline_name=\"sql_database_example\",\n    destination=\"duckdb\",\n    dataset_name=\"sql_data\",\n)\n\nload_info = pipeline.run(source)\n\n# print load info and the \"family\" table as dataframe\nprint(load_info)\nprint(pipeline.dataset().family.df())\n```\n\n----------------------------------------\n\nTITLE: Creating Postgres User for dlt\nDESCRIPTION: SQL command to create a new user named 'loader' with a password for use with dlt.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/postgres.md#2025-04-14_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER loader WITH PASSWORD '<password>';\n```\n\n----------------------------------------\n\nTITLE: Installing dlt via pip\nDESCRIPTION: Command to install the dlt Python package using pip. The library supports Python 3.9+ with experimental support for Python 3.13.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/README.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install dlt\n```\n\n----------------------------------------\n\nTITLE: Configuring API Secrets in TOML\nDESCRIPTION: Configuration file for storing the ExchangeRate-API key securely\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/currency_conversion_data_enrichment.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[sources]\napi_key= \"Please set me up!\"  # ExchangeRate-API key\n```\n\n----------------------------------------\n\nTITLE: Granting External Volume Usage in Snowflake\nDESCRIPTION: SQL command to grant usage permissions on external volume to a specific role in Snowflake\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/snowflake_plus.md#2025-04-14_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nGRANT USAGE ON EXTERNAL VOLUME <external_volume_name> TO ROLE <role_name>;\n```\n\n----------------------------------------\n\nTITLE: Credentials Configuration\nDESCRIPTION: TOML configuration for storing email service credentials securely\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/inbox.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# put your secret values and credentials here\n# do not share this file and do not push it to GitHub\n[sources.inbox]\nhost = \"Please set me up!\" # The host address of the email service provider.\nemail_account = \"Please set me up!\" # Email account associated with the service.\npassword = \"Please set me up!\" # APP Password for the above email account.\n```\n\n----------------------------------------\n\nTITLE: Running Full Check on Embedded Snippets in DLT Documentation\nDESCRIPTION: Command to run a full check on all embedded code snippets in the DLT documentation. This includes parsing, linting, and future type checking.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/tools/README.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npython check_embedded_snippets.py full\n```\n\n----------------------------------------\n\nTITLE: Rendering Help Admonition with Community Links in JSX\nDESCRIPTION: Creates an admonition component with a title about getting help and includes links to Slack community and onboarding call booking.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/_source-info-header.md#2025-04-14_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<Admonition title=\"Need help deploying these sources or figuring out how to run them in your data stack?\">\n<a href=\"https://dlthub.com/community\">Join our Slack community</a> or <Link/>.\n</Admonition>\n```\n\n----------------------------------------\n\nTITLE: Initialize Pipeline Command\nDESCRIPTION: Shell command to initialize the inbox pipeline with DuckDB destination\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/inbox.md#2025-04-14_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndlt init inbox duckdb\n```\n\n----------------------------------------\n\nTITLE: Inspecting Dataset Row Counts for Different Profiles\nDESCRIPTION: These commands display the row counts for the 'my_duckdb_destination_dataset' dataset using both 'dev' and 'prod' profiles.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\ndlt dataset --profile dev my_duckdb_destination_dataset row-counts\ndlt dataset --profile prod my_duckdb_destination_dataset row-counts\n```\n\n----------------------------------------\n\nTITLE: Verifying Pipeline Execution\nDESCRIPTION: Command to display the status and results of the Workable pipeline execution.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/workable.md#2025-04-14_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Installing General Dependencies for dlt SQL Pipeline\nDESCRIPTION: Command to install the general dependencies required for the SQL database pipeline as specified in the requirements.txt file generated during project initialization.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/sql-database.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Initializing Chess.com API Pipeline with DuckDB Destination\nDESCRIPTION: Command to initialize a dlt pipeline using Chess.com as the source and DuckDB as the destination. This creates a new directory with necessary files and configurations.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/chess.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init chess duckdb\n```\n\n----------------------------------------\n\nTITLE: Pipeline Status Check\nDESCRIPTION: Command to verify pipeline execution status.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_ads.md#2025-04-14_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Installing dlt using Conda\nDESCRIPTION: Command to install dlt using the Conda package manager from the conda-forge channel.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/installation.md#2025-04-14_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nconda install -c conda-forge dlt\n```\n\n----------------------------------------\n\nTITLE: Filesystem SQL Client Query Example\nDESCRIPTION: Demonstrates usage of the filesystem SQL client implementation, which uses an in-memory DuckDB instance to query data stored in files.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/sql-client.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(destination=\"filesystem\", dataset_name=\"my_dataset\")\nwith pipeline.sql_client() as client:\n    with client.execute_query(\"SELECT * FROM my_table\") as cursor:\n        print(cursor.fetchall())\n```\n\n----------------------------------------\n\nTITLE: Verifying Pipeline Execution\nDESCRIPTION: This command displays the results of the pipeline execution, allowing verification of loaded data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/setup.md#2025-04-14_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Installing dlt with Weaviate Support\nDESCRIPTION: Command to install dlt with Weaviate integration support using pip.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/weaviate.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[weaviate]\"\n```\n\n----------------------------------------\n\nTITLE: Running the SQL Database Pipeline\nDESCRIPTION: Command to execute the Python script that contains the dlt pipeline. This command will extract data from the SQL database and load it into DuckDB based on the configuration.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/sql-database.md#2025-04-14_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\npython sql_database_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Initializing Scrapy Verified Source with DuckDB Destination\nDESCRIPTION: Command to initialize a new dlt pipeline using Scrapy as the source and DuckDB as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/scrapy.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init scraping duckdb\n```\n\n----------------------------------------\n\nTITLE: Windows UNC Paths with File Scheme\nDESCRIPTION: Configuration examples for Windows UNC paths using the file:// scheme with host and path components.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_21\n\nLANGUAGE: toml\nCODE:\n```\n[destination.unc_with_host]\nbucket_url=\"file://localhost/c$/a/b/c\"\n\n[destination.unc_with_path]\nbucket_url=\"file:////localhost/c$/a/b/c\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Google Sheets ID in TOML Configuration\nDESCRIPTION: This snippet shows how to specify just the Google Sheets ID in the config.toml file for the spreadsheet_identifier.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\nspreadsheet_identifier=\"1VTtCiYgxjAwcIw7UM1_BSaxC3rzIpr0HwXZwd2OlPD4\"\n```\n\n----------------------------------------\n\nTITLE: Checking Python Environment\nDESCRIPTION: Commands to verify Python and pip versions are properly installed.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/installation.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npython --version\npip --version\n```\n\n----------------------------------------\n\nTITLE: Running DLT Pipelines with Different Profiles\nDESCRIPTION: These commands run the 'my_pipeline' pipeline using both 'dev' and 'prod' profiles to load data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline --profile dev my_pipeline run\ndlt pipeline --profile prod my_pipeline run\n```\n\n----------------------------------------\n\nTITLE: Configuring Procfile for Cloud Run Job\nDESCRIPTION: Content of the Procfile that instructs Cloud Run to execute the notion_pipeline.py script using Python 3.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-google-cloud-run.md#2025-04-14_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nweb: python3 notion_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Sample URL Parser API Response Structure\nDESCRIPTION: Example JSON response structure from the URL Parse API showing the various URL components that are extracted.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/url-parser-data-enrichment.md#2025-04-14_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"authority\": \"urlparse.com\",\n    \"domain\": \"urlparse.com\",\n    \"domain_label\": \"urlparse\",\n    \"file\": \"/\",\n    \"fragment\": null,\n    \"host\": \"urlparse.com\",\n    \"href\": \"https://urlparse.com/\",\n    \"is_valid\": true,\n    \"origin\": \"https://urlparse.com\",\n    \"params\": null,\n    \"path\": \"/\",\n    \"port\": null,\n    \"query\": null,\n    \"request_url\": \"https://urlparse.com\",\n    \"scheme\": \"https\",\n    \"subdomains\": null,\n    \"tld\": \"com\"\n}\n```\n\n----------------------------------------\n\nTITLE: Launching dlt's Data Browser Application\nDESCRIPTION: Command to start dlt's built-in data browser application, which enables interactive exploration of the data loaded into DuckDB by the pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/sql-database.md#2025-04-14_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline sql_to_duckdb_pipeline show\n```\n\n----------------------------------------\n\nTITLE: Installing Pipeline Dependencies\nDESCRIPTION: Shell command to install required Python dependencies for the pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/jira.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for AWS S3 Destination\nDESCRIPTION: Command to install the dependencies specified in the requirements.txt file generated during project initialization.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring BigQuery Additional Options\nDESCRIPTION: TOML configuration for setting BigQuery options including data location and various timeouts.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/bigquery.md#2025-04-14_snippet_13\n\nLANGUAGE: toml\nCODE:\n```\n[destination.bigquery]\nlocation=\"US\"\nhttp_timeout=15.0\nfile_upload_timeout=1800.0\nretry_deadline=60.0\n```\n\n----------------------------------------\n\nTITLE: Importing DocCardList Component in Markdown\nDESCRIPTION: This snippet imports the DocCardList component, likely used to display a list of documentation cards for the code examples.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/examples/index.md#2025-04-14_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n```\n\n----------------------------------------\n\nTITLE: Running the Notion Pipeline\nDESCRIPTION: Command to execute the Notion pipeline script, which will extract data from Notion databases and load it into the specified destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/notion.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython notion_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Initializing Mux Pipeline via Shell Command\nDESCRIPTION: Command to initialize a new Mux pipeline with DuckDB as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mux.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init mux duckdb\n```\n\n----------------------------------------\n\nTITLE: Basic Delta Reference in dlt\nDESCRIPTION: Simple reference showing how to refer to Delta in dlt code, demonstrating the basic syntax for Delta table format.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/table-formats/delta.md#2025-04-14_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\ndlt\n```\n\n----------------------------------------\n\nTITLE: Installing Streamlit via pip\nDESCRIPTION: Command to install the Streamlit package using pip. This is a prerequisite for viewing DLT pipeline data with Streamlit.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/streamlit.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install streamlit\n```\n\n----------------------------------------\n\nTITLE: Installing DuckDB Destination for dlt Pipeline\nDESCRIPTION: This command installs the necessary dependencies for using DuckDB as the destination in a dlt pipeline. It uses pip to install the dlt package with DuckDB support.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/currency_conversion_data_enrichment.md#2025-04-14_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[duckdb]\"\n```\n\n----------------------------------------\n\nTITLE: DLT Pipeline Initialization\nDESCRIPTION: Command to initialize a new DLT pipeline for Google Ads with DuckDB destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_ads.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndlt init google_ads duckdb\n```\n\n----------------------------------------\n\nTITLE: SFTP Username/Password Authentication\nDESCRIPTION: Configuration example for SFTP using username and password authentication.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_25\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\nbucket_url = \"sftp://[hostname]/[path]\"  # The hostname of your SFTP server and the remote path\nfile_glob = \"*\"                          # Pattern to match the files you want to upload/download\n\n[destination.filesystem.credentials]\nsftp_username = \"foo\"                    # Replace \"foo\" with your SFTP username\nsftp_password = \"pass\"                   # Replace \"pass\" with your SFTP password\n```\n\n----------------------------------------\n\nTITLE: Matomo Secrets Configuration\nDESCRIPTION: TOML configuration for storing Matomo API credentials securely\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/matomo.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n# put your secret values and credentials here\n# do not share this file and do not push it to GitHub\n[sources.matomo]\napi_token= \"access_token\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Installing Delta Dependencies for dlt\nDESCRIPTION: Command to install the necessary Python packages for using the Delta destination in dlt.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/delta.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install deltalake\npip install pyarrow>=2.0.18\n```\n\n----------------------------------------\n\nTITLE: Initializing Synapse Project\nDESCRIPTION: Commands to initialize a dlt project with Synapse destination and install requirements\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/synapse.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init chess synapse\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Setting AWS S3 Credentials for Iceberg\nDESCRIPTION: Configure AWS S3 credentials for Iceberg destination in the secrets.toml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/iceberg.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n# secrets.toml\n[destination.iceberg.credentials]\naws_access_key_id=\"Please set me up!\"\naws_secret_access_key=\"Please set me up!\"\n```\n\n----------------------------------------\n\nTITLE: Example of NULL Representation in CSV\nDESCRIPTION: Demonstrates how NULL values are represented in CSV files generated by dlt. Empty values can be represented either as empty strings or as empty quoted strings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/file-formats/csv.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ntext1,text2,text3\nA,B,C\nA,,\"\"\n```\n\n----------------------------------------\n\nTITLE: Basic Import of DLT Requests\nDESCRIPTION: Shows how to import the dlt requests wrapper instead of the standard requests library\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/http/requests.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.helpers import requests\n```\n\n----------------------------------------\n\nTITLE: Verifying Amazon Kinesis Pipeline Execution\nDESCRIPTION: Command to show the results of the Amazon Kinesis pipeline execution.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/amazon_kinesis.md#2025-04-14_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: dlt Init Command Output\nDESCRIPTION: Shows the output from the dlt init command with instructions for installing dependencies and next steps.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add-a-verified-source.md#2025-04-14_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nVerified source pipedrive was added to your project!\n* See the usage examples and code snippets to copy from pipedrive_pipeline.py\n* Add credentials for bigquery and other secrets in .dlt/secrets.toml\n* Add the required dependencies to pyproject.toml:\n  dlt[bigquery]>=0.3.1\n  If the dlt dependency is already added, make sure you install the extra for bigquery to it\n  If you are using poetry you may issue the following command:\n  poetry add dlt -E bigquery\n\n* Read https://dlthub.com/docs/walkthroughs/create-a-pipeline for more information\n```\n\n----------------------------------------\n\nTITLE: Checking dlt Telemetry Status\nDESCRIPTION: Command to check the current telemetry status in dlt.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/telemetry.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndlt telemetry\n```\n\n----------------------------------------\n\nTITLE: Adding Google Sheets API Scope for OAuth\nDESCRIPTION: Specifies the required scope for accessing Google Sheets API with read-only permissions.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n\"https://www.googleapis.com/auth/spreadsheets.readonly\"\n```\n\n----------------------------------------\n\nTITLE: Installing Streamlit for Data Visualization\nDESCRIPTION: Command to install Streamlit, which is used by dlt's built-in data browser application for visualizing and exploring the loaded data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/sql-database.md#2025-04-14_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npip install streamlit\n```\n\n----------------------------------------\n\nTITLE: Assigning Database Ownership in Redshift SQL\nDESCRIPTION: SQL command to alter the ownership of the 'dlt_ci' database to the 'loader' user. This grants full control of the database to the loader account.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/dlt/destinations/impl/redshift/README.md#2025-04-14_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nALTER DATABASE dlt_ci OWNER TO loader\n```\n\n----------------------------------------\n\nTITLE: Setting Database Owner for DLT in PostgreSQL\nDESCRIPTION: SQL command to set the 'loader' user as the owner of the 'dlt_data' database, granting full permissions.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/dlt/destinations/impl/postgres/README.md#2025-04-14_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nALTER DATABASE dlt_data OWNER TO loader\n```\n\n----------------------------------------\n\nTITLE: Granting Database Ownership to dlt User\nDESCRIPTION: SQL command to give the 'loader' user owner permissions on the 'dlt_data' database.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/postgres.md#2025-04-14_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nALTER DATABASE dlt_data OWNER TO loader;\n```\n\n----------------------------------------\n\nTITLE: Defining Replication Slot and Publication Names in Python\nDESCRIPTION: Sets the names for the replication slot and publication, which are used in the PostgreSQL logical replication process.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/pg_replication.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nslot_name = \"example_slot\"\npub_name = \"example_pub\"\n```\n\n----------------------------------------\n\nTITLE: Using Replace Write Disposition with LanceDB\nDESCRIPTION: Example of using the replace write disposition when running a dlt pipeline with LanceDB.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/lancedb.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ninfo = pipeline.run(\n  lancedb_adapter(\n    movies,\n    embed=\"title\",\n  ),\n  write_disposition=\"replace\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing dlt+ with MCP Support in Shell\nDESCRIPTION: Command to install dlt+ with Model Context Protocol support in a virtual environment using pip.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/ai.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install dlt-plus[mcp]\n```\n\n----------------------------------------\n\nTITLE: Initializing a dlt Project with Chess Source and Filesystem Destination\nDESCRIPTION: Command to initialize a new dlt project using chess as the data source and filesystem as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndlt init chess filesystem\n```\n\n----------------------------------------\n\nTITLE: Running the Chess.com API Pipeline\nDESCRIPTION: Command to execute the Chess.com API pipeline script, which will fetch and load data from Chess.com into the specified destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/chess.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npython chess_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Disabling dlt Telemetry via Command Line\nDESCRIPTION: Command to disable telemetry both in the current project and globally for the whole machine.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/telemetry.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt --disable-telemetry\n```\n\n----------------------------------------\n\nTITLE: Validating dlt+ Project Configuration in Shell\nDESCRIPTION: Shell command to validate the dlt+ project configuration using the dlt CLI.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/ai.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ndlt project config validate\n```\n\n----------------------------------------\n\nTITLE: Example JSON Secret Format\nDESCRIPTION: Sample JSON format for storing secrets in Google Cloud Secret Manager.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add_credentials.md#2025-04-14_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"api_token\": \"ghp_Kskdgf98dugjf98ghd....\"}\n```\n\n----------------------------------------\n\nTITLE: Initializing dlt Project with GitHub Source and Postgres Destination\nDESCRIPTION: Command to initialize a new dlt project with GitHub as a verified source and Postgres as the destination. This sets up the basic project structure for further customization.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/create-new-destination.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init github postgres\n```\n\n----------------------------------------\n\nTITLE: Installing dlt with SQLAlchemy Support\nDESCRIPTION: Commands to install dlt with SQLAlchemy and database-specific client libraries required for connection.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/sqlalchemy.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[sqlalchemy]\"\n```\n\nLANGUAGE: sh\nCODE:\n```\npip install mysqlclient\n```\n\n----------------------------------------\n\nTITLE: Google Ads OAuth Scope Configuration\nDESCRIPTION: Required OAuth scope for Google Ads API authentication.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_ads.md#2025-04-14_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n\"https://www.googleapis.com/auth/adwords\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Generic DLT Pipeline\nDESCRIPTION: Command option for creating a generic pipeline template using the --generic flag instead of a specific source variant.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/tests/cli/cases/deploy_pipeline/README.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n--generic\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: This command installs the necessary dependencies for the dlt pipeline from the requirements.txt file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/setup.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring License in TOML\nDESCRIPTION: TOML configuration for setting the dlt+ license key in secrets file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/installation.md#2025-04-14_snippet_9\n\nLANGUAGE: toml\nCODE:\n```\n[runtime]\nlicense=\"eyJhbGciOiJSUz...vKSjbEc===\"\n```\n\n----------------------------------------\n\nTITLE: Fixing Grammar in All DLT Documentation Pages\nDESCRIPTION: Command to run grammar correction using GPT on all DLT documentation pages. Requires an OpenAI API key in a .env file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/tools/README.md#2025-04-14_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npython fix_grammar_gpt.py\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment (Windows)\nDESCRIPTION: Commands to create and activate a Python virtual environment on Windows.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/installation.md#2025-04-14_snippet_6\n\nLANGUAGE: bat\nCODE:\n```\nC:\\> uv venv --python 3.10\nC:\\> .\\venv\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Installing Delta Dependencies for Python\nDESCRIPTION: Commands to install the required dependencies for using Delta table format with dlt.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/delta-iceberg.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[deltalake]\"\npip install 'pyarrow>=17.0.0'\n```\n\n----------------------------------------\n\nTITLE: Clearing Docusaurus Documentation Versions\nDESCRIPTION: This command clears all generated documentation versions, allowing for a clean slate when updating versions.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/README.md#2025-04-14_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nnpm run clear-versions\n```\n\n----------------------------------------\n\nTITLE: Disabling dlt Telemetry via Configuration File\nDESCRIPTION: Configuration snippet for the config.toml file to disable telemetry by setting the runtime.dlthub_telemetry option to false.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/telemetry.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[runtime]\n\ndlthub_telemetry=false\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Command to install the necessary dependencies for the MotherDuck project from the requirements file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/motherduck.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Verifying pipeline execution results\nDESCRIPTION: Command to display information about a pipeline's execution, including loaded tables and row counts. Replace <pipeline_name> with your actual pipeline name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/freshdesk.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Creating Empty dlt.yml File in Shell\nDESCRIPTION: Shell command to create an empty dlt.yml configuration file for a new dlt+ project.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/ai.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ntouch dlt.yml\n```\n\n----------------------------------------\n\nTITLE: Project Directory Structure\nDESCRIPTION: The required directory structure for implementing the URL parser enrichment pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/url-parser-data-enrichment.md#2025-04-14_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nurl_parser_enrichment/\nâ”œâ”€â”€ .dlt/\nâ”‚   â””â”€â”€ secrets.toml\nâ””â”€â”€ url_enrichment_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Installing DLT with DuckDB Dependencies\nDESCRIPTION: Command to install the DLT library with DuckDB support\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/duckdb.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[duckdb]\"\n```\n\n----------------------------------------\n\nTITLE: Initializing DLT Pipeline with Source and Destination\nDESCRIPTION: Command syntax for initializing a new DLT pipeline. The command clones the repository using the DLT version as a git tag and sets up the pipeline with specified source and destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/tests/cli/cases/deploy_pipeline/README.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndlt init <source> <destination>\n```\n\n----------------------------------------\n\nTITLE: Initializing DLT Pipeline Command\nDESCRIPTION: Shell command to initialize a new DLT pipeline for Jira with DuckDB as destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/jira.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init jira duckdb\n```\n\n----------------------------------------\n\nTITLE: Installing dlt with Qdrant Support\nDESCRIPTION: Command to install dlt with Qdrant destination support using pip.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/qdrant.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[qdrant]\"\n```\n\n----------------------------------------\n\nTITLE: Running the Slack Pipeline\nDESCRIPTION: Command to execute the Slack pipeline script, which will fetch data from Slack and load it into the configured destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/slack.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython slack_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Initializing HubSpot Pipeline with dlt CLI\nDESCRIPTION: Command to initialize a new dlt pipeline for HubSpot with DuckDB as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/hubspot.md#2025-04-14_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndlt init hubspot duckdb\n```\n\n----------------------------------------\n\nTITLE: Committing Pipeline Files\nDESCRIPTION: Git commands to add and commit pipeline files to the repository\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-github-actions.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ngit add . && git commit -m 'pipeline deployed with github action'\n```\n\n----------------------------------------\n\nTITLE: Generating SSH Key Pair Without Passphrase\nDESCRIPTION: Creates an RSA key pair for user 'foo' without passphrase protection and sets appropriate permissions on the private key. Uses 4096-bit key length for enhanced security.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/tests/load/filesystem_sftp/bootstrap/SETUP.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Generate the key pair\nssh-keygen -t rsa -b 4096 -C \"foo@example.com\" -f foo_rsa\n\n# Secure the private key\nchmod 600 foo_rsa\n```\n\n----------------------------------------\n\nTITLE: Installing Iceberg Dependencies for Python\nDESCRIPTION: Install the necessary Python packages for using the Iceberg destination in dlt.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/iceberg.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install pyiceberg\npip install sqlalchemy>=2.0.18\n```\n\n----------------------------------------\n\nTITLE: Checking Specific DLT Documentation Files\nDESCRIPTION: Command to run all stages of snippet checking, but only for files containing 'walkthrough' in the filepath. Useful when working on a specific documentation page.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/tools/README.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython check_embedded_snippets.py full -f walkthrough\n```\n\n----------------------------------------\n\nTITLE: Running Matomo Pipeline\nDESCRIPTION: Command to execute the Matomo data pipeline\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/matomo.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npython matomo_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Project Configuration with UV Dependencies\nDESCRIPTION: Complete pyproject.toml configuration including dependencies and UV-specific settings for dlt+.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/quality/tests.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[project]\nname = \"dlt-portable-data-lake-demo\"\n\ndependencies = [\n    \"dlt[duckdb,parquet,deltalake,filesystem,snowflake]>=1.4.1a0\",\n    \"dlt-plus>=0.2.6\",\n    \"enlighten\",\n    \"duckdb<=1.1.2\"\n]\n\n[tool.uv]\ndev-dependencies = [\n    \"dlt-plus-tests>=0.1.2\",\n]\n\n[[tool.uv.index]]\nname = \"dlt-hub\"\nurl = \"https://pypi.dlthub.com\"\nexplicit=true\n\n[tool.uv.sources]\ndlt-plus = { index = \"dlt-hub\" }\ndlt-plus-tests = { index = \"dlt-hub\" }\n```\n\n----------------------------------------\n\nTITLE: Installing dlt with pip\nDESCRIPTION: Basic installation command for the dlt Python library using pip package manager\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/intro.md#2025-04-14_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dlt\n```\n\n----------------------------------------\n\nTITLE: Verifying DLT Installation via Command Line\nDESCRIPTION: Command to check the installed version of DLT to verify that it's properly installed before proceeding with the REST API tutorial.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/rest-api.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt --version\n```\n\n----------------------------------------\n\nTITLE: Running the REST API Pipeline\nDESCRIPTION: Command to execute the REST API pipeline script which will extract data from the configured API and load it into the DuckDB destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/rest-api.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython rest_api_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Running the Freshdesk pipeline\nDESCRIPTION: Command to execute the Freshdesk pipeline Python script that will extract data from Freshdesk and load it into the configured destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/freshdesk.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython freshdesk_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Verbose Full Check of DLT Documentation Snippets\nDESCRIPTION: Command to run all checks on embedded snippets with increased verbosity in the output.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/tools/README.md#2025-04-14_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\npython check_embedded_snippets.py full -v\n```\n\n----------------------------------------\n\nTITLE: Generating dlt Transformation Template\nDESCRIPTION: Command to generate a new transformation template in the ./transformations folder. Creates transformation functions, staging view, and main output table structure.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/transformations/python-transformations.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt transformation <transformation-name> render-t-layer\n```\n\n----------------------------------------\n\nTITLE: Installing DLT with Synapse Dependencies\nDESCRIPTION: Command to install dlt library with Synapse support via pip\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/synapse.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[synapse]\"\n```\n\n----------------------------------------\n\nTITLE: Setting the INSERT File Format in dlt-hub\nDESCRIPTION: A component import that references a file explaining how to set the file format to INSERT VALUES format.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/file-formats/insert-format.md#2025-04-14_snippet_0\n\nLANGUAGE: mdx\nCODE:\n```\nimport SetTheFormat from './_set_the_format.mdx';\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Development Server\nDESCRIPTION: Command to start the Dagster development server locally.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-dagster.md#2025-04-14_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Fixing Grammar in a Single DLT Documentation File\nDESCRIPTION: Command to run grammar correction using GPT on a specific DLT documentation file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/tools/README.md#2025-04-14_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\npython fix_grammar_gpt.py -f ../website/docs/intro.md\n```\n\n----------------------------------------\n\nTITLE: Using dlt Init with a Custom Repository\nDESCRIPTION: Initializes a dlt project using a custom repository instead of the default one.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add-a-verified-source.md#2025-04-14_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\ndlt init pipedrive bigquery --location \"https://github.com/dlt-hub/verified-sources\"\n```\n\n----------------------------------------\n\nTITLE: Installing dlt with Redshift Dependencies\nDESCRIPTION: Command to install the dlt library with Redshift dependencies using pip.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/redshift.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[redshift]\"\n```\n\n----------------------------------------\n\nTITLE: Running the Workable Pipeline\nDESCRIPTION: Command to execute the Workable data pipeline script.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/workable.md#2025-04-14_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython workable_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Generating SSH Key Pair With Passphrase\nDESCRIPTION: Creates an RSA key pair for user 'bobby' with passphrase protection and sets appropriate permissions on the private key. Uses 4096-bit key length for enhanced security.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/tests/load/filesystem_sftp/bootstrap/SETUP.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Generate the key pair with a passphrase\nssh-keygen -t rsa -b 4096 -C \"bobby@example.com\" -f bobby_rsa\n\n# Secure the private key\nchmod 600 bobby_rsa\n```\n\n----------------------------------------\n\nTITLE: Dataset Row Count Query\nDESCRIPTION: CLI command to count rows in a dataset\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ndlt dataset my_pipeline_dataset row-counts\n```\n\n----------------------------------------\n\nTITLE: Running Google Analytics Pipeline Script\nDESCRIPTION: Command to execute the Google Analytics pipeline Python script.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_analytics.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npython google_analytics_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Initializing Google Analytics Pipeline with DuckDB\nDESCRIPTION: Command to initialize a new dlt pipeline using Google Analytics as the source and DuckDB as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_analytics.md#2025-04-14_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ndlt init google_analytics duckdb\n```\n\n----------------------------------------\n\nTITLE: DBT Logging Configuration\nDESCRIPTION: TOML configuration for setting logging level in the dbt project.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/transformations/dbt-transformations.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[runtime]\nlog_level=\"INFO\"\n```\n\n----------------------------------------\n\nTITLE: Inserting DocCardList Component in Markdown\nDESCRIPTION: This code snippet inserts the DocCardList component into the markdown document, which will render a list of related documentation cards when the page is built.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/index.md#2025-04-14_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies (Shell)\nDESCRIPTION: Installs required Python dependencies for the project.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/create-a-pipeline.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Creating Loader User Account\nDESCRIPTION: SQL command to create a new user named 'loader' with a specified password\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/dlt/destinations/impl/mssql/README.md#2025-04-14_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER loader WITH PASSWORD = 'loader'\n```\n\n----------------------------------------\n\nTITLE: Specifying S3 Dependencies for DLT\nDESCRIPTION: Lists the S3 plugin dependencies required for DLT (Data Loading Tool). This simple requirement specification enables S3 functionality within DLT.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/examples/partial_loading/requirements.txt#2025-04-14_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndlt[s3]\n```\n\n----------------------------------------\n\nTITLE: Default Value Injection in Slack Source\nDESCRIPTION: Example demonstrating how arguments with default values are handled in the injection process.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/advanced.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef slack_source(\n  page_size: int = MAX_PAGE_SIZE,\n  access_token: str = dlt.secrets.value,\n  start_date: Optional[TAnyDateTime] = START_DATE\n):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Installing PyArrow Backend Dependencies\nDESCRIPTION: Command to install required dependencies for using the PyArrow backend with DLT SQL database source\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/configuration.md#2025-04-14_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npip install dlt[sql_database] pyarrow numpy pandas\n```\n\n----------------------------------------\n\nTITLE: Installing and Running OpenAPI Generator\nDESCRIPTION: Shell commands to install and run the OpenAPI generator with a Pokemon API example.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/openapi-generator.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt-init-openapi pokemon --url https://raw.githubusercontent.com/dlt-hub/dlt-init-openapi/devel/tests/cases/e2e_specs/pokeapi.yml --global-limit 2\n```\n\nLANGUAGE: sh\nCODE:\n```\ndlt-init-openapi pokemon --path ./my_specs/pokeapi.yml\n```\n\n----------------------------------------\n\nTITLE: Generating Certificate Authority Key Pair\nDESCRIPTION: Creates a self-signed Certificate Authority (CA) key pair for signing user certificates. Uses 4096-bit RSA key with no passphrase for automated signing.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/tests/load/filesystem_sftp/bootstrap/SETUP.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Generate a self-signed CA key pair\nssh-keygen -t rsa -b 4096 -f ca_rsa -N \"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Database for DLT in PostgreSQL\nDESCRIPTION: SQL command to create a new database named 'dlt_data' for storing DLT-related data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/dlt/destinations/impl/postgres/README.md#2025-04-14_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE DATABASE dlt_data\n```\n\n----------------------------------------\n\nTITLE: Installing Python 3.10 and uv on macOS\nDESCRIPTION: Commands to install Python 3.10 and the uv package manager on macOS using Homebrew.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/installation.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nbrew update\nbrew install python@3.10\npip install uv\n```\n\n----------------------------------------\n\nTITLE: Running dlt+ Pipeline in Shell\nDESCRIPTION: Shell command to execute the 'pokemon' pipeline in the dlt+ project using the dlt CLI.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/ai.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline pokemon run\n```\n\n----------------------------------------\n\nTITLE: Stopping Weaviate Container with Docker Compose\nDESCRIPTION: Command to stop and remove the Weaviate container, including associated volumes and orphaned containers.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/dlt/destinations/impl/weaviate/README.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose -f .github/weaviate-compose.yml down -v --remove-orphans\n```\n\n----------------------------------------\n\nTITLE: Running HubSpot Pipeline in Python\nDESCRIPTION: Command to execute the HubSpot pipeline script after setting up the credentials and dependencies.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/hubspot.md#2025-04-14_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython hubspot_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies\nDESCRIPTION: This snippet lists required Python packages and their specific versions. It includes pydoc-markdown, typing-extensions, and two databind packages. Each line specifies a package name followed by the exact version number.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/requirements.txt#2025-04-14_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npydoc-markdown==4.8.2\ntyping-extensions==4.6.3\ndatabind.json==4.4.2\ndatabind.core==4.4.2\n```\n\n----------------------------------------\n\nTITLE: Configuring SerpAPI Credentials\nDESCRIPTION: TOML configuration file for storing the SerpAPI authentication key securely.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/user_agent_device_data_enrichment.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[sources]\napi_key= \"Please set me up!\"  # Serp Api key.\n```\n\n----------------------------------------\n\nTITLE: Installing Python on macOS\nDESCRIPTION: Commands to install Python 3.10 and uv package manager using Homebrew on macOS.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/installation.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nbrew update\nbrew install python@3.10\npip install uv\n```\n\n----------------------------------------\n\nTITLE: Installing Pipeline Dependencies with pip\nDESCRIPTION: Command to install required Python dependencies for the Matomo pipeline\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/matomo.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running Pipeline\nDESCRIPTION: CLI command to execute a defined pipeline\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline my_pipeline run\n```\n\n----------------------------------------\n\nTITLE: dlt Project Directory Structure\nDESCRIPTION: Shows the file structure created after initializing a dlt project with Pipedrive and BigQuery.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add-a-verified-source.md#2025-04-14_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nâ”œâ”€â”€ .dlt\nâ”‚   â”œâ”€â”€ config.toml\nâ”‚   â””â”€â”€ secrets.toml\nâ”œâ”€â”€ pipedrive\nâ”‚   â””â”€â”€ helpers\nâ”‚   â””â”€â”€ __init__.py\nâ”‚   â””â”€â”€ settings.py\nâ”‚   â””â”€â”€ typing.py\nâ”œâ”€â”€ .gitignore\nâ”œâ”€â”€ pipedrive_pipeline.py\nâ””â”€â”€ requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Version Constraints\nDESCRIPTION: This snippet lists Python package dependencies with pinned versions. It requires python-dotenv 1.0.1 for environment variable management and openai 1.14.2 for interacting with OpenAI APIs.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/tools/requirements.txt#2025-04-14_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npython-dotenv==1.0.1\nopenai==1.14.2\n```\n\n----------------------------------------\n\nTITLE: MongoDB Shell Commands\nDESCRIPTION: Series of shell commands for connecting to MongoDB and listing databases and collections.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mongodb.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nmongo \"mongodb://dbuser:passwd@your_host:27017\"\n```\n\nLANGUAGE: sh\nCODE:\n```\nshow dbs\n```\n\nLANGUAGE: sh\nCODE:\n```\nuse your_database_name\n```\n\nLANGUAGE: sh\nCODE:\n```\nshow collections\n```\n\nLANGUAGE: sh\nCODE:\n```\nexit\n```\n\n----------------------------------------\n\nTITLE: Installing dlt pipeline dependencies\nDESCRIPTION: Command to install all the required dependencies for the Freshdesk pipeline from the requirements.txt file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/freshdesk.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Downloading Kestra Docker Compose File with curl\nDESCRIPTION: Command to download the Docker Compose configuration file for Kestra from the official GitHub repository. This file is essential for setting up the Kestra environment using Docker.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-with-kestra.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncurl -o docker-compose.yml \\\nhttps://raw.githubusercontent.com/kestra-io/kestra/develop/docker-compose.yml\n```\n\n----------------------------------------\n\nTITLE: Installing pip and uv on Windows\nDESCRIPTION: Commands to update pip and install the uv package manager on Windows after installing Python 3.10.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/installation.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nC:\\> pip3 install -U pip\nC:\\> pip3 install uv\n```\n\n----------------------------------------\n\nTITLE: Setting Database Owner Permissions\nDESCRIPTION: SQL command to assign database ownership to the loader user\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/dlt/destinations/impl/mssql/README.md#2025-04-14_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER DATABASE dlt_data OWNER TO loader\n```\n\n----------------------------------------\n\nTITLE: Verifying Pipeline Execution\nDESCRIPTION: Command to display the status and results of the executed pipeline using dlt CLI.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_analytics.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Installing DLT with Dremio Dependencies\nDESCRIPTION: Command to install DLT library with Dremio and S3 dependencies using pip.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/dremio.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[dremio,s3]\"\n```\n\n----------------------------------------\n\nTITLE: Sample Airtable URL Structure\nDESCRIPTION: Example URL showing the structure of Airtable identifiers for base, table, view, and record.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/airtable.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nhttps://airtable.com/appve10kl227BIT4GV/tblOUnZVLFWbemTP1/viw3qtF76bRQC3wKx/rec9khXgeTotgCQ62?blocks=hide\n```\n\n----------------------------------------\n\nTITLE: Viewing Pipeline Results with dlt CLI\nDESCRIPTION: Command to show the pipeline execution results and verify that data was loaded correctly using the dlt command-line interface.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/asana.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Defining DLT Plugin Requirements in Markdown\nDESCRIPTION: Lists the key requirements for creating a DLT plugin, including naming conventions, module export, and use of pluggy hookspecs.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/tests/plugins/dlt_example_plugin/README.md#2025-04-14_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Example DLT Plugin\n1. Plugin name must start with dlt- to be recognized at run time\n2. Export the module that registers plugin in an entry point\n3. Use pluggy hookspecs thst you can find here and there in the dlt\n```\n\n----------------------------------------\n\nTITLE: Creating and Running DLT Pipeline in Python\nDESCRIPTION: Pipeline setup and execution code that combines the resource and currency conversion function\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/currency_conversion_data_enrichment.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"data_enrichment_two\",\n    destination=\"duckdb\",\n    dataset_name=\"currency_conversion_enrichment\",\n)\n\nload_info = pipeline.run(enriched_data_part_two.add_map(converted_amount))\n\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Viewing Pipeline Data (Shell)\nDESCRIPTION: Command to explore the loaded data using DLT's built-in Streamlit viewer.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/create-a-pipeline.md#2025-04-14_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline github_api_pipeline show\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Asana Pipeline\nDESCRIPTION: Command to install all necessary dependencies for the Asana pipeline from the requirements.txt file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/asana.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Creating User for DLT in PostgreSQL\nDESCRIPTION: SQL command to create a new user named 'loader' with a password for accessing the DLT database.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/dlt/destinations/impl/postgres/README.md#2025-04-14_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE USER loader WITH PASSWORD 'loader';\n```\n\n----------------------------------------\n\nTITLE: Listing Available dlt Sources\nDESCRIPTION: Lists all available verified sources with their names and descriptions.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add-a-verified-source.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init --list-sources\n```\n\n----------------------------------------\n\nTITLE: Installing DLT Package\nDESCRIPTION: Command to install DLT without additional dependencies using pip.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/destination.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install dlt\n```\n\n----------------------------------------\n\nTITLE: Specifying Required DLT Redshift Package Version\nDESCRIPTION: Defines the minimum required version for the dlt Redshift package as 0.2.5a1 or higher. This is typically used in requirements.txt or dependency specifications for Python projects.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/tests/cli/cases/deploy_pipeline/requirements.txt#2025-04-14_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndlt[redshift] >= 0.2.5a1\n```\n\n----------------------------------------\n\nTITLE: Creating Database for DLT Data\nDESCRIPTION: SQL command to create a new database named dlt_data\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/dlt/destinations/impl/mssql/README.md#2025-04-14_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE DATABASE dlt_data\n```\n\n----------------------------------------\n\nTITLE: Installing Python 3.10 and uv on Ubuntu\nDESCRIPTION: Commands to install Python 3.10 and the uv package manager on Ubuntu using apt.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/installation.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt update\nsudo apt install python3.10\npip install uv\n```\n\n----------------------------------------\n\nTITLE: Checking Python and pip versions\nDESCRIPTION: Commands to verify the installed versions of Python and pip, which are prerequisites for installing dlt.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/installation.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npython --version\npip --version\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies\nDESCRIPTION: A list of required Python packages including pyarrow, cffi, idna, simplejson, pendulum, grpcio, and google-crc32c, separated by pipe characters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/compiled_packages.txt#2025-04-14_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npyarrow\\|cffi\\|idna\\|simplejson\\|pendulum\\|grpcio\\|google-crc32c\n```\n\n----------------------------------------\n\nTITLE: Installing Pipeline Dependencies\nDESCRIPTION: Command to install the required dependencies for the Notion pipeline using pip and the requirements.txt file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/notion.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Updating Poetry Lock File\nDESCRIPTION: Command to update the Poetry lock file after adding new dependencies without updating existing ones.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/examples/CONTRIBUTING.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry lock --no-update\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies\nDESCRIPTION: Command to install required Python dependencies for the pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/airtable.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Creating Directory Structure\nDESCRIPTION: Basic directory structure setup for the device enrichment pipeline project.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/user_agent_device_data_enrichment.md#2025-04-14_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nuser_device_enrichment/\nâ”œâ”€â”€ .dlt/\nâ”‚   â””â”€â”€ secrets.toml\nâ””â”€â”€ device_enrichment_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Fixing Grammar in Specific DLT Documentation Files\nDESCRIPTION: Command to run grammar correction using GPT on DLT documentation files containing 'walkthrough' in the filepath.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/tools/README.md#2025-04-14_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\npython fix_grammar_gpt.py -f walkthrough\n```\n\n----------------------------------------\n\nTITLE: Project Configuration Display\nDESCRIPTION: CLI command to show current project configuration with profile\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\ndlt project --profile dev config show\n```\n\n----------------------------------------\n\nTITLE: DuckDB Naming Convention Configuration\nDESCRIPTION: Examples of configuring naming conventions for DuckDB using different methods\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/duckdb.md#2025-04-14_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[schema]\nnaming=\"duck_case\"\n```\n\nLANGUAGE: python\nCODE:\n```\ndlt.config[\"schema.naming\"] = \"duck_case\"\n```\n\n----------------------------------------\n\nTITLE: Installing Ibis Framework with DuckDB Backend\nDESCRIPTION: Command to install the Ibis framework package with the DuckDB backend using pip. This is a prerequisite for using Ibis with DLT and DuckDB.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/ibis-backend.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install ibis-framework[duckdb]\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Command to install required dependencies from requirements.txt file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/dremio.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Generating and Signing User Certificate with CA\nDESCRIPTION: Creates an RSA key pair for user 'billy' and signs the public key with the CA to generate a certificate. Uses 4096-bit key length and includes user identification in the certificate.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/tests/load/filesystem_sftp/bootstrap/SETUP.md#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Generate the user key pair for billy\nssh-keygen -t rsa -b 4096 -C \"billy@example.com\" -f billy_rsa\n\n# Sign billy's public key with the CA\nssh-keygen -s ca_rsa -I billy-cert -n billy billy_rsa.pub\n```\n\n----------------------------------------\n\nTITLE: Installing Shopify Pipeline Dependencies\nDESCRIPTION: Shell command to install required dependencies for the Shopify pipeline from requirements.txt file\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/shopify.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing dlt with Snowflake Dependencies\nDESCRIPTION: Command to install the dlt library with Snowflake dependencies using pip.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[snowflake]\"\n```\n\n----------------------------------------\n\nTITLE: Installing dlt with MotherDuck Dependencies\nDESCRIPTION: Command to install the dlt library with MotherDuck dependencies using pip.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/motherduck.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[motherduck]\"\n```\n\n----------------------------------------\n\nTITLE: Listing Available Sources\nDESCRIPTION: CLI command to display all available DLT sources\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndlt source list-available\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Directory Structure\nDESCRIPTION: Shows the required directory structure for the currency conversion enrichment project\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/data-enrichments/currency_conversion_data_enrichment.md#2025-04-14_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ncurrency_conversion_enrichment/\nâ”œâ”€â”€ .dlt/\nâ”‚   â””â”€â”€ secrets.toml\nâ””â”€â”€ currency_enrichment_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies with pip\nDESCRIPTION: Command to install required Python dependencies for the pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/github.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Initializing Matomo Pipeline\nDESCRIPTION: Command to initialize a new Matomo pipeline with DuckDB destination\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/matomo.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init matomo duckdb\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Poetry Configuration\nDESCRIPTION: Instructions for adding new dependencies to the pyproject.toml file in the docs group for Poetry package management.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/examples/CONTRIBUTING.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[tool.poetry.group.docs.dependencies]\n```\n\n----------------------------------------\n\nTITLE: Installing dlt+ in Virtual Environment\nDESCRIPTION: Command to install or upgrade dlt+ package in a virtual environment using uv.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/installation.md#2025-04-14_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nuv pip install -U dlt-plus\n```\n\n----------------------------------------\n\nTITLE: Showing Pipeline Status\nDESCRIPTION: Command to display the status of a specific pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/kafka.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Installing Latest LTS Version of Node.js\nDESCRIPTION: This command installs and uses the latest stable version of Node.js using nvm (Node Version Manager).\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/README.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnvm install --lts\n```\n\n----------------------------------------\n\nTITLE: Installing DLT with ClickHouse Dependencies\nDESCRIPTION: Command to install DLT library with ClickHouse dependencies using pip\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/clickhouse.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[clickhouse]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Workable Subdomain in TOML\nDESCRIPTION: TOML configuration for specifying the Workable subdomain.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/workable.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[sources.workable]\nsubdomain = \"subdomain\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Show Pipeline Status\nDESCRIPTION: Command to display the status of a specific pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/github.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Updating Docusaurus Documentation Versions\nDESCRIPTION: This command executes a script to update and manage different versions of the documentation, including creating versions for major releases and a 'devel' version for future updates.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/README.md#2025-04-14_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnpm run update-versions\n```\n\n----------------------------------------\n\nTITLE: Configuring Presto Database Connection Settings\nDESCRIPTION: TOML configuration block specifying database connection parameters for Presto destination including host, port, credentials, and database name\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/create-new-destination.md#2025-04-14_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[destination.presto.credentials]\ndatabase = \"dlt_data\"\npassword = \"loader\"\nusername = \"loader\"\nhost = \"localhost\"\nport = 5432\n```\n\n----------------------------------------\n\nTITLE: SFTP Credential Parameters\nDESCRIPTION: List of available SFTP configuration parameters and their descriptions.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_23\n\nLANGUAGE: text\nCODE:\n```\nsftp_port             # The port for SFTP, defaults to 22 (standard for SSH/SFTP)\nsftp_username         # Your SFTP username, defaults to None\nsftp_password         # Your SFTP password (if using password-based auth), defaults to None\nsftp_key_filename     # Path to your private key file for key-based authentication, defaults to None\nsftp_key_passphrase   # Passphrase for your private key (if applicable), defaults to None\nsftp_timeout          # Timeout for establishing a connection, defaults to None\nsftp_banner_timeout   # Timeout for receiving the banner during authentication, defaults to None\nsftp_auth_timeout     # Authentication timeout, defaults to None\nsftp_channel_timeout  # Channel timeout for SFTP operations, defaults to None\nsftp_allow_agent      # Use SSH agent for key management (if available), defaults to True\nsftp_look_for_keys    # Search for SSH keys in the default SSH directory (~/.ssh/), defaults to True\nsftp_compress         # Enable compression (can improve performance over slow networks), defaults to False\nsftp_gss_auth         # Use GSS-API for authentication, defaults to False\nsftp_gss_kex          # Use GSS-API for key exchange, defaults to False\nsftp_gss_deleg_creds  # Delegate credentials with GSS-API, defaults to True\nsftp_gss_host         # Host for GSS-API, defaults to None\nsftp_gss_trust_dns    # Trust DNS for GSS-API, defaults to True\n```\n\n----------------------------------------\n\nTITLE: Installing dlt+ Package with pip\nDESCRIPTION: Basic installation command for the dlt+ package using pip package manager.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/installation.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install dlt-plus\n```\n\n----------------------------------------\n\nTITLE: Creating Directory for dlt Project\nDESCRIPTION: Creates a new empty directory for your dlt project and navigates into it.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add-a-verified-source.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nmkdir various_pipelines\ncd various_pipelines\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Dependencies for Docusaurus Website\nDESCRIPTION: This command installs the Node.js package dependencies defined in package.json for the Docusaurus website.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/README.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ npm install\n```\n\n----------------------------------------\n\nTITLE: Running Pipeline Installation Commands\nDESCRIPTION: Shell commands for installing dependencies and running the Mux pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mux.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\nLANGUAGE: sh\nCODE:\n```\npython mux_pipeline.py\n```\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: Initializing dlt Project with Athena Destination\nDESCRIPTION: Command to create a new dlt project that uses chess as the source and AWS Athena as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/athena.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init chess athena\n```\n\n----------------------------------------\n\nTITLE: Azure Blob Storage Configuration with Container Path\nDESCRIPTION: Configuration for Azure Blob Storage using the az:// URL scheme with container name and path.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_11\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\nbucket_url = \"az://<container_name>/path\" # replace with your container name and path\n```\n\n----------------------------------------\n\nTITLE: Viewing dlt Init Help Documentation\nDESCRIPTION: Displays help information for the dlt init command.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add-a-verified-source.md#2025-04-14_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\ndlt init --help\n```\n\n----------------------------------------\n\nTITLE: Running Linting on DLT Documentation Snippets\nDESCRIPTION: Command to run only the linting stage on embedded code snippets in the DLT documentation.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/tools/README.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython check_embedded_snippets.py lint\n```\n\n----------------------------------------\n\nTITLE: Parsing Specific Snippets in DLT Documentation\nDESCRIPTION: Command to run the parsing stage only on specific snippets (49, 345, and 789) in the DLT documentation.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/tools/README.md#2025-04-14_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npython check_embedded_snippets.py parse -s 49,345,789\n```\n\n----------------------------------------\n\nTITLE: Displaying Help for DLT Embedded Snippet Checker\nDESCRIPTION: Command to show all available subcommands and arguments for the check_embedded_snippets.py script.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/tools/README.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npython check_embedded_snippets.py --help\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Commands to install required dependencies for an Athena destination project using either requirements.txt or individual package installation.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/athena.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\nLANGUAGE: sh\nCODE:\n```\npip install dlt\npip install s3fs\npip install pyarrow\npip install pyathena\n```\n\n----------------------------------------\n\nTITLE: Configuring Additional S3 Permissions for Iceberg\nDESCRIPTION: JSON configuration for additional IAM permissions required when using Iceberg with S3.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/delta-iceberg.md#2025-04-14_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\n  \"s3:ListBucketMultipartUploads\",\n  \"s3:GetBucketLocation\",\n  \"s3:AbortMultipartUpload\",\n  \"s3:PutObjectTagging\",\n  \"s3:GetObjectTagging\"\n]\n```\n\n----------------------------------------\n\nTITLE: Setting up Documentation Page Header with Frontmatter in Markdown\nDESCRIPTION: Defines the page metadata using YAML frontmatter in Markdown, including title, description, and keywords for the Destinations documentation page.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/index.md#2025-04-14_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Destinations\ndescription: List of destinations\nkeywords: ['destinations']\n---\n```\n\n----------------------------------------\n\nTITLE: Installing dlt+ Project Package\nDESCRIPTION: Shell command to install the dlt+ project package from a private PyPI repository.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/data-access.md#2025-04-14_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install -U --index-url https://pypi.dlthub.com dlt_example_project\n```\n\n----------------------------------------\n\nTITLE: Verifying Python C Library Support\nDESCRIPTION: Command to check if Python is installed with C library support, which is necessary for Streamlit to function properly.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/streamlit.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npython -c \"import _ctypes\"\n```\n\n----------------------------------------\n\nTITLE: Initializing a dlt project with MS SQL destination\nDESCRIPTION: Command to create a new dlt project that uses Microsoft SQL Server as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/mssql.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init chess mssql\n```\n\n----------------------------------------\n\nTITLE: Configuring Filesystem Destination Layout in TOML\nDESCRIPTION: Example TOML configuration for customizing the filesystem destination layout pattern, including the current default naming scheme and alternative examples with timestamps and custom placeholders.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_27\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\nlayout=\"{table_name}/{load_id}.{file_id}.{ext}\" # current preconfigured naming scheme\n\n# More examples\n# With timestamp\n# layout = \"{table_name}/{timestamp}/{load_id}.{file_id}.{ext}\"\n\n# With timestamp of the load package\n# layout = \"{table_name}/{load_package_timestamp}/{load_id}.{file_id}.{ext}\"\n\n# Parquet-like layout (note: it is not compatible with the internal datetime of the parquet file)\n# layout = \"{table_name}/year={YYYY}/month={MM}/day={DD}/{load_id}.{file_id}.{ext}\"\n\n# Custom placeholders\n# extra_placeholders = { \"owner\" = \"admin\", \"department\" = \"finance\" }\n# layout = \"{table_name}/{owner}/{department}/{load_id}.{file_id}.{ext}\"\n```\n\n----------------------------------------\n\nTITLE: Installing pip on Windows\nDESCRIPTION: Commands to upgrade pip and install uv package manager on Windows.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/installation.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nC:\\> pip3 install -U pip\nC:\\> pip3 install uv\n```\n\n----------------------------------------\n\nTITLE: Defining Google Sheets Source Function in Python\nDESCRIPTION: This Python snippet defines the google_spreadsheet source function, which is responsible for loading data from a Google Spreadsheet. It includes parameters for spreadsheet identification, range selection, credentials, and options for importing sheets and named ranges.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source()\ndef google_spreadsheet(\n      spreadsheet_url_or_id: str = dlt.config.value,\n      range_names: Sequence[str] = dlt.config.value,\n      credentials: Union[\n          GcpOAuthCredentials, GcpServiceAccountCredentials\n      ] = dlt.secrets.value,\n      get_sheets: bool = False,\n      get_named_ranges: bool = True,\n) -> Iterable[DltResource]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Full Load (Replace) Strategy in SQL with dlt in Python\nDESCRIPTION: Demonstrates how to implement a full load strategy that completely overwrites existing data in a destination table with the latest dataset from the source. This approach uses write_disposition=\"replace\" to refresh the entire table.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add-incremental-configuration.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef load_full_table_resource() -> None:\n    \"\"\"Load a full table, replacing existing data.\"\"\"\n    pipeline = dlt.pipeline(\n        pipeline_name=\"mysql_database\",\n        destination='bigquery',\n        dataset_name=\"dlt_contacts\"\n    )\n\n    # Load the full table \"contact\"\n    source = sql_database().with_resources(\"contact\")\n\n    # Run the pipeline\n    info = pipeline.run(source, write_disposition=\"replace\")\n\n    # Print the info\n    print(info)\n\nload_full_table_resource()\n```\n\n----------------------------------------\n\nTITLE: Column Reordering with Python Map\nDESCRIPTION: Example of reordering columns using add_map function in Python.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/adjust-a-schema.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmy_resource = resource().add_map(lambda row: {\n    'column3': row['column3'],\n    'column1': row['column1'],\n    'column2': row['column2']\n})\n\nload_info = pipeline.run(my_resource)\n```\n\n----------------------------------------\n\nTITLE: Example dlt Init Telemetry Message\nDESCRIPTION: JSON example of a telemetry message sent when running the dlt init command, showing the structure and type of anonymous data collected.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/telemetry.md#2025-04-14_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"anonymousId\": \"933dd165453d196a58adaf49444e9b4c\",\n  \"context\": {\n    \"ci_run\": false,\n    \"cpu\": 8,\n    \"exec_info\": [],\n    \"library\": {\n      \"name\": \"dlt\",\n      \"version\": \"0.2.0a25\"\n    },\n    \"os\": {\n      \"name\": \"Linux\",\n      \"version\": \"4.19.128-microsoft-standard\"\n    },\n    \"python\": \"3.8.11\"\n  },\n  \"event\": \"command_init\",\n  \"properties\": {\n    \"destination_name\": \"bigquery\",\n    \"elapsed\": 3.1720383167266846,\n    \"event_category\": \"command\",\n    \"event_name\": \"init\",\n    \"pipeline_name\": \"pipedrive\",\n    \"success\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Incremental Loading with Arrow Tables\nDESCRIPTION: Example of implementing incremental loading using Arrow tables with DLT. Creates a resource that yields dataframes and uses the ordered_at field as an incremental cursor.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/arrow-pandas.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.common import pendulum\nimport pandas as pd\n\n# Create a resource that yields a dataframe, using the `ordered_at` field as an incremental cursor\n@dlt.resource(primary_key=\"order_id\")\ndef orders(ordered_at = dlt.sources.incremental('ordered_at')):\n    # Get a dataframe/arrow table from somewhere\n    # If your database supports it, you can use the last_value to filter data at the source.\n    # Otherwise, it will be filtered automatically after loading the data.\n    df = _get_orders(since=ordered_at.last_value)\n    yield df\n\npipeline = dlt.pipeline(\"orders_pipeline\", destination=\"snowflake\")\npipeline.run(orders)\n# Run again to load only new data\npipeline.run(orders)\n```\n\n----------------------------------------\n\nTITLE: Multi-level Schema Contract Configuration in DLT\nDESCRIPTION: Demonstrates hierarchical schema contract configuration at resource, source, and pipeline run levels. This example shows how settings can be inherited and overridden at different levels of the data pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema-contracts.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(schema_contract={\"columns\": \"evolve\"})\ndef items():\n    ...\n\n@dlt.resource()\ndef other_items():\n    ...\n\n@dlt.source(schema_contract={\"columns\": \"freeze\", \"data_type\": \"freeze\"})\ndef frozen_source():\n  return [items(), other_items()]\n\n\n# this will use the settings defined by the decorators\npipeline.run(frozen_source())\n\n# this will freeze the whole schema, regardless of the decorator settings\npipeline.run(frozen_source(), schema_contract=\"freeze\")\n```\n\n----------------------------------------\n\nTITLE: Incremental Loading by ID in SQL with dlt in Python\nDESCRIPTION: Shows how to implement an incremental loading strategy that appends only new records based on an incremental ID. It uses dlt.sources.incremental(\"id\") to track which records are new or have been updated.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/add-incremental-configuration.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef load_incremental_id_table_resource() -> None:\n    \"\"\"Load a table incrementally based on an ID.\"\"\"\n    pipeline = dlt.pipeline(\n        pipeline_name=\"mysql_database\",\n        destination='bigquery',\n        dataset_name=\"dlt_contacts\",\n    )\n\n    # Load table \"contact\" incrementally based on ID\n    source = sql_database().with_resources(\"contact\")\n    source.contact.apply_hints(incremental=dlt.sources.incremental(\"id\"))\n\n    # Run the pipeline with append write disposition\n    info = pipeline.run(source, write_disposition=\"append\")\n\n    # Print the info\n    print(info)\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration Example\nDESCRIPTION: YAML configuration file example for customizing the package name in the OpenAPI generator.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/openapi-generator.md#2025-04-14_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\npackage_name: \"other_package_name\"\n```\n\n----------------------------------------\n\nTITLE: Basic Pipeline Example in Python using dlt\nDESCRIPTION: Demonstrates creating and running a basic dlt pipeline that processes nested data into a DuckDB destination. The example shows how to initialize a pipeline and load structured data with nested arrays.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/explainers/how-dlt-works.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\n\npipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\npipeline.run(\n    [\n        {\"id\": 1},\n        {\"id\": 2},\n        {\"id\": 3, \"nested\": [{\"id\": 1}, {\"id\": 2}]},\n    ],\n    table_name=\"items\",\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Processing Data with Pandas DataFrame\nDESCRIPTION: Shows how to retrieve SQL query results as a Pandas DataFrame and perform data manipulation operations on the retrieved data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/sql-client.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\nwith pipeline.sql_client() as client:\n    with client.execute_query(\n        'SELECT \"reactions__+1\", \"reactions__-1\", reactions__laugh, reactions__hooray, reactions__rocket FROM issues'\n    ) as cursor:\n        # calling `df` on a cursor, returns the data as a pandas DataFrame\n        reactions = cursor.df()\ncounts = reactions.sum(0).sort_values(0, ascending=False)\n```\n\n----------------------------------------\n\nTITLE: Configuring Response Actions for REST API Endpoint in Python\nDESCRIPTION: This snippet demonstrates how to configure response actions for a REST API endpoint. It shows examples of ignoring responses based on status codes or content, and applying custom transformations to responses.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/rest_api/advanced.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"path\": \"issues\",\n    \"response_actions\": [\n        {\"status_code\": 404, \"action\": \"ignore\"},\n        {\"content\": \"Not found\", \"action\": \"ignore\"},\n        {\"status_code\": 200, \"content\": \"some text\", \"action\": \"ignore\"},\n    ],\n}\n```\n\n----------------------------------------\n\nTITLE: Airtable Credentials Configuration\nDESCRIPTION: TOML configuration for storing Airtable access credentials securely.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/airtable.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[sources.airtable]\naccess_token = \"Please set me up!\" # Please set me up!\n```\n\n----------------------------------------\n\nTITLE: Creating Contact Link for dltHub Customer Success Team in HTML\nDESCRIPTION: This HTML code creates a span element containing an anchor tag that links to the dltHub customer success contact page. The link text 'Get in touch' appears as clickable, followed by descriptive text about contacting the customer success team.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/_book-onboarding-call.md#2025-04-14_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<span><a href=\"https://dlthub.com/contact\">Get in touch</a> with the dltHub Customer Success team</span>\n```\n\n----------------------------------------\n\nTITLE: Pipeline Execution Code (Python)\nDESCRIPTION: Main function that configures and runs the pipeline to load data into DuckDB.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/create-a-pipeline.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nif __name__=='__main__':\n    # configure the pipeline with your destination details\n    pipeline = dlt.pipeline(\n        pipeline_name='github_api_pipeline',\n        destination='duckdb',\n        dataset_name='github_api_data'\n    )\n\n    # print credentials by running the resource\n    data = list(github_api_resource())\n\n    # print the data yielded from resource\n    print(data)\n\n    # run the pipeline with your parameters\n    load_info = pipeline.run(github_api_source())\n\n    # pretty print the information on data that was loaded\n    print(load_info)\n```\n\n----------------------------------------\n\nTITLE: Implementing Workspaces Resource for Asana\nDESCRIPTION: Python function decorated with @dlt.resource that fetches workspace data from Asana API using the provided access token and specified fields from settings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/asana.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(write_disposition=\"replace\")\ndef workspaces(\n    access_token: str = dlt.secrets.value,\n    fields: Iterable[str] = WORKSPACE_FIELDS\n) -> Iterable[TDataItem]:\n    yield from get_client(access_token).workspaces.find_all(opt_fields=\",\".join(fields))\n```\n\n----------------------------------------\n\nTITLE: Kafka Consumer Resource Definition\nDESCRIPTION: Python function definition for the Kafka consumer resource that retrieves messages from specified Kafka topics with customizable parameters.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/kafka.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(name=\"kafka_messages\", table_name=lambda msg: msg[\"_kafka\"][\"topic\"], standalone=True)\ndef kafka_consumer(\n    topics: Union[str, List[str]],\n    credentials: Union[KafkaCredentials, Consumer] = dlt.secrets.value,\n    msg_processor: Optional[Callable[[Message], Dict[str, Any]]] = default_msg_processor,\n    batch_size: Optional[int] = 3000,\n    batch_timeout: Optional[int] = 3,\n    start_from: Optional[TAnyDateTime] = None,\n) -> Iterable[TDataItem]:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Pytest Configuration in pyproject.toml\nDESCRIPTION: Example configuration showing pytest.ini options for dlt+ testing setup.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/features/quality/tests.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[tool.pytest.ini_options]\ndlt_tests_project_path=\"...\"\ndlt_tests_project_profile=\"...\"\n```\n\n----------------------------------------\n\nTITLE: Databricks OAuth2 Credentials in Python\nDESCRIPTION: Python code for setting up Databricks credentials through environment variables\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/databricks.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Do not set up the secrets directly in the code!\n# What you can do is reassign env variables.\nos.environ[\"DESTINATIONS__DATABRICKS__CREDENTIALS__SERVER_HOSTNAME\"] = \"MY_DATABRICKS.azuredatabricks.net\"\nos.environ[\"DESTINATIONS__DATABRICKS__CREDENTIALS__HTTP_PATH\"]=\"/sql/1.0/warehouses/12345\"\nos.environ[\"DESTINATIONS__DATABRICKS__CREDENTIALS__CATALOG\"]=\"my_catalog\"\nos.environ[\"DESTINATIONS__DATABRICKS__CREDENTIALS__CLIENT_ID\"]=os.environ.get(\"CLIENT_ID\")\nos.environ[\"DESTINATIONS__DATABRICKS__CREDENTIALS__CLIENT_SECRET\"]=os.environ.get(\"CLIENT_SECRET\")\n```\n\n----------------------------------------\n\nTITLE: Configuring DLT Pipeline in Python\nDESCRIPTION: This snippet shows how to configure a DLT pipeline by specifying the pipeline name, destination, and dataset name. It uses the dlt.pipeline() function to set up the pipeline structure.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/inbox.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"standard_inbox\",  # Use a custom name if desired\n    destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"standard_inbox_data\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Resources with Transformers\nDESCRIPTION: Shows how to add custom resources to an existing source using transformers for data processing.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/source.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom hubspot import hubspot\n\n# source contains `deals` resource\nsource = hubspot()\n\n@dlt.transformer\ndef deal_scores(deal_item):\n    # obtain the score, deal_items contains data yielded by source.deals\n    score = model.predict(featurize(deal_item))\n    yield {\"deal_id\": deal_item, \"score\": score}\n\n# connect the data from `deals` resource into `deal_scores` and add to the source\nsource.resources.add(source.deals | deal_scores)\n# load the data: you'll see the new table `deal_scores` in your destination!\npipeline.run(source)\n```\n\n----------------------------------------\n\nTITLE: Specifying Dependencies for Cloud Function in requirements.txt\nDESCRIPTION: This requirements.txt file specifies the necessary Python packages needed for the Cloud Function to operate correctly. It includes dlt and the BigQuery extension for dlt.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/walkthroughs/deploy-a-pipeline/deploy-gcp-cloud-function-as-webhook.md#2025-04-14_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n# Function dependencies, for example:\n# package>=version\ndlt\ndlt[bigquery]\n```\n\n----------------------------------------\n\nTITLE: Creating Dynamic Resources with Hubspot API Example\nDESCRIPTION: Demonstrates creating multiple resources dynamically using a single generator function to handle different Hubspot API endpoints.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/source.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef hubspot(api_key=dlt.secrets.value):\n\n    endpoints = [\"companies\", \"deals\", \"products\"]\n\n    def get_resource(endpoint):\n        yield requests.get(url + \"/\" + endpoint).json()\n\n    for endpoint in endpoints:\n        # calling get_resource creates a generator,\n        # the actual code of the function will be executed in pipeline.run\n        yield dlt.resource(get_resource(endpoint), name=endpoint)\n```\n\n----------------------------------------\n\nTITLE: Using Replace Write Disposition with Qdrant\nDESCRIPTION: Example of running a pipeline with 'replace' write disposition to completely replace existing data in the Qdrant collection.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/qdrant.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ninfo = pipeline.run(\n    qdrant_adapter(\n        movies,\n        embed=\"title\",\n    ),\n    write_disposition=\"replace\",\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Preferred Data Types in Python\nDESCRIPTION: Python code that updates preferred data types in a schema, mapping column patterns to specific data types using regular expressions, particularly focusing on timestamp-related columns.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema.md#2025-04-14_snippet_10\n\nLANGUAGE: py\nCODE:\n```\nsource = data_source()\nsource.schema.update_preferred_types(\n  {\n    TSimpleRegex(\"re:timestamp\"): \"timestamp\",\n    TSimpleRegex(\"inserted_at\"): \"timestamp\",\n    TSimpleRegex(\"created_at\"): \"timestamp\",\n    TSimpleRegex(\"updated_at\"): \"timestamp\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Shared Google Credentials in Python Code\nDESCRIPTION: This snippet demonstrates how to set shared Google credentials by reassigning environment variables in Python code.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Do not set up the secrets directly in the code!\n# What you can do is reassign env variables.\nos.environ[\"CREDENTIALS__CLIENT_EMAIL\"] = os.environ.get(\"GOOGLE_CLIENT_EMAIL\")\nos.environ[\"CREDENTIALS__PRIVATE_KEY\"] = os.environ.get(\"GOOGLE_PRIVATE_KEY\")\nos.environ[\"CREDENTIALS__PROJECT_ID\"] = os.environ.get(\"GOOGLE_PROJECT_ID\")\n```\n\n----------------------------------------\n\nTITLE: BigQuery Destination Credential Configuration\nDESCRIPTION: Example demonstrating how to programmatically configure BigQuery destination credentials using GCP service account credentials.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/advanced.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport dlt\nfrom dlt.sources.credentials import GcpServiceAccountCredentials\nfrom dlt.destinations import bigquery\n\ncreds_dict = os.getenv('BIGQUERY_CREDENTIALS')\n\ngcp_credentials = GcpServiceAccountCredentials()\ngcp_credentials.parse_native_representation(creds_dict)\n\npipeline = dlt.pipeline(destination=bigquery(credentials=gcp_credentials))\npipeline.run([{\"key1\": \"value1\"}], table_name=\"temp\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Production Profile in YAML\nDESCRIPTION: This YAML configuration sets up the 'prod' profile with specific settings for the data source, runtime, and destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\n  prod:\n    sources:\n      my_arrow_source:\n        row_count: 200\n    runtime:\n      log_level: INFO\n    destinations:\n      my_duckdb_destination:\n        credentials: my_data_prod.duckdb\n```\n\n----------------------------------------\n\nTITLE: Implementing Views Resource in Python\nDESCRIPTION: Python function for fetching video view data from the previous day with configurable authentication and limits.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mux.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(write_disposition=\"append\")\ndef views_resource(\n    mux_api_access_token: str = dlt.secrets.value,\n    mux_api_secret_key: str = dlt.secrets.value,\n    limit: int = DEFAULT_LIMIT,\n) -> Iterable[DltResource]:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Initializing Google Sheets Pipeline in Python\nDESCRIPTION: Configures the basic pipeline settings including name, destination, and dataset name for Google Sheets data extraction.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_sheets.md#2025-04-14_snippet_18\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n        pipeline_name=\"google_sheets\",  # Use a custom name if desired\n        destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n        dataset_name=\"google_spreadsheet_data\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Iceberg Dependencies for Python\nDESCRIPTION: Commands to install the required dependencies for using Iceberg table format with dlt.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/delta-iceberg.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[pyiceberg]\"\npip install 'sqlalchemy>=2.0.18'\n```\n\n----------------------------------------\n\nTITLE: Displaying Project Directory Structure\nDESCRIPTION: Command showing the file structure generated by the DLT init command, including the main pipeline script, requirements file, and configuration directory.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/rest-api.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nrest_api_pipeline.py\nrequirements.txt\n.dlt/\n    config.toml\n    secrets.toml\n```\n\n----------------------------------------\n\nTITLE: Azure Blob Storage Service Principal Credentials\nDESCRIPTION: Configuration for Azure Blob Storage using service principal authentication with client ID, client secret, and tenant ID.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_15\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem.credentials]\nazure_storage_account_name = \"account_name\" # please set me up!\nazure_client_id = \"client_id\" # please set me up!\nazure_client_secret = \"client_secret\"\nazure_tenant_id = \"tenant_id\" # please set me up!\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Database Connection in TOML\nDESCRIPTION: This TOML configuration sets up the connection string for a SQL database source in dlt. It specifies the database type, server, and authentication method.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/advanced.md#2025-04-14_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[sources.sql_database]\ncredentials=\"mssql+pyodbc://loader.database.windows.net/dlt_data?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Timestamp Precision in DuckDB Resource\nDESCRIPTION: Example showing how to configure timestamp precision for a DuckDB resource using the DLT framework\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/duckdb.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(\n    columns={\"event_tstamp\": {\"data_type\": \"timestamp\", \"precision\": 3}},\n    primary_key=\"event_id\",\n)\ndef events():\n    yield [{\"event_id\": 1, \"event_tstamp\": \"2024-07-30T10:00:00.123\"}]\n\npipeline = dlt.pipeline(destination=\"duckdb\")\npipeline.run(events())\n```\n\n----------------------------------------\n\nTITLE: Loading All Collections from MongoDB Database in Python\nDESCRIPTION: This snippet shows how to load all collections from a MongoDB database using DLT. It creates a MongoDB source and runs the pipeline to load the data.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mongodb.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nload_data = mongodb()\nload_info = pipeline.run(load_data, write_disposition=\"replace\")\nprint(load_info)\n```\n\n----------------------------------------\n\nTITLE: Loading Nested JSON as Python Objects\nDESCRIPTION: Example of loading nested JSON data using Python objects with custom nesting level\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/bigquery.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom dlt.destinations.adapters import bigquery_adapter\n\n@dlt.resource(name=\"cve\", max_table_nesting=1)\ndef load_cve():\n  with open(\"cve.json\", 'rb') as f:\n    yield json.load(f)\n\npipeline = dlt.pipeline(\"load_json_struct\", destination=\"bigquery\")\npipeline.run(\n  bigquery_adapter(load_cve(), autodetect_schema=True)\n)\n```\n\n----------------------------------------\n\nTITLE: Printing Pipeline Results\nDESCRIPTION: Displaying the results of the pipeline execution to verify successful data loading into Weaviate.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/weaviate.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Importing DocCardList Component in Markdown\nDESCRIPTION: This code snippet imports the DocCardList component from the theme, which is likely used to generate a list of related documentation pages.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/dataset-access/index.md#2025-04-14_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n```\n\n----------------------------------------\n\nTITLE: Initializing Zendesk Pipeline with DuckDB Destination\nDESCRIPTION: This command initializes a dlt pipeline for Zendesk data extraction with DuckDB as the destination. It creates a new directory with necessary files and configuration settings.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/zendesk.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndlt init zendesk duckdb\n```\n\n----------------------------------------\n\nTITLE: Disabling Orphan Removal in LanceDB Merge Operation\nDESCRIPTION: Example of disabling the automatic removal of orphaned chunks during a merge operation in LanceDB.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/lancedb.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npipeline.run(\n  lancedb_adapter(\n    movies,\n    embed=\"title\",\n    no_remove_orphans=True # Disable with the `no_remove_orphans` flag.\n  ),\n  write_disposition={\"disposition\": \"merge\", \"strategy\": \"upsert\"},\n  primary_key=[\"doc_id\", \"chunk_id\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Timezone Settings in DuckDB Resource\nDESCRIPTION: Example demonstrating timestamp timezone configuration for a DuckDB resource\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/duckdb.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(\n    columns={\"event_tstamp\": {\"data_type\": \"timestamp\", \"timezone\": False}},\n    primary_key=\"event_id\",\n)\ndef events():\n    yield [{\"event_id\": 1, \"event_tstamp\": \"2024-07-30T10:00:00.123+00:00\"}]\n\npipeline = dlt.pipeline(destination=\"duckdb\")\npipeline.run(events())\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL Connection for RDS (TOML)\nDESCRIPTION: TOML configuration to ensure SSL connection to RDS Postgres instance. This setting prevents fallback to non-SSL connections.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/pg_replication.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\nsources.pg_replication.credentials=\"postgresql://loader:password@host.rds.amazonaws.com:5432/dlt_data?sslmode=require&connect_timeout=300\"\n```\n\n----------------------------------------\n\nTITLE: SQL Server Connection Configuration Options\nDESCRIPTION: Various connection string configurations for SQL Server addressing Windows authentication, SSL settings, and string handling.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/troubleshooting.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\nsources.sql_database.credentials=\"mssql+pyodbc://loader.database.windows.net/dlt_data?trusted_connection=yes&driver=ODBC+Driver 17+for+SQL+Server\"\n```\n\nLANGUAGE: toml\nCODE:\n```\nsources.sql_database.credentials=\"mssql+pyodbc://loader:loader@localhost/dlt_data?encrypt=no&driver=ODBC+Driver 17+for+SQL+Server\"\n```\n\nLANGUAGE: toml\nCODE:\n```\nsources.sql_database.credentials=\"mssql+pyodbc://loader:loader@localhost/dlt_data?TrustServerCertificate=yes&driver=ODBC+Driver 17+for+SQL+Server\"\n```\n\nLANGUAGE: toml\nCODE:\n```\nsources.sql_database.credentials=\"mssql+pyodbc://loader:loader@localhost/dlt_data?LongAsMax=yes&driver=ODBC+Driver 17+for+SQL+Server\"\n```\n\n----------------------------------------\n\nTITLE: SFTP Key-Based Authentication\nDESCRIPTION: Configuration example for SFTP using key-based authentication.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/filesystem.md#2025-04-14_snippet_24\n\nLANGUAGE: toml\nCODE:\n```\n[destination.filesystem]\nbucket_url = \"sftp://[hostname]/[path]\"\nfile_glob = \"*\"\n\n[destination.filesystem.credentials]\nsftp_username = \"foo\"\nsftp_key_filename = \"/path/to/id_rsa\"     # Replace with the path to your private key file\nsftp_key_passphrase = \"your_passphrase\"   # Optional: passphrase for your private key\n```\n\n----------------------------------------\n\nTITLE: MongoDB Collection Source Function\nDESCRIPTION: Python function definition for loading a single MongoDB collection using PyMongo.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mongodb.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef mongodb_collection(\n    connection_url: str = dlt.secrets.value,\n    database: Optional[str] = dlt.config.value,\n    collection: str = dlt.config.value,\n    incremental: Optional[dlt.sources.incremental] = None,  # type: ignore[type-arg]\n    write_disposition: Optional[str] = dlt.config.value,\n    data_item_format: Optional[TDataItemFormat] = \"object\",\n) -> Any:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Cloud Build Configuration for Airflow Deployment\nDESCRIPTION: YAML configuration file (cloudbuild.yaml) referenced in the setup process for configuring Cloud Build triggers to deploy DAGs to Cloud Composer bucket.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/explainers/airflow-gcp-cloud-composer.md#2025-04-14_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Referenced but not shown in the content: build/cloudbuild.yaml\n```\n\n----------------------------------------\n\nTITLE: Verifying Pipeline Execution Results\nDESCRIPTION: Command to display the results of the pipeline execution, showing what data was loaded and any potential issues encountered.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/chess.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ndlt pipeline <pipeline_name> show\n```\n\n----------------------------------------\n\nTITLE: MongoDB Connection URL Format\nDESCRIPTION: Standard format for MongoDB connection URL string.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/mongodb.md#2025-04-14_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nconnection_url = \"mongodb://dbuser:passwd@host.or.ip:27017\"\n```\n\n----------------------------------------\n\nTITLE: Installing Parquet Support for dlt\nDESCRIPTION: Command to install the pyarrow package as a dlt dependency extra, which is required to use the Parquet file format with dlt.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/file-formats/parquet.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[parquet]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Azure Blob Storage Stage in TOML\nDESCRIPTION: This snippet shows how to configure a Snowflake Azure Blob Storage stage in the TOML configuration file by setting the stage name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/snowflake.md#2025-04-14_snippet_18\n\nLANGUAGE: toml\nCODE:\n```\n[destination]\nstage_name=\"PUBLIC.my_azure_stage\"\n```\n\n----------------------------------------\n\nTITLE: Disabling Dataset Name Normalization in TOML\nDESCRIPTION: This configuration snippet shows how to disable dataset name normalization for a specific destination (Snowflake) using a TOML configuration file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/naming-convention.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[destination.snowflake]\nenable_dataset_name_normalization=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Direct Naming Convention in TOML\nDESCRIPTION: Configuration example showing how to preserve original naming conventions in DLT by setting the schema naming to 'direct' in the config.toml file.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/schema.md#2025-04-14_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[schema]\nnaming=\"direct\"\n```\n\n----------------------------------------\n\nTITLE: TOML Source Configuration\nDESCRIPTION: Example of source configuration in TOML format with pipeline-specific credentials\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/setup.md#2025-04-14_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[pipeline_name_1.sources.google_sheets.credentials]\nclient_email = \"<client_email_1>\"\nprivate_key = \"<private_key_1>\"\nproject_id = \"<project_id_1>\"\n\n[pipeline_name_2.sources.google_sheets.credentials]\nclient_email = \"<client_email_2>\"\nprivate_key = \"<private_key_2>\"\nproject_id = \"<project_id_2>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Qdrant Destination in dlt\nDESCRIPTION: TOML configuration for Qdrant destination in dlt secrets file, specifying the Qdrant server URL and API key.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/qdrant.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[destination.qdrant]\nqd_location = \"https://your-qdrant-url\"\n[destination.qdrant.credentials]\napi_key = \"your-qdrant-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Loading All Google Ads Dimensions in Python\nDESCRIPTION: Example showing how to load all dimensions from Google Ads using the configured pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_ads.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndata_default = google_ads()\ninfo = pipeline.run(data=[data_default])\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies from Requirements File\nDESCRIPTION: Command to install the necessary dependencies specified in the requirements file generated during project initialization.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/redshift.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Filesystem Source with Config File\nDESCRIPTION: Configuration split between Python code and TOML config file for filesystem source initialization.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/filesystem/basic.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.sources.filesystem import filesystem\n\nfilesystem_source = filesystem()\n```\n\nLANGUAGE: toml\nCODE:\n```\n[sources.filesystem]\nbucket_url=\"file://Users/admin/Documents/csv_files\"\nfile_glob=\"*.csv\"\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Command to install the required Python packages listed in the requirements.txt file that was generated during project initialization.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/tutorial/rest-api.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Setting SQLAlchemy Credentials via Environment Variable\nDESCRIPTION: Command to set database connection credentials as an environment variable for use with dlt.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/sqlalchemy.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nexport DESTINATION__SQLALCHEMY__CREDENTIALS=\"mysql://loader:<password>@localhost:3306/dlt_data\"\n```\n\n----------------------------------------\n\nTITLE: Partial Credentials Specification in Python\nDESCRIPTION: Shows how to specify partial credentials using a dictionary format.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/destination.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.destinations import redshift\n\nredshift_dest = redshift(credentials={\"connection_string\": \"postgresql://admin@localhost/postgres\"})\n```\n\n----------------------------------------\n\nTITLE: Initializing DLT Project with ClickHouse\nDESCRIPTION: Commands to initialize a new DLT project with chess source and ClickHouse destination, and install requirements\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/clickhouse.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndlt init chess clickhouse\n```\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Creating a New Profile in DLT\nDESCRIPTION: This command creates a new profile called 'prod' in a DLT project.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/tutorial.md#2025-04-14_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\ndlt profile prod add\n```\n\n----------------------------------------\n\nTITLE: Installing dlt with Athena Dependencies\nDESCRIPTION: Command to install the dlt library with Athena-specific dependencies using pip.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/athena.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[athena]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Partitioning in Python\nDESCRIPTION: Python code to configure partitioning for a resource using a decorator.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/ecosystem/delta.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(\n  columns={\"_dlt_load_id\": {\"partition\": True}}\n)\ndef my_resource():\n    ...\n\npipeline = dlt.pipeline(\"loads_delta\", destination=\"delta\")\n```\n\n----------------------------------------\n\nTITLE: Installing specific version of dlt\nDESCRIPTION: Command to install a specific version of dlt (example for versions before 0.5.0) using uv.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/reference/installation.md#2025-04-14_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nuv pip install \"dlt<0.5.0\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Postgres Destination Options in dlt\nDESCRIPTION: TOML configuration example for disabling automatic index creation in the Postgres destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/postgres.md#2025-04-14_snippet_11\n\nLANGUAGE: toml\nCODE:\n```\n[destination.postgres]\ncreate_indexes=false\n```\n\n----------------------------------------\n\nTITLE: Setting File Format in TOML Configuration\nDESCRIPTION: Configure the loader file format in config.toml or secrets.toml configuration files under the normalize section.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/file-formats/_set_the_format.mdx#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[normalize]\\nloader_file_format=\"{props.file_type}\"\n```\n\n----------------------------------------\n\nTITLE: Running the SQL Database Pipeline\nDESCRIPTION: This command executes the SQL database pipeline script created during initialization.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/setup.md#2025-04-14_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npython sql_database_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Installing Pipeline Dependencies\nDESCRIPTION: Command to install required Python dependencies for the Kafka pipeline.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/kafka.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Python on Ubuntu\nDESCRIPTION: Commands to install Python 3.10 and uv package manager on Ubuntu Linux.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/plus/getting-started/installation.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt update\nsudo apt install python3.10\npip install uv\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Command to install required dependencies for a Databricks DLT project\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/databricks.md#2025-04-14_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Initializing dlt Project for SQL Database and DuckDB\nDESCRIPTION: This command initializes a dlt project in the current directory, setting up necessary files and configurations for a pipeline with SQL database as the source and DuckDB as the destination.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/sql_database/setup.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndlt init sql_database duckdb\n```\n\n----------------------------------------\n\nTITLE: Configuring DLT Pipeline for Google Ads in Python\nDESCRIPTION: Example of creating a custom pipeline configuration for Google Ads data loading. Specifies pipeline name, destination, and dataset name.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/google_ads.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"dlt_google_ads_pipeline\",  # Use a custom name if desired\n    destination=\"duckdb\",  # Choose the appropriate destination (e.g., duckdb, redshift, post)\n    dataset_name=\"full_load_google_ads\"  # Use a custom name if desired\n)\n```\n\n----------------------------------------\n\nTITLE: Connecting to local MS SQL without SSL\nDESCRIPTION: Configuration to connect to a local MS SQL Server instance that runs without SSL encryption.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/mssql.md#2025-04-14_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\ndestination.mssql.credentials=\"mssql://loader:loader@localhost/dlt_data?encrypt=no\"\n```\n\n----------------------------------------\n\nTITLE: Creating Projects Resource-Transformer for Asana\nDESCRIPTION: Python function that transforms data from the workspaces resource, fetching projects for each workspace. Uses the @dlt.defer decorator for parallel execution in a thread pool.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/verified-sources/asana.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dlt.transformer(\n    data_from=workspaces,\n    write_disposition=\"replace\",\n)\n@dlt.defer\ndef projects(\n    workspace: TDataItem,\n    access_token: str = dlt.secrets.value,\n    fields: Iterable[str] = PROJECT_FIELDS,\n) -> Iterable[TDataItem]:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Google Analytics Configuration in TOML\nDESCRIPTION: TOML configuration example for Google Analytics property ID.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/general-usage/credentials/complex_types.md#2025-04-14_snippet_9\n\nLANGUAGE: toml\nCODE:\n```\n[sources.google_analytics]\nproperty_id = \"213025502\"\n```\n\n----------------------------------------\n\nTITLE: Installing dlt with PostgreSQL Dependencies\nDESCRIPTION: Command to install the dlt library with PostgreSQL dependencies using pip.\nSOURCE: https://github.com/dlt-hub/dlt/blob/devel/docs/website/docs/dlt-ecosystem/destinations/postgres.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"dlt[postgres]\"\n```"
  }
]
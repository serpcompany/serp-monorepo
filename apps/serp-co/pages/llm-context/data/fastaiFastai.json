[
  {
    "owner": "fastai",
    "repo": "fastai",
    "content": "TITLE: Gradual Unfreezing and Fine-tuning Classifier (Unfreeze to -2) (Python)\nDESCRIPTION: Unfreezes all layers except the two deepest, halves learning rate, and retrains. Uses learning rate slicing to gradually introduce lower base rate for deeply frozen layers. Input: classifier Learner, previous lr; Output: improved classifier generalization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nlearn.freeze_to(-2)\nlr /= 2\nlearn.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7,0.8), wd=0.1)\n```\n\n----------------------------------------\n\nTITLE: Performing Learning Rate Finder with Suggestion Functions in FastAI Learner in Python\nDESCRIPTION: This patched method on the Learner class conducts a learning rate finder process by running mock training with exponentially increasing learning rates. It employs a callback to stop training on loss divergence and collects losses and learning rates to compute suggestions using provided suggestion functions. The results can be plotted and returned as a named tuple for easy access to multiple suggested learning rates.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\n@patch\n    def lr_find(self:Learner, start_lr=1e-7, end_lr=10, num_it=100, stop_div=True, show_plot=True, suggest_funcs=(SuggestionMethod.Valley)):\n        \"Launch a mock training to find a good learning rate and return suggestions based on `suggest_funcs` as a named tuple\"\n        n_epoch = num_it//len(self.dls.train) + 1\n        cb=LRFinder(start_lr=start_lr, end_lr=end_lr, num_it=num_it, stop_div=stop_div)\n        with self.no_logging(): self.fit(n_epoch, cbs=cb)\n        if suggest_funcs is not None:\n            lrs, losses = tensor(self.recorder.lrs[num_it//10:-5]), tensor(self.recorder.losses[num_it//10:-5])\n            nan_idxs = torch.nonzero(torch.isnan(losses.view(-1)))\n            if len(nan_idxs) > 0:\n                drop_idx = min(nan_idxs)\n                lrs = lrs[:drop_idx]\n                losses = losses[:drop_idx]\n            _suggestions, nms = [], []\n            for func in tuplify(suggest_funcs):\n                nms.append(func.__name__ if not isinstance(func, partial) else func.func.__name__) # deal with partials\n                _suggestions.append(func(lrs, losses, num_it))\n            \n            SuggestedLRs = collections.namedtuple('SuggestedLRs', nms)\n            lrs, pnts = [], []\n            for lr, pnt in _suggestions:\n                lrs.append(lr)\n                pnts.append(pnt)\n            if show_plot: self.recorder.plot_lr_find(suggestions=pnts, nms=nms)\n            return SuggestedLRs(*lrs)\n\n        elif show_plot: self.recorder.plot_lr_find()\n```\n\n----------------------------------------\n\nTITLE: Loading Text Classification Data with Shared Vocabulary (Python)\nDESCRIPTION: Creates TextDataLoaders for classification using data from the IMDB dataset folder. It specifies the validation set ('test') and crucially uses the vocabulary (`dls_lm.vocab`) from a previously trained language model (`dls_lm`) to ensure compatibility with the pre-trained encoder.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndls_clas = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test', text_vocab=dls_lm.vocab)\n```\n\n----------------------------------------\n\nTITLE: Recursive context manager to control randomness with seed management (Python)\nDESCRIPTION: The `no_random` context manager saves RNG states, sets seeds for `random`, `torch`, and `numpy`, and restores the states afterward. It allows defining a block of code where the randomness is controlled and reproducible, useful for testing or deterministic sampling in experiments.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\n#|export\ndef no_random(seed=42,reproducible=True):\n    \"Stores and retrieves state of random number generators. Sets random seed for `random`, `torch`, and `numpy`.\"\n    states = get_random_states()\n    set_seed(seed,reproducible=reproducible)\n    try:\n        yield #we are managing global variables\n    finally:\n        set_random_states(**states)\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Language Model (Unfrozen)\nDESCRIPTION: This unfreezes the language model and then fine-tunes it for a set number of epochs and a specific learning rate. This allows the entire model to be trained. This step typically follows the initial frozen training phase, letting the model fine-tune further.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nlearn.unfreeze()\nlearn.fit_one_cycle(10, 1e-3)\n```\n\n----------------------------------------\n\nTITLE: Defining DistributedTrainer Callback Class - Python\nDESCRIPTION: Implements a fastai Callback that wraps models in Accelerate's DistributedDataParallel and distributed-aware DataLoaders, converts batch norms if desired, and manages distributed dataset partitioning for training and validation. Designed for learners configured for distributed synchronous training. Dependencies: fastai Callback, Accelerator (Accelerate), torch, nn.SyncBatchNorm for BN conversion, DataLoader patching logic.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nclass DistributedTrainer(Callback):\n    \"Wrap `model` in `DistributedDataParallel` and `dls` in `DistributedDL`\"\n    order = 11\n    @delegates(Accelerator, but=_hidden_params)\n    def __init__(self,\n        sync_bn=True, # Whether to replace all batch norm with `nn.SyncBatchNorm`\n        **kwargs\n    ):\n        store_attr()\n        self.accelerator = Accelerator(**kwargs)\n    def before_fit(self):\n        self.learn.model = self.accelerator.prepare(\n            nn.SyncBatchNorm.convert_sync_batchnorm(self.model) if self.sync_bn else self.model\n        )\n        self.old_dls = list(self.dls)\n        self.learn.dls.loaders = [self._wrap_dl(dl) for dl in self.dls]\n        if rank_distrib(): self.learn.logger=noop\n\n    def _wrap_dl(self, dl): return dl if isinstance(dl,DistributedDL) else DistributedDL(dl, device=self.learn.model.device)\n    def _backward(self): self.accelerator.backward(self.learn.loss_grad)\n    \n    def before_train(self):    self.learn.dl = self._wrap_dl(self.learn.dl)\n    def before_validate(self): self.learn.dl = self._wrap_dl(self.learn.dl)\n    def after_fit(self): self.learn.model,self.learn.dls.loaders = self.learn.model.module,self.old_dls\n```\n\n----------------------------------------\n\nTITLE: Loading and preparing text data for classification\nDESCRIPTION: This snippet loads the IMDB movie review dataset and creates `TextDataLoaders`. It uses `from_folder` to load text data from directories, treating the 'test' folder as the validation set. This sets up the data needed for text classification, preparing it for model training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/quick_start.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\n```\n\n----------------------------------------\n\nTITLE: Displaying Batch Visualization in Python\nDESCRIPTION: Shows a visualization of a transformed batch using the DataLoader's show_batch method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ntdl.show_batch((x,y))\n```\n\n----------------------------------------\n\nTITLE: Implementing FBeta Score Wrapper for Multi-label Classification in Python\nDESCRIPTION: A wrapper function that adapts scikit-learn's fbeta_score for use in fastai's multi-label classification tasks, allowing specification of beta parameter with other customization options.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef FBetaMulti(beta, thresh=0.5, sigmoid=True, labels=None, pos_label=1, average='macro', sample_weight=None):\n    \"FBeta score with `beta` for multi-label classification problems\"\n    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n    return skm_to_fastai(skm.fbeta_score, thresh=thresh, activation=activation, flatten=False,\n                beta=beta, labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight)\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained Encoder Weights (Python)\nDESCRIPTION: Loads the weights from a previously saved encoder state ('finetuned') into the current text classifier learner (`learn`). This step transfers the knowledge learned during the language model fine-tuning phase to the classifier.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nlearn = learn.load_encoder('finetuned')\n```\n\n----------------------------------------\n\nTITLE: Example DataBlock usage with MNIST dataset\nDESCRIPTION: Shows how to create a DataBlock for the MNIST dataset with ImageBlock and CategoryBlock, along with appropriate getters and splitters.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmnist = DataBlock(blocks = (ImageBlock(cls=PILImageBW),CategoryBlock),\n                  get_items = get_image_files,\n                  splitter = GrandparentSplitter(),\n                  get_y = parent_label)\n```\n\n----------------------------------------\n\nTITLE: Defining ParamScheduler Callback\nDESCRIPTION: Defines the `ParamScheduler` callback class. This callback is used to schedule hyper-parameters, typically learning rate and momentum, during the training process. It takes a dictionary of schedules as input, where keys are hyperparameter names and values are scheduling functions. The callback updates these hyperparameters before each batch and records them after each batch.  It also saves the hyperparameters in the recorder.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@docs\nclass ParamScheduler(Callback):\n    \"Schedule hyper-parameters according to `scheds`\"\n    order,run_valid = 60,False\n\n    def __init__(self, scheds): self.scheds = scheds\n    def before_fit(self): self.hps = {p:[] for p in self.scheds.keys()}\n    def before_batch(self): self._update_val(self.pct_train)\n\n    def _update_val(self, pct):\n        for n,f in self.scheds.items(): self.opt.set_hyper(n, f(pct))\n\n    def after_batch(self):\n        for p in self.scheds.keys(): self.hps[p].append(self.opt.hypers[-1][p])\n\n    def after_fit(self):\n        if hasattr(self.learn, 'recorder') and hasattr(self, 'hps'): self.recorder.hps = self.hps\n\n    _docs = {\"before_fit\": \"Initialize container for hyper-parameters\",\n             \"before_batch\": \"Set the proper hyper-parameters in the optimizer\",\n             \"after_batch\": \"Record hyper-parameters of this batch\",\n             \"after_fit\": \"Save the hyper-parameters in the recorder if there is one\"}\n```\n\n----------------------------------------\n\nTITLE: Implementing the Callback Class\nDESCRIPTION: Defines the base `Callback` class, which handles the logic of event dispatching and allows for modifications to the training loop. It provides methods to respond to various events, and a mechanism to define actions at different stages of the training process, and handles exception during these calls.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@funcs_kwargs(as_method=True)\nclass Callback(Stateful,GetAttr):\n    \"Basic class handling tweaks of the training loop by changing a `Learner` in various events\"\n    order,_default,learn,run,run_train,run_valid = 0,'learn',None,True,True,True\n    _methods = _events\n\n    def __init__(self, **kwargs): assert not kwargs, f'Passed unknown events: {kwargs}'\n    def __repr__(self): return type(self).__name__\n\n    def __call__(self, event_name):\n        \"Call `self.{event_name}` if it's defined\"\n        _run = (event_name not in _inner_loop or (self.run_train and getattr(self, 'training', True)) or\n               (self.run_valid and not getattr(self, 'training', False)))\n        res = None\n        if self.run and _run: \n            try: res = getcallable(self, event_name)()\n            except (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): raise\n            except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\t{e.args[0]}', replace=True)\n        if event_name=='after_fit': self.run=True #Reset self.run to True at each end of fit\n        return res\n\n    def __setattr__(self, name, value):\n        \"Set an attribute for a `Callback`\"\n        if hasattr(self.learn,name):\n            warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n        super().__setattr__(name, value)\n\n    @property\n    def name(self):\n        \"Name of the `Callback`, camel-cased and with '*Callback*' removed\"\n        return class2attr(self, 'Callback')\n```\n\n----------------------------------------\n\nTITLE: Creating and fine-tuning a text classifier\nDESCRIPTION: This code creates a text classifier learner with an AWD_LSTM architecture and fine-tunes it. It uses a drop multiplier of 0.5 and the accuracy metric, then fine-tunes the model for 2 epochs with a learning rate of 1e-2. It builds upon the `TextDataLoaders` created in the previous step.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/quick_start.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(2, 1e-2)\n```\n\n----------------------------------------\n\nTITLE: fine_tune method\nDESCRIPTION: Defines the `fine_tune` method, which fine-tunes a pre-trained model. It freezes the model, trains for a few epochs, unfreezes the model, and trains for a longer duration using discriminative learning rates. It automates the process of freezing and unfreezing layers during fine-tuning.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\n@delegates(Learner.fit_one_cycle)\ndef fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n              pct_start=0.3, div=5.0, **kwargs):\n    \"Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\"\n    self.freeze()\n    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n    base_lr /= 2\n    self.unfreeze()\n    self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions on a Row - Python\nDESCRIPTION: This code predicts using the trained model on a single row of data. It uses the `predict` method of the learner object to get the model's prediction for the given row. This demonstrates how to use a trained model to infer or predict results from unseen data.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-tabular.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlearn.predict(row)\n```\n\n----------------------------------------\n\nTITLE: Showing a Single Data Batch from Dataset\nDESCRIPTION: Uses the `show` method to display the first data item from the dataset with an indicator to verify visual output.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_64\n\nLANGUAGE: Python\nCODE:\n```\ntest_stdout(lambda: dsets.show(dsets[1]), '-2')\n```\n\n----------------------------------------\n\nTITLE: Implementing the Fastai Learner Class (Python)\nDESCRIPTION: Defines the `Learner` class, the central component for training models in fastai. It takes DataLoaders, a model, loss function, optimizer, and other parameters to set up and manage the training process. It includes methods for fitting the model (`fit`), handling individual batches (`one_batch`), managing callbacks (`add_cbs`, `remove_cbs`), performing validation (`validate`), and generating predictions (`get_preds`). The class heavily relies on a callback system to allow customization at various stages of the training loop.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass Learner(GetAttr):\n    _default='model'\n    def __init__(self,\n        dls:DataLoaders, # `DataLoaders` containing fastai or PyTorch `DataLoader`s\n        model:Callable, # PyTorch model for training or inference\n        loss_func:Callable|None=None, # Loss function. Defaults to `dls` loss\n        opt_func:Optimizer|OptimWrapper=Adam, # Optimization function for training\n        lr:float|slice=defaults.lr, # Default learning rate\n        splitter:Callable=trainable_params, # Split model into parameter groups. Defaults to one parameter group\n        cbs:Callback|MutableSequence|None=None, # `Callback`s to add to `Learner`\n        metrics:Callable|MutableSequence|None=None, # `Metric`s to calculate on validation set\n        path:str|Path|None=None, # Parent directory to save, load, and export models. Defaults to `dls` `path`\n        model_dir:str|Path='models', # Subdirectory to save and load models\n        wd:float|int|None=None, # Default weight decay\n        wd_bn_bias:bool=False, # Apply weight decay to normalization and bias parameters\n        train_bn:bool=True, # Train frozen normalization layers\n        moms:tuple=(0.95,0.85,0.95), # Default momentum for schedulers\n        default_cbs:bool=True # Include default `Callback`s\n    ):\n        path = Path(path) if path is not None else getattr(dls, 'path', Path('.'))\n        if loss_func is None:\n            loss_func = getattr(dls.train_ds, 'loss_func', None)\n            assert loss_func is not None, \"Could not infer loss function from the data, please pass a loss function.\"\n        self.dls,self.model = dls,model\n        store_attr(but='dls,model,cbs')\n        self.training,self.create_mbar,self.logger,self.opt,self.cbs = False,True,print,None,L()\n        if default_cbs: self.add_cbs(L(defaults.callbacks))\n        self.add_cbs(cbs)\n        self.lock = threading.Lock()\n        self(\"after_create\")\n\n    @property\n    def metrics(self): return self._metrics\n    @metrics.setter\n    def metrics(self,v): self._metrics = L(v).map(mk_metric)\n\n    def _grab_cbs(self, cb_cls): return L(cb for cb in self.cbs if isinstance(cb, cb_cls))\n\n    def add_cbs(self, cbs):\n        L(cbs).map(self.add_cb)\n        return self\n\n    def remove_cbs(self, cbs):\n        L(cbs).map(self.remove_cb)\n        return self\n\n    def add_cb(self, cb):\n        if isinstance(cb, type): cb = cb()\n        cb.learn = self\n        setattr(self, cb.name, cb)\n        self.cbs.append(cb)\n        return self\n\n    def remove_cb(self, cb):\n        if isinstance(cb, type): self.remove_cbs(self._grab_cbs(cb))\n        else:\n            cb.learn = None\n            if hasattr(self, cb.name): delattr(self, cb.name)\n            if cb in self.cbs: self.cbs.remove(cb)\n        return self\n\n    @contextmanager\n    def added_cbs(self, cbs):\n        self.add_cbs(cbs)\n        try: yield\n        finally: self.remove_cbs(cbs)\n\n    @contextmanager\n    def removed_cbs(self, cbs):\n        self.remove_cbs(cbs)\n        try: yield self\n        finally: self.add_cbs(cbs)\n\n    def ordered_cbs(self, event): return [cb for cb in self.cbs.sorted('order') if hasattr(cb, event)]\n    def __call__(self, event_name): L(event_name).map(self._call_one)\n\n    def _call_one(self, event_name):\n        if not hasattr(event, event_name): raise Exception(f'missing {event_name}')\n        for cb in self.cbs.sorted('order'): cb(event_name)\n\n    def _bn_bias_state(self, with_bias): return norm_bias_params(self.model, with_bias).map(self.opt.state)\n\n    def create_opt(self):\n        if isinstance(self.opt_func, partial):\n            if 'lr' in self.opt_func.keywords:\n                self.lr = self.opt_func.keywords['lr']\n        if isinstance(self.opt_func, OptimWrapper):\n            self.opt = self.opt_func\n            self.opt.clear_state()\n        else:\n            self.opt = self.opt_func(self.splitter(self.model), lr=self.lr)\n        if not self.wd_bn_bias:\n            for p in self._bn_bias_state(True ): p['do_wd'] = False\n        if self.train_bn:\n            for p in self._bn_bias_state(False): p['force_train'] = True\n\n    def _split(self, b):\n        i = getattr(self.dls, 'n_inp', 1 if len(b)==1 else len(b)-1)\n        self.xb,self.yb = b[:i],b[i:]\n\n    def _with_events(self, f, event_type, ex, final=noop):\n        try: self(f'before_{event_type}');  f()\n        except ex: self(f'after_cancel_{event_type}')\n        self(f'after_{event_type}');  final()\n\n    def all_batches(self):\n        self.n_iter = len(self.dl)\n        for o in enumerate(self.dl): self.one_batch(*o)\n\n    def _backward(self): self.loss_grad.backward()\n    def _step(self): self.opt.step()\n\n    def _do_grad_opt(self):\n        self._with_events(self._backward, 'backward', CancelBackwardException)\n        self._with_events(self._step, 'step', CancelStepException)\n        self.opt.zero_grad()\n\n    def _do_one_batch(self):\n        self.pred = self.model(*self.xb)\n        self('after_pred')\n        if len(self.yb):\n            self.loss_grad = self.loss_func(self.pred, *self.yb)\n            self.loss = self.loss_grad.clone()\n        self('after_loss')\n        if not self.training or not len(self.yb): return\n        self._do_grad_opt()\n\n    def _set_device(self, b):\n        model_device = next(self.model.parameters()).device\n        dls_device = getattr(self.dls, 'device', default_device())\n        if model_device == dls_device: return to_device(b, dls_device)\n        else: return to_device(b, model_device)\n\n    def one_batch(self, i, b):\n        self.iter = i\n        b = self._set_device(b)\n        self._split(b)\n        self._with_events(self._do_one_batch, 'batch', CancelBatchException)\n\n    def _do_epoch_train(self):\n        self.dl = self.dls.train\n        self._with_events(self.all_batches, 'train', CancelTrainException)\n\n    def _do_epoch_validate(self, ds_idx=1, dl=None):\n        if dl is None: dl = self.dls[ds_idx]\n        self.dl = dl\n        with torch.no_grad(): self._with_events(self.all_batches, 'validate', CancelValidException)\n\n    def _do_epoch(self):\n        self._do_epoch_train()\n        self._do_epoch_validate()\n\n    def _do_fit(self):\n        for epoch in range(self.n_epoch):\n            self.epoch=epoch\n            self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n\n    def fit(self, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False, start_epoch=0):\n        if start_epoch != 0:\n            cbs = L(cbs) + SkipToEpoch(start_epoch)\n        with self.added_cbs(cbs):\n            if reset_opt or not self.opt: self.create_opt()\n            if wd is None: wd = self.wd\n            if wd is not None: self.opt.set_hypers(wd=wd)\n            self.opt.set_hypers(lr=self.lr if lr is None else lr)\n            self.n_epoch = n_epoch\n            self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n\n    def _end_cleanup(self): self.dl,self.xb,self.yb,self.pred,self.loss = None,(None,),(None,),None,None\n    def __enter__(self): self(_before_epoch); return self\n    def __exit__(self, exc_type, exc_value, tb): self(_after_epoch)\n\n    def validation_context(self, cbs=None, inner=False):\n        cms = [self.no_logging(),self.no_mbar(), self.lock]\n        if cbs: cms.append(self.added_cbs(cbs))\n        if not inner: cms.append(self)\n        return ContextManagers(cms)\n\n    def validate(self, ds_idx=1, dl=None, cbs=None):\n        if dl is None: dl = self.dls[ds_idx]\n        with self.validation_context(cbs=cbs): self._do_epoch_validate(ds_idx, dl)\n        return getattr(self, 'final_record', None)\n\n    @delegates(GatherPredsCallback.__init__)\n    def get_preds(self,\n        ds_idx:int=1, # `DataLoader` to use for predictions if `dl` is None. 0: train. 1: valid\n        dl=None, # `DataLoader` to use for predictions, defaults to `ds_idx=1` if None\n        with_input:bool=False, # Return inputs with predictions\n        with_decoded:bool=False, # Return decoded predictions\n        with_loss:bool=False, # Return per item loss with predictions\n        act=None, # Apply activation to predictions, defaults to `self.loss_func`'s activation\n        inner:bool=False, # If False, create progress bar, show logger, use temporary `cbs`\n        reorder:bool=True, # Reorder predictions on dataset indicies, if applicable\n        cbs:Callback|MutableSequence|None=None, # Temporary `Callback`s to apply during prediction\n        **kwargs\n    )-> tuple:\n        if dl is None: dl = self.dls[ds_idx].new(shuffle=False, drop_last=False)\n        else:\n            try: len(dl)\n            except TypeError as e:\n                raise TypeError(f\"`dl` is {type(dl)} and doesn't have len(dl)\")\n        if isinstance(dl, DataLoader):\n            if dl.drop_last: dl = dl.new(shuffle=False, drop_last=False)\n        if reorder and hasattr(dl, 'get_idxs'):\n            idxs = dl.get_idxs()\n            dl = dl.new(get_idxs = _ConstantFunc(idxs))\n        cb = GatherPredsCallback(with_input=with_input, with_loss=with_loss, **kwargs)\n        ctx_mgrs = self.validation_context(cbs=L(cbs)+[cb], inner=inner)\n        if with_loss: ctx_mgrs.append(self.loss_not_reduced())\n        with ContextManagers(ctx_mgrs):\n            self._do_epoch_validate(dl=dl)\n            if act is None: act = getcallable(self.loss_func, 'activation')\n            res = cb.all_tensors()\n            pred_i = 1 if with_input else 0\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Transformation for Subset 1 using fastai Transform\nDESCRIPTION: This code defines a custom transformation class `_Tfm` which doubles the input during encoding and halves it during decoding, applied specifically to subset 1 of the dataset. It is used to modify data within that subset for experiments or preprocessing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_52\n\nLANGUAGE: Python\nCODE:\n```\n# only transform subset 1\nclass _Tfm(Transform):\n    split_idx=1\n    def encodes(self, x): return x*2\n    def decodes(self, x): return TitledStr(x//2)\n```\n\n----------------------------------------\n\nTITLE: Loading Fastai Text Model with Optional Optimizer State - Python\nDESCRIPTION: Defines load_model_text to load a saved fastai text model (and optimizer state, optionally) from a file, handling device placement, DISTRIBUTED context, and past version compatibility. Inputs: file (str), model (nn.Module), opt (Optimizer), with_opt (bool), device (int/str/device), strict (bool). Output: None, but replaces model and optimizer weights in memory. Dependencies: torch, fastai, distributed environment. Limitations: relies on correct file structure and requires matching model structure.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef load_model_text(\n    file:str, # File name of saved text model\n    model, # Model architecture\n    opt:Optimizer, # `Optimizer` used to fit the model\n    with_opt:bool=None, # Enable to load `Optimizer` state\n    device:int|str|torch.device=None, # Sets the device, uses 'cpu' if unspecified\n    strict:bool=True # Whether to strictly enforce the keys of `file`s state dict match with the model `Module.state_dict`\n):\n    \"Load `model` from `file` along with `opt` (if available, and if `with_opt`)\"\n    distrib_barrier()\n    if isinstance(device, int): device = torch.device('cuda', device)\n    elif device is None: device = 'cpu'\n    state = torch.load(file, map_location=device)\n    hasopt = set(state)=={'model', 'opt'}\n    model_state = state['model'] if hasopt else state\n    get_model(model).load_state_dict(clean_raw_keys(model_state), strict=strict)\n    if hasopt and ifnone(with_opt,True):\n        try: opt.load_state_dict(state['opt'])\n        except:\n            if with_opt: warn(\"Could not load the optimizer state.\")\n    elif with_opt: warn(\"Saved file doesn't contain an optimizer state.\")\n```\n\n----------------------------------------\n\nTITLE: Download IMDB Sample Dataset\nDESCRIPTION: Downloads a sample of the IMDB dataset using `untar_data` from the `fastai.data.external` module and lists the contents of the extracted directory. This allows for quick experimentation before using the full dataset.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.IMDB_SAMPLE)\npath.ls()\n```\n\n----------------------------------------\n\nTITLE: Test Time Augmentation (TTA) - fastai - Python\nDESCRIPTION: This patched `tta` method for the `Learner` class performs Test Time Augmentation (TTA) to improve prediction accuracy. It applies data augmentations `n` times, makes predictions for each augmented version, and averages the predictions. It also allows the use of maximum prediction instead of average. Item and batch transforms can be passed to this function for custom transformations.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_76\n\nLANGUAGE: python\nCODE:\n```\n@patch\ndef tta(self:Learner, ds_idx=1, dl=None, n=4, item_tfms=None, batch_tfms=None, beta=0.25, use_max=False):\n    \"Return predictions on the `ds_idx` dataset or `dl` using Test Time Augmentation\"\n    if dl is None: dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n    if item_tfms is not None or batch_tfms is not None: dl = dl.new(after_item=item_tfms, after_batch=batch_tfms)\n    try:\n        self(_before_epoch)\n        with dl.dataset.set_split_idx(0), self.no_mbar():\n            if hasattr(self,'progress'): self.progress.mbar = master_bar(list(range(n)))\n            aug_preds = []\n            for i in self.progress.mbar if hasattr(self,'progress') else range(n):\n                self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n                aug_preds.append(self.get_preds(dl=dl, inner=True)[0][None])\n        aug_preds = torch.cat(aug_preds)\n        aug_preds = aug_preds.max(0)[0] if use_max else aug_preds.mean(0)\n        self.epoch = n\n        with dl.dataset.set_split_idx(1): preds,targs = self.get_preds(dl=dl, inner=True)\n    finally: self(event.after_fit)\n\n    if use_max: return torch.stack([preds, aug_preds], 0).max(0)[0],targs\n    preds = (aug_preds,preds) if beta is None else torch.lerp(aug_preds, preds, beta)\n    return preds,targs\n```\n\n----------------------------------------\n\nTITLE: Building a U-Net Learner for Segmentation Tasks\nDESCRIPTION: This function constructs a U-Net learner, configuring architecture, normalization, loss, metrics, and other training parameters. It infers number of output classes from the data if not provided, and prepares the model with optional normalization and splitting. Dependencies include fastai's vision and data modules, designed for image segmentation tasks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\n@delegates(create_unet_model)\ndef unet_learner(dls, arch, normalize=True, n_out=None, pretrained=True, weights=None, config=None,  \n                 # learner args\n                 loss_func=None, opt_func=Adam, lr=defaults.lr, splitter=None, cbs=None, metrics=None, path=None,  \n                 model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95,0.85,0.95), **kwargs):\n    \"Build a unet learner from `dls` and `arch`\"\n\n    if config:\n        warnings.warn('config param is deprecated. Pass your args directly to unet_learner.')\n        kwargs = {**config, **kwargs}\n\n    meta = model_meta.get(arch, _default_meta)\n    n_in = kwargs['n_in'] if 'n_in' in kwargs else 3\n    if normalize: _add_norm(dls, meta, pretrained, n_in)\n\n    n_out = ifnone(n_out, get_c(dls))\n    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n    img_size = dls.one_batch()[0].shape[-2:]\n    assert img_size, \"image size could not be inferred from data\"\n    model = create_unet_model(arch, n_out, img_size, pretrained=pretrained, weights=weights, **kwargs)\n\n    splitter = ifnone(splitter, meta['split'])\n    learn = Learner(dls=dls, model=model, loss_func=loss_func, opt_func=opt_func, lr=lr, splitter=splitter, cbs=cbs,\n                   metrics=metrics, path=path, model_dir=model_dir, wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn,\n                   moms=moms)\n    if pretrained: learn.freeze()\n    # keep track of args for loggers\n    store_attr('arch,normalize,n_out,pretrained', self=learn, **kwargs)\n    return learn\n```\n\n----------------------------------------\n\nTITLE: Implementing DataBlock class for dataset creation\nDESCRIPTION: Defines the core DataBlock class which serves as a generic container to build Datasets and DataLoaders with customizable transforms, splitters, and getters.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@docs\n@funcs_kwargs\nclass DataBlock():\n    \"Generic container to quickly build `Datasets` and `DataLoaders`.\"\n    get_x=get_items=splitter=get_y = None\n    blocks,dl_type = (TransformBlock,TransformBlock),TfmdDL\n    _methods = 'get_items splitter get_y get_x'.split()\n    _msg = \"If you wanted to compose several transforms in your getter don't forget to wrap them in a `Pipeline`.\"\n    def __init__(self, \n        blocks:list=None, # One or more `TransformBlock`s\n        dl_type:TfmdDL=None, # Task specific `TfmdDL`, defaults to `block`'s dl_type or`TfmdDL`\n        getters:list=None, # Getter functions applied to results of `get_items`\n        n_inp:int=None, # Number of inputs\n        item_tfms:list=None, # `ItemTransform`s, applied on an item \n        batch_tfms:list=None, # `Transform`s or `RandTransform`s, applied by batch\n        **kwargs, \n    ):\n        blocks = L(self.blocks if blocks is None else blocks)\n        blocks = L(b() if callable(b) else b for b in blocks)\n        self.type_tfms = blocks.attrgot('type_tfms', L())\n        self.default_item_tfms  = _merge_tfms(*blocks.attrgot('item_tfms',  L()))\n        self.default_batch_tfms = _merge_tfms(*blocks.attrgot('batch_tfms', L()))\n        for b in blocks:\n            if getattr(b, 'dl_type', None) is not None: self.dl_type = b.dl_type\n        if dl_type is not None: self.dl_type = dl_type\n        self.dataloaders = delegates(self.dl_type.__init__)(self.dataloaders)\n        self.dls_kwargs = merge(*blocks.attrgot('dls_kwargs', {}))\n\n        self.n_inp = ifnone(n_inp, max(1, len(blocks)-1))\n        self.getters = ifnone(getters, [noop]*len(self.type_tfms))\n        if self.get_x:\n            if len(L(self.get_x)) != self.n_inp:\n                raise ValueError(f'get_x contains {len(L(self.get_x))} functions, but must contain {self.n_inp} (one for each input)\\n{self._msg}')\n            self.getters[:self.n_inp] = L(self.get_x)\n        if self.get_y:\n            n_targs = len(self.getters) - self.n_inp\n            if len(L(self.get_y)) != n_targs:\n                raise ValueError(f'get_y contains {len(L(self.get_y))} functions, but must contain {n_targs} (one for each target)\\n{self._msg}')\n            self.getters[self.n_inp:] = L(self.get_y)\n\n        if kwargs: raise TypeError(f'invalid keyword arguments: {\", \".join(kwargs.keys())}')\n        self.new(item_tfms, batch_tfms)\n\n    def _combine_type_tfms(self): return L([self.getters, self.type_tfms]).map_zip(\n        lambda g,tt: (g.fs if isinstance(g, Pipeline) else L(g)) + tt)\n\n    def new(self, \n        item_tfms:list=None, # `ItemTransform`s, applied on an item\n        batch_tfms:list=None, # `Transform`s or `RandTransform`s, applied by batch \n    ):\n        self.item_tfms  = _merge_tfms(self.default_item_tfms,  item_tfms)\n        self.batch_tfms = _merge_tfms(self.default_batch_tfms, batch_tfms)\n        return self\n\n    @classmethod\n    def from_columns(cls, \n        blocks:list =None, # One or more `TransformBlock`s\n        getters:list =None, # Getter functions applied to results of `get_items`\n        get_items:Callable=None, # A function to get items\n        **kwargs,\n    ):\n        if getters is None: getters = L(ItemGetter(i) for i in range(2 if blocks is None else len(L(blocks))))\n        get_items = _zip if get_items is None else compose(get_items, _zip)\n        return cls(blocks=blocks, getters=getters, get_items=get_items, **kwargs)\n\n    def datasets(self, \n        source, # The data source\n        verbose:bool=False, # Show verbose messages\n    ) -> Datasets:\n        self.source = source                     ; pv(f\"Collecting items from {source}\", verbose)\n        items = (self.get_items or noop)(source) ; pv(f\"Found {len(items)} items\", verbose)\n        splits = (self.splitter or RandomSplitter())(items)\n        pv(f\"{len(splits)} datasets of sizes {','.join([str(len(s)) for s in splits])}\", verbose)\n        return Datasets(items, tfms=self._combine_type_tfms(), splits=splits, dl_type=self.dl_type, n_inp=self.n_inp, verbose=verbose)\n\n    def dataloaders(self, \n        source, # The data source\n        path:str='.', # Data source and default `Learner` path \n        verbose:bool=False, # Show verbose messages\n        **kwargs\n    ) -> DataLoaders:\n        dsets = self.datasets(source, verbose=verbose)\n        kwargs = {**self.dls_kwargs, **kwargs, 'verbose': verbose}\n        return dsets.dataloaders(path=path, after_item=self.item_tfms, after_batch=self.batch_tfms, **kwargs)\n\n    _docs = dict(new=\"Create a new `DataBlock` with other `item_tfms` and `batch_tfms`\",\n                 datasets=\"Create a `Datasets` object from `source`\",\n                 dataloaders=\"Create a `DataLoaders` object from `source`\")\n```\n\n----------------------------------------\n\nTITLE: Create collaborative data loaders from merged data\nDESCRIPTION: Creates collaborative data loaders (`CollabDataLoaders`) from the merged `rating_movie` DataFrame. Sets the random seed to 42, validation percentage to 0.1, batch size to 64, specifies the `item_name` as `title`, and provides the path to the dataset.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndls = CollabDataLoaders.from_df(rating_movie, seed=42, valid_pct=0.1, bs=64, item_name=title, path=path)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Transformation Pipeline with Normalize in Python\nDESCRIPTION: Creates a transformation pipeline using IntToFloatTensor and Normalize with predefined mean and standard deviation values, then creates a transformed DataLoader.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nmean,std = [0.5]*3,[0.5]*3\nmean,std = broadcast_vec(1, 4, mean, std)\nbatch_tfms = [IntToFloatTensor(), Normalize.from_stats(mean,std)]\ntdl = TfmdDL(train_ds, after_batch=batch_tfms, bs=4, device=default_device())\n```\n\n----------------------------------------\n\nTITLE: Creating a Text Classifier Learner (Python)\nDESCRIPTION: Initializes a text classifier learner (`learn`) using the prepared data loaders (`dls_clas`), specifying the AWD_LSTM architecture, a dropout multiplier (`drop_mult=0.5`), and the accuracy metric for evaluation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nlearn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n```\n\n----------------------------------------\n\nTITLE: Implementing show_results function for displaying model predictions with inputs and targets\nDESCRIPTION: A type-dispatched function that shows decoded samples and their corresponding model predictions. It handles displaying inputs, targets, and outputs up to a maximum number, using context objects for visualization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dispatch\ndef show_results(\n    x, # Input(s) in the batch\n    y, # Target(s) in the batch\n    samples, # List of (`x`, `y`) pairs of length `max_n`\n    outs, # List of predicted output(s) from the model\n    ctxs=None, # List of `ctx` objects to show data. Could be a matplotlib axis, DataFrame, etc.\n    max_n=9, # Maximum number of `samples` to show\n    **kwargs\n):\n    \"Show `max_n` results with input(s), target(s) and prediction(s).\"\n    if ctxs is None: ctxs = Inf.nones\n    for i in range(len(samples[0])):\n        ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]\n    for i in range(len(outs[0])):\n        ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(outs.itemgot(i),ctxs,range(max_n))]\n    return ctxs\n```\n\n----------------------------------------\n\nTITLE: fit_one_cycle using ParamScheduler\nDESCRIPTION: Defines the `fit_one_cycle` method which fits a model for a specified number of epochs using the 1cycle policy. This policy schedules the learning rate and momentum with cosine annealing, involving an initial phase, and a final phase.  It calculates and sets the learning rate and momentum schedules based on parameters like `lr_max`, `div`, `div_final`, `pct_start`, and `moms`. It uses  `ParamScheduler` internally for scheduling.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\ndef fit_one_cycle(self:Learner, n_epoch, lr_max=None, div=25., div_final=1e5, pct_start=0.25, wd=None,\n                  moms=None, cbs=None, reset_opt=False, start_epoch=0):\n    \"Fit `self.model` for `n_epoch` using the 1cycle policy.\"\n    if self.opt is None: self.create_opt()\n    self.opt.set_hyper('lr', self.lr if lr_max is None else lr_max)\n    lr_max = np.array([h['lr'] for h in self.opt.hypers])\n    scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),\n              'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}\n    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n```\n\n----------------------------------------\n\nTITLE: Defining TextDataLoaders Factory Methods - Python\nDESCRIPTION: This snippet defines the `TextDataLoaders` class, a wrapper around `DataLoaders` offering simplified factory methods for common NLP setups. The methods (`from_folder`, `from_df`, `from_csv`) internally configure the necessary `DataBlock` with `TextBlock` and target blocks, providing a high-level interface for creating DataLoaders from various sources.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nclass TextDataLoaders(DataLoaders):\n    \"Basic wrapper around several `DataLoader`s with factory methods for NLP problems\"\n    @classmethod\n    @delegates(DataLoaders.from_dblock)\n    def from_folder(cls, path, train='train', valid='valid', valid_pct=None, seed=None, vocab=None, text_vocab=None, is_lm=False,\n                    tok_tfm=None, seq_len=72, splitter=None, backwards=False, **kwargs):\n        \"Create from imagenet style dataset in `path` with `train` and `valid` subfolders (or provide `valid_pct`)\"\n        if splitter is None:\n            splitter = GrandparentSplitter(train_name=train, valid_name=valid) if valid_pct is None else RandomSplitter(valid_pct, seed=seed)\n        blocks = [TextBlock.from_folder(path, text_vocab, is_lm, seq_len, backwards, tok=tok_tfm)]\n        if not is_lm: blocks.append(CategoryBlock(vocab=vocab))\n        get_items = partial(get_text_files, folders=[train,valid]) if valid_pct is None else get_text_files\n        dblock = DataBlock(blocks=blocks,\n                           get_items=get_items,\n                           splitter=splitter,\n                           get_y=None if is_lm else parent_label)\n        return cls.from_dblock(dblock, path, path=path, seq_len=seq_len, **kwargs)\n\n    @classmethod\n    @delegates(DataLoaders.from_dblock)\n    def from_df(cls, df, path='.', valid_pct=0.2, seed=None, text_col=0, label_col=1, label_delim=None, y_block=None,\n                text_vocab=None, is_lm=False, valid_col=None, tok_tfm=None, tok_text_col=\"text\", seq_len=72, backwards=False, **kwargs):\n        \"Create from `df` in `path` with `valid_pct`\"\n        blocks = [TextBlock.from_df(text_col, text_vocab, is_lm, seq_len, backwards, tok=tok_tfm)]\n        if y_block is None and not is_lm:\n            blocks.append(MultiCategoryBlock if is_listy(label_col) and len(label_col) > 1 else CategoryBlock)\n        if y_block is not None and not is_lm: blocks += (y_block if is_listy(y_block) else [y_block])\n        splitter = RandomSplitter(valid_pct, seed=seed) if valid_col is None else ColSplitter(valid_col)\n        dblock = DataBlock(blocks=blocks,\n                           get_x=ColReader(tok_text_col),\n                           get_y=None if is_lm else ColReader(label_col, label_delim=label_delim),\n                           splitter=splitter)\n        return cls.from_dblock(dblock, df, path=path, seq_len=seq_len, **kwargs)\n\n    @classmethod\n    def from_csv(cls, path, csv_fname='labels.csv', header='infer', delimiter=None, quoting=csv.QUOTE_MINIMAL, **kwargs):\n        \"Create from `csv` file in `path/csv_fname`\"\n        df = pd.read_csv(Path(path)/csv_fname, header=header, delimiter=delimiter, quoting=quoting)\n        return cls.from_df(df, path=path, **kwargs)\n\nTextDataLoaders.from_csv = delegates(to=TextDataLoaders.from_df)(TextDataLoaders.from_csv)\n```\n\n----------------------------------------\n\nTITLE: Product Layer for Skip Connections in Python\nDESCRIPTION: Similar to `MergeLayer`, this layer is intended for use within a `SequentialEx` block. It retrieves the original input to the block from the `orig` attribute of the current input tensor (`x`). Instead of adding or concatenating, it merges the current result (`x`) with the original input (`x.orig`) by performing element-wise multiplication. This is used in some network architectures like Squeeze-and-Excitation modules.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_67\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass ProdLayer(Module):\n    \"Merge a shortcut with the result of the module by multiplying them.\"\n    def forward(self, x): return x * x.orig\n```\n\n----------------------------------------\n\nTITLE: Creating a Text Classifier Learner in FastAI\nDESCRIPTION: Defines a factory function `text_classifier_learner` for creating a `TextLearner` specifically for text classification tasks. It sets up the specified model architecture (`arch`) with a classification head, configures it based on `dls` (vocabulary, number of classes `n_out`), sequence length (`seq_len`), dropout (`drop_mult`), and other model parameters (`lin_ftrs`, `ps`, `max_len`). It optionally loads pretrained encoder weights and freezes them for fine-tuning. Requires `dls`, `arch`, and infers `n_out` from `dls` if not provided.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@delegates(Learner.__init__)\ndef text_classifier_learner(dls, arch, seq_len=72, config=None, backwards=False, pretrained=True, drop_mult=0.5, n_out=None,\n                            lin_ftrs=None, ps=None, max_len=72*20, y_range=None, **kwargs):\n    \"Create a `Learner` with a text classifier from `dls` and `arch`.\"\n    vocab = _get_text_vocab(dls)\n    if n_out is None: n_out = get_c(dls)\n    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n    model = get_text_classifier(arch, len(vocab), n_out, seq_len=seq_len, config=config, y_range=y_range,\n                                drop_mult=drop_mult, lin_ftrs=lin_ftrs, ps=ps, max_len=max_len)\n    meta = _model_meta[arch]\n    learn = TextLearner(dls, model, splitter=meta['split_clas'], **kwargs)\n    url = 'url_bwd' if backwards else 'url'\n    if pretrained:\n        if url not in meta:\n            warn(\"There are no pretrained weights for that architecture yet!\")\n            return learn\n        model_path = untar_data(meta[url], c_key='model')\n        try: fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n        except IndexError: print(f'The model in {model_path} is incomplete, download again'); raise\n        learn = learn.load_pretrained(*fnames, model=learn.model[0])\n        learn.freeze()\n    return learn\n```\n\n----------------------------------------\n\nTITLE: GradientClip - Clipping Gradients During Training\nDESCRIPTION: This callback clips the norm of the gradients to avoid exploding gradients during training, which can stabilize training especially with high learning rates or mixed precision.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18a_callback.training.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nclass GradientClip(Callback):\n    \"Clip norm of gradients\"\n    order=MixedPrecision.order+1\n    def __init__(self,max_norm:float=1., norm_type:float=2.0): store_attr()\n    def before_step(self): nn.utils.clip_grad_norm_(self.parameters(), self.max_norm, self.norm_type)\n```\n\n----------------------------------------\n\nTITLE: Using MCDropoutCallback with get_preds\nDESCRIPTION: Demonstrates how to use the `MCDropoutCallback` with a fastai Learner's `get_preds` method. It iterates multiple times, collecting predictions with dropout enabled on each call, to approximate model uncertainty using Monte Carlo Dropout. The resulting predictions are stacked into a single tensor.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18b_callback.preds.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner()\n\n# Call get_preds 10 times, then stack the predictions, yielding a tensor with shape [# of samples, batch_size, ...]\ndist_preds = []\nfor i in range(10):\n    preds, targs = learn.get_preds(cbs=[MCDropoutCallback()])\n    dist_preds += [preds]\n\ntorch.stack(dist_preds).shape\n```\n\n----------------------------------------\n\nTITLE: Defining SegmentationDataLoaders with fastai in Python\nDESCRIPTION: Defines a subclass SegmentationDataLoaders of DataLoaders specialized for segmentation tasks with factory method from_label_func. This method creates a DataBlock with image and mask blocks, splits data randomly into training and validation subsets by valid_pct and seed, obtains labels via a label_func, and applies optional item and batch transformations. Requires fastai and dependencies like PILImage, DataBlock, ImageBlock, MaskBlock. Inputs include path, filenames, label function, and segmentation codes; outputs a SegmentationDataLoaders instance ready for segmentation model training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/08_vision.data.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass SegmentationDataLoaders(DataLoaders):\n    \"Basic wrapper around several `DataLoader`s with factory methods for segmentation problems\"\n    @classmethod\n    @delegates(DataLoaders.from_dblock)\n    def from_label_func(cls, path, fnames, label_func, valid_pct=0.2, seed=None, codes=None, item_tfms=None, batch_tfms=None, \n                        img_cls=PILImage, **kwargs):\n        \"Create from list of `fnames` in `path`s with `label_func`.\"\n        dblock = DataBlock(blocks=(ImageBlock(img_cls), MaskBlock(codes=codes)),\n                           splitter=RandomSplitter(valid_pct, seed=seed),\n                           get_y=label_func,\n                           item_tfms=item_tfms,\n                           batch_tfms=batch_tfms)\n        res = cls.from_dblock(dblock, fnames, path=path, **kwargs)\n        return res\n```\n\n----------------------------------------\n\nTITLE: Creating and fine-tuning a collaborative filtering model\nDESCRIPTION: This code creates a collaborative filtering learner and fine-tunes it.  It sets the `y_range` and fine-tunes the model for 6 epochs. This allows the user to build a model for predicting movie ratings based on user-item interactions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/quick_start.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nlearn = collab_learner(dls, y_range=(0.5,5.5))\nlearn.fine_tune(6)\n```\n\n----------------------------------------\n\nTITLE: Creating a Tabular Learner with tabular_learner Function in Python\nDESCRIPTION: Defines a factory function `tabular_learner` to conveniently create a `TabularLearner` based on a `TabularDataLoaders` object. It optionally accepts layers configuration, embedding sizes for categorical variables, model config dictionary, output size (`n_out`), and output range (`y_range`). The function manages defaults and infers necessary parameters, builds a `TabularModel`, and returns a configured `TabularLearner`. It is designed for ease of use with fastai tabular datasets and includes utilities like `tabular_config` and `get_emb_sz` for advanced customization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/43_tabular.learner.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@delegates(Learner.__init__)\ndef tabular_learner(\n        dls:TabularDataLoaders,\n        layers:list=None, # Size of the layers generated by `LinBnDrop`\n        emb_szs:list=None, # Tuples of `n_unique, embedding_size` for all categorical features\n        config:dict=None, # Config params for TabularModel from `tabular_config`\n        n_out:int=None, # Final output size of the model\n        y_range:Tuple=None, # Low and high for the final sigmoid function\n        **kwargs\n):\n    \"Get a `Learner` using `dls`, with `metrics`, including a `TabularModel` created using the remaining params.\"\n    if config is None: config = tabular_config()\n    if layers is None: layers = [200,100]\n    to = dls.train_ds\n    emb_szs = get_emb_sz(dls.train_ds, {} if emb_szs is None else emb_szs)\n    if n_out is None: n_out = get_c(dls)\n    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n    if y_range is None and 'y_range' in config: y_range = config.pop('y_range')\n    model = TabularModel(emb_szs, len(dls.cont_names), n_out, layers, y_range=y_range, **config)\n    return TabularLearner(dls, model, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Tokenizing and Preparing Prompt for Model Input in Python (HuggingFace/PyTorch)\nDESCRIPTION: Tokenizes the `prompt` string using a HuggingFace `tokenizer`'s `encode` method to convert it into numerical input IDs. These IDs are then converted into a PyTorch tensor, a batch dimension is added (`[None]`), the tensor is moved to the GPU (`cuda()`), and its shape is printed. This prepares the prompt tensor (`inp`) for input to the model's generation method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nprompt_ids = tokenizer.encode(prompt)\ninp = tensor(prompt_ids)[None].cuda()\ninp.shape\n```\n\n----------------------------------------\n\nTITLE: Testing consistent sampling behavior with AdamantDL\nDESCRIPTION: This code tests consistent sampling behavior when `num_workers` > 1, by overriding the `get_idxs` method to always return the same index.  It initializes a custom `AdamantDL` class that overrides the `get_idxs` method of the base `DataLoader` class to always return the same index, in order to test consistent sampling behavior.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nclass AdamantDL(DataLoader):\n    def get_idxs(self):\n        r=random.randint(0,self.n-1)\n        return [r] * self.n\n\ntest_eq(torch.cat(tuple(AdamantDL((list(range(50))),bs=16,num_workers=4))).unique().numel(),1)\n```\n\n----------------------------------------\n\nTITLE: Installing fastai on Colab\nDESCRIPTION: This code snippet installs or upgrades the fastai library using pip. It first checks if the '/content' directory exists, which is common in Google Colab environments, and then installs fastai using pip with the '-Uqq' options for upgrading quietly.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Define TrackerCallback\nDESCRIPTION: Defines a callback that tracks the best value of a monitored metric. The `before_fit` method initializes the best value. The `after_epoch` method compares the current value to the best and updates if necessary.  The `monitor` parameter specifies the metric to track, `comp` is the comparison operator (defaults to `np.less` for loss and `np.greater` for other metrics), and `min_delta` is the minimum improvement required to update the best value.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass TrackerCallback(Callback):\n    \"A `Callback` that keeps track of the best value in `monitor`.\"\n    order,remove_on_fetch,_only_train_loop = 60,True,True\n    def __init__(self,\n        monitor='valid_loss', # value (usually loss or metric) being monitored.\n        comp=None, # numpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n        min_delta=0., # minimum delta between the last monitor value and the best monitor value.\n        reset_on_fit=True # before model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss).\n    ):\n        if comp is None: comp = np.less if 'loss' in monitor or 'error' in monitor else np.greater\n        if comp == np.less: min_delta *= -1\n        self.monitor,self.comp,self.min_delta,self.reset_on_fit,self.best= monitor,comp,min_delta,reset_on_fit,None\n\n    def before_fit(self):\n        \"Prepare the monitored value\"\n        self.run = not hasattr(self, \"lr_finder\") and not hasattr(self, \"gather_preds\")\n        if self.reset_on_fit or self.best is None: self.best = float('inf') if self.comp == np.less else -float('inf')\n        assert self.monitor in self.recorder.metric_names[1:]\n        self.idx = list(self.recorder.metric_names[1:]).index(self.monitor)\n\n    def after_epoch(self):\n        \"Compare the last value to the best up to now\"\n        val = self.recorder.values[-1][self.idx]\n        if self.comp(val - self.min_delta, self.best): self.best,self.new_best = val,True\n        else: self.new_best = False\n\n    def after_fit(self): self.run=True\n```\n\n----------------------------------------\n\nTITLE: Creating Submission DataFrame - Python\nDESCRIPTION: Builds a pandas DataFrame with columns 'image_name' and 'tags' for direct CSV export, following Kaggle submission requirements. Inputs: lists of filenames and predicted labels. Output: DataFrame ready for CSV export.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'image_name':fnames, 'tags':labelled_preds}, columns=['image_name', 'tags'])\n```\n\n----------------------------------------\n\nTITLE: Accumulating Perplexity Metric in a Fastai Learner Environment\nDESCRIPTION: Demonstrates usage of the Perplexity metric class with synthetic data tensor batches and a test learner. The snippet shows how to reset the perplexity metric, accumulate loss and target batches from random data, and verify the computed metric against PyTorch's built-in cross-entropy loss exponentiation to ensure correctness. It reveals expected inputs as tensors of logits and target indices and uses fastai-like Learner objects to simulate training step outputs during metric accumulation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nx1,x2 = torch.randn(20,5),torch.randint(0, 5, (20,))\ntst = perplexity\ntst.reset()\nvals = [0,6,15,20]\nlearn = TstLearner()\nfor i in range(3): \n    learn.yb = (x2[vals[i]:vals[i+1]],)\n    learn.loss = F.cross_entropy(x1[vals[i]:vals[i+1]],x2[vals[i]:vals[i+1]])\n    tst.accumulate(learn)\ntest_close(tst.value, torch.exp(F.cross_entropy(x1,x2)))\n```\n\n----------------------------------------\n\nTITLE: Defining a before_batch Callback Function\nDESCRIPTION: This code defines a function that modifies `xb` and `yb` of the learner before each batch, as a common data augmentation/transformation point in the training.  The `before_batch_cb` decorator is used to turn a function into a callback that is called on the `before_batch` event. The resulting callback can be then used to modify the data provided to your model.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\ndef _before_batch_cb(f, self):\n    xb,yb = f(self, self.xb, self.yb)\n    self.learn.xb,self.learn.yb = xb,yb\n```\n\nLANGUAGE: python\nCODE:\n```\ndef before_batch_cb(f):\n    \"Shortcut for creating a Callback on the `before_batch` event, which takes and returns `xb,yb`\"\n    return Callback(before_batch=partial(_before_batch_cb, f))\n```\n\n----------------------------------------\n\nTITLE: Defining LMLearner Class for Language Modeling in FastAI\nDESCRIPTION: Defines the `LMLearner` class, inheriting from `TextLearner`, tailored for language modeling. It overrides the `predict` method to generate text sequences based on an initial prompt. The prediction process iteratively samples words based on model output probabilities, supporting features like temperature scaling, minimum probability thresholding (`min_p`), and preventing unknown token (`UNK`) generation. It also includes an override for `get_preds`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass LMLearner(TextLearner):\n    \"Add functionality to `TextLearner` when dealing with a language model\"\n    def predict(self, text, n_words=1, no_unk=True, temperature=1., min_p=None, no_bar=False,\n                decoder=decode_spec_tokens, only_last_word=False):\n        \"Return `text` and the `n_words` that come after\"\n        self.model.reset()\n        idxs = idxs_all = self.dls.test_dl([text]).items[0].to(self.dls.device)\n        if no_unk: unk_idx = self.dls.vocab.index(UNK)\n        for _ in (range(n_words) if no_bar else progress_bar(range(n_words), leave=False)):\n            with self.no_bar(): preds,_ = self.get_preds(dl=[(idxs[None],)])\n            res = preds[0][-1]\n            if no_unk: res[unk_idx] = 0.\n            if min_p is not None:\n                if (res >= min_p).float().sum() == 0:\n                    warn(f\"There is no item with probability >= {min_p}, try a lower value.\")\n                else: res[res < min_p] = 0.\n            if temperature != 1.: res.pow_(1 / temperature)\n            idx = torch.multinomial(res, 1).item()\n            idxs = idxs_all = torch.cat([idxs_all, idxs.new([idx])])\n            if only_last_word: idxs = idxs[-1][None]\n\n        num = self.dls.train_ds.numericalize\n        tokens = [num.vocab[i] for i in idxs_all if num.vocab[i] not in [BOS, PAD]]\n        sep = self.dls.train_ds.tokenizer.sep\n        return sep.join(decoder(tokens))\n\n    @delegates(Learner.get_preds)\n    def get_preds(self, concat_dim=1, **kwargs): return super().get_preds(concat_dim=1, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Defining a Customizable Convolutional Layer (Python)\nDESCRIPTION: Defines the `ConvLayer` class, a `torch.nn.Sequential` module that combines a convolutional layer, an optional activation function, and optional normalization. It handles dimension selection (1D, 2D, 3D via `ndim`), transposed convolutions (`transpose`), kernel size (`ks`), stride, padding (auto-calculated if None), bias (auto-determined based on `norm_type` if None), normalization type (`norm_type`), activation function (`act_cls`), weight initialization (`init`), and allows adding an extra layer (`xtra`). It utilizes `_conv_func` for selecting the core convolution operation and supports various normalization techniques like Batch Normalization, Instance Normalization, Weight Normalization, and Spectral Normalization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass ConvLayer(nn.Sequential):\n    \"Create a sequence of convolutional (`ni` to `nf`), ReLU (if `use_activ`) and `norm_type` layers.\"\n    @delegates(nn.Conv2d)\n    def __init__(self, ni, nf, ks=3, stride=1, padding=None, bias=None, ndim=2, norm_type=NormType.Batch, bn_1st=True,\n                 act_cls=defaults.activation, transpose=False, init='auto', xtra=None, bias_std=0.01, **kwargs):\n        if padding is None: padding = ((ks-1)//2 if not transpose else 0)\n        bn = norm_type in (NormType.Batch, NormType.BatchZero)\n        inn = norm_type in (NormType.Instance, NormType.InstanceZero)\n        if bias is None: bias = not (bn or inn)\n        conv_func = _conv_func(ndim, transpose=transpose)\n        conv = conv_func(ni, nf, kernel_size=ks, bias=bias, stride=stride, padding=padding, **kwargs)\n        act = None if act_cls is None else act_cls()\n        init_linear(conv, act, init=init, bias_std=bias_std)\n        if   norm_type==NormType.Weight:   conv = weight_norm(conv)\n        elif norm_type==NormType.Spectral: conv = spectral_norm(conv)\n        layers = [conv]\n        act_bn = []\n        if act is not None: act_bn.append(act)\n        if bn: act_bn.append(BatchNorm(nf, norm_type=norm_type, ndim=ndim))\n        if inn: act_bn.append(InstanceNorm(nf, norm_type=norm_type, ndim=ndim))\n        if bn_1st: act_bn.reverse()\n        layers += act_bn\n        if xtra: layers.append(xtra)\n        super().__init__(*layers)\n```\n\n----------------------------------------\n\nTITLE: Image Utility Functions (Python)\nDESCRIPTION: Provides utility functions for image conversions and loading. `to_image` converts a PyTorch Tensor or NumPy array to a PIL Image (assuming uint8 format or converting float32). `load_image` opens an image file using PIL, ensures its data is loaded, and optionally converts it to a specific mode. `image2tensor` converts a PIL Image or NumPy array into a PyTorch Tensor with dimensions in `c*h*w` order.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef to_image(x):\n    \"Convert a tensor or array to a PIL int8 Image\"\n    if isinstance(x,Image.Image): return x\n    if isinstance(x,Tensor): x = to_np(x.permute((1,2,0)))\n    if x.dtype==np.float32: x = (x*255).astype(np.uint8)\n    return Image.fromarray(x, mode=['RGB','CMYK'][x.shape[0]==4])\n```\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef load_image(fn, mode=None):\n    \"Open and load a `PIL.Image` and convert to `mode`\"\n    im = Image.open(fn)\n    im.load()\n    im = im._new(im.im)\n    return im.convert(mode) if mode else im\n```\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef image2tensor(img):\n    \"Transform image to byte tensor in `c*h*w` dim order.\"\n    res = tensor(img)\n    if res.dim()==2: res = res.unsqueeze(-1)\n    return res.permute(2,0,1)\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Tabular Model Class in FastAI - Python\nDESCRIPTION: Class `TabularModel` defines a basic feedforward neural network for tabular data combining categorical and continuous inputs. It embeds categorical variables using `nn.Embedding` layers with optional dropout, applies optional batch normalization to continuous variables, and stacks multiple `LinBnDrop` layers with configurable activations and dropouts. The final output layer can be optionally constrained by a `SigmoidRange` activation. The model constructor supports detailed configuration including embedding dropout, batch normalization flags, layer sizes, and activation ordering. The forward pass separately processes categorical and continuous inputs, concatenates them, and applies the stacked layers for output. Dependencies include PyTorch and FastAI's utility layers.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/42_tabular.model.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass TabularModel(Module):\n    \"Basic model for tabular data.\"\n    def __init__(self, \n        emb_szs:list, # Sequence of (num_embeddings, embedding_dim) for each categorical variable\n        n_cont:int, # Number of continuous variables\n        out_sz:int, # Number of outputs for final `LinBnDrop` layer\n        layers:list, # Sequence of ints used to specify the input and output size of each `LinBnDrop` layer\n        ps:float|MutableSequence=None, # Sequence of dropout probabilities for `LinBnDrop`\n        embed_p:float=0., # Dropout probability for `Embedding` layer\n        y_range=None, # Low and high for `SigmoidRange` activation \n        use_bn:bool=True, # Use `BatchNorm1d` in `LinBnDrop` layers\n        bn_final:bool=False, # Use `BatchNorm1d` on final layer\n        bn_cont:bool=True, # Use `BatchNorm1d` on continuous variables\n        act_cls=nn.ReLU(inplace=True), # Activation type for `LinBnDrop` layers\n        lin_first:bool=True # Linear layer is first or last in `LinBnDrop` layers\n    ):\n        ps = ifnone(ps, [0]*len(layers))\n        if not is_listy(ps): ps = [ps]*len(layers)\n        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(embed_p)\n        self.bn_cont = nn.BatchNorm1d(n_cont) if bn_cont else None\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb,self.n_cont = n_emb,n_cont\n        sizes = [n_emb + n_cont] + layers + [out_sz]\n        actns = [act_cls for _ in range(len(sizes)-2)] + [None]\n        _layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first)\n                       for i,(p,a) in enumerate(zip(ps+[0.],actns))]\n        if y_range is not None: _layers.append(SigmoidRange(*y_range))\n        self.layers = nn.Sequential(*_layers)\n\n    def forward(self, x_cat, x_cont=None):\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            if self.bn_cont is not None: x_cont = self.bn_cont(x_cont)\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n        return self.layers(x)\n```\n\n----------------------------------------\n\nTITLE: Finding Learning Rate\nDESCRIPTION: This finds the optimal learning rate using `learn.lr_find()`. It sweeps through different learning rates and plots the loss to identify a suitable learning rate range, used for faster and more effective training.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\nlearn.lr_find(end_lr=100)\n```\n\n----------------------------------------\n\nTITLE: Splitting Data into Training/Validation Sets\nDESCRIPTION: This code splits the image files into training and validation sets using `GrandparentSplitter`. It uses pre-defined split names for training and validation. The output is a set of split indices.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nsplits = GrandparentSplitter(train_name='training', valid_name='testing')(items)\n```\n\n----------------------------------------\n\nTITLE: Creating RegressionBlock for float targets in FastAI Python\nDESCRIPTION: This function returns a `TransformBlock` configured for regression tasks, accommodating continuous numerical targets. It accepts an optional parameter specifying the number of output values (`n_out`) and uses the `RegressionSetup` transform for proper data formatting. This block facilitates setting up data pipelines for regression models cleanly.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef RegressionBlock(\n    n_out:int=None, # Number of output values\n):\n    \"`TransformBlock` for float targets\"\n    return TransformBlock(type_tfms=RegressionSetup(c=n_out))\n```\n\n----------------------------------------\n\nTITLE: Creating _inner_loop\nDESCRIPTION: Defines `_inner_loop`, a list of events that occur within the inner training loop (i.e., per batch).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#|export\n_inner_loop = \"before_batch after_pred after_loss before_backward after_cancel_backward after_backward before_step after_step after_cancel_batch after_batch\".split()\n```\n\n----------------------------------------\n\nTITLE: Testing Normalization on New DataFrames - Python\nDESCRIPTION: This code tests the application of a pre-fitted `Normalize` transform on a new DataFrame using the `new` method of `TabularPandas`. It verifies that the normalization is applied correctly using the original means and standard deviations, and also tests the decoding process.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame({'a':[5,6,7]})\nto1 = to.new(df1)\nto1.process()\ntest_close(to1['a'].values, (np.array([5,6,7])-m)/s)\nto2 = norm.decode(to1)\ntest_close(to2['a'].values, [5,6,7])\n```\n\n----------------------------------------\n\nTITLE: Show Top Losses with Predictions and Raw Outputs\nDESCRIPTION: This function visualizes the top loss examples, displaying input images, model predictions, true labels, losses, and raw output scores, arranged in a grid with the title 'Prediction/Actual/Loss/Probability'. Useful for debugging and understanding model failure cases.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_43\n\nLANGUAGE: Python\nCODE:\n```\n@dispatch\ndef plot_top_losses(x: TensorImage, y:TensorCategory, samples, outs, raws, losses, nrows=None, ncols=None, figsize=None, **kwargs):\n    axs = get_grid(len(samples), nrows=nrows, ncols=ncols, figsize=figsize, title='Prediction/Actual/Loss/Probability')\n    for ax,s,o,r,l in zip(axs, samples, outs, raws, losses):\n        s[0].show(ctx=ax, **kwargs)\n        ax.set_title(f'{o[0]}/{s[1]} / {l.item():.2f} / {r.max().item():.2f}')\n```\n\n----------------------------------------\n\nTITLE: Constructing DataBlock for Multi-label Classification - Python\nDESCRIPTION: Builds a fastai DataBlock configured for image inputs and multi-label targets, using column readers for file paths and tags. Includes random train/validation split and a sequence of augmentations plus normalization. Depends on fastai Data API and DataFrame structure; outputs an object for data loading. Key parameters: image path patterns, label delimiter, augmentation transforms.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nplanet = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   get_x=ColReader(0, pref=str(path/\"train-jpg\")+\"/\", suff='.jpg'),\n                   get_y=ColReader(1, label_delim=' '),\n                   splitter=RandomSplitter(seed=42),\n                   batch_tfms=tfms+[Normalize.from_stats(*imagenet_stats)])\n```\n\n----------------------------------------\n\nTITLE: Defining fastai Tokenizer Class (Python)\nDESCRIPTION: Defines the `Tokenizer` class, a fastai `Transform` designed to provide a consistent interface for tokenizing text from DataFrames or folders. It wraps an underlying tokenizer (`tok`) and manages processing rules, token counts, and lengths. It offers class methods (`from_df`, `from_folder`) for convenient initialization based on data source type.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/30_text.core.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass Tokenizer(Transform):\n    \"Provides a consistent `Transform` interface to tokenizers operating on `DataFrame`s and folders\"\n    input_types = (str, list, L, tuple, Path)\n    def __init__(self, tok, rules=None, counter=None, lengths=None, mode=None, sep=' '):\n        if isinstance(tok,type): tok=tok()\n        store_attr('tok,counter,lengths,mode,sep')\n        self.rules = defaults.text_proc_rules if rules is None else rules\n\n    @classmethod\n    @delegates(tokenize_df, keep=True)\n    def from_df(cls, text_cols, tok=None, rules=None, sep=' ', **kwargs):\n        if tok is None: tok = WordTokenizer()\n        res = cls(tok, rules=rules, mode='df')\n        res.kwargs,res.train_setup = merge({'tok': tok}, kwargs),False\n        res.text_cols,res.sep = text_cols,sep\n        default_val = inspect.signature(tokenize_df).parameters['tok_text_col'].default\n        res.tok_text_col = kwargs.get('tok_text_col', default_val)\n        return res\n\n    @classmethod\n    @delegates(tokenize_folder, keep=True)\n    def from_folder(cls, path, tok=None, rules=None, **kwargs):\n        path = Path(path)\n        if tok is None: tok = WordTokenizer()\n        output_dir = tokenize_folder(path, tok=tok, rules=rules, **kwargs)\n        res = cls(tok, counter=load_pickle(output_dir/fn_counter_pkl),\n                  lengths=load_pickle(output_dir/fn_lengths_pkl), rules=rules, mode='folder')\n        res.path,res.output_dir = path,output_dir\n        return res\n\n    def setups(self, dsets):\n        if not self.mode == 'df' or not isinstance(dsets.items, pd.DataFrame): return\n        dsets.items,count = tokenize_df(dsets.items, self.text_cols, rules=self.rules, **self.kwargs)\n        if self.counter is None: self.counter = count\n        if self.lengths is None: self.lengths = dsets.items[f'{self.tok_text_col}_length'].values\n        return dsets\n\n    def encodes(self, o:Path):\n        if self.mode=='folder' and str(o).startswith(str(self.path)):\n            tok = self.output_dir/o.relative_to(self.path)\n            return L(tok.read_text(encoding='UTF-8').split(' '))\n        else: return self._tokenize1(o.read_text())\n\n    def encodes(self, o:str): return self._tokenize1(o)\n    def _tokenize1(self, o): return first(self.tok([compose(*self.rules)(o)]))\n\n    def get_lengths(self, items):\n        if self.lengths is None: return None\n        if self.mode == 'df':\n            if isinstance(items, pd.DataFrame) and f'{self.tok_text_col}_length' in items.columns:\n                return items[f'{self.tok_text_col}_length'].values\n        if self.mode == 'folder':\n            try:\n                res = [self.lengths[str(Path(i).relative_to(self.path))] for i in items]\n                if len(res) == len(items): return res\n            except: return None\n\n    def decodes(self, o): return TitledStr(self.sep.join(o))\n```\n\n----------------------------------------\n\nTITLE: TabularPandas Pipeline with Split Test - Python\nDESCRIPTION: This code tests the application of a `TabularPandas` pipeline with specified data splits.  The `Normalize`, `Categorify`, `FillMissing`, and `noop` transforms are applied, and the code verifies that the categorical, continuous, and target variables are transformed correctly according to the splits.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'a':[0,1,2,1,1,2,0], 'b':[0,np.nan,1,1,2,3,4], 'c': ['b','a','b','a','a','b','a']})\nto = TabularPandas(df, procs, cat_names='a', cont_names='b', y_names='c', splits=[[0,1,4,6], [2,3,5]])\n\ntest_series(to.cat_names, ['a', 'b_na'])\ntest_series(to['a'], [1,2,2,1,0,2,0])\ntest_eq(df.a.dtype, np.int64 if sys.platform == \"win32\" else int)\ntest_series(to['b_na'], [1,2,1,1,1,1,1])\ntest_series(to['c'], [1,0,0,0,1,0,1])\n```\n\n----------------------------------------\n\nTITLE: Unit Testing ToTensor Output Shapes and Types - fastai (Python)\nDESCRIPTION: Applies ToTensor to MNIST images and masks, then uses 'test_eq' to assert correct tensor shapes and types. Demonstrates expected outputs with specific sample dimensions and type checks. Input variables (mnist_img, mask) and functions (test_eq, ToTensor) must be defined. No outputs except assertion exceptions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\ntfm = ToTensor()\ntest_eq(tfm(mnist_img).shape, (1,28,28))\ntest_eq(type(tfm(mnist_img)), TensorImageBW)\ntest_eq(tfm(mask).shape, (96,128))\ntest_eq(type(tfm(mask)), TensorMask)\n```\n\n----------------------------------------\n\nTITLE: Test SimpleCNN Custom Strides in Python\nDESCRIPTION: This test verifies that the `SimpleCNN` correctly applies custom strides specified during initialization. It constructs a `SimpleCNN` with custom stride values and then asserts that the stride property of the convolutional layers in the model matches the input strides, confirming the customization is applied.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_66\n\nLANGUAGE: python\nCODE:\n```\ntst = SimpleCNN([8,16,32], strides=[1,2])\nmods = list(tst.children())\ntest_eq([m[0].stride for m in mods[:2]], [(1,1),(2,2)])\n```\n\n----------------------------------------\n\nTITLE: Plotting Training and Validation Loss with Recorder in fastai (Python)\nDESCRIPTION: Demonstrates usage of the Recorder's plot_loss method, plotting loss curves starting from a specific step (skip_start=1). It is intended for use within a Jupyter notebook or interactive Python environment, with an instance of a Learner featuring an attached Recorder. Input is a configured Learner; output is a matplotlib graph.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_74\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nlearn.recorder.plot_loss(skip_start=1)\n```\n\n----------------------------------------\n\nTITLE: Saving PyTorch/Fastai Model and Optimizer State to File in Python\nDESCRIPTION: Defines 'save_model' to persist a model's parameters and optionally optimizer state to disk, supporting both Path and file-like targets. Uses PyTorch's save function and customizable pickle protocol options. Handles distributed training nuances and suppresses redundant saves on child processes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef save_model(file, model, opt, with_opt=True, pickle_protocol=2, **torch_save_kwargs):\n    \"Save `model` to `file` along with `opt` (if available, and if `with_opt`)\"\n    if rank_distrib(): return # don't save if child proc\n    if opt is None: with_opt=False\n    state = get_model(model).state_dict()\n    if with_opt: state = {'model': state, 'opt':opt.state_dict()}\n    torch.save(state, file, pickle_protocol=pickle_protocol, **torch_save_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Full Fine-tuning on 256x256 Images with fit_one_cycle - Python\nDESCRIPTION: Runs full fine-tuning for five epochs with a decreasing learning rate slice (1e-5 to lr/5) on the large-image DataLoaders, improving generalization. Outputs improved metrics and final model state.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(5, slice(1e-5, lr/5))\n```\n\n----------------------------------------\n\nTITLE: Implementing SelfAttention Layer\nDESCRIPTION: Defines a self-attention layer using PyTorch modules. It initializes the query, key, and value transformations using 1x1 convolutions with spectral normalization. The forward pass calculates attention weights using softmax and applies them to the value tensor, incorporating a trainable `gamma` parameter for residual connections.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_42\n\nLANGUAGE: Python\nCODE:\n```\nclass SelfAttention(Module):\n    \"Self attention layer for `n_channels`.\"\n    def __init__(self, n_channels):\n        self.query,self.key,self.value = [self._conv(n_channels, c) for c in (n_channels//8,n_channels//8,n_channels)]\n        self.gamma = nn.Parameter(tensor([0.]))\n\n    def _conv(self,n_in,n_out):\n        return ConvLayer(n_in, n_out, ks=1, ndim=1, norm_type=NormType.Spectral, act_cls=None, bias=False)\n\n    def forward(self, x):\n        #Notation from the paper.\n        size = x.size()\n        x = x.view(*size[:2],-1)\n        f,g,h = self.query(x),self.key(x),self.value(x)\n        beta = F.softmax(torch.bmm(f.transpose(1,2), g), dim=1)\n        o = self.gamma * torch.bmm(h, beta) + x\n        return o.view(*size).contiguous()\n```\n\n----------------------------------------\n\nTITLE: Recursively Moving PyTorch Tensors to a Device in fastai\nDESCRIPTION: Defines the `to_device` function that recursively traverses nested data structures (like lists, tuples, dictionaries) and moves any encountered PyTorch `Tensor` objects to the specified target `device`. If no `device` is provided, it defaults to the device determined by `default_device()`. It respects the `defaults.use_cuda=False` setting to force CPU usage. The `non_blocking` flag can be passed to the underlying `tensor.to()` call.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef to_device(b, device=None, non_blocking=False):\n    \"Recursively put `b` on `device`.\"\n    if defaults.use_cuda==False: device='cpu'\n    elif device is None: device=default_device()\n    def _inner(o):\n        # ToDo: add TensorDict when released\n        if isinstance(o,Tensor): return o.to(device, non_blocking=non_blocking)\n        return o\n    return apply(_inner, b)\n```\n\n----------------------------------------\n\nTITLE: Defining Image TransformBlock for fastai Vision in Python\nDESCRIPTION: Defines `ImageBlock`, a TransformBlock factory function specialized for images in fastai. It accepts an optional PIL image class (defaulting to `PILImage`) and returns a TransformBlock configured with creation and batch transformation methods suitable for images. The `type_tfms` uses the class's `create` method to instantiate images from raw data, while `batch_tfms` applies `IntToFloatTensor` transformation to convert integer images to float tensors typically used for model input.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/08_vision.data.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef ImageBlock(cls:PILBase=PILImage):\n    \"A `TransformBlock` for images of `cls`\"\n    return TransformBlock(type_tfms=cls.create, batch_tfms=IntToFloatTensor)\n```\n\n----------------------------------------\n\nTITLE: Implementing AccumMetric Metric Class - Python\nDESCRIPTION: Defines the AccumMetric class, which stores model predictions and targets on the CPU, accumulating them across batches for final aggregation using a user-provided function. Handles softmax/sigmoid activations, argmax, thresholding, conversion to numpy, and argument inversion as needed. Key parameters include func, dim_argmax, activation, thresh, to_np, invert_arg, flatten, and name; outputs per-epoch or per-validation metrics. Dependencies: torch, fastai. Limitations: func signature and activation/thresh settings must match task.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nclass AccumMetric(Metric):\n    \"Stores predictions and targets on CPU in accumulate to perform final calculations with `func`.\"\n    def __init__(self, func, dim_argmax=None, activation=ActivationType.No, thresh=None, to_np=False,\n                 invert_arg=False, flatten=True, name=None, **kwargs):\n        store_attr('func,dim_argmax,activation,thresh,flatten')\n        self._name = ifnone(name, self.func.func.__name__ if hasattr(self.func, 'func') else  self.func.__name__)\n        self.to_np,self.invert_args,self.kwargs = to_np,invert_arg,kwargs\n\n    def reset(self):\n        \"Clear all targs and preds\"\n        self.targs,self.preds = [],[]\n\n    def accumulate(self, learn):\n        \"Store targs and preds from `learn`, using activation function and argmax as appropriate\"\n        pred = learn.pred\n        if self.activation in [ActivationType.Softmax, ActivationType.BinarySoftmax]:\n            pred = F.softmax(pred, dim=self.dim_argmax)\n            if self.activation == ActivationType.BinarySoftmax: pred = pred[:, -1]\n        elif self.activation == ActivationType.Sigmoid: pred = torch.sigmoid(pred)\n        elif self.dim_argmax: pred = pred.argmax(dim=self.dim_argmax)\n        if self.thresh:  pred = (pred >= self.thresh)\n        self.accum_values(pred,learn.y,learn)\n\n    def accum_values(self, preds, targs,learn=None):\n        \"Store targs and preds\"\n        to_d = learn.to_detach if learn is not None else to_detach\n        preds,targs = to_d(preds),to_d(targs)\n        if self.flatten: preds,targs = flatten_check(preds,targs)\n        self.preds.append(preds)\n        self.targs.append(targs)\n\n    def __call__(self, preds, targs):\n        \"Calculate metric on one batch of data\"\n        self.reset()\n        self.accum_values(preds,targs)\n        return self.value\n\n    @property\n    def value(self):\n        \"Value of the metric using accumulated preds and targs\"\n        if len(self.preds) == 0: return\n        preds,targs = torch.cat(self.preds),torch.cat(self.targs)\n        if self.to_np: preds,targs = preds.numpy(),targs.numpy()\n        return self.func(targs, preds, **self.kwargs) if self.invert_args else self.func(preds, targs, **self.kwargs)\n\n    @property\n    def name(self):  return self._name\n\n    @name.setter\n    def name(self, value): self._name = value\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders from DataBlock\nDESCRIPTION: This creates `DataLoaders` using the `imdb` datablock and the provided `path`.  It converts the datablock blueprint into concrete `DataLoaders` suitable for training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndls = imdb.dataloaders(path)\n```\n\n----------------------------------------\n\nTITLE: Finding Optimal Learning Rate Using lr_find - Python\nDESCRIPTION: Runs fastai's learning rate finder to plot a suggested learning rate range for training. Input: the learner object. Output: a plot (in notebook) showing loss vs learning rate. No returned value; requires fastai.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nlearn.lr_find()\n```\n\n----------------------------------------\n\nTITLE: Preparing Image Data Loaders and Training Vision Model - FastAI Python\nDESCRIPTION: Sets up image files from the Oxford-IIIT Pets dataset and creates data loaders with an 80/20 training/validation split. Defines a custom label function that labels images based on whether the file name starts with an uppercase letter. Includes a Windows notebook optimization to reduce samples for faster testing. Creates an ImageDataLoaders object with resizing transformations and instantiates a ResNet18 learner. Fits the model for one epoch and retrieves optimizer state to inspect average gradients. Dependencies include fastai.vision.all and torch backends.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n#|cuda\nset_seed(99, True)\npath = untar_data(URLs.PETS)/'images'\n\nimage_files = get_image_files(path)\nif sys.platform == \"win32\" and IN_NOTEBOOK:\n    image_files = random.choices(image_files, k=int(len(image_files)/8))\n    print(\"Randomly select 1/8 files in NOTEBOOK on Windows to save time\")\n\n# pickle can\\'t serializer lamda function.\ndef _label_func(x):\n    return x[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(\n    path, image_files, valid_pct=0.2,\n    label_func=_label_func, item_tfms=Resize(224))\n\nlearn = vision_learner(dls, resnet18)\nlearn.fit(1)\nlearn.opt.state_dict()['state'][1]['grad_avg']\n```\n\n----------------------------------------\n\nTITLE: Subclassing Learner for NLP with Fastai TextLearner - Python\nDESCRIPTION: Defines the TextLearner class, a subclass of fastai's Learner, specialized for text and NLP models. Adds callbacks such as ModelResetter and RNNRegularizer, and provides save_encoder, load_encoder, load_pretrained, and load utilities for serializing and restoring models and embeddings. Inputs: DataLoaders, model, alpha, beta, momentum schedule, and additional kwargs. Outputs: None instantiated class or modified TextLearner. Dependencies: fastai, torch. Limitations: tightly coupled to fastai APIs and specific neural net conventions. Handles multiple devices, distributed settings, and backward compatibility.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@delegates(Learner.__init__)\nclass TextLearner(Learner):\n    \"Basic class for a `Learner` in NLP.\"\n    def __init__(self, \n        dls:DataLoaders, # Text `DataLoaders`\n        model, # A standard PyTorch model\n        alpha:float=2., # Param for `RNNRegularizer`\n        beta:float=1., # Param for `RNNRegularizer`\n        moms:tuple=(0.8,0.7,0.8), # Momentum for `Cosine Annealing Scheduler`\n        **kwargs\n    ):\n        super().__init__(dls, model, moms=moms, **kwargs)\n        self.add_cbs(rnn_cbs())\n\n    def save_encoder(self, \n        file:str # Filename for `Encoder` \n    ):\n        \"Save the encoder to `file` in the model directory\"\n        if rank_distrib(): return # don't save if child proc\n        encoder = get_model(self.model)[0]\n        if hasattr(encoder, 'module'): encoder = encoder.module\n        torch.save(encoder.state_dict(), join_path_file(file, self.path/self.model_dir, ext='.pth'))\n\n    def load_encoder(self, \n        file:str, # Filename of the saved encoder \n        device:int|str|torch.device=None # Device used to load, defaults to `dls` device\n    ):\n        \"Load the encoder `file` from the model directory, optionally ensuring it's on `device`\"\n        encoder = get_model(self.model)[0]\n        if device is None: device = self.dls.device\n        if hasattr(encoder, 'module'): encoder = encoder.module\n        distrib_barrier()\n        wgts = torch.load(join_path_file(file,self.path/self.model_dir, ext='.pth'), map_location=device)\n        encoder.load_state_dict(clean_raw_keys(wgts))\n        self.freeze()\n        return self\n\n    def load_pretrained(self, \n        wgts_fname:str, # Filename of saved weights \n        vocab_fname:str, # Saved vocabulary filename in pickle format\n        model=None # Model to load parameters from, defaults to `Learner.model`\n    ):\n        \"Load a pretrained model and adapt it to the data vocabulary.\"\n        old_vocab = load_pickle(vocab_fname)\n        new_vocab = _get_text_vocab(self.dls)\n        distrib_barrier()\n        wgts = torch.load(wgts_fname, map_location = lambda storage,loc: storage)\n        if 'model' in wgts: wgts = wgts['model'] #Just in case the pretrained model was saved with an optimizer\n        wgts = match_embeds(wgts, old_vocab, new_vocab)\n        load_ignore_keys(self.model if model is None else model, clean_raw_keys(wgts))\n        self.freeze()\n        return self\n\n    #For previous versions compatibility. Remove at release\n    @delegates(load_model_text)\n    def load(self, \n        file:str, # Filename of saved model \n        with_opt:bool=None, # Enable to load `Optimizer` state\n        device:int|str|torch.device=None, # Device used to load, defaults to `dls` device\n        **kwargs\n    ):\n        if device is None: device = self.dls.device\n        if self.opt is None: self.create_opt()\n        file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n        load_model_text(file, self.model, self.opt, device=device, **kwargs)\n        return self\n```\n\n----------------------------------------\n\nTITLE: Upgrade fastai on Colab\nDESCRIPTION: This code snippet checks if the current environment is Google Colab and, if so, upgrades the fastai library to the latest version using pip.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders from DataBlock and DataFrame - Python\nDESCRIPTION: Creates DataLoaders for training and validation using the DataBlock and DataFrame of labels, with a specified batch size and data path. Outputs a DataLoaders object used in the training loop. Prerequisites: DataBlock properly configured, DataFrame loaded, fastai installed.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndls = planet.dataloaders(df, bs=64, path=path)\n```\n\n----------------------------------------\n\nTITLE: Create Datasets for Language Modeling\nDESCRIPTION: This code creates a `Datasets` object for language modeling using fastai's data pipeline.  It splits the DataFrame into training and validation sets, defines a transformation pipeline consisting of extracting the text, tokenizing using fastai's tokenizer, and numericalizing the tokens. The data is then prepared for language modeling with `LMDataLoader`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/35_tutorial.wikitext.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsplits = [list(range_of(df_train)), list(range(len(df_train), len(df_all)))]\ntfms = [attrgetter(\"text\"), Tokenizer.from_df(0), Numericalize()]\ndsets = Datasets(df_all, [tfms], splits=splits, dl_type=LMDataLoader)\n```\n\n----------------------------------------\n\nTITLE: Creating Language Model from Architecture and Configuration in Python\nDESCRIPTION: Function get_language_model constructs a language model by combining a provided architecture (such as AWD_LSTM) encoder and a LinearDecoder that acts as a decoder. It takes model vocabulary size, optional configuration dictionary, and a dropout multiplier to scale dropout probabilities. Supports tying decoder weights with encoder embeddings and optionally applying a specified initialization method. Returns a SequentialRNN combining encoder and decoder modules.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef get_language_model(\n    arch, # Function or class that can generate a language model architecture\n    vocab_sz:int, # Size of the vocabulary\n    config:dict=None, # Model configuration dictionary\n    drop_mult:float=1. # Multiplicative factor to scale all dropout probabilities in `config`\n) -> SequentialRNN: # Language model with `arch` encoder and linear decoder\n    \"Create a language model from `arch` and its `config`.\"\n    meta = _model_meta[arch]\n    config = ifnone(config, meta['config_lm']).copy()\n    for k in config.keys():\n        if k.endswith('_p'): config[k] *= drop_mult\n    tie_weights,output_p,out_bias = map(config.pop, ['tie_weights', 'output_p', 'out_bias'])\n    init = config.pop('init') if 'init' in config else None\n    encoder = arch(vocab_sz, **config)\n    enc = encoder.encoder if tie_weights else None\n    decoder = LinearDecoder(vocab_sz, config[meta['hid_name']], output_p, tie_encoder=enc, bias=out_bias)\n    model = SequentialRNN(encoder, decoder)\n    return model if init is None else model.apply(init)\n```\n\n----------------------------------------\n\nTITLE: Basic Usage and Assignment with DataLoaders (fastai, Python)\nDESCRIPTION: Shows how to instantiate DataLoaders, fetch a batch, compare batches and test assignment by updating the train dataloader with a new batch size. These examples validate DataLoaders' interface and train/valid assignment logic. Requires the DataLoaders and TfmdDL classes and test_eq/assert utilities.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndls = DataLoaders(tdl,tdl)\nx = dls.train.one_batch()\nx2 = first(tdl)\ntest_eq(x,x2)\nx2 = dls.one_batch()\ntest_eq(x,x2)\n```\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#test assignment works\ndls.train = dls.train.new(bs=4)\n```\n\n----------------------------------------\n\nTITLE: Exporting a Learner for Inference\nDESCRIPTION: This code defines the `export` method for a `Learner`, responsible for preparing the `Learner` object for deployment and inference. It removes the optimizer state, the dataset and the items, and saves the model's state with its parameters to a file, usually a `.pkl` file. The method utilizes `torch.save`. It restores the optimizer after the serialization. This is used for the export step, preparing the model for use without the training environment and only for inference.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\n@patch\ndef export(self:Learner, fname='export.pkl', pickle_module=cloudpickle, pickle_protocol=2):\n    \"Export the content of `self` without the items and the optimizer state for inference\"\n    if rank_distrib(): return # don't export if child proc\n    self._end_cleanup()\n    old_dbunch = self.dls\n    self.dls = self.dls.new_empty()\n    state = self.opt.state_dict() if self.opt is not None else None\n    self.opt = None\n    with warnings.catch_warnings():\n        #To avoid the warning that come from PyTorch about model not being checked\n        warnings.simplefilter(\"ignore\")\n        torch.save(self, self.path/fname, pickle_module=pickle_module, pickle_protocol=pickle_protocol)\n    self.create_opt()\n    if state is not None: self.opt.load_state_dict(state)\n    self.dls = old_dbunch\n```\n\n----------------------------------------\n\nTITLE: Managing RNN Outputs with RNNCallback in FastAI Python\nDESCRIPTION: Defines a callback that processes the outputs of an RNN after prediction. Specifically, it saves the raw outputs, dropped-out outputs, and extracts the actual output used for loss computation. This supports handling the multiple outputs that language models produce, ensuring that only the relevant outputs are used for loss calculations. It relies on the standard FastAI Callback structure and expects predictions to be potentially list-like objects.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/34_callback.rnn.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass RNNCallback(Callback):\n    \"Save the raw and dropped-out outputs and only keep the true output for loss computation\"\n    def after_pred(self): self.learn.pred,self.raw_out,self.out = [o[-1] if is_listy(o) else o for o in self.pred]\n```\n\n----------------------------------------\n\nTITLE: Freezing Layers in Learner - fastai - Python\nDESCRIPTION: These patched methods for the `Learner` class enable freezing and unfreezing layers of a neural network for transfer learning. `freeze_to` freezes layers up to a given index, `freeze` freezes all but the last layer group, and `unfreeze` unfreezes all layers. The methods also clear the optimizer state after freezing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_75\n\nLANGUAGE: python\nCODE:\n```\n@patch\ndef freeze_to(self:Learner, n):\n    if self.opt is None: self.create_opt()\n    self.opt.freeze_to(n)\n    self.opt.clear_state()\n\n@patch\ndef freeze(self:Learner): self.freeze_to(-1)\n\n@patch\ndef unfreeze(self:Learner): self.freeze_to(0)\n\nadd_docs(Learner,\n         freeze_to=\"Freeze parameter groups up to `n`\",\n         freeze=\"Freeze up to last parameter group\",\n         unfreeze=\"Unfreeze the entire model\")\n```\n\n----------------------------------------\n\nTITLE: Defining Loss Function - PyTorch/Python\nDESCRIPTION: Initializes the cross-entropy loss function (`nn.CrossEntropyLoss`) from PyTorch. This loss function is commonly used for multi-class classification problems and combines LogSoftmax and NLLLoss.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nloss_func = nn.CrossEntropyLoss()\n```\n\n----------------------------------------\n\nTITLE: Unit Testing Embedding Remapping (No Bias) - Python\nDESCRIPTION: Tests match_embeds with random embedding weights and vocabularies, ensuring that the remapping works for known words, reordered words, and unknown words using numpy/test_eq assertions. Dependency: torch, test_eq, assumes function match_embeds is defined above. Inputs: random weight dictionary and vocab lists. No output unless assertion fails. Designed for quick in-memory validation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwgts = {'0.encoder.weight': torch.randn(5,3)}\nnew_wgts = match_embeds(wgts.copy(), ['a', 'b', 'c'], ['a', 'c', 'd', 'b'])\nold,new = wgts['0.encoder.weight'],new_wgts['0.encoder.weight']\ntest_eq(new[0], old[0])\ntest_eq(new[1], old[2])\ntest_eq(new[2], old.mean(0))\ntest_eq(new[3], old[1])\n```\n\n----------------------------------------\n\nTITLE: Implementing Jaccard Score Wrapper for Multi-label Classification in Python\nDESCRIPTION: A wrapper function that adapts scikit-learn's jaccard_score for use in fastai's multi-label classification tasks, with options for threshold, activation type, and averaging method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef JaccardMulti(thresh=0.5, sigmoid=True, labels=None, pos_label=1, average='macro', sample_weight=None):\n    \"Jaccard score for multi-label classification problems\"\n    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n    return skm_to_fastai(skm.jaccard_score, thresh=thresh, activation=activation, flatten=False,\n                         labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight)\n```\n\n----------------------------------------\n\nTITLE: Printing Model Summary for the Resnet-ish Model\nDESCRIPTION: This prints the summary of the ResNet-ish model to inspect its architecture and verify its parameters. This verifies the model's structure and the number of parameters.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_38\n\nLANGUAGE: Python\nCODE:\n```\nprint(learn.summary())\n```\n\n----------------------------------------\n\nTITLE: Suggesting Learning Rate by Minimum Loss in Python\nDESCRIPTION: The minimum function suggests a learning rate that is one-tenth the value corresponding to the minimum loss seen before divergence in the learning rate range. It finds the minimal loss index and returns the scaled-down learning rate along with the loss index. This algorithm is simple and useful for conservative learning rate selection.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ndef minimum(lrs:list, losses:list, num_it:int):\n    \"Suggests a learning rate one-tenth the minumum before divergance and returns its index\"\n    lr_min = lrs[losses.argmin()].item()\n    loss_idx = losses[min(range(len(lrs)), key=lambda i: abs(lrs[i]-lr_min))]\n    return lr_min/10, (lr_min, loss_idx)\n```\n\n----------------------------------------\n\nTITLE: GradientAccumulation - Accumulating Gradients over Multiple Batches\nDESCRIPTION: This callback accumulates gradients over several batches before updating model weights. It reduces memory pressure and allows larger effective batch sizes on limited hardware by controlling gradient updates after multiple steps.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18a_callback.training.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nclass GradientAccumulation(Callback):\n    \"Accumulate gradients before updating weights\"\n    order,run_valid = MixedPrecision.order-4,False\n    def __init__(self, n_acc=32): store_attr()\n    def before_fit(self): self.count=0\n    def after_loss(self): self.learn.loss_grad /= self.n_acc/find_bs(self.learn.yb)\n    def before_step(self):\n        \"Skip weight update if we have not seen enough items\" \n        self.learn.loss_grad *= self.n_acc/find_bs(self.learn.yb) # log correct loss\n        self.count += find_bs(self.learn.yb)\n        if self.count<self.n_acc: raise CancelBatchException() # skip step/zero_grad\n        else: self.count=0\n```\n\n----------------------------------------\n\nTITLE: Flatten Layer for fastai Models - Python\nDESCRIPTION: Builds a flattening layer that flattens a tensor to 2D (batch, -1) or 1D if 'full' is True. Uses the @module decorator, accepts an optional 'full' argument. Takes tensor input, returns reshaped tensor useful for transitioning from convolutional to dense layers in a network.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@module(full=False)\ndef Flatten(self, x):\n    \"Flatten `x` to a single dimension, e.g. at end of a model. `full` for rank-1 tensor\"\n    return x.view(-1) if self.full else x.view(x.size(0), -1)  # Removed cast to Tensorbase\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders for Language Modeling with fastai (Python)\nDESCRIPTION: Defines batch size and sequence length, then creates fastai DataLoaders from a TfmdLists object using the LMDataLoader class (suited for language modeling). Batch size (bs) and sequence length (sl) should be tuned based on available GPU resources; outputs training and validation DataLoader objects.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nbs,sl = 4,256\ndls = tls.dataloaders(bs=bs, seq_len=sl)\n```\n\n----------------------------------------\n\nTITLE: Training a fastai Learner in Python\nDESCRIPTION: This code creates a `Learner` object, defining the optimizer, the model, the loss function, and the metrics.  It then fits the model using `fit_one_cycle` for 1 cycle and a learning rate of 0.01. This example showcases the complete training setup, illustrating the ease of use after converting Ignite code to a fastai module.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_ignite.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nopt_func = partial(SGD, momentum=0.5)\nlearn = Learner(data, Net(), loss_func=nn.NLLLoss(), opt_func=opt_func, metrics=accuracy)\nlearn.fit_one_cycle(1, 0.01)\n```\n\n----------------------------------------\n\nTITLE: Using Gradient Clipping to Prevent Divergence\nDESCRIPTION: Shows that adding the GradientClip callback with a maximum norm prevents training divergence caused by high learning rates, even in mixed precision scenarios.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18a_callback.training.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfp16 = MixedPrecision()\n\nset_seed(99)\nlearn = synth_learner(lr=1.1, cuda=True)\nlearn.fit(3, cbs=fp16)\n```\n\nLANGUAGE: Python\nCODE:\n```\nset_seed(99)\nlearn = synth_learner(lr=1.1, cuda=True)\nlearn.fit(3, cbs=[GradientClip,fp16])\n```\n\n----------------------------------------\n\nTITLE: Fine-tune Language Model\nDESCRIPTION: Fine-tunes the language model for one cycle using `fit_one_cycle`. It specifies the number of epochs, the maximum learning rate, and the momentum values.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7,0.8))\n```\n\n----------------------------------------\n\nTITLE: Initializing Learner with Callbacks\nDESCRIPTION: This code snippet demonstrates how to initialize a `Learner` object with callbacks. It defines a custom callback `TstCallback` and then initializes a `Learner` instance with and without this callback. It checks that the callbacks are added to the Learner correctly, using `test_eq` to check the length of `cbs` and  `assert hasattr` to check the existence of attributes on the Learner.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nclass TstCallback(Callback):\n    def batch_begin(self): self.learn.a = self.a + 1\n\ntst_learn = synth_learner()\ntest_eq(len(tst_learn.cbs), 1)\nassert hasattr(tst_learn, ('train_eval'))\n\ntst_learn = synth_learner(cbs=TstCallback())\ntest_eq(len(tst_learn.cbs), 2)\nassert hasattr(tst_learn, ('tst'))\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Negation Transform and Collecting a Batch (fastai, Python)\nDESCRIPTION: Defines a NegTfm (negation transform), applies it as an after_batch transform, and fetches one batch from the dataloader. Validates the output tensor values are correctly negated. Serves as an example for applying custom transforms in the fastai data pipeline.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntfm = NegTfm()\ntdl = TfmdDL(start, after_batch=tfm, bs=4)\n```\n\nLANGUAGE: python\nCODE:\n```\nb = tdl.one_batch()\ntest_eq(tensor([0,-1,-2,-3]), b)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Callback Instantiation\nDESCRIPTION: Demonstrates another way to define a callback by passing a function as the event to the constructor. This shows an alternative to subclassing, offering flexibility when handling events.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef cb(self): return \"maybe\"\n_t = Callback(before_fit=cb)\ntest_eq(_t(event.before_fit), \"maybe\")\n```\n\n----------------------------------------\n\nTITLE: TabularPandas Pipeline with Target Test - Python\nDESCRIPTION: This code tests a `TabularPandas` pipeline with target variables. It applies `Normalize`, `Categorify`, `FillMissing`, and `noop` to a DataFrame, ensuring that the categorical, continuous, and target variables are transformed correctly. It also checks the classes and vocabulary.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n#Test apply on y_names\ndf = pd.DataFrame({'a':[0,1,2,1,1,2,0], 'b':[0,1,np.nan,1,2,3,4], 'c': ['b','a','b','a','a','b','a']})\nto = TabularPandas(df, procs, 'a', 'b', y_names='c')\n\ntest_series(to.cat_names, ['a', 'b_na'])\ntest_series(to['a'], [1,2,3,2,2,3,1])\ntest_series(to['b_na'], [1,1,2,1,1,1,1])\ntest_series(to['c'], [1,0,1,0,0,1,0])\nx = np.array([0,1,1.5,1,2,3,4])\nm,s = x.mean(),x.std()\ntest_close(to['b'].values, (x-m)/s)\ntest_eq(to.classes, {'a': ['#na#',0,1,2], 'b_na': ['#na#',False,True]})\ntest_eq(to.vocab, ['a','b'])\n```\n\n----------------------------------------\n\nTITLE: Loading Pretrained GPT-2 Model and Tokenizer (Python)\nDESCRIPTION: This code loads the pretrained GPT-2 tokenizer and language model using HuggingFace transformers. The 'pretrained_weights' parameter specifies the model version (default is 'gpt2'). The tokenizer prepares input data, while the model is used for generating or fine-tuning language model outputs. Both components are central dependencies for subsequent NLP tasks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npretrained_weights = 'gpt2'\ntokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\nmodel = GPT2LMHeadModel.from_pretrained(pretrained_weights)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions on New Text\nDESCRIPTION: This demonstrates how to use the trained `Learner` to make predictions on a new text input.  It uses the `predict` method to classify the sentiment of the provided review, outputting both the predicted class and class probabilities.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlearn.predict(\"I really liked that movie!\")\n```\n\n----------------------------------------\n\nTITLE: Illustrating Correct Attribute Modification\nDESCRIPTION: Demonstrates the correct way to modify an attribute of the learner inside a callback. This involves using `self.learn.attribute_name`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass TstCallback(Callback):\n    def batch_begin(self): self.learn.a = self.a + 1\n\nlearn,cb = TstLearner(1),TstCallback()\ncb.learn = learn\ncb('batch_begin')\ntest_eq(cb.learn.a, 2)\n```\n\n----------------------------------------\n\nTITLE: Setting Number of Output Variables - Python\nDESCRIPTION: Sets the number of output variables for the model. `dls.c = 1` indicates a single output, which is appropriate for regression tasks like predicting sales.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndls.c = 1\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export for Data Core - Python\nDESCRIPTION: This line sets the default export for the module to 'data.core'. This influences how the module is organized and exported when using the nbdev library, helping to manage module structure within the fastai project. It has no specific parameters or outputs, and its main function is to organize module exports within the project.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp data.core\n```\n\n----------------------------------------\n\nTITLE: Recursively Finding Batch Size from Tensor or Container in Python\nDESCRIPTION: Recursively extracts the batch size from a given batch 'b' by finding an element with a shape attribute and returning its size in dimension 0. If the element lacks a shape (e.g., list), returns the length of the container. Useful for dynamical batch dimension inference.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_69\n\nLANGUAGE: python\nCODE:\n```\ndef find_bs(b):\n    \"Recursively search the batch size of `b`.\"\n    res = item_find(b)\n    if not hasattr(res, \"shape\"): return len(b)\n    return res.shape[0]\n```\n\n----------------------------------------\n\nTITLE: Define Regression Model\nDESCRIPTION: Defines a simple linear regression model using PyTorch's `Module` class. This model takes a single input `x` and applies a linear transformation using learnable parameters `a` and `b`. It is used in conjunction with the `synth_learner` function for easy model training and validation testing, facilitating the assessment of model behavior in different scenarios.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/97_test_utils.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass RegModel(Module):\n    def __init__(self): self.a,self.b = nn.Parameter(torch.randn(1)),nn.Parameter(torch.randn(1))\n    def forward(self, x): return x*self.a + self.b\n```\n\n----------------------------------------\n\nTITLE: Apply Pipeline with Logging\nDESCRIPTION: This function applies a pipeline of transformations to a given input `x`. It prints the pipeline and the initial value of `x`, then iterates through each transformation in the pipeline, applying it to `x`.  It logs each transformation applied and the resulting value of `x`. If an exception occurs during the application of a transformation, it prints an error message and re-raises the exception.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef _apply_pipeline(p, x):\n    print(f\"  {p}\\n    starting from\\n      {_short_repr(x)}\")\n    for f in p.fs:\n        name = f.name\n        try:\n            x = f(x)\n            if name != \"noop\": print(f\"    applying {name} gives\\n      {_short_repr(x)}\")\n        except Exception as e:\n            print(f\"    applying {name} failed.\")\n            raise e\n    return x\n```\n\n----------------------------------------\n\nTITLE: Training with Last Three Layers Unfrozen (Python)\nDESCRIPTION: Unfreezes the last three parameter groups of the model using `freeze_to(-3)`. Continues training for one epoch using `fit_one_cycle` with discriminative learning rates (`slice(5e-3/(2.6**4),5e-3)`), allowing more layers to adapt to the classification task.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n```\n\n----------------------------------------\n\nTITLE: Defining Embedding Layer with Truncated Normal Init (Python)\nDESCRIPTION: Defines a custom `Embedding` class that inherits from `torch.nn.Embedding`. It initializes the embedding weights using the `trunc_normal_` function with a specified standard deviation (`std`), defaulting to 0.01. This ensures the initial embedding weights are bounded.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass Embedding(nn.Embedding):\n    \"Embedding layer with truncated normal initialization\"\n    def __init__(self, ni, nf, std=0.01):\n        super().__init__(ni, nf)\n        trunc_normal_(self.weight.data, std=std)\n```\n\n----------------------------------------\n\nTITLE: Sample Notebook Distributed Training with notebook_launcher - Python\nDESCRIPTION: Illustrates how to structure a training function using fastai, combine it with learn.distrib_ctx context manager, and launch distributed notebook training with notebook_launcher from Accelerate. Integrates fastai vision_learner, sets up data and metrics, and performs fine-tuning in a distributed-accelerated context. Dependencies: fastai.vision, fastai.distributed, accelerate.notebook_launcher.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\nfrom fastai.vision.all import *\nfrom fastai.distributed import *\n\ndef train():\n    set_seed(99, True)\n    path = untar_data(URLs.PETS)/'images'\n    dls = ImageDataLoaders.from_name_func(\n        path, get_image_files(path), valid_pct=0.2,\n        label_func=lambda x: x[0].isupper(), item_tfms=Resize(224))\n    \n    learn = vision_learner(dls, resnet34, metrics=error_rate).to_fp16()\n    with learn.distrib_ctx(in_notebook=True):\n        learn.fine_tune(1)\nfrom accelerate import notebook_launcher\nnotebook_launcher(train, num_processes=2)\n```\n\n----------------------------------------\n\nTITLE: Downloading and listing model weights\nDESCRIPTION: This code demonstrates downloading pre-trained model weights using `untar_data` and then listing the files in the downloaded directory. It assumes `URLs.WT103_BWD` is a predefined constant pointing to the URL for the desired weights.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/04_data.external.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.WT103_BWD)\npath.ls()\n```\n\n----------------------------------------\n\nTITLE: Defining Initial Training Function (Python)\nDESCRIPTION: Defines a basic `train` function that encapsulates the data loading, model creation, and training steps using fastai. It sets up `ImageDataLoaders` for the PETS dataset, creates a `vision_learner` with a ResNet34 model, enables mixed-precision training (`to_fp16`), and calls `fine_tune` for one epoch. This version does not yet include distributed training specifics.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20b_tutorial.distributed.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndef get_y(o): return o[0].isupper()\ndef train(path):\n    dls = ImageDataLoaders.from_name_func(\n        path, get_image_files(path), valid_pct=0.2,\n        label_func=get_y, item_tfms=Resize(224))\n    learn = vision_learner(dls, resnet34, metrics=error_rate).to_fp16()\n    learn.fine_tune(1)\n```\n\n----------------------------------------\n\nTITLE: Creating Synthetic DataLoaders and a Simple Regression Model in Fastai (Python)\nDESCRIPTION: Defines 'synth_dbunch' to generate synthetic regression datasets and associated DataLoaders, either using Fastai or raw PyTorch, with adjustable batch size, device, and loader type. Introduces 'RegModel', a simple linear regression nn.Module for toy experiments. Relies on torch, fastai, and proper device configuration if CUDA is used.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef synth_dbunch(a=2, b=3, bs=16, n_train=10, n_valid=2, cuda=False, tfmdDL=True):\n    \"A simple dataset where `x` is random and `y = a*x + b` plus some noise.\"\n    def get_data(n):\n        x = torch.randn(int(bs*n))\n        return TensorDataset(x, a*x + b + 0.1*torch.randn(int(bs*n)))\n    train_ds = get_data(n_train)\n    valid_ds = get_data(n_valid)\n    device = default_device() if cuda else None\n    if tfmdDL:\n        train_dl = TfmdDL(train_ds, bs=bs, shuffle=True, num_workers=0, drop_last=True)\n        valid_dl = TfmdDL(valid_ds, bs=bs, num_workers=0)\n    else:\n        train_dl = TorchDL(train_ds, batch_size=bs, shuffle=True, num_workers=0, drop_last=True)\n        valid_dl = TorchDL(valid_ds, batch_size=bs, num_workers=0)\n        device = None\n    return DataLoaders(train_dl, valid_dl, device=device)\n\nclass RegModel(Module):\n    \"A r\"\n    def __init__(self): self.a,self.b = nn.Parameter(torch.randn(1)),nn.Parameter(torch.randn(1))\n    def forward(self, x): return x*self.a + self.b\n```\n\n----------------------------------------\n\nTITLE: Creating Language Model Learner\nDESCRIPTION: This creates a `Learner` for language modeling. It initializes a `Learner` object with the `DataLoaders` prepared for language modeling (`dls_lm`), the AWD_LSTM architecture, accuracy and perplexity metrics, sets the model path and weight decay, and activates mixed precision for potentially faster training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nlearn = language_model_learner(dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], path=path, wd=0.1).to_fp16()\n```\n\n----------------------------------------\n\nTITLE: Making text predictions\nDESCRIPTION: This code demonstrates making a prediction with the trained text classifier. It calls the `predict` method on a sample movie review and prints the predicted sentiment. This shows how to use the trained model to classify new text.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/quick_start.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nlearn.predict(\"I really liked that movie!\")\n```\n\n----------------------------------------\n\nTITLE: Training Model with One-Cycle Policy - Python\nDESCRIPTION: Trains the tabular model using the one-cycle learning rate policy for a specified number of epochs (5), a maximum learning rate (3e-3), and weight decay (0.2).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(5, 3e-3, wd=0.2)\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning the Language Model using fastai's fit_one_cycle in Python\nDESCRIPTION: Trains (fine-tunes) the language model using the `fit_one_cycle` method of the fastai `Learner`. This method implements the 1cycle learning rate policy. The example runs training for 1 epoch (`1`) with a maximum learning rate of 1e-4 (`1e-4`).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(1, 1e-4)\n```\n\n----------------------------------------\n\nTITLE: Creating TextDataLoaders from DataFrame - Python\nDESCRIPTION: This example demonstrates how to create `DataLoaders` for a text task using `TextDataLoaders.from_df`. It loads the IMDB sample data from a CSV into a DataFrame and then calls `from_df`, specifying the columns for text, label, and validation splits. A sample batch is shown to inspect the output.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\npath = untar_data(URLs.IMDB_SAMPLE)\ndf = pd.read_csv(path/\"texts.csv\")\ndls = TextDataLoaders.from_df(df, path=path, text_col='text', label_col='label', valid_col='is_valid')\ndls.show_batch(max_n=3)\n```\n\n----------------------------------------\n\nTITLE: Training Head with Larger Images Using fit_one_cycle - Python\nDESCRIPTION: Trains the classifier head for five epochs using the higher-resolution dataloader and the newly chosen learning rate. Inputs: learning rate, number of epochs. Outputs: training metrics and loss history.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(5, slice(lr))\n```\n\n----------------------------------------\n\nTITLE: Creating a Learner\nDESCRIPTION: This creates a `Learner` object for training the CNN model. It takes the data loaders `dls`, the model, the loss function (`nn.CrossEntropyLoss`), and the metrics (`accuracy`) as inputs. The Learner object encapsulates the training process, allowing for convenient training and evaluation.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nlearn = Learner(dls, model, loss_func = nn.CrossEntropyLoss(), metrics=accuracy)\n```\n\n----------------------------------------\n\nTITLE: Testing Normalization with Splits - Python\nDESCRIPTION: This code snippet tests the normalization process when using data splits within `TabularPandas`. It creates a DataFrame, applies normalization with a specified split, and verifies that the mean and standard deviation are calculated based only on the training split.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nnorm = Normalize()\ndf = pd.DataFrame({'a':[0,1,2,3,4]})\nto = TabularPandas(df, norm, cont_names='a', splits=[[0,1,2],[3,4]])\nx = np.array([0,1,2])\nm,s = x.mean(),x.std()\ntest_eq(norm.means['a'], m)\ntest_close(norm.stds['a'], s)\ntest_close(to['a'].values, (np.array([0,1,2,3,4])-m)/s)\n```\n\n----------------------------------------\n\nTITLE: Defining and Training a Tabular Learner - Python\nDESCRIPTION: This code creates a tabular learner and trains it. It defines the model architecture using `layers` which specifies the number of neurons in hidden layers. `metrics` sets the metric used to evaluate the model during training. `fit` trains the model for a specified number of epochs with a specified learning rate.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-tabular.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlearn = tabular_learner(dls, layers=[200,100], metrics=accuracy)\nlearn.fit(1, 1e-2)\n```\n\n----------------------------------------\n\nTITLE: Defining Mixed Precision Modes Enum in fastai Python\nDESCRIPTION: This snippet defines an Enum `AMPMode` to provide clear and type-safe options for specifying the desired automatic mixed precision mode. It maps the enum members (`FP16`, `BF16`) to their corresponding string values ('fp16', 'bf16') for internal use.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass AMPMode(Enum):\n    \"Automatic mixed precision modes for ease of completion\"\n    FP16 = 'fp16'\n    BF16 = 'bf16'\n```\n\n----------------------------------------\n\nTITLE: Defining Data Path - Python\nDESCRIPTION: Configures the path to the dataset directory using fastai's configuration utility, making the code portable and standardizing data location.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npath = Config().data/'rossmann'\n```\n\n----------------------------------------\n\nTITLE: Displaying Batch of Points with Custom Colormap - fastai (Python)\nDESCRIPTION: Displays a batch of images and point tensors using a grayscale colormap with specified figure size. Demonstrates the show_batch convenience for qualitative inspection. Requires pnt_tdl and matplotlib. No return value; shows plot.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\npnt_tdl.show_batch(figsize=(2,2), cmap='Greys');\n```\n\n----------------------------------------\n\nTITLE: Training Model Head Using fit_one_cycle - Python\nDESCRIPTION: Trains the classifier's final layers for five epochs using fastai's fit_one_cycle, with the chosen learning rate. Input: learning rate and number of epochs. Output: training/validation loss and metric history.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(5, slice(lr))\n```\n\n----------------------------------------\n\nTITLE: Testing BF16 Mixed Precision Training in fastai Python\nDESCRIPTION: This snippet tests BF16 mixed precision training, conditional on GPU support. It sets up a synthetic learner with the `MixedPrecision` callback configured for BF16 and the `BF16TestCallback`. After fitting, it verifies that the loss has decreased, confirming successful training in bfloat16.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#|cuda\nif torch.cuda.is_bf16_supported():\n    set_seed(99, True)\n    learn = synth_learner(cbs=[MixedPrecision(amp_mode=AMPMode.BF16),BF16TestCallback], cuda=True)\n    learn.model = nn.Sequential(nn.Linear(1,1), nn.Linear(1,1)).cuda()\n    learn.opt_func = partial(SGD, mom=0.)\n    learn.splitter = lambda m: [list(m[0].parameters()), list(m[1].parameters())]\n    learn.fit(3)\n    assert learn.recorder.values[-1][-1]<learn.recorder.values[0][-1]\n```\n\n----------------------------------------\n\nTITLE: Creating a Vision Learner with ResNet18 Architecture\nDESCRIPTION: This snippet initializes a vision learner with a ResNet18 backbone, CrossEntropyLoss, and dropout rate 0.25. It depends on fastai's vision module, takes data loaders 'dls', and creates a learner suitable for classification tasks. Inputs are data loaders and model parameters, output is a fastai Learner object.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\nlearn = vision_learner(dls, models.resnet18, loss_func=CrossEntropyLossFlat(), ps=0.25)\n```\n\n----------------------------------------\n\nTITLE: Adding FP16 Mixed Precision to Learner in fastai Python\nDESCRIPTION: This patch adds a convenience method `to_fp16` to the `Learner` class, allowing users to easily enable FP16 mixed precision training. It achieves this by adding the `MixedPrecision` callback to the learner with the default FP16 mode.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\n@delegates(GradScaler)\ndef to_fp16(self:Learner, **kwargs):\n    \"Set `Learner` to float16 mixed precision using PyTorch AMP\"\n    return self.add_cb(MixedPrecision(**kwargs))\n```\n\n----------------------------------------\n\nTITLE: model_sizes Function Usage Example\nDESCRIPTION: This snippet demonstrates the usage of the `model_sizes` function. It creates a sequential model with three convolutional layers and then uses `model_sizes` to determine the shapes of the activations at each layer. It verifies that the returned shapes are as expected.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nm = nn.Sequential(ConvLayer(3, 16), ConvLayer(16, 32, stride=2), ConvLayer(32, 32))\ntest_eq(model_sizes(m), [[1, 16, 64, 64], [1, 32, 32, 32], [1, 32, 32, 32]])\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders from CSV (Factory Method)\nDESCRIPTION: Creates fastai DataLoaders directly from the 'adult.csv' file. It specifies the dependent variable ('salary'), lists of categorical and continuous column names, and the preprocessing steps (Categorify, FillMissing, Normalize) to be applied.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n    cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race'],\n    cont_names = ['age', 'fnlwgt', 'education-num'],\n    procs = [Categorify, FillMissing, Normalize])\n```\n\n----------------------------------------\n\nTITLE: Testing Normalization Pipeline in Python\nDESCRIPTION: Tests the normalization pipeline by fetching and decoding a batch, then verifying tensor types, mean, and standard deviation values before and after decoding.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nx,y  = tdl.one_batch()\nxd,yd = tdl.decode((x,y))\n\nassert x.type().endswith('.FloatTensor')\ntest_eq(xd.type(), 'torch.LongTensor')\ntest_eq(type(x), TensorImage)\ntest_eq(type(y), TensorCategory)\nassert x.mean()<0.0\nassert x.std()>0.3\nassert 0<xd.float().mean()/255.<1\nassert 0<xd.float().std()/255.<0.7\n```\n\n----------------------------------------\n\nTITLE: Finding an Optimal Learning Rate with fastai's Learning Rate Finder in Python\nDESCRIPTION: Uses the fastai `Learner`'s built-in learning rate finder (`lr_find()`) to help determine an appropriate learning rate for fine-tuning the language model. This method runs a mock training process with exponentially increasing learning rates and typically generates a plot of loss versus learning rate to guide selection.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nlearn.lr_find()\n```\n\n----------------------------------------\n\nTITLE: Defining exported names and batch transform stages\nDESCRIPTION: Defines the names to be exported from the module and the standard stages for batch transformations in the data pipeline.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n_all_ = [\"show_batch\", \"show_results\"]\n\n_batch_tfms = ('after_item','before_batch','after_batch')\n```\n\n----------------------------------------\n\nTITLE: Using ParamScheduler with SchedLin\nDESCRIPTION: Demonstrates the use of `ParamScheduler` with a simple learning rate schedule defined by `SchedLin`. It initializes a learner, defines a schedule for the learning rate using `SchedLin`, and then trains the model using the `fit` method with the `ParamScheduler` callback.  It then verifies that the learning rates are being updated correctly by comparing the recorded values to the expected values. It depends on the fastai library and its scheduling functionality.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner()\nsched = {'lr': SchedLin(1e-3, 1e-2)}\nlearn.fit(1, cbs=ParamScheduler(sched))\nn = len(learn.dls.train)\ntest_close(learn.recorder.hps['lr'], [1e-3 + (1e-2-1e-3) * i/n for i in range(n)])\n```\n\n----------------------------------------\n\nTITLE: Show Top Losses for Multi-Category with Probabilities\nDESCRIPTION: This visualization displays top loss examples for multi-category classification, including predicted labels, true labels, confidence probabilities, and loss values. It arranges information in a grid, elaborately presenting model errors and confidences, aiding model diagnostics.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_44\n\nLANGUAGE: Python\nCODE:\n```\n@dispatch\ndef plot_top_losses(x: TensorImage, y:TensorMultiCategory, samples, outs, raws, losses, nrows=None, ncols=None, figsize=None, **kwargs):\n    axs = get_grid(len(samples), nrows=nrows, ncols=ncols, figsize=figsize)\n    for i,(ax,s) in enumerate(zip(axs, samples)): s[0].show(ctx=ax, title=f'Image {i}', **kwargs)\n    rows = get_empty_df(len(samples))\n    outs = L(s[1:] + o + (TitledStr(r), TitledFloat(l.item())) for s,o,r,l in zip(samples, outs, raws, losses))\n    for i,l in enumerate([\"target\", \"predicted\", \"probabilities\", \"loss\"]):\n        rows = [b.show(ctx=r, label=l, **kwargs) for b,r in zip(outs.itemgot(i),rows)]\n    display_df(pd.DataFrame(rows))\n```\n\n----------------------------------------\n\nTITLE: Padding Batched Bounding Box Samples for Uniform Shape in fastai Vision Using Python\nDESCRIPTION: Defines `bb_pad` which accepts a list of samples containing images, bounding boxes, and labels, clips and removes empty bounding boxes using `clip_remove_empty`, and pads the bounding boxes and labels so that all samples have the same number of bounding boxes. It pads bounding boxes with zero coordinates and labels with a specified padding index (`pad_idx`). This ensures uniform tensor shapes for batching in training and inference, and is used internally by the `BBoxBlock` TransformBlock.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/08_vision.data.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef bb_pad(\n    samples:list, # List of 3-tuples like (image, bounding_boxes, labels)\n    pad_idx=0 # Label that will be used to pad each list of labels\n):\n    \"Function that collects `samples` of labelled bboxes and adds padding with `pad_idx`.\"\n    samples = [(s[0], *clip_remove_empty(*s[1:])) for s in samples]\n    max_len = max([len(s[2]) for s in samples])\n    def _f(img,bbox,lbl):\n        bbox = torch.cat([bbox,bbox.new_zeros(max_len-bbox.shape[0], 4)])\n        lbl  = torch.cat([lbl, lbl .new_zeros(max_len-lbl .shape[0])+pad_idx])\n        return img,bbox,lbl\n    return [_f(*s) for s in samples]\n```\n\n----------------------------------------\n\nTITLE: Defining FilteredBase Class for Subset Handling in fastai (Python)\nDESCRIPTION: Defines the `FilteredBase` class, a base class for lists that support subsets (like training/validation splits). It integrates with `TfmdDL` and `DataLoaders`, providing methods for creating `DataLoaders` from subsets (`dataloaders`) and accessing subset properties (`n_subsets`, `train`, `valid`). It utilizes `delegates` for flexible `DataLoader` initialization and `add_props` to dynamically add `train` and `valid` properties.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass FilteredBase:\n    \"Base class for lists with subsets\"\n    _dl_type,_dbunch_type = TfmdDL,DataLoaders\n    def __init__(self, *args, dl_type=None, **kwargs):\n        if dl_type is not None: self._dl_type = dl_type\n        self.dataloaders = delegates(self._dl_type.__init__)(self.dataloaders)\n        super().__init__(*args, **kwargs)\n\n    @property\n    def n_subsets(self): return len(self.splits)\n    def _new(self, items, **kwargs): return super()._new(items, splits=self.splits, **kwargs)\n    def subset(self): raise NotImplemented\n\n    def dataloaders(self, \n        bs:int=64, # Batch size\n        shuffle_train:bool=None, # (Deprecated, use `shuffle`) Shuffle training `DataLoader`\n        shuffle:bool=True, # Shuffle training `DataLoader`\n        val_shuffle:bool=False, # Shuffle validation `DataLoader`\n        n:int=None, # Size of `Datasets` used to create `DataLoader`\n        path:str|Path='.', # Path to put in `DataLoaders`\n        dl_type:TfmdDL=None, # Type of `DataLoader`\n        dl_kwargs:list=None, # List of kwargs to pass to individual `DataLoader`s\n        device:torch.device=None, # Device to put `DataLoaders`\n        drop_last:bool=None, # Drop last incomplete batch, defaults to `shuffle`\n        val_bs:int=None, # Validation batch size, defaults to `bs`\n        **kwargs\n    ) -> DataLoaders:\n        if shuffle_train is not None: \n            shuffle=shuffle_train\n            warnings.warn('`shuffle_train` is deprecated. Use `shuffle` instead.',DeprecationWarning)\n        if device is None: device=default_device()\n        if dl_kwargs is None: dl_kwargs = [{}] * self.n_subsets\n        if dl_type is None: dl_type = self._dl_type\n        if drop_last is None: drop_last = shuffle\n        val_kwargs={k[4:]:v for k,v in kwargs.items() if k.startswith('val_')}\n        def_kwargs = {'bs':bs,'shuffle':shuffle,'drop_last':drop_last,'n':n,'device':device}\n        dl = dl_type(self.subset(0), **merge(kwargs,def_kwargs, dl_kwargs[0]))\n        def_kwargs = {'bs':bs if val_bs is None else val_bs,'shuffle':val_shuffle,'n':None,'drop_last':False}\n        dls = [dl] + [dl.new(self.subset(i), **merge(kwargs,def_kwargs,val_kwargs,dl_kwargs[i]))\n                      for i in range(1, self.n_subsets)]\n        return self._dbunch_type(*dls, path=path, device=device)    \n\nFilteredBase.train,FilteredBase.valid = add_props(lambda i,x: x.subset(i))\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-class Dice Coefficient (Macro F1) for Segmentation in PyTorch\nDESCRIPTION: Class that implements the averaged Dice coefficient for multiclass segmentation, calculating class-wise Dice scores and then averaging them across all classes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nclass DiceMulti(Metric):\n    \"Averaged Dice metric (Macro F1) for multiclass target in segmentation\"\n    def __init__(self, axis=1): self.axis = axis\n    def reset(self): self.inter,self.union = {},{}   \n    def accumulate(self, learn):\n        pred,targ = flatten_check(learn.pred.argmax(dim=self.axis), learn.y)\n        for c in range(learn.pred.shape[self.axis]):\n            p = torch.where(pred == c, 1, 0)\n            t = torch.where(targ == c, 1, 0)\n            c_inter = (p*t).float().sum().item()\n            c_union = (p+t).float().sum().item()\n            if c in self.inter:\n                self.inter[c] += c_inter\n                self.union[c] += c_union\n            else:\n                self.inter[c] = c_inter\n                self.union[c] = c_union\n\n    @property\n    def value(self):\n        binary_dice_scores = np.array([])\n        for c in self.inter:\n            binary_dice_scores = np.append(binary_dice_scores, 2.*self.inter[c]/self.union[c] if self.union[c] > 0 else np.nan)\n        return np.nanmean(binary_dice_scores)\n```\n\n----------------------------------------\n\nTITLE: Importing fastai Tabular Modules in Python\nDESCRIPTION: Imports essential modules and functions from fastai related to basic utilities and specifically the tabular submodules for core utilities, model architectures, and data processing. These imports allow access to fastai's tabular learner and model components needed later in the file.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/43_tabular.learner.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.basics import *\nfrom fastai.tabular.core import *\nfrom fastai.tabular.model import *\nfrom fastai.tabular.data import *\n```\n\n----------------------------------------\n\nTITLE: Create TabularDataLoaders from DataFrame\nDESCRIPTION: This code creates a `TabularDataLoaders` object from the adult sample DataFrame, specifying the path, preprocessors, categorical and continuous column names, target column name, validation indices, and batch size.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/41_tabular.data.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndls = TabularDataLoaders.from_df(df, path, procs=procs, cat_names=cat_names, cont_names=cont_names, \n                                 y_names=\"salary\", valid_idx=list(range(800,1000)), bs=64)\n```\n\n----------------------------------------\n\nTITLE: Loading ImageDataLoaders from CSV file using fastai in Python\nDESCRIPTION: Initializes ImageDataLoaders directly from a CSV file containing metadata with fastai's ImageDataLoaders.from_csv. Supports parameters for folder location and validation column to split training and validation sets. Requires CSV file compliant with expected formatting, a path to the image directory, and fastai library. Input is the path and CSV filename; output is a DataLoader instance wrapping training and validation datasets.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/08_vision.data.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#|eval: false\ndls = ImageDataLoaders.from_csv(path, 'train.csv', folder='train', valid_col='is_valid')\n```\n\n----------------------------------------\n\nTITLE: Defining a ResBlock\nDESCRIPTION: This defines a `ResBlock` class, which is a basic building block for ResNet architectures.  The `ResBlock` contains two convolutional layers and uses a residual connection.  This increases the depth of the network without degrading performance.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\nclass ResBlock(Module):\n    def __init__(self, nf):\n        self.conv1 = ConvLayer(nf,nf)\n        self.conv2 = ConvLayer(nf,nf)\n        \n    def forward(self, x): return x + self.conv2(self.conv1(x))\n```\n\n----------------------------------------\n\nTITLE: Loading and preparing image data\nDESCRIPTION: This code snippet downloads and prepares image data from the Oxford-IIIT Pet Dataset. It specifies a function to determine if an image represents a cat and then creates a `DataLoaders` object with a validation split of 20%, a random seed for reproducibility, and image resizing.  The dataset and pretrained model will be used for fine-tuning a model to recognize cats and dogs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/quick_start.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat, item_tfms=Resize(224))\n```\n\n----------------------------------------\n\nTITLE: View Vocabulary of DataLoaders\nDESCRIPTION: Displays the first 20 elements of the vocabulary used by the `DataLoaders` object. This shows the mapping between tokens and numerical IDs.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndbunch_lm.vocab[:20]\n```\n\n----------------------------------------\n\nTITLE: Splitting AWD-LSTM Language Model for Differential Learning Rates in Python\nDESCRIPTION: Defines a function that groups components of an AWD-LSTM language model (RNN layers and dropout layers) into sequential modules, including the encoder and decoder layers, to facilitate applying differential learning rates during training. It leverages PyTorch's nn.Sequential for module grouping and the fastai L container for list operations. The function returns a mapped collection of model parameters organized by groups. Prerequisites include PyTorch and fastai libraries, and a model consisting of an AWD-LSTM encoder and a decoder.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef awd_lstm_lm_split(model):\n    \"Split a RNN `model` in groups for differential learning rates.\"\n    groups = [nn.Sequential(rnn, dp) for rnn, dp in zip(model[0].rnns, model[0].hidden_dps)]\n    groups = L(groups + [nn.Sequential(model[0].encoder, model[0].encoder_dp, model[1])])\n    return groups.map(params)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Batches Using show_batch - Python\nDESCRIPTION: Displays a batch of augmented and labeled images from the DataLoaders using fastai's visualization utility. Useful for sanity checking transforms and label parsing. No inputs/outputs beyond visualization in notebook. Requires matplotlib and fastai.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndls.show_batch(max_n=9, figsize=(12,9))\n```\n\n----------------------------------------\n\nTITLE: AWD-LSTM Classifier Hyperparameter Configuration in Python\nDESCRIPTION: Specifies a dictionary with hyperparameters tailored for an AWD-LSTM classifier model. It includes embedding size, hidden size, the number of layers, padding token, bidirectionality flag, and several dropout rates aimed at classifier regularization. The dictionary also controls weight properties affecting model capacity and regularization. This configuration aligns with expected AWD-LSTM classifier architectures for effective training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nawd_lstm_clas_config = dict(emb_sz=400, n_hid=1152, n_layers=3, pad_token=1, bidir=False, output_p=0.4,\n                            hidden_p=0.3, input_p=0.4, embed_p=0.05, weight_p=0.5)\n```\n\n----------------------------------------\n\nTITLE: Install/Upgrade fastai on Colab\nDESCRIPTION: This code snippet checks if the code is running in a Colab environment. If it is, it upgrades the fastai library to the latest version using pip.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/41_tabular.data.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#| eval: false\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Inspecting ToTensor Transform Behavior with Sample Images - fastai (Python)\nDESCRIPTION: Initializes a ToTensor object and prints its representation along with the types before and after applying it to an MNIST image (mnist_img). Useful for debugging and confirming correct type conversion. Requires ToTensor and variables mnist_img (a PIL image). Outputs textual descriptions to standard output.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\ntfm = ToTensor()\nprint(tfm)\nprint(type(mnist_img))\nprint(type(tfm(mnist_img)))\n```\n\n----------------------------------------\n\nTITLE: Testing Dropout Multiplicative Scaling in get_language_model Outputs in Python\nDESCRIPTION: Verifies that the drop_mult parameter correctly scales all dropout probabilities in the model configuration by testing the instantiated model's dropout layers and recurrent weight dropout probabilities. Ensures that dropout modification through drop_mult propagates as expected across embedding, input, hidden, and output dropout components of the language model.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntst = get_language_model(AWD_LSTM, 100, config=config, drop_mult=0.5)\ntest_eq(tst[1].output_dp.p, config['output_p']*0.5)\nfor rnn in tst[0].rnns: test_eq(rnn.weight_p, config['weight_p']*0.5)\nfor dp in tst[0].hidden_dps: test_eq(dp.p, config['hidden_p']*0.5)\ntest_eq(tst[0].encoder_dp.embed_p, config['embed_p']*0.5)\ntest_eq(tst[0].input_dp.p, config['input_p']*0.5)\n```\n\n----------------------------------------\n\nTITLE: Create Vision Model from torchvision/fastai Architecture - Python\nDESCRIPTION: Builds a complete vision model from a specified architecture (`arch`) from torchvision or fastai. It handles loading pretrained weights (`pretrained`, `weights`), optionally cuts the model body (`cut`), determines the number of features, and adds a head using `add_head`. The head can be custom or automatically generated based on parameters like `n_out`, `concat_pool`, `pool`, `lin_ftrs`, `ps`, `first_bn`, `bn_final`, `lin_first`, and `y_range`. Supports varying input channels (`n_in`).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef create_vision_model(arch, n_out, pretrained=True, weights=None, cut=None, n_in=3, init=nn.init.kaiming_normal_, custom_head=None,\n                        concat_pool=True, pool=True, lin_ftrs=None, ps=0.5, first_bn=True, bn_final=False, lin_first=False, y_range=None):\n    \"Create custom vision architecture\"\n    meta = model_meta.get(arch, _default_meta)\n    if parse(torchvision.__version__) >= parse('0.13') and 'weights' in meta:\n        if weights is not None and not pretrained:\n            warn(f'{pretrained=} but `weights` are set {weights=}. To randomly initialize set `pretrained=False` & `weights=None`')\n        model = arch(weights=meta['weights'] if (weights is None and pretrained) else weights)\n    else:\n        model = arch(pretrained=pretrained)\n    body = create_body(model, n_in, pretrained, ifnone(cut, meta['cut']))\n    nf = num_features_model(nn.Sequential(*body.children())) if custom_head is None else None\n    return add_head(body, nf, n_out, init=init, head=custom_head, concat_pool=concat_pool, pool=pool,\n                    lin_ftrs=lin_ftrs, ps=ps, first_bn=first_bn, bn_final=bn_final, lin_first=lin_first, y_range=y_range)\n```\n\n----------------------------------------\n\nTITLE: Saving the Language Model Encoder\nDESCRIPTION: This saves the encoder of the language model (the model without the final classification layer) to the file system, so it can be reused for other models. The `save_encoder` method is used to accomplish this.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nlearn.save_encoder('finetuned')\n```\n\n----------------------------------------\n\nTITLE: Loading ImageDataLoaders from lists of filenames and labels using fastai in Python\nDESCRIPTION: Constructs ImageDataLoaders from separate lists of image filenames and their corresponding labels using fastai's ImageDataLoaders.from_lists. The validation set is defined as a random subset determined by valid_pct, optionally controlled by a random seed. Requires lists of file paths and labels, the base path, and fastai. Outputs an ImageDataLoader with datasets prepared for training and validation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/08_vision.data.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.PETS)\nfnames = get_image_files(path/\"images\")\nlabels = ['_'.join(x.name.split('_')[:-1]) for x in fnames]\ndls = ImageDataLoaders.from_lists(path, fnames, labels)\n```\n\n----------------------------------------\n\nTITLE: Showing Model Results (Sample Predictions)\nDESCRIPTION: Displays a sample of the model's predictions alongside the actual values from the validation set. This provides a quick visual check of the model's performance.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nlearn.show_results()\n```\n\n----------------------------------------\n\nTITLE: Demonstrate ShowGraphCallback Usage in Python\nDESCRIPTION: Creates a synthetic `Learner` instance, explicitly passing `ShowGraphCallback()` in the `cbs` list. It then trains the learner for 5 epochs. This demonstrates how to use `ShowGraphCallback` to visualize the training and validation loss progression during `fit`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/16_callback.progress.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#|slow\nlearn = synth_learner(cbs=ShowGraphCallback())\nlearn.fit(5)\n```\n\n----------------------------------------\n\nTITLE: Initializing a Tabular Data Wrapper Class in fastai (Python)\nDESCRIPTION: The Tabular class is a wrapper around a pandas DataFrame designed for tabular data with categorical, continuous, and dependent (target) columns. It manages column categorization, applies preprocessing pipelines (procs), supports splits for train/test subsets, and optionally reduces memory usage by downcasting data types. It includes facilities for setup, copying, decoding processed data, and device transfer for deep learning use. Key parameters include df (input DataFrame), categorical/continuous column names, target column names, data splits, transformation pipelines, device placement, and flags for inplace modification and memory optimization. The class relies on pandas, fastais transformation and pipeline utilities, and assumes proper preprocessing blocks (CategoryBlock, RegressionBlock) are provided or inferable.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nclass Tabular(CollBase, GetAttr, FilteredBase):\n    \"A `DataFrame` wrapper that knows which cols are cont/cat/y, and returns rows in `__getitem__`\"\n    _default,with_cont='procs',True\n    def __init__(self, df, procs=None, cat_names=None, cont_names=None, y_names=None, y_block=None, splits=None,\n                 do_setup=True, device=None, inplace=False, reduce_memory=True):\n        if inplace and splits is not None and pd.options.mode.chained_assignment is not None:\n            warn(\"Using inplace with splits will trigger a pandas error. Set `pd.options.mode.chained_assignment=None` to avoid it.\")\n        if not inplace: df = df.copy()\n        if reduce_memory: df = df_shrink(df)\n        if splits is not None: df = df.iloc[sum(splits, [])]\n        self.dataloaders = delegates(self._dl_type.__init__)(self.dataloaders)\n        super().__init__(df)\n\n        self.y_names,self.device = L(y_names),device\n        if y_block is None and self.y_names:\n            # Make ys categorical if they're not numeric\n            ys = df[self.y_names]\n            if len(ys.select_dtypes(include='number').columns)!=len(ys.columns): y_block = CategoryBlock()\n            else: y_block = RegressionBlock()\n        if y_block is not None and do_setup:\n            if callable(y_block): y_block = y_block()\n            procs = L(procs) + y_block.type_tfms\n        self.cat_names,self.cont_names,self.procs = L(cat_names),L(cont_names),Pipeline(procs)\n        self.split = len(df) if splits is None else len(splits[0])\n        if do_setup: self.setup()\n\n    def new(self, df, inplace=False):\n        return type(self)(df, do_setup=False, reduce_memory=False, y_block=TransformBlock(), inplace=inplace,\n                          **attrdict(self, 'procs','cat_names','cont_names','y_names', 'device'))\n\n    def subset(self, i): return self.new(self.items[slice(0,self.split) if i==0 else slice(self.split,len(self))])\n    def copy(self): self.items = self.items.copy(); return self\n    def decode(self): return self.procs.decode(self)\n    def decode_row(self, row): return self.new(pd.DataFrame(row).T).decode().items.iloc[0]\n    def show(self, max_n=10, **kwargs): display_df(self.new(self.all_cols[:max_n]).decode().items)\n    def setup(self): self.procs.setup(self)\n    def process(self): self.procs(self)\n    def loc(self): return self.items.loc\n    def iloc(self): return _TabIloc(self)\n    def targ(self): return self.items[self.y_names]\n    def x_names (self): return self.cat_names + self.cont_names\n    def n_subsets(self): return 2\n    def y(self): return self[self.y_names[0]]\n    def new_empty(self): return self.new(pd.DataFrame({}, columns=self.items.columns))\n    def to_device(self, d=None):\n        self.device = d\n        return self\n\n    def all_col_names (self):\n        ys = [n for n in self.y_names if n in self.items.columns]\n        return self.x_names + self.y_names if len(ys) == len(self.y_names) else self.x_names\n\nproperties(Tabular,'loc','iloc','targ','all_col_names','n_subsets','x_names','y')\n```\n\n----------------------------------------\n\nTITLE: Showing Tabular Learner Results by Displaying DataFrame in Python\nDESCRIPTION: Defines a specialized `show_results` function for tabular data that displays the features and predicted outputs for a batch of samples as a DataFrame. It overrides default behavior to concatenate columns of the tabular input data with the predicted labels and shows up to a maximum number of rows. Utilizes fastai's dispatch system for automatically calling the correct method based on types.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/43_tabular.learner.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dispatch\ndef show_results(x:Tabular, y:Tabular, samples, outs, ctxs=None, max_n=10, **kwargs):\n    df = x.all_cols[:max_n]\n    for n in x.y_names: df[n+'_pred'] = y[n][:max_n].values\n    display_df(df)\n```\n\n----------------------------------------\n\nTITLE: Import Core Fastai Modules\nDESCRIPTION: Imports necessary components from the `fastai.basics` module, enabling the use of core fastai functionalities like `Callback`, `Learner`, `synth_learner`, etc. The `__future__` import ensures compatibility with newer Python type hint features.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/16_callback.progress.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom __future__ import annotations\nfrom fastai.basics import *\n```\n\n----------------------------------------\n\nTITLE: AWD-LSTM Batch Size Change Test - PyTorch\nDESCRIPTION: This code snippet tests the `AWD_LSTM` model's ability to handle changes in the batch size. It creates a dummy input with a different batch size compared to the one during model initialization, calls the model and asserts that the batch size is correctly updated after forward.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n#test bs change\nx = torch.randint(0, 100, (6,5))\nr = tst(x)\ntest_eq(tst.bs, 6)\n```\n\n----------------------------------------\n\nTITLE: Sorting and Decoding in TfmdLists\nDESCRIPTION: This snippet verifies that transformations in TfmdLists are sorted by their 'order' attribute and '_lbl' is applied first. It then checks the vocab and decoding functionality to ensure transformations are correctly applied, with dependencies on TfmdLists, test_eq, and related utilities.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\n# Check that tfms are sorted by `order` & `_lbl` is called first\nfns = ['dog_0.jpg','cat_0.jpg','cat_2.jpg','cat_1.jpg','dog_1.jpg']\ntl = TfmdLists(fns, [tcat,_lbl])\nexp_voc = ['cat','dog']\ntest_eq(tcat.vocab, exp_voc)\ntest_eq(tl.tfms.vocab, exp_voc)\ntest_eq(tl.vocab, exp_voc)\ntest_eq(tl, (1,0,0,0,1))\ntest_eq([tl.decode(o) for o in tl], ('dog','cat','cat','cat','dog'))\n```\n\n----------------------------------------\n\nTITLE: ActivationStats Callback for Monitoring Layer Activations\nDESCRIPTION: A callback class that captures and records activation statistics (mean, standard deviation, near-zero values) during model training. Includes methods for visualizing these statistics and generating histograms of activation values.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_41\n\nLANGUAGE: Python\nCODE:\n```\n@delegates()\nclass ActivationStats(HookCallback):\n    \"Callback that record the mean and std of activations.\"\n    order=-20\n    def __init__(self, with_hist=False, **kwargs):\n        super().__init__(**kwargs)\n        self.with_hist = with_hist\n\n    def before_fit(self):\n        \"Initialize stats.\"\n        super().before_fit()\n        self.stats = L()\n\n    def hook(self, m, i, o):\n        if isinstance(o, tuple): return self.hook_multi_ouput(o)\n        o = o.float()\n        res = {'mean': o.mean().item(), 'std': o.std().item(),\n               'near_zero': (o<=0.05).long().sum().item()/o.numel()}\n        if self.with_hist: res['hist'] = o.histc(40,0,10)\n        return res\n    \n    def hook_multi_ouput(self,o_tuple):\n        \"For outputs of RNN which are [nested] tuples of tensors\"\n        res = []\n        for o in self._flatten_tuple(o_tuple):\n            if not(isinstance(o, Tensor)): continue\n            res.append(self.hook(None, None, o))\n        return res\n\n    def _flatten_tuple(self, o_tuple):\n        \"Recursively flatten a [nested] tuple\"\n        res = []\n        for it in o_tuple:\n            if isinstance(it, tuple): res += self._flatten_tuple(it)\n            else: res += [it]\n        return tuple(res)\n\n    def after_batch(self):\n        \"Take the stored results and puts it in `self.stats`\"\n        if self.training and (self.every is None or self.train_iter%self.every == 0): self.stats.append(self.hooks.stored)\n        super().after_batch()\n\n    def layer_stats(self, idx):\n        lstats = self.stats.itemgot(idx)\n        return L(lstats.itemgot(o) for o in ('mean','std','near_zero'))\n\n    def hist(self, idx):\n        res = self.stats.itemgot(idx).itemgot('hist')\n        return torch.stack(tuple(res)).t().float().log1p()\n\n    def color_dim(self, idx, figsize=(10,5), ax=None):\n        \"The 'colorful dimension' plot\"\n        res = self.hist(idx)\n        if ax is None: ax = subplots(figsize=figsize)[1][0]\n        ax.imshow(res, origin='lower')\n        ax.axis('off')\n\n    def plot_layer_stats(self, idx):\n        _,axs = subplots(1, 3, figsize=(12,3))\n        for o,ax,title in zip(self.layer_stats(idx),axs,('mean','std','% near zero')):\n            ax.plot(o)\n            ax.set_title(title)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiple Input Model Class in PyTorch\nDESCRIPTION: A PyTorch module implementation that accepts two inputs, concatenates them, and processes them through a sequential network of linear layers, ReLU activation, and batch normalization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\nclass _2InpModel(Module):\n    def __init__(self):\n        super().__init__()\n        self.seq = nn.Sequential(nn.Linear(2,50), nn.ReLU(), nn.BatchNorm1d(50), nn.Linear(50, 1))\n    def forward(self, *inps):\n        outputs = torch.cat(inps, dim=-1)\n        return self.seq(outputs)\n```\n\n----------------------------------------\n\nTITLE: Defining Cancel Exceptions\nDESCRIPTION: Defines exception classes to be raised to skip certain parts of the training process: `CancelBatchException`, `CancelTrainException`, `CancelValidException`, `CancelEpochException`, `CancelStepException`, `CancelBackwardException`, and `CancelFitException`. These are used to control the flow of the training loop within callbacks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#|export\n_ex_docs = dict(\n    CancelBatchException=\"Skip the rest of this batch and go to `after_batch`\",\n    CancelTrainException=\"Skip the rest of the training part of the epoch and go to `after_train`\",\n    CancelValidException=\"Skip the rest of the validation part of the epoch and go to `after_validate`\",\n    CancelEpochException=\"Skip the rest of this epoch and go to `after_epoch`\",\n    CancelStepException =\"Skip stepping the optimizer\",\n    CancelBackwardException=\"Skip the backward pass and go to `after_backward`\",\n    CancelFitException  =\"Interrupts training and go to `after_fit`\")\n\nfor c,d in _ex_docs.items(): mk_class(c,sup=Exception,doc=d)\n```\n\n----------------------------------------\n\nTITLE: Launching Distributed Training from Notebook (Python)\nDESCRIPTION: Demonstrates how to use `accelerate.notebook_launcher` to execute the defined `train` function across multiple processes (GPUs). It passes the `train` function, its arguments (`path`) as a tuple, and specifies the number of processes (`num_processes=2`) to use for distributed execution.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20b_tutorial.distributed.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nnotebook_launcher(train, (path,), num_processes=2)\n```\n\n----------------------------------------\n\nTITLE: Creating a DataBlock for Text Classification\nDESCRIPTION: This creates a `DataBlock` object, which provides a blueprint for the creation of a fastai `DataLoaders` object, using the data block API.  It specifies the `blocks` (text and category), how to get the items, how to label the items (parent folder) and how to split the items (grandparent folder).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimdb = DataBlock(blocks=(TextBlock.from_folder(path), CategoryBlock),\n                 get_items=get_text_files,\n                 get_y=parent_label,\n                 splitter=GrandparentSplitter(valid_name='test'))\n```\n\n----------------------------------------\n\nTITLE: Predicting Single Row (fastai tabular)\nDESCRIPTION: Performs a prediction on a single row of the original, unprocessed DataFrame (the first row in this case). The method returns the processed input row, the predicted class, and the probabilities for each class.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nrow, clas, probs = learn.predict(df.iloc[0])\n```\n\n----------------------------------------\n\nTITLE: Combining Two Cosine Annealing Schedules with a Transition Point in Python\nDESCRIPTION: Defines combined_cos function that returns a composite scheduler combining two cosine annealing schedules joined at pct. The schedule runs from start to middle in the first interval and middle to end in the second, useful for implementing schemes like the 1cycle policy. Supports float or array-like parameters.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef combined_cos(pct, start, middle, end):\n    \"Return a scheduler with cosine annealing from `start`\\u2192`middle` & `middle`\\u2192`end`\"\n    return combine_scheds([pct,1-pct], [SchedCos(start, middle), SchedCos(middle, end)])\n```\n\n----------------------------------------\n\nTITLE: Create a collaborative learner for Movielens 100k\nDESCRIPTION: Creates a collaborative learner (`collab_learner`) using the data loaders (`dls`), sets the number of factors to 40, and specifies the target variable range using `y_range`. This learner uses a dot product model trained on the Movielens 100k dataset.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nlearn = collab_learner(dls, n_factors=40, y_range=y_range)\n```\n\n----------------------------------------\n\nTITLE: Define Optimizer and Callbacks\nDESCRIPTION: This code defines the optimizer function and callbacks for training the language model. It uses Adam with specified weight decay and epsilon values, and includes callbacks for mixed precision training, gradient clipping, and RNN-specific regularization techniques (alpha and beta).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/35_tutorial.wikitext.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nopt_func = partial(Adam, wd=0.1, eps=1e-7)\ncbs = [MixedPrecision(), GradientClip(0.1)] + rnn_cbs(alpha=2, beta=1)\n```\n\n----------------------------------------\n\nTITLE: Creating Datasets\nDESCRIPTION: This creates `Datasets` object, which is used to encapsulate the data and apply transformations using the provided `tfms`. The `tfms` are a set of transformations that are applied to the data.  Inputs are a list of image file paths, with a set of transformations. The outputs are the datasets ready for data loading.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndsets = Datasets(items, tfms=[[PILImageBW.create], [parent_label, Categorize]], splits=splits)\n```\n\n----------------------------------------\n\nTITLE: Initial Training Cycle for Classifier Learner (Python)\nDESCRIPTION: Trains the classifier for one epoch on frozen layers using fit_one_cycle with determined learning rate, momentum, and weight decay settings. Inputs: classifier Learner, lr; Output: updates model weights for classifier head.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(1, lr, moms=(0.8,0.7,0.8), wd=0.1)\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders from TabularPandas\nDESCRIPTION: Generates fastai DataLoaders from the preprocessed `TabularPandas` object (`to`). This prepares the data in batches for model training and validation, using a batch size of 64.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndls = to.dataloaders(bs=64)\n```\n\n----------------------------------------\n\nTITLE: Loading and preparing tabular data\nDESCRIPTION: This snippet loads the Adult sample dataset and creates `TabularDataLoaders` for tabular data. It specifies the target variable, categorical and continuous variables, and preprocessing steps.  It prepares tabular data for model training by defining the feature types and preprocessing steps.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/quick_start.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.ADULT_SAMPLE)\n\ndls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n    cat_names = ['workclass', 'education', 'marital-status', 'occupation',\n                 'relationship', 'race'],\n    cont_names = ['age', 'fnlwgt', 'education-num'],\n    procs = [Categorify, FillMissing, Normalize])\n```\n\n----------------------------------------\n\nTITLE: Loading Pretrained XResNet Variants and Defining Factory Functions in Python\nDESCRIPTION: Defines helper function _xresnet to instantiate an XResNet with specified expansion and layers, optionally loading pretrained weights from a fixed URL for xresnet50. It also provides factory functions for commonly used XResNet architectures, including xresnet18, xresnet34, xresnet50, and deeper or extended versions, each passing specific layer configurations and expansion values. Note that pretrained weights currently only support xresnet50.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/11_vision.models.xresnet.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef _xresnet(pretrained, expansion, layers, **kwargs):\n    # TODO pretrain all sizes. Currently will fail with non-xrn50\n    url = 'https://s3.amazonaws.com/fast-ai-modelzoo/xrn50_940.pth'\n    res = XResNet(ResBlock, expansion, layers, **kwargs)\n    if pretrained: res.load_state_dict(load_state_dict_from_url(url, map_location='cpu')['model'], strict=False)\n    return res\n\ndef xresnet18 (pretrained=False, **kwargs): return _xresnet(pretrained, 1, [2, 2,  2, 2], **kwargs)\ndef xresnet34 (pretrained=False, **kwargs): return _xresnet(pretrained, 1, [3, 4,  6, 3], **kwargs)\ndef xresnet50 (pretrained=False, **kwargs): return _xresnet(pretrained, 4, [3, 4,  6, 3], **kwargs)\ndef xresnet101(pretrained=False, **kwargs): return _xresnet(pretrained, 4, [3, 4, 23, 3], **kwargs)\ndef xresnet152(pretrained=False, **kwargs): return _xresnet(pretrained, 4, [3, 8, 36, 3], **kwargs)\ndef xresnet18_deep  (pretrained=False, **kwargs): return _xresnet(pretrained, 1, [2,2,2,2,1,1], **kwargs)\ndef xresnet34_deep  (pretrained=False, **kwargs): return _xresnet(pretrained, 1, [3,4,6,3,1,1], **kwargs)\ndef xresnet50_deep  (pretrained=False, **kwargs): return _xresnet(pretrained, 4, [3,4,6,3,1,1], **kwargs)\ndef xresnet18_deeper(pretrained=False, **kwargs): return _xresnet(pretrained, 1, [2,2,1,1,1,1,1,1], **kwargs)\ndef xresnet34_deeper(pretrained=False, **kwargs): return _xresnet(pretrained, 1, [3,4,6,3,1,1,1,1], **kwargs)\ndef xresnet50_deeper(pretrained=False, **kwargs): return _xresnet(pretrained, 4, [3,4,6,3,1,1,1,1], **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Implementing WandbCallback for fastai training logging\nDESCRIPTION: Defines the WandbCallback class that extends fastai's Callback to log model topology, metrics, predictions, datasets and checkpoints to wandb during training, validation, and after training. It manages connection initialization, periodic logging, and optional prediction sample logging.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nclass WandbCallback(Callback):\n    \"Saves model topology, losses & metrics\"\n    remove_on_fetch,order = True,Recorder.order+1\n    # Record if watch has been called previously (even in another instance)\n    _wandb_watch_called = False\n\n    def __init__(self, \n                 log:str=None, # What to log (can be `gradients`, `parameters`, `all` or None) \n                 log_preds:bool=True, # Whether to log model predictions on a `wandb.Table`\n                 log_preds_every_epoch:bool=False, # Whether to log predictions every epoch or at the end\n                 log_model:bool=False, # Whether to save the model checkpoint to a `wandb.Artifact` \n                 model_name:str=None, # The name of the `model_name` to save, overrides `SaveModelCallback`\n                 log_dataset:bool=False, # Whether to log the dataset to a `wandb.Artifact`\n                 dataset_name:str=None, # A name to log the dataset with\n                 valid_dl:TfmdDL=None, # If `log_preds=True`, then the samples will be drawn from `valid_dl`\n                 n_preds:int=36, # How many samples to log predictions \n                 seed:int=12345, # The seed of the samples drawn\n                 reorder=True):\n        store_attr()\n    \n    def after_create(self):\n        # log model\n        if self.log_model:\n            if not hasattr(self, 'save_model'):\n                # does not have the SaveModelCallback\n                self.learn.add_cb(SaveModelCallback(fname=ifnone(self.model_name, 'model')))\n            else:\n                # override SaveModelCallback\n                if self.model_name is not None:\n                    self.save_model.fname = self.model_name\n            \n    def before_fit(self):\n        \"Call watch method to log model topology, gradients & weights\"\n        # Check if wandb.init has been called\n        if wandb.run is None:\n            raise ValueError('You must call wandb.init() before WandbCallback()')\n        # W&B log step\n        self._wandb_step = wandb.run.step - 1  # -1 except if the run has previously logged data (incremented at each batch)\n        self._wandb_epoch = 0 if not(wandb.run.step) else math.ceil(wandb.run.summary['epoch']) # continue to next epoch\n        \n        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\") and rank_distrib()==0\n        if not self.run: return\n\n        # Log config parameters\n        log_config = self.learn.gather_args()\n        _format_config(log_config)\n        try:\n            wandb.config.update(log_config, allow_val_change=True)\n        except Exception as e:\n            print(f'WandbCallback could not log config parameters -> {e}')\n\n        if not WandbCallback._wandb_watch_called:\n            WandbCallback._wandb_watch_called = True\n            # Logs model topology and optionally gradients and weights\n            if self.log is not None:\n                wandb.watch(self.learn.model, log=self.log)\n\n        # log dataset\n        assert isinstance(self.log_dataset, (str, Path, bool)), 'log_dataset must be a path or a boolean'\n        if self.log_dataset is True:\n            if Path(self.dls.path) == Path('.'):\n                print('WandbCallback could not retrieve the dataset path, please provide it explicitly to \"log_dataset\"')\n                self.log_dataset = False\n            else:\n                self.log_dataset = self.dls.path\n        if self.log_dataset:\n            self.log_dataset = Path(self.log_dataset)\n            assert self.log_dataset.is_dir(), f'log_dataset must be a valid directory: {self.log_dataset}'\n            metadata = {'path relative to learner': os.path.relpath(self.log_dataset, self.learn.path)}\n            log_dataset(path=self.log_dataset, name=self.dataset_name, metadata=metadata)\n\n        if self.log_preds:\n            try:\n                if not self.valid_dl:\n                    #Initializes the batch watched\n                    wandbRandom = random.Random(self.seed)  # For repeatability\n                    self.n_preds = min(self.n_preds, len(self.dls.valid_ds))\n                    idxs = wandbRandom.sample(range(len(self.dls.valid_ds)), self.n_preds)\n                    if isinstance(self.dls,  TabularDataLoaders):\n                        test_items = getattr(self.dls.valid_ds.items, 'iloc', self.dls.valid_ds.items)[idxs]\n                        self.valid_dl = self.dls.test_dl(test_items, with_labels=True, process=False)\n                    else:\n                        test_items = [getattr(self.dls.valid_ds.items, 'iloc', self.dls.valid_ds.items)[i] for i in idxs]\n                        self.valid_dl = self.dls.test_dl(test_items, with_labels=True)\n                self.learn.add_cb(FetchPredsCallback(dl=self.valid_dl, with_input=True, with_decoded=True, reorder=self.reorder))\n            except Exception as e:\n                self.log_preds = False\n                print(f'WandbCallback was not able to prepare a DataLoader for logging prediction samples -> {e}')\n        \n    def before_batch(self): \n        self.ti_batch = time.perf_counter()\n        \n    def after_batch(self):\n        \"Log hyper-parameters and training loss\"\n        if self.training:\n            batch_time = time.perf_counter() - self.ti_batch\n            self._wandb_step += 1\n            self._wandb_epoch += 1/self.n_iter\n            hypers = {f'{k}_{i}':v for i,h in enumerate(self.opt.hypers) for k,v in h.items()}\n            wandb.log({'epoch': self._wandb_epoch, 'train_loss': self.smooth_loss, 'raw_loss': self.loss, **hypers}, step=self._wandb_step)\n            wandb.log({'train_samples_per_sec': len(self.xb[0]) / batch_time}, step=self._wandb_step)\n            \n    def log_predictions(self):\n        try:\n            inp,preds,targs,out = self.learn.fetch_preds.preds\n            b = tuplify(inp) + tuplify(targs)\n            x,y,its,outs = self.valid_dl.show_results(b, out, show=False, max_n=self.n_preds)\n            wandb.log(wandb_process(x, y, its, outs, preds), step=self._wandb_step)\n        except Exception as e:\n            self.log_preds = False\n            self.remove_cb(FetchPredsCallback)\n            print(f'WandbCallback was not able to get prediction samples -> {e}')\n\n\n    def after_epoch(self):\n        \"Log validation loss and custom metrics & log prediction samples\"\n        # Correct any epoch rounding error and overwrite value\n        self._wandb_epoch = round(self._wandb_epoch)\n        if self.log_preds and self.log_preds_every_epoch:\n            self.log_predictions()\n        wandb.log({'epoch': self._wandb_epoch}, step=self._wandb_step)\n        wandb.log({n:s for n,s in zip(self.recorder.metric_names, self.recorder.log) if n not in ['train_loss', 'epoch', 'time']}, step=self._wandb_step)\n    \n    \n    def after_fit(self):\n        if self.log_preds and not self.log_preds_every_epoch:\n            self.log_predictions()\n        if self.log_model:\n            if self.save_model.last_saved_path is None:\n                print('WandbCallback could not retrieve a model to upload')\n            else:\n                metadata = {n:s for n,s in zip(self.recorder.metric_names, self.recorder.log) if n not in ['train_loss', 'epoch', 'time']}\n                log_model(self.save_model.last_saved_path, name=self.save_model.fname, metadata=metadata)                \n        self.run = True\n        if self.log_preds: self.remove_cb(FetchPredsCallback)\n        \n        wandb.log({})  # ensure sync of last step\n        self._wandb_step += 1\n        \n\n```\n\n----------------------------------------\n\nTITLE: Define TabularDataLoaders class (from_df)\nDESCRIPTION: This code defines the `TabularDataLoaders` class, which is a wrapper around several `DataLoader`s. The `from_df` method is a factory method that creates a `TabularDataLoaders` object from a Pandas DataFrame. It takes parameters such as the DataFrame, path, preprocessors, categorical and continuous column names, target column names, target block, and validation indices.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/41_tabular.data.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass TabularDataLoaders(DataLoaders):\n    \"Basic wrapper around several `DataLoader`s with factory methods for tabular data\"\n    @classmethod\n    @delegates(Tabular.dataloaders, but=[\"dl_type\", \"dl_kwargs\"])\n    def from_df(cls,\n        df:pd.DataFrame,\n        path:str|Path='.', # Location of `df`, defaults to current working directory\n        procs:list=None, # List of `TabularProc`s\n        cat_names:list=None, # Column names pertaining to categorical variables\n        cont_names:list=None, # Column names pertaining to continuous variables\n        y_names:list=None, # Names of the dependent variables\n        y_block:TransformBlock=None, # `TransformBlock` to use for the target(s)\n        valid_idx:list=None, # List of indices to use for the validation set, defaults to a random split\n        **kwargs\n    ):\n        \"Create `TabularDataLoaders` from `df` in `path` using `procs`\"\n        if cat_names is None: cat_names = []\n        if cont_names is None: cont_names = list(set(df)-set(L(cat_names))-set(L(y_names)))\n        splits = RandomSplitter()(df) if valid_idx is None else IndexSplitter(valid_idx)(df)\n        to = TabularPandas(df, procs, cat_names, cont_names, y_names, splits=splits, y_block=y_block)\n        return to.dataloaders(path=path, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Determining and Setting Default PyTorch Device (CUDA/MPS/CPU) in fastai\nDESCRIPTION: Provides functions to check for Metal Performance Shaders (MPS) availability (`_has_mps`) using `torch.backends.mps` and determine or set the default PyTorch device (`default_device`). The `default_device` function prioritizes CUDA, then MPS, then CPU based on availability and the global `defaults.use_cuda` setting (-1 or None for auto, True for CUDA/MPS required, False for CPU).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef _has_mps():\n    if nested_attr(torch, 'backends.mps.is_available', noop)(): return True\n    return nested_attr(torch, 'backends.mps.is_built', False)()\n\ndef default_device(use=-1):\n    \"Return or set default device; `use_cuda`: -1 - CUDA/mps if available; True - error if not available; False - CPU\"\n    if use == -1: use = defaults.use_cuda\n    else: defaults.use_cuda=use\n    if use is None:\n        if torch.cuda.is_available() or _has_mps(): use = True\n    if use:\n        if torch.cuda.is_available(): return torch.device(torch.cuda.current_device())\n        if _has_mps(): return torch.device('mps')\n    return torch.device('cpu')\n```\n\n----------------------------------------\n\nTITLE: Manual Training Update with Weight Decay - PyTorch/Python\nDESCRIPTION: Defines a function `update` that performs a single training step (batch update). It calculates the loss (including weight decay), computes gradients manually using `loss.backward()`, updates model parameters using gradient descent (`p.sub_(lr * p.grad)`) within a `torch.no_grad()` block, and finally zeros the gradients (`p.grad.zero_()`). It returns the loss value.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef update(x,y,lr):\n    wd = 1e-5\n    y_hat = model(x)\n    # weight decay\n    w2 = 0.\n    for p in model.parameters(): w2 += (p**2).sum()\n    # add to regular loss\n    loss = loss_func(y_hat, y) + w2*wd\n    loss.backward()\n    with torch.no_grad():\n        for p in model.parameters():\n            p.sub_(lr * p.grad)\n            p.grad.zero_()\n    return loss.item()\n```\n\n----------------------------------------\n\nTITLE: Adding DistributedTrainer to Learner with to_distributed - Python\nDESCRIPTION: Adds a DistributedTrainer callback to a fastai Learner instance, configuring synchronization of batch norm and custom Accelerator parameters. Optionally removes the ProgressCallback if in a distributed rank context. Expects a Learner object as self; dependencies: fastai, Accelerate. Returns the modified Learner instance.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\n@patch\n@delegates(Accelerator, but=_hidden_params)\ndef to_distributed(self: Learner,\n        sync_bn=True, # Whether to replace all batch norm with `nn.SyncBatchNorm`\n        **kwargs\n    ):\n    \"Add `AcceleratedTrainer` to a learner, and configures an Accelerator\"\n    self.add_cb(DistributedTrainer(sync_bn, **kwargs))\n    if rank_distrib(): self.remove_cb(ProgressCallback)\n    return self\n```\n\n----------------------------------------\n\nTITLE: Recursively Checking Presence of Pooling Layers in PyTorch Models Using FastAI in Python\nDESCRIPTION: Implements 'has_pool_type', a recursive function that returns True if the given model or any of its children contain a pooling layer, detected by '_is_pool_type'. This is used to identify appropriate cut points in pretrained networks for transfer learning setups where pooling layers signify breakpoints.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef has_pool_type(m):\n    \"Return `True` if `m` is a pooling layer or has one in its children\"\n    if _is_pool_type(m): return True\n    for l in m.children():\n        if has_pool_type(l): return True\n    return False\n```\n\n----------------------------------------\n\nTITLE: Enhanced TransformersTokenizer for Pre-Tokenized Inputs (Python)\nDESCRIPTION: Redefines the TransformersTokenizer class to handle both raw text strings and pre-tokenized tensors in its encodes method; the decodes method remains unchanged. This makes the transformation pipeline modular, allowing reuse of pre-tokenized data for increased efficiency. Requires HuggingFace tokenizer, PyTorch, and fastai.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nclass TransformersTokenizer(Transform):\n    def __init__(self, tokenizer): self.tokenizer = tokenizer\n    def encodes(self, x): \n        return x if isinstance(x, Tensor) else tokenize(x)\n        \n    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))\n```\n\n----------------------------------------\n\nTITLE: Plotting Training Schedules with plot_sched\nDESCRIPTION: Calls the `plot_sched` method on the learner's recorder to visualize the hyperparameter schedules after a training run.  It depends on the training run having finished before plotting.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nlearn.recorder.plot_sched()\n```\n\n----------------------------------------\n\nTITLE: Scaling and Unscaling Point Tensors for Data Augmentation - fastai (Python)\nDESCRIPTION: Provides functions to scale 2D points to (-1,1) range (for data augmentation such as grid_sample), and invert the scaling. Handles y/x coordinate ordering and shape. Depends on TensorPoint, tensor, and standard PyTorch operations. Expects tensor input, outputs TensorPoint objects.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\n#|export\ndef _scale_pnts(y, sz, do_scale=True, y_first=False):\n    if y_first: y = y.flip(1)\n    res = y * 2/tensor(sz).float() - 1 if do_scale else y\n    return TensorPoint(res, img_size=sz)\n\ndef _unscale_pnts(y, sz): return TensorPoint((y+1) * tensor(sz).float()/2, img_size=sz)\n```\n\n----------------------------------------\n\nTITLE: Returning All Parameters of PyTorch Module in Python\nDESCRIPTION: Simple helper function that retrieves a list of all parameters, including trainable and non-trainable, from a given PyTorch model or module. Useful as a baseline utility to access model weights and biases.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_76\n\nLANGUAGE: python\nCODE:\n```\ndef params(m):\n    \"Return all parameters of `m`\"\n    return [p for p in m.parameters()]\n```\n\n----------------------------------------\n\nTITLE: Setting Default Initialization for ReLU\nDESCRIPTION: This code sets the `__default_init__` attribute of several ReLU activation functions (both functional and module versions) to `kaiming_uniform_`. This allows these activation functions to be initialized using the Kaiming uniform initialization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfor o in F.relu,nn.ReLU,F.relu6,nn.ReLU6,F.leaky_relu,nn.LeakyReLU:\n    o.__default_init__ = kaiming_uniform_\n```\n\n----------------------------------------\n\nTITLE: Resuming from Checkpoint\nDESCRIPTION: Demonstrates how to resume training from a checkpoint. It saves the model and optimizer state with `SaveModelCallback` (using `with_opt=True`) and uses the `InterruptCallback` to simulate an interruption after a few epochs.  It then loads the saved model and resumes training from the interrupted epoch, ensuring that the learning rate schedule continues correctly. The code tests the lr schedule by plotting and comparing against the un-interrupted learning rate schedule.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n#|slow\nwith tempfile.TemporaryDirectory() as d:\n    learn1 = synth_learner(path=d, cbs=SaveModelCallback(with_opt=True, fname=\"ckpt\"))\n    learn1.fit_one_cycle(5, cbs=InterruptCallback(2))\n    \n    learn2 = synth_learner(path=d)\n    learn2 = learn2.load(\"ckpt\")\n    learn2.fit_one_cycle(5, start_epoch=2)\n    \n    fig, axs = plt.subplots(1,2, sharey=True)\n    axs[0].plot(learn1.recorder.lrs)\n    axs[1].plot(learn2.recorder.lrs)\n```\n\n----------------------------------------\n\nTITLE: Generating Text with the Language Model\nDESCRIPTION: This demonstrates how to use the trained language model to generate new text. It sets an initial text prompt, generates a specified number of words and sentences, and prints the generated text. The parameters control the creativity and variability of the generated text.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nTEXT = \"I liked this movie because\"\nN_WORDS = 40\nN_SENTENCES = 2\npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n         for _ in range(N_SENTENCES)]\n```\n\n----------------------------------------\n\nTITLE: Implementing transform merging utility functions\nDESCRIPTION: Defines _merge_tfms to combine transformation lists while handling duplicates, and a helper _zip function for zipping collections.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef _merge_tfms(*tfms):\n    \"Group the `tfms` in a single list, removing duplicates (from the same class) and instantiating\"\n    g = groupby(concat(*tfms), _merge_grouper)\n    return L(v[-1] for k,v in g.items()).map(instantiate)\n\ndef _zip(x): return L(x).zip()\n```\n\n----------------------------------------\n\nTITLE: Create a collaborative learner\nDESCRIPTION: Creates a collaborative learner (`collab_learner`) using the data loaders (`dls`), sets the number of factors to 50, and specifies the target variable range using `y_range`. This learner uses a dot product model.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlearn = collab_learner(dls, n_factors=50, y_range=y_range)\n```\n\n----------------------------------------\n\nTITLE: Using fastai Tokenizer with SentencePieceTokenizer Examples (Python)\nDESCRIPTION: Illustrates how the generic `Tokenizer` class can be configured to use `SentencePieceTokenizer` as its underlying tokenization engine. Examples show integration with `fastai.data.core.Datasets` for both folder-based data (`Tokenizer.from_folder`) and DataFrame-based data (`Tokenizer.from_df`), demonstrating flexible usage of the `Tokenizer` wrapper.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/30_text.core.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwith tempfile.TemporaryDirectory() as tmp_d:\n    path,df,csv_fname = _prepare_texts(Path(tmp_d))\n    items = get_text_files(path)\n    splits = RandomSplitter()(items)\n    tok = SentencePieceTokenizer(special_toks=[])\n    dsets = Datasets(items, [Tokenizer.from_folder(path, tok=tok)], splits=splits)\n    print(dsets.train[0][0])\n    \nwith warnings.catch_warnings():\n    dsets = Datasets(df, [Tokenizer.from_df('text', tok=tok)], splits=splits)\n    print(dsets.train[0][0].text)\n```\n\n----------------------------------------\n\nTITLE: Initializing TextDataLoaders from DataFrame for LM\nDESCRIPTION: Creates a TextDataLoaders object from a DataFrame using `from_df` method for language modeling. It specifies column names for text and validation sets. `is_lm=True` enables language modeling functionality. `show_batch` displays a small sample of the loaded data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndls = TextDataLoaders.from_df(df, path=path, text_col='text', is_lm=True, valid_col='is_valid')\ndls.show_batch(max_n=3)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Categorical Classes - Python\nDESCRIPTION: Accesses and displays the learned mapping of categorical levels for a specific column ('DayOfWeek') after the `Categorify` processor has been applied.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nto.classes['DayOfWeek']\n```\n\n----------------------------------------\n\nTITLE: Retrieving Top Losses from Interpretation (Python)\nDESCRIPTION: Provides the `top_losses` method to retrieve the `k` largest or smallest losses and their corresponding indices. It can optionally return the original input items associated with these losses, defaulting to returning all losses if `k` is not specified.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef top_losses(self,\n        k:int|None=None, # Return `k` losses, defaults to all\n        largest:bool=True, # Sort losses by largest or smallest\n        items:bool=False # Whether to return input items\n    ):\n        \"`k` largest(/smallest) losses and indexes, defaulting to all losses.\"\n        losses, idx = self.losses.topk(ifnone(k, len(self.losses)), largest=largest)\n        if items: return losses, idx, getattr(self.dl.items, 'iloc', L(self.dl.items))[idx]\n        else:     return losses, idx\n```\n\n----------------------------------------\n\nTITLE: Show Classification Results with Labels\nDESCRIPTION: This utility visualizes classification predictions alongside true labels, coloring the labels based on correctness (green for correct, red for incorrect). It displays a grid of samples with predictions and labels, useful for quick qualitative assessments.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_40\n\nLANGUAGE: Python\nCODE:\n```\n@dispatch\ndef show_results(x:TensorImage, y:TensorCategory, samples, outs, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=None, **kwargs):\n    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), nrows=nrows, ncols=ncols, figsize=figsize)\n    for i in range(2):\n        ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]\n    ctxs = [r.show(ctx=c, color='green' if b==r else 'red', **kwargs)\n            for b,r,c,_ in zip(samples.itemgot(1),outs.itemgot(0),ctxs,range(max_n))]\n    return ctxs\n```\n\n----------------------------------------\n\nTITLE: Datasets Class for Tuple Creation with Transforms\nDESCRIPTION: The `Datasets` class constructs dataset objects that produce tuples of items, applying specified transforms (`tfms`) to each item. It manages splitting, subset selection, and decoding, allowing flexible data processing pipelines. Dependencies include TfmdLists and related utility functions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_41\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n@docs\n@delegates(TfmdLists)\nclass Datasets(FilteredBase):\n    \"\"\"A dataset that creates a tuple from each `tfms`\"\"\"\n    def __init__(self, \n        items:list=None, # List of items to create `Datasets`\n        tfms:MutableSequence|Pipeline=None, # List of `Transform`(s) or `Pipeline` to apply\n        tls:TfmdLists=None, # If None, `self.tls` is generated from `items` and `tfms`\n        n_inp:int=None, # Number of elements in `Datasets` tuple that should be considered part of input\n        dl_type=None, # Default type of `DataLoader` used when function `FilteredBase.dataloaders` is called\n        **kwargs\n    ):\n        super().__init__(dl_type=dl_type)\n        self.tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n        self.n_inp = ifnone(n_inp, max(1, len(self.tls)-1))\n\n    def __getitem__(self, it):\n        res = tuple([tl[it] for tl in self.tls])\n        return res if is_indexer(it) else list(zip(*res))\n\n    def __getattr__(self,k): return gather_attrs(self, k, 'tls')\n    def __dir__(self): return super().__dir__() + gather_attr_names(self, 'tls')\n    def __len__(self): return len(self.tls[0])\n    def __iter__(self): return (self[i] for i in range(len(self)))\n    def __repr__(self): return coll_repr(self)\n    def decode(self, o, full=True): return tuple(tl.decode(o_, full=full) for o_,tl in zip(o,tuplify(self.tls, match=o)))\n    def subset(self, i): return type(self)(tls=L(tl.subset(i) for tl in self.tls), n_inp=self.n_inp)\n    def _new(self, items, *args, **kwargs): return super()._new(items, tfms=self.tfms, do_setup=False, **kwargs)\n    def overlapping_splits(self): return self.tls[0].overlapping_splits()\n    def new_empty(self): return type(self)(tls=[tl.new_empty() for tl in self.tls], n_inp=self.n_inp)\n    @property\n    def splits(self): return self.tls[0].splits\n    @property\n    def split_idx(self): return self.tls[0].tfms.split_idx\n    @property\n    def items(self): return self.tls[0].items\n    @items.setter\n    def items(self, v):\n        for tl in self.tls: tl.items = v\n\n    def show(self, o, ctx=None, **kwargs):\n        for o_,tl in zip(o,self.tls): ctx = tl.show(o_, ctx=ctx, **kwargs)\n        return ctx\n\n    @contextmanager\n    def set_split_idx(self, i):\n        old_split_idx = self.split_idx\n        for tl in self.tls: tl.tfms.split_idx = i\n        try: yield self\n        finally:\n            for tl in self.tls: tl.tfms.split_idx = old_split_idx\n\n    _docs=dict(\n        decode=\"Compose `decode` of all `tuple_tfms` then all `tfms` on `i`\",\n        show=\"Show item `o` in `ctx`\",\n        dataloaders=\"Get a `DataLoaders`\",\n        overlapping_splits=\"All splits that are in more than one split\",\n        subset=\"New `Datasets` that only includes subset `i`\",\n        new_empty=\"Create a new empty version of the `self`, keeping only the transforms\",\n        set_split_idx=\"Contextmanager to use the same `Datasets` with another `split_idx`\"\n    )\n        \n// Note: Additional implementation details and comments describe class behavior and relationships.\n\n```\n\n----------------------------------------\n\nTITLE: Testing Learner GPU Support with Callbacks in fastai (Python)\nDESCRIPTION: This code runs the training loop with the TestTrainEvalCallback, explicitly placing the synthetic Learner on GPU if available. It serves to confirm that models and data properly move to CUDA devices and that callbacks work seamlessly in this environment. CUDA must be available and dependencies include synth_learner and TestTrainEvalCallback.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#|cuda\n#Check model is put on the GPU if needed\nlearn = synth_learner(cbs=TestTrainEvalCallback, cuda=True)\nlearn.fit(1)\n```\n\n----------------------------------------\n\nTITLE: Example imports for vision modules\nDESCRIPTION: Imports vision-related modules from fastai for example purposes, not exported as part of the API.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#For example, so not exported\nfrom fastai.vision.core import *\nfrom fastai.vision.data import *\n```\n\n----------------------------------------\n\nTITLE: Defining TensorCategory and TensorMultiCategory Subclasses for Categorical Data in Python\nDESCRIPTION: Implements TensorCategory subclass of TensorBase to represent categorical labels as tensors, enabling indexing and integration with image tensors. Registers indexing operations between TensorImageBase and TensorCategory. Also defines TensorMultiCategory as a subclass of TensorCategory for handling multi-label categorical tensors, enhancing category-related metadata capabilities in fastai.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_60\n\nLANGUAGE: python\nCODE:\n```\nclass TensorCategory(TensorBase): pass\n\nTensorBase.register_func(Tensor.__getitem__, TensorImageBase, TensorCategory)\n```\n\nLANGUAGE: python\nCODE:\n```\ntc = TensorCategory([1,2,3])\nmask_t = TensorMask([0,2,4,5])\nim_t = TensorImage([0,2,4,5])\ntest_eq(mask_t[tc], tensor([2,4,5]))\ntest_eq(im_t[tc], tensor([2,4,5]))\n```\n\nLANGUAGE: python\nCODE:\n```\nclass TensorMultiCategory(TensorCategory): pass\n```\n\n----------------------------------------\n\nTITLE: Creating an Empty Dataset Instance\nDESCRIPTION: Calls `new_empty` on a dataset to generate an empty dataset, verifying that it contains no items afterward.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_65\n\nLANGUAGE: Python\nCODE:\n```\nshow_doc(Datasets.new_empty)\n\nitems = [1,2,3,4]\nnrm = Norm()\ndsets = Datasets(items, [[neg_tfm,int2f_tfm], [neg_tfm]])\nempty = dsets.new_empty()\ntest_eq(empty.items, [])\n```\n\n----------------------------------------\n\nTITLE: hook_output Function Usage Example\nDESCRIPTION: This snippet demonstrates the usage of the `hook_output` function. It creates a hook on a linear layer using `hook_output`, performs a forward pass, and verifies that the stored activations are equal to the output of the layer and that they are detached from the computation graph. It also demonstrates how to use `hook_output` to store gradients during a backward pass and verifies the stored gradients.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntst_model = nn.Linear(5,10)\nx = torch.randn(4,5)\nwith hook_output(tst_model) as h:\n    y = tst_model(x)\n    test_eq(y, h.stored)\n    assert not h.stored.requires_grad\n    \nwith hook_output(tst_model, grad=True) as h:\n    y = tst_model(x)\n    loss = y.pow(2).mean()\n    loss.backward()\n    test_close(2*y / y.numel(), h.stored[0])\n```\n\n----------------------------------------\n\nTITLE: Training Model with One Cycle Policy (Python)\nDESCRIPTION: This line initiates the training process using fastai's `fit_one_cycle` method, a recommended training approach that implements the superconvergence paper's one-cycle learning rate and momentum policy. It trains the model for a specified number of `epochs` using a maximum learning rate `lr`, automatically handling the scheduling of these parameters during training. This single line replaces the entire manual epoch iteration loop from the original PyTorch code.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_pytorch.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nlearn.fit_one_cycle(epochs, lr)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Model Network Conversion to FP16 in fastai Python\nDESCRIPTION: This example demonstrates the usage of the `convert_network` utility function to change the precision of model parameters. It shows how linear layers are converted to FP16 while BatchNorm layers are kept in FP32 for stability, and then verifies the dtypes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmodel = nn.Sequential(nn.Linear(10,30), nn.BatchNorm1d(30), nn.Linear(30,2)).cuda()\nmodel = convert_network(model, torch.float16)\n\nfor i,t in enumerate([torch.float16, torch.float32, torch.float16]):\n    test_eq(model[i].weight.dtype, t)\n    test_eq(model[i].bias.dtype,   t)\n    \nmodel = nn.Sequential(nn.Linear(10,30), BatchNorm(30, ndim=1), nn.Linear(30,2)).cuda()\nmodel = convert_network(model, torch.float16)\n\nfor i,t in enumerate([torch.float16, torch.float32, torch.float16]):\n    test_eq(model[i].weight.dtype, t)\n    test_eq(model[i].bias.dtype,   t)\n```\n\n----------------------------------------\n\nTITLE: Defining LinearDecoder Class for Language Models in Python\nDESCRIPTION: LinearDecoder is a PyTorch module acting as a decoder atop an RNNCore, converting last-layer hidden states to outputs representing vocabulary logits. It supports optional tying of its weight matrix to an encoder's embedding weights to reduce model parameters. Dropout is applied to inputs before linear transformation, aiding regularization. Key parameters include number of output units (vocab size), hidden dimension size, dropout probability, optional tied encoder module, and bias inclusion.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass LinearDecoder(Module):\n    \"To go on top of a RNNCore module and create a Language Model.\"\n    initrange=0.1\n\n    def __init__(self, \n        n_out:int, # Number of output channels \n        n_hid:int, # Number of features in encoder last layer output\n        output_p:float=0.1, # Input dropout probability\n        tie_encoder:nn.Module=None, # If module is supplied will tie decoder weight to `tie_encoder.weight`  \n        bias:bool=True # If `False` the layer will not learn additive bias\n    ):\n        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n        self.output_dp = RNNDropout(output_p)\n        if bias: self.decoder.bias.data.zero_()\n        if tie_encoder: self.decoder.weight = tie_encoder.weight\n\n    def forward(self, input):\n        dp_inp = self.output_dp(input)\n        return self.decoder(dp_inp), input, dp_inp\n```\n\n----------------------------------------\n\nTITLE: Training the Tabular Model\nDESCRIPTION: Trains the tabular learner model using the one-cycle policy. This method is efficient for training neural networks and is applied here for 1 epoch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nlearn.fit_one_cycle(1)\n```\n\n----------------------------------------\n\nTITLE: Find learning rate\nDESCRIPTION: Uses the `lr_find` method to find an appropriate learning rate for training the model. This helps in selecting a learning rate that allows for stable and effective training.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nlearn.lr_find()\n```\n\n----------------------------------------\n\nTITLE: Initializing TextDataLoaders from DataFrame with Tokenizer\nDESCRIPTION: Initializes a TextDataLoaders object from a Pandas DataFrame using the `from_df` method. A custom tokenizer (`WordTokenizer`) is passed via the `tok_tfm` parameter. It specifies text and label columns, indicates validation data, and sets whether it's a language model or not. The batch is displayed for verification.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ntknzer = WordTokenizer()\ndls = TextDataLoaders.from_df(df, path=path, is_lm=False, tok_tfm=tknzer,\n                              text_col='text', label_col='label', valid_col='is_valid'\n                             )\ndls.show_batch(max_n=1)\ndls = TextDataLoaders.from_df(df, path=path, is_lm=True, tok_tfm=tknzer,\n                              text_col='text', label_col='label', valid_col='is_valid'\n                             )\ndls.show_batch(max_n=1)\n```\n\n----------------------------------------\n\nTITLE: Creating Text Classifier Learner\nDESCRIPTION: This initializes a `Learner` object suitable for text classification using a pretrained model (AWD_LSTM).  It specifies the `DataLoaders` and the model architecture, and sets `drop_mult` to control dropout magnitude and `metrics` to track accuracy.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n```\n\n----------------------------------------\n\nTITLE: Unfreezing All Layers and Final Fine-tuning of Classifier (Python)\nDESCRIPTION: Fully unfreezes the classifier model, reduces learning rate further by factor of 5, and trains for two more epochs with graduated learning rate scheduling. Input: classifier Learner, final lr; Output: final, fully fine-tuned text classifier.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nlearn.unfreeze()\nlr /= 5\nlearn.fit_one_cycle(2, slice(lr/(2.6**4),lr), moms=(0.8,0.7,0.8), wd=0.1)\n```\n\n----------------------------------------\n\nTITLE: Example Backward Hook Registration\nDESCRIPTION: This snippet defines a simple backward hook function `example_backward_hook` that prints the module, gradient of the input, and gradient of the output of a layer. It registers this hook with a linear layer `tst_model`, performs a forward and backward pass, and then removes the hook. This demonstrates the basic usage of backward hooks in PyTorch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef example_backward_hook(m,gi,go): print(m,gi,go)\nhook = tst_model.register_backward_hook(example_backward_hook)\n\nx = torch.randn(4,5)\ny = tst_model(x)\nloss = y.pow(2).mean()\nloss.backward()\nhook.remove()\n```\n\n----------------------------------------\n\nTITLE: Expanding Rank-1 Tensors for DataFrame - Python\nDESCRIPTION: This function checks if the input object `o` is a rank-1 tensor. If it is, it expands the dimensions by adding a new axis, effectively turning it into a column vector. Otherwise, it returns the original object without modification. This handles cases where single dimensional tensors need to be reshaped for operations requiring 2D tensors or arrays.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndef _maybe_expand(o): return o[:,None] if o.ndim==1 else o\n```\n\n----------------------------------------\n\nTITLE: Defining European Languages List (Python)\nDESCRIPTION: Defines a simple Python list named `eu_langs` containing two-letter ISO 639-1 language codes for several European languages. This list is likely used within other parts of the library to provide language-specific default configurations, such as character coverage settings for tokenizers.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/30_text.core.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#|export\neu_langs = [\"bg\", \"cs\", \"da\", \"de\", \"el\", \"en\", \"es\", \"et\", \"fi\", \"fr\", \"ga\", \"hr\", \"hu\",\n            \"it\",\"lt\",\"lv\",\"mt\",\"nl\",\"pl\",\"pt\",\"ro\",\"sk\",\"sl\",\"sv\"] # all European langs\n```\n\n----------------------------------------\n\nTITLE: Context Manager for Distributed Training (distrib_ctx) - Python\nDESCRIPTION: Defines a context manager for safely adapting a fastai Learner to distributed data parallel training, handling setup of distributed process groups (especially within Jupyter notebooks), and managing cleanup. Handles Accelerate import, context entry/exit, and proper callback attachment/detachment. Dependencies: fastai Learner, Accelerate, torch, contextlib. In notebook mode, handles rank-based printing and distributed group initialization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\n@patch\n@contextmanager\n@delegates(Accelerator, but=_hidden_params)\ndef distrib_ctx(self: Learner,\n        sync_bn=True, # Whether to replace all batch norm with `nn.SyncBatchNorm`\n        in_notebook=False, # Whether we are launching from a notebook or not\n        **kwargs\n   ):\n    \"A context manager to adapt a learner to train in distributed data parallel mode.\"\n    try: import accelerate\n    except ImportError as e: \n        e.args = [\"Accelerate is required. Install with `pip install accelerate`\"]\n        raise\n    # Adapt self to DistributedDataParallel, yield, and cleanup afterwards.\n    cleanup_dpg = False\n    try:\n        if in_notebook:\n            cuda_id = rank_distrib()\n            if not torch.distributed.is_initialized():\n                setup_distrib(cuda_id)\n                cleanup_dpg = torch.distributed.is_initialized()\n            if not rank_distrib(): print(\"Training Learner...\")\n        if num_distrib(): self.to_distributed(sync_bn, **kwargs)\n        yield self\n    finally:\n        self.detach_distributed()\n        if cleanup_dpg: teardown_distrib()\n```\n\n----------------------------------------\n\nTITLE: Defining a Refactored CNN Model\nDESCRIPTION: This defines another CNN model using the refactored `conv2` function and `nn.Sequential`. It has a similar architecture to the previous model but uses the refactored convolutional layers. This refactoring improves readability and maintainability.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\nmodel = nn.Sequential(\n    conv2(1, 8),   # 14\n    conv2(8, 16),  # 7\n    conv2(16, 32), # 4\n    conv2(32, 16), # 2\n    conv2(16, 10), # 1\n    Flatten()      # remove (1,1) grid\n)\n```\n\n----------------------------------------\n\nTITLE: Remapping Embeddings for a New Vocabulary - Python\nDESCRIPTION: Defines match_embeds, a function that adapts pretrained embedding weights and biases from an old vocabulary to a new vocabulary. It copies matching embeddings by index, uses mean vectors for unknown words, and handles both weights and decoder bias keys. Dependencies: torch, fastai DataLoaders, and that the old_weights dictionary matches the expected keynames. Inputs: old_wgts (dict of weights), old_vocab (list), new_vocab (list). Returns a dictionary with retargeted embedding weights. Limitation: Only supports state dicts with specific key formats and may fail if underlying tensor shapes are inconsistent.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef match_embeds(\n    old_wgts:dict, # Embedding weights  \n    old_vocab:list, # Vocabulary of corpus used for pre-training\n    new_vocab:list # Current corpus vocabulary\n) -> dict:\n    \"Convert the embedding in `old_wgts` to go from `old_vocab` to `new_vocab`.\"\n    bias, wgts = old_wgts.get('1.decoder.bias', None), old_wgts['0.encoder.weight']\n    wgts_m = wgts.mean(0)\n    new_wgts = wgts.new_zeros((len(new_vocab),wgts.size(1)))\n    if bias is not None:\n        bias_m = bias.mean(0)\n        new_bias = bias.new_zeros((len(new_vocab),))\n    old_o2i = old_vocab.o2i if hasattr(old_vocab, 'o2i') else {w:i for i,w in enumerate(old_vocab)}\n    for i,w in enumerate(new_vocab):\n        idx = old_o2i.get(w, -1)\n        new_wgts[i] = wgts[idx] if idx>=0 else wgts_m\n        if bias is not None: new_bias[i] = bias[idx] if idx>=0 else bias_m\n    old_wgts['0.encoder.weight'] = new_wgts\n    if '0.encoder_dp.emb.weight' in old_wgts: old_wgts['0.encoder_dp.emb.weight'] = new_wgts.clone()\n    old_wgts['1.decoder.weight'] = new_wgts.clone()\n    if bias is not None: old_wgts['1.decoder.bias'] = new_bias\n    return old_wgts\n```\n\n----------------------------------------\n\nTITLE: Showing a Batch of Data - Python\nDESCRIPTION: This snippet displays a batch of the data, giving a preview of the data's structure after preprocessing and batching.  `show_batch` helps to visualize the transformed data and verify that preprocessing steps have been applied correctly, and allows for debugging and understanding the data format.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-tabular.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndls.show_batch()\n```\n\n----------------------------------------\n\nTITLE: Adding Documentation and Defaults for Recorder in fastai (Python)\nDESCRIPTION: This code assigns detailed documentation strings to various Recorder methods using the 'add_docs' function for better introspection within interactive environments. It also ensures that 'Recorder' is present in the module's callback defaults. No parameters or outputs are expected from running this snippet directly. Dependencies: 'add_docs' and 'defaults' objects from fastai.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_68\n\nLANGUAGE: python\nCODE:\n```\n#|export\nadd_docs(Recorder,\n         before_train = \"Reset loss and metrics state\",\n         after_train = \"Log loss and metric values on the training set (if `self.training_metrics=True`)\",\n         before_validate = \"Reset loss and metrics state\",\n         after_validate = \"Log loss and metric values on the validation set\",\n         after_cancel_train = \"Ignore training metrics for this epoch\",\n         after_cancel_validate = \"Ignore validation metrics for this epoch\",\n         plot_loss = \"Plot the losses from `skip_start` and onward. Optionally `log=True` for logarithmic axis, `show_epochs=True` for indicate epochs and a matplotlib axis `ax` to plot on.\")\n\nif Recorder not in defaults.callbacks: defaults.callbacks.append(Recorder)\n```\n\n----------------------------------------\n\nTITLE: Plotting Learning Rate Finder Results with Recorder Patch in Python\nDESCRIPTION: This patched method on the Recorder class visualizes the learning rate finder results by plotting loss against learning rate on a logarithmic scale. It optionally annotates suggested learning rates provided by the different suggestion methods with custom markers and legends. This facilitates an intuitive interpretation of the LR Finder outputs during model tuning.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\n@patch\n    def plot_lr_find(self:Recorder, skip_end=5, return_fig=True, suggestions=None, nms=None, **kwargs):\n        \"Plot the result of an LR Finder test (won't work if you didn't do `learn.lr_find()` before)\"\n        lrs    = self.lrs    if skip_end==0 else self.lrs   [:-skip_end]\n        losses = self.losses if skip_end==0 else self.losses[:-skip_end]\n        fig, ax = plt.subplots(1,1)\n        ax.plot(lrs, losses)\n        ax.set_ylabel(\"Loss\")\n        ax.set_xlabel(\"Learning Rate\")\n        ax.set_xscale('log')\n        if suggestions:\n            colors = plt.rcParams['axes.prop_cycle'].by_key()['color'][1:]\n            for (val, idx), nm, color in zip(suggestions, nms, colors):\n                ax.plot(val, idx, 'o', label=nm, c=color)\n            ax.legend(loc='best')\n```\n\n----------------------------------------\n\nTITLE: Set Tabular _dbunch_type and delegates\nDESCRIPTION: This code sets the `_dbunch_type` attribute of the `Tabular` class to `TabularDataLoaders` and delegates the `from_csv` method of `TabularDataLoaders` to the `from_df` method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/41_tabular.data.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nTabular._dbunch_type = TabularDataLoaders\nTabularDataLoaders.from_csv = delegates(to=TabularDataLoaders.from_df)(TabularDataLoaders.from_csv)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Listing wikitext-2 Dataset Files (Python)\nDESCRIPTION: Downloads the tiny wikitext-2 dataset and lists its contents using fastai's untar_data utility function and Path object. Returns the path to downloaded data and lists available files for further processing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.WIKITEXT_TINY)\npath.ls()\n```\n\n----------------------------------------\n\nTITLE: Creating a Segmentation Learner Example\nDESCRIPTION: This code example loads tiny CamVid data, defines a label function, and creates data loaders appropriate for image segmentation tasks. It then initializes a U-Net learner with a ResNet34 backbone, cross-entropy loss, and a specified label range. Demonstrates practical usage of the above function for segmentation tasks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\npath = untar_data(URLs.CAMVID_TINY)\nfnames = get_image_files(path/'images')\ndef label_func(x): return path/'labels'/f'{x.stem}_P{x.suffix}'\ncodes = np.loadtxt(path/'codes.txt', dtype=str)\ndls = SegmentationDataLoaders.from_label_func(path, fnames, label_func, codes=codes)\n\nlearn = unet_learner(dls, models.resnet34, loss_func=CrossEntropyLossFlat(axis=1), y_range=(0,1))\n```\n\n----------------------------------------\n\nTITLE: Setting the default export for the module\nDESCRIPTION: This code snippet sets the default export for the module by specifying the desired path. This is used by nbdev.  This line isn't related to the current code.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp callback.core\n```\n\n----------------------------------------\n\nTITLE: Checking for FP16 Gradient Overflow using Summation in PyTorch (Python)\nDESCRIPTION: Tests if a PyTorch tensor, typically representing FP16 gradients, contains overflow values (infinity or NaN). It performs this check by summing the tensor elements after converting to FP32 and checking if the sum is infinite or NaN. This summation approach is noted as potentially faster than checking each element individually using methods like `torch.isinf(x).any()`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n#|export \ndef test_overflow(x:torch.Tensor):\n    \"Tests whether fp16 gradients have overflown.\"\n    s = float(x.float().sum())\n    return (s == float('inf') or s == float('-inf') or s != s)\n```\n\n----------------------------------------\n\nTITLE: Example Usage of TabularModel with Sample Inputs in Python\nDESCRIPTION: Demonstrates creating an instance of `TabularModel` with specified embedding sizes, continuous variable count, output size, and layer sizes. It then defines example input tensors for categorical and continuous data, performs a forward pass calling the model to obtain output predictions. This snippet shows expected input shapes and tensor types, and the usage pattern for the model defined earlier.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/42_tabular.model.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nemb_szs = [(4,2), (17,8)]\nm = TabularModel(emb_szs, n_cont=2, out_sz=2, layers=[200,100]).eval()\nx_cat = torch.tensor([[2,12]]).long()\nx_cont = torch.tensor([[0.7633, -0.1887]]).float()\nout = m(x_cat, x_cont)\n```\n\n----------------------------------------\n\nTITLE: Download Images Function - Test\nDESCRIPTION: This code tests the `download_images` function. It creates a temporary directory and a text file containing image URLs. Then, it calls `download_images` to download the images from the URLs listed in the file. It verifies the file's content and the downloaded images to ensure they were downloaded successfully and with the correct filenames.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nwith tempfile.TemporaryDirectory() as d:\n    d = Path(d)\n    url_file = d/'urls.txt'\n    url_file.write_text(\"\\n\".join([f\"https://www.fast.ai/images/{n}\" for n in \"jh-head.jpg headshot-small.jpg\".split()]))\n    \n    download_images(d, url_file, preserve_filename=True)\n    assert (d/'jh-head.jpg').is_file()\n    assert (d/'headshot-small.jpg').is_file()\n    assert not (d/'jh-head1.jpg').exists()\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders - Python\nDESCRIPTION: Generates fastai DataLoaders from the processed `TabularPandas` object. DataLoaders handle batching and data loading for training and validation, using a specified batch size.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndls = to.dataloaders(bs=512, path=path)\n```\n\n----------------------------------------\n\nTITLE: Importing GPT2 Model and Tokenizer from transformers (Python)\nDESCRIPTION: Imports the GPT2LMHeadModel and GPT2TokenizerFast classes from the transformers library to be used for loading a pretrained GPT-2 model and its tokenizer. These imports are prerequisites for the following code snippets that load and utilize these classes for tokenization and model inference.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\n```\n\n----------------------------------------\n\nTITLE: Applying fastai Tokenizer to Test Set (Python)\nDESCRIPTION: Shows how to use the `Tokenizer` transform with `fastai.data.core.test_set`. This applies the previously configured tokenization process from a `Datasets` object to a new list of text strings, ensuring consistent preprocessing for inference or evaluation. The output is a list of tokenized text representations.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/30_text.core.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntst = test_set(dsets, ['This is a test', 'this is another test'])\ntest_eq(tst, [(['xxbos', 'xxmaj', 'this','is','a','test'],), \n              (['xxbos','this','is','another','test'],)])\n```\n\n----------------------------------------\n\nTITLE: Testing Type Recovery for Visualization in Python\nDESCRIPTION: Tests that the DataLoader correctly recovers tensor types even after they have been cast to generic tensors, which emulates predictions in a model.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nx,y = cast(x,Tensor),cast(y,Tensor) #Lose type of tensors (to emulate predictions)\ntest_ne(type(x), TensorImage)\ntdl.show_batch((x,y), figsize=(1,1)) #Check that types are put back by dl.\n```\n\n----------------------------------------\n\nTITLE: Adding wrapper to enable channels last format with optional mixed precision\nDESCRIPTION: Extends the Learner with a method to set the model and inputs to channels last format, optionally enabling mixed precision (FP16 or BF16). It intelligently adds callbacks if needed to support mixed precision and channels last formatting on compatible hardware.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18c_callback.channelslast.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n@patch\ndef to_channelslast(self:Learner,\n    use_amp:bool=True, # Add `MixedPrecision` with `amp_mode`. Recommended for full channels last performance\n    amp_mode:str|AMPMode=AMPMode.FP16, # Mixed Precision training mode. Supports fp16 and bf16.\n    **kwargs\n):\n    \"Set `Learner` and inputs to `channels_last` format and float16 Mixed Precision by default\"\n    if use_amp and not hasattr(self, 'mixed_precision') and not hasattr(self, 'channels_last'):\n        return self.add_cbs([ChannelsLast(), MixedPrecision(amp_mode, **kwargs)])\n    elif not hasattr(self, 'channels_last'):\n        return self.add_cb(ChannelsLast())\n```\n\n----------------------------------------\n\nTITLE: Implementing SaveModelCallback in FastAI\nDESCRIPTION: A TrackerCallback implementation that saves the model when it achieves the best monitored value during training and optionally loads it at the end. Options include saving at every epoch or at the end of training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass SaveModelCallback(TrackerCallback):\n    \"A `TrackerCallback` that saves the model's best during training and loads it at the end.\"\n    order = TrackerCallback.order+1\n    def __init__(self, \n        monitor='valid_loss', # value (usually loss or metric) being monitored.\n        comp=None, # numpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n        min_delta=0., # minimum delta between the last monitor value and the best monitor value.\n        fname='model', # model name to be used when saving model.\n        every_epoch=False, # if true, save model after every epoch; else save only when model is better than existing best.\n        at_end=False, # if true, save model when training ends; else load best model if there is only one saved model.\n        with_opt=False, # if true, save optimizer state (if any available) when saving model. \n        reset_on_fit=True # before model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss).\n    ):\n        super().__init__(monitor=monitor, comp=comp, min_delta=min_delta, reset_on_fit=reset_on_fit)\n        assert not (every_epoch and at_end), \"every_epoch and at_end cannot both be set to True\"\n        # keep track of file path for loggers\n        self.last_saved_path = None\n        store_attr('fname,every_epoch,at_end,with_opt')\n\n    def _save(self, name): self.last_saved_path = self.learn.save(name, with_opt=self.with_opt)\n\n    def after_epoch(self):\n        \"Compare the value monitored to its best score and save if best.\"\n        if self.every_epoch:\n            if (self.epoch%self.every_epoch) == 0: self._save(f'{self.fname}_{self.epoch}')\n        else: #every improvement\n            super().after_epoch()\n            if self.new_best:\n                print(f'Better model found at epoch {self.epoch} with {self.monitor} value: {self.best}.')\n                self._save(f'{self.fname}')\n\n    def after_fit(self, **kwargs):\n        \"Load the best model.\"\n        if self.at_end: self._save(f'{self.fname}')\n        elif not self.every_epoch: self.learn.load(f'{self.fname}', with_opt=self.with_opt)\n```\n\n----------------------------------------\n\nTITLE: Constructing and Configuring Text Classification Learner (Python)\nDESCRIPTION: Creates a text classifier learner using AWD_LSTM architecture, accuracy metric, weight decay regularization, and partial Adam optimizer. drop_mult is set for regularization scaling. Inputs: classification dls, pretrained encoder path, optimizer, and architecture. Output: prepared Learner model.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nopt_func = partial(Adam, wd=0.1)\nlearn = text_classifier_learner(dls, AWD_LSTM, metrics=[accuracy], path=path, drop_mult=0.5, opt_func=opt_func)\n```\n\n----------------------------------------\n\nTITLE: Integration Test for fit_one_cycle\nDESCRIPTION: Performs an integration test to verify the functionality of `fit_one_cycle`.  It trains a model for a few epochs and asserts that the loss has decreased, indicating that the training process is working correctly. It uses `synth_learner` to create a synthetic learner and depends on other fastai components for calculating loss and model training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#Integration test: training a few epochs should make the model better\nlearn = synth_learner(lr=1e-2)\nxb,yb = learn.dls.one_batch()\ninit_loss = learn.loss_func(learn.model(xb), yb)\nlearn.fit_one_cycle(2)\nxb,yb = learn.dls.one_batch()\nfinal_loss = learn.loss_func(learn.model(xb), yb)\nassert final_loss < init_loss\n```\n\n----------------------------------------\n\nTITLE: Creating parallel_ctx Context Manager for Learner\nDESCRIPTION: Adds a context manager to the Learner class that temporarily enables parallel training within its scope. Ensures proper cleanup by detaching parallel mode after execution.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@patch\n@contextmanager\ndef parallel_ctx(self: Learner, device_ids=None):\n    \"A context manager to adapt a learner to train in data parallel mode.\"\n    try:\n        self.to_parallel(device_ids)\n        yield self\n    finally: self.detach_parallel()\n```\n\n----------------------------------------\n\nTITLE: Defining a CNN model with Batchnorm\nDESCRIPTION: This defines a CNN model using `nn.Sequential` and the `conv` function. It comprises several convolutional layers, batch normalization layers, ReLU activation functions, and a `Flatten` layer. The convolutional layers extract features, batch normalization stabilizes training, ReLU introduces non-linearity, and Flatten reduces the output to a vector.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nmodel = nn.Sequential(\n    conv(1, 8), # 14\n    nn.BatchNorm2d(8),\n    nn.ReLU(),\n    conv(8, 16), # 7\n    nn.BatchNorm2d(16),\n    nn.ReLU(),\n    conv(16, 32), # 4\n    nn.BatchNorm2d(32),\n    nn.ReLU(),\n    conv(32, 16), # 2\n    nn.BatchNorm2d(16),\n    nn.ReLU(),\n    conv(16, 10), # 1\n    nn.BatchNorm2d(10),\n    Flatten()     # remove (1,1) grid\n)\n```\n\n----------------------------------------\n\nTITLE: Choosing pretrained Model Architecture - Python\nDESCRIPTION: Selects the ResNet50 architecture to use as the backbone for the image classifier. Uses fastai's model import, with no additional configuration at this stage. No parameters taken; 'arch' is typically fed to the learner.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\narch = resnet50\n```\n\n----------------------------------------\n\nTITLE: Plot Learning Rate Finder Results\nDESCRIPTION: Plots the results of the learning rate finder, skipping the last 15 values to focus on the relevant part of the plot.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nlearn.recorder.plot_lr_find(skip_end=15)\n```\n\n----------------------------------------\n\nTITLE: Initializing LR Finder Callback - FastAI Python\nDESCRIPTION: Defines the LRFinder callback class inheriting from ParamScheduler to perform training with an exponentially growing learning rate. The constructor accepts the starting and ending learning rates, the number of iterations over which to increase the LR, and whether to stop training if the loss diverges significantly. It sets up the learning rate schedules and stores internal state to manage training flow. Input parameters include start_lr (initial learning rate), end_lr (final learning rate), num_it (number of iterations), and stop_div (flag to stop on loss divergence). This class is used as a callback during training to adjust learning rates dynamically and stop early if loss explodes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@docs\nclass LRFinder(ParamScheduler):\n    \"Training with exponentially growing learning rate\"\n    def __init__(self, start_lr=1e-7, end_lr=10, num_it=100, stop_div=True):\n        if num_it < 6: num_it = 6\n        self.scheds = {'lr': [SchedExp(s, e) for (s,e) in zip(start_lr,end_lr)\n                             ] if is_listy(start_lr) else SchedExp(start_lr, end_lr)}\n        self.num_it,self.stop_div = num_it,stop_div\n```\n\n----------------------------------------\n\nTITLE: Saving Model and Optimizer State\nDESCRIPTION: This code demonstrates how to save a `Learner`'s model and optimizer state using the `save` method. The `save` method is patched to `Learner` and it saves to a specific file path within the model directory. The `file` parameter can be a `Path`, a `string`, or a buffer. Optional parameters like `with_opt` and `pickle_protocol` can be passed to the underlying `torch.save` call.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\n@patch\n@delegates(save_model)\ndef save(self:Learner, file, **kwargs):\n    \"Save model and optimizer state (if `with_opt`) to `self.path/self.model_dir/file`\"\n    file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n    save_model(file, self.model, getattr(self,'opt',None), **kwargs)\n    return file\n```\n\n----------------------------------------\n\nTITLE: Unfreezing Model for Full Fine-tuning - Python\nDESCRIPTION: Unfreezes earlier layers in the model to allow fine-tuning of all parameters. Used before a second round of learning rate search and training. Requires fastai learner object. Side effect, no outputs.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nlearn.unfreeze()\n```\n\n----------------------------------------\n\nTITLE: Loading MNIST Training and Validation Datasets Using Torchvision and Creating DataLoaders in Python\nDESCRIPTION: Downloads the MNIST dataset for training and validation splits using torchvision.datasets with applied transforms. Then creates PyTorch DataLoader objects with specified batch sizes, shuffling, number of workers, and pinned memory options to efficiently load data during training and validation. These DataLoaders serve as inputs to fastai's DataLoaders wrapper.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_pytorch_verbose.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\n\ntrain_dset = datasets.MNIST('../data', train=True, download=True, transform=tfms)\nvalid_dset = datasets.MNIST('../data', train=False, transform=tfms)\n\ntrain_loader = DataLoader(train_dset, batch_size=256, \n                          shuffle=True, num_workers=1, pin_memory=True)\n\ntest_loader = DataLoader(valid_dset, batch_size=512,\n                         shuffle=False, num_workers=1, pin_memory=True)\n```\n\n----------------------------------------\n\nTITLE: Adding Date Part Features - Python\nDESCRIPTION: Uses the `add_datepart` function to extract various date-related features (like year, month, day of week, day of year, etc.) from a 'Date' column in pandas DataFrames. This is crucial for time series analysis.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nadd_datepart(train, \"Date\", drop=False)\nadd_datepart(test, \"Date\", drop=False)\n```\n\n----------------------------------------\n\nTITLE: Using TfmdLists with a Pandas DataFrame in fastai (Python)\nDESCRIPTION: Demonstrates initializing a `TfmdLists` instance using a Pandas DataFrame as the input `items`. A simple lambda function is used as the transform (adds 1 to column 'a'). The example applies splits, accesses multiple items from the transformed list, and creates and verifies the content of train (`tr`) and validation (`val`) subsets.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(dict(a=[1,2,3],b=[2,3,4]))\ntl = TfmdLists(df, lambda o: o.a+1, splits=[[0],[1,2]])\ntest_eq(tl[1,2], [3,4])\ntr = tl.subset(0)\ntest_eq(tr[:], [2])\nval = tl.subset(1)\ntest_eq(val[:], [3,4])\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Callbacks to Learner\nDESCRIPTION: This code snippet demonstrates adding multiple callbacks at once using the `add_cbs` method on a `Learner` instance. This is useful to add several custom callbacks to enhance the learning procedure. The number of added callbacks are verified using `test_eq`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nlearn.add_cbs([TestTrainEvalCallback(), TestTrainEvalCallback()])\ntest_eq(len(learn.cbs), 4)\n```\n\n----------------------------------------\n\nTITLE: Type-Preserving Convert Function for Items - Python\nDESCRIPTION: Defines fa_convert as a type-preserving version of PyTorch's default_convert for converting single items or batches. Recursively applies to sequences, maintaining nested structure and types, while falling back to PyTorch defaults when applicable. Supports custom batch handling in DataLoader pipelines.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef fa_convert(t):\n    \"A replacement for PyTorch `default_convert` which maintains types and handles `Sequence`s\"\n    return (default_convert(t) if isinstance(t, _collate_types)\n            else type(t)([fa_convert(s) for s in t]) if isinstance(t, Sequence)\n            else default_convert(t))\n```\n\n----------------------------------------\n\nTITLE: Import TensorDataset\nDESCRIPTION: Imports `TensorDataset` from `torch.utils.data`, a crucial component for creating datasets from tensors. It is used to create datasets from the synthetic data generated by the `synth_dbunch` function.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/97_test_utils.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.data import TensorDataset\n```\n\n----------------------------------------\n\nTITLE: Hook Class Usage Example\nDESCRIPTION: This snippet demonstrates how to use the `Hook` class. It creates a hook on a linear layer that stores the output of the layer in the `stored` attribute of the hook. It then performs a forward pass and verifies that the stored output is equal to the actual output of the layer.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntst_model = nn.Linear(5,3)\nhook = Hook(tst_model, lambda m,i,o: o)\ny = tst_model(x)\ntest_eq(hook.stored, y)\n```\n\n----------------------------------------\n\nTITLE: Define PartialDL Dataloader in fastai Python\nDESCRIPTION: Implements a custom `TfmdDL` that loads only a random subset of the dataset during each epoch. The size of this subset is specified by `partial_n`. If `partial_n` is less than the total dataset size, the dataloader's length (`__len__`) is adjusted accordingly.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14a_callback.data.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@delegates()\nclass PartialDL(TfmdDL):\n    \"Select randomly partial quantity of data at each epoch\"\n    def __init__(self, dataset=None, bs=None, partial_n=None, **kwargs):\n        super().__init__(dataset=dataset, bs=bs, **kwargs)\n        self.partial_n = min(partial_n, self.n) if partial_n else None\n\n    def get_idxs(self):\n        if self.partial_n is None: return super().get_idxs()\n        return list(np.random.choice(self.n, self.partial_n, replace=False))\n\n    def __len__(self):\n        if self.partial_n is None: return super().__len__()\n        return self.partial_n//self.bs + (0 if self.drop_last or self.partial_n%self.bs==0 else 1)\n```\n\n----------------------------------------\n\nTITLE: Using Attribute Replacement Context Manager for Testing in Python\nDESCRIPTION: Defines a small helper class '_A' with a context-managed method to temporarily change its 'a' attribute using 'replacing_yield'. Includes assertion-style test logic to ensure the value is restored properly. Useful for demonstrating context manager utilities.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass _A:\n    def __init__(self, a): self.a = a\n    @contextmanager\n    def a_changed(self, v): return replacing_yield(self, 'a', v)\n\na = _A(42)\nwith a.a_changed(32):\n    test_eq(a.a, 32)\ntest_eq(a.a, 42)\n```\n\n----------------------------------------\n\nTITLE: Instantiating and Using CombinedLoss in PyTorch\nDESCRIPTION: Demonstrates the instantiation and usage of the `CombinedLoss` class. It creates random tensors for model output and target and then calls the `cl` instance with these tensors to compute the combined loss. Requires the `CombinedLoss` class and PyTorch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ncl = CombinedLoss()\noutput = torch.randn(32, 4, 5, 10)\ntarget = torch.randint(0,2,(32, 5, 10))\n_ = cl(output, target)\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders for Custom Dataset\nDESCRIPTION: This creates `DataLoader` for the custom dataset `dsrc1`, and applies the same item and batch transforms, but only converts to float tensor. This allows for experimenting with a controlled, smaller dataset and for checking the effect of the item and batch transforms.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\ndbunch1 = dsrc1.dataloaders(bs=bs, after_item=tfms, after_batch=[IntToFloatTensor()])\n```\n\n----------------------------------------\n\nTITLE: BatchNorm Layer Creation\nDESCRIPTION: This function, `BatchNorm`, creates a BatchNorm layer with a specified number of features (`nf`) and dimension (`ndim`). It uses the `_get_norm` function internally. The `norm_type` parameter determines whether the weights are initialized to zero. The `delegates` decorator forwards arguments to `nn.BatchNorm2d`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@delegates(nn.BatchNorm2d)\ndef BatchNorm(nf, ndim=2, norm_type=NormType.Batch, **kwargs):\n    \"BatchNorm layer with `nf` features and `ndim` initialized depending on `norm_type`.\"\n    return _get_norm('BatchNorm', nf, ndim, zero=norm_type==NormType.BatchZero, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Importing Model Metadata for FastAI Text Models\nDESCRIPTION: Imports the `_model_meta` dictionary from `fastai.text.models.core`. This dictionary contains metadata about various text model architectures supported by FastAI, such as default configurations, pretrained model URLs, and layer splitting functions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom fastai.text.models.core import _model_meta\n```\n\n----------------------------------------\n\nTITLE: Defining Lambda Layer for PyTorch Models - Python\nDESCRIPTION: Defines a Lambda layer using the @module decorator, allowing arbitrary function wrapping as a PyTorch nn.Module. The layer requires 'func' during initialization and calls self.func(x) in forward. Dependencies: accepts a callable func; input is tensor x; output is the result of func(x). Enables rapid prototyping of custom transformations.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@module('func')\ndef Lambda(self, x):\n    \"An easy way to create a pytorch layer for a simple `func`\"\n    return self.func(x)\n```\n\n----------------------------------------\n\nTITLE: Merge ratings and movie titles\nDESCRIPTION: Merges the `ratings` DataFrame with the `movies` DataFrame to include movie titles in the ratings data. The merge is performed on the `item` (movieId) column.  The resulting DataFrame is stored in `rating_movie`.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nrating_movie = ratings.merge(movies[[item, title]])\nrating_movie.head()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Learner.one_batch Output Assignment (Python)\nDESCRIPTION: Runs a single batch through the one_batch method of the Learner, then asserts that the instance attributes 'x', 'y', 'pred', and 'loss' are populated with the results of the operation. This snippet relies on a previously constructed Learner and uses fastai's test_eq for validation. Intended to verify internal state consistency after one batch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nb = learn.dls.one_batch()\nlearn.one_batch(0, b)\ntest_eq(learn.x, b[0])\ntest_eq(learn.y, b[1])\nout = learn.model(learn.x)\ntest_eq(learn.pred, out)\ntest_eq(learn.loss, learn.loss_func(out, b[1]))\n```\n\n----------------------------------------\n\nTITLE: Saving Model State After Initial Training - Python\nDESCRIPTION: Saves the learner's trained weights to disk with the label 'stage-1-rn50'. No parameters required beyond a learner already trained. Allows future loading and resuming.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nlearn.save('stage-1-rn50')\n```\n\n----------------------------------------\n\nTITLE: Defining Perplexity Metric for Language Models in PyTorch\nDESCRIPTION: Implements the Perplexity metric class that computes the exponential of the average cross-entropy loss, a standard evaluation metric in language modeling tasks. This class inherits from fastai's AvgLoss and overrides the value property to return the perplexity, which is torch.exp(total loss divided by count). It also defines a name property returning 'perplexity' for identification. The metric requires the model's total loss and count to be accumulated prior to calling value and leverages PyTorch's tensor operations. Example usage shows accumulation across batches and tests equivalence to torch's cross-entropy exponential.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass Perplexity(AvgLoss):\n    \"Perplexity (exponential of cross-entropy loss) for Language Models\"\n    @property\n    def value(self): return torch.exp(self.total/self.count) if self.count != 0 else None\n    @property\n    def name(self):  return \"perplexity\"\n\nperplexity = Perplexity()\n```\n\n----------------------------------------\n\nTITLE: GradientAccumulation with Excess Steps per Batch\nDESCRIPTION: Demonstrates that when gradient accumulation steps are set higher than the number of batches, training and validation loss do not change, effectively simulating large batch training without affecting model performance.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18a_callback.training.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nlearn = synth_learner()\nlearn.fit(1, lr=0.01, cbs=GradientAccumulation(n_acc=1000))\n# ensure valid_loss didn't change\nassert learn.recorder.values[-1][1] == learn.recorder.values[0][1]\n```\n\n----------------------------------------\n\nTITLE: Creating Test Set for Validation and Inference\nDESCRIPTION: Defines a function `test_set` to create a test dataset or list by applying validation transforms from a `Datasets` or `TfmdLists`. Supports optional removal of transforms and handling labeled data. Includes an example with dataset splits and validation of applied transformations.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_68\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n\ndef test_set(\n    dsets:Datasets|TfmdLists, # Map- or iterable-style dataset from which to load the data\n    test_items, # Items in test dataset\n    rm_tfms=None, # Start index of `Transform`(s) from validation set in `dsets` to apply\n    with_labels:bool=False # Whether the test items contain labels\n):\n    \"Create a test set from `test_items` using validation transforms of `dsets`\"\n    if isinstance(dsets, Datasets):\n        tls = dsets.tls if with_labels else dsets.tls[:dsets.n_inp]\n        test_tls = [tl._new(test_items, split_idx=1) for tl in tls]\n        if rm_tfms is None: rm_tfms = [tl.infer_idx(get_first(test_items)) for tl in test_tls]\n        else:               rm_tfms = tuplify(rm_tfms, match=test_tls)\n        for i,j in enumerate(rm_tfms): test_tls[i].tfms.fs = test_tls[i].tfms.fs[j:]\n        return Datasets(tls=test_tls)\n    elif isinstance(dsets, TfmdLists):\n        test_tl = dsets._new(test_items, split_idx=1)\n        if rm_tfms is None: rm_tfms = dsets.infer_idx(get_first(test_items))\n        test_tl.tfms.fs = test_tl.tfms.fs[rm_tfms:]\n        return test_tl\n    else: raise Exception(f\"This method requires using the fastai library to assemble your data. Expected a `Datasets` or a `TfmdLists` but got {dsets.__class__.__name__}\")\n```\n\n----------------------------------------\n\nTITLE: Decoding Model Predictions to Text (Python)\nDESCRIPTION: Converts the model-generated token predictions (from a numpy array) back into text using the tokenizer's decode method. Expects a 1D array of token IDs; output is a readable string. Useful for interpreting generation outputs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntokenizer.decode(preds[0].numpy())\n```\n\n----------------------------------------\n\nTITLE: Preparing and Validating Datasets for Object Detection with BBoxes and Labels - fastai (Python)\nDESCRIPTION: Creates Datasets containing images, bounding box coordinates, and multi-category labels, then builds a DataLoader with BBoxLabeler, PointScaler, and ToTensor for batch processing. Inputs: PIL image paths, bbox info, label vocab; output: ready-to-train data. Assumes Datasets, TfmdDL, PILImage, TensorBBox, MultiCategorize, and respective functions/labels are defined.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\ndef _coco_bb(x):  return TensorBBox.create(bbox[0])\ndef _coco_lbl(x): return bbox[1]\n\ncoco_tds = Datasets([coco_fn], [PILImage.create, [_coco_bb], [_coco_lbl, MultiCategorize(add_na=True)]], n_inp=1)\ncoco_tdl = TfmdDL(coco_tds, bs=1, after_item=[BBoxLabeler(), PointScaler(), ToTensor()])\n```\n\n----------------------------------------\n\nTITLE: Creating TextDataLoaders from Folder - Python\nDESCRIPTION: This example demonstrates the straightforward usage of the `TextDataLoaders.from_folder` factory method. It downloads the full IMDB dataset and creates DataLoaders directly from the folder structure, using the default configuration for text classification (`is_lm=False`). It concludes by displaying a sample batch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\n#|slow\npath = untar_data(URLs.IMDB)\ndls = TextDataLoaders.from_folder(path)\ndls.show_batch(max_n=3)\n```\n\n----------------------------------------\n\nTITLE: Default Weight and Bias Initialization\nDESCRIPTION: This function, `init_default`, initializes the weights of a module `m` using the provided function `func` (defaults to Kaiming normal initialization) and sets the bias to 0. It uses `torch.no_grad()` to prevent the bias initialization from being tracked by autograd.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef init_default(m, func=nn.init.kaiming_normal_):\n    \"Initialize `m` weights with `func` and set `bias` to 0.\"\n    if func and hasattr(m, 'weight'): func(m.weight)\n    with torch.no_grad(): nested_callable(m, 'bias.fill_')(0.)\n    return m\n```\n\n----------------------------------------\n\nTITLE: Training with Last Two Layers Unfrozen (Python)\nDESCRIPTION: Unfreezes the last two parameter groups of the model using `freeze_to(-2)`. Then, trains the model for one epoch using `fit_one_cycle` with discriminative learning rates specified by `slice(1e-2/(2.6**4),1e-2)`. This allows fine-tuning the upper layers while keeping the lower layers frozen.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n```\n\n----------------------------------------\n\nTITLE: Add Classification/Regression Head to Model Body - Python\nDESCRIPTION: Creates and attaches a classification or regression head to a given model body. The head is either provided directly (`custom_head`) or created using `create_head` with specified parameters like number of features (`nf`), output size (`n_out`), pooling options (`concat_pool`, `pool`), linear layers (`lin_ftrs`), dropout (`ps`), batch normalization (`first_bn`, `bn_final`, `lin_first`), and output range (`y_range`). The function returns a `nn.Sequential` model combining the body and the head and applies specified initialization (`init`) to the head.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef add_head(body, nf, n_out, init=nn.init.kaiming_normal_, head=None, concat_pool=True, pool=True,\n                lin_ftrs=None, ps=0.5, first_bn=True, bn_final=False, lin_first=False, y_range=None):\n    \"Add a head to a vision body\"\n    if head is None:\n        head = create_head(nf, n_out, concat_pool=concat_pool, pool=pool,\n                           lin_ftrs=lin_ftrs, ps=ps, first_bn=first_bn, bn_final=bn_final, lin_first=lin_first, y_range=y_range)\n    model = nn.Sequential(body, head)\n    if init is not None: apply_init(model[1], init)\n    return model\n```\n\n----------------------------------------\n\nTITLE: Creating Datasets from File Names with Splits\nDESCRIPTION: This snippet creates a `Datasets` object from a list of image file names, applies label transformation, and specifies train/test splits. It validates vocab, transform outputs, subset access, and data visualization, emphasizing dataset creation from filenames and metadata.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_45\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n#test n_inp\ntest_fns = ['dog_0.jpg','cat_0.jpg','cat_2.jpg','cat_1.jpg','kid_1.jpg']\ntcat = _Cat()\ndsets = Datasets(test_fns, [[tcat,_lbl]], splits=[[0,1,2], [3,4]])\ntest_eq(tcat.vocab, ['cat','dog'])\ntest_eq(dsets.train, [(1,),(0,),(0,)])\ntest_eq(dsets.valid[0], (0,))\nt_stdout(lambda: show_at(dsets.train, 0), \"dog\")\n```\n\n----------------------------------------\n\nTITLE: Building Data Pipelines for Pre-Tokenized Inputs (Python)\nDESCRIPTION: Creates TfmdLists and DataLoaders from previously tokenized text data, using the redefined TransformersTokenizer for decoding only. Data split indices are reused from earlier. Outputs DataLoaders ready for model training or evaluation. Dependencies: fastai, tokenized data, defined splits.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ntls = TfmdLists(tokenized, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)\ndls = tls.dataloaders(bs=bs, seq_len=sl)\n```\n\n----------------------------------------\n\nTITLE: Subclassing HookCallback for Output Hook Verification (Python)\nDESCRIPTION: Defines the TstCallback class, a HookCallback subclass whose hook method returns the module output as-is. The after_batch callback asserts that hook outputs match predictions via test_eq. Demonstrates usage with a synth_learner training loop. Dependencies include the HookCallback definition, test_eq functionality, and a valid fastai learner setup.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\nclass TstCallback(HookCallback):\n    def hook(self, m, i, o): return o\n    def after_batch(self): test_eq(self.hooks.stored[0], self.pred)\n        \nlearn = synth_learner(n_trn=5, cbs = TstCallback())\nlearn.fit(1)\n```\n\n----------------------------------------\n\nTITLE: Installing and Importing FastAI Dependencies in Python\nDESCRIPTION: Performs a conditional upgrade of fastai if running in a Colab environment and imports necessary modules from fastai.basics for further usage. The code includes future annotations support to enable forward references in type hints.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/34_callback.rnn.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#| eval: false\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom __future__ import annotations\nfrom fastai.basics import *\n```\n\n----------------------------------------\n\nTITLE: Sorting Movie Ratings\nDESCRIPTION: This code sorts movie ratings based on the bias values.  It first defines a function `item0` to extract the first element of a tuple.  It then sorts the `movie_ratings` list and displays the first 15 movies with the worst and best biases. This allows for analyzing the influence of movie bias on the model's predictions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nitem0 = lambda o:o[0]\nsorted(movie_ratings, key=item0)[:15]\nsorted(movie_ratings, key=lambda o: o[0], reverse=True)[:15]\n```\n\n----------------------------------------\n\nTITLE: Patching PIL.Image with Enhanced Functionality (Python)\nDESCRIPTION: Adds several methods and properties to the base `PIL.Image.Image` class using fastai's `@patch` decorator. This includes a custom `__repr__`, properties for `size` (returning a `fastuple`), `n_px` (total pixels), `shape` (height, width), `aspect` (width/height), and methods for `reshape` (alias for `resize`), `to_bytes_format` (convert to bytes), `to_thumb` (create a thumbnail copy), and `resize_max` (resize based on maximum pixel count, height, or width).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#|exporti\n@patch\ndef __repr__(x:Image.Image):\n    return \"<%s.%s image mode=%s size=%dx%d>\" % (x.__class__.__module__, x.__class__.__name__, x.mode, x.size[0], x.size[1])\n```\n\nLANGUAGE: python\nCODE:\n```\n#|export\nif not hasattr(Image,'_patched'):\n    _old_sz = Image.Image.size.fget\n    @patch(as_prop=True)\n    def size(x:Image.Image): return fastuple(_old_sz(x))\n    Image._patched = True\n```\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch(as_prop=True)\ndef n_px(x: Image.Image): return x.size[0] * x.size[1]\n```\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch(as_prop=True)\ndef shape(x: Image.Image): return x.size[1],x.size[0]\n```\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch(as_prop=True)\ndef aspect(x: Image.Image): return x.size[0]/x.size[1]\n```\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\ndef reshape(x: Image.Image, h, w, resample=0):\n    \"`resize` `x` to `(w,h)`\"\n    return x.resize((w,h), resample=resample)\n```\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\ndef to_bytes_format(im:Image.Image, format='png'):\n    \"Convert to bytes, default to PNG format\"\n    arr = io.BytesIO()\n    im.save(arr, format=format)\n    return arr.getvalue()\n```\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\ndef to_thumb(self:Image.Image, h, w=None):\n    \"Same as `thumbnail`, but uses a copy\"\n    if w is None: w=h\n    im = self.copy()\n    im.thumbnail((w,h))\n    return im\n```\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\ndef resize_max(x: Image.Image, resample=0, max_px=None, max_h=None, max_w=None):\n    \"`resize` `x` to `max_px`, or `max_h`, or `max_w`\"\n    h,w = x.shape\n    if max_px and x.n_px>max_px: h,w = fastuple(h,w).mul(math.sqrt(max_px/x.n_px))\n    if max_h and h>max_h: h,w = (max_h    ,max_h*w/h)\n    if max_w and w>max_w: h,w = (max_w*h/w,max_w    )\n    return x.reshape(round(h), round(w), resample=resample)\n```\n\n----------------------------------------\n\nTITLE: Registering and Managing Hooks with HookCallback in fastai (Python)\nDESCRIPTION: Implements the HookCallback class, a fastai Callback subclass for safely registering hooks on modules during training or evaluation. The callback supports configuration options such as interval registration (every), on-the-fly inclusion of parameterless modules, and safe removal on completion or error. Users are expected to subclass and provide a hook function or pass one at instantiation; this function handles (module, input, output) for forward or backward passes. Requires fastai's Callback, Hooks, and supporting utilities; assumes presence of a self.model attribute.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n@funcs_kwargs\nclass HookCallback(Callback):\n    \"`Callback` that can be used to register hooks on `modules`\"\n    _methods = [\"hook\"]\n    hook = noops\n    def __init__(self, modules=None, every=None, remove_end=True, is_forward=True, detach=True, cpu=True, include_paramless=False , **kwargs):\n        store_attr('modules,every,remove_end,is_forward,detach,cpu, include_paramless')\n        assert not kwargs\n\n    def before_fit(self):\n        \"Register the `Hooks` on `self.modules`.\"\n        if self.modules is None: self.modules = [m for m in flatten_model(self.model) if self.include_paramless or has_params(m)]\n        if self.every is None: self._register()\n\n    def before_batch(self):\n        if self.every is None: return\n        if self.training and self.train_iter%self.every==0: self._register()\n\n    def after_batch(self):\n        if self.every is None: return\n        if self.training and self.train_iter%self.every==0: self._remove()\n\n    def after_fit(self):\n        \"Remove the `Hooks`.\"\n        if self.remove_end: self._remove()\n\n    def _register(self): self.hooks = Hooks(self.modules, self.hook, self.is_forward, self.detach, self.cpu)\n    def _remove(self):\n        if getattr(self, 'hooks', None): self.hooks.remove()\n\n    def __del__(self): self._remove()\n```\n\n----------------------------------------\n\nTITLE: Defining Full Preprocessing Steps - Python\nDESCRIPTION: Specifies the complete list of fastai processors to be applied to the data: `FillMissing` (imputes missing values), `Categorify` (converts categories to numbers), and `Normalize` (scales continuous variables).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nprocs=[FillMissing, Categorify, Normalize]\n```\n\n----------------------------------------\n\nTITLE: Testing Flattened Master Parameter Creation in fastai Python\nDESCRIPTION: This test specifically checks the `get_master` function when the `flat_master` flag is set to `True`. It confirms that each parameter group in the master parameters is represented by a single flattened tensor and that the values match the original model parameters.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#|cuda\n#Flattened version\nmodel_pf,master_pf = get_master(learn.opt, flat_master=True)\ntest_eq(len(model_pf), 2)   #2 pqrqm groups\ntest_eq(len(master_pf), 2)\nfor pg1,pg2 in zip(model_pf,master_pf):\n    test_eq(len(pg2), 1) #One flattened tensor\n    test_eq([p.float().squeeze() for p in pg1], [p for p in pg2[0]]) #Same values but different types\n    for p in pg1: assert p.dtype == torch.float16\n```\n\n----------------------------------------\n\nTITLE: View Layer for Arbitrary Tensor Reshape - Python\nDESCRIPTION: Implements a Module layer for reshaping tensor x to a fixed size given at instantiation. Accepts *size arguments in the constructor, expects x as input, and outputs x reshaped via .view(*size). Relies on torch and fastai.Module imports. Used for custom reshaping within a model architecture.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass View(Module):\n    \"Reshape `x` to `size`\"\n    def __init__(self, *size): self.size = size\n    def forward(self, x): return x.view(self.size)\n```\n\n----------------------------------------\n\nTITLE: EmbeddingDotBias Model\nDESCRIPTION: Implements a dot product model with bias for collaborative filtering, featuring weight and bias access methods.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/45_collab.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass EmbeddingDotBias(Module):\n    \"Base dot model for collaborative filtering.\"\n    def __init__(self, n_factors, n_users, n_items, y_range=None):\n        self.y_range = y_range\n        (self.u_weight, self.i_weight, self.u_bias, self.i_bias) = [Embedding(*o) for o in [\n            (n_users, n_factors), (n_items, n_factors), (n_users,1), (n_items,1)\n        ]]\n\n    def forward(self, x):\n        users,items = x[:,0],x[:,1]\n        dot = self.u_weight(users)* self.i_weight(items)\n        res = dot.sum(1) + self.u_bias(users).squeeze() + self.i_bias(items).squeeze()\n        if self.y_range is None: return res\n        return torch.sigmoid(res) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]\n\n    @classmethod\n    def from_classes(cls, n_factors, classes, user=None, item=None, y_range=None):\n        \"Build a model with `n_factors` by inferring `n_users` and  `n_items` from `classes`\"\n        if user is None: user = list(classes.keys())[0]\n        if item is None: item = list(classes.keys())[1]\n        res = cls(n_factors, len(classes[user]), len(classes[item]), y_range=y_range)\n        res.classes,res.user,res.item = classes,user,item\n        return res\n\n    def _get_idx(self, arr, is_item=True):\n        \"Fetch item or user (based on `is_item`) for all in `arr`\"\n        assert hasattr(self, 'classes'), \"Build your model with `EmbeddingDotBias.from_classes` to use this functionality.\"\n        classes = self.classes[self.item] if is_item else self.classes[self.user]\n        c2i = {v:k for k,v in enumerate(classes)}\n        try: return tensor([c2i[o] for o in arr])\n        except KeyError as e:\n            message = f\"You're trying to access {'an item' if is_item else 'a user'} that isn't in the training data. If it was in your original data, it may have been split such that it's only in the validation set now.\"\n            raise modify_exception(e, message, replace=True)\n\n    def bias(self, arr, is_item=True):\n        \"Bias for item or user (based on `is_item`) for all in `arr`\"\n        idx = self._get_idx(arr, is_item)\n        layer = (self.i_bias if is_item else self.u_bias).eval().cpu()\n        return to_detach(layer(idx).squeeze(),gather=False)\n\n    def weight(self, arr, is_item=True):\n        \"Weight for item or user (based on `is_item`) for all in `arr`\"\n        idx = self._get_idx(arr, is_item)\n        layer = (self.i_weight if is_item else self.u_weight).eval().cpu()\n        return to_detach(layer(idx),gather=False)\n```\n\n----------------------------------------\n\nTITLE: Tabular Data Loader Definition - Python\nDESCRIPTION: This class `TabDataLoader` extends Fastai's `TfmdDL` to create a data loader specifically for tabular data.  It configures batch transformations and uses `ReadTabBatch` to convert data into tensors.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n@delegates()\nclass TabDataLoader(TfmdDL):\n    \"A transformed `DataLoader` for Tabular data\"\n    def __init__(self, dataset, bs=16, shuffle=False, after_batch=None, num_workers=0, **kwargs):\n        if after_batch is None: after_batch = L(TransformBlock().batch_tfms)+ReadTabBatch(dataset)\n        super().__init__(dataset, bs=bs, shuffle=shuffle, after_batch=after_batch, num_workers=num_workers, **kwargs)\n\n    def create_item(self, s):  return self.dataset.iloc[s or 0]\n    def create_batch(self, b): return self.dataset.iloc[b]\n    def do_item(self, s):      return 0 if s is None else s\n\nTabularPandas._dl_type = TabDataLoader\n```\n\n----------------------------------------\n\nTITLE: Sigmoid with Arbitrary Output Range Function - Python\nDESCRIPTION: Defines a utility function applying a sigmoid map to a tensor input, with output range mapped to (low, high). Requires torch.sigmoid; inputs are tensor x, scalar low, scalar high; output is tensor of same shape as x within specified bounds. Used for squashing activations to custom intervals.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef sigmoid_range(x, low, high):\n    \"Sigmoid function with range `(low, high)`\"\n    return torch.sigmoid(x) * (high - low) + low\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Optional Packages for FastAI Vision Learners in Python\nDESCRIPTION: This snippet imports core FastAI vision and basic modules as well as torchvision. It attempts to import the 'timm' library optionally, handling its absence gracefully. This setup is necessary for access to pretrained model architectures, vision transformation utilities, and external repositories used in model construction.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom packaging.version import parse\n\nfrom fastai.basics import *\nfrom fastai.vision.core import *\nfrom fastai.vision.data import *\nfrom fastai.vision.augment import *\nfrom fastai.vision import models\n\nimport torchvision\ntry: import timm\nexcept ModuleNotFoundError: pass\n```\n\n----------------------------------------\n\nTITLE: Documenting TfmdLists Methods\nDESCRIPTION: These calls utilize `show_doc` to generate documentation for TfmdLists methods: subset, infer_idx, and infer. They produce help or API docs for these functions within the FastAI library context.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_38\n\nLANGUAGE: Python\nCODE:\n```\nshow_doc(TfmdLists.subset)\nshow_doc(TfmdLists.infer_idx)\nshow_doc(TfmdLists.infer)\n```\n\n----------------------------------------\n\nTITLE: Creating a Language Model Learner in FastAI\nDESCRIPTION: Provides a factory function `language_model_learner` to simplify the creation of an `LMLearner` instance for language modeling. It automatically sets up the specified model architecture (`arch`), configures it with vocabulary size from `dls`, handles dropout (`drop_mult`), sets the appropriate loss function (`CrossEntropyLossFlat`), and optionally loads pretrained weights (either default FastAI weights or custom ones via `pretrained_fnames`). Additional arguments are passed to the base `Learner` constructor.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@delegates(Learner.__init__)\ndef language_model_learner(dls, arch, config=None, drop_mult=1., backwards=False, pretrained=True, pretrained_fnames=None, **kwargs):\n    \"Create a `Learner` with a language model from `dls` and `arch`.\"\n    vocab = _get_text_vocab(dls)\n    model = get_language_model(arch, len(vocab), config=config, drop_mult=drop_mult)\n    meta = _model_meta[arch]\n    learn = LMLearner(dls, model, loss_func=CrossEntropyLossFlat(), splitter=meta['split_lm'], **kwargs)\n    url = 'url_bwd' if backwards else 'url'\n    if pretrained or pretrained_fnames:\n        if pretrained_fnames is not None:\n            fnames = [learn.path/learn.model_dir/f'{fn}.{ext}' for fn,ext in zip(pretrained_fnames, ['pth', 'pkl'])]\n        else:\n            if url not in meta:\n                warn(\"There are no pretrained weights for that architecture yet!\")\n                return learn\n            model_path = untar_data(meta[url] , c_key='model')\n            try: fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n            except IndexError: print(f'The model in {model_path} is incomplete, download again'); raise\n        learn = learn.load_pretrained(*fnames)\n    return learn\n```\n\n----------------------------------------\n\nTITLE: Custom PyTorch Module Class Supporting Automatic Super Init Call in Python\nDESCRIPTION: Defines a Module class extending torch.nn.Module with PrePostInitMeta metaclass that ensures subclasses don't need to explicitly call super().__init__(). The __pre_init__ hook calls the parent constructor automatically, simplifying custom module definitions for users.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_72\n\nLANGUAGE: python\nCODE:\n```\nclass Module(nn.Module, metaclass=PrePostInitMeta):\n    \"Same as `nn.Module`, but no need for subclasses to call `super().__init__`\"\n    def __pre_init__(self, *args, **kwargs): super().__init__()\n    def __init__(self): pass\n```\n\n----------------------------------------\n\nTITLE: Testing `to_concat` Functionality in fastai\nDESCRIPTION: Contains various unit tests using `test_eq` and `test_eq_type` to validate the `to_concat` function. These tests cover concatenation of simple tensors along different dimensions, recursive concatenation within nested lists and tuples, handling dictionaries of tensors, and verifying the fallback mechanism for non-concatenable inputs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ntest_eq(to_concat([tensor([1,2]), tensor([3,4])]), tensor([1,2,3,4]))\ntest_eq(to_concat([tensor([[1,2]]), tensor([[3,4]])], dim=1), tensor([[1,2,3,4]]))\ntest_eq_type(to_concat([(tensor([1,2]), tensor([3,4])), (tensor([3,4]), tensor([5,6]))]), (tensor([1,2,3,4]), tensor([3,4,5,6])))\ntest_eq_type(to_concat([[tensor([1,2]), tensor([3,4])], [tensor([3,4]), tensor([5,6])]]), [tensor([1,2,3,4]), tensor([3,4,5,6])])\ntest_eq_type(to_concat([(tensor([1,2]),), (tensor([3,4]),)]), (tensor([1,2,3,4]),))\n\ntest_eq(to_concat([tensor([[1,2]]), tensor([[3,4], [5,6]])], dim=1), [tensor([1]),tensor([3, 5]),tensor([4, 6])])\n```\n\nLANGUAGE: python\nCODE:\n```\ntest_eq(type(to_concat([dict(foo=tensor([1,2]), bar=tensor(3,4))])), dict)\n```\n\n----------------------------------------\n\nTITLE: Defining TransformBlock class for FastAI data block API in Python\nDESCRIPTION: This class serves as a wrapper to bundle default transforms used in FastAI's data block API. It manages type transforms (`type_tfms`), item-level transforms (`item_tfms`), and batch-level transforms (`batch_tfms`) along with a specific data loader class (`dl_type`) and additional keyword arguments (`dls_kwargs`). It initializes with defaults including conversion to tensors at the item transform level. This abstraction aids in assembling complex data processing pipelines cleanly.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass TransformBlock():\n    \"A basic wrapper that links defaults transforms for the data block API\"\n    def __init__(self, \n        type_tfms:list=None, # One or more `Transform`s\n        item_tfms:list=None, # `ItemTransform`s, applied on an item\n        batch_tfms:list=None, # `Transform`s or `RandTransform`s, applied by batch\n        dl_type:TfmdDL=None, # Task specific `TfmdDL`, defaults to `TfmdDL`\n        dls_kwargs:dict=None, # Additional arguments to be passed to `DataLoaders`\n    ):\n        self.type_tfms  =            L(type_tfms)\n        self.item_tfms  = ToTensor + L(item_tfms)\n        self.batch_tfms =            L(batch_tfms)\n        self.dl_type,self.dls_kwargs = dl_type,({} if dls_kwargs is None else dls_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Training image classification model with ResNet34 using notebook_launcher\nDESCRIPTION: Defines a function to train an image classifier on the Oxford-IIIT Pet dataset using ResNet34, then launches it across 2 GPUs using notebook_launcher.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/distributed_app_examples.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.PETS)/'images'\n\ndef train():\n    dls = ImageDataLoaders.from_name_func(\n        path, get_image_files(path), valid_pct=0.2,\n        label_func=lambda x: x[0].isupper(), item_tfms=Resize(224))\n    learn = vision_learner(dls, resnet34, metrics=error_rate).to_fp16()\n    with learn.distrib_ctx(in_notebook=True, sync_bn=False):\n        learn.fine_tune(1)\n\nnotebook_launcher(train, num_processes=2)\n```\n\n----------------------------------------\n\nTITLE: Example: Using Decorated NumPy F1 Score Function as a PyTorch Metric in Python\nDESCRIPTION: Illustrates usage of the np_func decorator to adapt the sklearn.metrics f1_score function so it works seamlessly with PyTorch tensor inputs and outputs. This example confirms equivalence between the tensor version and the original numpy-based metric, ensuring integration with fastai metrics.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_71\n\nLANGUAGE: python\nCODE:\n```\n@np_func\ndef f1(inp,targ): return f1_score(targ, inp)\n\na1,a2 = array([0,1,1]),array([1,0,1])\nt = f1(tensor(a1),tensor(a2))\ntest_eq(f1_score(a1,a2), t)\nassert isinstance(t,Tensor)\n```\n\n----------------------------------------\n\nTITLE: TabularPandas Pipeline Test - Python\nDESCRIPTION: This code tests a pipeline using `TabularPandas` with `Normalize`, `Categorify`, `FillMissing`, and `noop` processes. It checks if the categorical and continuous variables are transformed correctly, missing values are filled, and the classes/vocabularies are created as expected.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nprocs = [Normalize, Categorify, FillMissing, noop]\ndf = pd.DataFrame({'a':[0,1,2,1,1,2,0], 'b':[0,1,np.nan,1,2,3,4]})\nto = TabularPandas(df, procs, cat_names='a', cont_names='b')\n\n#Test setup and apply on df_main\ntest_series(to.cat_names, ['a', 'b_na'])\ntest_series(to['a'], [1,2,3,2,2,3,1])\ntest_series(to['b_na'], [1,1,2,1,1,1,1])\nx = np.array([0,1,1.5,1,2,3,4])\nm,s = x.mean(),x.std()\ntest_close(to['b'].values, (x-m)/s)\ntest_eq(to.classes, {'a': ['#na#',0,1,2], 'b_na': ['#na#',False,True]})\n```\n\n----------------------------------------\n\nTITLE: Initializing TabularPandas for Preprocessing\nDESCRIPTION: Creates a TabularPandas object, which processes the raw DataFrame according to the specified preprocessing steps (Categorify, FillMissing, Normalize). It defines categorical, continuous, and dependent variables and applies the previously defined random split.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nto = TabularPandas(df, procs=[Categorify, FillMissing,Normalize],\n                   cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race'],\n                   cont_names = ['age', 'fnlwgt', 'education-num'],\n                   y_names='salary',\n                   splits=splits)\n```\n\n----------------------------------------\n\nTITLE: Redefine range for target variable\nDESCRIPTION: Defines the range for the target variable (ratings) using `y_range`. This is used to clip the predicted ratings to a reasonable range. It's the same range as the first definition, but included here for completeness of the second training example.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ny_range = [0,5.5]\n```\n\n----------------------------------------\n\nTITLE: Creating Body of Pretrained Vision Model with Custom Input Channels and Cut Layer Using FastAI in Python\nDESCRIPTION: The 'create_body' function returns the body of a pretrained CNN model, optionally modifying the first layer to accept different input channels using '_update_first_layer'. It identifies the cut point automatically by default at the last pooling layer or uses a user-provided cut integer/function. This enables construction of custom feature extractors from standard architectures.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef create_body(model, n_in=3, pretrained=True, cut=None):\n    \"Cut off the body of a typically pretrained `arch` as determined by `cut`\"\n    _update_first_layer(model, n_in, pretrained)\n    if cut is None:\n        ll = list(enumerate(model.children()))\n        cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n    return cut_model(model, cut)\n```\n\n----------------------------------------\n\nTITLE: Defining Utility Methods for Fastai Learner Class - Python\nDESCRIPTION: This code block defines several utility methods and properties for the `Learner` class. These methods provide functionality like making predictions on a single item (`predict`), displaying sample results (`show_results`), managing context for training loop behavior (`no_logging`, `no_mbar`, `loss_not_reduced`), handling object state for pickling (`__getstate__`, `__setstate__`), and utility functions for detaching tensors (`to_detach`) and accessing data batch components (`add_props`).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n            if res[pred_i] is not None:\n                res[pred_i] = act(res[pred_i])\n                if with_decoded: res.insert(pred_i+2, getcallable(self.loss_func, 'decodes')(res[pred_i]))\n            if reorder and hasattr(dl, 'get_idxs'): res = nested_reorder(res, tensor(idxs).argsort())\n            return tuple(res)\n        self._end_cleanup()\n\n    def predict(self, item, rm_type_tfms=None, with_input=False):\n        dl = self.dls.test_dl([item], rm_type_tfms=rm_type_tfms, num_workers=0)\n        inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True)\n        i = getattr(self.dls, 'n_inp', -1)\n        inp = (inp,) if i==1 else tuplify(inp)\n        dec = self.dls.decode_batch(inp + tuplify(dec_preds))[0]\n        dec_inp,dec_targ = map(detuplify, [dec[:i],dec[i:]])\n        res = dec_targ,dec_preds[0],preds[0]\n        if with_input: res = (dec_inp,) + res\n        return res\n\n    def show_results(self, ds_idx=1, dl=None, max_n=9, shuffle=True, **kwargs):\n        if dl is None: dl = self.dls[ds_idx].new(shuffle=shuffle)\n        b = dl.one_batch()\n        _,_,preds = self.get_preds(dl=[b], with_decoded=True)\n        dl.show_results(b, preds, max_n=max_n, **kwargs)\n\n    def show_training_loop(self):\n        indent = 0\n        for s in _loop:\n            if s.startswith('Start'): print(f'{\" \"*indent}{s}'); indent += 2\n            elif s.startswith('End'): indent -= 2; print(f'{\" \"*indent}{s}')\n            else: print(f'{\" \"*indent} - {s:15}:', self.ordered_cbs(s))\n\n    @contextmanager\n    def no_logging(self): return replacing_yield(self, 'logger', noop)\n    @contextmanager\n    def no_mbar(self):    return replacing_yield(self, 'create_mbar', False)\n\n    @contextmanager\n    def loss_not_reduced(self):\n        if hasattr(self.loss_func, 'reduction'): return replacing_yield(self.loss_func, 'reduction', 'none')\n        else: return replacing_yield(self, 'loss_func', partial(self.loss_func, reduction='none'))\n    \n    def to_detach(self,b,cpu=True,gather=True):\n        return self.dl.to_detach(b,cpu,gather) if hasattr(getattr(self,'dl',None),'to_detach') else to_detach(b,cpu,gather)\n    \n    def __getstate__(self): return {k:v for k,v in self.__dict__.items() if k!='lock'}\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        self.lock = threading.Lock()\n\nLearner.x,Learner.y = add_props(lambda i,x: detuplify((x.xb,x.yb)[i]))\n```\n\n----------------------------------------\n\nTITLE: Managing Training Lifecycle for LR Finder - FastAI Python\nDESCRIPTION: Implements the lifecycle hooks of the LRFinder callback during training. before_fit initializes temporary directories for checkpointing and saves the current model state. before_batch updates the learning rate scheduler based on the current iteration. after_batch tracks the smoothed loss value, stops training early if loss diverges or iteration limit is reached, and after_fit reloads the original model state to reset training status. before_validate raises an exception to skip validation during learning rate finding since validation is unnecessary and time-consuming. These methods ensure controlled training loop behavior for learning rate range tests.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n    def before_fit(self):\n        super().before_fit()\n        path = self.path/self.model_dir\n        path.mkdir(parents=True, exist_ok=True)\n        self.tmp_d = tempfile.TemporaryDirectory(dir=path)\n        self.tmp_p = Path(self.tmp_d.name).stem\n        self.learn.save(f'{self.tmp_p}/_tmp')\n        self.best_loss = float('inf')\n\n    def before_batch(self): self._update_val(self.train_iter/self.num_it)\n\n    def after_batch(self):\n        super().after_batch()\n        if self.smooth_loss < self.best_loss: self.best_loss = self.smooth_loss\n        if self.smooth_loss > 4*self.best_loss and self.stop_div: raise CancelFitException()\n        if self.train_iter >= self.num_it: raise CancelFitException()\n\n    def before_validate(self): raise CancelValidException()\n\n    def after_fit(self):\n        self.learn.opt.zero_grad() # Needed before detaching the optimizer for future fits\n        tmp_f = self.path/self.model_dir/self.tmp_p/'_tmp.pth'\n        if tmp_f.exists():\n            self.learn.load(f'{self.tmp_p}/_tmp', with_opt=True)\n            self.tmp_d.cleanup()\n```\n\n----------------------------------------\n\nTITLE: Defining the _Annealer Class for Hyperparameter Interpolation in Python\nDESCRIPTION: Defines the _Annealer class that wraps a function f(start, end, pos) to produce a callable object that stores start and end parameters and returns the interpolated value based on pos. This class serves as the core abstraction for all scheduling functions to take start and end bounds and produce varying values according to a position parameter.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass _Annealer:\n    def __init__(self, f, start, end): store_attr('f,start,end')\n    def __call__(self, pos): return self.f(self.start, self.end, pos)\n```\n\n----------------------------------------\n\nTITLE: Defining MultiCategoryBlock for multi-label categorical targets in FastAI Python\nDESCRIPTION: This function provides a `TransformBlock` for multi-label classification tasks. Depending on whether the input is already one-hot encoded (`encoded` flag), it creates the appropriate transformation pipeline, either using `EncodedMultiCategorize` or a combination of `MultiCategorize` and `OneHotEncode`. It supports optional vocabulary and missing category addition, enabling flexible multi-label target processing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef MultiCategoryBlock(\n    encoded:bool=False, # Whether the data comes in one-hot encoded\n    vocab:MutableSequence|pd.Series=None, # List of unique class names \n    add_na:bool=False, # Add `#na#` to `vocab`\n):\n    \"`TransformBlock` for multi-label categorical targets\"\n    tfm = EncodedMultiCategorize(vocab=vocab) if encoded else [MultiCategorize(vocab=vocab, add_na=add_na), OneHotEncode]\n    return TransformBlock(type_tfms=tfm)\n```\n\n----------------------------------------\n\nTITLE: Importing FastAI Vision Module and Setting Seed - Python\nDESCRIPTION: Imports all functions and classes from fastai.vision.all, enabling image processing and vision model training utilities. Sets a reproducible random seed with set_seed for consistent experiment results. This snippet prepares the environment for vision tasks within FastAI, ensuring deterministic behavior across runs on CUDA-enabled devices.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n#|cuda\nfrom fastai.vision.all import *\n```\n\n----------------------------------------\n\nTITLE: Define ProgressCallback Class in Python\nDESCRIPTION: Defines the `ProgressCallback` class, a fastai `Callback` responsible for displaying master and progress bars during model training and validation loops. It hooks into various points of the training lifecycle (`before_fit`, `before_epoch`, `before_train`, etc.) to initialize, update, and close the progress bars. It also integrates with the logger to optionally write stats.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/16_callback.progress.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@docs\nclass ProgressCallback(Callback):\n    \"A `Callback` to handle the display of progress bars\"\n    order,_stateattrs = 60,('mbar','pbar')\n\n    def before_fit(self):\n        assert hasattr(self.learn, 'recorder')\n        if self.create_mbar: self.mbar = master_bar(list(range(self.n_epoch)))\n        if self.learn.logger != noop:\n            self.old_logger,self.learn.logger = self.logger,self._write_stats\n            self._write_stats(self.recorder.metric_names)\n        else: self.old_logger = noop\n\n    def before_epoch(self):\n        if getattr(self, 'mbar', False): self.mbar.update(self.epoch)\n\n    def before_train(self):    self._launch_pbar()\n    def before_validate(self): self._launch_pbar()\n    def after_train(self):     self.pbar.on_iter_end()\n    def after_validate(self):  self.pbar.on_iter_end()\n    def after_batch(self):\n        self.pbar.update(self.iter+1)\n        if hasattr(self, 'smooth_loss'): self.pbar.comment = f'{self.smooth_loss.item():.4f}'\n\n    def _launch_pbar(self):\n        self.pbar = progress_bar(self.dl, parent=getattr(self, 'mbar', None), leave=False)\n        self.pbar.update(0)\n\n    def after_fit(self):\n        if getattr(self, 'mbar', False):\n            self.mbar.on_iter_end()\n            delattr(self, 'mbar')\n        if hasattr(self, 'old_logger'): self.learn.logger = self.old_logger\n\n    def _write_stats(self, log):\n        if getattr(self, 'mbar', False): self.mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in log], table=True)\n\n    _docs = dict(before_fit=\"Setup the master bar over the epochs\",\n                 before_epoch=\"Update the master bar\",\n                 before_train=\"Launch a progress bar over the training dataloader\",\n                 before_validate=\"Launch a progress bar over the validation dataloader\",\n                 after_train=\"Close the progress bar over the training dataloader\",\n                 after_validate=\"Close the progress bar over the validation dataloader\",\n                 after_batch=\"Update the current progress bar\",\n                 after_fit=\"Close the master bar\")\n\nif not hasattr(defaults, 'callbacks'): defaults.callbacks = [TrainEvalCallback, Recorder, ProgressCallback]\nelif ProgressCallback not in defaults.callbacks: defaults.callbacks.append(ProgressCallback)\n```\n\n----------------------------------------\n\nTITLE: Implementing SimpleSelfAttention Layer\nDESCRIPTION: Defines a `SimpleSelfAttention` layer. It includes a convolutional layer with spectral normalization. If `sym` is true, the weights of the convolutional layer are made symmetric by averaging with their transpose. The forward pass reshapes the input, applies the convolutional layer, calculates a matrix of dot products, and uses a trainable gamma for the residual connection.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_49\n\nLANGUAGE: Python\nCODE:\n```\nclass SimpleSelfAttention(Module):\n    def __init__(self, n_in:int, ks=1, sym=False):\n        self.sym,self.n_in = sym,n_in\n        self.conv = _conv1d_spect(n_in, n_in, ks, padding=ks//2, bias=False)\n        self.gamma = nn.Parameter(tensor([0.]))\n\n    def forward(self,x):\n        if self.sym:\n            c = self.conv.weight.view(self.n_in,self.n_in)\n            c = (c + c.t())/2\n            self.conv.weight = c.view(self.n_in,self.n_in,1)\n\n        size = x.size()\n        x = x.view(*size[:2],-1)\n\n        convx = self.conv(x)\n        xxT = torch.bmm(x,x.permute(0,2,1).contiguous())\n        o = torch.bmm(xxT, convx)\n        o = self.gamma * o + x\n        return o.view(*size).contiguous()\n```\n\n----------------------------------------\n\nTITLE: Train Language Model\nDESCRIPTION: This code trains the language model using the `fit_one_cycle` method with a specified number of epochs, maximum learning rate, and momentum values. `div` controls the initial learning rate.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/35_tutorial.wikitext.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(1, 5e-3, moms=(0.8,0.7,0.8), div=10)\n```\n\n----------------------------------------\n\nTITLE: Defining Labeled Bounding Box TransformBlock for fastai Vision in Python\nDESCRIPTION: Defines `BBoxLblBlock`, a TransformBlock factory for labeled bounding boxes including class vocabularies with optional addition of a background category (`NaN`). The block applies `MultiCategorize` to convert bounding box labels into multi-category tensors and uses `BBoxLabeler` as an item transform to process labeled bounding box data. When `add_na` is true, a category representing no object (background) is included, which is useful for object detection tasks with background classes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/08_vision.data.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef BBoxLblBlock(\n    vocab:list=None, # Vocab labels for bounding boxes\n    add_na:bool=True # Add NaN as a background class\n):\n    \"A `TransformBlock` for labeled bounding boxes, potentially with `vocab`\"\n    return TransformBlock(type_tfms=MultiCategorize(vocab=vocab, add_na=add_na), item_tfms=BBoxLabeler)\n```\n\n----------------------------------------\n\nTITLE: Define ShowGraphCallback Class in Python\nDESCRIPTION: Defines the `ShowGraphCallback` class, a fastai `Callback` that plots the training and validation losses on a graph updated after each epoch. It relies on the `ProgressCallback`'s master bar (`mbar`) to display the graph. It records the number of batches per epoch to correctly plot losses against iteration counts.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/16_callback.progress.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass ShowGraphCallback(Callback):\n    \"Update a graph of training and validation loss\"\n    order,run_valid=65,False\n\n    def before_fit(self):\n        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\")\n        if not(self.run): return\n        self.nb_batches = []\n        assert hasattr(self.learn, 'progress')\n\n    def after_train(self): self.nb_batches.append(self.train_iter)\n\n    def after_epoch(self):\n        \"Plot validation loss in the pbar graph\"\n        if not self.nb_batches: return\n        rec = self.learn.recorder\n        iters = range_of(rec.losses)\n        val_losses = [v[1] for v in rec.values]\n        x_bounds = (0, (self.n_epoch - len(self.nb_batches)) * self.nb_batches[0] + len(rec.losses))\n        y_bounds = (0, max((max(Tensor(rec.losses)), max(Tensor(val_losses)))))\n        self.progress.mbar.update_graph([(iters, rec.losses), (self.nb_batches, val_losses)], x_bounds, y_bounds)\n```\n\n----------------------------------------\n\nTITLE: Creating Splits for Training and Validation Data - Python\nDESCRIPTION: This code creates the splits for the data into training and validation sets. `IndexSplitter` creates a split based on the provided indices, dividing the DataFrame into training and validation sets. The indices are created from a range of rows in the DataFrame. This is crucial for evaluating the model's performance on unseen data.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-tabular.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsplits = IndexSplitter(list(range(800,1000)))(range_of(df))\n```\n\n----------------------------------------\n\nTITLE: Updating First Convolution Layer in Pretrained PyTorch Model to Match Input Channels Using FastAI in Python\nDESCRIPTION: The '_update_first_layer' function replaces the model's initial Conv2d layer if the input channel count requested differs from 3. It constructs a new Conv2d with appropriate parameters, loads pretrained weights using '_load_pretrained_weights' if applicable, then substitutes the old layer in the model. It verifies the layer type and expected default input channels to maintain integrity during transfer learning.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef _update_first_layer(model, n_in, pretrained):\n    \"Change first layer based on number of input channels\"\n    if n_in == 3: return\n    first_layer, parent, name = _get_first_layer(model)\n    assert isinstance(first_layer, nn.Conv2d), f'Change of input channels only supported with Conv2d, found {first_layer.__class__.__name__}'\n    assert getattr(first_layer, 'in_channels') == 3, f'Unexpected number of input channels, found {getattr(first_layer, \"in_channels\")} while expecting 3'\n    params = {attr:getattr(first_layer, attr) for attr in 'out_channels kernel_size stride padding dilation groups padding_mode'.split()}\n    params['bias'] = getattr(first_layer, 'bias') is not None\n    params['in_channels'] = n_in\n    new_layer = nn.Conv2d(**params)\n    if pretrained:\n        _load_pretrained_weights(new_layer, first_layer)\n    setattr(parent, name, new_layer)\n```\n\n----------------------------------------\n\nTITLE: Defining TfmdLists Class for Applying Transformations in fastai (Python)\nDESCRIPTION: Defines the `TfmdLists` class, inheriting from `FilteredBase` and `L`. It applies a `Pipeline` of transformations (`tfms`) to a collection of `items`. It handles data splitting (`splits`), transform setup (`setup`), item access and transformation (`__getitem__`, `_after_item`), decoding (`decode`), showing (`show`), type inference (`infer_idx`, `infer`), and subset creation (`subset`).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass TfmdLists(FilteredBase, L, GetAttr):\n    \"A `Pipeline` of `tfms` applied to a collection of `items`\"\n    _default='tfms'\n    def __init__(self, \n        items:list, # Items to apply `Transform`s to\n        tfms:MutableSequence|Pipeline, # `Transform`(s) or `Pipeline` to apply\n        use_list:bool=None, # Use `list` in `L`\n        do_setup:bool=True, # Call `setup()` for `Transform`\n        split_idx:int=None, # Apply `Transform`(s) to training or validation set. `0` for training set and `1` for validation set\n        train_setup:bool=True, # Apply `Transform`(s) only on training `DataLoader`\n        splits:list=None, # Indices for training and validation sets\n        types=None, # Types of data in `items`\n        verbose:bool=False, # Print verbose output\n        dl_type:TfmdDL=None # Type of `DataLoader`\n    ):\n        super().__init__(items, use_list=use_list)\n        if dl_type is not None: self._dl_type = dl_type\n        self.splits = L([slice(None),[]] if splits is None else splits).map(mask2idxs)\n        if isinstance(tfms,TfmdLists): tfms = tfms.tfms\n        if isinstance(tfms,Pipeline): do_setup=False\n        self.tfms = Pipeline(tfms, split_idx=split_idx)\n        store_attr('types,split_idx')\n        if do_setup:\n            pv(f\"Setting up {self.tfms}\", verbose)\n            self.setup(train_setup=train_setup)\n\n    def _new(self, items, split_idx=None, **kwargs):\n        split_idx = ifnone(split_idx,self.split_idx)\n        try: return super()._new(items, tfms=self.tfms, do_setup=False, types=self.types, split_idx=split_idx, **kwargs)\n        except IndexError as e:\n            e.args = [f\"Tried to grab subset {i} in the Dataset, but it contained no items.\\n\\t{e.args[0]}\"]\n            raise\n    def subset(self, i): return self._new(self._get(self.splits[i]), split_idx=i)\n    def _after_item(self, o): return self.tfms(o)\n    def __repr__(self): return f\"{self.__class__.__name__}: {self.items}\\ntfms - {self.tfms.fs}\"\n    def __iter__(self): return (self[i] for i in range(len(self)))\n    def show(self, o, **kwargs): return self.tfms.show(o, **kwargs)\n    def decode(self, o, **kwargs): return self.tfms.decode(o, **kwargs)\n    def __call__(self, o, **kwargs): return self.tfms.__call__(o, **kwargs)\n    def overlapping_splits(self): return L(Counter(self.splits.concat()).values()).filter(gt(1))\n    def new_empty(self): return self._new([])\n\n    def setup(self, \n        train_setup:bool=True # Apply `Transform`(s) only on training `DataLoader`\n    ):\n        self.tfms.setup(self, train_setup)\n        if len(self) != 0:\n            x = super().__getitem__(0) if self.splits is None else super().__getitem__(self.splits[0])[0]\n            self.types = []\n            for f in self.tfms.fs:\n                self.types.append(getattr(f, 'input_types', type(x)))\n                x = f(x)\n            self.types.append(type(x))\n        types = L(t if is_listy(t) else [t] for t in self.types).concat().unique()\n        self.pretty_types = '\\n'.join([f'  - {t}' for t in types])\n\n    def infer_idx(self, x):\n        # TODO: check if we really need this, or can simplify\n        idx = 0\n        for t in self.types:\n            if isinstance(x, t): break\n            idx += 1\n        types = L(t if is_listy(t) else [t] for t in self.types).concat().unique()\n        pretty_types = '\\n'.join([f'  - {t}' for t in types])\n        assert idx < len(self.types), f\"Expected an input of type in \\n{pretty_types}\\n but got {type(x)}\"\n        return idx\n\n    def infer(self, x):\n        return compose_tfms(x, tfms=self.tfms.fs[self.infer_idx(x):], split_idx=self.split_idx)\n\n    def __getitem__(self, idx):\n        res = super().__getitem__(idx)\n        if self._after_item is None: return res\n        return self._after_item(res) if is_indexer(idx) else res.map(self._after_item)\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning the Classifier\nDESCRIPTION: This performs fine-tuning of the pretrained model. It calls `fine_tune` twice. Each call fine-tunes the classifier using a specified number of epochs and a learning rate. This trains the model on the specific classification task.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlearn.fine_tune(4, 1e-2)\n```\n\nLANGUAGE: python\nCODE:\n```\nlearn.fine_tune(4, 1e-2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Pad_Input Transform for Text Classification\nDESCRIPTION: A transform class that handles padding for text inputs with different lengths. It supports padding at the beginning or end, backwards processing, and type preservation. The transform can be applied to specific fields within samples.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass Pad_Input(ItemTransform):\n    def encodes(self,samples, pad_idx=1, pad_fields=0, pad_first=False, backwards=False):\n        \"Function that collect `samples` and adds padding\"\n        self.pad_idx = pad_idx\n        pad_fields = L(pad_fields)\n        max_len_l = pad_fields.map(lambda f: max([len(s[f]) for s in samples]))\n        if backwards: pad_first = not pad_first\n        def _f(field_idx, x):\n            if field_idx not in pad_fields: return x\n            idx = pad_fields.items.index(field_idx) #TODO: remove items if L.index is fixed\n            sl = slice(-len(x), sys.maxsize) if pad_first else slice(0, len(x))\n            pad =  x.new_zeros(max_len_l[idx]-x.shape[0])+pad_idx\n            x1 = torch.cat([pad, x] if pad_first else [x, pad])\n            if backwards: x1 = x1.flip(0)\n            return retain_type(x1, x)\n        return [tuple(map(lambda idxx: _f(*idxx), enumerate(s))) for s in samples]\n    def decodes(self, o:TensorText):\n        pad_idx = self.pad_idx if hasattr(self,'pad_idx') else 1\n        return o[o != pad_idx]\npad_input=Pad_Input()\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders for Classification with Input Padding (Python)\nDESCRIPTION: Initializes DataLoaders for classification with specified batch size and a pre-batch padding function. Input: dsets object and pad_input_chunk as before_batch callback; Output: dls, a DataLoaders object ready for classification model training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndls = dsets.dataloaders(before_batch=pad_input_chunk, bs=bs)\n```\n\n----------------------------------------\n\nTITLE: Loading ImageDataLoaders from DataFrame using fastai in Python\nDESCRIPTION: Creates ImageDataLoaders from a pandas DataFrame and a path to images with fastai's ImageDataLoaders.from_df method. Supports specifying folder and validation column either by name or index for dataset splitting. Requires fastai library and a DataFrame structured with image file names and labels. Inputs include 'df' as the DataFrame and 'path' as the base directory; outputs an ImageDataLoaders object ready for model training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/08_vision.data.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndls = ImageDataLoaders.from_df(df, path)\n```\n\nLANGUAGE: python\nCODE:\n```\n#|eval: false\npath = untar_data(URLs.PASCAL_2007)\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n```\n\nLANGUAGE: python\nCODE:\n```\n#|eval: false\ndls = ImageDataLoaders.from_df(df, path, folder='train', valid_col='is_valid')\n```\n\n----------------------------------------\n\nTITLE: Create Language Model Learner\nDESCRIPTION: Creates a language model learner using the `language_model_learner` function from FastAI. It specifies the `DataLoaders` object, the architecture (AWD_LSTM), dropout multiplier, and metrics (accuracy, perplexity). The model is then moved to fp16 precision for faster training.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nlearn = language_model_learner(dbunch_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16()\n```\n\n----------------------------------------\n\nTITLE: Testing TerminateOnNaNCallback\nDESCRIPTION: This code creates a synthetic learner and fits it with the `TerminateOnNaNCallback`. It asserts that the training is terminated early if NaN values are encountered, and verifies that none of the recorded losses are infinite or NaN.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner()\nlearn.fit(10, lr=100, cbs=TerminateOnNaNCallback())\n\n```\n\nLANGUAGE: python\nCODE:\n```\nassert len(learn.recorder.losses) < 10 * len(learn.dls.train)\nfor l in learn.recorder.losses:\n    assert not torch.isinf(l) and not torch.isnan(l) \n```\n\n----------------------------------------\n\nTITLE: Add Normalization Transform to DataLoader (fastai)\nDESCRIPTION: This function adds a normalization transform to the `DataLoader`'s `after_batch` transforms if it doesn't already exist. It uses the statistics provided in the model metadata to create the `Normalize` transform.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef _add_norm(dls, meta, pretrained, n_in=3):\n    if not pretrained: return\n    stats = meta.get('stats')\n    if stats is None: return\n    if n_in != len(stats[0]): return\n    if not dls.after_batch.fs.filter(risinstance(Normalize)):\n        dls.add_tfms([Normalize.from_stats(*stats)],'after_batch')\n```\n\n----------------------------------------\n\nTITLE: Loading IMDB Sample CSV into DataFrame - Python\nDESCRIPTION: This snippet reads the 'texts.csv' file from the previously set IMDB sample dataset path into a pandas DataFrame. It then displays the header and the first few rows of the DataFrame using `df.head()` to show the structure of the data being loaded.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\ndf = pd.read_csv(path/\"texts.csv\"); df.head()\n```\n\n----------------------------------------\n\nTITLE: TensorBoard Projector Callback for Embedding Visualization\nDESCRIPTION: Defines a callback class for extracting and exporting image features for visualization in TensorBoard's projector during inference, setting up the necessary environment before validation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nclass TensorBoardProjectorCallback(TensorBoardBaseCallback):\n    \"Extracts and exports image featuers for tensorboard projector during inference\"\n    def __init__(self, log_dir=None, layer=None):\n        super().__init__()\n        store_attr()\n    \n    def before_fit(self):\n        self.run = not hasattr(self.learn, 'lr_finder') and hasattr(self, \"gather_preds\") and rank_distrib()==0\n        if not self.run: return\n        self._setup_writer()\n\n    def before_validate(self):\n        self._setup_projector()\n```\n\n----------------------------------------\n\nTITLE: Defining ToTensor Transform for PILBase and PILMask - fastai (Python)\nDESCRIPTION: Creates 'encodes' methods for the ToTensor transform for both generic PIL images and mask images, facilitating conversion to the expected fastai tensor types using 'image2tensor'. Requires 'ToTensor', 'PILBase', 'PILMask', and 'image2tensor' from fastai. Expects PIL image/mask objects as input and outputs corresponding tensor objects.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n@ToTensor\ndef encodes(self, o:PILBase): return o._tensor_cls(image2tensor(o))\n@ToTensor\ndef encodes(self, o:PILMask): return o._tensor_cls(image2tensor(o)[0])\n```\n\n----------------------------------------\n\nTITLE: Using RandDL with Multiple Workers in Python\nDESCRIPTION: Illustrates creating an instance of `RandDL` configured to use multiple worker processes (`num_workers=4`) for potentially faster data loading, along with batching (`bs=4`) and `drop_last=True`. The output batches are collected using `L()` and their lengths checked using `.map(len)`, demonstrating parallel batch generation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndl = RandDL(bs=4, num_workers=4, drop_last=True)\nL(dl).map(len)\n```\n\n----------------------------------------\n\nTITLE: Using TextBlock in DataBlock for Text Classification - Python\nDESCRIPTION: This snippet provides an example of integrating `TextBlock` into a fastai `DataBlock` for a text classification task. It loads the IMDB sample dataset, defines a `DataBlock` using `TextBlock.from_df` for the input and `CategoryBlock` for the target, and then creates `DataLoaders`. Finally, it displays a batch to visualize the processed data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\npath = untar_data(URLs.IMDB_SAMPLE)\ndf = pd.read_csv(path/'texts.csv')\n\nimdb_clas = DataBlock(\n    blocks=(TextBlock.from_df('text', seq_len=72), CategoryBlock),\n    get_x=ColReader('text'), get_y=ColReader('label'), splitter=ColSplitter())\n\ndls = imdb_clas.dataloaders(df, bs=64)\ndls.show_batch(max_n=2)\n```\n\n----------------------------------------\n\nTITLE: Working with TensorImage and TensorMask Casting and Display in Python\nDESCRIPTION: Demonstrates casting of PIL Image and PyTorch tensor objects to TensorImage and TensorMask subclasses, asserting correct types. Shows rendering of images via the show method and tests for figure existence. Also illustrates arithmetic operations producing expected subclasses between TensorMask and TensorImageBase objects, verifying type preservation and correct output types.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nim = Image.open(TEST_IMAGE)\nim_t = cast(array(im), TensorImage)\ntest_eq(type(im_t), TensorImage)\n```\n\nLANGUAGE: python\nCODE:\n```\nim_t2 = cast(tensor(1), TensorMask)\ntest_eq(type(im_t2), TensorMask)\ntest_eq(im_t2, tensor(1))\nax = im_t.show(figsize=(2,2))\n_ =(im_t == im_t2)\n```\n\nLANGUAGE: python\nCODE:\n```\ntest_fig_exists(ax)\n```\n\nLANGUAGE: python\nCODE:\n```\na = TensorMask([1,2])\ntest_eq_type(TensorImage(1)+a, TensorImage([2,3]))\ntest_eq_type(1-a, TensorMask([0,-1]))\n```\n\nLANGUAGE: python\nCODE:\n```\n#|hide (last test of to_concat)\ntest_eq_type(to_concat([TensorImage([1,2]), TensorImage([3,4])]), TensorImage([1,2,3,4]))\n```\n\n----------------------------------------\n\nTITLE: Exporting nbdev module\nDESCRIPTION: Exports the current nbdev module using `nbdev_export`. This is commonly used in nbdev-based projects to generate Python modules from Jupyter notebooks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting PETS Dataset (Python)\nDESCRIPTION: Uses the `fastai.data.external.untar_data` function to download and extract the PETS dataset from the specified URL. The `path` variable will store the location of the extracted data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20b_tutorial.distributed.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\npath = untar_data(URLs.PETS)\n```\n\n----------------------------------------\n\nTITLE: Example and Tests for `to_np` in fastai\nDESCRIPTION: Shows an example of using the `to_np` function to convert a tensor `t3` (already on the CPU) into a NumPy array. Includes `test_eq` assertions to confirm that the resulting object's type is `np.ndarray` and that its value is correctly preserved during the conversion.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nt3 = to_np(t3)\ntest_eq(type(t3), np.ndarray)\ntest_eq(t3, 2)\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns for Sample DataFrame - Python\nDESCRIPTION: Filters the small sample DataFrame to include only the specified categorical, continuous, and dependent variables, then resets the index.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsmall_df = small_df[small_cat_vars + small_cont_vars + ['Sales']].reset_index(drop=True)\n```\n\n----------------------------------------\n\nTITLE: Example Plotting Range of Top Losses (Python)\nDESCRIPTION: Shows how to use the `plot_top_losses` method with a Python `range` object to plot losses from a specific interval within the sorted list of top losses. This allows examining losses beyond the very top ones, such as the 7th through 16th largest losses.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ninterp.plot_top_losses(range(7,16))\n```\n\n----------------------------------------\n\nTITLE: masked_concat_pool - Example Usage\nDESCRIPTION: Demonstrates usage of the `masked_concat_pool` function by creating a dummy tensor `out` and a mask `mask`.  It calls the function and compares its output `x` to the expected values based on the pooling and concatenation logic. Checks are made for the last hidden state, max pool and average pool.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nout = torch.randn(2,4,5)\nmask = tensor([[True,True,False,False], [False,False,False,True]])\nx = masked_concat_pool(out, mask, 2)\n\ntest_close(x[0,:5], out[0,-1])\ntest_close(x[1,:5], out[1,-2])\ntest_close(x[0,5:10], out[0,2:].max(dim=0)[0])\ntest_close(x[1,5:10], out[1,:3].max(dim=0)[0])\ntest_close(x[0,10:], out[0,2:].mean(dim=0))\ntest_close(x[1,10:], out[1,:3].mean(dim=0))\n```\n\n----------------------------------------\n\nTITLE: Moving PyTorch Tensors to CPU in fastai\nDESCRIPTION: Defines the `to_cpu` function, which serves as a convenient wrapper around the `to_device` function. It specifically targets the CPU by calling `to_device(b, 'cpu')`, recursively moving all tensors within the input structure `b` to the CPU.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef to_cpu(b):\n    \"Recursively map tensors in `b ` to the cpu.\"\n    return to_device(b,'cpu')\n```\n\n----------------------------------------\n\nTITLE: Filling Missing Values in Tabular Data - Python\nDESCRIPTION: This code defines a `FillMissing` transform that fills missing values in continuous columns of a `TabularPandas` object.  It allows for specifying a filling strategy (median, constant, or mode) and adds a boolean column indicating where missing values were filled.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nclass FillMissing(TabularProc):\n    \"Fill the missing values in continuous columns.\"\n    def __init__(self, fill_strategy=FillStrategy.median, add_col=True, fill_vals=None):\n        if fill_vals is None: fill_vals = defaultdict(int)\n        store_attr()\n\n    def setups(self, to):\n        missing = pd.isnull(to.conts).any()\n        store_attr(but='to', na_dict={n:self.fill_strategy(to[n], self.fill_vals[n])\n                            for n in missing[missing].keys()})\n        self.fill_strategy = self.fill_strategy.__name__\n\n    def encodes(self, to):\n        missing = pd.isnull(to.conts)\n        for n in missing.any()[missing.any()].keys():\n            assert n in self.na_dict, f\"nan values in `{n}` but not in setup training set\"\n        for n in self.na_dict.keys():\n            to[n].fillna(self.na_dict[n], inplace=True)\n            if self.add_col:\n                to.loc[:,n+'_na'] = missing[n]\n                if n+'_na' not in to.cat_names: to.cat_names.append(n+'_na')\n```\n\n----------------------------------------\n\nTITLE: Defining a Refactored Convolutional Layer\nDESCRIPTION: This function `conv2` redefines the convolutional layer using `ConvLayer`.  It takes the number of input and output filters (`ni` and `nf`) and returns a `ConvLayer` with stride of 2. This re-architecting makes the code cleaner and more readable.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\ndef conv2(ni,nf): return ConvLayer(ni,nf,stride=2)\n```\n\n----------------------------------------\n\nTITLE: Getting a Batch from DataLoaders - fastai/Python\nDESCRIPTION: Retrieves a single batch of data (input features `x` and target labels `y`) from the training DataLoader contained within the `dls` object. It then prints the shapes of the retrieved input and target tensors.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nx,y = dls.one_batch()\nx.shape,y.shape\n```\n\n----------------------------------------\n\nTITLE: Converting Sklearn Metrics to fastai AccumMetric - Python\nDESCRIPTION: Defines skm_to_fastai, a helper function that wraps any sklearn.metrics function as a fastai AccumMetric instance, automatically choosing activations, argmax, threshold, and numpy conversion options. Parameters include the sklearn metric, is_class to distinguish classification/regression, thresh for thresholding, axis for argmax, activation override, and extra kwargs. Returns an AccumMetric instance compatible with fastai's training loop. Dependencies: sklearn, fastai.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ndef skm_to_fastai(func, is_class=True, thresh=None, axis=-1, activation=None, **kwargs):\n    \"Convert `func` from sklearn.metrics to a fastai metric\"\n    dim_argmax = axis if is_class and thresh is None else None\n    if activation is None:\n        activation = ActivationType.Sigmoid if (is_class and thresh is not None) else ActivationType.No\n    return AccumMetric(func, dim_argmax=dim_argmax, activation=activation, thresh=thresh,\n                       to_np=True, invert_arg=True, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Printing Generated Text\nDESCRIPTION: This prints the text generated by the language model in a readable format by joining the list of generated sentences. This displays the text that was created by the predict function.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\n\".join(preds))\n```\n\n----------------------------------------\n\nTITLE: Checking Shape of Tokenized Training and Validation Examples (Python)\nDESCRIPTION: Applies the transformation pipeline (tfms) to training and validation examples, returning their respective tensor shapes. Useful for verifying correct tokenization and sequence lengths.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntls.tfms(tls.train.items[0]).shape, tls.tfms(tls.valid.items[0]).shape\n```\n\n----------------------------------------\n\nTITLE: Mapping floating point tensors to FP16 recursively (Python)\nDESCRIPTION: The `to_half` function traverses nested structures and converts all floating point tensors to 16-bit floating point (FP16). It is used to reduce memory usage or accelerate training with reduced precision.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\n#|export\ndef to_half(b):\n    \"Recursively map floating point tensors in `b ` to FP16.\"\n    return apply(lambda x: x.half() if torch.is_floating_point(x) else x, b)\n```\n\n----------------------------------------\n\nTITLE: Displaying Processed Input Row (fastai tabular)\nDESCRIPTION: Displays the single input row *after* it has been processed by the model's preprocessing pipeline. This shows how the raw input from `df.iloc[0]` was transformed before being fed to the model.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nrow.show()\n```\n\n----------------------------------------\n\nTITLE: Categorify Processor for Encoding Categorical Variables in fastai (Python)\nDESCRIPTION: The Categorify class inherits from TabularProc and converts categorical columns into integer codes suitable for machine learning. During setup, it creates a mapping of unique categories per column with optional NA handling. Encoding replaces categorical values by their integer codes plus one (due to offset), while decoding restores original categorical labels. It operates eagerly on pandas DataFrames within the fastai tabular pipeline and depends on the CategoryMap utility class for managing category mappings.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nclass Categorify(TabularProc):\n    \"Transform the categorical variables to something similar to `pd.Categorical`\"\n    order = 1\n    def setups(self, to):\n        store_attr(classes={n:CategoryMap(to.iloc[:,n].items, add_na=(n in to.cat_names)) for n in to.cat_names}, but='to')\n\n    def encodes(self, to): to.transform(to.cat_names, partial(_apply_cats, self.classes, 1))\n    def decodes(self, to): to.transform(to.cat_names, partial(_decode_cats, self.classes))\n    def __getitem__(self,k): return self.classes[k]\n```\n\n----------------------------------------\n\nTITLE: Show Batch of Classifier Data\nDESCRIPTION: Displays a batch of data from the `DataLoaders` object for the text classification task. This is useful for verifying the data pipeline and the format of the batches.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndbunch_clas.show_batch()\n```\n\n----------------------------------------\n\nTITLE: Defining an Average Metric Class (Python)\nDESCRIPTION: This snippet defines `AvgMetric`, a class that averages the values of a given function, taking into account potentially different batch sizes. It inherits from the base `Metric` class and implements `reset` and `accumulate` to keep track of the total value and count, as well as value to return the average.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_61\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass AvgMetric(Metric):\n    \"Average the values of `func` taking into account potential different batch sizes\"\n    def __init__(self, func):  self.func = func\n    def reset(self):           self.total,self.count = 0.,0\n    def accumulate(self, learn):\n        bs = find_bs(learn.yb)\n        self.total += learn.to_detach(self.func(learn.pred, *learn.yb))*bs\n        self.count += bs\n    @property\n    def value(self): return self.total/self.count if self.count != 0 else None\n    @property\n    def name(self):  return self.func.func.__name__ if hasattr(self.func, 'func') else  self.func.__name__\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(AvgMetric, title_level=3)\n```\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner()\ntst = AvgMetric(lambda x,y: (x-y).abs().mean())\nt,u = torch.randn(100),torch.randn(100)\ntst.reset()\nfor i in range(0,100,25): \n    learn.pred,learn.yb = t[i:i+25],(u[i:i+25],)\n    tst.accumulate(learn)\ntest_close(tst.value, (t-u).abs().mean())\n```\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#With varying batch size\ntst.reset()\nsplits = [0, 30, 50, 60, 100]\nfor i in range(len(splits )-1): \n    learn.pred,learn.yb = t[splits[i]:splits[i+1]],(u[splits[i]:splits[i+1]],)\n    tst.accumulate(learn)\ntest_close(tst.value, (t-u).abs().mean())\n```\n\n----------------------------------------\n\nTITLE: Using fastai Tokenizer with Datasets Examples (Python)\nDESCRIPTION: Demonstrates initializing `fastai.data.core.Datasets` with the `Tokenizer` transform. It shows two common patterns: one for tokenizing text files from a folder using `Tokenizer.from_folder` and another for tokenizing a text column in a pandas DataFrame using `Tokenizer.from_df`. This integrates tokenization directly into the fastai data loading pipeline.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/30_text.core.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith tempfile.TemporaryDirectory() as tmp_d:\n    path,df,csv_fname = _prepare_texts(Path(tmp_d))\n    items = get_text_files(path)\n    splits = RandomSplitter()(items)\n    dsets = Datasets(items, [Tokenizer.from_folder(path)], splits=splits)\n    print(dsets.train[0])\n    \n    dsets = Datasets(df, [Tokenizer.from_df('text')], splits=splits)\n    print(dsets.train[0][0].text)\n```\n\n----------------------------------------\n\nTITLE: ClassificationInterpretation Class in fastai\nDESCRIPTION: This code defines the `ClassificationInterpretation` class. This class inherits from the base `Interpretation` class and provides specialized methods for interpreting the results of classification models. It calculates and displays a confusion matrix, plots the confusion matrix, identifies the most confused classes, and prints a classification report.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass ClassificationInterpretation(Interpretation):\n    \"Interpretation methods for classification models.\"\n\n    def __init__(self, \n        learn:Learner, \n        dl:DataLoader, # `DataLoader` to run inference over\n        losses:TensorBase, # Losses calculated from `dl`\n        act=None # Activation function for prediction\n    ):\n        super().__init__(learn, dl, losses, act)\n        self.vocab = self.dl.vocab\n        if is_listy(self.vocab): self.vocab = self.vocab[-1]\n\n    def confusion_matrix(self):\n        \"Confusion matrix as an `np.ndarray`.\"\n        x = torch.arange(0, len(self.vocab))\n        _,targs,decoded = self.learn.get_preds(dl=self.dl, with_decoded=True, with_preds=True, \n                                               with_targs=True, act=self.act)\n        d,t = flatten_check(decoded, targs)\n        cm = ((d==x[:,None]) & (t==x[:,None,None])).long().sum(2)\n        return to_np(cm)\n\n    def plot_confusion_matrix(self, \n        normalize:bool=False, # Whether to normalize occurrences\n        title:str='Confusion matrix', # Title of plot\n        cmap:str=\"Blues\", # Colormap from matplotlib\n        norm_dec:int=2, # Decimal places for normalized occurrences\n        plot_txt:bool=True, # Display occurrence in matrix\n        **kwargs\n    ):\n        \"Plot the confusion matrix, with `title` and using `cmap`.\"\n        # This function is mainly copied from the sklearn docs\n        cm = self.confusion_matrix()\n        if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        fig = plt.figure(**kwargs)\n        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n        plt.title(title)\n        tick_marks = np.arange(len(self.vocab))\n        plt.xticks(tick_marks, self.vocab, rotation=90)\n        plt.yticks(tick_marks, self.vocab, rotation=0)\n\n        if plot_txt:\n            thresh = cm.max() / 2.\n            for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n                coeff = f'{cm[i, j]:.{norm_dec}f}' if normalize else f'{cm[i, j]}'\n                plt.text(j, i, coeff, horizontalalignment=\"center\", verticalalignment=\"center\", color=\"white\"\n                         if cm[i, j] > thresh else \"black\")\n\n        ax = fig.gca()\n        ax.set_ylim(len(self.vocab)-.5,-.5)\n\n        plt.tight_layout()\n        plt.ylabel('Actual')\n        plt.xlabel('Predicted')\n        plt.grid(False)\n\n    def most_confused(self, min_val=1):\n        \"Sorted descending largest non-diagonal entries of confusion matrix (actual, predicted, # occurrences\"\n        cm = self.confusion_matrix()\n        np.fill_diagonal(cm, 0)\n        res = [(self.vocab[i],self.vocab[j],cm[i,j]) for i,j in zip(*np.where(cm>=min_val))]\n        return sorted(res, key=itemgetter(2), reverse=True)\n\n    def print_classification_report(self):\n        \"Print scikit-learn classification report\"\n        _,targs,decoded = self.learn.get_preds(dl=self.dl, with_decoded=True, with_preds=True, \n                                               with_targs=True, act=self.act)\n        d,t = flatten_check(decoded, targs)\n        names = [str(v) for v in self.vocab]\n        print(skm.classification_report(t, d, labels=list(self.vocab.o2i.values()), target_names=names))\n```\n\n----------------------------------------\n\nTITLE: Loading Pretrained Weights for Adapted Input Channels in PyTorch Convolution Layers Using FastAI in Python\nDESCRIPTION: The '_load_pretrained_weights' function modifies the weights of a newly created first convolutional layer with a different number of input channels to best utilize pretrained weights from the original 3-channel layer. It handles special cases for 1 or 2 input channels by summing or scaling weights, otherwise keeps the first 3 channels and zeros others. This is crucial for transfer learning when input data channels differ from the pretrained model.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef _load_pretrained_weights(new_layer, previous_layer):\n    \"Load pretrained weights based on number of input channels\"\n    n_in = getattr(new_layer, 'in_channels')\n    if n_in==1:\n        # we take the sum\n        new_layer.weight.data = previous_layer.weight.data.sum(dim=1, keepdim=True)\n    elif n_in==2:\n        # we take first 2 channels + 50%\n        new_layer.weight.data = previous_layer.weight.data[:,:2] * 1.5\n    else:\n        # keep 3 channels weights and set others to null\n        new_layer.weight.data[:,:3] = previous_layer.weight.data\n        new_layer.weight.data[:,3:].zero_()\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders\nDESCRIPTION: This code creates a `DataLoaders` object for the collaborative filtering task. It uses the `from_df` method to create a `DataLoaders` object from the `ratings` DataFrame, specifying the columns for user, item (movie title), and ratings. It also sets the batch size (bs) to 64.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\n```\n\n----------------------------------------\n\nTITLE: Labeling Test Predictions Using Threshold - Python\nDESCRIPTION: Generates multi-label predictions for each image by applying a threshold (0.2) and mapping activated indices to class names. Output is a list of space-delimited label strings. Inputs: predictions tensor and vocab list. Threshold can be tuned for competition optimization.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nthresh = 0.2\nlabelled_preds = [' '.join([learn.dls.vocab[i] for i,p in enumerate(pred) if p > thresh]) for pred in preds.numpy()]\n```\n\n----------------------------------------\n\nTITLE: Extracting Vocabulary from FastAI DataLoaders\nDESCRIPTION: Defines a helper function `_get_text_vocab` that retrieves the vocabulary from a FastAI `DataLoaders` object (`dls`). It accounts for scenarios where the vocabulary might be stored directly or nested within a FastAI `L` list (e.g., in multi-modal data).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef _get_text_vocab(dls):\n    vocab = dls.vocab\n    if isinstance(vocab, L): vocab = vocab[0]\n    return vocab\n```\n\n----------------------------------------\n\nTITLE: Equality Comparison Patch for fastai Tensor Class in Python\nDESCRIPTION: The `__array_eq__` method patch for the fastai `Tensor` class overloads equality checking to use `torch.equal` for tensors with dimension greater than zero, otherwise a direct equality check. This ensures correct element-wise equality comparisons on tensors versus scalars.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@patch\ndef __array_eq__(self:Tensor,b):\n    return torch.equal(self,b) if self.dim() else self==b\n```\n\n----------------------------------------\n\nTITLE: Creating Item Getter Transform\nDESCRIPTION: Transform class that extracts an item at a specified index position from collections like lists, tuples, or arrays.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass ItemGetter(ItemTransform):\n    \"Creates a proper transform that applies `itemgetter(i)` (even on a tuple)\"\n    _retain = False\n    def __init__(self, i): self.i = i\n    def encodes(self, x): return x[self.i]\n```\n\n----------------------------------------\n\nTITLE: Exporting Functions and Final Setup\nDESCRIPTION: Hides the import and export function calls for nbdev, a development environment, and executes the 'nbdev_export()' function to prepare the module for export. This setup facilitates exporting the code to a package or documentation system.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/99_pytorch_doc.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Implementing SortedDL DataLoader for Length-based Sorting\nDESCRIPTION: A specialized DataLoader that sorts examples by length for efficient batch processing. It supports custom sorting functions, efficient reuse of pre-computed sorting results, and intelligent shuffling strategies that keep similar-length items in the same batch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef _default_sort(x): return len(x[0])\n\n@delegates(TfmdDL)\nclass SortedDL(TfmdDL):\n    \"A `DataLoader` that goes throught the item in the order given by `sort_func`\"\n    def __init__(self, dataset, sort_func=None, res=None, **kwargs):\n        super().__init__(dataset, **kwargs)\n        self.sort_func = _default_sort if sort_func is None else sort_func\n        if res is None and self.sort_func == _default_sort: res = _get_lengths(dataset)\n        self.res = [self.sort_func(self.do_item(i)) for i in range_of(self.dataset)] if res is None else res\n        if len(self.res) > 0: self.idx_max = np.argmax(self.res)\n\n    def get_idxs(self):\n        idxs = super().get_idxs()\n        if self.shuffle: return idxs\n        return sorted(idxs, key=lambda i: self.res[i], reverse=True)\n\n    def shuffle_fn(self,idxs):\n        idxs = np.random.permutation(len(self.dataset))\n        idx_max = np.where(idxs==self.idx_max)[0][0]\n        idxs[0],idxs[idx_max] = idxs[idx_max],idxs[0]\n        sz = self.bs*50\n        chunks = [idxs[i:i+sz] for i in range(0, len(idxs), sz)]\n        chunks = [sorted(s, key=lambda i: self.res[i], reverse=True) for s in chunks]\n        sort_idx = np.concatenate(chunks)\n\n        sz = self.bs\n        batches = [sort_idx[i:i+sz] for i in range(0, len(sort_idx), sz)]\n        sort_idx = np.concatenate(np.random.permutation(batches[1:-1])) if len(batches) > 2 else np.array([],dtype=int)\n        sort_idx = np.concatenate((batches[0], sort_idx) if len(batches)==1 else (batches[0], sort_idx, batches[-1]))\n        return iter(sort_idx)\n\n    @delegates(TfmdDL.new)\n    def new(self, dataset=None, **kwargs):\n        if 'val_res' in kwargs and kwargs['val_res'] is not None: res = kwargs['val_res']\n        else: res = self.res if dataset is None else None\n        return super().new(dataset=dataset, res=res, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Testing FillMissing with Different Strategies - Python\nDESCRIPTION: This code tests the `FillMissing` transform with different filling strategies: median, constant, and mode. It creates a DataFrame with missing values, applies the transform using different strategies, and asserts that the missing values are filled correctly and that the '_na' column is added as expected.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nfill1,fill2,fill3 = (FillMissing(fill_strategy=s)\n                     for s in [FillStrategy.median, FillStrategy.constant, FillStrategy.mode])\ndf = pd.DataFrame({'a':[0,1,np.nan,1,2,3,4]})\ndf1 = df.copy(); df2 = df.copy()\ntos = (TabularPandas(df, fill1, cont_names='a'),\n       TabularPandas(df1, fill2, cont_names='a'),\n       TabularPandas(df2, fill3, cont_names='a'))\ntest_eq(fill1.na_dict, {'a': 1.5})\ntest_eq(fill2.na_dict, {'a': 0})\ntest_eq(fill3.na_dict, {'a': 1.0})\n\nfor t in tos: test_eq(t.cat_names, ['a_na'])\n\nfor to_,v in zip(tos, [1.5, 0., 1.]):\n    test_eq(to_['a'].values, np.array([0, 1, v, 1, 2, 3, 4]))\n    test_eq(to_['a_na'].values, np.array([0, 0, 1, 0, 0, 0, 0]))\n```\n\n----------------------------------------\n\nTITLE: Testing total_params Utility for Model Layers (Python)\nDESCRIPTION: Runs test_eq assertions on total_params with various torch.nn modules (Linear, BatchNorm2d, Conv2d, LSTM), confirming correct counts and trainable flags for both parameterized and non-parameterized settings. Relies on torch.nn import as nn, a working total_params function, and test_eq.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\ntest_eq(total_params(nn.Linear(10,32)), (32*10+32,True))\ntest_eq(total_params(nn.Linear(10,32, bias=False)), (32*10,True))\ntest_eq(total_params(nn.BatchNorm2d(20)), (20*2, True))\ntest_eq(total_params(nn.BatchNorm2d(20, affine=False)), (0,False))\ntest_eq(total_params(nn.Conv2d(16, 32, 3)), (16*32*3*3 + 32, True))\ntest_eq(total_params(nn.Conv2d(16, 32, 3, bias=False)), (16*32*3*3, True))\n#First ih layer 20--10, all else 10--10. *4 for the four gates\ntest_eq(total_params(nn.LSTM(20, 10, 2)), (4 * (20*10 + 10) + 3 * 4 * (10*10 + 10), True))\n```\n\n----------------------------------------\n\nTITLE: Import nbdev showdoc Utility\nDESCRIPTION: Imports the `show_doc` function from `nbdev.showdoc`. This function is used within notebooks managed by `nbdev` to render documentation for specified Python objects.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/16_callback.progress.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Text Classifier - drop_mult Test\nDESCRIPTION: Tests whether `drop_mult` correctly scales the dropout probabilities in the model's configuration.  It creates a new text classifier with a specified `drop_mult` value and then checks that the dropout probabilities in the layers are scaled correctly.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ntst = get_text_classifier(AWD_LSTM, 100, 3, config=config, drop_mult=0.5)\ntest_eq(tst[1].layers[1][1].p, 0.1)\ntest_eq(tst[1].layers[0][1].p, config['output_p']*0.5)\nfor rnn in tst[0].module.rnns: test_eq(rnn.weight_p, config['weight_p']*0.5)\nfor dp in tst[0].module.hidden_dps: test_eq(dp.p, config['hidden_p']*0.5)\ntest_eq(tst[0].module.encoder_dp.embed_p, config['embed_p']*0.5)\ntest_eq(tst[0].module.input_dp.p, config['input_p']*0.5)\n```\n\n----------------------------------------\n\nTITLE: Create Language Model\nDESCRIPTION: This code creates a language model using AWD_LSTM architecture from fastai. It updates the default configuration with specified dropout probabilities and then instantiates the model with the vocabulary size from the dataloaders and the updated configuration.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/35_tutorial.wikitext.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nconfig = awd_lstm_lm_config.copy()\nconfig.update({'input_p': 0.6, 'output_p': 0.4, 'weight_p': 0.5, 'embed_p': 0.1, 'hidden_p': 0.2})\nmodel = get_language_model(AWD_LSTM, len(dls.vocab), config=config)\n```\n\n----------------------------------------\n\nTITLE: Generating Lists of LossMetric Instances for Multiple Loss Attributes in Python\nDESCRIPTION: Defines a helper function LossMetrics that takes attribute names and optional custom names, producing a list of LossMetric objects for easy integration into fastai learners. It supports string inputs that are comma-separated and lists, facilitating better metric tracking during training for multiple loss terms simultaneously. Dependencies include the LossMetric class and fastai utility functions for batch size handling.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef LossMetrics(attrs, nms=None):\n    \"List of `LossMetric` for each of `attrs` and `nms`\"\n    if isinstance(attrs, str): attrs = attrs.split(',')\n    nms = attrs if nms is None else nms.split(',') if isinstance(nms, str) else nms\n    return [LossMetric(a, n) for a,n in zip(attrs,nms)]\n```\n\n----------------------------------------\n\nTITLE: Importing Custom PyTorch Components (Python)\nDESCRIPTION: This code imports all public objects from the `migrating_pytorch` module, which contains the original PyTorch training code components like data loaders (`train_loader`, `test_loader`), model (`Net`), etc. This makes these components available for use when setting up the fastai `Learner`, demonstrating how fastai can integrate with existing PyTorch code structures.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_pytorch.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom migrating_pytorch import *\n```\n\n----------------------------------------\n\nTITLE: Installing fastai library in Google Colab\nDESCRIPTION: This snippet upgrades or installs the fastai library in a Colab environment to ensure the latest version is available. It checks for the existence of the /content directory as a simple condition before executing the pip install command.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18c_callback.channelslast.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai\n```\n\n----------------------------------------\n\nTITLE: Leaky ReLU Function\nDESCRIPTION: This function, `vleaky_relu`, is a wrapper around `F.leaky_relu` with a fixed negative slope of 0.3.  The `inplace` parameter controls whether the operation is performed in-place.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef vleaky_relu(input, inplace=True):\n    \"`F.leaky_relu` with 0.3 slope\"\n    return F.leaky_relu(input, negative_slope=0.3, inplace=inplace)\n```\n\n----------------------------------------\n\nTITLE: DataBlock Summary Method\nDESCRIPTION: This patched `summary` method for `DataBlock` steps through the transform pipeline for one batch of data. It sets up the type transforms pipelines, builds one sample, applies item transforms, before_batch transforms, collates items into a batch, and applies batch transforms.  It then optionally calls `show_batch(**kwargs)` on the transient `Dataloaders` to display the transformed data. It provides detailed logging of each step, aiding in debugging data pipeline issues.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n@patch\ndef summary(self:DataBlock,\n    source, # The data source  \n    bs:int=4, # The batch size\n    show_batch:bool=False, # Call `show_batch` after the summary\n    **kwargs, # Additional keyword arguments to `show_batch`\n):\n    \"Steps through the transform pipeline for one batch, and optionally calls `show_batch(**kwargs)` on the transient `Dataloaders`.\"\n    print(f\"Setting-up type transforms pipelines\")\n    dsets = self.datasets(source, verbose=True)\n    print(\"\\nBuilding one sample\")\n    for tl in dsets.train.tls:\n        _apply_pipeline(tl.tfms, get_first(dsets.train.items))\n    print(f\"\\nFinal sample: {dsets.train[0]}\\n\\n\")\n\n    dls = self.dataloaders(source, bs=bs, verbose=True)\n    print(\"\\nBuilding one batch\")\n    if len([f for f in dls.train.after_item.fs if f.name != 'noop'])!=0:\n        print(\"Applying item_tfms to the first sample:\")\n        s = [_apply_pipeline(dls.train.after_item, dsets.train[0])]\n        print(f\"\\nAdding the next {bs-1} samples\")\n        s += [dls.train.after_item(dsets.train[i]) for i in range(1, bs)]\n    else:\n        print(\"No item_tfms to apply\")\n        s = [dls.train.after_item(dsets.train[i]) for i in range(bs)]\n\n    if len([f for f in dls.train.before_batch.fs if f.name != 'noop'])!=0:\n        print(\"\\nApplying before_batch to the list of samples\")\n        s = _apply_pipeline(dls.train.before_batch, s)\n    else: print(\"\\nNo before_batch transform to apply\")\n\n    print(\"\\nCollating items in a batch\")\n    try:\n        b = dls.train.create_batch(s)\n        b = retain_types(b, s[0] if is_listy(s) else s)\n    except Exception as e:\n        print(\"Error! It's not possible to collate your items in a batch\")\n        why = _find_fail_collate(s)\n        print(\"Make sure all parts of your samples are tensors of the same size\" if why is None else why)\n        raise e\n\n    if len([f for f in dls.train.after_batch.fs if f.name != 'noop'])!=0:\n        print(\"\\nApplying batch_tfms to the batch built\")\n        b = to_device(b, dls.device)\n        b = _apply_pipeline(dls.train.after_batch, b)\n    else: print(\"\\nNo batch_tfms to apply\")\n\n    if show_batch: dls.show_batch(**kwargs)\n```\n\n----------------------------------------\n\nTITLE: Defining Interpretation Base Class (Python)\nDESCRIPTION: Defines the `Interpretation` base class, designed for analyzing model predictions. It stores the learner, dataloader, and pre-calculated losses upon initialization and provides a `__getitem__` method to efficiently retrieve data for specific indices by processing in batches.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass Interpretation():\n    \"Interpretation base class, can be inherited for task specific Interpretation classes\"\n    def __init__(self,\n        learn:Learner,\n        dl:DataLoader, # `DataLoader` to run inference over\n        losses:TensorBase, # Losses calculated from `dl`\n        act=None # Activation function for prediction\n    ): \n        store_attr()\n\n    def __getitem__(self, idxs):\n        \"Return inputs, preds, targs, decoded outputs, and losses at `idxs`\"\n        if isinstance(idxs, Tensor): idxs = idxs.tolist()\n        if not is_listy(idxs): idxs = [idxs]\n        items = getattr(self.dl.items, 'iloc', L(self.dl.items))[idxs]\n        tmp_dl = self.learn.dls.test_dl(items, with_labels=True, process=not isinstance(self.dl, TabDataLoader))\n        inps,preds,targs,decoded = self.learn.get_preds(dl=tmp_dl, with_input=True, with_loss=False, \n                                                        with_decoded=True, act=self.act, reorder=False)\n        return inps, preds, targs, decoded, self.losses[idxs]\n```\n\n----------------------------------------\n\nTITLE: Generating Predictions for Test Set - Python\nDESCRIPTION: Uses the trained learner to get prediction scores for all images in the test DataLoader. Output: tuple of (predictions, targets). Main output is a tensor of per-class probabilities. Prerequisite: trained model and test DataLoader.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\npreds, _ = learn.get_preds(dl=dl)\n```\n\n----------------------------------------\n\nTITLE: TabularPandas Pipeline with Target Type Test - Python\nDESCRIPTION: This code snippet tests a TabularPandas pipeline, ensuring that the types of the categorical variables are handled correctly across different operating systems. This tests the integrity of categorical encoding within a data processing pipeline.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'a':[0,1,2,1,1,2,0], 'b':[0,1,np.nan,1,2,3,4], 'c': ['b','a','b','a','a','b','a']})\nto = TabularPandas(df, procs, 'a', 'b', y_names='c')\n\ntest_series(to.cat_names, ['a', 'b_na'])\ntest_series(to['a'], [1,2,3,2,2,3,1])\ntest_eq(df.a.dtype, np.int64 if sys.platform == \"win32\" else int)\ntest_series(to['b_na'], [1,1,2,1,1,1,1])\ntest_series(to['c'], [1,0,1,0,0,1,0])\n```\n\n----------------------------------------\n\nTITLE: Creating Matplotlib Subplots for Image Display using Python\nDESCRIPTION: Defines `subplots` as a wrapper around `plt.subplots`, with parameters aimed at displaying images suitably sized for visualization. It manages figure size based on image size and number of rows/columns, optionally adds a super title, and guarantees the axes output is always an array for consistency. It supports all parameters accepted by matplotlib's `plt.subplots`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@delegates(plt.subplots, keep=True)\ndef subplots(\n    nrows:int=1, # Number of rows in returned axes grid\n    ncols:int=1, # Number of columns in returned axes grid\n    figsize:tuple=None, # Width, height in inches of the returned figure\n    imsize:int=3, # Size (in inches) of images that will be displayed in the returned figure\n    suptitle:str=None, # Title to be set to returned figure\n    **kwargs\n) -> (plt.Figure, plt.Axes): # Returns both fig and ax as a tuple\n    \"Returns a figure and set of subplots to display images of `imsize` inches\"\n    if figsize is None:\n        h=nrows*imsize if suptitle is None or imsize>2 else nrows*imsize+0.6 #https://github.com/matplotlib/matplotlib/issues/5355\n        figsize=(ncols*imsize, h)\n    fig,ax = plt.subplots(nrows, ncols, figsize=figsize, **kwargs)\n    if suptitle is not None: fig.suptitle(suptitle)\n    if nrows*ncols==1: ax = array([ax])\n    return fig,ax\n```\n\n----------------------------------------\n\nTITLE: FastAI Callback Module Import and Basic Setup\nDESCRIPTION: Imports essential modules for fastai callback functionality and future annotations support, establishing the groundwork for callback creation and model interaction.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n#|default_exp callback.tensorboard\nfrom __future__ import annotations\nfrom fastai.basics import *\n```\n\n----------------------------------------\n\nTITLE: Implementing spectral_norm for Conv1d\nDESCRIPTION: Defines a helper function `_conv1d_spect` to create and initialize a 1D convolutional layer with spectral normalization. This function sets up the convolution layer, initializes the weights using Kaiming normal initialization, and applies spectral normalization. Includes the `spectral_norm` method, assumed to be defined elsewhere.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_48\n\nLANGUAGE: Python\nCODE:\n```\ndef _conv1d_spect(ni:int, no:int, ks:int=1, stride:int=1, padding:int=0, bias:bool=False):\n    \"Create and initialize a `nn.Conv1d` layer with spectral normalization.\"\n    conv = nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)\n    nn.init.kaiming_normal_(conv.weight)\n    if bias: conv.bias.data.zero_()\n    return spectral_norm(conv)\n```\n\n----------------------------------------\n\nTITLE: Initializing Language Modeling DataLoader with Sequence Batching in fastai (Python)\nDESCRIPTION: Defines the LMDataLoader class, a subclass of TfmdDL, optimized for language modeling tasks. It preprocesses a dataset of numericalized text sequences by concatenating and splitting them into batches of size `bs` and sequences of length `seq_len`. The DataLoader supports optional sequence length input for optimization and enables shuffling. The `create_item` method generates paired input and target tensors shifted by one token, as required for language modeling tasks. Key internal data structures include chunked texts and batch length computations to ensure full coverage of the corpus. This DataLoader is integral for training language models efficiently.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@delegates()\nclass LMDataLoader(TfmdDL):\n    \"A `DataLoader` suitable for language modeling\"\n    def __init__(self, dataset, lens=None, cache=2, bs=64, seq_len=72, num_workers=0, **kwargs):\n        self.items = ReindexCollection(dataset, cache=cache, tfm=_maybe_first)\n        self.seq_len = seq_len\n        if lens is None: lens = _get_lengths(dataset)\n        if lens is None: lens = [len(o) for o in self.items]\n        self.lens = ReindexCollection(lens, idxs=self.items.idxs)\n        # The \"-1\" is to allow for final label, we throw away the end that's less than bs\n        corpus = round_multiple(sum(lens)-1, bs, round_down=True)\n        self.bl = corpus//bs #bl stands for batch length\n        self.n_batches = self.bl//(seq_len) + int(self.bl%seq_len!=0)\n        self.last_len = self.bl - (self.n_batches-1)*seq_len\n        self.make_chunks()\n        super().__init__(dataset=dataset, bs=bs, num_workers=num_workers, **kwargs)\n        self.n = self.n_batches*bs\n\n    def make_chunks(self): self.chunks = Chunks(self.items, self.lens)\n    def shuffle_fn(self,idxs):\n        self.items.shuffle()\n        self.make_chunks()\n        return idxs\n\n    def create_item(self, seq):\n        if seq is None: seq = 0\n        if seq>=self.n: raise IndexError\n        sl = self.last_len if seq//self.bs==self.n_batches-1 else self.seq_len\n        st = (seq%self.bs)*self.bl + (seq//self.bs)*self.seq_len\n        txt = self.chunks[st : st+sl+1]\n        return LMTensorText(txt[:-1]),txt[1:]\n\n    @delegates(TfmdDL.new)\n    def new(self, dataset=None, seq_len=None, **kwargs):\n        lens = self.lens.coll if dataset is None else None\n        seq_len = self.seq_len if seq_len is None else seq_len\n        return super().new(dataset=dataset, lens=lens, seq_len=seq_len, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Adding to_parallel Method to Learner\nDESCRIPTION: Patches the Learner class to add a method that enables parallel training by adding the ParallelTrainer callback, which handles wrapping the model in DataParallel.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@patch\ndef to_parallel(self: Learner, device_ids=None):\n    \"Add `ParallelTrainer` callback to a `Learner`\"\n    self.add_cb(ParallelTrainer(device_ids))\n    return self\n```\n\n----------------------------------------\n\nTITLE: Define TabularDataLoaders class (test_dl)\nDESCRIPTION: This method creates a test `TabDataLoader` from `test_items`. It uses the validation `TabularProc`s to process the test items.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/41_tabular.data.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n    @delegates(TabDataLoader.__init__)\n    def test_dl(self,\n        test_items, # Items to create new test `TabDataLoader` formatted the same as the training data\n        rm_type_tfms=None, # Number of `Transform`s to be removed from `procs`\n        process:bool=True, # Apply validation `TabularProc`s to `test_items` immediately\n        inplace:bool=False, # Keep separate copy of original `test_items` in memory if `False`\n        **kwargs\n    ):\n        \"Create test `TabDataLoader` from `test_items` using validation `procs`\"\n        to = self.train_ds.new(test_items, inplace=inplace)\n        if process: to.process()\n        return self.valid.new(to, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Adding Column Transform Properties to Tabular Wrapper Class in Python\nDESCRIPTION: This utility function dynamically adds property getter/setter pairs to the Tabular class for categorical, continuous, y (target), x (features), and all columns. These properties allow accessing or setting values of specific column groups directly as attributes for convenience and code clarity. The function uses Python descriptors and naming conventions (e.g., cat_names) to manage underlying pandas DataFrame access, streamlining the interface to grouped columns.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef _add_prop(cls, nm):\n    @property\n    def f(o): return o[list(getattr(o,nm+'_names'))]\n    @f.setter\n    def fset(o, v): o[getattr(o,nm+'_names')] = v\n    setattr(cls, nm+'s', f)\n    setattr(cls, nm+'s', fset)\n\n_add_prop(Tabular, 'cat')\n_add_prop(Tabular, 'cont')\n_add_prop(Tabular, 'y')\n_add_prop(Tabular, 'x')\n_add_prop(Tabular, 'all_col')\n```\n\n----------------------------------------\n\nTITLE: Define Architecture-Specific Model Splitters and Metadata - Python\nDESCRIPTION: Defines functions (_xresnet_split, _resnet_split, etc.) for splitting specific model architectures at predefined layers based on common practices for transfer learning. Also defines corresponding metadata dictionaries containing 'cut' points (layer index for the body/head split), 'split' functions, default 'stats' (normalization statistics), and default 'weights' for pretrained models.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef _xresnet_split(m): return L(m[0][:3], m[0][3:], m[1:]).map(params)\ndef  _resnet_split(m): return L(m[0][:6], m[0][6:], m[1:]).map(params)\ndef _squeezenet_split(m:nn.Module): return L(m[0][0][:5], m[0][0][5:], m[1:]).map(params)\ndef _densenet_split(m:nn.Module): return L(m[0][0][:7],m[0][0][7:], m[1:]).map(params)\ndef _vgg_split(m:nn.Module): return L(m[0][0][:22], m[0][0][22:], m[1:]).map(params)\ndef _alexnet_split(m:nn.Module): return L(m[0][0][:6], m[0][0][6:], m[1:]).map(params)\n\n_default_meta    = {'cut':None, 'split':default_split}\n_xresnet_meta    = {'cut':-4, 'split':_xresnet_split, 'stats':imagenet_stats}\n_resnet_meta     = {'cut':-2, 'split':_resnet_split, 'stats':imagenet_stats, 'weights':'DEFAULT'}\n_squeezenet_meta = {'cut':-1, 'split': _squeezenet_split, 'stats':imagenet_stats, 'weights':'DEFAULT'}\n_densenet_meta   = {'cut':-1, 'split':_densenet_split, 'stats':imagenet_stats, 'weights':'DEFAULT'}\n_vgg_meta        = {'cut':-2, 'split':_vgg_split, 'stats':imagenet_stats, 'weights':'DEFAULT'}\n_alexnet_meta    = {'cut':-2, 'split':_alexnet_split, 'stats':imagenet_stats, 'weights':'DEFAULT'}\n```\n\n----------------------------------------\n\nTITLE: Datasets with Pandas DataFrame\nDESCRIPTION: This example shows dataset creation from a pandas DataFrame with a specific attribute getter transform. It tests item retrieval via index, multiple indices, and boolean mask, verifying correct extraction of data columns.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_47\n\nLANGUAGE: Python\nCODE:\n```\ninp = pd.DataFrame(dict(a=[5,1,2,3,4]))\ndsets = Datasets(inp, tfms=attrgetter('a')).subset(0)\ntest_eq(*dsets[2], 2)          # Retrieve one item (subset 0 is the default)\ntest_eq(dsets[1,2], [(1,),(2,)])    # Retrieve two items by index\nmask = [True,False,False,True,False]\ntest_eq(dsets[mask], [(5,),(3,)])   # Retrieve two items by mask\n```\n\n----------------------------------------\n\nTITLE: Show Top Losses for Mask Inputs with Predictions\nDESCRIPTION: This visualization arranges input images, true masks, predicted masks, associated losses, and raw output scores in a grid with basic titles for each image. It facilitates detailed analysis of segmentation errors.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_45\n\nLANGUAGE: Python\nCODE:\n```\n@dispatch\ndef plot_top_losses(x:TensorImage, y:TensorMask, samples, outs, raws, losses, nrows=None, ncols=None, figsize=None, **kwargs):\n    axes = get_grid(len(samples)*3, nrows=len(samples), ncols=3, figsize=figsize, flatten=False, title=\"Input | Target | Prediction\")\n    if axes.ndim == 1: axes = (axes,)\n    titles = [\"input\", \"target\", \"pred\"]\n    for axs,s,o,l in zip(axes, samples, outs, losses):\n        imgs = (s[0], s[1], o[0])\n        for ax,im,title in zip(axs, imgs, titles):\n            if title==\"pred\": title += f\"; loss = {l.item():.4f}\"\n            im.show(ctx=ax, **kwargs)\n            ax.set_title(title)\n```\n\n----------------------------------------\n\nTITLE: Creating Interpretation Instance from Learner (Python)\nDESCRIPTION: Defines the class method `from_learner` to construct an `Interpretation` object directly from a `Learner`. It automatically creates a suitable dataloader (if not provided) and calculates losses by running predictions on the specified dataset index or dataloader.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@classmethod\n    def from_learner(cls,\n        learn, # Model used to create interpretation\n        ds_idx:int=1, # Index of `learn.dls` when `dl` is None\n        dl:DataLoader=None, # `Dataloader` used to make predictions\n        act=None # Override default or set prediction activation function\n    ):\n        \"Construct interpretation object from a learner\"\n        if dl is None: dl = learn.dls[ds_idx].new(shuffle=False, drop_last=False)\n        _,_,losses = learn.get_preds(dl=dl, with_input=False, with_loss=True, with_decoded=False,\n                                     with_preds=False, with_targs=False, act=act)\n        return cls(learn, dl, losses, act)\n```\n\n----------------------------------------\n\nTITLE: Saving fastai Learner Model Weights and Loading Them into PyTorch Model for Inference in Python\nDESCRIPTION: Saves the trained model's state dictionary using fastai Learner's save method without optimizer state, then reloads these weights into a new instance of the PyTorch model class independently to perform raw PyTorch inference. This process supports exporting and inference workflows outside fastai's full data API, requiring that the model source code be preserved for reconstruction.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_pytorch_verbose.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlearn.save('myModel', with_opt=False)\n```\n\nLANGUAGE: python\nCODE:\n```\nnew_net = Net()\nnet_dict = torch.load('models/myModel.pth') \nnew_net.load_state_dict(net_dict);\n```\n\n----------------------------------------\n\nTITLE: Showing TensorText Batch - Python\nDESCRIPTION: This function implements `show_batch` for `TensorText` objects. It prepares a DataFrame for displaying samples from the batch, truncating long text sequences to a specified length. It uses `get_empty_df` to create an empty DataFrame, truncates samples using a list comprehension, retrieves a display function via `get_show_batch_func`, then displays the resulting DataFrame using `display_df`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@dispatch\ndef show_batch(x: TensorText, y, samples, ctxs=None, max_n=10, trunc_at=150, **kwargs):\n    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n    if trunc_at is not None: samples = L((s[0].truncate(trunc_at),*s[1:]) for s in samples)\n    ctxs = get_show_batch_func(object)(x, y, samples, max_n=max_n, ctxs=ctxs, **kwargs)\n    display_df(pd.DataFrame(ctxs))\n    return ctxs\n```\n\n----------------------------------------\n\nTITLE: Configuring LM DataLoaders for Training (Python)\nDESCRIPTION: Sets batch size and sequence length, then creates DataLoaders for the language model using the dsets object. bs specifies batch size, sl specifies max sequence length, and the returned dbunch_lm is used for model training. No explicit output beyond prepared DataLoaders.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nbs,sl=256,80\ndbunch_lm = dsets.dataloaders(bs=bs, seq_len=sl, val_bs=bs)\n```\n\n----------------------------------------\n\nTITLE: Instantiating Logistic Model and Moving to GPU - PyTorch/Python\nDESCRIPTION: Creates an instance of the `Mnist_Logistic` model and moves it to the GPU if one is available, allowing for faster computation. The model object is assigned to the `model` variable.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmodel = Mnist_Logistic().cuda()\n```\n\n----------------------------------------\n\nTITLE: Create Language Model DataBlock for Full Dataset\nDESCRIPTION: Creates a `DataBlock` for language model training on the full IMDB dataset. It specifies the text block, data source (files in specified folders), and splitter. The `get_items` parameter utilizes a `partial` function to apply the `get_text_files` function to specific folders. RandomSplitter splits the data randomly.  `seq_len` sets the sequence length for the language model.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimdb_lm = DataBlock(blocks=(TextBlock.from_folder(path, is_lm=True),),\n                    get_items=partial(get_text_files, folders=['train', 'test', 'unsup']),\n                    splitter=RandomSplitter(0.1))\n\ndbunch_lm = imdb_lm.dataloaders(path, path=path, bs=bs, seq_len=80)\n```\n\n----------------------------------------\n\nTITLE: Using Context Manager to Change Dataset Split Index\nDESCRIPTION: This code uses a context manager to temporarily change the dataset's split index to 1, applying the transformation only to the training subset within the context. It validates the transformation effect and restores the original split index afterwards.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_54\n\nLANGUAGE: Python\nCODE:\n```\n#A context manager to change the split_idx and apply the validation transform on the training set\nds = dsets.train\nwith ds.set_split_idx(1):\n    test_eq(ds,[(2,),(4,)])\ntest_eq(dsets.train,[(1,),(2,)])\n```\n\n----------------------------------------\n\nTITLE: Retrieving Embedding Sizes for Tabular Data in FastAI - Python\nDESCRIPTION: Function to retrieve embedding sizes for all categorical variables in a `Tabular` or `TabularPandas` object, optionally overridden by an embedding size dictionary `sz_dict`. Returns a list of tuples for each categorical variable, pairing the number of unique classes with the embedding dimension size. This utility facilitates automated or manual embedding size assignment when building tabular models.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/42_tabular.model.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_emb_sz(\n    to:Tabular|TabularPandas, \n    sz_dict:dict=None # Dictionary of {'class_name' : size, ...} to override default `emb_sz_rule` \n) -> list: # List of embedding sizes for each category\n    \"Get embedding size for each cat_name in `Tabular` or `TabularPandas`, or populate embedding size manually using sz_dict\"\n    return [_one_emb_sz(to.classes, n, sz_dict) for n in to.cat_names]\n```\n\n----------------------------------------\n\nTITLE: Defining Random Data Split\nDESCRIPTION: Defines a random splitting strategy for the dataset. It creates a RandomSplitter that reserves 20% of the data for validation, to be applied to the range of indices in the DataFrame.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nsplits = RandomSplitter(valid_pct=0.2)(range_of(df))\n```\n\n----------------------------------------\n\nTITLE: Defining Event Lists for Epoch Handling in Fastai (Python)\nDESCRIPTION: Assigns lists '_before_epoch' and '_after_epoch' containing references to event hooks used in the fit/training workflow. These drive callback and event-based execution order during learner training. Relies on the 'event' utility or enum in the fastai ecosystem.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n_before_epoch = [event.before_fit, event.before_epoch]\n_after_epoch  = [event.after_epoch, event.after_fit]\n```\n\n----------------------------------------\n\nTITLE: SegmentationInterpretation Class in fastai\nDESCRIPTION: This code defines the `SegmentationInterpretation` class. This class is intended to provide methods for interpreting the results of segmentation models. It is currently a placeholder, as it has no methods or attributes beyond the basic initialization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass SegmentationInterpretation(Interpretation):\n    \"Interpretation methods for segmentation models.\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Defining FocalLossFlat in Fastai\nDESCRIPTION: This code defines the `FocalLossFlat` class, which inherits from `BaseLoss` and implements focal loss. It flattens the input and target tensors and provides methods for decoding the model output (argmax) and applying the softmax activation function. It accepts a `gamma` parameter to control the down-weighting of easy examples.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass FocalLossFlat(BaseLoss):\n    \"\"\"\n    Same as CrossEntropyLossFlat but with focal paramter, `gamma`. Focal loss is introduced by Lin et al. \n    https://arxiv.org/pdf/1708.02002.pdf. Note the class weighting factor in the paper, alpha, can be \n    implemented through pytorch `weight` argument passed through to F.cross_entropy.\n    \"\"\"\n    y_int = True # y interpolation\n    @use_kwargs_dict(keep=True, weight=None, reduction='mean')\n    def __init__(self, \n        *args, \n        gamma:float=2.0, # Focusing parameter. Higher values down-weight easy examples' contribution to loss\n        axis:int=-1, # Class axis\n        **kwargs\n    ):\n        super().__init__(FocalLoss, *args, gamma=gamma, axis=axis, **kwargs)\n        \n    def decodes(self, x:Tensor) -> Tensor: \n        \"Converts model output to target format\"\n        return x.argmax(dim=self.axis)\n    \n    def activation(self, x:Tensor) -> Tensor: \n        \"`F.cross_entropy`'s fused activation function applied to model output\"\n        return F.softmax(x, dim=self.axis)\n```\n\n----------------------------------------\n\nTITLE: Identifying continuous and categorical variables in Python\nDESCRIPTION: Separates dataframe columns into continuous and categorical variables based on data type and cardinality. Integer columns with unique values above max_card are treated as continuous.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef cont_cat_split(df, max_card=20, dep_var=None):\n    \"Helper function that returns column names of cont and cat variables from given `df`.\"\n    cont_names, cat_names = [], []\n    for label in df:\n        if label in L(dep_var): continue\n        if ((pd.api.types.is_integer_dtype(df[label].dtype) and\n            df[label].unique().shape[0] > max_card) or\n            pd.api.types.is_float_dtype(df[label].dtype)):\n            cont_names.append(label)\n        else: cat_names.append(label)\n    return cont_names, cat_names\n```\n\n----------------------------------------\n\nTITLE: Creating Datasets and DataLoaders - PyTorch/fastai/Python\nDESCRIPTION: Defines the batch size (`bs`), creates `TensorDataset` objects for training and validation data, and then creates fastai `TfmdDL` (Transformed DataLoaders) and a `DataLoaders` object. The training DataLoader shuffles data, while the validation DataLoader uses a batch size twice as large.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nbs=64\ntrain_ds = TensorDataset(x_train, y_train)\nvalid_ds = TensorDataset(x_valid, y_valid)\ntrain_dl = TfmdDL(train_ds, bs=bs, shuffle=True)\nvalid_dl = TfmdDL(valid_ds, bs=2*bs)\ndls = DataLoaders(train_dl, valid_dl)\n```\n\n----------------------------------------\n\nTITLE: num_features_model Function Definition\nDESCRIPTION: This code defines the `num_features_model` function, which returns the number of output features for a given model `m`. It uses the `model_sizes` function to determine the shapes of the activations at various layers and returns the number of channels in the last layer. It tries different input sizes until it finds a size that works, or raises an exception if it cannot find a suitable size after trying sizes up to 2048x2048.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef num_features_model(m):\n    \"Return the number of output features for `m`.\"\n    sz,ch_in = 32,in_channels(m)\n    while True:\n        #Trying for a few sizes in case the model requires a big input size.\n        try:\n            return model_sizes(m, (sz,sz))[-1][1]\n        except Exception as e:\n            sz *= 2\n            if sz > 2048: raise e\n```\n\n----------------------------------------\n\nTITLE: Processing TensorImage with Category Data for Wandb\nDESCRIPTION: This `wandb_process` function processes `TensorImage` data with `TensorCategory` or `TensorMultiCategory` labels for logging to Weights & Biases (W&B). It creates a W&B table with columns for the input image, ground truth, and predictions. It iterates through the samples and predictions, adding the data to the table. The `_unlist` function is used to simplify predicted labels.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n@dispatch\ndef wandb_process(x:TensorImage, y:TensorCategory|TensorMultiCategory, samples, outs, preds):\n    table = wandb.Table(columns=[\"Input image\", \"Ground_Truth\", \"Predictions\"])\n    for (image, label), pred_label in zip(samples,outs):\n        table.add_data(wandb.Image(image.permute(1,2,0)), label, _unlist(pred_label))\n    return {\"Prediction_Samples\": table}\n```\n\n----------------------------------------\n\nTITLE: Viewing Processed Training Data Head - Python\nDESCRIPTION: Displays the first few rows of the processed training data within the `TabularPandas` object, showing the effect of the applied processors.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nto.train.items.head()\n```\n\n----------------------------------------\n\nTITLE: Imports: ResNet34 encoder (Python)\nDESCRIPTION: Imports the `resnet34` function from `fastai.vision.models`. This function provides a standard ResNet34 convolutional neural network architecture, which is commonly used as a pretrained encoder backbone for transfer learning tasks, including segmentation with models like the Dynamic UNet.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15a_vision.models.unet.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.vision.models import resnet34\n```\n\n----------------------------------------\n\nTITLE: Normalize with Splits Example\nDESCRIPTION: This code demonstrates `Normalize` being used on a `TabularGPU` with train/valid splits. The `Normalize` statistics are calculated ONLY on the training split, and then applied to both the training and validation splits.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf = cudf.from_pandas(pd.DataFrame({'a':[0,1,2,3,4]}))\nto = TabularGPU(df, Normalize, cont_names='a', splits=[[0,1,2], [3,4]])\nnorm = to.procs.normalize\n\nx = np.array([0,1,2])\nm,s = x.mean(),x.std()\ntest_eq(norm.means, {'a': m})\ntest_close(norm.stds['a'], s)\ntest_close(to.a.to_array(), (np.array([0,1,2,3,4])-m)/s)\n```\n\n----------------------------------------\n\nTITLE: Creating Tabular Learner Model\nDESCRIPTION: Initializes a fastai Learner object specifically for tabular data. It takes the DataLoaders (`dls`) and specifies 'accuracy' as the evaluation metric during training. fastai infers the appropriate model architecture and loss function.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nlearn = tabular_learner(dls, metrics=accuracy)\n```\n\n----------------------------------------\n\nTITLE: Making image predictions\nDESCRIPTION: This code uses the trained model to make a prediction on a sample image. It creates a `PILImage` object, calls the `predict` method, and prints whether the image is a cat and the probability of it being a cat. It demonstrates how to use the model to predict the image category.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/quick_start.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimg = PILImage.create('images/cat.jpg')\nimg\n```\n\nLANGUAGE: python\nCODE:\n```\nis_cat,_,probs = learn.predict(img)\nprint(f\"Is this a cat?: {is_cat}.\")\nprint(f\"Probability it's a cat: {probs[1].item():.6f}\")\n```\n\n----------------------------------------\n\nTITLE: Loading a Learner for Inference\nDESCRIPTION: This code defines a function `load_learner` to load a serialized `Learner` object.  The function sets the device to 'cpu' by default, and it includes a warning that the `pickle` module is insecure, and provides a callout warning about the use of `pickle` for loading custom code. This loads the `Learner` and moves it to the specified device.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\ndef load_learner(fname, cpu=True, pickle_module=pickle):\n    \"Load a `Learner` object in `fname`, by default putting it on the `cpu`\"\n    distrib_barrier()\n    map_loc = 'cpu' if cpu else default_device()\n    try:\n        warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n        load_kwargs = {\"weights_only\": False} if ismin_torch(\"2.6\") else {}\n        res = torch.load(fname, map_location=map_loc, pickle_module=pickle_module, **load_kwargs)\n    except ImportError as e:\n        if any(o in str(e) for o in (\"fastcore.transform\",\"fastcore.dispatch\")):\n            raise RuntimeError(f\"Loading model {fname=}, attempted to import from `fastcore.dispatch` and/or `fastcore.transform` which are deprecated in `fastai>=2.8.0`.\\nDowngrade to `fastai<2.8.0` if you want to load this model.\")\n    except AttributeError as e:\n        e.args = [f\"Custom classes or functions exported with your `Learner` not available in namespace. Re-declare/import before loading:\\n\\t{e.args[0]}\"]\n        raise\n    if cpu:\n        res.dls.cpu()\n        if hasattr(res, 'channels_last'): res = res.to_contiguous(to_fp32=True)\n        elif hasattr(res, 'mixed_precision'): res = res.to_fp32()\n        elif hasattr(res, 'non_native_mixed_precision'): res = res.to_non_native_fp32()\n    return res\n```\n\n----------------------------------------\n\nTITLE: TensorBoard Embedding Projector Support Details\nDESCRIPTION: Outlines how to enable embedding visualization for images and words during training and inference, including example code for exporting embeddings from models and language models with transformers.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_4\n\n\n\n----------------------------------------\n\nTITLE: Example and Tests for `to_cpu` in fastai\nDESCRIPTION: Demonstrates using the `to_cpu` function to move a tensor `t3` (potentially on CUDA) back to the CPU. It includes `test_eq` assertions to verify that the tensor's type changes to a standard CPU tensor type (e.g., `torch.LongTensor`) and that its value remains correct after the move.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nt3 = to_cpu(t3)\ntest_eq(t3.type(), \"torch.LongTensor\")\ntest_eq(t3, 2)\n```\n\n----------------------------------------\n\nTITLE: AWD-LSTM Implementation - PyTorch\nDESCRIPTION: The `AWD_LSTM` class implements the AWD-LSTM model. It takes various parameters, including vocabulary size, embedding size, hidden size, number of layers, dropout probabilities for embeddings, hidden states, input and hidden-to-hidden weights. It creates an embedding layer, applies embedding dropout, multiple LSTM layers with weight dropout, and hidden state dropout between the LSTM layers. It handles batch size changes and resets the hidden states.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nclass AWD_LSTM(Module):\n    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182\"\n    initrange=0.1\n\n    def __init__(self, \n        vocab_sz:int, # Size of the vocabulary\n        emb_sz:int, # Size of embedding vector\n        n_hid:int, # Number of features in hidden state\n        n_layers:int, # Number of LSTM layers\n        pad_token:int=1, # Padding token id\n        hidden_p:float=0.2, # Dropout probability for hidden state between layers\n        input_p:float=0.6, # Dropout probability for LSTM stack input\n        embed_p:float=0.1, # Embedding layer dropout probabillity\n        weight_p:float=0.5, # Hidden-to-hidden wight dropout probability for LSTM layers\n        bidir:bool=False # If set to `True` uses bidirectional LSTM layers\n    ):\n        store_attr('emb_sz,n_hid,n_layers,pad_token')\n        self.bs = 1\n        self.n_dir = 2 if bidir else 1\n        self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n        self.encoder_dp = EmbeddingDropout(self.encoder, embed_p)\n        self.rnns = nn.ModuleList([self._one_rnn(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.n_dir,\n                                                 bidir, weight_p, l) for l in range(n_layers)])\n        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n        self.input_dp = RNNDropout(input_p)\n        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n        self.reset()\n\n    def forward(self, inp:Tensor, from_embeds:bool=False):\n        bs,sl = inp.shape[:2] if from_embeds else inp.shape\n        if bs!=self.bs: self._change_hidden(bs)\n\n        output = self.input_dp(inp if from_embeds else self.encoder_dp(inp))\n        new_hidden = []\n        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n            output, new_h = rnn(output, self.hidden[l])\n            new_hidden.append(new_h)\n            if l != self.n_layers - 1: output = hid_dp(output)\n        self.hidden = to_detach(new_hidden, cpu=False, gather=False)\n        return output\n\n    def _change_hidden(self, bs):\n        self.hidden = [self._change_one_hidden(l, bs) for l in range(self.n_layers)]\n        self.bs = bs\n\n    def _one_rnn(self, n_in, n_out, bidir, weight_p, l):\n        \"Return one of the inner rnn\"\n        rnn = nn.LSTM(n_in, n_out, 1, batch_first=True, bidirectional=bidir)\n        return WeightDropout(rnn, weight_p)\n\n    def _one_hidden(self, l):\n        \"Return one hidden state\"\n        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n        return (one_param(self).new_zeros(self.n_dir, self.bs, nh), one_param(self).new_zeros(self.n_dir, self.bs, nh))\n\n    def _change_one_hidden(self, l, bs):\n        if self.bs < bs:\n            nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n            return tuple(torch.cat([h, h.new_zeros(self.n_dir, bs-self.bs, nh)], dim=1) for h in self.hidden[l])\n        if self.bs > bs: return (self.hidden[l][0][:,:bs].contiguous(), self.hidden[l][1][:,:bs].contiguous())\n        return self.hidden[l]\n\n    def reset(self):\n        \"Reset the hidden states\"\n        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n        self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-class Jaccard Coefficient (mIoU) for Segmentation in PyTorch\nDESCRIPTION: Class that implements the averaged Jaccard coefficient (mean IoU) for multiclass segmentation, extending DiceMulti to calculate class-wise IoU scores and then average them.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nclass JaccardCoeffMulti(DiceMulti):\n    \"Averaged Jaccard coefficient metric (mIoU) for multiclass target in segmentation\"\n    @property\n    def value(self):\n        binary_jaccard_scores = np.array([])\n        for c in self.inter:\n            binary_jaccard_scores = np.append(binary_jaccard_scores, self.inter[c]/(self.union[c]-self.inter[c]) if self.union[c] > 0 else np.nan)\n        return np.nanmean(binary_jaccard_scores)\n```\n\n----------------------------------------\n\nTITLE: Sentence Encoder Module\nDESCRIPTION: The `SentenceEncoder` class is a PyTorch `Module` designed to encode sentences using a given module (e.g., an embedding layer or an RNN) that can process tokens. It takes a `bptt` (backpropagation through time), a `module` to process the tokens, a `pad_idx` (padding index), and an optional `max_len`.  It processes the input in chunks of size `bptt`, masks the padding tokens, and concatenates the outputs.  It expects the inputs to be padded with the padding tokens first with the sequence beginning at a round multiple of `bptt`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass SentenceEncoder(Module):\n    \"Create an encoder over `module` that can process a full sentence.\"\n    def __init__(self, \n        bptt:int, # Backpropagation through time\n        module:nn.Module, # A module that can process up to [`bs`, `bptt`] tokens\n        pad_idx:int=1, # Padding token id \n        max_len:int=None # Maximal output length\n    ): \n        store_attr('bptt,module,pad_idx,max_len')\n    \n    def reset(self): getcallable(self.module, 'reset')()\n\n    def forward(self, input):\n        bs,sl = input.size()\n        self.reset()\n        mask = input == self.pad_idx\n        outs,masks = [],[]\n        for i in range(0, sl, self.bptt):\n            #Note: this expects that sequence really begins on a round multiple of bptt\n            real_bs = (input[:,i] != self.pad_idx).long().sum()\n            o = self.module(input[:real_bs,i: min(i+self.bptt, sl)])\n            if self.max_len is None or sl-i <= self.max_len:\n                outs.append(o)\n                masks.append(mask[:,i: min(i+self.bptt, sl)])\n        outs = torch.cat([_pad_tensor(o, bs) for o in outs], dim=1)\n        mask = torch.cat(masks, dim=1)\n        return outs,mask\n```\n\n----------------------------------------\n\nTITLE: Plotting Hyperparameter Schedules\nDESCRIPTION: Defines a `plot_sched` method for the `Recorder` class, which allows for visualizing the scheduled hyper-parameters. It takes the hyperparameter keys to plot as input and creates plots of their values over the training epochs. It uses matplotlib for creating the plots.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\ndef plot_sched(self:Recorder, keys=None, figsize=None):\n    keys = self.hps.keys() if keys is None else L(keys)\n    rows,cols = (len(keys)+1)//2, min(2, len(keys))\n    figsize = figsize or (6*cols,4*rows)\n    _, axs = plt.subplots(rows, cols, figsize=figsize)\n    axs = axs.flatten() if len(keys) > 1 else L(axs)\n    for p,ax in zip(keys, axs):\n        ax.plot(self.hps[p])\n        ax.set_ylabel(p)\n```\n\n----------------------------------------\n\nTITLE: Define TabularDataLoaders class (from_csv)\nDESCRIPTION: This method creates a `TabularDataLoaders` object from a CSV file. It reads the CSV file into a Pandas DataFrame and then calls the `from_df` method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/41_tabular.data.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n    @classmethod\n    def from_csv(cls,\n        csv:str|Path|io.BufferedReader, # A csv of training data\n        skipinitialspace:bool=True, # Skip spaces after delimiter\n        **kwargs\n    ):\n        \"Create `TabularDataLoaders` from `csv` file in `path` using `procs`\"\n        return cls.from_df(pd.read_csv(csv, skipinitialspace=skipinitialspace), **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Hook Class Removal Example\nDESCRIPTION: This snippet demonstrates the importance of removing hooks after usage. It registers a forward hook using the `Hook` class and the `example_forward_hook` function, then uses `test_stdout` to ensure that the forward hook is executed during a forward pass. After removing the hook, it verifies that the hook is no longer executed during a subsequent forward pass.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntst_model = nn.Linear(5,10)\nx = torch.randn(4,5)\ny = tst_model(x)\nhook = Hook(tst_model, example_forward_hook)\ntest_stdout(lambda: tst_model(x), f\"{tst_model} ({x},) {y.detach()}\")\nhook.remove()\ntest_stdout(lambda: tst_model(x), \"\")\n```\n\n----------------------------------------\n\nTITLE: Implementing EmbeddingNN\nDESCRIPTION: Defines a neural network model for collaborative filtering by subclassing TabularModel.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/45_collab.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#|export \nclass EmbeddingNN(TabularModel):\n    \"Subclass `TabularModel` to create a NN suitable for collaborative filtering.\"\n    @delegates(TabularModel.__init__)\n    def __init__(self, emb_szs, layers, **kwargs):\n        super().__init__(emb_szs=emb_szs, n_cont=0, out_sz=1, layers=layers, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Generating Predictions with GPT-2 Model (Python)\nDESCRIPTION: This code prepares input tensor 't' from the token IDs, adds a batch dimension for model compatibility, and uses the model's generate() method to produce predicted token sequences. Input should be a tensor of shape [batch, seq_len]; output is typically a tensor of generated token IDs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nt = torch.LongTensor(ids)[None]\npreds = model.generate(t)\n```\n\n----------------------------------------\n\nTITLE: Retrieving First Element from Containers including pandas DataFrames in Python\nDESCRIPTION: Defines a function that extracts the first element from various container types, including lists, tuples, and pandas DataFrames. When the input is a DataFrame, it uses the 'iloc' accessor to get the first row. This utility simplifies retrieval of the initial element regardless of container type.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_65\n\nLANGUAGE: python\nCODE:\n```\ndef get_first(c):\n    \"Get the first element of c, even if c is a dataframe\"\n    return getattr(c, 'iloc', c)[0]\n```\n\n----------------------------------------\n\nTITLE: Implementing TfmdDL class as a transformed DataLoader in fastai\nDESCRIPTION: Defines the TfmdDL class that extends PyTorch's DataLoader with transformation pipelines. It provides functionality for batch processing, decoding, and visualization while handling complex data types and transformations through the fastai pipeline system.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass TfmdDL(DataLoader):\n    \"Transformed `DataLoader`\"\n    @delegates(DataLoader.__init__)\n    def __init__(self,\n        dataset, # Map- or iterable-style dataset from which to load the data\n        bs:int=64, # Size of batch\n        shuffle:bool=False, # Whether to shuffle data\n        num_workers:int=None, # Number of CPU cores to use in parallel (default: All available up to 16)\n        verbose:bool=False, # Whether to print verbose logs\n        do_setup:bool=True, # Whether to run `setup()` for batch transform(s)\n        **kwargs\n    ):\n        if num_workers is None: num_workers = min(16, defaults.cpus)\n        for nm in _batch_tfms: kwargs[nm] = Pipeline(kwargs.get(nm,None))\n        super().__init__(dataset, bs=bs, shuffle=shuffle, num_workers=num_workers, **kwargs)\n        if do_setup:\n            for nm in _batch_tfms:\n                pv(f\"Setting up {nm}: {kwargs[nm]}\", verbose)\n                kwargs[nm].setup(self)\n\n    def _one_pass(self):\n        b = self.do_batch([self.do_item(None)])\n        if self.device is not None: b = to_device(b, self.device)\n        its = self.after_batch(b)\n        self._n_inp = 1 if not isinstance(its, (list,tuple)) or len(its)==1 else len(its)-1\n        self._types = explode_types(its)\n\n    def _retain_dl(self,b):\n        if not getattr(self, '_types', None): self._one_pass()\n        return retain_types(b, typs=self._types)\n\n    @delegates(DataLoader.new)\n    def new(self, \n        dataset=None, # Map- or iterable-style dataset from which to load the data\n        cls=None, # Class of the newly created `DataLoader` object\n        **kwargs\n    ):\n        res = super().new(dataset, cls, do_setup=False, **kwargs)\n        if not hasattr(self, '_n_inp') or not hasattr(self, '_types'):\n            try:\n                self._one_pass()\n                res._n_inp,res._types = self._n_inp,self._types\n            except Exception as e: \n                print(\"Could not do one pass in your dataloader, there is something wrong in it. Please see the stack trace below:\")\n                raise\n        else: res._n_inp,res._types = self._n_inp,self._types\n        return res\n\n    def before_iter(self):\n        super().before_iter()\n        split_idx = getattr(self.dataset, 'split_idx', None)\n        for nm in _batch_tfms:\n            f = getattr(self,nm)\n            if isinstance(f,Pipeline): f.split_idx=split_idx\n\n    def decode(self, \n        b # Batch to decode\n    ):\n        return to_cpu(self.after_batch.decode(self._retain_dl(b)))\n    def decode_batch(self, \n        b, # Batch to decode\n        max_n:int=9, # Maximum number of items to decode\n        full:bool=True # Whether to decode all transforms. If `False`, decode up to the point the item knows how to show itself\n    ): \n        return self._decode_batch(self.decode(b), max_n, full)\n\n    def _decode_batch(self, b, max_n=9, full=True):\n        f = self.after_item.decode\n        f1 = self.before_batch.decode\n        f = compose(f1, f, partial(getcallable(self.dataset,'decode'), full = full))\n        return L(batch_to_samples(b, max_n=max_n)).map(f)\n\n    def _pre_show_batch(self, b, max_n=9):\n        \"Decode `b` to be ready for `show_batch`\"\n        b = self.decode(b)\n        if hasattr(b, 'show'): return b,None,None\n        its = self._decode_batch(b, max_n, full=False)\n        if not is_listy(b): b,its = [b],L((o,) for o in its)\n        return detuplify(b[:self.n_inp]),detuplify(b[self.n_inp:]),its\n\n    def show_batch(self,\n        b=None, # Batch to show\n        max_n:int=9, # Maximum number of items to show\n        ctxs=None, # List of `ctx` objects to show data. Could be matplotlib axis, DataFrame etc\n        show:bool=True, # Whether to display data\n        unique:bool=False, # Whether to show only one \n        **kwargs\n    ):\n        \"Show `max_n` input(s) and target(s) from the batch.\"\n        if unique:\n            old_get_idxs = self.get_idxs\n            self.get_idxs = lambda: Inf.zeros\n        if b is None: b = self.one_batch()\n        if not show: return self._pre_show_batch(b, max_n=max_n)\n        show_batch(*self._pre_show_batch(b, max_n=max_n), ctxs=ctxs, max_n=max_n, **kwargs)\n        if unique: self.get_idxs = old_get_idxs\n\n    def show_results(self, \n        b, # Batch to show results for\n        out, # Predicted output from model for the batch\n        max_n:int=9, # Maximum number of items to show\n        ctxs=None, # List of `ctx` objects to show data. Could be matplotlib axis, DataFrame etc\n        show:bool=True, # Whether to display data\n        **kwargs\n    ):\n        \"Show `max_n` results with input(s), target(s) and prediction(s).\"\n        x,y,its = self.show_batch(b, max_n=max_n, show=False)\n        b_out = type(b)(b[:self.n_inp] + (tuple(out) if is_listy(out) else (out,)))\n        x1,y1,outs = self.show_batch(b_out, max_n=max_n, show=False)\n        res = (x,x1,None,None) if its is None else (x, y, its, outs.itemgot(slice(self.n_inp,None)))\n        if not show: return res\n        show_results(*res, ctxs=ctxs, max_n=max_n, **kwargs)\n\n    @property\n    def n_inp(self) -> int:\n        \"Number of elements in `Datasets` or `TfmdDL` tuple to be considered part of input.\"\n        if hasattr(self.dataset, 'n_inp'): return self.dataset.n_inp\n        if not hasattr(self, '_n_inp'): self._one_pass()\n        return self._n_inp\n```\n\n----------------------------------------\n\nTITLE: Testing Normalization Process Attribute - Python\nDESCRIPTION: This code tests whether the `Normalize` process has an attribute named 'to' within a `TabularPandas` object, which ensures that there is no persistent storage of the original `Tabular` object within the processor itself.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nnorm = Normalize()\ndf = pd.DataFrame({'a':[0,1,2,3,4]})\nto = TabularPandas(df, norm, cont_names='a', splits=[[0,1,2],[3,4]])\ntest_eq(hasattr(to.procs.normalize, 'to'), False)\n```\n\n----------------------------------------\n\nTITLE: Testing Auto-Statistics Normalization in Python\nDESCRIPTION: Tests the Normalize class without providing explicit mean and standard deviation, allowing it to calculate statistics from the data automatically.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nnrm = Normalize()\nbatch_tfms = [IntToFloatTensor(), nrm]\ntdl = TfmdDL(train_ds, after_batch=batch_tfms, bs=4)\nx,y  = tdl.one_batch()\ntest_close(x.mean(), 0.0, 1e-4)\nassert x.std()>0.9, x.std()\n```\n\n----------------------------------------\n\nTITLE: num_features_model Function Usage Example\nDESCRIPTION: This snippet demonstrates the usage of the `num_features_model` function. It creates two sequential models with different convolutional layers and then uses `num_features_model` to determine the number of output features for each model. It verifies that the returned number of features is as expected.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nm = nn.Sequential(nn.Conv2d(5,4,3), nn.Conv2d(4,3,3))\ntest_eq(num_features_model(m), 3)\nm = nn.Sequential(ConvLayer(3, 16), ConvLayer(16, 32, stride=2), ConvLayer(32, 32))\ntest_eq(num_features_model(m), 32)\n```\n\n----------------------------------------\n\nTITLE: Creating Data Directory Using fastai Config - Python\nDESCRIPTION: Creates a directory for the 'planet' dataset using fastai's Config object. Ensures the directory exists; parent folders created as needed. No inputs aside from default configuration paths. Path is returned for subsequent use. Prerequisite: fastai installed.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npath = Config().data/'planet'\npath.mkdir(parents=True, exist_ok=True)\npath\n```\n\n----------------------------------------\n\nTITLE: Logit Function with Clamping to Avoid Infinite Values in Python\nDESCRIPTION: Computes the logit (inverse sigmoid) of tensor 'x' with input values clamped between 1e-7 and 1-1e-7 to prevent infinite outputs. Useful for numerical stability when transforming probabilities to log-odds.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_82\n\nLANGUAGE: python\nCODE:\n```\ndef logit(x):\n    \"Logit of `x`, clamped to avoid inf.\"\n    x = x.clamp(1e-7, 1-1e-7)\n    return -(1/x-1).log()\n```\n\n----------------------------------------\n\nTITLE: Creating Test DataLoaders - Python\nDESCRIPTION: Generates DataLoaders for the processed test dataset using the same batch size as training. `shuffle_train=False` is set as shuffling is not needed for inference.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ntest_dls = test_to.dataloaders(bs=512, path=path, shuffle_train=False)\n```\n\n----------------------------------------\n\nTITLE: Testing DataLoader Worker Count and no_multiproc Context in Python\nDESCRIPTION: This snippet tests the behavior of `DataLoader` workers. It first determines the expected number of workers based on the operating system (disabling workers on Windows and macOS in certain contexts). It then asserts the initial worker count. Using the `no_multiproc()` context manager on the internal `fake_l` object, it temporarily disables multiprocessing, asserts the worker count becomes 0, iterates through the dataloader, and finally asserts that the worker count is restored to its original value after exiting the context.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntest_num_workers = 0 if sys.platform in (\"win32\",\"darwin\") else 4\ntest_eq(dl.fake_l.num_workers, test_num_workers)\nwith dl.fake_l.no_multiproc(): \n    test_eq(dl.fake_l.num_workers, 0)\n    L(dl).map(len)\ntest_eq(dl.fake_l.num_workers, test_num_workers)\n```\n\n----------------------------------------\n\nTITLE: Export Utility for fastai\nDESCRIPTION: Exports the notebook modules to be used externally, facilitating model deployment or sharing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18a_callback.training.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Export the code\nDESCRIPTION: The function `nbdev_export` export the functions define on the notebook.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Inspecting Prediction Shape and Content (Python)\nDESCRIPTION: Displays the shape of the predictions tensor and its first sequence. 'preds' is typically a tensor of generated token IDs; this step is primarily for output verification and debugging.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npreds.shape,preds[0]\n```\n\n----------------------------------------\n\nTITLE: Basic Neptune Integration Example in Python\nDESCRIPTION: Demonstrates the basic usage of the Neptune integration with fastai. It shows how to import the callback, initialize the Neptune project, instantiate a fastai Learner with the NeptuneCallback, create a Neptune experiment, and start the training loop.\nSOURCE: https://github.com/fastai/fastai/blob/main/disabled/70b_callback.neptune.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.callback.neptune import NeptuneCallback\n\nneptune.init('USERNAME/PROJECT_NAME')  # specify project\n\nlearn = Learner(dls, model,\n                cbs=NeptuneCallback()\n                )\n\nneptune.create_experiment()  # start experiment\nlearn.fit_one_cycle(1)\n```\n\n----------------------------------------\n\nTITLE: LabeledBBox Class Python\nDESCRIPTION: The `LabeledBBox` class is a subclass of `L` that holds a list of bounding boxes and their corresponding labels. It utilizes the `TensorBBox` class to hold the bounding box data. It includes a `show` method to display each bounding box with its label on a provided context. The dependencies include the `TensorBBox` class, fastai's `L` class and `add_props` function. The output is a visualization of the bounding boxes on the image with text labels.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nclass LabeledBBox(L):\n    \"Basic type for a list of bounding boxes in an image\"\n    def show(self, ctx=None, **kwargs):\n        for b,l in zip(self.bbox, self.lbl):\n            if l != '#na#': ctx = retain_type(b, self.bbox).show(ctx=ctx, text=l)\n        return ctx\n\n    bbox,lbl = add_props(lambda i,self: self[i])\n```\n\n----------------------------------------\n\nTITLE: Testing `n_inp` Parameter in Datasets\nDESCRIPTION: These tests check the effects of the `n_inp` parameter on dataset behavior, ensuring correct setting and retrieval for datasets with different `tfms` configurations. It validates that `n_inp` defaults or is overridden as intended.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_48\n\nLANGUAGE: Python\nCODE:\n```\n#test n_inp\ninp = [0,1,2,3,4]\ndsets = Datasets(inp, tfms=[None])\ntest_eq(dsets.n_inp, 1)\ndsets = Datasets(inp, tfms=[[None],[None],[None]])\ntest_eq(dsets.n_inp, 2)\ndsets = Datasets(inp, tfms=[[None],[None],[None]], n_inp=1)\ntest_eq(dsets.n_inp, 1)\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoader for Test Images - Python\nDESCRIPTION: Creates a test-only DataLoader (no type transforms, batch size 64) for prediction on the test image set. Input: list of test image files. Output: DataLoader suitable for fastai's get_preds. Prerequisite: trained learner and loaded test images.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ndl = learn.dls.test_dl(test_items, rm_type_tfms=1, bs=64)\n```\n\n----------------------------------------\n\nTITLE: Implementing TestOneBatch for Batch-level State Checking in fastai Learner (Python)\nDESCRIPTION: TestOneBatch is a custom callback derived from VerboseCallback. It performs step-wise assertion checks before and after each training event for an assigned batch, verifying state such as intermediate predictions, parameter gradients, and optimizer steps. Used with a Learner to ensure batch method correctness and event sequencing. Requires fastais Callback class, test utility functions, and model components.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nclass TestOneBatch(VerboseCallback):\n    def __init__(self, xb, yb, i):\n        self.save_xb,self.save_yb,self.i = xb,yb,i\n        self.old_pred,self.old_loss = None,tensor(0.)\n        \n    def before_batch(self):\n        self.old_a,self.old_b = self.a.data.clone(),self.b.data.clone()\n        test_eq(self.iter,    self.i)\n        test_eq(self.save_xb, *self.xb)\n        test_eq(self.save_yb, *self.yb)\n        if hasattr(self.learn, 'pred'): test_eq(self.pred, self.old_pred)\n    \n    def after_pred(self):\n        self.old_pred = self.pred\n        test_eq(self.pred, self.a.data * self.x + self.b.data)\n        test_eq(self.loss, self.old_loss)\n    \n    def after_loss(self):\n        self.old_loss = self.loss\n        test_eq(self.loss, self.loss_func(self.old_pred, self.save_yb))\n        for p in self.parameters(): \n            if not hasattr(p, 'grad') or p.grad is not None: test_eq(p.grad, tensor([0.]))\n    \n    def before_step(self):\n        self.grad_a = (2 * self.x * (self.pred.data - self.y)).mean()\n        self.grad_b = 2 * (self.pred.data - self.y).mean()\n        test_close(self.a.grad.data, self.grad_a)\n        test_close(self.b.grad.data, self.grad_b)\n        test_eq(self.a.data, self.old_a)\n        test_eq(self.b.data, self.old_b)\n        \n    def after_step(self):\n        test_close(self.a.data, self.old_a - self.lr * self.grad_a)\n        test_close(self.b.data, self.old_b - self.lr * self.grad_b)\n        self.old_a,self.old_b = self.a.data.clone(),self.b.data.clone()\n        test_close(self.a.grad.data, self.grad_a)\n        test_close(self.b.grad.data, self.grad_b)\n    \n    def after_batch(self):\n        for p in self.parameters(): test_eq(p.grad, tensor([0.]))\n```\n\n----------------------------------------\n\nTITLE: Subclassing DataLoader for Random Data Generation in Python\nDESCRIPTION: Defines a `RandDL` class by subclassing `DataLoader`. It overrides the `create_item` method to generate a random float between 0 and 1. It introduces probabilistic stopping by returning `stop()` if the random number is 0.95 or greater. The example then demonstrates creating an instance of `RandDL` and iterating through it using `L()`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass RandDL(DataLoader):\n    def create_item(self, s):\n        r = random.random()\n        return r if r<0.95 else stop()\n\nL(RandDL())\n```\n\n----------------------------------------\n\nTITLE: Defining TensorFlowField Subclass and Registering Grid Sample Operation in Python\nDESCRIPTION: Defines TensorFlowField as a subclass of TensorBase representing flow fields for image transformations. Registers PyTorch's F.grid_sample to support operations between TensorImageBase and TensorFlowField, allowing spatial transformations of image tensors with metadata preservation. Includes an example demonstrating grid-sampling producing expected output tensor types.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_59\n\nLANGUAGE: python\nCODE:\n```\nclass TensorFlowField(TensorBase): pass\nTensorImage.register_func(F.grid_sample, TensorImageBase, TensorFlowField)\n```\n\nLANGUAGE: python\nCODE:\n```\nt1 = TensorImage([1.]).view(1,1,1,1)\nt2 = TensorFlowField([1.,1.]).view(1,1,1,2)\ntest_eq_type(F.grid_sample(t1, t2), TensorImage([[[[0.25]]]]))\n```\n\n----------------------------------------\n\nTITLE: Discriminative Learning Rate Example\nDESCRIPTION: This snippet provides an example of using discriminative learning rates, where different layers or parameter groups in the model have different learning rates. It defines a custom `_splitter` function to split the model parameters.  Then, it trains the model using a schedule with `combined_cos` for the learning rate, which likely applies different learning rates to the two parameter groups. This allows for fine-tuning the model with layer-specific learning rates.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#test discriminative lrs\ndef _splitter(m): return [[m.a], [m.b]]\nlearn = synth_learner(splitter=_splitter)\nsched = {'lr': combined_cos(0.5, np.array([1e-4,1e-3]), np.array([1e-3,1e-2]), np.array([1e-5,1e-4]))}\nlearn.fit(1, cbs=ParamScheduler(sched))\n```\n\n----------------------------------------\n\nTITLE: Example Usage: Creating and Scripting Timm Model - Python\nDESCRIPTION: Demonstrates the usage of the `create_timm_model` function to create a ResNet34 model with 1 output class. It then attempts to convert the resulting model to a TorchScript model using `torch.jit.script` to verify its scriptability, asserting that the conversion was successful.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# make sure that timm models can be scripted:\ntst, _ = create_timm_model('resnet34', 1)\nscripted = torch.jit.script(tst)\nassert scripted, \"model could not be converted to TorchScript\"\n```\n\n----------------------------------------\n\nTITLE: Upgrading fastai in Google Colab Environment - Python\nDESCRIPTION: This snippet checks if the script is running in a Google Colab environment by testing for the presence of \"/content\", and if so, it upgrades fastai to the latest version using pip. It should be run in an interactive Python environment and requires internet access and pip. No parameters are required, and the expected output is an updated fastai installation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Creating DataBlock with multiple input blocks\nDESCRIPTION: Shows how to create a DataBlock with multiple input blocks by specifying n_inp parameter to indicate how many blocks should be considered inputs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nmnist = DataBlock((ImageBlock, ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(),\n                   get_y=parent_label)\ndsets = mnist.datasets(untar_data(URLs.MNIST_TINY))\ntest_eq(mnist.n_inp, 2)\ntest_eq(len(dsets.train[0]), 3)\n```\n\n----------------------------------------\n\nTITLE: Defining a fastai DataBlock for MNIST in Python\nDESCRIPTION: Configures a fastai `DataBlock` for loading and processing MNIST data. It specifies `ImageBlock(cls=PILImageBW)` for black-and-white image inputs and `CategoryBlock` for categorical outputs. `get_items=get_image_files` retrieves image file paths, `splitter=GrandparentSplitter()` splits data based on the grandparent directory (e.g., 'training' vs 'testing'), and `get_y=parent_label` extracts labels from the parent directory name.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_lightning.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmnist = DataBlock(blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), \n                  get_items=get_image_files, \n                  splitter=GrandparentSplitter(),\n                  get_y=parent_label)\n```\n\n----------------------------------------\n\nTITLE: Example: Download MNIST_SAMPLE using untar_data\nDESCRIPTION: This snippet shows how to download the MNIST_SAMPLE dataset using the `untar_data` function. The data is downloaded and extracted to the default location specified by the fastai configuration (typically `~/.fastai/data`).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/04_data.external.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nuntar_data(URLs.MNIST_SAMPLE)\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders from Folder\nDESCRIPTION: This creates `DataLoaders` from the IMDb dataset using the `TextDataLoaders.from_folder` method.  It specifies the data path, and sets the `valid` argument to 'test' to indicate the validation set location. This prepares the data for training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\n```\n\n----------------------------------------\n\nTITLE: Creating DataBlock with customized input and target configuration\nDESCRIPTION: Shows how to create a DataBlock with a specific number of inputs and target getter functions, including the use of Pipeline for transforms.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nmnist = DataBlock((ImageBlock, ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(),\n                  n_inp=1,\n                  get_y=[noop, Pipeline([noop, parent_label])])\ndsets = mnist.datasets(untar_data(URLs.MNIST_TINY))\ntest_eq(len(dsets.train[0]), 3)\n```\n\n----------------------------------------\n\nTITLE: Example: Show batch from WeightedDL in fastai Python\nDESCRIPTION: Calls the `show_batch()` method on the created weighted dataloaders (`dls`). This is used to visualize a sample batch from the dataloader, confirming that the data loading pipeline is working as expected.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14a_callback.data.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndls.show_batch() # if len(wgts) != 8, this will fail\"\n```\n\n----------------------------------------\n\nTITLE: Logging Model to Wandb\nDESCRIPTION: The `log_model` function logs a model file to Weights & Biases (W&B). It checks if `wandb.init` has been called and raises an error if it hasn't. It creates a W&B artifact for the model, reads the model file's bytes, and logs the artifact. The `metadata` dictionary is formatted using `_format_metadata` before artifact creation. It names the artifact `run-{wandb.run.id}-model` if no name is provided.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef log_model(path, name=None, metadata={}, description='trained model'):\n    \"Log model file\"\n    if wandb.run is None:\n        raise ValueError('You must call wandb.init() before log_model()')\n    path = Path(path)\n    if not path.is_file():\n        raise f'path must be a valid file: {path}'\n    name = ifnone(name, f'run-{wandb.run.id}-model')\n    _format_metadata(metadata)    \n    artifact_model = wandb.Artifact(name=name, type='model', metadata=metadata, description=description)\n    with artifact_model.new_file(str(Path(name).with_suffix(\".pth\")), mode='wb') as fa:\n        fa.write(path.read_bytes())\n    wandb.run.log_artifact(artifact_model)\n```\n\n----------------------------------------\n\nTITLE: Create Classifier DataBlock\nDESCRIPTION: Creates a `DataBlock` for text classification. It specifies the text and category blocks, the data source (tokenized files), the getter functions for x and y, the splitter, and the vocabulary from the language model. `GrandparentSplitter` splits the data based on the parent folder (train/test).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimdb_clas = DataBlock(blocks=(TextBlock.from_folder(path, vocab=dbunch_lm.vocab),CategoryBlock),\n                      get_x=read_tokenized_file,\n                      get_y = parent_label,\n                      get_items=partial(get_text_files, folders=['train', 'test']),\n                      splitter=GrandparentSplitter(valid_name='test'))\n\ndbunch_clas = imdb_clas.dataloaders(path, path=path, bs=bs, seq_len=80)\n```\n\n----------------------------------------\n\nTITLE: Implementing show_batch function for displaying input and target samples in fastai\nDESCRIPTION: A type-dispatched function that shows decoded samples from a batch. It handles displaying inputs and targets up to a maximum number, with context objects for visualization. It can process samples that have their own show method or iterate through items if needed.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dispatch\ndef show_batch(\n    x, # Input(s) in the batch\n    y, # Target(s) in the batch\n    samples, # List of (`x`, `y`) pairs of length `max_n`\n    ctxs=None, # List of `ctx` objects to show data. Could be a matplotlib axis, DataFrame, etc.\n    max_n=9, # Maximum number of `samples` to show\n    **kwargs\n):\n    \"Show `max_n` input(s) and target(s) from the batch.\"\n    if ctxs is None: ctxs = Inf.nones\n    if hasattr(samples[0], 'show'):\n        ctxs = [s.show(ctx=c, **kwargs) for s,c,_ in zip(samples,ctxs,range(max_n))]\n    else:\n        for i in range_of(samples[0]):\n            ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]\n    return ctxs\n```\n\n----------------------------------------\n\nTITLE: Removing Multiple Callbacks from Learner\nDESCRIPTION: This snippet shows removing a range of callbacks using `remove_cbs`, where `cbs` are a list of callbacks to remove from the Learner.  The example utilizes slicing to remove a subset of the callbacks. The assertions confirm that the correct callbacks have been removed and that the `Learner` is updated.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner()\nlearn.add_cbs([TestTrainEvalCallback() for _ in range(3)])\ncb = learn.cbs[1]\nlearn.remove_cbs(learn.cbs[1:])\ntest_eq(len(learn.cbs), 1)\n```\n\n----------------------------------------\n\nTITLE: Helper Functions for Encoding and Decoding Categorical Columns in pandas (Python)\nDESCRIPTION: The _apply_cats function converts columns to categorical codes by replacing values with category indices, adding offset for missing values. It handles columns which are already categorical and those that are not. The _decode_cats function reverses this operation, mapping indices back to category labels using stored vocabularies. These helpers support encoding/decoding categorical features during tabular data transformations and require pandas with CategoricalDtype support.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef _apply_cats (voc, add, c):\n    if not (hasattr(c, 'dtype') and isinstance(c.dtype, CategoricalDtype)):\n        return pd.Categorical(c, categories=voc[c.name][add:]).codes+add\n    return c.cat.codes+add #if is_categorical_dtype(c) else c.map(voc[c.name].o2i)\ndef _decode_cats(voc, c): return c.map(dict(enumerate(voc[c.name].items)))\n```\n\n----------------------------------------\n\nTITLE: Create Language Model DataBlock\nDESCRIPTION: Creates a `DataBlock` for language model training, specifying the text block, data source (DataFrame column), and splitter. This allows for more control over data loading and preprocessing.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimdb_lm = DataBlock(blocks=(TextBlock.from_df('text', is_lm=True),),\n                    get_x=ColReader('text'),\n                    splitter=RandomSplitter())\n\ndbunch_lm = imdb_lm.dataloaders(df)\n```\n\n----------------------------------------\n\nTITLE: Importing Core FastAI Modules for Data and Model Utilities\nDESCRIPTION: This snippet imports fundamental modules from FastAI, including basic core functionalities, callbacks for progress tracking, text data handling, tabular data processing, model parameter hooks, and model checkpointing callbacks. It sets up the environment for developing machine learning models using FastAI.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.basics import *\nfrom fastai.callback.progress import *\nfrom fastai.text.data import TensorText\nfrom fastai.tabular.all import TabularDataLoaders, Tabular\nfrom fastai.callback.hook import total_params\nfrom fastai.callback.tracker import SaveModelCallback\n```\n\n----------------------------------------\n\nTITLE: Unfreezing Model for Final Fine-tuning - Python\nDESCRIPTION: Unfreezes the entire neural network for final round of fine-tuning on high-resolution data. Called before a last set of learning rate finder and full training cycle.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nlearn.unfreeze()\n```\n\n----------------------------------------\n\nTITLE: AWD-LSTM Language Model Hyperparameter Configuration in Python\nDESCRIPTION: Defines a dictionary containing hyperparameters for configuring an AWD-LSTM language model architecture. The parameters include embedding size, hidden layer size, number of layers, padding token index, bidirectionality flag, and multiple dropout probabilities (output, hidden, input, embedding, weight), plus flags for weight tying and output bias usage. This configuration dictionary serves as a template for consistent model initialization and training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nawd_lstm_lm_config = dict(emb_sz=400, n_hid=1152, n_layers=3, pad_token=1, bidir=False, output_p=0.1,\n                          hidden_p=0.15, input_p=0.25, embed_p=0.02, weight_p=0.2, tie_weights=True, out_bias=True)\n```\n\n----------------------------------------\n\nTITLE: Merge Layer for Skip Connections in Python\nDESCRIPTION: This layer is designed to be used within a `SequentialEx` block to implement skip connections. It accesses the original input to the `SequentialEx` block stored in the `orig` attribute of the current input tensor (`x`). It merges the current tensor (`x`) with the original input (`x.orig`) by either adding them (default) or concatenating them along dimension 1 if `dense` is set to `True`. This is commonly used in ResNet-like architectures.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass MergeLayer(Module):\n    \"Merge a shortcut with the result of the module by adding them or concatenating them if `dense=True`.\"\n    def __init__(self, dense:bool=False): self.dense=dense\n    def forward(self, x): return torch.cat([x,x.orig], dim=1) if self.dense else (x+x.orig)\n```\n\n----------------------------------------\n\nTITLE: Defining Flatten Layer and Simple CNN Model Architecture in PyTorch\nDESCRIPTION: Defines a custom Flatten module to reshape tensors before feeding into fully connected layers. Then, constructs a basic CNN model as an nn.Sequential, including convolutional layers, ReLU activations, max pooling, dropout, flattening, linear layers, and LogSoftmax output. This model mimics fastai's typical head structure for classification and serves as the network architecture for training the MNIST dataset.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_pytorch_verbose.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom torch import nn\n\nclass Flatten(nn.Module):\n    \"Flattens an input\"\n    def forward(self, x): return x.view(x.size(0), -1)\n```\n\nLANGUAGE: python\nCODE:\n```\nclass Net(nn.Sequential):\n    def __init__(self):\n        super().__init__(\n            nn.Conv2d(1, 32, 3, 1), nn.ReLU(),\n            nn.Conv2d(32, 64, 3, 1), \n            # A head to the model\n            nn.MaxPool2d(2), nn.Dropout2d(0.25),\n            Flatten(), nn.Linear(9216, 128), nn.ReLU(),\n            nn.Dropout2d(0.5), nn.Linear(128, 10), nn.LogSoftmax(dim=1)\n        )\n```\n\n----------------------------------------\n\nTITLE: Example Usage and Output Shape Validation of get_language_model in Python\nDESCRIPTION: Constructs a language model using AWD_LSTM architecture and a modified configuration with smaller hidden size. Generates a random input tensor and verifies the shapes of the model's outputs correspond to expected vocabulary logits and hidden features. Also confirms weight tying between decoder and encoder embeddings. This example illustrates practical usage and correctness checks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nconfig = awd_lstm_lm_config.copy()\nconfig.update({'n_hid':10, 'emb_sz':20})\n\ntst = get_language_model(AWD_LSTM, 100, config=config)\nx = torch.randint(0, 100, (10,5))\ny = tst(x)\ntest_eq(y[0].shape, [10, 5, 100])\ntest_eq(y[1].shape, [10, 5, 20])\ntest_eq(y[2].shape, [10, 5, 20])\ntest_eq(tst[1].decoder.weight, tst[0].encoder.weight)\n```\n\n----------------------------------------\n\nTITLE: Defining XResNet Architecture Class in Python with Multi-Dimensional Support\nDESCRIPTION: The XResNet class inherits from nn.Sequential to build a ResNet-based CNN with customizable parameters such as block type, expansion factor, layer configuration, input channels, output classes, activation functions, kernel size, stride, and support for 1D/2D/3D inputs. The model constructs a stem of convolution layers, multiple residual blocks using the specified block type, pooling, flattening, dropout, and a final fully connected layer. It validates kernel size to be odd and uses internal methods to assemble blocks and layers. The init_cnn function is called to initialize weights.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/11_vision.models.xresnet.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass XResNet(nn.Sequential):\n    @delegates(ResBlock)\n    def __init__(self, block, expansion, layers, p=0.0, c_in=3, n_out=1000, stem_szs=(32,32,64),\n                 widen=1.0, sa=False, act_cls=defaults.activation, ndim=2, ks=3, stride=2, **kwargs):\n        store_attr('block,expansion,act_cls,ndim,ks')\n        if ks % 2 == 0: raise Exception('kernel size has to be odd!')\n        stem_szs = [c_in, *stem_szs]\n        stem = [ConvLayer(stem_szs[i], stem_szs[i+1], ks=ks, stride=stride if i==0 else 1, \n                          act_cls=act_cls, ndim=ndim)\n                for i in range(3)]\n\n        block_szs = [int(o*widen) for o in [64,128,256,512] +[256]*(len(layers)-4)]\n        block_szs = [64//expansion] + block_szs\n        blocks    = self._make_blocks(layers, block_szs, sa, stride, **kwargs)\n\n        super().__init__(\n            *stem, MaxPool(ks=ks, stride=stride, padding=ks//2, ndim=ndim),\n            *blocks,\n            AdaptiveAvgPool(sz=1, ndim=ndim), Flatten(), nn.Dropout(p),\n            nn.Linear(block_szs[-1]*expansion, n_out),\n        )\n        init_cnn(self)\n\n    def _make_blocks(self, layers, block_szs, sa, stride, **kwargs):\n        return [self._make_layer(ni=block_szs[i], nf=block_szs[i+1], blocks=l,\n                                 stride=1 if i==0 else stride, sa=sa and i==len(layers)-4, **kwargs)\n                for i,l in enumerate(layers)]\n\n    def _make_layer(self, ni, nf, blocks, stride, sa, **kwargs):\n        return nn.Sequential(\n            *[self.block(self.expansion, ni if i==0 else nf, nf, stride=stride if i==0 else 1,\n                      sa=sa and i==(blocks-1), act_cls=self.act_cls, ndim=self.ndim, ks=self.ks, **kwargs)\n              for i in range(blocks)])\n```\n\n----------------------------------------\n\nTITLE: Loading Model and Optimizer State\nDESCRIPTION: This code snippet demonstrates the loading of a model and optimizer state from a saved file using the `load` method of a `Learner` object. The `load` method, also patched, restores the model and optimizer state from the specified file.  It can use a specified `device` parameter to load the model on a device that's different from where it was saved.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\n@patch\n@delegates(load_model)\ndef load(self:Learner, file, device=None, **kwargs):\n    \"Load model and optimizer state (if `with_opt`) from `self.path/self.model_dir/file` using `device`\"\n    if device is None and hasattr(self.dls, 'device'): device = self.dls.device\n    if self.opt is None: self.create_opt()\n    file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n    distrib_barrier()\n    load_model(file, self.model, self.opt, device=device, **kwargs)\n    return self\n```\n\n----------------------------------------\n\nTITLE: Defining Point and Bounding Box TransformBlocks with Padding in fastai Vision Using Python\nDESCRIPTION: Defines `PointBlock` and `BBoxBlock` TransformBlocks specialized for handling points and bounding boxes respectively in vision datasets. Both utilize `PointScaler` as an item transform to standardize coordinates. `BBoxBlock` also specifies a data loader keyword argument to use `bb_pad` for padding bounding boxes to uniform sizes across batches. These blocks simplify integrating object localization data into fastai's data block API with built-in tensor transformations and padding for batching.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/08_vision.data.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nPointBlock = TransformBlock(type_tfms=TensorPoint.create, item_tfms=PointScaler)\nBBoxBlock = TransformBlock(type_tfms=TensorBBox.create, item_tfms=PointScaler, dls_kwargs = {'before_batch': bb_pad})\n\nPointBlock.__doc__ = \"A `TransformBlock` for points in an image\"\nBBoxBlock.__doc__  = \"A `TransformBlock` for bounding boxes in an image\"\n```\n\n----------------------------------------\n\nTITLE: Accessing the First Layer in a PyTorch Model with Parent Reference in Python\nDESCRIPTION: Function '_get_first_layer' traverses nested attributes derived from named parameters to reach and return the first parameter layer in a model along with its parent module and attribute name. This is mainly used for modifications of initial convolution layers in pretrained vision models.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef _get_first_layer(m):\n    \"Access first layer of a model\"\n    c,p,n = m,None,None  # child, parent, name\n    for n in next(m.named_parameters())[0].split('.')[:-1]:\n        p,c=c,getattr(c,n)\n    return c,p,n\n```\n\n----------------------------------------\n\nTITLE: Processing TensorText with Category Data for Wandb\nDESCRIPTION: This `wandb_process` function processes `TensorText` data with `TensorCategory` or `TensorMultiCategory` labels for logging to Weights & Biases (W&B). It creates a W&B table with columns for the text, target, and prediction. It adds data to the table using the sample text, ground truth labels, and model predictions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n@dispatch\ndef wandb_process(x:TensorText, y:TensorCategory|TensorMultiCategory, samples, outs, preds):\n    data = [[s[0], s[1], o[0]] for s,o in zip(samples,outs)]\n    return {\"Prediction_Samples\": wandb.Table(data=data, columns=[\"Text\", \"Target\", \"Prediction\"])}\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Tabular Processing\nDESCRIPTION: This snippet imports necessary libraries for tabular data processing. It imports core fastai modules for data handling and tabular operations, and attempts to import cudf and nvcategory for GPU-accelerated dataframes. If rapids isn't installed, it prints instructions for setting it up.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom fastai.torch_basics import *\nfrom fastai.data.all import *\nfrom fastai.tabular.core import *\ntry: import cudf,nvcategory\nexcept: print(\"This requires rapids, see https://rapids.ai/ for installation details\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Movielens Data\nDESCRIPTION: This code downloads and extracts the Movielens 100k dataset from a URL. The `untar_data` function automatically downloads and decompresses the dataset into a local directory, preparing it for use in the collaborative filtering task.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.ML_100k)\n```\n\n----------------------------------------\n\nTITLE: Defining TensorImage and TensorImageBW Subclasses in Python\nDESCRIPTION: Defines TensorImage as a direct subclass of TensorImageBase without additional changes. Defines TensorImageBW as a black-and-white image tensor subclass, overriding _show_args with ArrayImageBW's default display arguments, enabling specialized handling and visualization of grayscale images.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nclass TensorImage(TensorImageBase): pass\n```\n\nLANGUAGE: python\nCODE:\n```\nclass TensorImageBW(TensorImage): _show_args = ArrayImageBW._show_args\n```\n\n----------------------------------------\n\nTITLE: SE-enhanced ResNeXt Block Creation in Python\nDESCRIPTION: This function provides a wrapper for `ResBlock` configured for a Squeeze and Excitation-enhanced ResNeXt block. It sets the `groups` and `reduction` parameters and calculates the intermediate channel size `nh2` based on the output feature map size (`nf`), `base_width`, and `groups`, according to the ResNeXt architecture principles. It forwards other arguments to the `ResBlock` constructor.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_72\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef SEResNeXtBlock(expansion, ni, nf, groups=32, reduction=16, stride=1, base_width=4, **kwargs):\n    w = math.floor(nf * (base_width / 64)) * groups\n    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, reduction=reduction, nh2=w, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Import Matplotlib Modules Python\nDESCRIPTION: This snippet imports the `patches` and `patheffects` modules from the `matplotlib` library.  These modules are used for drawing shapes and applying effects, respectively. This import is a prerequisite for drawing bounding boxes and adding outlines.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom matplotlib import patches, patheffects\n```\n\n----------------------------------------\n\nTITLE: Testing all Batch-level Events in fastai Learner.one_batch (Python)\nDESCRIPTION: This snippet attaches TestOneBatch to a synthetic Learner, prepares the model, and verifies stdout outputs the correct sequence of training events for a single batch using test_stdout. It ensures batch events are consistent for repeated calls. Relies on fastai test utility routines and prior construction of Learner and batch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nlearn = synth_learner()\nb = learn.dls.one_batch()\nlearn = synth_learner(cbs=TestOneBatch(*b, 42), lr=1e-2)\n#Remove train/eval\nlearn.cbs = learn.cbs[1:]\n#Setup\nlearn.loss,learn.training = tensor(0.),True\nlearn.opt = SGD(learn.parameters(), lr=learn.lr)\nlearn.model.train()\nbatch_events = ['before_batch', 'after_pred', 'after_loss', 'before_backward', 'after_backward', 'before_step', 'after_step', 'after_batch']\ntest_stdout(lambda: learn.one_batch(42, b), '\\n'.join(batch_events))\ntest_stdout(lambda: learn.one_batch(42, b), '\\n'.join(batch_events)) #Check it works for a second batch\n```\n\n----------------------------------------\n\nTITLE: Logging Dataset to Wandb\nDESCRIPTION: The `log_dataset` function logs a dataset folder to Weights & Biases (W&B). It checks if `wandb.init` has been called and raises an error if it hasn't. It creates a W&B artifact for the dataset, adds the directory contents (excluding the 'models' folder), and uses the artifact in the current W&B run. The `metadata` dictionary is formatted using `_format_metadata` before artifact creation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef log_dataset(path, name=None, metadata={}, description='raw dataset'):\n    \"Log dataset folder\"\n    # Check if wandb.init has been called in case datasets are logged manually\n    if wandb.run is None:\n        raise ValueError('You must call wandb.init() before log_dataset()')\n    path = Path(path)\n    if not path.is_dir():\n        raise f'path must be a valid directory: {path}'\n    name = ifnone(name, path.name)\n    _format_metadata(metadata)\n    artifact_dataset = wandb.Artifact(name=name, type='dataset', metadata=metadata, description=description)\n    # log everything except \"models\" folder\n    for p in path.ls():\n        if p.is_dir():\n            if p.name != 'models': artifact_dataset.add_dir(str(p.resolve()), name=p.name)\n        else: artifact_dataset.add_file(str(p.resolve()))\n    wandb.run.use_artifact(artifact_dataset)\n```\n\n----------------------------------------\n\nTITLE: Installing or Upgrading fastai Package in Python\nDESCRIPTION: A snippet intended to upgrade the fastai library on Google Colab if running in that environment. It checks whether the '/content' directory exists (indicating Colab) and executes a pip install command with upgrade and quiet flags.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Installing or Upgrading FastAI on Colab\nDESCRIPTION: This snippet checks if the '/content' directory exists and, if so, upgrades the FastAI library to ensure latest features and fixes are available for subsequent code. It uses pip within a shell command to perform the installation, with output suppressed for a cleaner environment.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai\n```\n\n----------------------------------------\n\nTITLE: Using before_batch Callback\nDESCRIPTION: This snippet demonstrates the application of a `before_batch` callback, which is used to modify data before it is passed to the model. The callback, implemented as a class `TstCallback`, changes the values of `xb` and `yb`.  This callback is useful for data transformations and is attached to the `before_batch` event.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nclass TstCallback(Callback):\n    def before_batch(self):\n        self.learn.xb = self.xb + 1000\n        self.learn.yb = self.yb - 1000\n```\n\n----------------------------------------\n\nTITLE: Initializing Default Callbacks in fastai Learner (Python)\nDESCRIPTION: This snippet ensures that the 'defaults' object contains an attribute 'callbacks' and initializes it to a list containing TrainEvalCallback if it doesn't exist. It is a preparatory step for proper event management within the Learner's training loop. This requires the 'defaults' and 'TrainEvalCallback' objects to already be imported into the environment.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n#|export\nif not hasattr(defaults, 'callbacks'): defaults.callbacks = [TrainEvalCallback]\n```\n\n----------------------------------------\n\nTITLE: Initializing ResNet50 with Random Weights in FastAI\nDESCRIPTION: This snippet shows how to initialize a ResNet50 model without pretrained weights using FastAI's vision API. It demonstrates usage with the new TorchVision multi-weight API, explicitly setting 'pretrained=False' and 'weights=None'. Dependencies include torchvision and fastai. The purpose is to create an untrained ResNet50 model for further customization or training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nvision_learner(models.resnet50, pretrained=False, weights=None, ...)\n```\n\n----------------------------------------\n\nTITLE: Setting Default Learning Rate in Fastai Defaults (Python)\nDESCRIPTION: Assigns the default learning rate (1e-3) used throughout fastai training utilities. Affects automatic hyperparameter selection during learner or optimizer creation if not otherwise specified. Requires the fastai defaults object.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndefaults.lr = 1e-3\n```\n\n----------------------------------------\n\nTITLE: Testing Save and Load Functionality\nDESCRIPTION: This code provides a basic test for the `save` and `load` functions. It creates a `Learner`, fits it for one epoch, saves the model and optimizer, and then loads it back into another `Learner` instance. It asserts that the loaded model's state matches the original model's state, including the optimizer state using `test_eq`.  Uses `tempfile.TemporaryDirectory` to provide a temporary directory for testing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nwith tempfile.TemporaryDirectory() as d:\n    learn = synth_learner(path=d)\n    learn.fit(1)\n    \n    #Test save created a file\n    learn.save('tmp')\n    assert (Path(d)/'models/tmp.pth').exists()\n    \n    #Test load did load the model\n    learn1 = synth_learner(path=d)\n    learn1 = learn1.load('tmp')\n    test_eq(learn.a, learn1.a)\n    test_eq(learn.b, learn1.b)\n    test_eq(learn.opt.state_dict(), learn1.opt.state_dict())\n```\n\n----------------------------------------\n\nTITLE: Decoding Generated Token IDs back to Text in Python (HuggingFace)\nDESCRIPTION: Decodes the sequence of generated token IDs (stored in `preds[0]`) back into a human-readable string using the HuggingFace `tokenizer`'s `decode` method. The tensor containing the IDs is first moved from the GPU to the CPU (`cpu()`) and converted to a NumPy array (`numpy()`) as required by the `decode` method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ntokenizer.decode(preds[0].cpu().numpy())\n```\n\n----------------------------------------\n\nTITLE: Importing Documentation Tools\nDESCRIPTION: Imports tools from nbdev for generating documentation in the notebook.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/45_collab.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Testing FillMissing Process Attribute - Python\nDESCRIPTION: This code tests whether the `FillMissing` process has an attribute named 'to' within a `TabularPandas` object, ensuring that there is no persistent storage of the original `Tabular` object within the processor itself.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nfill = FillMissing()\ndf = pd.DataFrame({'a':[0,1,np.nan,1,2,3,4], 'b': [0,1,2,3,4,5,6]})\nto = TabularPandas(df, fill, cont_names=['a', 'b'])\ntest_eq(hasattr(to.procs.fill_missing, 'to'), False)\n```\n\n----------------------------------------\n\nTITLE: BnFreeze - Freezing BatchNorm Statistics in Transfer Learning\nDESCRIPTION: Defines a callback that sets all non-trainable BatchNorm layers to eval mode to freeze their running mean and variance during training. Useful for transfer learning scenarios where BatchNorm stats should not change.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18a_callback.training.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nbn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n\ndef set_bn_eval(m:nn.Module, use_eval=True)->None:\n    \"Set bn layers in eval mode for all recursive children of `m`.\"\n    for l in m.children():\n        if isinstance(l, bn_types) and not next(l.parameters()).requires_grad:\n            if use_eval: l.eval()\n            else:        l.train()\n        set_bn_eval(l)\n\nclass BnFreeze(Callback):\n    run_after=TrainEvalCallback\n    \"Freeze moving average statistics in all non-trainable batchnorm layers.\"\n    def before_train(self):\n        set_bn_eval(self.model)\n```\n\n----------------------------------------\n\nTITLE: Implementing PixelShuffle_ICNR Layer\nDESCRIPTION: Implements `PixelShuffle_ICNR`, a sequential layer using `nn.PixelShuffle` for upsampling with ICNR initialization for the preceding convolutional layer. It also adds optional blur (using a replication padding and average pooling) to reduce checkerboard artifacts. It's designed for super-resolution applications. Dependencies: `torch`, `fastai` (for ConvLayer, NormType, etc.)\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_52\n\nLANGUAGE: Python\nCODE:\n```\nclass PixelShuffle_ICNR(nn.Sequential):\n    \"Upsample by `scale` from `ni` filters to `nf` (default `ni`), using `nn.PixelShuffle`.\"\n    def __init__(self, ni, nf=None, scale=2, blur=False, norm_type=NormType.Weight, act_cls=defaults.activation):\n        super().__init__()\n        nf = ifnone(nf, ni)\n        layers = [ConvLayer(ni, nf*(scale**2), ks=1, norm_type=norm_type, act_cls=act_cls, bias_std=0),\n                  nn.PixelShuffle(scale)]\n        if norm_type == NormType.Weight:\n            layers[0][0].weight_v.data.copy_(icnr_init(layers[0][0].weight_v.data))\n            layers[0][0].weight_g.data.copy_(((layers[0][0].weight_v.data**2).sum(dim=[1,2,3])**0.5)[:,None,None,None])\n        else:\n            layers[0][0].weight.data.copy_(icnr_init(layers[0][0].weight.data))\n        if blur: layers += [nn.ReplicationPad2d((1,0,1,0)), nn.AvgPool2d(2, stride=1)]\n        super().__init__(*layers)\n```\n\n----------------------------------------\n\nTITLE: Defining a Categorical Transform for TfmdLists in fastai (Python)\nDESCRIPTION: Defines a `_Cat` transform designed for handling categorical data within a `TfmdLists` pipeline. Its `setups` method automatically builds a vocabulary (`vocab`) and an object-to-index mapping (`o2i`) from the unique items passed to it. The `encodes` method converts input items to their integer indices based on `o2i`, while `decodes` converts indices back to `TitledStr` representations using the `vocab`. A helper function `_lbl` is included to simulate extracting labels (e.g., from filenames).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nclass _Cat(Transform):\n    order = 1\n    def encodes(self, o):    return int(self.o2i[o])\n    def decodes(self, o):    return TitledStr(self.vocab[o])\n    def setups(self, items): self.vocab,self.o2i = uniqueify(L(items), sort=True, bidir=True)\ntcat = _Cat()\n\ndef _lbl(o): return TitledStr(o.split('_')[0])\n```\n\n----------------------------------------\n\nTITLE: Showing a Batch from Main DataLoaders\nDESCRIPTION: This displays a batch of images from the main data loaders `dls`. `figsize` sets the size of the plot. This allows for inspecting the processed data that will be used for training.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ndls.show_batch(figsize=(5,5))\n```\n\n----------------------------------------\n\nTITLE: Hook Context Manager Example\nDESCRIPTION: This snippet demonstrates using the `Hook` class as a context manager. It registers a forward hook using the `Hook` class and the `example_forward_hook` function within a `with` statement, then uses `test_stdout` to ensure that the forward hook is executed during a forward pass. After exiting the `with` statement, the hook is automatically removed, and it verifies that the hook is no longer executed during a subsequent forward pass.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntst_model = nn.Linear(5,10)\nx = torch.randn(4,5)\ny = tst_model(x)\nwith Hook(tst_model, example_forward_hook) as h:\n    test_stdout(lambda: tst_model(x), f\"{tst_model} ({x},) {y.detach()}\")\ntest_stdout(lambda: tst_model(x), \"\")\n```\n\n----------------------------------------\n\nTITLE: Creating Sequential Module with Lambda in Python\nDESCRIPTION: This function creates a PyTorch `nn.Sequential` module, automatically wrapping any input functions or callable objects that are not already `nn.Module` instances into fastai's `Lambda` layer. This allows mixing functions and modules easily within a sequential container. It accepts a variable number of arguments or an OrderedDict.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef sequential(*args):\n    \"Create an `nn.Sequential`, wrapping items with `Lambda` if needed\"\n    if len(args) != 1 or not isinstance(args[0], OrderedDict):\n        args = list(args)\n        for i,o in enumerate(args):\n            if not isinstance(o,nn.Module): args[i] = Lambda(o)\n    return nn.Sequential(*args)\n```\n\n----------------------------------------\n\nTITLE: Reading Full DataFrames - Python\nDESCRIPTION: Loads the complete pre-processed training and test DataFrames from pickle files.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ntrain_df = pd.read_pickle(path/'train_clean')\ntest_df = pd.read_pickle(path/'test_clean')\n```\n\n----------------------------------------\n\nTITLE: Using BnFreeze to Stabilize BatchNorms during Transfer Learning\nDESCRIPTION: Demonstrates how to use the BnFreeze callback to ensure BatchNorm stats do not update during training, maintaining consistent feature extractor behavior across training sessions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18a_callback.training.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n#|slow\npath = untar_data(URLs.MNIST_TINY)\ndls  = ImageDataLoaders.from_folder(path, valid_pct=0.2)\n```\n\nLANGUAGE: Python\nCODE:\n```\n#|slow\nlearn1 = vision_learner(deepcopy(dls), resnet18, pretrained=True, train_bn=False)\n```\n\nLANGUAGE: Python\nCODE:\n```\n#|slow\nm = learn1.model[0][1].running_mean.clone()\n```\n\nLANGUAGE: Python\nCODE:\n```\n#|slow\nlearn1.fit(1, lr=0.02)\ntest_ne(to_detach(learn1.model[0][1].running_mean), m)\n```\n\nLANGUAGE: Python\nCODE:\n```\n#|slow\nlearn1 = vision_learner(deepcopy(dls), resnet18, pretrained=True, train_bn=False, cbs=BnFreeze)\n\nm = learn1.model[0][1].running_mean.detach().clone()\nlearn1.fit(1, lr=0.02)\ntest_eq(to_detach(learn1.model[0][1].running_mean), m)\n```\n\n----------------------------------------\n\nTITLE: Simple CNN Model Creation in Python\nDESCRIPTION: This class creates a basic Convolutional Neural Network model based on `nn.Sequential`. It consists of a series of `ConvLayer` instances specified by the input `filters` list, followed by a `PoolFlatten` layer. Optional arguments allow customization of `kernel_szs`, `strides`, and whether Batch Normalization (`bn`) is included after the convolutional layers (except the last one). The convolutional layers follow a conv-relu-(batchnorm if bn) structure.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_63\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass SimpleCNN(nn.Sequential):\n    \"Create a simple CNN with `filters`.\"\n    def __init__(self, filters, kernel_szs=None, strides=None, bn=True):\n        nl = len(filters)-1\n        kernel_szs = ifnone(kernel_szs, [3]*nl)\n        strides    = ifnone(strides   , [2]*nl)\n        layers = [ConvLayer(filters[i], filters[i+1], kernel_szs[i], stride=strides[i],\n                  norm_type=(NormType.Batch if bn and i<nl-1 else None)) for i in range(nl)]\n        layers.append(PoolFlatten())\n        super().__init__(*layers)\n```\n\n----------------------------------------\n\nTITLE: Defining Distributed Training Function (Python)\nDESCRIPTION: Modifies the `train` function to incorporate distributed training. The `fine_tune` call is wrapped within the `learn.distrib_ctx` context manager. `sync_bn` is explicitly set to False for compatibility, and `in_notebook` is set to True to signal execution within a notebook environment. The trained model is exported using `learn.export` after training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20b_tutorial.distributed.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef train(path):\n    dls = ImageDataLoaders.from_name_func(\n        path, get_image_files(path), valid_pct=0.2,\n        label_func=get_y, item_tfms=Resize(224))\n    learn = vision_learner(dls, resnet34, metrics=error_rate).to_fp16()\n    with learn.distrib_ctx(sync_bn=False, in_notebook=True):\n        learn.fine_tune(1)\n    learn.export(\"pets\")\n```\n\n----------------------------------------\n\nTITLE: Viewing Google Trends Data Before Processing\nDESCRIPTION: Displays the first few rows of the googletrend dataframe to understand its structure before adding date features.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ngoogletrend.head()\n```\n\n----------------------------------------\n\nTITLE: Patching _to_detach for DistributedDL - Python\nDESCRIPTION: Overrides or patches the _to_detach method for the DistributedDL (Distributed DataLoader) class. Ensures proper gathering of tensor outputs across distributed processes, setting -100 for tensors corresponding to ranks other than the current one. Dependency: torch, fastai's DistributedDL and to_detach, proper distributed initialization. Expects tensors as input batches, outputs properly gathered and masked tensors suitable for multi-process evaluation or loss computation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n@patch\ndef _to_detach(self:DistributedDL,b,cpu=True,gather=True):\n    b = to_detach(b,cpu,gather)\n    if not gather: return b\n    def _inner(b, cpu, gather):\n        if b.ndim == 0: b=b[None]\n        b = torch.cat([b if i==self.rank else torch.full_like(b,-100) for i in range(self.world_size)])\n        return b if b.ndim > 0 else b.mean()\n    return apply(_inner,b,cpu,gather)\n```\n\n----------------------------------------\n\nTITLE: Showing Batch of Language Modeling Data\nDESCRIPTION: This displays a batch of data from the language model `DataLoaders` (`dls_lm`).  The `max_n=5` argument limits the number of examples shown. This gives an understanding of how text is structured for the language modeling task (predicting the next word).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndls_lm.show_batch(max_n=5)\n```\n\n----------------------------------------\n\nTITLE: Creating a DataLoader with Transform Pipeline\nDESCRIPTION: This code prepares a dataset with range 0-49, applies an `A()` transform, wraps it in a `TfmdDL` with an `NegTfm()` after-item transform, and retrieves a batch. It decodes and shows the batch, demonstrating data loading and transformation workflow.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_57\n\nLANGUAGE: Python\nCODE:\n```\nstart = torch.arange(0,50)\ntds = Datasets(start, [A()])\ntdl = TfmdDL(tds, after_item=NegTfm(), bs=4)\nb = tdl.one_batch()\ntest_eq(tdl.decode_batch(b), ((0,),(1,),(2,),(3,)))\ntest_stdout(tdl.show_batch, \"0\\n1\\n2\\n3\")\n```\n\n----------------------------------------\n\nTITLE: TabularCollab Class Definition\nDESCRIPTION: Defines a specialized TabularPandas class for collaborative filtering with no continuous variables.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/45_collab.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass TabularCollab(TabularPandas):\n    \"Instance of `TabularPandas` suitable for collaborative filtering (with no continuous variable)\"\n    with_cont=False\n```\n\n----------------------------------------\n\nTITLE: Define path for the Movielens 100k dataset\nDESCRIPTION: Defines the path to the Movielens 100k dataset within the fastai configuration's data directory. This assumes the dataset is located at `~/.fastai/data/ml-100k`.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npath=Config().data/'ml-100k'\n```\n\n----------------------------------------\n\nTITLE: Get Environment Variable\nDESCRIPTION: Retrieves the value of an environment variable if it's defined and not empty; otherwise, it returns \"Unknown\". This function is used to capture environment variables such as conda env. This utility helps determine the current environment.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/97_test_utils.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_env(name):\n    \"Return env var value if it's defined and not an empty string, or return Unknown\"\n    res = os.environ.get(name,'')\n    return res if len(res) else \"Unknown\"\n```\n\n----------------------------------------\n\nTITLE: Exporting nbdev Module - fastai - Python\nDESCRIPTION: This code snippet demonstrates how to export a module using `nbdev`. It imports the necessary functions from `nbdev` and then calls `nbdev_export()` to export the module. The `#|hide` directive indicates that this code should be hidden when generating documentation or other outputs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/45_collab.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev import *\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Module: Resize to Original Dimensions (Python)\nDESCRIPTION: A simple PyTorch module designed to resize its input tensor `x` to match the spatial dimensions of `x.orig`, which is expected to hold the original input tensor or a tensor with its shape. It uses interpolation (defaulting to `nearest` mode) if the dimensions do not match, often used as a final layer or within a U-Net architecture to handle size discrepancies.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15a_vision.models.unet.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass ResizeToOrig(Module):\n    \"Merge a shortcut with the result of the module by adding them or concatenating them if `dense=True`.\"\n    def __init__(self, mode='nearest'): self.mode = mode\n    def forward(self, x):\n        if x.orig.shape[-2:] != x.shape[-2:]:\n            x = F.interpolate(x, x.orig.shape[-2:], mode=self.mode)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Testing Master Parameter Creation in fastai Python\nDESCRIPTION: This test snippet demonstrates the usage of `get_master` without flattening. It creates a synthetic learner and retrieves the model and master parameters, verifying that they are organized into parameter groups, contain the same values, but have different data types (FP16 for model, FP32 for master).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#|cuda\nlearn = synth_learner()\nlearn.model = convert_network(nn.Sequential(nn.Linear(1,1), nn.Linear(1,1)), torch.float16).cuda()\nlearn.splitter = lambda m: [list(m[0].parameters()), list(m[1].parameters())]\nlearn.opt = learn.opt_func(learn.splitter(learn.model), learn.lr)\nmodel_p,master_p = get_master(learn.opt)\ntest_eq(len(model_p), 2)   #2 pqrqm groups\ntest_eq(len(master_p), 2)\nfor pg1,pg2 in zip(model_p,master_p):\n    test_eq([p.float() for p in pg1], pg2) #Same values but different types\n    for p in pg1: assert p.dtype == torch.float16\n```\n\n----------------------------------------\n\nTITLE: Creating a Learner for the Refactored Model\nDESCRIPTION: This creates a `Learner` object for the refactored CNN model, with the same loss function and metrics as the first model. This sets up the training process for the new model.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\nlearn = Learner(dls, model, loss_func = nn.CrossEntropyLoss(), metrics=accuracy)\n```\n\n----------------------------------------\n\nTITLE: Text Classifier - Example Usage\nDESCRIPTION: This example creates a text classifier using `get_text_classifier`, sets up an AWD_LSTM, and provides example input to the network, testing the shape of the outputs. It also checks for correct behaviour using padding, and how drop_mult is applied.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nconfig = awd_lstm_clas_config.copy()\nconfig.update({'n_hid':10, 'emb_sz':20})\n\ntst = get_text_classifier(AWD_LSTM, 100, 3, config=config)\nx = torch.randint(2, 100, (10,5))\ny = tst(x)\ntest_eq(y[0].shape, [10, 3])\ntest_eq(y[1].shape, [10, 5, 20])\ntest_eq(y[2].shape, [10, 5, 20])\n```\n\n----------------------------------------\n\nTITLE: Testing DistributedDL to_detach with Custom Partitioning - Python\nDESCRIPTION: Examines the to_detach function on DistributedDLs sharing a single underlying TfmdDL, checking for correct batch detachment and masking using -100 for ranks without data. Uses L (fastai list-like object) for mapping and test_eq for assertion. Dependencies: fastai TfmdDL, DistributedDL, L, torch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\ndl = TfmdDL(list(zip(range(12),range(100,112))), bs=12, num_workers=4)\nres,dls = [],[]\nfor i in range(5): dls.append(DistributedDL(dl, i, 5))\nfor b in zip(*dls):\n    for r in range(5):\n        d=L(dls[r].to_detach(b[r]))\n        test_eq(d.map(lambda x:(x!=-100).sum().item()),(3,3) if r!=4 else (0,0))\n```\n\n----------------------------------------\n\nTITLE: Creating a ResNet34 Model with Conditional Weights\nDESCRIPTION: This snippet creates a ResNet34 model with optional weights depending on torchvision version, then initializes a fastai vision learner with that model, control over pretraining, and dropout. It performs a test to verify the data loaders. Dependencies include fastai, torchvision, and torch. It demonstrates flexible model creation with version-aware weight loading.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\nlearn = vision_learner(dls, models.resnet34, weights=weights, loss_func=CrossEntropyLossFlat(), ps=0.25, concat_pool=False)\ntest_ne(learn.cbs, None)\n```\n\n----------------------------------------\n\nTITLE: Creating SuggestionMethod Class for Accessing Learning Rate Suggestion Algorithms in Python\nDESCRIPTION: This snippet dynamically constructs a class named SuggestionMethod with class attributes corresponding to the implemented learning rate suggestion functions (valley, slide, minimum, steep). This wrapper provides tab-completion and typo-proofing for referencing suggestion methods, improving usability and maintainability.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nmk_class(\"SuggestionMethod\", **{o.__name__.capitalize():o for o in [valley,slide,minimum,steep]},\n         doc=\"All possible suggestion methods as convience attributes to get tab-completion and typo-proofing\")\n```\n\n----------------------------------------\n\nTITLE: Defining fastai SentencePieceTokenizer Class (Python)\nDESCRIPTION: Implements a subword tokenization strategy using the external `sentencepiece` library, wrapped for compatibility with fastai's data processing. It handles training a SentencePiece model on provided text data and applying the trained model for tokenization. It requires the `sentencepiece` library to be installed.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/30_text.core.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass SentencePieceTokenizer():#TODO: pass the special tokens symbol to sp\n    \"SentencePiece tokenizer for `lang`\"\n    def __init__(self, lang='en', special_toks=None, sp_model=None, vocab_sz=None, max_vocab_sz=30000,\n                 model_type='unigram', char_coverage=None, cache_dir='tmp'):\n        try: from sentencepiece import SentencePieceTrainer,SentencePieceProcessor\n        except ImportError:\n            raise Exception('sentencepiece module is missing: run `pip install sentencepiece!=0.1.90,!=0.1.91`')\n        self.sp_model,self.cache_dir = sp_model,Path(cache_dir)\n        self.vocab_sz,self.max_vocab_sz,self.model_type = vocab_sz,max_vocab_sz,model_type\n        self.char_coverage = ifnone(char_coverage, 0.99999 if lang in eu_langs else 0.9998)\n        self.special_toks = ifnone(special_toks, defaults.text_spec_tok)\n        if sp_model is None: self.tok = None\n        else:\n            self.tok = SentencePieceProcessor()\n            self.tok.Load(str(sp_model))\n        os.makedirs(self.cache_dir, exist_ok=True)\n\n    def _get_vocab_sz(self, raw_text_path):\n        cnt = Counter()\n        with open(raw_text_path, 'r') as f:\n            for line in f.readlines():\n                cnt.update(line.split())\n                if len(cnt)//4 > self.max_vocab_sz: return self.max_vocab_sz\n        res = len(cnt)//4\n        while res%8 != 0: res+=1\n        return max(res,29)\n\n    def train(self, raw_text_path):\n        \"Train a sentencepiece tokenizer on `texts` and save it in `path/tmp_dir`\"\n        from sentencepiece import SentencePieceTrainer\n        vocab_sz = self._get_vocab_sz(raw_text_path) if self.vocab_sz is None else self.vocab_sz\n        spec_tokens = [''+s for s in self.special_toks]\n        SentencePieceTrainer.Train(\" \".join([\n            f\"--input={raw_text_path} --vocab_size={vocab_sz} --model_prefix={self.cache_dir/'spm'}\",\n            f\"--character_coverage={self.char_coverage} --model_type={self.model_type}\",\n            f\"--unk_id={len(spec_tokens)} --pad_id=-1 --bos_id=-1 --eos_id=-1 --minloglevel=2\",\n            f\"--user_defined_symbols={','.join(spec_tokens)} --hard_vocab_limit=false\"]))\n        raw_text_path.unlink()\n        return self.cache_dir/'spm.model'\n\n    def setup(self, items, rules=None):\n        from sentencepiece import SentencePieceProcessor\n        if rules is None: rules = []\n        if self.tok is not None: return {'sp_model': self.sp_model}\n        raw_text_path = self.cache_dir/'texts.out'\n        with open(raw_text_path, 'w') as f:\n            for t in progress_bar(maps(*rules, items), total=len(items), leave=False):\n                f.write(f'{t}\\n')\n        sp_model = self.train(raw_text_path)\n        self.tok = SentencePieceProcessor()\n        self.tok.Load(str(sp_model))\n        return {'sp_model': sp_model}\n\n    def __call__(self, items):\n        if self.tok is None: self.setup(items)\n        for t in items: yield self.tok.EncodeAsPieces(t)\n```\n\n----------------------------------------\n\nTITLE: Visualizing an Image and Checking Shape - Python\nDESCRIPTION: Displays the first training image using matplotlib's `imshow` function after reshaping the flat pixel array (784 elements) into a 28x28 grid. The image is shown in grayscale. It also prints the shape of the training data array.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nplt.imshow(x_train[0].reshape((28,28)), cmap=\"gray\")\nx_train.shape\n```\n\n----------------------------------------\n\nTITLE: General Tensor Conversion Function Handling Various Input Types in fastai Python\nDESCRIPTION: The `tensor` function behaves like `torch.as_tensor` but includes enhanced flexibility for input types, such as Python lists, tuples, numbers, numpy ndarrays, and pandas Series/DataFrames. It uses internal helpers as necessary with options for dtype, device placement, gradient requirements, and pinned memory for efficient data transfer to GPUs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@use_kwargs_dict(dtype=None, device=None, requires_grad=False, pin_memory=False)\ndef tensor(x, *rest, **kwargs):\n    \"Like `torch.as_tensor`, but handle lists too, and can pass multiple vector elements directly.\"\n    if len(rest): x = (x,)+rest\n    # There was a Pytorch bug in dataloader using num_workers>0. Haven't confirmed if fixed\n    # if isinstance(x, (tuple,list)) and len(x)==0: return tensor(0)\n    res = (x if isinstance(x, Tensor)\n           else torch.tensor(x, **kwargs) if isinstance(x, (tuple,list,numbers.Number))\n           else _array2tensor(x, **kwargs) if isinstance(x, ndarray)\n           else as_tensor(x.values, **kwargs) if isinstance(x, (pd.Series, pd.DataFrame))\n```\n\n----------------------------------------\n\nTITLE: Configurable ResNet Block in Python\nDESCRIPTION: This class implements a highly configurable ResNet-style block (either basic or bottleneck depending on `expansion`). It includes options for stride, groups (ResNeXt), depthwise convolutions (`dw`), Squeeze and Excitation (`reduction`), Self-Attention (`sa`), different normalization types (including zero-initialized), and activation functions. It constructs both the main convolutional path and the identity shortcut path, handling dimension/stride mismatches with convolutions and/or pooling. The final output is the activation of the sum of the conv path and identity path results.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_70\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass ResBlock(Module):\n    \"Resnet block from `ni` to `nh` with `stride`\"\n    @delegates(ConvLayer.__init__)\n    def __init__(self, expansion, ni, nf, stride=1, groups=1, reduction=None, nh1=None, nh2=None, dw=False, g2=1,\n                 sa=False, sym=False, norm_type=NormType.Batch, act_cls=defaults.activation, ndim=2, ks=3,\n                 pool=AvgPool, pool_first=True, **kwargs):\n        norm2 = (NormType.BatchZero if norm_type==NormType.Batch else\n                 NormType.InstanceZero if norm_type==NormType.Instance else norm_type)\n        if nh2 is None: nh2 = nf\n        if nh1 is None: nh1 = nh2\n        nf,ni = nf*expansion,ni*expansion\n        k0 = dict(norm_type=norm_type, act_cls=act_cls, ndim=ndim, **kwargs)\n        k1 = dict(norm_type=norm2, act_cls=None, ndim=ndim, **kwargs)\n        convpath  = [ConvLayer(ni,  nh2, ks, stride=stride, groups=ni if dw else groups, **k0),\n                     ConvLayer(nh2,  nf, ks, groups=g2, **k1)\n        ] if expansion == 1 else [\n                     ConvLayer(ni,  nh1, 1, **k0),\n                     ConvLayer(nh1, nh2, ks, stride=stride, groups=nh1 if dw else groups, **k0),\n                     ConvLayer(nh2,  nf, 1, groups=g2, **k1)]\n        if reduction: convpath.append(SEModule(nf, reduction=reduction, act_cls=act_cls))\n        if sa: convpath.append(SimpleSelfAttention(nf,ks=1,sym=sym))\n        self.convpath = nn.Sequential(*convpath)\n        idpath = []\n        if ni!=nf: idpath.append(ConvLayer(ni, nf, 1, act_cls=None, ndim=ndim, **kwargs))\n        if stride!=1: idpath.insert((1,0)[pool_first], pool(stride, ndim=ndim, ceil_mode=True))\n        self.idpath = nn.Sequential(*idpath)\n        self.act = defaults.activation(inplace=True) if act_cls is defaults.activation else act_cls()\n\n    def forward(self, x): return self.act(self.convpath(x) + self.idpath(x))\n```\n\n----------------------------------------\n\nTITLE: Checking Model Parameter Shapes - PyTorch/Python\nDESCRIPTION: Retrieves all trainable parameters of the `model` using `model.parameters()`. It then creates a list comprehension to get and print the shape of each parameter tensor.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n[p.shape for p in model.parameters()]\n```\n\n----------------------------------------\n\nTITLE: Defining decode_at Helper Function in fastai (Python)\nDESCRIPTION: Defines a utility function `decode_at` that takes an object `o` (typically a `TfmdLists` or similar fastai dataset object) and an index `idx`. It retrieves the transformed item at the specified index (`o[idx]`) and then decodes it using the object's `decode` method, effectively reversing the transformations for inspection.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n#|exports\ndef decode_at(o, idx):\n    \"Decoded item at `idx`\"\n    return o.decode(o[idx])\n```\n\n----------------------------------------\n\nTITLE: Demonstrate Learner.no_bar Usage in Python\nDESCRIPTION: Creates a synthetic `Learner` and trains it for 5 epochs within the `learn.no_bar()` context. This demonstrates how to use the context manager to suppress the progress bars during training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/16_callback.progress.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner()\nwith learn.no_bar(): learn.fit(5)\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders for Tabular Data - Python\nDESCRIPTION: This code creates data loaders using the `TabularPandas` object, setting up the batches for training. `bs` sets the batch size for training. The dataloaders are essential for feeding the data into the model during training and validation.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-tabular.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndls = to.dataloaders(bs=64)\n```\n\n----------------------------------------\n\nTITLE: Documentation for plot_confusion_matrix method\nDESCRIPTION: This snippet generates documentation for the `plot_confusion_matrix` method of the `ClassificationInterpretation` class, including a detailed explanation of its purpose, arguments, and how it operates to visualize model performance.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(ClassificationInterpretation.plot_confusion_matrix, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Upgrading fastai on Colab - Python\nDESCRIPTION: This code installs or upgrades the fastai library using pip.  It checks if the current environment is Google Colaboratory. If it is, the script updates the fastai library. This is a common practice to ensure that the correct version is used for the environment. It requires pip and an active internet connection during execution.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#| eval: false\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Implementing Corpus BLEU Metric for NLP in Python\nDESCRIPTION: Defines the CorpusBLEUMetric class to compute the BLEU score over an entire validation corpus for translation or sequence prediction tasks. It calculates n-gram precision with smoothing to avoid zero precision values, using 1 to 4-grams and length penalty to produce a final BLEU score. The class depends on PyTorch tensors and numpy for n-gram hashing, and fastai's Metric class structure. Key methods include reset for initializing state, accumulate for batch-wise metric updates during validation, and a value property to compute the final BLEU score. Limitations include reliance on GPU tensors being converted to numpy arrays for n-gram processing and an internal smoothing technique adapted from SacreBLEU and academic literature.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass CorpusBLEUMetric(Metric):\n    def __init__(self, vocab_sz=5000, axis=-1):\n        \"BLEU Metric calculated over the validation corpus\"\n        self.metric_name = 'CorpusBLEU'\n        self.axis, self.vocab_sz = axis, vocab_sz\n        self.pred_len,self.targ_len,self.samp_idx,self.corrects,self.counts, = 0,0,0,[0]*4,[0]*4\n\n    def reset(self):\n        self.pred_len,self.targ_len,self.corrects,self.counts = 0,0,[0]*4,[0]*4\n\n    class NGram():\n        def __init__(self, ngram, max_n=5000): self.ngram,self.max_n = ngram,max_n\n        def __eq__(self, other):\n            if len(self.ngram) != len(other.ngram): return False\n            return np.all(np.array(self.ngram) == np.array(other.ngram))\n        def __hash__(self): return int(sum([o * self.max_n**i for i,o in enumerate(self.ngram)]))\n\n    def get_grams(self, x, n, max_n=5000):\n        return x if n==1 else [self.NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)]\n\n    def get_correct_ngrams(self, pred, targ, n, max_n=5000):\n        pred_grams,targ_grams = self.get_grams(pred, n, max_n=max_n),self.get_grams(targ, n, max_n=max_n)\n        pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams)\n        return sum([min(c, targ_cnt[g]) for g,c in pred_cnt.items()]),len(pred_grams)\n\n    def accumulate(self, learn):\n        if learn.training: return None\n        else:\n            last_output = learn.pred.argmax(dim=self.axis)\n            last_target = learn.y\n            for pred,targ in zip(last_output.cpu().numpy(),last_target.cpu().numpy()):\n                self.pred_len += len(pred)\n                self.targ_len += len(targ)\n                smooth_mteval = 1\n                for i in range(4):\n                    c,t = self.get_correct_ngrams(pred, targ, i+1, max_n=self.vocab_sz)\n                    if c == 0:\n                        smooth_mteval *= 2\n                        c = 1 / smooth_mteval    # exp smoothing, method 3 from http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf\n                    self.corrects[i] += c\n                    self.counts[i]   += t\n\n    @property\n    def value(self):\n        if self.counts == 0: return None\n        elif max(self.corrects) == 0: return 0.0\n        else:\n            precs = [c/t for c,t in zip(self.corrects,self.counts)]\n            len_penalty = math.exp(1 - self.targ_len/self.pred_len) if self.pred_len < self.targ_len else 1\n            return len_penalty * ((precs[0]*precs[1]*precs[2]*precs[3]) ** 0.25)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating TfmdLists with Data Splitting in fastai (Python)\nDESCRIPTION: Shows how to use the `splits` argument in `TfmdLists` to create training and validation subsets. It defines `splits` as a list of index lists. The example verifies the number of subsets (`n_subsets`), accesses subsets using `.train`, `.valid`, and `.subset(i)`, checks the items within each subset, confirms the `split_idx` is correctly assigned to the transforms in each subset, and tests creating new empty subsets and checking for overlapping splits.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n# add splits to TfmdLists\nsplits = [[0,2],[1]]\ntl = TfmdLists(items, tfms=tfms, splits=splits)\ntest_eq(tl.n_subsets, 2)\ntest_eq(tl.train, tl.subset(0))\ntest_eq(tl.valid, tl.subset(1))\ntest_eq(tl.train.items, items[splits[0]])\ntest_eq(tl.valid.items, items[splits[1]])\ntest_eq(tl.train.tfms.split_idx, 0)\ntest_eq(tl.valid.tfms.split_idx, 1)\ntest_eq(tl.train.new_empty().split_idx, 0)\ntest_eq(tl.valid.new_empty().split_idx, 1)\ntest_eq_type(tl.splits, L(splits))\nassert not tl.overlapping_splits()\n```\n\n----------------------------------------\n\nTITLE: Upgrading fastai on Google Colab via Shell\nDESCRIPTION: This shell command checks if the execution environment is Google Colab by verifying the existence of the '/content' directory. If it exists, the command proceeds to upgrade the 'fastai' library to the latest version using pip, suppressing output with the '-Uqq' flags.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Defining ModelResetter Callback for FastAI RNNs in Python\nDESCRIPTION: Implements a FastAI callback that resets the RNN model state at key stages: before training, before validation, and after fitting. This reset ensures the RNN hidden states are cleared between runs, which is essential for proper sequence handling during training and evaluation. The callback overrides methods corresponding to lifecycle events to invoke the model's reset method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/34_callback.rnn.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@docs\nclass ModelResetter(Callback):\n    \"`Callback` that resets the model at each validation/training step\"\n    def before_train(self):    self.model.reset()\n    def before_validate(self): self.model.reset()\n    def after_fit(self):       self.model.reset()\n    _docs = dict(before_train=\"Reset the model before training\",\n                 before_validate=\"Reset the model before validation\",\n                 after_fit=\"Reset the model after fitting\")\n```\n\n----------------------------------------\n\nTITLE: Loading a fastai Learner from Hugging Face Hub (Python)\nDESCRIPTION: Python code example demonstrating how to download and load a fastai `Learner` object from a repository on the Hugging Face Hub using the `from_pretrained_fastai` function. The `repo_id` specifies the Hugging Face repository containing the saved Learner.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/74_huggingface.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import from_pretrained_fastai\n\n# repo_id = \"YOUR_USERNAME/YOUR_LEARNER_NAME\"\nrepo_id = \"espejelomar/identify-my-cat\"\n\nlearner = from_pretrained_fastai(repo_id)\n```\n\n----------------------------------------\n\nTITLE: Save Classifier (Third Layer Group)\nDESCRIPTION: Saves the trained model after the third training stage.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nlearn.save('third')\n```\n\n----------------------------------------\n\nTITLE: Predicting a Single Row with TabularLearner in Python\nDESCRIPTION: Shows how to use the `predict` method of a `TabularLearner` instance to generate prediction outputs for a single row of tabular data. The method returns three outputs: the fully decoded input row, the decoded prediction category, and the raw model prediction probabilities or values. This usage highlights the class-specific behavior differing from standard `Learner` predict methods.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/43_tabular.learner.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nrow, clas, probs = learn.predict(df.iloc[0])\n```\n\n----------------------------------------\n\nTITLE: Plotting fit_flat_cos Schedules with plot_sched\nDESCRIPTION: Uses `plot_sched` to visualize the learning rate schedule generated by `fit_flat_cos`. It calls the method on the learner's recorder to generate and display the learning rate schedule.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nlearn.recorder.plot_sched()\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Categorify\nDESCRIPTION: This code demonstrates how to use the `Categorify` processor with `TabularGPU`. It creates a `cudf.DataFrame`, wraps it with `TabularGPU`, and applies the `Categorify` processor. It includes tests to ensure the categorical encoding is performed correctly, including handling values not seen during training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf = cudf.from_pandas(pd.DataFrame({'a':[0,1,2,0,2]}))\nto = TabularGPU(df, Categorify, 'a')\ncat = to.procs.categorify\ntest_eq(list(cat['a']), ['#na#','0','1','2'])\ntest_eq(to.a.to_array(), np.array([1,2,3,1,3]))\ndf1 = cudf.from_pandas(pd.DataFrame({'a':[1,0,3,-1,2]}))\nto1 = to.new(df1)\ncat(to1)\n#Values that weren't in the training df are sent to 0 (na)\ntest_eq(to1.a.to_array(), np.array([2,1,0,0,3]))\n```\n\n----------------------------------------\n\nTITLE: Loading Sample Text Data - Python\nDESCRIPTION: This code block downloads a sample dataset (IMDB movie reviews) using fastai's `untar_data` utility and then loads the `texts.csv` file into a pandas DataFrame. It then displays the first two rows of the DataFrame to inspect the data structure, which includes text reviews.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.IMDB_SAMPLE)\ndf = pd.read_csv(path/'texts.csv')\ndf.head(2)\n```\n\n----------------------------------------\n\nTITLE: Testing WeightedDL with DistributedDL and Custom Weights - Python\nDESCRIPTION: Initializes a WeightedDL with custom binary weights (via NumPy) for half the dataset and applies DistributedDL partitioning. Checks that all results in the distributed sample conform to the weighting constraint using equality and lambda-based tests. Requires: fastai WeightedDL, DistributedDL, NumPy, test functions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\ndl = WeightedDL(list(range(50)), bs=16, num_workers=2, shuffle=True,wgts=list(np.arange(50)>=25))\nres = []\nfor i in range(4):\n    dl1 = DistributedDL(dl, i, 4)\n    res += list(dl1)[0].tolist()\ntest(res,[25]*len(res),operator.ge)        # all res >=25\ntest(res,[25]*len(res),lambda a,b: not (a<b)) # all res NOT < 25\n```\n\n----------------------------------------\n\nTITLE: Verify TrackerCallback isn't run during lr_find\nDESCRIPTION: This creates a learner and runs `lr_find`. It asserts that the `new_best` attribute is not set on the learner, indicating that the `TrackerCallback` is not executed during the learning rate finder process.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#A tracker callback is not run during an lr_find\nfrom fastai.callback.schedule import *\n```\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nlearn = synth_learner(n_trn=2, cbs=TrackerCallback(monitor='tst_metric'), metrics=tst_metric)\nlearn.lr_find(num_it=15, show_plot=False)\nassert not hasattr(learn, 'new_best')\n```\n\n----------------------------------------\n\nTITLE: Synthetic DataLoaders creation for GPU testing of channels last format\nDESCRIPTION: Constructs a small dataset and DataLoader pipeline, activating GPU if specified. Provides a controlled environment to test model training with channels last format and mixed precision, assuming compatible hardware.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18c_callback.channelslast.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n#|cuda\ndef synth_dbunch(bs=16, n_train=10, n_valid=2, cuda=True):\n    def get_data(n):\n        return TensorDataset(TensorImage(torch.randn(bs*n, 3, 32, 32)))\n    train_ds = get_data(n_train)\n    valid_ds = get_data(n_valid)\n    device = default_device() if cuda else None\n    train_dl = TfmdDL(train_ds, bs=bs, shuffle=True, num_workers=0)\n    valid_dl = TfmdDL(valid_ds, bs=bs, num_workers=0)\n    return DataLoaders(train_dl, valid_dl, device=device)\n```\n\n----------------------------------------\n\nTITLE: Adding detach_parallel Method to Learner\nDESCRIPTION: Patches the Learner class to add a method that removes the ParallelTrainer callback, disabling parallel training and reverting to single-device operation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@patch\ndef detach_parallel(self: Learner):\n    \"Remove `ParallelTrainer` callback from a Learner\"\n    self.remove_cb(ParallelTrainer)\n    return self\n```\n\n----------------------------------------\n\nTITLE: Setting Default Activation Function (Python)\nDESCRIPTION: Assigns `torch.nn.ReLU` as the default activation function to `defaults.activation`. This default is used by layers like `ConvLayer` unless explicitly overridden.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndefaults.activation=nn.ReLU\n```\n\n----------------------------------------\n\nTITLE: Showing DataLoaders Batch\nDESCRIPTION: This code displays a batch of data from the `DataLoaders` object.  The `show_batch` method allows inspection of the data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndls.show_batch()\n```\n\n----------------------------------------\n\nTITLE: Setting Hidden Parameters for DistributedTrainer - Python\nDESCRIPTION: Defines a list of private or hidden parameters (_hidden_params) to be excluded/delegated during callback initializations or argument forwarding. Used to refine which attributes are passed to Accelerate and distributed training infrastructure. No dependencies except standard Python list.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\n_hidden_params = [\"mixed_precision\", \"fp16\", \"log_with\", \"logging_dir\", \"step_scheduler_with_optimizer\"]\n```\n\n----------------------------------------\n\nTITLE: Creating LossMetric and LossMetrics Classes for Flexible Loss Tracking in Python\nDESCRIPTION: Introduces the LossMetric class, a fastai-style metric that extracts and accumulates a specific attribute (e.g., 'l1' or 'l2') from a loss function object during training. It tracks weighted sums based on batch size and supports custom metric naming. The accompanying LossMetrics function generates a list of LossMetric instances given attribute names and optional display names. These classes depend on fastai utilities, require the loss function attributes to be properly defined, and help monitor various loss components during model training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass LossMetric(AvgMetric):\n    \"Create a metric from `loss_func.attr` named `nm`\"\n    def __init__(self, attr, nm=None): store_attr('attr,nm')\n    def accumulate(self, learn):\n        bs = find_bs(learn.yb)\n        self.total += learn.to_detach(getattr(learn.loss_func, self.attr, 0))*bs\n        self.count += bs\n\n    @property\n    def name(self): return self.attr if self.nm is None else self.nm\n```\n\n----------------------------------------\n\nTITLE: Implementing Exponential Root Mean Square Percentage Error in PyTorch\nDESCRIPTION: Creates a metric for exponential root mean square percentage error by defining a function that calculates the error between exponential of predictions and targets, then wraps it in an AccumMetric.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndef _exp_rmspe(inp,targ):\n    inp,targ = torch.exp(inp),torch.exp(targ)\n    return torch.sqrt(((targ - inp)/targ).pow(2).mean())\nexp_rmspe = AccumMetric(_exp_rmspe)\nexp_rmspe.__doc__ = \"Root mean square percentage error of the exponential of  predictions and targets\"\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Example\nDESCRIPTION: An example usage of the `fine_tune` method. It applies the fine-tuning process to the model using default parameters.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nlearn.fine_tune(1)\n```\n\n----------------------------------------\n\nTITLE: ShortEpochCallback - Early Stopping after a Percentage of an Epoch\nDESCRIPTION: This callback stops training or validation after a specified percentage of an epoch has been processed. It is useful for quick prototyping or debugging by limiting training to small fractions of an epoch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18a_callback.training.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nclass ShortEpochCallback(Callback):\n    \"Fit just `pct` of an epoch, then stop\"\n    def __init__(self,pct=0.01,short_valid=True): self.pct,self.short_valid = pct,short_valid\n    def after_batch(self):\n        if self.iter/self.n_iter < self.pct: return\n        if self.training:    raise CancelTrainException\n        if self.short_valid: raise CancelValidException\n```\n\n----------------------------------------\n\nTITLE: Example Usage: Creating Vision Models - Python\nDESCRIPTION: Demonstrates how to use the `create_vision_model` function to instantiate a ResNet18 model. The first example creates a model with 10 output classes using default settings (pretrained, 3 input channels). The second example shows how to specify a different number of input channels (`n_in=1`) for tasks like grayscale image classification.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntst = create_vision_model(models.resnet18, 10, True)\ntst = create_vision_model(models.resnet18, 10, True, n_in=1)\n```\n\n----------------------------------------\n\nTITLE: Plotting Top Losses from Interpretation (Python)\nDESCRIPTION: Implements the `plot_top_losses` method for the `Interpretation` class. It retrieves the specified top/bottom `k` losses (or a range of indices), fetches the corresponding inputs, predictions, and targets, and then calls the dispatched `plot_top_losses` function to visualize them.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef plot_top_losses(self,\n        k:int|MutableSequence, # Number of losses to plot\n        largest:bool=True, # Sort losses by largest or smallest\n        **kwargs\n    ):\n        \"Show `k` largest(/smallest) preds and losses. Implementation based on type dispatch\"\n        if is_listy(k) or isinstance(k, range):\n            losses, idx = (o[k] for o in self.top_losses(None, largest))\n        else: \n            losses, idx = self.top_losses(k, largest)\n        inps, preds, targs, decoded, _ = self[idx]\n        inps, targs, decoded = tuplify(inps), tuplify(targs), tuplify(decoded)\n        x, y, its = self.dl._pre_show_batch(inps+targs, max_n=len(idx))\n        x1, y1, outs = self.dl._pre_show_batch(inps+decoded, max_n=len(idx))\n        if its is not None:\n            plot_top_losses(x, y, its, outs.itemgot(slice(len(inps), None)), preds, losses, **kwargs)\n        #TODO: figure out if this is needed\n        #its None means that a batch knows how to show itself as a whole, so we pass x, x1\n        #else: show_results(x, x1, its, ctxs=ctxs, max_n=max_n, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Documentation for TensorBoardProjectorCallback\nDESCRIPTION: Provides documentation for the callback class responsible for exporting image features for embedding visualization in TensorBoard during inference.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nshow_doc(TensorBoardProjectorCallback)\n```\n\n----------------------------------------\n\nTITLE: RNNDropout Test - PyTorch\nDESCRIPTION: This snippet tests the `RNNDropout` class. It instantiates an `RNNDropout` layer with a dropout probability, creates a random input tensor, and applies the layer. It then iterates through the output to verify that if an element in the output is 0, all elements across the time dimension for the same position also are 0, and that the non-zero elements are scaled by 1/(1-p).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndp = RNNDropout(0.3)\ntst_inp = torch.randn(4,3,7)\ntst_out = dp(tst_inp)\nfor i in range(4):\n    for j in range(7):\n        if tst_out[i,0,j] == 0: assert (tst_out[i,:,j] == 0).all()\n        else: test_close(tst_out[i,:,j], tst_inp[i,:,j]/(1-0.3))\n```\n\n----------------------------------------\n\nTITLE: Testing EarlyStoppingCallback (Valid Loss)\nDESCRIPTION: This tests the `EarlyStoppingCallback` using the validation loss as the monitored metric. It fits a learner with specified parameters and the early stopping callback, and then asserts that the training stopped after a certain number of epochs based on the patience.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner(n_trn=2)\nlearn.fit(n_epoch=200, lr=1e-7, cbs=EarlyStoppingCallback(monitor='valid_loss', min_delta=0.1, patience=2))\n```\n\nLANGUAGE: python\nCODE:\n```\n#|hide\ntest_eq(len(learn.recorder.values), 3)\n```\n\n----------------------------------------\n\nTITLE: Implementing setup_distrib for Distributed Training\nDESCRIPTION: Creates a function that configures the current process for distributed training by setting the CUDA device and initializing the distributed process group using NCCL backend.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef setup_distrib(gpu=None):\n    \"Setup this process to participate in distributed training\"\n    if gpu is None: return gpu\n    gpu = int(gpu)\n    torch.cuda.set_device(int(gpu))\n    if num_distrib() > 0: torch.distributed.init_process_group(backend='nccl', init_method='env://')\n    return gpu\n```\n\n----------------------------------------\n\nTITLE: Splits as Index Lists and Boolean Masks\nDESCRIPTION: These snippets demonstrate flexible dataset splitting via index lists and boolean masks, allowing disjoint subsets that do not need to cover all data items. They validate that the correct data is assigned to train and validation splits.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_49\n\nLANGUAGE: Python\nCODE:\n```\n# splits can be indices\ndsets = Datasets(range(5), tfms=[None], splits=[tensor([0,2]), [1,3,4]])\ntest_eq(dsets.subset(0), [(0,),(2,)])\ntest_eq(dsets.train, [(0,),(2,)])\n\ntest_eq(dsets.subset(1), [(1,),(3,),(4,)])\ntest_eq(dsets.valid, [(1,),(3,),(4,)])\n\ntest_eq(*dsets.valid[2], 4)\n#assert '[(1,),(3,),(4,)]' in str(dsets) and '[(0,),(2,)]' in str(dsets)\ndsets\n```\n\n----------------------------------------\n\nTITLE: Ordering Callbacks in Learner\nDESCRIPTION: This snippet touches on the internal ordering of callbacks. It uses `ordered_cbs` to access callbacks in the order in which they are executed. It highlights that the order is defined internally, referring to `callback.core` for details. Demonstrates using `add_cb` followed by `ordered_cbs('before_fit')` to showcase order behavior.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner()\nlearn.add_cb(TestTrainEvalCallback())\nlearn.ordered_cbs('before_fit')\n```\n\n----------------------------------------\n\nTITLE: Suggesting Learning Rate via the Slide Interval Rule in Python\nDESCRIPTION: The slide function suggests a learning rate by analyzing the gradient of the loss values and looking for intervals where the gradient difference falls below a threshold over a specified range. This method was developed by Andrew Chang and utilizes numpy for gradient calculation and interpolation to find the learning rate suggestion and index.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndef slide(lrs:list, losses:list, num_it:int, lr_diff:int=15, thresh:float=.005, adjust_value:float=1.):\n    \"Suggests a learning rate following an interval slide rule and returns its index\"\n    losses = to_np(losses)\n    loss_grad = np.gradient(losses)\n\n    r_idx = -1\n    l_idx = r_idx - lr_diff\n    local_min_lr = lrs[l_idx]\n    while (l_idx >= -len(losses)) and (abs(loss_grad[r_idx] - loss_grad[l_idx]) > thresh):\n        local_min_lr = lrs[l_idx]\n        r_idx -= 1\n        l_idx -= 1\n    \n    suggestion = float(local_min_lr) * adjust_value\n    idx = np.interp(np.log10(suggestion), np.log10(lrs), losses)\n    return suggestion, (suggestion, idx)\n```\n\n----------------------------------------\n\nTITLE: Defining Generic plot_top_losses Placeholder (Python)\nDESCRIPTION: Provides a generic definition for the `plot_top_losses` function using the `@dispatch` decorator. This placeholder raises an exception indicating that the function must be implemented specifically for different data types (x, y) through the dispatch mechanism.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@dispatch\ndef plot_top_losses(x, y, *args, **kwargs):\n    raise Exception(f\"plot_top_losses is not implemented for {type(x)},{type(y)}\")\n```\n\n----------------------------------------\n\nTITLE: DataFrame Creation and Date Feature Engineering (pandas)\nDESCRIPTION: This snippet creates a pandas DataFrame with various data types, including categorical, integer, float, and datetime. It then uses `add_datepart` to extract date-related features and sets the categories for a categorical column. Finally, `cont_cat_split` is used to split columns into continuous and categorical based on cardinality.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'cat1': pd.Series(['l','xs','xl','s'], dtype='category'),\n                    'ui32': pd.Series([1, 2, 3, 4], dtype='UInt32'),\n                    'i64': pd.Series([1, 2, 3, 4], dtype='Int64'),\n                    'f16': pd.Series([1, 2, 3, 4], dtype='Float64'),\n                    'd1_date': ['2021-02-09', None, '2020-05-12', '2020-08-14'],\n                    })\ndf = add_datepart(df, 'd1_date', drop=False)\ndf['cat1'] = df['cat1'].cat.set_categories(['xl','l','m','s','xs'], ordered=True)\ncont_names, cat_names = cont_cat_split(df, max_card=0)\n```\n\n----------------------------------------\n\nTITLE: Demonstrate ProgressCallback Usage in Python\nDESCRIPTION: Creates a synthetic fastai `Learner` using `synth_learner` and trains it for 5 epochs using `learn.fit(5)`. Since `ProgressCallback` is added to `defaults.callbacks` by the previous snippet, this demonstrates the default behavior of displaying progress bars during training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/16_callback.progress.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner()\nlearn.fit(5)\n```\n\n----------------------------------------\n\nTITLE: Testing TabularPandas with Categorify - Python\nDESCRIPTION: This snippet tests the `TabularPandas` object with the `Categorify` preprocessor. It initializes `Categorify`, creates a DataFrame, and then creates a `TabularPandas` object. The `test_series` function verifies if the `vocab` attribute of the `TabularPandas` object contains the expected values.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ncat = Categorify()\ndf = pd.DataFrame({'a':[0,1,2,3,2], 'b': ['a', 'b', 'a', 'c', 'b']})\nto = TabularPandas(df, cat, 'a', splits=[[0,1,2],[3,4]], y_names='b')\ntest_series(to.vocab, ['a', 'b'])\n```\n\n----------------------------------------\n\nTITLE: Splits as Disjoint Boolean Masks\nDESCRIPTION: This code shows that datasets can be split using boolean masks, enabling disjoint subsets that don't need to cover all data, useful for custom partitioning strategies. It verifies correct subset creation for train and validation splits.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_50\n\nLANGUAGE: Python\nCODE:\n```\n# splits can be boolean masks (they don't have to cover all items, but must be disjoint)\nsplits = [[False,True,True,False,True], [True,False,False,False,False]]\ndsets = Datasets(range(5), tfms=[None], splits=splits)\n\ntest_eq(dsets.train, [(1,),(2,),(4,)])\ntest_eq(dsets.valid, [(0,)])\n```\n\n----------------------------------------\n\nTITLE: Filtering Only Trainable Parameters from PyTorch Module in Python\nDESCRIPTION: Returns a list of model parameters that require gradient computation by filtering those with requires_grad set to True. This allows distinguishing parameters that will be updated during training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_77\n\nLANGUAGE: python\nCODE:\n```\ndef trainable_params(m):\n    \"Return all trainable parameters of `m`\"\n    return [p for p in m.parameters() if p.requires_grad]\n```\n\n----------------------------------------\n\nTITLE: Constructing and Training fastai Learner with PyTorch Components in Python\nDESCRIPTION: Imports necessary fastai components including Learner and metrics then builds a Learner object combining the fastai DataLoaders, the custom PyTorch model, negative log likelihood loss as loss function, wrapped Adam optimizer, and accuracy metric. Afterward, uses fastai's fit_one_cycle scheduler to train the model for one epoch at a specified learning rate. This demonstrates training a pure PyTorch model with PyTorch DataLoaders inside fastai's flexible training loop.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_pytorch_verbose.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport fastai.callback.schedule # To get `fit_one_cycle`, `lr_find`\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.learner import Learner\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.metrics import accuracy\n```\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn.functional as F\n```\n\nLANGUAGE: python\nCODE:\n```\nlearn = Learner(dls, Net(), loss_func=F.nll_loss, opt_func=opt_func, metrics=accuracy)\n```\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(n_epoch=1, lr_max=1e-2)\n```\n\n----------------------------------------\n\nTITLE: Create DataLoaders\nDESCRIPTION: This code creates DataLoaders from the Datasets object with specified batch size and sequence length for language modeling. `bs` is the batch size and `sl` is the sequence length.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/35_tutorial.wikitext.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nbs,sl = 104,72\ndls = dsets.dataloaders(bs=bs, seq_len=sl)\n```\n\n----------------------------------------\n\nTITLE: Example Usage of SortedDL Batching and Padding - Python\nDESCRIPTION: This example provides a concrete demonstration of how `SortedDL` batches and pads sequences. It initializes `SortedDL` with a small list of tensors and specifies a batch size (`bs=2`) and a `before_batch` function (`pad_input`) for padding. The assertion verifies the expected output batches, showing sorted order and padding.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nds = [(tensor([1,2]),1), (tensor([3,4,5,6]),2), (tensor([7]),3), (tensor([8,9,10]),4)]\ndl = SortedDL(ds, bs=2, before_batch=partial(pad_input, pad_idx=0))\ntest_eq(list(dl), [(tensor([[ 3,  4,  5,  6], [ 8,  9, 10,  0]]), tensor([2, 4])), \n                   (tensor([[1, 2], [7, 0]]), tensor([1, 3]))])\n```\n\n----------------------------------------\n\nTITLE: Removing Context-Managed Callbacks\nDESCRIPTION: This snippet illustrates the usage of the `removed_cbs` context manager to temporarily remove a callback from the `Learner`. It shows how to remove a callback from the `Learner`'s callback list within a `with` statement, and restores it when the block ends.  Assertions check callback counts before, within, and after the context.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nwith learn.removed_cbs(learn.cbs[1]):\n    test_eq(len(learn.cbs), 1)\ntest_eq(len(learn.cbs), 2)\n```\n\n----------------------------------------\n\nTITLE: Getting Text Files from a Path\nDESCRIPTION: Specialized function to retrieve only text files (.txt) from a specified path, with support for recursion and folder filtering.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef get_text_files(path, recurse=True, folders=None):\n    \"Get text files in `path` recursively, only in `folders`, if specified.\"\n    return get_files(path, extensions=['.txt'], recurse=recurse, folders=folders)\n```\n\n----------------------------------------\n\nTITLE: EmbeddingDropout Test - PyTorch\nDESCRIPTION: This code tests the `EmbeddingDropout` module. It instantiates an `Embedding` layer and wraps it with `EmbeddingDropout`. It then generates random input indices, applies `EmbeddingDropout`, and checks the output. It confirms that elements in the output are either all zero or equal to the original embeddings scaled by the dropout factor, in accordance with the dropout mechanics.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nenc = nn.Embedding(10, 7, padding_idx=1)\nenc_dp = EmbeddingDropout(enc, 0.5)\ntst_inp = torch.randint(0,10,(8,))\ntst_out = enc_dp(tst_inp)\nfor i in range(8):\n    assert (tst_out[i]==0).all() or torch.allclose(tst_out[i], 2*enc.weight[tst_inp[i]])\n```\n\n----------------------------------------\n\nTITLE: Example of in-place unsqueezing of tensor (Python)\nDESCRIPTION: This example applies `unsqueeze_` to a tensor to add two singleton dimensions in-place, resulting in a tensor with shape (1,1,1). It demonstrates how to reshape tensors directly without creating new instances.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\nt = tensor([1])\nunsqueeze_(t, n=2)\ntest_eq(t, tensor([1]).view(1,1,1))\n```\n\n----------------------------------------\n\nTITLE: PartialLambda Layer for Partial Application in fastai - Python\nDESCRIPTION: A subclass of Lambda that applies functools.partial to the function, enabling partial arguments to be bound. Requires the functools.partial utility, and is initialized with func and keyword arguments for partial application. Expects input x, returns func(x) with partially applied parameters. Improves reusability for parameterized layer functions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass PartialLambda(Lambda):\n    \"Layer that applies `partial(func, **kwargs)`\"\n    def __init__(self, func, **kwargs):\n        super().__init__(partial(func, **kwargs))\n        self.repr = f'{func.__name__}, {kwargs}'\n\n    def forward(self, x): return self.func(x)\n    def __repr__(self): return f'{self.__class__.__name__}({self.repr})'\n```\n\n----------------------------------------\n\nTITLE: Initializing TextDataLoaders from CSV\nDESCRIPTION: Creates a TextDataLoaders object using the `from_csv` method. It specifies the path to the CSV file and the column names for text, labels, and validation sets. `show_batch` is then used to display a sample of the data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndls = TextDataLoaders.from_csv(path=path, csv_fname='texts.csv', text_col='text', label_col='label', valid_col='is_valid')\ndls.show_batch(max_n=3)\n```\n\n----------------------------------------\n\nTITLE: Defining MSELossFlat - Fastai/PyTorch\nDESCRIPTION: This function defines `MSELossFlat` within the fastai library. It wraps PyTorch's `nn.MSELoss` using the `BaseLoss` class, providing automatic flattening of input and target tensors. It supports standard `MSELoss` arguments like `reduction`, along with fastai-specific ones like `axis` and `floatify`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@use_kwargs_dict(reduction='mean')\ndef MSELossFlat(\n    *args, \n    axis:int=-1, # Class axis\n    floatify:bool=True, # Convert `targ` to `float`\n    **kwargs\n):\n    \"Same as `nn.MSELoss`, but flattens input and target.\"\n    return BaseLoss(nn.MSELoss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Removing \\'module\\' from State Dict Keys - Python\nDESCRIPTION: Defines _rm_module, a string manipulation utility for removing a 'module' token from a PyTorch state dict key. Input: n (string key). Output: cleaned string with first occurrence of 'module' removed. No dependencies or side effects. Used primarily for compatibility handling of multi-GPU saved models.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef _rm_module(n:str):\n    t = n.split('.')\n    for i in range(len(t)-1, -1, -1):\n        if t[i] == 'module':\n            t.pop(i)\n            break\n    return '.'.join(t)\n```\n\n----------------------------------------\n\nTITLE: Show Batch of Language Model Data (Full Dataset)\nDESCRIPTION: Displays a batch of data from the `DataLoaders` object for the full IMDB dataset.  This is useful for understanding the structure of the batches and verifying the data pipeline.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndbunch_lm.show_batch()\n```\n\n----------------------------------------\n\nTITLE: Test Learner.get_preds (using validate) in Python\nDESCRIPTION: Tests the `get_preds` functionality (indirectly, by calling `validate` which is often used internally by `get_preds` for validation/test sets) of a fastai `Learner` without prior training. Similar to the previous test, it creates a synthetic learner, runs validation, and captures predictions and targets.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/16_callback.progress.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#Check get_preds works without any training\nlearn = synth_learner(n_trn=5, metrics=tst_metric)\npreds,targs = learn.validate()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating TfmdLists Transform Setup Method in fastai (Python)\nDESCRIPTION: Illustrates the `setup` mechanism for Transforms within `TfmdLists`. A custom transform `_B` is defined with a `setups` method that calculates and stores the mean of the input items. When `TfmdLists` is initialized with `items` and an instance of `_B`, the `setups` method is automatically called, computing the mean (2.0) and storing it in `tl.m`, which is then verified.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nclass _B(Transform):\n    def __init__(self): self.m = 0\n    def encodes(self, o): return o+self.m\n    def decodes(self, o): return o-self.m\n    def setups(self, items): \n        print(items)\n        self.m = tensor(items).float().mean().item()\n\n# test for setup, which updates `self.m`\ntl = TfmdLists(items, _B())\ntest_eq(tl.m, 2)\n```\n\n----------------------------------------\n\nTITLE: Documenting CometCallback Class\nDESCRIPTION: This uses the `show_doc` function from the `nbdev` library to generate documentation for the `CometCallback` class.  This function likely parses the class definition and creates user-friendly documentation (possibly including code examples) for the class and its methods.  This is a part of the nbdev library used for documentation generation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70d_callback.comet.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(CometCallback)\n```\n\n----------------------------------------\n\nTITLE: Importing Modules for fastai Integration\nDESCRIPTION: This snippet imports necessary modules for integrating fastai with an Ignite example.  It imports all the code from `migrating_ignite` and imports all of fastai's vision functionality.  These imports are essential for utilizing the functionalities of both frameworks for the training process.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_ignite.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom migrating_ignite import *\n\nfrom fastai.vision.all import *\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders and Visualizing a Batch with fastai in Python\nDESCRIPTION: Creates `DataLoaders` (`dls`) from the previously defined `mnist` DataBlock, using the tiny MNIST dataset (downloaded and extracted via `untar_data(URLs.MNIST_TINY)`) as the data source. It then uses the `show_batch` method on the `DataLoaders` to display a grid of 9 sample images with their corresponding labels, facilitating quick data verification.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_lightning.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndls = mnist.dataloaders(untar_data(URLs.MNIST_TINY))\ndls.show_batch(max_n=9, figsize=(4,4))\n```\n\n----------------------------------------\n\nTITLE: Exception Handling in Callbacks\nDESCRIPTION: Demonstrates how exceptions during callback events can be caught.  This example shows a TypeError being raised inside of a callback, and a check is done to catch it.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nclass TstCallback(Callback):\n    def batch_begin(self): self.learn.a = 1 + \"a\"\nlearn,cb = TstLearner(1),TstCallback()\ncb.learn = learn\nwith ExceptionExpected(TypeError, regex=\" in `TstCallback` when calling event `batch_begin`\"):\n    cb('batch_begin')\n```\n\n----------------------------------------\n\nTITLE: Displaying PIL or PyTorch Images on Matplotlib Axes in Python\nDESCRIPTION: The `show_image` function renders images on a matplotlib axis supporting multiple image formats: PIL images, PyTorch tensors, and numpy arrays. It handles channel order permutations for tensors, converts single-channel images, dynamically sets figure sizes, and optionally adds a title. It returns the matplotlib axis with the displayed image. Additional `imshow` parameters can be passed for customization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@delegates(plt.Axes.imshow, keep=True, but=['shape', 'imlim'])\ndef show_image(im, ax=None, figsize=None, title=None, ctx=None, **kwargs):\n    \"Show a PIL or PyTorch image on `ax`.\"\n    # Handle pytorch axis order\n    if hasattrs(im, ('data','cpu','permute')):\n        im = im.data.cpu()\n        if im.shape[0]<5: im=im.permute(1,2,0)\n    elif not isinstance(im,np.ndarray): im=array(im)\n    # Handle 1-channel images\n    if im.shape[-1]==1: im=im[...,0]\n\n    ax = ifnone(ax,ctx)\n    if figsize is None: figsize = (_fig_bounds(im.shape[0]), _fig_bounds(im.shape[1]))\n    if ax is None: _,ax = plt.subplots(figsize=figsize)\n    ax.imshow(im, **kwargs)\n    if title is not None: ax.set_title(title)\n    ax.axis('off')\n    return ax\n```\n\n----------------------------------------\n\nTITLE: Create Vision Model from timm Architecture - Python\nDESCRIPTION: Builds a complete vision model using a specified architecture (`arch`) from the `timm` library. It loads the timm model, sets `num_classes=0` to get just the body, wraps the body in `TimmBody`, determines the number of features (`nf`), and adds a classification/regression head using `add_head`. Returns the combined model and the timm model's default configuration.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef create_timm_model(arch, n_out, cut=None, pretrained=True, n_in=3, init=nn.init.kaiming_normal_, custom_head=None,\n                     concat_pool=True, pool=True, lin_ftrs=None, ps=0.5, first_bn=True, bn_final=False, lin_first=False, y_range=None, **kwargs):\n    \"Create custom architecture using `arch`, `n_in` and `n_out` from the `timm` library\"\n    model = timm.create_model(arch, pretrained=pretrained, num_classes=0, in_chans=n_in, **kwargs)\n    body = TimmBody(model, pretrained, None, n_in)\n    nf = body.model.num_features\n    res = add_head(body, nf, n_out, init=init, head=custom_head, concat_pool=concat_pool, pool=body.needs_pool,\n                   lin_ftrs=lin_ftrs, ps=ps, first_bn=first_bn, bn_final=bn_final, lin_first=lin_first, y_range=y_range)\n    return res,model.default_cfg\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export Namespace for Callbacks\nDESCRIPTION: This snippet specifies the default export namespace as 'callback.wandb', indicating the module deals with Weights & Biases integration for experiment tracking within the callback architecture of FastAI.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n#|default_exp callback.wandb\n```\n\n----------------------------------------\n\nTITLE: Creating Datasets with Custom Transform and Splits\nDESCRIPTION: This snippet creates a dataset with range 0-4, applies the custom `_Tfm` transform, and specifies dataset splits. It prepares data for training and validation, enabling subset-specific transformations and splits.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_53\n\nLANGUAGE: Python\nCODE:\n```\ndsets = Datasets(range(5), [_Tfm()], splits=[[1,2],[0,3,4]])\ntest_eq(dsets.train,[(1,),(2,)])\ntest_eq(dsets.valid,[(0,),(6,),(8,)])\ntest_eq(dsets.train[False,True], [(2,)])\ndsets\n```\n\n----------------------------------------\n\nTITLE: Installing fastai with Pip (Bash)\nDESCRIPTION: This command installs the fastai library using the standard pip package manager. It is an alternative installation method if conda is not used or preferred.\nSOURCE: https://github.com/fastai/fastai/blob/main/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install fastai\n```\n\n----------------------------------------\n\nTITLE: hook_outputs Function Usage Example\nDESCRIPTION: This snippet demonstrates how to use the `hook_outputs` function. It creates a sequential model with three layers, then creates a `hook_outputs` object that attaches a hook to each layer. It performs a forward pass and verifies that the stored activations of each hook are equal to the output of the corresponding layer. It also demonstrates how to use `hook_outputs` to store gradients during a backward pass and verifies the stored gradients.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nlayers = [nn.Linear(5,10), nn.ReLU(), nn.Linear(10,3)]\ntst_model = nn.Sequential(*layers)\nx = torch.randn(4,5)\nwith hook_outputs(layers) as h:\n    y = tst_model(x)\n    test_eq(h.stored[0], layers[0](x))\n    test_eq(h.stored[1], F.relu(layers[0](x)))\n    test_eq(h.stored[2], y)\n    for s in h.stored: assert not s.requires_grad\n    \nwith hook_outputs(layers, grad=True) as h:\n    y = tst_model(x)\n    loss = y.pow(2).mean()\n    loss.backward()\n    g = 2*y / y.numel()\n    test_close(g, h.stored[2][0])\n    g = g @ layers[2].weight.data\n    test_close(g, h.stored[1][0])\n    g = g * (layers[0](x) > 0).float()\n    test_close(g, h.stored[0][0])\n```\n\n----------------------------------------\n\nTITLE: Testing ROC AUC Score for Multi-label Classification in Python\nDESCRIPTION: A test case for the RocAucMulti function using predefined input and target tensors to validate that the metric correctly returns the expected 0.5 score for uninformative predictions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nroc_auc_metric = RocAucMulti(sigmoid=False)\nx,y = torch.tensor([np.arange(start=0, stop=0.2, step=0.04)]*20), torch.tensor([0, 0, 1, 1]).repeat(5)\nassert compute_val(roc_auc_metric, x, y) == 0.5\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries in fastai (Python)\nDESCRIPTION: Imports essential modules from the fastai.text library for NLP operations and nbdev.showdoc for documentation display. These imports are prerequisites for all following examples, providing access to dataset utilities, tokenization, modeling, and documentation tools. No explicit user input is required; subsequent code relies on these imports being present.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.text.all import *\nfrom nbdev.showdoc import show_doc\n```\n\n----------------------------------------\n\nTITLE: Transposing PCA Result\nDESCRIPTION: This transposes the result of the PCA. The `.t()` method transposes the PCA results to prepare it for further analysis. This transformation makes it easier to access the principal components.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfac0,fac1,fac2 = movie_pca.t()\n```\n\n----------------------------------------\n\nTITLE: Visualizing a Batch of Tokenized Data with show_batch (Python)\nDESCRIPTION: Uses the DataLoaders object to display a sample of maximum two tokenized text examples and their corresponding targets, aiding in visual verification of batch construction for language modeling. Expects dls to be defined; outputs formatted batch preview.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndls.show_batch(max_n=2)\n```\n\n----------------------------------------\n\nTITLE: Loading the Language Model State\nDESCRIPTION: This loads a previously saved language model state. It reloads the '1epoch' model from the file created with the `learn.save()` command. This assumes a learner has been initialized, and the model's state can then be updated based on the loaded weights.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nlearn = learn.load('1epoch')\n```\n\n----------------------------------------\n\nTITLE: Defining Worker Initialization and FakeLoader Class - Python\nDESCRIPTION: Implements the _wif worker initialization function and the _FakeLoader data loader class for custom data iteration. _wif configures worker thread count, seeds, and dataset partitioning for reproducibility. _FakeLoader mimics key DataLoader functionality, handling batch generation, multiprocessing context, and attribute storage. Useful for iterables and streaming datasets where typical DataLoader semantics do not apply.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef _wif(worker_id):\n    set_num_threads(1)\n    info = get_worker_info()\n    ds = info.dataset.d\n    ds.num_workers,ds.offs = info.num_workers,info.id\n    set_seed(info.seed)\n    ds.wif()\n\nclass _FakeLoader:\n    def _fn_noops(self, x=None, *args, **kwargs): return x\n\n    _IterableDataset_len_called,_auto_collation,collate_fn,drop_last,in_order = None,False,_fn_noops,False,True\n    _index_sampler,generator,prefetch_factor,_get_shared_seed  = Inf.count,None,2,noop\n    dataset_kind = _dataset_kind = _DatasetKind.Iterable\n\n    def __init__(self, d, pin_memory, num_workers, timeout, persistent_workers,pin_memory_device):\n        self.dataset,self.default,self.worker_init_fn,self.pin_memory_device = self,d,_wif,pin_memory_device\n        store_attr('d,pin_memory,num_workers,timeout,persistent_workers,pin_memory_device')\n\n    def __iter__(self): return iter(self.d.create_batches(self.d.sample()))\n\n    @property\n    def multiprocessing_context(self): return (None,multiprocessing)[self.num_workers>0]\n\n    @contextmanager\n    def no_multiproc(self):\n        old_num_workers = self.num_workers\n        try:\n            self.num_workers = 0\n            yield self.d\n        finally: self.num_workers = old_num_workers\n\n_collate_types = (ndarray, Tensor, typing.Mapping, str)\n```\n\n----------------------------------------\n\nTITLE: Implementing IntToFloatTensor Class in Python for fastai\nDESCRIPTION: Defines a transformation class that converts integer tensors to float tensors with optional division (typically by 255 for images). Handles both regular images and masks with different encoding/decoding logic.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass IntToFloatTensor(DisplayedTransform):\n    \"Transform image to float tensor, optionally dividing by 255 (e.g. for images).\"\n    order = 10 #Need to run after PIL transforms on the GPU\n    def __init__(self, div=255., div_mask=1): store_attr()\n    def encodes(self, o:TensorImage): return o.float().div_(self.div)\n    def encodes(self, o:TensorMask ): return (o.long() / self.div_mask).long()\n    def decodes(self, o:TensorImage): return ((o.clamp(0., 1.) * self.div).long()) if self.div else o\n```\n\n----------------------------------------\n\nTITLE: Testing ReduceLROnPlateau with synthetic learners\nDESCRIPTION: Examples showing how ReduceLROnPlateau automatically reduces the learning rate when valid_loss stops improving, with different patience values and minimum learning rate settings.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner(n_trn=2)\nlearn.fit(n_epoch=4, lr=1e-7, cbs=ReduceLROnPlateau(monitor='valid_loss', min_delta=0.1, patience=2))\n```\n\n----------------------------------------\n\nTITLE: Viewing Processed Validation Data Head - Python\nDESCRIPTION: Displays the first few rows of the processed validation data within the `TabularPandas` object.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nto.valid.items.head()\n```\n\n----------------------------------------\n\nTITLE: Hooks Context Manager Example\nDESCRIPTION: This snippet demonstrates using the `Hooks` class as a context manager. It creates a sequential model with three layers, then creates a `Hooks` object that attaches a hook to each layer within a `with` statement. The hook function simply stores the output of each layer. It then performs a forward pass and verifies that the stored activations of each hook are equal to the output of the corresponding layer. When exiting the `with` statement, the hooks are automatically removed.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nlayers = [nn.Linear(5,10), nn.ReLU(), nn.Linear(10,3)]\ntst_model = nn.Sequential(*layers)\nwith Hooks(layers, lambda m,i,o: o) as h:\n    y = tst_model(x)\n    test_eq(h.stored[0], layers[0](x))\n    test_eq(h.stored[1], F.relu(layers[0](x)))\n    test_eq(h.stored[2], y)\n```\n\n----------------------------------------\n\nTITLE: Applying Transforms to All Dataset Items\nDESCRIPTION: This example applies trans-form functions to all dataset items, with support for train and validation splits. It verifies that transformations modify data as expected and that selection by boolean masks is functional.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_51\n\nLANGUAGE: Python\nCODE:\n```\n# apply transforms to all items\ntfm = [[lambda x: x*2,lambda x: x+1]]\nsplits = [[1,2],[0,3,4]]\ndsets = Datasets(range(5), tfm, splits=splits)\ntest_eq(dsets.train,[(3,),(5,)])\ntest_eq(dsets.valid,[(1,),(7,),(9,)])\ntest_eq(dsets.train[False,True], [(5,)])\n```\n\n----------------------------------------\n\nTITLE: Detecting Gradient Overflow Across Parameter Groups in fastai (Python)\nDESCRIPTION: Checks all parameters within a list of parameter groups (pgs) for gradient overflow using the `test_overflow` helper function. It iterates through each parameter (`p`) in each group (`pg`), checks if its gradient (`p.grad`) exists and contains overflow by calling `test_overflow` on `p.grad.data`. It returns True immediately if any overflow is detected, otherwise returns False after checking all parameters. This is crucial for implementing dynamic loss scaling in mixed-precision training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n#|export \ndef grad_overflow(pgs:list)->bool: \n    \"Tests all fp16 parameters in pgs for gradient overflow\"\n    for pg in pgs:\n        for p in pg:\n            if p.grad is not None and test_overflow(p.grad.data): return True\n    return False\n```\n\n----------------------------------------\n\nTITLE: Implementing R2 Score for Regression from Scikit-learn\nDESCRIPTION: Function that creates a fastai metric using scikit-learn's r2_score, adapting it to fastai's metric interface for regression tasks to measure prediction quality.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndef R2Score(sample_weight=None):\n    \"R2 score between predictions and targets\"\n    return skm_to_fastai(skm.r2_score, is_class=False, sample_weight=sample_weight)\n```\n\n----------------------------------------\n\nTITLE: Linear Layer Initialization\nDESCRIPTION: This function, `init_linear`, initializes a linear layer `m`. It initializes the bias with a normal distribution (mean 0, std `bias_std`) if `bias_std` is provided. It chooses a weight initialization method based on the activation function `act_func`: Kaiming uniform for ReLU/Leaky ReLU, or the `__default_init__` attribute of the activation function if available. If no suitable initializer is found, it does nothing.  Finally, it initializes the weights using the selected initialization function.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef init_linear(m, act_func=None, init='auto', bias_std=0.01):\n    if getattr(m,'bias',None) is not None and bias_std is not None:\n        if bias_std != 0: normal_(m.bias, 0, bias_std)\n        else: m.bias.data.zero_()\n    if init=='auto':\n        if act_func in (F.relu_,F.leaky_relu_): init = kaiming_uniform_\n        else: init = nested_callable(act_func, '__class__.__default_init__')\n        if init == noop: init = getcallable(act_func, '__default_init__')\n    if callable(init): init(m.weight)\n```\n\n----------------------------------------\n\nTITLE: Defining FocalLoss Module\nDESCRIPTION: Defines `FocalLoss` as a PyTorch `Module`. Applies focal loss, which down-weights easy-to-classify examples. The `forward` method calculates cross-entropy loss, computes `p_t`, and applies the focal loss formula based on the given `gamma` value.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass FocalLoss(Module):\n    y_int=True # y interpolation\n    def __init__(self, \n        gamma:float=2.0, # Focusing parameter. Higher values down-weight easy examples' contribution to loss\n        weight:Tensor=None, # Manual rescaling weight given to each class\n        reduction:str='mean' # PyTorch reduction to apply to the output\n    ): \n        \"Applies Focal Loss: https://arxiv.org/pdf/1708.02002.pdf\"\n        store_attr()\n    \n    def forward(self, inp:Tensor, targ:Tensor) -> Tensor:\n        \"Applies focal loss based on https://arxiv.org/pdf/1708.02002.pdf\"\n        ce_loss = F.cross_entropy(inp, targ, weight=self.weight, reduction=\"none\")\n        p_t = torch.exp(-ce_loss)\n        loss = (1 - p_t)**self.gamma * ce_loss\n        if self.reduction == \"mean\":\n            loss = loss.mean()\n        elif self.reduction == \"sum\":\n            loss = loss.sum()\n        return loss\n```\n\n----------------------------------------\n\nTITLE: Downloading MNIST Dataset\nDESCRIPTION: This code downloads the MNIST dataset using `untar_data` from the fastai library. The `URLs.MNIST` provides the URL. The downloaded data is stored in a local directory represented by the `path` variable. The output is the local path to the extracted data.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npath = untar_data(URLs.MNIST)\n```\n\n----------------------------------------\n\nTITLE: Running Manual Training Loop (Adam Optimizer) - PyTorch/Python\nDESCRIPTION: Executes the *new* `update` function (which uses the Adam optimizer) for every batch in the training DataLoader, using the latest `Mnist_NN` model instance. This trains the neural network for one epoch using Adam with a learning rate of 1e-3. The loss for each batch is collected in the `losses` list.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nlosses = [update(x,y,1e-3) for x,y in dls.train]\n```\n\n----------------------------------------\n\nTITLE: Defining DiceLoss Class for Segmentation - Fastai/PyTorch\nDESCRIPTION: This class implements a Dice loss function specifically for segmentation tasks. It calculates the Dice score (related to Intersection over Union) between model predictions (after softmax) and one-hot encoded targets, returning `1 - Dice Score`. It includes options for smoothing and squaring terms in the union calculation, plus `activation` and `decodes` methods.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass DiceLoss:\n    \"Dice loss for segmentation\"\n    def __init__(self, \n        axis:int=1, # Class axis\n        smooth:float=1e-6, # Helps with numerical stabilities in the IoU division\n        reduction:str=\"sum\", # PyTorch reduction to apply to the output\n        square_in_union:bool=False # Squares predictions to increase slope of gradients\n    ):\n        store_attr()\n        \n    def __call__(self, pred:Tensor, targ:Tensor) -> Tensor:\n        \"One-hot encodes targ, then runs IoU calculation then takes 1-dice value\"\n        targ = self._one_hot(targ, pred.shape[self.axis])\n        pred, targ = TensorBase(pred), TensorBase(targ)\n        assert pred.shape == targ.shape, 'input and target dimensions differ, DiceLoss expects non one-hot targs'\n        pred = self.activation(pred)\n        sum_dims = list(range(2, len(pred.shape)))\n        inter = torch.sum(pred*targ, dim=sum_dims)        \n        union = (torch.sum(pred**2+targ, dim=sum_dims) if self.square_in_union\n            else torch.sum(pred+targ, dim=sum_dims))\n        dice_score = (2. * inter + self.smooth)/(union + self.smooth)\n        loss = 1- dice_score\n        if self.reduction == 'mean':\n            loss = loss.mean()\n        elif self.reduction == 'sum':\n            loss = loss.sum()\n        return loss\n    @staticmethod\n    def _one_hot(\n        x:Tensor, # Non one-hot encoded targs\n        classes:int, # The number of classes \n        axis:int=1 # The axis to stack for encoding (class dimension)\n    ) -> Tensor:\n        \"Creates one binary mask per class\"\n        return torch.stack([torch.where(x==c, 1, 0) for c in range(classes)], axis=axis)\n    \n    def activation(self, x:Tensor) -> Tensor: \n        \"Activation function applied to model output\"\n        return F.softmax(x, dim=self.axis)\n    \n    def decodes(self, x:Tensor) -> Tensor:\n        \"Converts model output to target format\"\n        return x.argmax(dim=self.axis)\n```\n\n----------------------------------------\n\nTITLE: Downloading and listing dataset files\nDESCRIPTION: This code demonstrates how to download a dataset using `untar_data` and then list the files within the downloaded directory. `untar_data` downloads and extracts the specified dataset.  `path.ls()` returns a list of Path objects representing the files and subdirectories within the dataset directory.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/04_data.external.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.PETS)\npath.ls()\n```\n\n----------------------------------------\n\nTITLE: Implementing PointScaler Transform for Point Coordinate Normalization - fastai (Python)\nDESCRIPTION: Defines the PointScaler class used to transform and untransform points for deep learning pipelines, tracking image sizes and handling y/x flipping. Integrates with fastai's Transform workflow and expects tensor or image input. Depends on TensorPoint, PILBase, TensorImageBase, and various fastai utilities. Output is either size-annotated points or unchanged input for other types.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nclass PointScaler(Transform):\n    \"Scale a tensor representing points\"\n    order = 1\n    def __init__(self, do_scale=True, y_first=False): self.do_scale,self.y_first = do_scale,y_first\n    def _grab_sz(self, x):\n        self.sz = [x.shape[-1], x.shape[-2]] if isinstance(x, Tensor) else x.size\n        return x\n\n    def _get_sz(self, x): return getattr(x, 'img_size') if self.sz is None else self.sz\n\n    def setups(self, dl):\n        res = first(dl.do_item(None), risinstance(TensorPoint))\n        if res is not None: self.c = res.numel()\n\n    def encodes(self, x:PILBase|TensorImageBase): return self._grab_sz(x)\n    def decodes(self, x:PILBase|TensorImageBase): return self._grab_sz(x)\n\n    def encodes(self, x:TensorPoint): return _scale_pnts(x, self._get_sz(x), self.do_scale, self.y_first)\n    def decodes(self, x:TensorPoint): return _unscale_pnts(x.view(-1, 2), self._get_sz(x))\n```\n\n----------------------------------------\n\nTITLE: Importing Spacy and Defining Wrapper - Python\nDESCRIPTION: This code block imports the `English` language model from Spacy and defines two helper components. `conv_sp` is a function to convert a Spacy Doc or Token object to a list of strings. `SpTok` is a class that wraps Spacy's tokenizer, initializing the English model and tokenizer in its constructor (`__init__`) and providing a `__call__` method to tokenize a string and return the results converted to strings.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.en import English\n\ndef conv_sp(doc): return L(doc).map(str)\n\nclass SpTok:\n    def __init__(self):\n        nlp = English()\n        self.tok = nlp.Defaults.create_tokenizer(nlp)\n    \n    def __call__(self, x): return L(self.tok(str(x))).map(conv_sp)\n```\n\n----------------------------------------\n\nTITLE: Verify Loss Calculation and Matching for Classification\nDESCRIPTION: This code snippet verifies that the stored losses equal calculated losses. It uses an `Interpretation` object (`interp`) to get the top losses and corresponding indices. It then creates a temporary `DataLoader` (`tmp_dl`) to compute the losses for the selected items. Finally, it compares the `top_losses` with the newly computed losses using `test_close`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# verify stored losses equal calculated losses for idx\ntop_losses, idx = interp.top_losses(9)\n\ndl = test_learner.dls[1].new(shuffle=False, drop_last=False)\nitems = getattr(dl.items, 'iloc', L(dl.items))[idx]\ntmp_dl = test_learner.dls.test_dl(items, with_labels=True, process=not isinstance(dl, TabDataLoader))\n_, _, _, _, losses = test_learner.get_preds(dl=tmp_dl, with_input=True, with_loss=True, \n                                            with_decoded=True, act=None, reorder=False)\n\ntest_close(top_losses, losses, 1e-2)\n```\n\n----------------------------------------\n\nTITLE: Decoding One-Hot Encoded Tensors to Class Indices or Vocabulary Items in Python\nDESCRIPTION: Converts a one-hot encoded tensor or vector back to indices or optionally mapped vocabulary tokens. Iterates over the tensor, returning positions where the value is 1, allowing transformation back from a sparse one-hot representation to class labels.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_75\n\nLANGUAGE: python\nCODE:\n```\ndef one_hot_decode(x, vocab=None):\n    return L(vocab[i] if vocab else i for i,x_ in enumerate(x) if x_==1)\n```\n\n----------------------------------------\n\nTITLE: Type-Preserving Collate Function for Batches - Python\nDESCRIPTION: Defines fa_collate, an enhanced version of PyTorch's default_collate, which preserves types for sequences, mappings, and standard tensor types. This collate function recursively processes sequences, maintaining structure in nested data and accommodating additional collatable types for batch creation. Used in DataLoader to facilitate custom batch structure for complex data, requiring numpy, torch, and fastai's L utility.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef fa_collate(t):\n    \"A replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s\"\n    b = t[0]\n    return (default_collate(t) if isinstance(b, _collate_types)\n            else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence)\n            else default_collate(t))\n```\n\n----------------------------------------\n\nTITLE: Exporting Modules with nbdev Export in Python Notebooks\nDESCRIPTION: Includes a minimal export script invoking nbdev's nbdev_export function to bundle and export notebook-defined classes and functions into Python modules. This step is essential for project packaging and distribution using the nbdev framework. No dependencies besides nbdev are required, and it is usually run at the end of a notebook to compile all code cells.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Test ClassificationInterpretation Class Functionality\nDESCRIPTION: This is a basic test to confirm the `ClassificationInterpretation` class functions correctly. It creates an instance of `ClassificationInterpretation`, calls the `confusion_matrix` method, and doesn't perform any assertions (the test validates that there are no errors).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n# simple test to make sure ClassificationInterpretation works\ninterp = ClassificationInterpretation.from_learner(test_learner)\ncm = interp.confusion_matrix()\n```\n\n----------------------------------------\n\nTITLE: Training the Classifier Head (Python)\nDESCRIPTION: Trains only the final layer (the classifier head) of the model for one epoch using the `fit_one_cycle` method with a learning rate of 2e-2. The pre-trained encoder layers remain frozen during this initial training phase.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(1, 2e-2)\n```\n\n----------------------------------------\n\nTITLE: Adding Documentation to Fastai Learner Class and Methods - Python\nDESCRIPTION: This snippet uses the fastai utility function `add_docs` to attach documentation strings (docstrings) to the `Learner` class itself and several of its methods. This is a mechanism within fastai to programmatically add documentation, often used in conjunction with notebook exports or inline documentation tools. Each key-value pair after the class description maps a method name to its documentation string.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n#|export\nadd_docs(Learner, \"Group together a `model`, some `dls` and a `loss_func` to handle training\",\n    add_cbs=\"Add `cbs` to the list of `Callback` and register `self` as their learner\",\n    add_cb=\"Add `cb` to the list of `Callback` and register `self` as their learner\",\n    remove_cbs=\"Remove `cbs` from the list of `Callback` and deregister `self` as their learner\",\n    remove_cb=\"Add `cb` from the list of `Callback` and deregister `self` as their learner\",\n    added_cbs=\"Context manage that temporarily adds `cbs`\",\n    removed_cbs=\"Context manage that temporarily removes `cbs`\",\n    ordered_cbs=\"List of `Callback`s, in order, for an `event` in the training loop\",\n    create_opt=\"Create an optimizer with default hyper-parameters\",\n    one_batch=\"Train or evaluate `self.model` on batch `(xb,yb)`\",\n    all_batches=\"Train or evaluate `self.model` on all the batches of `self.dl`\",\n    fit=\"Fit `self.model` for `n_epoch` using `cbs`. Optionally `reset_opt`.\",\n    validate=\"Validate on `dl` with potential new `cbs`.\",\n    get_preds=\"Get the predictions and targets on the `ds_idx`-th dbunchset or `dl`, optionally `with_input` and `with_loss`\",\n    predict=\"Prediction on `item`, fully decoded, loss function decoded and probabilities\",\n    validation_context=\"A `ContextManagers` suitable for validation, with optional `cbs`\",\n    show_results=\"Show some predictions on `ds_idx`-th dataset or `dl`\",\n    show_training_loop=\"Show each step in the training loop\",\n    no_logging=\"Context manager to temporarily remove `logger`\",\n    no_mbar=\"Context manager to temporarily prevent the master progress bar from being created\",\n    loss_not_reduced=\"A context manager to evaluate `loss_func` with reduction set to none.\",\n    to_detach=\"Calls `to_detach` if `self.dl` provides a `.to_detach` function otherwise calls global `to_detach`\",\n    __call__=\"Call `event_name` for all `Callback`s in `self.cbs`\"\n)\n```\n\n----------------------------------------\n\nTITLE: Download Image Inner Function\nDESCRIPTION: This internal function downloads a single image from a given URL. It handles the process of downloading an image, including extracting the filename and suffix from the URL and saving it to the specified destination. It uses the `download_url` function for the actual download and handles potential exceptions during the download process. It optionally preserves the original filename.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef _download_image_inner(dest, inp, timeout=4, preserve_filename=False):\n    i,url = inp\n    url = url.split(\"?\")[0]\n    url_path = Path(url)\n    suffix = url_path.suffix if url_path.suffix else '.jpg'\n    name = _get_downloaded_image_filename(dest, url_path.stem, suffix) if preserve_filename else str(uuid.uuid4())\n    try: download_url(url, dest/f\"{name}{suffix}\", show_progress=False, timeout=timeout)\n    except Exception as e: f\"Couldn't download {url}.\"\n```\n\n----------------------------------------\n\nTITLE: Load Classifier (Second Layer Group)\nDESCRIPTION: Loads the saved model from the second training stage.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nlearn.load('second');\n```\n\n----------------------------------------\n\nTITLE: Exporting callback.tracker module\nDESCRIPTION: This code specifies that the current module should be exported as `callback.tracker` when using the `nbdev` library for creating documentation and libraries from Jupyter Notebooks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp callback.tracker\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Datasets\nDESCRIPTION: This code is used to create a small custom dataset using a single image file repeated many times for demonstration. It also specifies a custom split (all items in the training set). This allows for creating a smaller and more manageable dataset for experimentation or testing purposes.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ndsrc1 = Datasets([items[0]]*128, tfms=[[PILImageBW.create], [parent_label, Categorize]], splits=[list(range(128)), []])\n```\n\n----------------------------------------\n\nTITLE: Adding Documentation Strings to TfmdLists Methods in fastai (Python)\nDESCRIPTION: Uses the `add_docs` function to programmatically add or update documentation strings (docstrings) for specified methods of the `TfmdLists` class. This enhances the built-in help and documentation for methods like `setup`, `decode`, `show`, `subset`, `infer_idx`, `infer`, `new_empty`, and `overlapping_splits`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n#|export\nadd_docs(TfmdLists,\n         setup=\"Transform setup with self\",\n         decode=\"From `Pipeline`\",\n         show=\"From `Pipeline`\",\n         overlapping_splits=\"All splits that are in more than one split\",\n         subset=\"New `TfmdLists` with same tfms that only includes items in `i`th split\",\n         infer_idx=\"Finds the index where `self.tfms` can be applied to `x`, depending on the type of `x`\",\n         infer=\"Apply `self.tfms` to `x` starting at the right tfm depending on the type of `x`\",\n         new_empty=\"A new version of `self` but with no items\")\n```\n\n----------------------------------------\n\nTITLE: Implementing the Recorder Callback for Training Statistics in fastai (Python)\nDESCRIPTION: This code defines the 'Recorder' class, a fastai Callback that registers and logs learning rate, loss, and custom metrics during training and validation. It stores metrics in state attributes, handles configuration for logging train and validation metrics, provides hooks for events (before/after batch, epoch, train/validate), supports plotting loss curves with various options (log scale, epochs marked), and integrates into fastai Learner workflows. Dependencies: fastai Callback/Metric classes, matplotlib/NumPy for plotting, and the 'format_time' utility. Parameters include toggles for train/valid metrics, 'beta' for smoothing, and plotting controls. Expects to be attached to a 'Learner' and will update its attributes (e.g., 'smooth_loss') during training/validation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_67\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass Recorder(Callback):\n    \"Callback that registers statistics (lr, loss and metrics) during training\"\n    _stateattrs=('lrs','iters','losses','values')\n    remove_on_fetch,order = True,50\n\n    def __init__(self, add_time=True, train_metrics=False, valid_metrics=True, beta=0.98):\n        store_attr('add_time,train_metrics,valid_metrics')\n        self.loss,self.smooth_loss = AvgLoss(),AvgSmoothLoss(beta=beta)\n\n    def before_fit(self):\n        \"Prepare state for training\"\n        self.lrs,self.iters,self.losses,self.values = [],[],[],[]\n        names = self.metrics.attrgot('name')\n        if self.train_metrics and self.valid_metrics:\n            names = L('loss') + names\n            names = names.map('train_{}') + names.map('valid_{}')\n        elif self.valid_metrics: names = L('train_loss', 'valid_loss') + names\n        else: names = L('train_loss') + names\n        if self.add_time: names.append('time')\n        self.metric_names = 'epoch'+names\n        self.smooth_loss.reset()\n\n    def after_batch(self):\n        \"Update all metrics and records lr and smooth loss in training\"\n        if len(self.yb) == 0: return\n        mets = self._train_mets if self.training else self._valid_mets\n        for met in mets: met.accumulate(self.learn)\n        if not self.training: return\n        self.lrs.append(self.opt.hypers[-1]['lr'])\n        self.losses.append(self.smooth_loss.value)\n        self.learn.smooth_loss = self.smooth_loss.value\n\n    def before_epoch(self):\n        \"Set timer if `self.add_time=True`\"\n        self.cancel_train,self.cancel_valid = False,False\n        if self.add_time: self.start_epoch = time.time()\n        self.log = L(getattr(self, 'epoch', 0))\n\n    def before_train   (self): self._train_mets[1:].map(Self.reset())\n    def before_validate(self): self._valid_mets.map(Self.reset())\n    def after_train   (self): self.log += self._train_mets.map(_maybe_item)\n    def after_validate(self): self.log += self._valid_mets.map(_maybe_item)\n    def after_cancel_train(self):    self.cancel_train = True\n    def after_cancel_validate(self): self.cancel_valid = True\n\n    def after_epoch(self):\n        \"Store and log the loss/metric values\"\n        self.learn.final_record = self.log[1:].copy()\n        self.values.append(self.learn.final_record)\n        if self.add_time: self.log.append(format_time(time.time() - self.start_epoch))\n        self.logger(self.log)\n        self.iters.append(self.smooth_loss.count)\n\n    @property\n    def _train_mets(self):\n        if getattr(self, 'cancel_train', False): return L()\n        return L(self.smooth_loss) + (self.metrics if self.train_metrics else L())\n\n    @property\n    def _valid_mets(self):\n        if getattr(self, 'cancel_valid', False): return L()\n        return (L(self.loss) + self.metrics if self.valid_metrics else L())\n\n    def plot_loss(self, skip_start=5, with_valid=True, log=False, show_epochs=False, ax=None):\n        if not ax:\n            ax=plt.gca()\n        if log:\n            ax.loglog(list(range(skip_start, len(self.losses))), self.losses[skip_start:], label='train')\n        else:\n            ax.plot(list(range(skip_start, len(self.losses))), self.losses[skip_start:], label='train')\n        if show_epochs:\n            for x in self.iters:\n                ax.axvline(x, color='grey', ls=':')\n        ax.set_ylabel('loss')\n        ax.set_xlabel('steps')\n        ax.set_title('learning curve')\n        if with_valid:\n            idx = (np.array(self.iters)<skip_start).sum()\n            valid_col = self.metric_names.index('valid_loss') - 1 \n            ax.plot(self.iters[idx:], L(self.values[idx:]).itemgot(valid_col), label='valid')\n            ax.legend()\n        return ax\n```\n\n----------------------------------------\n\nTITLE: Verifying Event Firing for Before Epoch in fastai Learner (Python)\nDESCRIPTION: Checks that running the Learner triggers the 'before_fit' and 'before_epoch' events in order and that the initial loss is set to zero. Utilizes VerboseCallback and test_stdout for output verification, and fastai's tensor utility for loss initialization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nlearn = synth_learner(n_train=5, cbs=VerboseCallback())\ntest_stdout(lambda: learn(_before_epoch), 'before_fit\\nbefore_epoch')\ntest_eq(learn.loss, tensor(0.))\n```\n\n----------------------------------------\n\nTITLE: Displaying recommendation results\nDESCRIPTION: This code displays the results of the collaborative filtering model using the `show_results` method. It shows the user and movie IDs, the actual ratings, and the model's predictions, allowing for the evaluation of the model's performance.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/quick_start.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nlearn.show_results()\n```\n\n----------------------------------------\n\nTITLE: Upgrading fastai on Colab using Shell in Python\nDESCRIPTION: This snippet conditionally upgrades the fastai library on Google Colab only if the '/content' directory exists, signaling the Colab environment. It uses a shell command inside a Python notebook with the '!' prefix. The command installs fastai quietly and forces upgrade.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/43_tabular.learner.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Importing Parameter Vectorization Utility from PyTorch Python\nDESCRIPTION: This import statement brings in the `parameters_to_vector` function from PyTorch's `nn.utils`. This function is used specifically when creating a flattened master copy of parameters for potential performance benefits during the optimizer step.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom torch.nn.utils import parameters_to_vector\n```\n\n----------------------------------------\n\nTITLE: Plotting Top Losses for Text Classification in FastAI\nDESCRIPTION: Defines `plot_top_losses` for text classification tasks (`TensorText` input, `TensorCategory` target). It identifies samples with the highest prediction losses, truncates the input text (`trunc_at`), and displays them in a Pandas DataFrame. The table includes the input text, target label, predicted label, prediction probability (for the predicted class), and the actual loss value.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@dispatch\ndef plot_top_losses(x: TensorText, y:TensorCategory, samples, outs, raws, losses, trunc_at=150, **kwargs):\n    rows = get_empty_df(len(samples))\n    samples = L((s[0].truncate(trunc_at),*s[1:]) for s in samples)\n    for i,l in enumerate(['input', 'target']):\n        rows = [b.show(ctx=c, label=l, **kwargs) for b,c in zip(samples.itemgot(i),rows)]\n    outs = L(o + (TitledFloat(r.max().item()), TitledFloat(l.item())) for o,r,l in zip(outs, raws, losses))\n    for i,l in enumerate(['predicted', 'probability', 'loss']):\n        rows = [b.show(ctx=c, label=l, **kwargs) for b,c in zip(outs.itemgot(i),rows)]\n    display_df(pd.DataFrame(rows))\n```\n\n----------------------------------------\n\nTITLE: Show Results for Image Classification\nDESCRIPTION: This function displays predicted versus actual results for image classification, arranging the results in a grid. It accepts a batch of images, labels, samples, outputs, and context, then plots them with appropriate labels. Pattern: visualization utility for model predictions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_39\n\nLANGUAGE: Python\nCODE:\n```\n@dispatch\ndef show_results(x:TensorImage, y, samples, outs, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=None, **kwargs):\n    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), nrows=nrows, ncols=ncols, figsize=figsize)\n    ctxs = show_results[object](x, y, samples, outs, ctxs=ctxs, max_n=max_n, **kwargs)\n    return ctxs\n```\n\n----------------------------------------\n\nTITLE: Loading Image Files from PETS Dataset\nDESCRIPTION: This snippet loads the PETS dataset using 'untar_data' and prepares image files for training with 'get_image_files'. It then creates an 'ImageDataLoaders' object with resizing transforms. Dependencies include fastai.vision and fastai.data, expected inputs are dataset URL and path, output is a data loader ready for training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\npath = untar_data(URLs.PETS)\nfnames = get_image_files(path/'images')\npat = r'^(.*)_\\d+.jpg$'\ndls = ImageDataLoaders.from_name_re(path, fnames, pat, item_tfms=Resize(224))\n```\n\n----------------------------------------\n\nTITLE: Dispatching Show Batch Methods for TensorImage in fastai Vision Using Python\nDESCRIPTION: Defines two overloaded `show_batch` functions using the `@dispatch` decorator for fastai vision. One handles generic samples and calls a type-dispatched show function; the other is specialized for input (`x`) and target (`y`) both as `TensorImage` to display images in paired grids. They arrange matplotlib axes grids using `get_grid` and invoke show methods on batches of images. Parameters include samples, matplotlib contexts, maximum number of items displayed, grid layout, figure size, and additional display kwargs. These functions enable flexible visualization of input-target image batches.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/08_vision.data.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dispatch\ndef show_batch(x:TensorImage, y, samples, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=None, **kwargs):\n    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), nrows=nrows, ncols=ncols, figsize=figsize)\n    return get_show_batch_func(object)(x, y, samples, ctxs=ctxs, max_n=max_n, **kwargs)\n```\n\nLANGUAGE: python\nCODE:\n```\n@dispatch\ndef show_batch(x:TensorImage, y:TensorImage, samples, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=None, **kwargs):\n    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), nrows=nrows, ncols=ncols, figsize=figsize, double=True)\n    for i in range(2):\n        ctxs[i::2] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs[i::2],range(max_n))]\n    return ctxs\n```\n\n----------------------------------------\n\nTITLE: Discriminative Learning Rate and plot_sched Test\nDESCRIPTION: This snippet provides another example of discriminative learning rates and tests the `plot_sched` method. It defines a custom `_splitter` function for discriminative learning and trains a model using the `fit_one_cycle` policy. It then optionally includes commented-out code that attempts to verify that the learning rates have the expected values, though it is disabled in the current state.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#test discriminative lrs\ndef _splitter(m): return [[m.a], [m.b]]\nlearn = synth_learner(splitter=_splitter)\nlearn.fit_one_cycle(1, lr_max=slice(1e-3,1e-2))\n#n = len(learn.dls.train)\n#est_close(learn.recorder.hps['lr'], [1e-3 + (1e-2-1e-3) * i/n for i in range(n)])\n```\n\n----------------------------------------\n\nTITLE: Defining MCDropoutCallback for fastai\nDESCRIPTION: Defines a fastai Callback named `MCDropoutCallback`. This callback forces dropout layers within the model to remain in training mode (`m.train()`) during the validation phase (`before_validate`) and resets them to evaluation mode (`m.eval()`) afterwards (`after_validate`). This enables Monte Carlo Dropout for uncertainty estimation during inference via `get_preds`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18b_callback.preds.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass MCDropoutCallback(Callback):\n    def before_validate(self):\n        for m in [m for m in flatten_model(self.model) if 'dropout' in m.__class__.__name__.lower()]:\n            m.train()\n    \n    def after_validate(self):\n        for m in [m for m in flatten_model(self.model) if 'dropout' in m.__class__.__name__.lower()]:\n            m.eval()\n```\n\n----------------------------------------\n\nTITLE: Filtering Rows with Missing Values - Python\nDESCRIPTION: Filters the processed training data to show rows where the `FillMissing` processor created an indicator column (`_na`) indicating the original value was missing for 'CompetitionDistance'.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nto.train.items[to.train.items['CompetitionDistance_na'] == True]\n```\n\n----------------------------------------\n\nTITLE: Get movie weights\nDESCRIPTION: Retrieves the weight vectors for the top movies from the model. These weights represent the learned feature embeddings for each movie. `is_item=True` specifies that we want weights for items (movies).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nmovie_w = learn.model.weight(top_movies, is_item=True)\nmovie_w.shape\n```\n\n----------------------------------------\n\nTITLE: Defining MNIST Dataset Transform Pipeline with Torchvision Transforms in Python\nDESCRIPTION: Creates a composed transform pipeline to convert raw MNIST images to tensors and normalize their pixel values with dataset-specific mean and standard deviation. This ensures input data are in a normalized tensor format suitable for model training. It depends on torchvision.transforms.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_pytorch_verbose.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntfms = transforms.Compose([transforms.ToTensor(),\n                                 transforms.Normalize((0.1307,), (0.3081))\n])\n```\n\n----------------------------------------\n\nTITLE: Instantiating a Collaborative Filtering Learner - fastai - Python\nDESCRIPTION: This example shows how to instantiate a `Learner` for collaborative filtering using the `collab_learner` function with a specified `y_range`. It demonstrates the basic usage of the function with the `dls` (DataLoaders) object and the desired output range.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/45_collab.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlearn = collab_learner(dls, y_range=(0,5))\n```\n\n----------------------------------------\n\nTITLE: Defining CombinedLoss Class in fastai/PyTorch\nDESCRIPTION: Defines a custom loss function class `CombinedLoss` that combines `FocalLossFlat` and `DiceLoss`. The `__call__` method computes the sum of the focal loss and the dice loss (scaled by `alpha`), providing a combined metric useful for segmentation tasks. Requires fastai's loss modules and PyTorch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nclass CombinedLoss:\n    \"Dice and Focal combined\"\n    def __init__(self, axis=1, smooth=1., alpha=1.):\n        store_attr()\n        self.focal_loss = FocalLossFlat(axis=axis)\n        self.dice_loss =  DiceLoss(axis, smooth)\n        \n    def __call__(self, pred, targ):\n        return self.focal_loss(pred, targ) + self.alpha * self.dice_loss(pred, targ)\n    \n    def decodes(self, x):    return x.argmax(dim=self.axis)\n    def activation(self, x): return F.softmax(x, dim=self.axis)\n```\n\n----------------------------------------\n\nTITLE: hook_outputs Function Definition\nDESCRIPTION: This code defines the `hook_outputs` function, which returns a `Hooks` object that stores the activations of a list of modules in the `stored` attribute of each hook. It uses the `_hook_inner` function as the default hook function. The `detach`, `cpu`, and `grad` parameters control whether the activations are detached from their history, moved to the CPU, or whether to store gradients instead of outputs, respectively.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef hook_outputs(modules, detach=True, cpu=False, grad=False):\n    \"Return `Hooks` that store activations of all `modules` in `self.stored`\"\n    return Hooks(modules, _hook_inner, detach=detach, cpu=cpu, is_forward=not grad)\n```\n\n----------------------------------------\n\nTITLE: Type Inference and Annotations in TfmdLists\nDESCRIPTION: This snippet tests the ability of TfmdLists to infer input types based on input data or annotations in transform functions. It verifies that type inference works correctly with both string and float inputs, and with annotated functions, ensuring robust input handling.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_40\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n#Test input_types works on a Transform\ncat = _Cat()\ncat.input_types = (str, float)\ntl = TfmdLists(fns, [_lbl,cat,mult])\ntest_eq(tl.infer_idx(2.0), 1)\n\n#Test type annotations work on a function\ndef mult(x:int|float): return x*2\nmult.order = 2\ntl = TfmdLists(fns, [_lbl,_Cat(),mult])\ntest_eq(tl.infer_idx(2.0), 2)\n```\n\n----------------------------------------\n\nTITLE: TabularGPU Pipeline Instantiation Timing\nDESCRIPTION: This measures the time it takes to run tabular data processing pipeline by making use of %time, and creating TabularGPU object for the adult dataset.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n%time to = TabularGPU(df_trn, procs, splits=splits, cat_names=cat_names, cont_names=cont_names, y_names=\"salary\")\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch and Torchvision Modules for Dataset Preparation using Python\nDESCRIPTION: Imports core PyTorch and torchvision modules along with torchvision transforms required for dataset loading and preprocessing. These imports set up functionality to download and transform the MNIST dataset into normalized tensor images suitable for training. Dependencies include pytorch and torchvision libraries.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_pytorch_verbose.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch, torchvision\nimport torchvision.transforms as transforms\n```\n\n----------------------------------------\n\nTITLE: Importing fastai and Accelerate libraries\nDESCRIPTION: Imports all necessary modules from fastai's vision, text, tabular, and collab packages, as well as Accelerate's notebook_launcher for multi-GPU training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/distributed_app_examples.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.vision.all import *\nfrom fastai.text.all import *\nfrom fastai.tabular.all import *\nfrom fastai.collab import *\n\nfrom accelerate import notebook_launcher\nfrom fastai.distributed import *\n```\n\n----------------------------------------\n\nTITLE: Adding BF16 Mixed Precision to Learner in fastai Python\nDESCRIPTION: This patch adds a convenience method `to_bf16` to the `Learner` class, enabling BF16 mixed precision training. It works by adding the `MixedPrecision` callback to the learner and explicitly setting the `amp_mode` to `AMPMode.BF16`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\ndef to_bf16(self:Learner):\n    \"Set `Learner` to bfloat16 mixed precision using PyTorch AMP\"\n    return self.add_cb(MixedPrecision(amp_mode=AMPMode.BF16))\n```\n\n----------------------------------------\n\nTITLE: Dataset with Multiple Transforms and Validation Transformation Application\nDESCRIPTION: Creates a dataset with range 0-7, applying different transforms with specific split indices. Demonstrates applying validation transforms to generate a test set for inference, validating functional transformations.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_69\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n#Test with different types\ntfm = _Tfm1()\ntfm.split_idx,tfm.order = None,2\ndsets = Datasets(['dog', 'cat', 'cat', 'dog'], [[_Cat(),tfm]])\n\n#With strings\ntest_eq(test_set(dsets, ['dog', 'cat', 'cat']), [(3,), (0,), (0,)])\n#With ints\ntest_eq(test_set(dsets, [1,2]), [(3,), (6,)])\n```\n\n----------------------------------------\n\nTITLE: Defining TextBlock for Fastai DataBlock API - Python\nDESCRIPTION: This snippet defines the `TextBlock` class, a specialized `TransformBlock` for handling text data. It includes the `__init__` method which sets up transformations like tokenization and numericalization, and configures the appropriate DataLoader type (`LMDataLoader` or `SortedDL`). It also includes factory methods (`from_df`, `from_folder`) for building the block from common data sources.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nclass TextBlock(TransformBlock):\n    \"A `TransformBlock` for texts\"\n    @delegates(Numericalize.__init__)\n    def __init__(self, tok_tfm, vocab=None, is_lm=False, seq_len=72, backwards=False, **kwargs):\n        type_tfms = [tok_tfm, Numericalize(vocab, **kwargs)]\n        if backwards: type_tfms += [reverse_text]\n        return super().__init__(type_tfms=type_tfms,\n                                dl_type=LMDataLoader if is_lm else SortedDL,\n                                dls_kwargs={'seq_len': seq_len} if is_lm else {'before_batch': Pad_Chunk(seq_len=seq_len)})\n\n    @classmethod\n    @delegates(Tokenizer.from_df, keep=True)\n    def from_df(cls, text_cols, vocab=None, is_lm=False, seq_len=72, backwards=False, min_freq=3, max_vocab=60000, **kwargs):\n        \"Build a `TextBlock` from a dataframe using `text_cols`\"\n        return cls(Tokenizer.from_df(text_cols, **kwargs), vocab=vocab, is_lm=is_lm, seq_len=seq_len,\n                   backwards=backwards, min_freq=min_freq, max_vocab=max_vocab)\n\n    @classmethod\n    @delegates(Tokenizer.from_folder, keep=True)\n    def from_folder(cls, path, vocab=None, is_lm=False, seq_len=72, backwards=False, min_freq=3, max_vocab=60000, **kwargs):\n        \"Build a `TextBlock` from a `path`\"\n        return cls(Tokenizer.from_folder(path, **kwargs), vocab=vocab, is_lm=is_lm, seq_len=seq_len,\n                   backwards=backwards, min_freq=min_freq, max_vocab=max_vocab)\n```\n\n----------------------------------------\n\nTITLE: Displaying Language Model Results in FastAI (LMTensorText)\nDESCRIPTION: Provides a specialized `show_results` implementation for language modeling tasks where the input is `LMTensorText`. It formats and displays the input text, target text, and predicted text for a given set of samples (`samples`) and model outputs (`outs`) in a Pandas DataFrame for easy comparison. It uses the `show` method of the underlying text objects.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@dispatch\ndef show_results(x: LMTensorText, y, samples, outs, ctxs=None, max_n=10, **kwargs):\n    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n    for i,l in enumerate(['input', 'target']):\n        ctxs = [b.show(ctx=c, label=l, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]\n    ctxs = [b.show(ctx=c, label='pred', **kwargs) for b,c,_ in zip(outs.itemgot(0),ctxs,range(max_n))]\n    display_df(pd.DataFrame(ctxs))\n    return ctxs\n```\n\n----------------------------------------\n\nTITLE: Constructing Classification Head for Vision Learners with Configurable Layers Using FastAI in Python\nDESCRIPTION: This 'create_head' function creates a model head for classification from extracted features of size 'nf' to output size 'n_out', using optional linear layers defined by 'lin_ftrs'. It applies either adaptive concat pooling or average pooling, optional batch normalizations, dropout layers with probabilities 'ps', and activation functions with options for layer order control and output range limiting (e.g., sigmoid range). This head is compatible with FastAI Learners' transfer learning pipelines.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef create_head(nf, n_out, lin_ftrs=None, ps=0.5, pool=True, concat_pool=True, first_bn=True, bn_final=False,\n                lin_first=False, y_range=None):\n    \"Model head that takes `nf` features, runs through `lin_ftrs`, and out `n_out` classes.\"\n    if pool and concat_pool: nf *= 2\n    lin_ftrs = [nf, 512, n_out] if lin_ftrs is None else [nf] + lin_ftrs + [n_out]\n    bns = [first_bn] + [True]*len(lin_ftrs[1:])\n    ps = L(ps)\n    if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n    actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs)-2) + [None]\n    layers = []\n    if pool:\n        pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)\n        layers += [pool, Flatten()]\n    if lin_first: layers.append(nn.Dropout(ps.pop(0)))\n    for ni,no,bn,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], bns, ps, actns):\n        layers += LinBnDrop(ni, no, bn=bn, p=p, act=actn, lin_first=lin_first)\n    if lin_first: layers.append(nn.Linear(lin_ftrs[-2], n_out))\n    if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n    if y_range is not None: layers.append(SigmoidRange(*y_range))\n    return nn.Sequential(*layers)\n```\n\n----------------------------------------\n\nTITLE: Defining Time Series Data Splits - Python\nDESCRIPTION: Creates training and validation split index lists based on the calculated cut-off point. The training set includes data up to `cut`, and the validation set includes data from `cut` onwards, mimicking a future-dated test set.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nsplits = (list(range(cut, len(train_df))),list(range(cut)))\n```\n\n----------------------------------------\n\nTITLE: DataFrame Shrinking and Type Comparison (Python)\nDESCRIPTION: This snippet demonstrates the usage of `df_shrink` to reduce the memory footprint of a DataFrame. It creates a DataFrame, applies the `df_shrink` function to create a modified DataFrame, and then compares the data types of the original and shrunken DataFrames. The 'date' column is skipped during shrinking.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'i': [-100, 0, 100], 'f': [-100.0, 0.0, 100.0], 'u':[0, 10,254],\n                  'date':['2019-12-04','2019-11-29','2019-11-15']})\ndf2 = df_shrink(df, skip=['date'])\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.dtypes\n```\n\nLANGUAGE: python\nCODE:\n```\ndf2.dtypes\n```\n\n----------------------------------------\n\nTITLE: Exporting Embeddings for the TensorBoard Projector\nDESCRIPTION: Defines a function to write embedding vectors and associated metadata (such as labels and images) into TensorBoard, facilitating visualization of learned features.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n def _write_projector_embedding(learn, writer, feat):\n    lbls = [learn.dl.vocab[l] for l in feat['lbl']] if getattr(learn.dl, 'vocab', None) else None     \n    vecs = feat['vec'].squeeze()\n    writer.add_embedding(vecs, metadata=lbls, label_img=feat['img'], global_step=learn.train_iter)\n```\n\n----------------------------------------\n\nTITLE: Decoding Data from Datasets\nDESCRIPTION: Tests the decoding functionality of a dataset or list, verifying that decoding a negative input yields the positive value and that decoding via the dataset's method matches expected results.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_62\n\nLANGUAGE: Python\nCODE:\n```\ntest_eq(*dsets[0], -1)\ntest_eq(*dsets.decode((-1,)), 1)\n```\n\n----------------------------------------\n\nTITLE: Testing layer_info with Conv2d, Flatten, and Dense Stack (Python)\nDESCRIPTION: Checks layer_info on a more complex sequential model including Conv2d, ReLU, Flatten, Linear, and BatchNorm1d layers. Tests expected extracted outputs for layer type, parameters, trainability, shape, and grouping. Requires torch.nn, synth_learner, layer_info, and a compatible test_eq implementation. Useful for validating model summaries with non-traditional layer arrangements.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n# Test for Flatten\ndef _tst_m(): return nn.Sequential(\n    nn.Conv2d(1, 2, kernel_size=3, padding=1, stride=2),\n    nn.ReLU(),\n    nn.Flatten(),\n    nn.Linear(8,50), \n    nn.ReLU(), \n    nn.BatchNorm1d(50), \n    nn.Linear(50, 1)\n)\n                                                              \nsample_input = torch.randn((1,1,4,4))\ntest_eq(layer_info(synth_learner(model=_tst_m()), sample_input), [\n    ('Conv2d', 20, True, [1, 2, 2, 2], False),\n    ('ReLU', '', '', [1, 2, 2, 2], True),\n    ('Flatten', '', '', [1, 8], False),\n    ('Linear', 450, True, [1, 50], False),\n    ('ReLU', '', '', [1,50], True),\n    ('BatchNorm1d', 100, True, [1, 50], True),\n    ('Linear', 51, True, [1, 1], False)\n])\n```\n\n----------------------------------------\n\nTITLE: Plotting PCA Results\nDESCRIPTION: This code visualizes the results of the PCA on the movie weights using a scatter plot.  It plots the movies based on their scores on the first and third principal components (fac0 and fac2). It also adds the movie titles as text labels to each point on the plot.  This facilitates visual interpretation of the results.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nidxs = np.random.choice(len(top_movies), 50, replace=False)\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(15,15))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate (commented)\nDESCRIPTION: Shows how to write a basic configuration for Accelerate, though commented out. Users need to configure Accelerate before running the examples.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/distributed_app_examples.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# from accelerate.utils import write_basic_config\n# write_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Defining BF16 Test Callback in fastai Python\nDESCRIPTION: This helper callback is used during testing to assert that the predictions made by the model are in `bfloat16` precision when BF16 mixed precision is enabled. It checks the dtype of the first element of the flattened prediction tensor.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nclass BF16TestCallback(Callback):\n    \"Asserts that predictions are `bfloat16` values\"\n    order = 9\n    def after_pred(self):\n        assert listify(flatten(self.pred))[0].dtype==torch.bfloat16\n```\n\n----------------------------------------\n\nTITLE: Implementing ToTensor Class in Python for fastai\nDESCRIPTION: Defines a Transform class that converts items to appropriate tensor classes. This is a basic transformation with an order of 5 in the transform pipeline.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass ToTensor(Transform):\n    \"Convert item to appropriate tensor class\"\n    order = 5\n```\n\n----------------------------------------\n\nTITLE: Reading Training and Validation CSV Files (Python)\nDESCRIPTION: Reads the training and test splits from CSV files into pandas DataFrames with no headers, using pandas' read_csv. 'df_train' and 'df_valid' respectively store each split's text data for further preprocessing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf_train = pd.read_csv(path/'train.csv', header=None)\ndf_valid = pd.read_csv(path/'test.csv', header=None)\ndf_train.head()\n```\n\n----------------------------------------\n\nTITLE: Upgrading fastai in Google Colab (Bash)\nDESCRIPTION: This snippet checks if the session is running in Google Colab (via the /content path) and upgrades fastai to its latest version using pip. It is intended to ensure that the runtime environment has the correct version of fastai before running subsequent tutorial code. No parameters are required, but internet access is necessary; only suitable for Colab environments.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Fine-tune Unfrozen Language Model\nDESCRIPTION: Fine-tunes the entire unfrozen language model for 10 cycles using `fit_one_cycle`. It specifies the number of epochs, the maximum learning rate, and the momentum values.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(10, 2e-3, moms=(0.8,0.7,0.8))\n```\n\n----------------------------------------\n\nTITLE: Setting default export for callback.comet\nDESCRIPTION: This sets the default export for the `callback.comet` module. This is likely used by a documentation generator or module bundler, and sets the default export to be `callback.comet`.  This is usually used in the context of the nbdev library, to generate library code from a notebook.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70d_callback.comet.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp callback.comet\n```\n\n----------------------------------------\n\nTITLE: Defining FP16 Test Callback in fastai Python\nDESCRIPTION: This helper callback is used during testing to assert that the predictions made by the model are indeed in `float16` precision when mixed precision is enabled. It checks the dtype of the first element of the flattened prediction tensor.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nclass FP16TestCallback(Callback):\n    \"Asserts that predictions are `float16` values\"\n    order = 9\n    def after_pred(self):\n        assert listify(flatten(self.pred))[0].dtype==torch.float16\n```\n\n----------------------------------------\n\nTITLE: Example Usage and Metadata Preservation Tests for TensorBase in Python\nDESCRIPTION: Demonstrates example uses of TensorBase, including enabling debug output to trace tensor operations, casting tensors preserving metadata, integration with PyTorch's default_collate function, and tests verifying correct behavior when stacking, indexing, and pickling TensorBase objects. Shows how subclasses inherit metadata and how common tensor operations behave maintaining subclass types and metadata intact.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\na = TensorBase(1)\nTensorBase.debug=True\n1/(a+1)\n```\n\nLANGUAGE: python\nCODE:\n```\na = TensorBase(1,img_size=(128,128))\ntest_eq(a.img_size,(128,128))\nb = cast(a,TensorBase)\ntest_eq(b.img_size,(128,128))\ntest_eq(torch.stack([a,b],0).img_size,(128,128))\n\ntest_eq(default_collate([a,b]).img_size,(128,128))\n```\n\nLANGUAGE: python\nCODE:\n```\nclass _TImage(TensorBase): pass\nclass _TImage2(_TImage): pass\nt1 = _TImage([1.])\nt2 = _TImage2([1.])\nt2+t1\n```\n\nLANGUAGE: python\nCODE:\n```\nclass _T(TensorBase): pass\n\nt = _T(range(5))\ntest_eq(t[0], 0)\ntest_eq_type(t+1, _T(range(1,6)))\ntest_eq(repr(t), '_T([0, 1, 2, 3, 4])')\ntest_eq_type(t[_T([False,False,True,True,True])], _T([2,3,4]))\ntest_eq_type(t[_T([2,3,4])], _T([2,3,4]))\ntest_eq(type(pickle.loads(pickle.dumps(t))), _T)\ntest_eq_type(t.new_ones(1), _T([1]))\ntest_eq_type(t.new_tensor([1,2]), _T([1,2]))\n```\n\nLANGUAGE: python\nCODE:\n```\nt = tensor([1,2,3])\nm = TensorBase([False,True,True])\ntest_eq(t[m], tensor([2,3]))\nt = tensor([[1,2,3],[1,2,3]])\nm = cast(tensor([[False,True,True],\n                 [False,True,True]]), TensorBase)\ntest_eq(t[m], tensor([2,3,2,3]))\n```\n\nLANGUAGE: python\nCODE:\n```\nt = tensor([[1,2,3],[1,2,3]])\nt.img_size = 1\nt2 = cast(t, TensorBase)\ntest_eq(t2.img_size, t.img_size)\nx = retain_type(tensor([4,5,6]), t2)\ntest_eq(x.img_size, t.img_size)\nt3 = TensorBase([[1,2,3],[1,2,3]], img_size=1)\ntest_eq(t3.img_size, t.img_size)\nt4 = t2+1\nt4.img_size = 2\ntest_eq(t2.img_size, 1)\ntest_eq(t4.img_size, 2)\n# this will fail with `Tensor` but works with `TensorBase`\ntest_eq(pickle.loads(pickle.dumps(t2)).img_size, t2.img_size)\n```\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n# test of https://github.com/pytorch/pytorch/issues/47186\nclass _T(TensorBase): ...\nt = _T([1.])\ntest_eq_type(t.new([1,2]), _T([1.,2.]))\ntest_eq_type(t.new(), _T([]))\n```\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n# test of https://github.com/pytorch/pytorch/issues/50219\nx = TensorBase(torch.rand(4,3,16,16))\nwith torch.no_grad():\n    y = x.requires_grad_()\n    assert y.requires_grad and x.requires_grad\n```\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nx = TensorBase(torch.rand(4,3,16,16))\nx.test = 'test metadata'\ny = deepcopy(x)\nassert hasattr(y, 'test') and y.test == x.test\n```\n\n----------------------------------------\n\nTITLE: Loading Model Weights by Order, Ignoring Keys - Python\nDESCRIPTION: Defines load_ignore_keys to load a model's state dict by copying parameters from another state dict in order, ignoring the actual parameter names. Useful when key names do not align but order matches. Inputs: model (torch.nn.Module), wgts (dict with tensors). Returns PyTorch's state dict load result. Limitation: requires precise ordering and compatible parameter shapes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef load_ignore_keys(\n    model, # Model architecture\n    wgts:dict # Model weights\n) -> tuple:\n    \"Load `wgts` in `model` ignoring the names of the keys, just taking parameters in order\"\n    sd = model.state_dict()\n    for k1,k2 in zip(sd.keys(), wgts.keys()): sd[k1].data = wgts[k2].data.clone()\n    return model.load_state_dict(sd)\n```\n\n----------------------------------------\n\nTITLE: Testing EarlyStoppingCallback (MSE Loss)\nDESCRIPTION: This tests the `EarlyStoppingCallback` with MSE loss as the monitored metric. It trains a synthetic learner with a specified learning rate and the early stopping callback.  The parameters `min_delta` and `patience` are set, ensuring the callback stops training if the MSE loss doesn't improve by at least `min_delta` for `patience` epochs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner(n_trn=2, metrics=F.mse_loss)\nlearn.fit(n_epoch=200, lr=1e-7, cbs=EarlyStoppingCallback(monitor='mse_loss', min_delta=0.1, patience=2))\n```\n\nLANGUAGE: python\nCODE:\n```\nlearn.validate()\n```\n\n----------------------------------------\n\nTITLE: Unfreeze Classifier\nDESCRIPTION: Unfreezes all layers in classifier.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7, 0.8))\n```\n\n----------------------------------------\n\nTITLE: Defining Sample Categorical Variables - Python\nDESCRIPTION: Defines a list of column names representing categorical variables chosen for the small sample experiment.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsmall_cat_vars =  ['Store', 'DayOfWeek', 'PromoInterval']\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Language Model (Frozen)\nDESCRIPTION: This fine-tunes the language model, training only the head of the model while the body remains frozen. The `fit_one_cycle` method is used with a learning rate of 1e-2 for one epoch to fine-tune the language model on the IMDB dataset.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(1, 1e-2)\n```\n\n----------------------------------------\n\nTITLE: Plotting Training Losses - Python/Matplotlib\nDESCRIPTION: Plots the list of training losses collected during the manual training loop. This visualization helps to see how the loss decreased over the batches processed in the epoch.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nplt.plot(losses);\n```\n\n----------------------------------------\n\nTITLE: RNNDropout Layer - PyTorch\nDESCRIPTION: The `RNNDropout` class implements dropout specifically for recurrent neural networks. It takes a dropout probability `p` as input and applies a consistent dropout mask along the sequence length dimension. It inherits from `Module` and uses the `dropout_mask` function to generate the dropout mask.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nclass RNNDropout(Module):\n    \"Dropout with probability `p` that is consistent on the seq_len dimension.\"\n    def __init__(self, p:float=0.5): self.p=p\n\n    def forward(self, x):\n        if not self.training or self.p == 0.: return x\n        return x * dropout_mask(x.data, (x.size(0), 1, *x.shape[2:]), self.p)\n```\n\n----------------------------------------\n\nTITLE: Defining an Average Smooth Loss Metric (Python)\nDESCRIPTION: This snippet defines `AvgSmoothLoss`, a class that calculates a smooth average of the losses using exponential weighting with a specified beta value. It inherits from the base `Metric` class and implements `reset` to initialize state and `accumulate` to update the smoothed loss value.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_63\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass AvgSmoothLoss(Metric):\n    \"Smooth average of the losses (exponentially weighted with `beta`)\"\n    def __init__(self, beta=0.98): self.beta = beta\n    def reset(self):               self.count,self.val = 0,tensor(0.)\n    def accumulate(self, learn):\n        self.count += 1\n        self.val = torch.lerp(to_detach(learn.loss.mean()), self.val, self.beta)\n    @property\n    def value(self): return self.val/(1-self.beta**self.count)\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(AvgSmoothLoss, title_level=3)\n```\n\nLANGUAGE: python\nCODE:\n```\ntst = AvgSmoothLoss()\nt = torch.randn(100)\ntst.reset()\nval = tensor(0.)\nfor i in range(4): \n    learn.loss = t[i*25:(i+1)*25].mean()\n    tst.accumulate(learn)\n    val = val*0.98 + t[i*25:(i+1)*25].mean()*(1-0.98)\n    test_close(val/(1-0.98**(i+1)), tst.value)\n```\n\n----------------------------------------\n\nTITLE: Initializing Convolutional Neural Network Weights in Python\nDESCRIPTION: Defines the init_cnn function which recursively initializes all layers in a given module. Bias terms are zero-initialized if present, and weights of convolutional and linear layers are initialized with kaiming normal distribution, suitable for ReLU activations. This function is used to properly set initial weights of the XResNet models.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/11_vision.models.xresnet.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef init_cnn(m):\n    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n    for l in m.children(): init_cnn(l)\n```\n\n----------------------------------------\n\nTITLE: Computing Principal Component Analysis (PCA) with PyTorch in Python\nDESCRIPTION: Defines a method for computing PCA on tensor 'x', centering the data and performing singular value decomposition on its transpose, then returns the projection of the input onto the first 'k' principal components. Enables dimensionality reduction using PyTorch tensors.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_81\n\nLANGUAGE: python\nCODE:\n```\n@patch\ndef pca(x:Tensor, k=2):\n    \"Compute PCA of `x` with `k` dimensions.\"\n    x = x-torch.mean(x,0)\n    U,S,V = torch.svd(x.t())\n    return torch.mm(x,U[:,:k])\n```\n\n----------------------------------------\n\nTITLE: TensorBoard Setup Instructions\nDESCRIPTION: Provides instructions for installing and launching TensorBoard, setting the log directory for visualization, and linking it to fastai's TensorBoardCallback via 'log_dir'.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_3\n\n\n\n----------------------------------------\n\nTITLE: Demonstrating Basic TfmdLists Functionality in fastai (Python)\nDESCRIPTION: Illustrates the basic usage of `TfmdLists`. It creates an instance `tl` using a list of floats (`items`) and a list of transforms (`tfms`). The example then tests the type of transformed items, decodes an item back to its original domain (approximately), shows a transformed item, and verifies the inferred data types throughout the pipeline using fastai's testing utilities (`test_eq_type`, `test_eq`, `test_stdout`).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nitems = L([1.,2.,3.]); tfms = [neg_tfm, int2f_tfm]\ntl = TfmdLists(items, tfms=tfms)\ntest_eq_type(tl[0], TitledInt(-1))\ntest_eq_type(tl[1], TitledInt(-2))\ntest_eq_type(tl.decode(tl[2]), TitledFloat(3.))\ntest_stdout(lambda: show_at(tl, 2), '-3')\ntest_eq(tl.types, [float, float, TitledInt])\ntl\n```\n\n----------------------------------------\n\nTITLE: Hooks Class Usage Example\nDESCRIPTION: This snippet demonstrates how to use the `Hooks` class. It creates a sequential model with three layers, then creates a `Hooks` object that attaches a hook to each layer. The hook function simply stores the output of each layer. It then performs a forward pass and verifies that the stored activations of each hook are equal to the output of the corresponding layer.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nlayers = [nn.Linear(5,10), nn.ReLU(), nn.Linear(10,3)]\ntst_model = nn.Sequential(*layers)\nhooks = Hooks(tst_model, lambda m,i,o: o)\ny = tst_model(x)\ntest_eq(hooks.stored[0], layers[0](x))\ntest_eq(hooks.stored[1], F.relu(layers[0](x)))\ntest_eq(hooks.stored[2], y)\nhooks.remove()\n```\n\n----------------------------------------\n\nTITLE: Upgrading fastai on Google Colab\nDESCRIPTION: Shell command to conditionally upgrade the fastai library using pip if running within a Google Colab environment. The `-Uqq` flags ensure a quiet upgrade.\nSOURCE: https://github.com/fastai/fastai/blob/main/disabled/70b_callback.neptune.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Importing Fastai Text and Model Components - Python\nDESCRIPTION: Imports major modules and submodules from fastai to provide access to basic, text, model, and callback functionality. Required dependency: fastai. There are no input parameters or return values; these are import statements essential for enabling downstream functionality in this file.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.basics import *\nfrom fastai.text.core import *\nfrom fastai.text.data import *\nfrom fastai.text.models.core import *\nfrom fastai.text.models.awdlstm import *\nfrom fastai.callback.rnn import *\nfrom fastai.callback.progress import *\n```\n\n----------------------------------------\n\nTITLE: Installing and Importing Dependencies for fastai XResNet Models in Python\nDESCRIPTION: This snippet upgrades the fastai library if running in a Colab environment and imports necessary modules from fastai and torchvision. It ensures compatibility by attempting to import load_state_dict_from_url from torchvision and falls back to torch.hub if unavailable. This setup is required for downloading pretrained weights and base functionalities.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/11_vision.models.xresnet.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.torch_basics import *\ntry: from torchvision.models.utils import load_state_dict_from_url\nexcept ModuleNotFoundError: from torch.hub import load_state_dict_from_url\n```\n\n----------------------------------------\n\nTITLE: Initializing Datasets with Various Data and Transform Pipelines\nDESCRIPTION: Creates a list of items and initializes a `Datasets` object with multiple transform lists to process data accordingly.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_60\n\nLANGUAGE: Python\nCODE:\n```\nitems = [1,2,3,4]\ndsets = Datasets(items, [[neg_tfm,int2f_tfm]])\n```\n\n----------------------------------------\n\nTITLE: Tokenizing Text with GPT-2 Tokenizer (Python)\nDESCRIPTION: This snippet uses the pretrained GPT-2 tokenizer to encode a sample string into token IDs for model input. The encoded result is a list of integer IDs. No parameters required aside from the text string; output is typically consumed by the model.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nids = tokenizer.encode('This is an example of text, and')\nids\n```\n\n----------------------------------------\n\nTITLE: Viewing Dependent Variable Head - Python\nDESCRIPTION: Displays the first few values of the dependent variable ('Sales') in the training DataFrame.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ntrain_df[dep_var].head()\n```\n\n----------------------------------------\n\nTITLE: Open Wheel File with wheel Library (Python)\nDESCRIPTION: Opens the specified Wheel file (`.whl`) using the `WheelFile` class from the `wheel` library. This class provides convenient methods for accessing Wheel-specific information.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfwhl = wheelfile.WheelFile(whl)\n```\n\n----------------------------------------\n\nTITLE: Importing Core fastai and Data Load - Python\nDESCRIPTION: This snippet imports necessary modules from the fastai library. These include `torch_basics`, which likely provides core PyTorch utilities, and `load`, which provides data loading utilities. These imports lay the groundwork for data transformations and loading operations and are necessary to build `TfmdLists`, `Datasets` and `DataLoaders`. There are no explicit parameters or expected outputs; dependencies include fastai and its associated packages.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom __future__ import annotations\nfrom fastai.torch_basics import *\nfrom fastai.data.load import *\n```\n\n----------------------------------------\n\nTITLE: Define Default Model Splitter - Python\nDESCRIPTION: Defines a default function to split a PyTorch model (`m`) into two parameter groups: the first module (`m[0]`) and the rest (`m[1:]`). This is typically used to define different learning rates for the body and head of a neural network during transfer learning. It utilizes fastai's `L` for collection manipulation and `params` to extract parameters.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef default_split(m):\n    \"Default split of a model between body and head\"\n    return L(m[0], m[1:]).map(params)\n```\n\n----------------------------------------\n\nTITLE: Testing FP16 Mixed Precision with Multi-Output Model in fastai Python\nDESCRIPTION: This test case validates FP16 mixed precision with a model that returns multiple outputs and a custom loss function. It sets up a synthetic learner with a multi-output model, the `MixedPrecision` and `FP16TestCallback`, and a custom loss, then runs a fit cycle. It asserts that the loss decreases over epochs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#|cuda\n#Multioutput version\nset_seed(99, True)\nlearn = synth_learner(cbs=[MixedPrecision,FP16TestCallback], cuda=True)\nclass MultiOutputModel(Module):\n    def __init__(self): self.linear1, self.linear2 = nn.Linear(1,1) , nn.Linear(1,1)\n    def forward(self,x): return self.linear1(x), self.linear2(x)\ndef multioutputloss(pred, val): return ((val-pred[0]).abs() + 0.5 * (val-pred[1]).abs()).sum()\nlearn.model = MultiOutputModel()\nlearn.opt_func = partial(SGD, mom=0.)\nlearn.splitter = lambda m: [list(m.linear1.parameters()), list(m.linear2.parameters())]\nlearn.loss_func=multioutputloss\nlearn.fit(3)\nassert learn.recorder.values[-1][-1]<learn.recorder.values[0][-1]\n```\n\n----------------------------------------\n\nTITLE: Creating ActivationType Enum Class - Python\nDESCRIPTION: Defines an enumeration class 'ActivationType' representing different activation functions used by fastai's metric system, such as 'No', 'Sigmoid', 'Softmax', 'BinarySoftmax'. The mk_class utility is required. Each enum value maps to its lowercase string representation. Used during metric accumulation to control activation application.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nmk_class('ActivationType', **{o:o.lower() for o in ['No', 'Sigmoid', 'Softmax', 'BinarySoftmax']},\n         doc=\"All possible activation classes for `AccumMetric\")\n```\n\n----------------------------------------\n\nTITLE: Testing DiceLoss Mean Reduction with Specific Cases - Fastai/PyTorch\nDESCRIPTION: This snippet tests the `DiceLoss` with `reduction='mean'` using specific tensor examples. It verifies that the loss is zero for identical masks and approximately 0.66 for a case with 50% intersection (Dice score 0.33, Loss = 1 - 0.33) after softmax.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndl = DiceLoss(reduction=\"mean\")\n#identical masks\nmodel_output = tensor([[[.1], [.1], [100.]]])\ntarget = tensor([[2]])\ntest_close(dl(model_output, target), 0)\n\n#50% intersection\nmodel_output = tensor([[[.1, 100.], [.1, .1], [100., .1]]])\ntarget = tensor([[2, 1]])\ntest_close(dl(model_output, target), .66, eps=0.01)\n```\n\n----------------------------------------\n\nTITLE: Documenting show_batch and Device Movement Methods (fastai, Python)\nDESCRIPTION: Documents the TfmdDL.show_batch and TfmdDL.to methods using show_doc. Show_batch visualizes a batch, while to moves the DataLoader to a specified device (CPU/GPU). These provide interactive and performance utilities for fastai data workflows.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TfmdDL.show_batch)\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TfmdDL.to)\n```\n\n----------------------------------------\n\nTITLE: Testing SequentialRNN Reset Propagation in Python\nDESCRIPTION: Defines a simple test module with a reset method that prints 'reset'. Creates a SequentialRNN with two instances and verifies that reset propagates the call to each child by capturing expected stdout output 'reset\\nreset'. This confirms reset functionality is properly forwarded to children modules.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass _TstMod(Module):\n    def reset(self): print('reset')\n\ntst = SequentialRNN(_TstMod(), _TstMod())\ntest_stdout(tst.reset, 'reset\\nreset')\n```\n\n----------------------------------------\n\nTITLE: Training the Wasserstein GAN Model Using fastai Python\nDESCRIPTION: Trains the GAN learner for 30 epochs using a learning rate of 0.0002 and no weight decay (`wd=0`). This step performs alternating training of the generator and critic as described in the Wasserstein GAN methodology.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-wgan.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit(30, 2e-4, wd=0)\n```\n\n----------------------------------------\n\nTITLE: Saving and restoring RNG states to enable reproducible randomness (Python)\nDESCRIPTION: This snippet demonstrates storing current RNG states using `get_random_states`, modifying randomness, and restoring previous states with `set_random_states`. It verifies that after restoring, the sequences of random numbers produced are identical, thus ensuring reproducibility of stochastic processes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nold_states = get_random_states()\nolds = (random.random(),np.random.random(),torch.rand(()))\nnews = (random.random(),np.random.random(),torch.rand(()))\nset_random_states(**old_states)\nrewinds = (random.random(),np.random.random(),torch.rand(()))\n\nprint('olds:    {0:3.3f} {1:3.3f} {2:3.3f}'.format(*olds))\nprint('news:    {0:3.3f} {1:3.3f} {2:3.3f}'.format(*news))\nprint('rewinds: {0:3.3f} {1:3.3f} {2:3.3f}'.format(*rewinds))\n```\n\n----------------------------------------\n\nTITLE: Calculating Target Variable Range - Python\nDESCRIPTION: Determines the upper bound for the target variable's range after log transformation. A small buffer (log(1.2)) is added to the maximum value. This range is used in the model configuration.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nmax_log_y = np.log(1.2) + np.max(train_df['Sales'])\ny_range = (0, max_log_y)\n```\n\n----------------------------------------\n\nTITLE: Testing ICNR Initialization\nDESCRIPTION: Tests `icnr_init` to confirm that, for a given input tensor and scale, the initialized weights have the desired property.  It verifies that weights corresponding to the pixel-shuffle channels are the same. It makes assertions about the equality of the elements in the initialized tensor.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_51\n\nLANGUAGE: Python\nCODE:\n```\ntst = torch.randn(16*4, 32, 1, 1)\ntst = icnr_init(tst)\nfor i in range(0,16*4,4):\n    test_eq(tst[i],tst[i+1])\n    test_eq(tst[i],tst[i+2])\n    test_eq(tst[i],tst[i+3])\n```\n\n----------------------------------------\n\nTITLE: Testing BCEWithLogitsLossFlat Functionality and Activation - Fastai/PyTorch\nDESCRIPTION: This sequence of snippets tests the `BCEWithLogitsLossFlat` function. It initializes the loss, performs a forward pass, checks that the standard loss would fail (implying `BCEWithLogitsLossFlat` handles shape differences), and verifies that the associated activation function is correctly identified as `torch.sigmoid`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntst = BCEWithLogitsLossFlat(pos_weight=torch.ones(10))\noutput = torch.randn(32, 5, 10)\ntarget = torch.randn(32, 5, 10)\n_ = tst(output, target)\ntest_fail(lambda x: nn.BCEWithLogitsLoss()(output,target))\n```\n\nLANGUAGE: python\nCODE:\n```\n#Associated activation is sigmoid\ntest_eq(tst.activation(output), torch.sigmoid(output))\n```\n\n----------------------------------------\n\nTITLE: Using no_random to control RNG within a code block (Python)\nDESCRIPTION: This example demonstrates saving RNG states, executing code within multiple `no_random` contexts with different seeds, and verifying that the sequences of generated random numbers are consistent and reset correctly. It confirms that inside `with` blocks, the RNG states are correctly managed.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nstates=get_random_states()\nolds = (random.random(),np.random.random(),torch.rand(()))\nset_random_states(**states) #rewinding above random calls\n\nwith no_random():\n    new1 = (random.random(),np.random.random(),torch.rand(()))\nwith no_random():\n    new2 = (random.random(),np.random.random(),torch.rand(()))\nwith no_random(seed=100):\n    seeded1 = (random.random(),np.random.random(),torch.rand(()))\nwith no_random(seed=100):\n    seeded2 = (random.random(),np.random.random(),torch.rand(()))\n        \nrewinds = (random.random(),np.random.random(),torch.rand(()))\n\nprint('olds:    {0:3.3f} {1:3.3f} {2:3.3f}'.format(*olds))\nprint('new1:    {0:3.3f} {1:3.3f} {2:3.3f}'.format(*new1))\nprint('new2:    {0:3.3f} {1:3.3f} {2:3.3f}'.format(*new2))\nprint('seeded1: {0:3.3f} {1:3.3f} {2:3.3f}'.format(*seeded1))\nprint('seeded2: {0:3.3f} {1:3.3f} {2:3.3f}'.format(*seeded2))\nprint('rewinds: {0:3.3f} {1:3.3f} {2:3.3f}'.format(*rewinds))\n```\n\n----------------------------------------\n\nTITLE: Unfreezing and Further Finetuning Language Model (Python)\nDESCRIPTION: Enables training of all model layers by unfreezing and further trains for 10 epochs with a lower learning rate. Uses the 1cycle policy for optimal scheduling. Input: previously trained learner; Output: more adapted model weights.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nlearn.unfreeze()\nlearn.fit_one_cycle(10, 2e-3, moms=(0.8,0.7,0.8))\n```\n\n----------------------------------------\n\nTITLE: Testing SortedDL with Shuffling and Padding - Python\nDESCRIPTION: This snippet tests `SortedDL` behavior with shuffling and custom padding. It generates a random dataset, initializes `SortedDL` with `shuffle=True` and `create_batch` using `pad_input` with a specified `pad_idx`. Assertions verify that batch lengths are managed and the custom padding value is applied correctly.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nds = [(tensor(range(random.randint(1,10))),i) for i in range(101)]\ndl = SortedDL(ds, bs=2, create_batch=partial(pad_input, pad_idx=-1), shuffle=True, num_workers=0)\nbatches = list(dl)\nmax_len = len(batches[0][0])\nfor b in batches: \n    assert(len(b[0])) <= max_len \n    test_ne(b[0][-1], -1)\n```\n\n----------------------------------------\n\nTITLE: Inference with Custom Transformations\nDESCRIPTION: This code defines a function `mult` with an 'order', then creates TfmdLists with multiple transforms, including a label transform and the custom function. It tests inference methods (`infer_idx`, `infer`) with various inputs, checking for correct index inference and error handling for improper inputs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_39\n\nLANGUAGE: Python\nCODE:\n```\ndef mult(x): return x*2\nmult.order = 2\n\nfns = ['dog_0.jpg','cat_0.jpg','cat_2.jpg','cat_1.jpg','dog_1.jpg']\ntl = TfmdLists(fns, [_lbl,_Cat(),mult])\n\ntest_eq(tl.infer_idx('dog_45.jpg'), 0)\ntest_eq(tl.infer('dog_45.jpg'), 2)\n\ntest_eq(tl.infer_idx(4), 2)\ntest_eq(tl.infer(4), 8)\n\ntest_fail(lambda: tl.infer_idx(2.0))\ntest_fail(lambda: tl.infer(2.0))\n```\n\n----------------------------------------\n\nTITLE: Checking Module Parameters with PyTorch (Python)\nDESCRIPTION: Defines the function has_params that verifies whether a PyTorch module has at least one parameter by checking the length of its parameter iterator. This utility aids in filtering modules when registering hooks. Requires torch (PyTorch) and is compatible with any torch.nn.Module instance. Expects a module instance as input and returns a boolean indicating if parameters exist.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\n#|export\ndef has_params(m):\n    \"Check if `m` has at least one parameter\"\n    return len(list(m.parameters())) > 0\n```\n\n----------------------------------------\n\nTITLE: Applying Multiple Sequential Transforms via from_dsets (fastai, Python)\nDESCRIPTION: Introduces two custom transforms: _T (negation) and _T2 (halving). Uses DataLoaders.from_dsets to create training and validation loaders with these transforms applied at after_item and after_batch stages respectively. Tests assert correct cumulative output, confirming sequential transform composition in the data pipeline.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nclass _T(Transform):  \n    def encodes(self, o):  return -o\nclass _T2(Transform): \n    def encodes(self, o):  return o/2\n\n#test tfms are applied on both traind and valid dl\ndls_from_ds = DataLoaders.from_dsets([1,], [5,], bs=1, after_item=_T, after_batch=_T2)\nb = first(dls_from_ds.train)\ntest_eq(b, tensor([-.5]))\nb = first(dls_from_ds.valid)\ntest_eq(b, tensor([-2.5]))\n```\n\n----------------------------------------\n\nTITLE: Example: Create WeightedDL from DataBlock in fastai Python\nDESCRIPTION: Demonstrates creating a weighted dataloader directly from a `DataBlock` instance using the patched method. It reuses the previously defined `dblock`, source, and weights to generate `WeightedDL` instances.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14a_callback.data.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndls = dblock.weighted_dataloaders(list(range(10)), wgts, bs=1)\ndls.show_batch()\n```\n\n----------------------------------------\n\nTITLE: Defining General Apply Function - Python\nDESCRIPTION: This snippet defines a utility function `apply` that takes a function `func` and an iterable `items`. It applies the given function to each item in the iterable using Python's built-in `map` function and returns the results as a standard Python list. This function is used for applying a tokenization method sequentially to a list of texts.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef apply(func, items): return list(map(func, items))\n```\n\n----------------------------------------\n\nTITLE: Show Documentation for log_model function\nDESCRIPTION: This snippet uses the `show_doc` function to display the documentation for the `log_model` function. The `log_model` function is used to log model information to Weights & Biases (wandb).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(log_model)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Parallel Simple Tokenization (Batched) - Jupyter\nDESCRIPTION: This snippet uses `%%timeit` to benchmark the execution time of applying the `delim_tok` function using fastai's `parallel` with 2 workers. This time, the `parallel` function operates on `batches` (2 batches), and each worker processes one batch by applying `delim_tok` to its contents using a `partial` application of the `apply` function. This measures the overhead of parallel processing with fewer, larger tasks. Runs 2 times per loop, 3 loops.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n%%timeit -n 2 -r 3\nparallel(partial(apply, delim_tok), batches, progress=False, n_workers=2)\n```\n\n----------------------------------------\n\nTITLE: Inspect the model\nDESCRIPTION: Prints the architecture of the underlying PyTorch model used in the `collab_learner`.  Useful for understanding the structure of the model and the layers it contains.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nlearn.model\n```\n\n----------------------------------------\n\nTITLE: Find Learning Rate\nDESCRIPTION: Runs the learning rate finder to identify the optimal learning rate for training the language model.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nlearn.lr_find()\n```\n\n----------------------------------------\n\nTITLE: Reduce DataFrame Memory Usage (Python)\nDESCRIPTION: This function, `df_shrink`, reduces the memory usage of a DataFrame by casting columns to smaller types identified by `df_shrink_dtypes`. It takes a DataFrame and optional parameters to control object-to-category conversion, integer-to-unsigned integer conversion, and column exclusion.  It returns the modified DataFrame with smaller data types.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef df_shrink(df, skip=[], obj2cat=True, int2uint=False):\n    \"Reduce DataFrame memory usage, by casting to smaller types returned by `df_shrink_dtypes()`.\"\n    dt = df_shrink_dtypes(df, skip, obj2cat=obj2cat, int2uint=int2uint)\n    return df.astype(dt)\n```\n\n----------------------------------------\n\nTITLE: Illustrating Attribute Modification\nDESCRIPTION: Illustrates a potential issue with callback attributes. If the callback modifies an attribute in the callback itself, it doesn't change the learner attribute and issues a warning.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nlearn.a\n```\n\nLANGUAGE: python\nCODE:\n```\nclass TstCallback(Callback):\n    def batch_begin(self): self.a += 1\n\nlearn,cb = TstLearner(1),TstCallback()\ncb.learn = learn\ncb('batch_begin')\ntest_eq(cb.a, 2)\ntest_eq(cb.learn.a, 1)\n```\n\n----------------------------------------\n\nTITLE: fit_one_cycle with plot_sched\nDESCRIPTION: Trains a synthetic model using `fit_one_cycle` and then calls `plot_sched` to visualize the learning rate and momentum schedules during the training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner()\nlearn.fit_one_cycle(2)\n```\n\n----------------------------------------\n\nTITLE: Creating and Modifying Custom Transform (fastai, Python)\nDESCRIPTION: Demonstrates defining a custom transform class with an initialization method and an encode logic. The snippet shows how to use PyTorch tensors and device handling within a transform, useful for creating batch or item transforms in the fastai pipeline. Dependencies include torch and the fastai Transform class, with expected inputs as PyTorch tensors.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nparameters = 'a'\ndef __init__(self): self.a = torch.tensor(0.)\ndef encodes(self, x): x\n```\n\n----------------------------------------\n\nTITLE: Numericalizing Tokenized Text with a Reversible Transform in Python\nDESCRIPTION: Implements the Numericalize transform to convert tokenized text into numerical indices and vice versa. The class allows vocab construction at setup from dataset counters if no vocab is passed. It supports parameters for minimum token frequency, maximum vocabulary size, and special tokens inclusion. The transform encodes token sequences into TensorText objects of indices and decodes these indices back into token strings using the vocab. This reversible transform is central to token-to-integer representation needed for model input.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass Numericalize(Transform):\n    \"Reversible transform of tokenized texts to numericalized ids\"\n    def __init__(self, vocab=None, min_freq=3, max_vocab=60000, special_toks=None):\n        store_attr('vocab,min_freq,max_vocab,special_toks')\n        self.o2i = None if vocab is None else defaultdict(int, {v:k for k,v in enumerate(vocab)})\n\n    def setups(self, dsets):\n        if dsets is None: return\n        if self.vocab is None:\n            count = dsets.counter if getattr(dsets, 'counter', None) is not None else Counter(p for o in dsets for p in o)\n            if self.special_toks is None and hasattr(dsets, 'special_toks'):\n                self.special_toks = dsets.special_toks\n            self.vocab = make_vocab(count, min_freq=self.min_freq, max_vocab=self.max_vocab, special_toks=self.special_toks)\n            self.o2i = defaultdict(int, {v:k for k,v in enumerate(self.vocab) if v != 'xxfake'})\n\n    def encodes(self, o): return TensorText(tensor([self.o2i  [o_] for o_ in o]))\n    def decodes(self, o): return L(self.vocab[o_] for o_ in o)\n```\n\n----------------------------------------\n\nTITLE: Wrapping Metrics for Argument Optimization - Python\nDESCRIPTION: Defines optim_metric, which creates a version of a metric function that automatically optimizes one of its arguments (argname) within specified bounds using scipy.optimize.minimize_scalar. Parameters are target function, argument name, optimization bounds, tolerance, whether to maximize (do_neg), and get_x to return the optimal value. Outputs the optimized metric value (and optionally the argument value) for given predictions and targets. Requires scipy.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ndef optim_metric(f, argname, bounds, tol=0.01, do_neg=True, get_x=False):\n    \"Replace metric `f` with a version that optimizes argument `argname`\"\n    def _f(preds, targs):\n        def minfunc(x):\n            kwargs = {argname:x}\n            res = f(preds, targs, **kwargs)\n            return -res if do_neg else res\n        optres = scipy.optimize.minimize_scalar(minfunc, bounds=bounds, method='bounded',\n                                                options={'xatol':0.01})\n        fun = -optres.fun if do_neg else optres.fun\n        return (fun,optres.x) if get_x else fun\n    _f.__name__ = f'opt_{f.__name__}'\n    return _f\n```\n\n----------------------------------------\n\nTITLE: Implementing F1 Score Wrapper for Multi-label Classification in Python\nDESCRIPTION: A wrapper function that adapts scikit-learn's f1_score for use in fastai's multi-label classification tasks, supporting customization of threshold, activation, label specification, and averaging method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef F1ScoreMulti(thresh=0.5, sigmoid=True, labels=None, pos_label=1, average='macro', sample_weight=None):\n    \"F1 score for multi-label classification problems\"\n    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n    return skm_to_fastai(skm.f1_score, thresh=thresh, activation=activation, flatten=False,\n                         labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight)\n```\n\n----------------------------------------\n\nTITLE: Load movie data from u.item\nDESCRIPTION: Loads the movie data from the `u.item` file, which contains movie information. Specifies the delimiter as `|`, sets the encoding to `latin-1`, sets `header=None`, and provides column names for movie ID, title, date, and genres. Uses a list comprehension to generate generic genre column names (g0 to g18).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmovies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1', header=None,\n                    names=[item, 'title', 'date', 'N', 'url', *[f'g{i}' for i in range(19)]])\nmovies.head()\n```\n\n----------------------------------------\n\nTITLE: Importing Metric Dependencies from scikit-learn and scipy - Python\nDESCRIPTION: Imports scikit-learn metrics as skm and scipy.stats as scs modules, making their metrics and statistics available for metric computation. Dependencies are scikit-learn and scipy. No parameters or direct outputs, used for downstream metric functions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport sklearn.metrics as skm\nimport scipy.stats as scs\n```\n\n----------------------------------------\n\nTITLE: Defining Mask TransformBlock for Segmentation Masks in fastai Vision Using Python\nDESCRIPTION: Defines `MaskBlock`, a TransformBlock factory designed for segmentation masks. It optionally accepts a list of codes (vocabulary labels) annotating the segmentation classes. The returned TransformBlock uses `PILMask.create` for type transformation, an item transformation to add mask codes (`AddMaskCodes`), and applies an integer-to-float tensor conversion for batching. This facilitates preparing diverse segmentation masks for fastai vision tasks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/08_vision.data.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef MaskBlock(\n    codes:list=None # Vocab labels for segmentation masks\n):\n    \"A `TransformBlock` for segmentation masks, potentially with `codes`\"\n    return TransformBlock(type_tfms=PILMask.create, item_tfms=AddMaskCodes(codes=codes), batch_tfms=IntToFloatTensor)\n```\n\n----------------------------------------\n\nTITLE: Processing Tabular Data for Wandb Logging\nDESCRIPTION: This `wandb_process` function processes `Tabular` data for logging to Weights & Biases (W&B). It creates a W&B table from the tabular data, adding a column for each target variable with the corresponding prediction.  It then returns a dictionary containing the table.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n@dispatch\ndef wandb_process(x:Tabular, y:Tabular, samples, outs, preds):\n    df = x.all_cols\n    for n in x.y_names: df[n+'_pred'] = y[n].values\n    return {\"Prediction_Samples\": wandb.Table(dataframe=df)}\n```\n\n----------------------------------------\n\nTITLE: Testing SelfAttention Layer (Weight Inspection)\nDESCRIPTION: This snippet examines the weights and intermediate calculations within the `SelfAttention` layer, specifically the query, key and value layers.  It checks their shapes, and shows how the attention mechanism matrices `f`, `g` and `h` are computed.  Dependencies: `torch`, `fastai` (for Module, ConvLayer, etc.).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_45\n\nLANGUAGE: Python\nCODE:\n```\nq,k,v = tst.query[0].weight.data,tst.key[0].weight.data,tst.value[0].weight.data\ntest_eq([q.shape, k.shape, v.shape], [[2, 16, 1], [2, 16, 1], [16, 16, 1]])\nf,g,h = map(lambda m: x.view(32, 16, 64).transpose(1,2) @ m.squeeze().t(), [q,k,v])\ntest_eq([f.shape, g.shape, h.shape], [[32,64,2], [32,64,2], [32,64,16]])\n```\n\n----------------------------------------\n\nTITLE: Creating a Vocabulary from Token Frequency Counts in Python\nDESCRIPTION: Implements a function to generate a vocabulary list from a Counter object containing token frequencies. It filters tokens by minimum frequency and limits vocabulary size to max_vocab. Special tokens are always included at the start, and padding with 'xxfake' tokens ensures vocabulary size is a multiple of 8 for performance optimization in mixed precision scenarios. Inputs are a Counter, minimum frequency threshold, max vocabulary size, and optional special tokens. Output is a list of vocabulary tokens.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef make_vocab(count, min_freq=3, max_vocab=60000, special_toks=None):\n    \"Create a vocab of `max_vocab` size from `Counter` `count` with items present more than `min_freq`\"\n    vocab = [o for o,c in count.most_common(max_vocab) if c >= min_freq]\n    special_toks = ifnone(special_toks, defaults.text_spec_tok)\n    for o in reversed(special_toks): #Make sure all special tokens are in the vocab\n        if o in vocab: vocab.remove(o)\n        vocab.insert(0, o)\n    vocab = vocab[:max_vocab]\n    return vocab + [f'xxfake' for i in range(0, 8-len(vocab)%8)]\n```\n\n----------------------------------------\n\nTITLE: Deprecated Function cnn_learner\nDESCRIPTION: This function wraps 'vision_learner' but is deprecated, issuing a warning to guide users to switch to the newer function. It maintains backwards compatibility for existing codebases.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_38\n\nLANGUAGE: Python\nCODE:\n```\n#|export\ndef cnn_learner(*args, **kwargs):\n    \"Deprecated name for `vision_learner` -- do not use\"\n    warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n    return vision_learner(*args, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Categorify with Splits Example\nDESCRIPTION: This code demonstrates the usage of `Categorify` with specified data splits. The purpose is to ensure that the vocabulary learned during `setups` is based only on the training split, which is important to avoid data leakage. Values in the validation set that are not in the training set will be mapped to `#na#`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf = cudf.from_pandas(pd.DataFrame({'a':[0,1,2,3,2]}))\nto = TabularGPU(df, Categorify, 'a', splits=[[0,1,2], [3,4]])\ncat = to.procs.categorify\ntest_eq(list(cat['a']), ['#na#','0','1','2'])\ntest_eq(to.a.to_array(), np.array([1,2,3,0,3]))\n```\n\n----------------------------------------\n\nTITLE: Creating a Grid of Matplotlib Axes for Vision Data Visualization in Python\nDESCRIPTION: Defines `get_grid` which returns a grid of matplotlib axes arranged in specified rows and columns to aid in displaying batches of images or results for vision applications. It calculates default rows and columns based on the total number of axes requested, optionally doubles the grid size, supports figure sizing, and can return either just axes or both figure and axes. It depends on matplotlib's `subplots` for figure creation and is primarily used by vision-specific `show_batch` and `show_results` functions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/08_vision.data.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@delegates(subplots)\ndef get_grid(\n    n:int, # Number of axes in the returned grid\n    nrows:int=None, # Number of rows in the returned grid, defaulting to `int(math.sqrt(n))`\n    ncols:int=None, # Number of columns in the returned grid, defaulting to `ceil(n/rows)` \n    figsize:tuple=None, # Width, height in inches of the returned figure\n    double:bool=False, # Whether to double the number of columns and `n`\n    title:str=None, # If passed, title set to the figure\n    return_fig:bool=False, # Whether to return the figure created by `subplots`\n    flatten:bool=True, # Whether to flatten the matplot axes such that they can be iterated over with a single loop\n    **kwargs,\n) -> (plt.Figure, plt.Axes): # Returns just `axs` by default, and (`fig`, `axs`) if `return_fig` is set to True\n    \"Return a grid of `n` axes, `rows` by `cols`\"\n    if nrows:\n        ncols = ncols or int(np.ceil(n/nrows))\n    elif ncols:\n        nrows = nrows or int(np.ceil(n/ncols))\n    else:\n        nrows = int(math.sqrt(n))\n        ncols = int(np.ceil(n/nrows))\n    if double: ncols*=2 ; n*=2\n    fig,axs = subplots(nrows, ncols, figsize=figsize, **kwargs)\n    if flatten: axs = [ax if i<n else ax.set_axis_off() for i, ax in enumerate(axs.flatten())][:n]\n    if title is not None: fig.suptitle(title, weight='bold', size=14)\n    return (fig,axs) if return_fig else axs\n```\n\n----------------------------------------\n\nTITLE: Getting and setting random states for all RNGs (Python)\nDESCRIPTION: The `get_random_states` function captures current states of `random`, `numpy`, and `torch` RNGs, including CUDA and backend settings for determinism. The `set_random_states` function restores these states, enabling precise control over reproducibility and the ability to revert to previous RNG states during execution.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\ndef get_random_states():\n    \"Gets states for `random`, `torch`, and `numpy` random number generators\"\n    return {'random_state':random.getstate(),\n            'numpy_state':np.random.get_state(),\n            'torch_state':torch.get_rng_state(),\n            'torch_cuda_state':torch.cuda.get_rng_state_all(),\n            'torch_deterministic':torch.backends.cudnn.deterministic,\n            'torch_benchmark':torch.backends.cudnn.benchmark}\n\ndef set_random_states(random_state,numpy_state,torch_state,torch_cuda_state,torch_deterministic,torch_benchmark):\n    \"Set states for `random`, `torch`, and `numpy` random number generators\"\n    random.setstate(random_state)\n    np.random.set_state(numpy_state)\n    torch.set_rng_state(torch_state)\n    torch.cuda.set_rng_state_all(torch_cuda_state)\n    torch.backends.cudnn.deterministic=torch_deterministic\n    torch.backends.cudnn.benchmark=torch_benchmark\n```\n\n----------------------------------------\n\nTITLE: Unfreeze Language Model\nDESCRIPTION: Unfreezes all layers of the language model, allowing the entire model to be fine-tuned.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nlearn.unfreeze()\n```\n\n----------------------------------------\n\nTITLE: Updating Data Augmentations for Higher Resolution - Python\nDESCRIPTION: Re-defines augmentation transforms for higher-resolution (size=256) image inputs, accommodating more detailed features. Used to create new DataLoaders for extended training. Requires fastai.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ntfms = aug_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0., size=256)\n```\n\n----------------------------------------\n\nTITLE: Load ratings data from u.data\nDESCRIPTION: Loads the ratings data from the `u.data` file, which is a tab-separated file. Specifies the delimiter as `\t`, sets `header=None`, and provides the column names for the DataFrame including user, item, rating, and timestamp.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n                      names=[user,item,'rating','timestamp'])\nratings.head()\n```\n\n----------------------------------------\n\nTITLE: Defining a TabularLearner Class Extending fastai Learner in Python\nDESCRIPTION: Defines a `TabularLearner` class that inherits from fastai's `Learner` and specializes it for tabular data by implementing a custom `predict` method. This method accepts a pandas Series representing a single row of tabular features, creates a test dataloader, converts continuous variables to float32, obtains predictions and decoded outputs, then decodes the full batch to return human-readable results along with raw predictions and decoded predictions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/43_tabular.learner.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass TabularLearner(Learner):\n    \"`Learner` for tabular data\"\n    def predict(self, \n        row:pd.Series, # Features to be predicted\n    ):\n        \"Predict on a single sample\"\n        dl = self.dls.test_dl(row.to_frame().T)\n        dl.dataset.conts = dl.dataset.conts.astype(np.float32)\n        inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True)\n        b = (*tuplify(inp),*tuplify(dec_preds))\n        full_dec = self.dls.decode(b)\n        return full_dec,dec_preds[0],preds[0]\n```\n\n----------------------------------------\n\nTITLE: Converting columns to date format in Python\nDESCRIPTION: Ensures that a specified column is converted to a proper datetime format. Handles different date formats and timezone-aware datetime types.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef make_date(df, date_field):\n    \"Make sure `df[date_field]` is of the right date type.\"\n    field_dtype = df[date_field].dtype\n    if isinstance(field_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        field_dtype = np.datetime64\n    if not np.issubdtype(field_dtype, np.datetime64):\n        df[date_field] = pd.to_datetime(df[date_field], infer_datetime_format=True)\n```\n\n----------------------------------------\n\nTITLE: Getting Predictions on Test Set\nDESCRIPTION: Generates predictions for all samples in the specified test DataLoader (`dl`). This returns the raw model outputs (typically probabilities or logits) for the entire test dataset.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nlearn.get_preds(dl=dl)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Listing Dataset\nDESCRIPTION: Downloads and extracts the sample Adult dataset using fastai's built-in utility. It then lists the contents of the downloaded data directory.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npath = untar_data(URLs.ADULT_SAMPLE)\npath.ls()\n```\n\n----------------------------------------\n\nTITLE: fit_sgdr method\nDESCRIPTION: Defines the `fit_sgdr` method, which implements Stochastic Gradient Descent with Warm Restarts (SGDR) training. This training method uses a cosine annealing schedule for the learning rate within multiple cycles.  The method takes the number of cycles, cycle length, and learning rate as parameters.  It relies on `ParamScheduler` to apply the schedule and combines it with the provided callbacks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\ndef fit_sgdr(self:Learner, n_cycles, cycle_len, lr_max=None, cycle_mult=2, cbs=None, reset_opt=False, wd=None,\n             start_epoch=0):\n    \"Fit `self.model` for `n_cycles` of `cycle_len` using SGDR.\"\n    if self.opt is None: self.create_opt()\n    self.opt.set_hyper('lr', self.lr if lr_max is None else lr_max)\n    lr_max = np.array([h['lr'] for h in self.opt.hypers])\n    n_epoch = cycle_len * (cycle_mult**n_cycles-1)//(cycle_mult-1)\n    pcts = [cycle_len * cycle_mult**i / n_epoch for i in range(n_cycles)]\n    scheds = [SchedCos(lr_max, 0) for _ in range(n_cycles)]\n    scheds = {'lr': combine_scheds(pcts, scheds)}\n    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n```\n\n----------------------------------------\n\nTITLE: hook_output Function Definition\nDESCRIPTION: This code defines the `hook_output` function, which returns a `Hook` object that stores the activations of a given module in its `stored` attribute. It uses the `_hook_inner` function as the default hook function, which returns the output of the module if it's a Tensor or a list-like object, or a list of the outputs otherwise. The `detach`, `cpu`, and `grad` parameters control whether the activations are detached from their history, moved to the CPU, or whether to store gradients instead of outputs, respectively.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef _hook_inner(m,i,o): return o if isinstance(o,Tensor) or is_listy(o) else list(o)\n\ndef hook_output(module, detach=True, cpu=False, grad=False):\n    \"Return a `Hook` that stores activations of `module` in `self.stored`\"\n    return Hook(module, _hook_inner, detach=detach, cpu=cpu, is_forward=not grad)\n```\n\n----------------------------------------\n\nTITLE: Formatting Configuration Parameters Recursively\nDESCRIPTION: The `_format_config` function recursively formats configuration parameters before logging. It iterates through the key-value pairs in the config dictionary, recursively calling itself if the value is a dictionary.  It uses `_format_config_value` to format other values.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef _format_config(config):\n    \"Format config parameters before logging them\"\n    for k,v in config.items():\n        if isinstance(v, dict):\n            config[k] = _format_config(v)\n        else:\n            config[k] = _format_config_value(v)\n    return config\n```\n\n----------------------------------------\n\nTITLE: Setting Initial Learning Rate - Python\nDESCRIPTION: Assigns an initial learning rate value (0.01) determined from lr_find or prior knowledge. Value fed into training loop. No logic, only variable assignment.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nlr = 0.01\n```\n\n----------------------------------------\n\nTITLE: Method to revert model to contiguous format, with optional FP32 conversion\nDESCRIPTION: Provides a utility to reset the model's memory format to contiguous (default layout), with an option to convert inputs and model to FP32 for precision reduction or debugging.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18c_callback.channelslast.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n@patch\ndef to_contiguous(self:Learner, to_fp32:bool=False):\n    \"Set `Learner` and inputs to `contiguous_format` (default format), optionally to single precision\"\n    self.model.to(memory_format=torch.contiguous_format)\n    if to_fp32:\n        return self.remove_cbs([ChannelsLast, MixedPrecision])\n    else:\n        return self.remove_cb(ChannelsLast)\n```\n\n----------------------------------------\n\nTITLE: Defining Generator and Critic Models for Wasserstein GAN in Python fastai\nDESCRIPTION: Creates a generator and a critic model for Wasserstein GAN training using fastai's `basic_generator` and `basic_critic` utilities. Both models operate on 64x64 images with 3 color channels, including an extra layer each. The critic uses LeakyReLU activation with a negative slope of 0.2 to improve training stability.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-wgan.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ngenerator = basic_generator(64, n_channels=3, n_extra_layers=1)\ncritic    = basic_critic   (64, n_channels=3, n_extra_layers=1, act_cls=partial(nn.LeakyReLU, negative_slope=0.2))\n```\n\n----------------------------------------\n\nTITLE: Displaying Model Architecture - Python\nDESCRIPTION: Prints a summary of the created tabular neural network model's architecture, showing the layers and components.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nlearn.model\n```\n\n----------------------------------------\n\nTITLE: Initializing TabularPandas for Full Data with Timing - Python\nDESCRIPTION: Creates the `TabularPandas` object for the full training dataset, applying the complete list of processors, using the defined categorical and continuous features, the dependent variable, and the time-based splits. The `%time` magic command measures execution time.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n%time to = TabularPandas(train_df, procs, cat_names, cont_names, dep_var, y_block=TransformBlock(), splits=splits)\n```\n\n----------------------------------------\n\nTITLE: Test CSVLogger Functionality in Python\nDESCRIPTION: Tests the `CSVLogger` by first training a learner with it (as done in the previous snippet), then using the `read_log` method to load the generated CSV into a pandas DataFrame. It asserts that the columns match the recorder's metric names and that the logged values correspond to the recorder's values. Finally, it cleans up by removing the created CSV file.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/16_callback.progress.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf = learn.csv_logger.read_log()\ntest_eq(df.columns.values, learn.recorder.metric_names)\nfor i,v in enumerate(learn.recorder.values):\n    test_close(df.iloc[i][:3], [i] + v)\nos.remove(learn.path/learn.csv_logger.fname)\n```\n\n----------------------------------------\n\nTITLE: Scheduler Test\nDESCRIPTION: Tests the learning rate and momentum schedules generated by `fit_one_cycle`. It verifies that the scheduled learning rates and momentums match the expected values, using `test_close` to compare the recorded values against the calculated cosine schedules. It checks that the schedule follows a combined cosine pattern with the appropriate parameters.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n#Scheduler test\nlrs,moms = learn.recorder.hps['lr'],learn.recorder.hps['mom']\ntest_close(lrs,  [combined_cos(0.25,1e-2/25,1e-2,1e-7)(i/20) for i in range(20)])\ntest_close(moms, [combined_cos(0.25,0.95,0.85,0.95)(i/20) for i in range(20)])\n```\n\n----------------------------------------\n\nTITLE: Read Tabular Batch Transformation - Python\nDESCRIPTION: This class `ReadTabBatch` transforms `TabularPandas` objects into tensors for model input. It converts categorical and continuous variables to tensors and optionally includes target variables. It supports decoding tensors back into a `TabularPandas` representation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nclass ReadTabBatch(ItemTransform):\n    \"Transform `TabularPandas` values into a `Tensor` with the ability to decode\"\n    def __init__(self, to): self.to = to.new_empty()\n\n    def encodes(self, to):\n        if not to.with_cont: res = (tensor(to.cats).long(),)\n        else: res = (tensor(to.cats).long(),tensor(to.conts).float())\n        ys = [n for n in to.y_names if n in to.items.columns]\n        if len(ys) == len(to.y_names): res = res + (tensor(to.targ),)\n        if to.device is not None: res = to_device(res, to.device)\n        return res\n\n    def decodes(self, o):\n        o = [_maybe_expand(o_) for o_ in to_np(o) if o_.size != 0]\n        vals = np.concatenate(o, axis=1)\n        try: df = pd.DataFrame(vals, columns=self.to.all_col_names)\n        except: df = pd.DataFrame(vals, columns=self.to.x_names)\n        to = self.to.new(df)\n        return to\n```\n\n----------------------------------------\n\nTITLE: Testing DataLoader with bs, drop_last, num_workers\nDESCRIPTION: This code tests `DataLoader` functionality with various parameters, including `bs`, `drop_last`, and `num_workers`. It defines a `twoepochs` function to simulate multiple epochs over data loading. The tests verify the correct behavior when processing multiple batches, including scenarios with and without dropping the last incomplete batch and multiple workers. It checks that batching, dropping last batches, and multi-worker loading function as expected.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef twoepochs(d): return ' '.join(''.join(list(o)) for _ in range(2) for o in d)\n\nds1 = DataLoader(letters, bs=4, drop_last=True, num_workers=0)\ntest_eq(twoepochs(ds1), 'abcd efgh ijkl mnop qrst uvwx abcd efgh ijkl mnop qrst uvwx')\n\nds1 = DataLoader(letters,4,num_workers=2)\ntest_eq(twoepochs(ds1), 'abcd efgh ijkl mnop qrst uvwx yz abcd efgh ijkl mnop qrst uvwx yz')\n\nds1 = DataLoader(range(12), bs=4, num_workers=3)\ntest_eq_type(L(ds1), L(tensor([0,1,2,3]),tensor([4,5,6,7]),tensor([8,9,10,11])))\n\nds1 = DataLoader([str(i) for i in range(11)], bs=4, after_iter=lambda: setattr(t3, 'f', 2))\ntest_eq_type(L(ds1), L(['0','1','2','3'],['4','5','6','7'],['8','9','10']))\ntest_eq(t3.f, 2)\n\nit = iter(DataLoader(map(noop,range(20)), bs=4, num_workers=1))\ntest_eq_type([next(it) for _ in range(3)], [tensor([0,1,2,3]),tensor([4,5,6,7]),tensor([8,9,10,11])])\n```\n\n----------------------------------------\n\nTITLE: Training with One-Cycle Policy - fastai/Python\nDESCRIPTION: Trains the model using fastai's `fit_one_cycle` method. This method implements the one-cycle learning rate policy, training for a specified number of epochs (1) with a given maximum learning rate (1e-2), which often leads to faster convergence and better results.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(1, 1e-2)\n```\n\n----------------------------------------\n\nTITLE: Reloading DataLoaders with Updated Transformations - Python\nDESCRIPTION: Sets a new NumPy random seed for reproducibility and re-initializes DataLoaders with updated batch_tfms and normalization for 256x256 input. Prerequisites: fastai, numpy, and correctly prepared DataFrame.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(42)\ndls = planet.dataloaders(df, bs=64, path=path, batch_tfms=tfms+[Normalize.from_stats(*imagenet_stats)])\n```\n\n----------------------------------------\n\nTITLE: Resize to Target Size\nDESCRIPTION: This function calculates the dimensions to resize an image to, maintaining the aspect ratio, such that the larger dimension matches the target size. It can use either the minimum or the maximum dimension for the calculation, controlled by `use_min` parameter.  It accepts a PIL image and the target size as input.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef resize_to(img, targ_sz, use_min=False):\n    \"Size to resize to, to hit `targ_sz` at same aspect ratio, in PIL coords (i.e w*h)\"\n    w,h = img.size\n    min_sz = (min if use_min else max)(w,h)\n    ratio = targ_sz/min_sz\n    return int(w*ratio),int(h*ratio)\n```\n\n----------------------------------------\n\nTITLE: Importing fastai Tabular Modules\nDESCRIPTION: Imports all necessary functions and classes from the fastai tabular application module, making them available for use in the script.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom fastai.tabular.all import *\n```\n\n----------------------------------------\n\nTITLE: Implementing Pearson Correlation Coefficient for Regression in PyTorch\nDESCRIPTION: Function that creates a fastai metric for Pearson correlation coefficient using scipy's pearsonr function, wrapped in AccumMetric for batch computation in regression tasks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n@delegates(AccumMetric)\ndef PearsonCorrCoef(dim_argmax=None, **kwargs):\n    \"Pearson correlation coefficient for regression problem\"\n    def pearsonr(x,y): return scs.pearsonr(x,y)[0]\n    return AccumMetric(pearsonr, invert_arg=False, dim_argmax=dim_argmax, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Applying FillMissing to a New DataFrame\nDESCRIPTION: This example demonstrates how to apply the trained `FillMissing` processor (with learned `na_dict`) to a new DataFrame with missing values. This showcases the behavior of the processor on unseen data, ensuring the missing values are filled using the same strategy learned from the training data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndfa = cudf.from_pandas(pd.DataFrame({'a':[np.nan,0,np.nan]}))\ntos = [t.new(o) for t,o in zip(tos,(dfa,dfa.copy(),dfa.copy()))]\nfor t in tos: t.process()\nfor to_,v in zip(tos, [1.5, 0., 1.]):\n    test_eq(to_.a.to_array(), np.array([v, 0, v]))\n    test_eq(to_.a_na.to_array(), np.array([1, 0, 1]))\n```\n\n----------------------------------------\n\nTITLE: Example: Assert batch size and length for PartialDL in fastai Python\nDESCRIPTION: Provides assertions to verify the behavior of the `PartialDL`. It checks that the training dataloader (first element `dls[0]`) contains the correct number of batches based on `partial_n` and `bs`, and that each batch has the expected size.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14a_callback.data.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nassert len(dls[0])==2\nfor batch in dls[0]:\n    assert len(batch[0])==16\n```\n\n----------------------------------------\n\nTITLE: Creating Random Dataset Splitter\nDESCRIPTION: Function factory that creates a splitter function which randomly divides data into training and validation sets based on a specified validation percentage and optional random seed.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef RandomSplitter(valid_pct=0.2, seed=None):\n    \"Create function that splits `items` between train/val with `valid_pct` randomly.\"\n    def _inner(o):\n        if seed is not None: torch.manual_seed(seed)\n        rand_idx = L(list(torch.randperm(len(o)).numpy()))\n        cut = int(valid_pct * len(o))\n        return rand_idx[cut:],rand_idx[:cut]\n    return _inner\n```\n\n----------------------------------------\n\nTITLE: Importing fastai Modules\nDESCRIPTION: This snippet imports necessary modules from the fastai library.  It imports all functionality from `fastai.vision.all`.  This includes tools for data handling, model definition, and training.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom fastai.vision.all import *\n```\n\n----------------------------------------\n\nTITLE: Testing TextDataLoaders.from_folder with Custom Tokenizer - Python\nDESCRIPTION: This hidden snippet verifies that a custom tokenizer (`tok_tfm`) can be successfully passed to `TextDataLoaders.from_folder`. It tests this functionality for both language modeling (`is_lm=True`) and non-language modeling (`is_lm=False`) tasks, using a `WordTokenizer` instance.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n# Test that `tok_tfm` parameter works\npath = untar_data(URLs.IMDB)\ntknzer = WordTokenizer()\ndls = TextDataLoaders.from_folder(path, tok_tfm=tknzer, is_lm=True)\ndls.show_batch(max_n=1)\ndls = TextDataLoaders.from_folder(path, tok_tfm=tknzer, is_lm=False)\ndls.show_batch(max_n=1)\n```\n\n----------------------------------------\n\nTITLE: Removing a Single Callback from Learner\nDESCRIPTION: This code demonstrates removing a single callback from a `Learner` using the `remove_cb` method. It shows how to remove a specific callback instance from the `Learner`'s callback list, and confirms that the correct callback has been removed. The removal and the `learn` attribute are confirmed through assertions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner()\nlearn.add_cb(TestTrainEvalCallback())\ncb = learn.cbs[1]\nlearn.remove_cb(learn.cbs[1])\ntest_eq(len(learn.cbs), 1)\nassert cb.learn is None\nassert not getattr(learn,'test_train_eval',None)\n```\n\n----------------------------------------\n\nTITLE: Showing a sample from the training set\nDESCRIPTION: This snippet shows a sample image from the training set using `show_at()`. The index `0` specifies to show the first sample. Useful for visualizing the transformed data.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nshow_at(dsets.train, 0);\n```\n\n----------------------------------------\n\nTITLE: Testing CrossEntropyLossFlat\nDESCRIPTION: This code demonstrates how to use the `CrossEntropyLossFlat` class and tests its functionality. It creates an instance of the class, generates random input and target tensors, and verifies that the loss function works correctly. It also tests the associated activation (softmax) and decoding (argmax) methods.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntst = CrossEntropyLossFlat(reduction='none')\noutput = torch.randn(32, 5, 10)\ntarget = torch.randint(0, 10, (32,5))\n#nn.CrossEntropy would fail with those two tensors, but not our flattened version.\n_ = tst(output, target)\n\ntest_fail(lambda x: nn.CrossEntropyLoss()(output,target))\n\n#Associated activation is softmax\ntest_eq(tst.activation(output), F.softmax(output, dim=-1))\n#This loss function has a decodes which is argmax\ntest_eq(tst.decodes(output), output.argmax(dim=-1))\n```\n\n----------------------------------------\n\nTITLE: Show Results for Mask, Point, or Bounding Box Segmentation\nDESCRIPTION: This function visualizes segmentation masks, points, or bounding boxes comparing target and predicted outputs, arranging results in a grid with a 'Target/Prediction' title. It displays both the target and predicted annotations for a batch of images, facilitating qualitative evaluation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_41\n\nLANGUAGE: Python\nCODE:\n```\n@dispatch\ndef show_results(x:TensorImage, y:TensorMask|TensorPoint|TensorBBox, samples, outs, ctxs=None, max_n=6,\n                 nrows=None, ncols=1, figsize=None, **kwargs):\n    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), nrows=nrows, ncols=ncols, figsize=figsize, double=True,\n                                     title='Target/Prediction')\n    for i in range(2):\n        ctxs[::2] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(2*max_n))]\n    for o in [samples,outs]:\n        ctxs[1::2] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(o.itemgot(0),ctxs[1::2],range(2*max_n))]\n    return ctxs\n```\n\n----------------------------------------\n\nTITLE: Defining a Dataframe iloc Wrapper for Row and Column Access in Python\nDESCRIPTION: This class, _TabIloc, provides get/set access to rows by iloc and columns by name for a wrapped dataframe. It supports indexing via tuples separating rows and columns, where columns can be slices, lists, or single column names. This abstraction allows for intuitive selection of data subsets while maintaining compatibility with pandas iloc indexing. The dependency is a parent object with .items as a pandas DataFrame and a .new method to create new instances wrapping slices.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass _TabIloc:\n    \"Get/set rows by iloc and cols by name\"\n    def __init__(self,to): self.to = to\n    def __getitem__(self, idxs):\n        df = self.to.items\n        if isinstance(idxs,tuple):\n            rows,cols = idxs\n            cols = df.columns.isin(cols) if is_listy(cols) else df.columns.get_loc(cols)\n        else: rows,cols = idxs,slice(None)\n        return self.to.new(df.iloc[rows, cols])\n```\n\n----------------------------------------\n\nTITLE: Base Class for Eager Tabular Data Processing in fastai (Python)\nDESCRIPTION: TabularProc is a base class inheriting from InplaceTransform designed for fast, non-lazy transformations applied immediately to tabular data represented as pandas DataFrames. The setup method initializes transformations upon receiving data, and the name property includes stored arguments for debugging or logging. This class serves as a foundation for defining tabular data processors that must act eagerly rather than deferred, integrating with fastai's data pipeline.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nclass TabularProc(InplaceTransform):\n    \"Base class to write a non-lazy tabular processor for dataframes\"\n    def setup(self, items=None, train_setup=False): #TODO: properly deal with train_setup\n        super().setup(getattr(items,'train',items), train_setup=False)\n        # Procs are called as soon as data is available\n        return self(items.items if isinstance(items,Datasets) else items)\n\n    @property\n    def name(self): return f\"{super().name} -- {getattr(self,'__stored_args__',{})}\"\n```\n\n----------------------------------------\n\nTITLE: Implementing pad_input_chunk for Batch Processing with Chunk Padding\nDESCRIPTION: A function that applies chunk-based padding to a batch of samples, supporting multiple input tensors. It works with variable-length inputs and ensures consistent padding across a batch while preserving tensor types.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@delegates(pad_chunk)\ndef pad_input_chunk(samples, n_inp=1,**kwargs):\n    \"Pad `samples` by adding padding by chunks of size `seq_len`\"\n    max_len = max([len(s[n]) for s in samples for n in range(n_inp)])\n    padeds = [[pad_chunk(s[n],pad_len=max_len,**kwargs) for n in range(n_inp) ] for s in samples]\n    return [(*p, *s[n_inp:]) for p,s in zip(padeds,samples)]\n```\n\n----------------------------------------\n\nTITLE: Creating Train-Test Splitter Using scikit-learn\nDESCRIPTION: Function factory that creates a splitter function based on scikit-learn's train_test_split, supporting stratified sampling, fixed random states, and customizable test/train sizes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef TrainTestSplitter(test_size=0.2, random_state=None, stratify=None, train_size=None, shuffle=True):\n    \"Split `items` into random train and test subsets using sklearn train_test_split utility.\"\n    def _inner(o, **kwargs):\n        train,valid = train_test_split(range_of(o), test_size=test_size, random_state=random_state,\n                                        stratify=stratify, train_size=train_size, shuffle=shuffle)\n        return L(train), L(valid)\n    return _inner\n```\n\n----------------------------------------\n\nTITLE: Defining Child Widget Layout Observer\nDESCRIPTION: Internal helper function `_update_children` designed to be used as an observer callback. When the children of a widget change, it ensures each child has a layout with `flex = '0 0 auto'` if not already set, standardizing layout behavior within containers like `carousel`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09c_vision.widgets.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef _update_children(\n    change:dict # A dictionary holding the information about the changed widget\n):\n    \"Sets a value to the `layout` attribute on widget initialization and change\"\n    for o in change['owner'].children:\n        if not o.layout.flex: o.layout.flex = '0 0 auto'\n```\n\n----------------------------------------\n\nTITLE: Decorator for Creating fastai nn.Module Classes - Python\nDESCRIPTION: Defines a factory decorator for producing PyTorch nn.Module classes from standalone functions, automatically setting their init and forward methods as well as their docstrings. Requires Python's inspect library and fastai's basic_repr, merge, and nn imports. Accepts field names and default arguments to construct module signatures; inputs are arguments to the generated module, and outputs are module outputs as given by the decorated function. Supports modularity and code reuse across fastai.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef module(*flds, **defaults):\n    \"Decorator to create an `nn.Module` using `f` as `forward` method\"\n    pa = [inspect.Parameter(o, inspect.Parameter.POSITIONAL_OR_KEYWORD) for o in flds]\n    pb = [inspect.Parameter(k, inspect.Parameter.POSITIONAL_OR_KEYWORD, default=v)\n          for k,v in defaults.items()]\n    params = pa+pb\n    all_flds = [*flds,*defaults.keys()]\n\n    def _f(f):\n        class c(nn.Module):\n            def __init__(self, *args, **kwargs):\n                super().__init__()\n                for i,o in enumerate(args): kwargs[all_flds[i]] = o\n                kwargs = merge(defaults,kwargs)\n                for k,v in kwargs.items(): setattr(self,k,v)\n            __repr__ = basic_repr(all_flds)\n            forward = f\n        c.__signature__ = inspect.Signature(params)\n        c.__name__ = c.__qualname__ = f.__name__\n        c.__doc__  = f.__doc__\n        return c\n    return _f\n```\n\n----------------------------------------\n\nTITLE: Array Patch for cuDF DataFrames\nDESCRIPTION: This code patches the `__array__` method to `cudf.DataFrame` to allow direct conversion to NumPy arrays via the `.pandas()` method. This is useful for interoperability with libraries that expect NumPy arrays as input.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\ndef __array__(self:cudf.DataFrame): return self.pandas().__array__()\n```\n\n----------------------------------------\n\nTITLE: Resize to Target Size - Test\nDESCRIPTION: This code tests the `resize_to` function. It creates a fake image object with specified dimensions and calls the function with different target sizes and the `use_min` parameter set to different values. Then, it uses test_eq to compare the result of the function with the expected output to verify its correctness.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nclass _FakeImg():\n    def __init__(self, size): self.size=size\n\nimg = _FakeImg((200,500))\ntest_eq(resize_to(img, 400), [160,400])\ntest_eq(resize_to(img, 400, use_min=True), [400,1000])\n```\n\n----------------------------------------\n\nTITLE: Testing Learner Training Improves Loss in fastai (Python)\nDESCRIPTION: This snippet creates a synthetic Learner, computes initial loss, trains for 10 epochs, and then checks that the training process improved (lowered) the loss. It demonstrates the proper orchestration of model evaluation before and after training as well as correct device handling. Dependencies are synth_learner, learn.dls.one_batch, and the loss function.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n#Training a few epochs should make the model better\nlearn = synth_learner(lr=0.1)\nlearn(_before_epoch)\nlearn.model = learn.model.cpu()\nxb,yb = learn.dls.one_batch()\ninit_loss = learn.loss_func(learn.model(xb), yb)\nlearn.fit(10)\nxb,yb = learn.dls.one_batch()\nfinal_loss = learn.loss_func(learn.model(xb), yb)\nassert final_loss < init_loss, (final_loss,init_loss)\n```\n\n----------------------------------------\n\nTITLE: Adding Reset Method to DistributedDataParallel\nDESCRIPTION: Patches the DistributedDataParallel class to include a reset method that delegates to the wrapped module, ensuring consistent behavior with the regular model during distributed training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@patch\ndef reset(self: DistributedDataParallel):\n    \"Patch required `reset` call into `DistributedDataParallel`\"\n    if hasattr(self.module, 'reset'): self.module.reset()\n```\n\n----------------------------------------\n\nTITLE: In-place Sigmoid with Clamping\nDESCRIPTION: This function, `sigmoid_`, applies the sigmoid function in-place to the input and clamps the output to the range `(eps, 1-eps)` to avoid numerical instability. `eps` defaults to 1e-7.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef sigmoid_(input, eps=1e-7):\n    \"Same as `torch.sigmoid_`, plus clamping to `(eps,1-eps)\"\n    return input.sigmoid_().clamp_(eps,1-eps)\n```\n\n----------------------------------------\n\nTITLE: Distributed Synchronization Barrier for PyTorch Distributed Training in Python\nDESCRIPTION: Implements a synchronization barrier for distributed training that blocks execution until all processes in the process group reach the barrier point. It checks if multiple distributed processes exist and if the PyTorch distributed backend is initialized before invoking torch.distributed.barrier.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_84\n\nLANGUAGE: python\nCODE:\n```\ndef distrib_barrier():\n    \"Place a synchronization barrier in distributed training\"\n    if num_distrib() > 1 and torch.distributed.is_initialized(): torch.distributed.barrier()\n```\n\n----------------------------------------\n\nTITLE: Utility Functions to Extract Elements and Tokenizer from Datasets in Python\nDESCRIPTION: Defines helper functions for internal usage: _maybe_first extracts the first element if input is a tuple, else returns input unchanged; _get_tokenizer attempts to retrieve a Tokenizer instance from the dataset, handling cases where tokenizer may be a list; _get_lengths obtains sequence lengths from dataset items using the tokenizer's get_lengths method. These helpers facilitate consistent retrieval of auxiliary data needed for language modeling preprocessing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef _maybe_first(o): return o[0] if isinstance(o, tuple) else o\n```\n\nLANGUAGE: python\nCODE:\n```\ndef _get_tokenizer(ds):\n    tok = getattr(ds, 'tokenizer', None)\n    if isinstance(tok, Tokenizer): return tok\n    if isinstance(tok, (list,L)):\n        for t in tok:\n            if isinstance(t, Tokenizer): return t\n```\n\nLANGUAGE: python\nCODE:\n```\ndef _get_lengths(ds):\n    tok = _get_tokenizer(ds)\n    if tok is None: return\n    return tok.get_lengths(ds.items)\n```\n\n----------------------------------------\n\nTITLE: Initializing TabularPandas with FillMissing - Python\nDESCRIPTION: Creates another `TabularPandas` object, this time demonstrating the application of the `FillMissing` processor along with `Categorify` for the small sample.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nsplits = [list(range(1000)),list(range(1000,2000))]\nto = TabularPandas(small_df.copy(), FillMissing, cat_names=small_cat_vars, cont_names=small_cont_vars, splits=splits)\n```\n\n----------------------------------------\n\nTITLE: Exporting Module with nbdev in Python\nDESCRIPTION: Exports the module using nbdev's export functionality to generate the final Python module from the notebook.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Adding a Single Callback to Learner\nDESCRIPTION: This code adds a single callback using the `add_cb` method to an existing `Learner` instance. It demonstrates the usage of `add_cb` and verifies that the callback is correctly added and linked to the learner instance, with `test_eq` and `isinstance` used for assertions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner()\nlearn.add_cb(TestTrainEvalCallback())\ntest_eq(len(learn.cbs), 2)\nassert isinstance(learn.cbs[1], TestTrainEvalCallback)\ntest_eq(learn.train_eval.learn, learn)\n```\n\n----------------------------------------\n\nTITLE: Setting Batch Size for Classification DataLoader (Python)\nDESCRIPTION: Defines the batch size (bs=64) used when creating classification DataLoaders. Input: N/A; Output: integer bs used in downstream DataLoader creation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nbs = 64\n```\n\n----------------------------------------\n\nTITLE: Saving Intermediate Model After 256x256 Training - Python\nDESCRIPTION: Saves the model's weights after fitting on high-resolution images under the label 'stage-1-256-rn50'. Can be loaded for later fine-tuning or inference.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nlearn.save('stage-1-256-rn50')\n```\n\n----------------------------------------\n\nTITLE: Creating Vocabulary Embeddings and Computing BLEU Score with PyTorch\nDESCRIPTION: Provides utility functions to create one-hot style vocabulary embeddings for predicted token sequences and to compute the BLEU score using the CorpusBLEUMetric class. These functions use PyTorch tensors and require a testing learner object compatible with fastai's conventions. The create_vcb_emb function generates a one-hot encoded representation of predictions based on the vocabulary size derived from predictions and targets. The compute_bleu_val function resets the BLEU metric, simulates a learner batch containing predictions and targets, accumulates the BLEU calculation, and returns the final metric value. The snippet includes test cases showing example calls with tensors, validating BLEU scores around 0.48549.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ndef create_vcb_emb(pred, targ):\n    # create vocab \"embedding\" for predictions\n    vcb_sz = max(torch.unique(torch.cat([pred, targ])))+1\n    pred_emb=torch.zeros(pred.size()[0], pred.size()[1] ,vcb_sz)\n    for i,v in enumerate(pred):\n        pred_emb[i].scatter_(1, v.view(len(v),1),1)\n    return pred_emb\n\ndef compute_bleu_val(met, x1, x2):\n    met.reset()\n    learn = TstLearner()\n    learn.training=False    \n    for i in range(len(x1)): \n        learn.pred,learn.yb = x1, (x2,)\n        met.accumulate(learn)\n    return met.value\n\ntarg = torch.tensor([[1,2,3,4,5,6,1,7,8]]) \npred = torch.tensor([[1,9,3,4,5,6,1,10,8]])\npred_emb = create_vcb_emb(pred, targ)\ntest_close(compute_bleu_val(CorpusBLEUMetric(), pred_emb, targ), 0.48549)\n\ntarg = torch.tensor([[1,2,3,4,5,6,1,7,8],[1,2,3,4,5,6,1,7,8]]) \npred = torch.tensor([[1,9,3,4,5,6,1,10,8],[1,9,3,4,5,6,1,10,8]])\npred_emb = create_vcb_emb(pred, targ)\ntest_close(compute_bleu_val(CorpusBLEUMetric(), pred_emb, targ), 0.48549)\n```\n\n----------------------------------------\n\nTITLE: Defining CrossEntropyLossFlat in Fastai\nDESCRIPTION: This code defines the `CrossEntropyLossFlat` class, which inherits from `BaseLoss` and provides a flattened version of `nn.CrossEntropyLoss`. It includes methods for decoding the model output (argmax) and applying the softmax activation function. It leverages `delegates()` to inherit arguments from `nn.CrossEntropyLoss`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@delegates()\nclass CrossEntropyLossFlat(BaseLoss):\n    \"Same as `nn.CrossEntropyLoss`, but flattens input and target.\"\n    y_int = True # y interpolation\n    @use_kwargs_dict(keep=True, weight=None, ignore_index=-100, reduction='mean')\n    def __init__(self, \n        *args, \n        axis:int=-1, # Class axis\n        **kwargs\n    ): \n        super().__init__(nn.CrossEntropyLoss, *args, axis=axis, **kwargs)\n    \n    def decodes(self, x:Tensor) -> Tensor:    \n        \"Converts model output to target format\"\n        return x.argmax(dim=self.axis)\n    \n    def activation(self, x:Tensor) -> Tensor: \n        \"`nn.CrossEntropyLoss`'s fused activation function applied to model output\"\n        return F.softmax(x, dim=self.axis)\n```\n\n----------------------------------------\n\nTITLE: Setting Path to IMDB Sample Dataset - Python\nDESCRIPTION: This snippet downloads and extracts the smaller IMDB sample dataset provided by fastai's URLs and utility functions. The resulting path to the extracted data is stored in the `path` variable, which serves as a base directory for subsequent data loading examples.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\npath = untar_data(URLs.IMDB_SAMPLE)\n```\n\n----------------------------------------\n\nTITLE: Getting Image Files from a Path\nDESCRIPTION: Specialized function to retrieve only image files from a specified path, with support for recursion and folder filtering.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef get_image_files(path, recurse=True, folders=None):\n    \"Get image files in `path` recursively, only in `folders`, if specified.\"\n    return get_files(path, extensions=image_extensions, recurse=recurse, folders=folders)\n```\n\n----------------------------------------\n\nTITLE: Hook Class Definition\nDESCRIPTION: This code defines the `Hook` class, which provides a convenient way to register and manage hooks for PyTorch modules. It allows users to specify a hook function, control whether to detach, move to CPU, or gather the input/output tensors before passing them to the hook function, and provides methods for removing the hook and using it as a context manager.  The `hook_fn` applies the user-defined `hook_func` to the module's input and output.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@docs\nclass Hook():\n    \"Create a hook on `m` with `hook_func`.\"\n    def __init__(self, m, hook_func, is_forward=True, detach=True, cpu=False, gather=False):\n        store_attr('hook_func,detach,cpu,gather')\n        f = m.register_forward_hook if is_forward else m.register_backward_hook\n        self.hook = f(self.hook_fn)\n        self.stored,self.removed = None,False\n\n    def hook_fn(self, module, input, output):\n        \"Applies `hook_func` to `module`, `input`, `output`.\"\n        if self.detach:\n            input,output = to_detach(input, cpu=self.cpu, gather=self.gather),to_detach(output, cpu=self.cpu, gather=self.gather)\n        self.stored = self.hook_func(module, input, output)\n\n    def remove(self):\n        \"Remove the hook from the model.\"\n        if not self.removed:\n            self.hook.remove()\n            self.removed=True\n\n    def __enter__(self, *args): return self\n    def __exit__(self, *args): self.remove()\n\n    _docs = dict(__enter__=\"Register the hook\",\n                 __exit__=\"Remove the hook\")\n```\n\n----------------------------------------\n\nTITLE: Allowing Passthrough of Already-Decoded LabeledBBox in MultiCategorize - fastai (Python)\nDESCRIPTION: Defines a decodes method in MultiCategorize that simply returns LabeledBBox objects unchanged, preventing unnecessary categorization of objects already in decoded form. Requires MultiCategorize and LabeledBBox definitions. Input is a LabeledBBox, output is the same object.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n#LabeledBBox can be sent in a tl with MultiCategorize (depending on the order of the tls) but it is already decoded.\n@MultiCategorize\ndef decodes(self, x:LabeledBBox): return x\n```\n\n----------------------------------------\n\nTITLE: Download Wikitext Tiny Dataset\nDESCRIPTION: This line downloads and extracts the Wikitext Tiny dataset using the `untar_data` function from fastai, storing the path to the extracted data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/35_tutorial.wikitext.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.WIKITEXT_TINY)\n```\n\n----------------------------------------\n\nTITLE: Defining CometCallback Class\nDESCRIPTION: This defines a custom callback class named `CometCallback` that inherits from the `Callback` class within fastai. The purpose of this callback is to log training metrics, losses, model weights, and architecture summaries to Comet.ml during the fastai training process. It integrates into fastai's training loop via `order`, and handles logging at various stages (before fit, after batch, after epoch, and after fit).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70d_callback.comet.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass CometCallback(Callback):\n    \"Log losses, metrics, model weights, model architecture summary to neptune\"\n    order = Recorder.order + 1\n\n    def __init__(self, project_name, log_model_weights=True):\n        self.log_model_weights = log_model_weights\n        self.keep_experiment_running = keep_experiment_running\n        self.project_name = project_name\n        self.experiment = None\n\n    def before_fit(self):\n        try:\n            self.experiment = comet_ml.Experiment(project_name=self.project_name)\n        except ValueError:\n            print(\"No active experiment\")\n\n        try:\n            self.experiment.log_parameter(\"n_epoch\", str(self.learn.n_epoch))\n            self.experiment.log_parameter(\"model_class\", str(type(self.learn.model)))\n        except:\n            print(f\"Did not log all properties.\")\n\n        try:\n            with tempfile.NamedTemporaryFile(mode=\"w\") as f:\n                with open(f.name, \"w\") as g:\n                    g.write(repr(self.learn.model))\n                self.experiment.log_asset(f.name, \"model_summary.txt\")\n        except:\n            print(\"Did not log model summary. Check if your model is PyTorch model.\")\n\n        if self.log_model_weights and not hasattr(self.learn, \"save_model\"):\n            print(\n                \"Unable to log model to Comet.\\n\",\n            )\n\n    def after_batch(self):\n        # log loss and opt.hypers\n        if self.learn.training:\n            self.experiment.log_metric(\"batch__smooth_loss\", self.learn.smooth_loss)\n            self.experiment.log_metric(\"batch__loss\", self.learn.loss)\n            self.experiment.log_metric(\"batch__train_iter\", self.learn.train_iter)\n            for i, h in enumerate(self.learn.opt.hypers):\n                for k, v in h.items():\n                    self.experiment.log_metric(f\"batch__opt.hypers.{k}\", v)\n\n    def after_epoch(self):\n        # log metrics\n        for n, v in zip(self.learn.recorder.metric_names, self.learn.recorder.log):\n            if n not in [\"epoch\", \"time\"]:\n                self.experiment.log_metric(f\"epoch__{n}\", v)\n            if n == \"time\":\n                self.experiment.log_text(f\"epoch__{n}\", str(v))\n\n        # log model weights\n        if self.log_model_weights and hasattr(self.learn, \"save_model\"):\n            if self.learn.save_model.every_epoch:\n                _file = join_path_file(\n                    f\"{self.learn.save_model.fname}_{self.learn.save_model.epoch}\",\n                    self.learn.path / self.learn.model_dir,\n                    ext=\".pth\",\n                )\n            else:\n                _file = join_path_file(\n                    self.learn.save_model.fname,\n                    self.learn.path / self.learn.model_dir,\n                    ext=\".pth\",\n                )\n            self.experiment.log_asset(_file)\n\n    def after_fit(self):\n        try:\n            self.experiment.end()\n        except:\n            print(\"No neptune experiment to stop.\")\n```\n\n----------------------------------------\n\nTITLE: Setting up dependencies for tabular data processing in Python\nDESCRIPTION: Imports necessary FastAI modules and configures pandas to raise errors for chained assignments to prevent silent data modifications.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.torch_basics import *\nfrom fastai.data.all import *\n\npd.set_option('mode.chained_assignment','raise')\n```\n\n----------------------------------------\n\nTITLE: Accessing Train and Validation Items from TfmdLists (Python)\nDESCRIPTION: Retrieves individual examples from the training and validation splits of the TfmdLists object for inspection or debugging. Expects a properly constructed TfmdLists instance with defined splits.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntls.train[0],tls.valid[0]\n```\n\n----------------------------------------\n\nTITLE: AWD-LSTM CUDA Test - PyTorch\nDESCRIPTION: This snippet tests the `AWD_LSTM` model with a bidir parameter set to True using the CUDA. It does an initialization using the to('cuda') method to move the model to a GPU, calls the reset method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n#|cuda\ntst = AWD_LSTM(100, 20, 10, 2, bidir=True).to('cuda')\ntst.reset()\n```\n\n----------------------------------------\n\nTITLE: Implementing CollabDataLoaders\nDESCRIPTION: Creates a DataLoaders class with factory methods specialized for collaborative filtering data from DataFrames or CSV files.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/45_collab.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass CollabDataLoaders(DataLoaders):\n    \"Base `DataLoaders` for collaborative filtering.\"\n    @delegates(DataLoaders.from_dblock)\n    @classmethod\n    def from_df(cls, ratings, valid_pct=0.2, user_name=None, item_name=None, rating_name=None, seed=None, path='.', **kwargs):\n        \"Create a `DataLoaders` suitable for collaborative filtering from `ratings`.\"\n        user_name   = ifnone(user_name,   ratings.columns[0])\n        item_name   = ifnone(item_name,   ratings.columns[1])\n        rating_name = ifnone(rating_name, ratings.columns[2])\n        cat_names = [user_name,item_name]\n        splits = RandomSplitter(valid_pct=valid_pct, seed=seed)(range_of(ratings))\n        to = TabularCollab(ratings, [Categorify], cat_names, y_names=[rating_name], y_block=TransformBlock(), splits=splits)\n        return to.dataloaders(path=path, **kwargs)\n\n    @classmethod\n    def from_csv(cls, csv, **kwargs):\n        \"Create a `DataLoaders` suitable for collaborative filtering from `csv`.\"\n        return cls.from_df(pd.read_csv(csv), **kwargs)\n\nCollabDataLoaders.from_csv = delegates(to=CollabDataLoaders.from_df)(CollabDataLoaders.from_csv)\n```\n\n----------------------------------------\n\nTITLE: Implementing Root Mean Squared Error (RMSE) Metric in PyTorch\nDESCRIPTION: Creates a root mean squared error metric by defining a function that calculates RMSE using the square root of MSE loss, then wraps it in an AccumMetric for batch processing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndef _rmse(inp, targ): return torch.sqrt(F.mse_loss(inp, targ))\nrmse = AccumMetric(_rmse)\nrmse.__doc__ = \"Root mean squared error\"\n```\n\n----------------------------------------\n\nTITLE: Unit Testing Embedding Remapping (With Bias) - Python\nDESCRIPTION: Extends match_embeds unit tests by including decoder bias, using randomized weights and biases for embedding remapping. Fastai's test_eq is used for assertion. Inputs: similar to prior snippet, but with both embedding and bias present. No output unless assertion fails. Used for internal validation of compatibility with bias handling.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#With bias\nwgts = {'0.encoder.weight': torch.randn(5,3), '1.decoder.bias': torch.randn(5)}\nnew_wgts = match_embeds(wgts.copy(), ['a', 'b', 'c'], ['a', 'c', 'd', 'b'])\nold_w,new_w = wgts['0.encoder.weight'],new_wgts['0.encoder.weight']\nold_b,new_b = wgts['1.decoder.bias'],  new_wgts['1.decoder.bias']\ntest_eq(new_w[0], old_w[0])\ntest_eq(new_w[1], old_w[2])\ntest_eq(new_w[2], old_w.mean(0))\ntest_eq(new_w[3], old_w[1])\ntest_eq(new_b[0], old_b[0])\ntest_eq(new_b[1], old_b[2])\ntest_eq(new_b[2], old_b.mean(0))\ntest_eq(new_b[3], old_b[1])\n```\n\n----------------------------------------\n\nTITLE: Pooling Linear Classifier\nDESCRIPTION: The `PoolingLinearClassifier` is a `Module` that combines the sentence encoder output with a linear classifier. It takes a `dims` list (hidden sizes for MLP), `ps` list (dropout probabilities), a `bptt` and an optional `y_range`. The `forward` method passes the output and mask from the sentence encoder to `masked_concat_pool` and feeds it into an MLP with dropout layers.  It uses the `mask` to determine which parts of the output should be used in the pooling and classification and includes a sigmoid range to clamp outputs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass PoolingLinearClassifier(Module):\n    \"Create a linear classifier with pooling\"\n    def __init__(self, \n        dims:list, # List of hidden sizes for MLP as `int`s \n        ps:list, # List of dropout probabilities as `float`s\n        bptt:int, # Backpropagation through time\n        y_range:tuple=None # Tuple of (low, high) output value bounds\n     ):\n        if len(ps) != len(dims)-1: raise ValueError(\"Number of layers and dropout values do not match.\")\n        acts = [nn.ReLU(inplace=True)] * (len(dims) - 2) + [None]\n        layers = [LinBnDrop(i, o, p=p, act=a) for i,o,p,a in zip(dims[:-1], dims[1:], ps, acts)]\n        if y_range is not None: layers.append(SigmoidRange(*y_range))\n        self.layers = nn.Sequential(*layers)\n        self.bptt = bptt\n\n    def forward(self, input):\n        out,mask = input\n        x = masked_concat_pool(out, mask, self.bptt)\n        x = self.layers(x)\n        return x, out, out\n```\n\n----------------------------------------\n\nTITLE: Datasets with Index and Mask Access\nDESCRIPTION: This code demonstrates how `Datasets` allows access to individual items, multiple items by indices, and masking selections via boolean lists. It verifies correct retrieval and behavior with different selection methods.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_46\n\nLANGUAGE: Python\nCODE:\n```\ninp = [0,1,2,3,4]\ndsets = Datasets(inp, tfms=[None])\ntest_eq(*dsets[2], 2)          # Retrieve one item (subset 0 is the default)\ntest_eq(dsets[1,2], [(1,),(2,)])    # Retrieve two items by index\nmask = [True,False,False,True,False]\ntest_eq(dsets[mask], [(0,),(3,)])   # Retrieve two items by mask\n```\n\n----------------------------------------\n\nTITLE: Define WeightedDL Dataloader in fastai Python\nDESCRIPTION: Implements a custom `TfmdDL` (fastai's transformed DataLoader) that samples data points during training based on provided weights. The weights (`wgts`) are normalized and used with `np.random.choice` to select indices for each batch when `shuffle` is enabled. Weights are only applied to the training set.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14a_callback.data.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@delegates()\nclass WeightedDL(TfmdDL):\n    \"Weighted dataloader where `wgts` is used for the training set only\"\n    def __init__(self, dataset=None, bs=None, wgts=None, **kwargs):\n        wgts = array([1.]*len(dataset) if wgts is None else wgts)\n        self.wgts = wgts/wgts.sum()\n        super().__init__(dataset=dataset, bs=bs, **kwargs)\n\n    def get_idxs(self):\n        if self.n==0: return []\n        if not self.shuffle: return super().get_idxs()\n        return list(np.random.choice(self.n, self.n, p=self.wgts))\n```\n\n----------------------------------------\n\nTITLE: Calling Callbacks Internally in Learner\nDESCRIPTION: This snippet shows how callbacks are invoked inside the `Learner` during various events. It utilizes a `VerboseCallback` to print event names, helping with debugging by showing event triggers. The `learn('after_fit')` call simulates calling a callback at the end of the training process, as an example.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner(cbs=VerboseCallback())\nlearn('after_fit')\n```\n\n----------------------------------------\n\nTITLE: Testing Internal State of PointScaler on Object Detection Datasets - fastai (Python)\nDESCRIPTION: Checks that PointScaler tracks and stores the size of images on Coco-style detection datasets. Useful for internal invariants and pipeline validation. Requires PointScaler, test_eq, coco_tds, and TensorBBox/TensorPoint definitions. No return except assertion error.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n#Check the size was grabbed by PointScaler and added to y\ntfm = PointScaler()\ntfm.as_item=False\nx,y,z = tfm(coco_tds[0])\ntest_eq(tfm.sz, x.size)\ntest_eq(y.img_size, x.size)\n```\n\n----------------------------------------\n\nTITLE: Resize Images Function - Test\nDESCRIPTION: This code tests the `resize_images` function. It creates a temporary directory with a subdirectory named 'resized_images', and calls the function to resize the images in the specified 'images' directory to 100 max_size. It uses `max_workers=0` which prevents parallel processing. The test also includes recursion.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nwith tempfile.TemporaryDirectory() as d:\n    dest = Path(d)/'resized_images'\n    resize_images('images', max_size=100, dest=dest, max_workers=0, recurse=True)\n```\n\n----------------------------------------\n\nTITLE: DataLoaders Class Definition and Core Methods (fastai, Python)\nDESCRIPTION: Defines the DataLoaders class, which acts as a container for several DataLoader objects, providing interfaces for train/valid splits, transformation, and device handling. Core methods include initialization, property setters for device and dataset access, data loading, and transformation management. This is the main abstraction for managing fastai data loading pipelines and requires fastai, PyTorch, and their dataset/transform abstractions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@docs\nclass DataLoaders(GetAttr):\n    \"Basic wrapper around several `DataLoader`s.\"\n    _default='train'\n    def __init__(self, \n        *loaders, # `DataLoader` objects to wrap\n        path:str|Path='.', # Path to store export objects\n        device=None # Device to put `DataLoaders`\n    ):\n        self.loaders,self.path = list(loaders),Path(path)\n        if device is not None and (loaders!=() and hasattr(loaders[0],'to')): self.device = device\n\n    def __getitem__(self, i): return self.loaders[i]\n    def __len__(self): return len(self.loaders)\n    def new_empty(self):\n        loaders = [dl.new(dl.dataset.new_empty()) for dl in self.loaders]\n        return type(self)(*loaders, path=self.path, device=self.device)\n\n    def _set(i, self, v): self.loaders[i] = v\n    train   ,valid    = add_props(lambda i,x: x[i], _set)\n    train_ds,valid_ds = add_props(lambda i,x: x[i].dataset)\n\n    @property\n    def device(self): return self._device\n\n    @device.setter\n    def device(self, \n        d # Device to put `DataLoaders`\n    ):\n        for dl in self.loaders: dl.to(d)\n        self._device = d\n\n    def to(self, \n        device # Device to put `DataLoaders`\n    ):\n        self.device = device\n        return self\n            \n    def _add_tfms(self, tfms, event, dl_idx):\n        \"Adds `tfms` to `event` on `dl`\"\n        if(isinstance(dl_idx,str)): dl_idx = 0 if(dl_idx=='train') else 1\n        dl_tfms = getattr(self[dl_idx], event)\n        apply(dl_tfms.add, tfms)\n        \n    def add_tfms(self,\n        tfms, # List of `Transform`(s) or `Pipeline` to apply\n        event, # When to run `Transform`. Events mentioned in `TfmdDL`\n        loaders=None # List of `DataLoader` objects to add `tfms` to\n    ):\n        \"Adds `tfms` to `events` on `loaders`\"\n        if(loaders is None): loaders=range(len(self.loaders))\n        if not is_listy(loaders): loaders = listify(loaders)\n        for loader in loaders:\n            self._add_tfms(tfms,event,loader)      \n\n    def cuda(self): return self.to(device=default_device())\n    def cpu(self):  return self.to(device=torch.device('cpu'))\n\n    @classmethod\n    def from_dsets(cls, \n        *ds, # `Datasets` object(s)\n        path:str|Path='.', # Path to put in `DataLoaders`\n        bs:int=64, # Size of batch\n        device=None, # Device to put `DataLoaders`\n        dl_type=TfmdDL, # Type of `DataLoader`\n        **kwargs\n    ):\n        default = (True,) + (False,) * (len(ds)-1)\n        defaults = {'shuffle': default, 'drop_last': default}\n        tfms = {k:tuple(Pipeline(kwargs[k]) for i in range_of(ds)) for k in _batch_tfms if k in kwargs}\n        kwargs = merge(defaults, {k: tuplify(v, match=ds) for k,v in kwargs.items() if k not in _batch_tfms}, tfms)\n        kwargs = [{k: v[i] for k,v in kwargs.items()} for i in range_of(ds)]\n        return cls(*[dl_type(d, bs=bs, **k) for d,k in zip(ds, kwargs)], path=path, device=device)\n\n    @classmethod\n    def from_dblock(cls, \n        dblock, # `DataBlock` object\n        source, # Source of data. Can be `Path` to files\n        path:str|Path='.', # Path to put in `DataLoaders`\n        bs:int=64, # Size of batch\n        val_bs:int=None, # Size of batch for validation `DataLoader`\n        shuffle:bool=True, # Whether to shuffle data\n        device=None, # Device to put `DataLoaders`\n        **kwargs\n    ):\n        return dblock.dataloaders(source, path=path, bs=bs, val_bs=val_bs, shuffle=shuffle, device=device, **kwargs)\n\n    _docs=dict(__getitem__=\"Retrieve `DataLoader` at `i` (`0` is training, `1` is validation)\",\n               train=\"Training `DataLoader`\",\n               valid=\"Validation `DataLoader`\",\n               train_ds=\"Training `Dataset`\",\n               valid_ds=\"Validation `Dataset`\",\n               to=\"Use `device`\",\n               add_tfms=\"Add `tfms` to `loaders` for `event\",\n               cuda=\"Use accelerator if available\",\n               cpu=\"Use the cpu\",\n               new_empty=\"Create a new empty version of `self` with the same transforms\",\n               from_dblock=\"Create a dataloaders from a given `dblock`\")\n```\n\n----------------------------------------\n\nTITLE: Initializing TabularPandas with Categorify - Python\nDESCRIPTION: Creates a `TabularPandas` object, a fastai class for managing tabular data preprocessing. It applies the `Categorify` processor to the specified categorical columns and defines the manual data splits.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nto = TabularPandas(small_df.copy(), Categorify, cat_names=small_cat_vars, cont_names=small_cont_vars, splits=splits)\n```\n\n----------------------------------------\n\nTITLE: Creating Movie Component\nDESCRIPTION: This code creates a list called `movie_comp` associating the first principal component with movie titles.  It is used for sorting and identifying movies with the highest and lowest scores along the first dimension of PCA.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nmovie_comp = [(f, i) for f,i in zip(fac0, top_movies)]\n```\n\n----------------------------------------\n\nTITLE: Download Image Inner Function - Test\nDESCRIPTION: This code tests the `_download_image_inner` function. It uses `tempfile.TemporaryDirectory` to create a temporary directory, downloads an image using the function with different settings (preserve_filename=False and True) and verifies the number of files and the filenames in the directory after download to ensure that they were downloaded successfully.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nwith tempfile.TemporaryDirectory() as d:\n    d = Path(d)\n    url = \"https://www.fast.ai/images/jh-head.jpg\"\n    _download_image_inner(d, (125,url))\n    test_eq(len(d.ls()), 1)\n\nwith tempfile.TemporaryDirectory() as d:\n    d = Path(d)\n    url = \"https://www.fast.ai/images/jh-head.jpg\"\n\n    _download_image_inner(d, (125,url), preserve_filename=True)\n    assert (d/'jh-head.jpg').is_file()\n    assert not (d/'jh-head.jpg1').exists()\n\n    _download_image_inner(d, (125,url), preserve_filename=True)\n    assert (d/'jh-head.jpg').is_file()\n    assert (d/'jh-head1.jpg').is_file()\n```\n\n----------------------------------------\n\nTITLE: Finding Smaller Data Types for DataFrame Columns (Python)\nDESCRIPTION: This function, `df_shrink_dtypes`, analyzes DataFrame columns and identifies smaller data types that can accommodate the existing data. It handles integer, float, and object columns, and provides options for converting object columns to category and integer columns to unsigned integers. It returns a dictionary mapping column names to their suggested new data types. Dependencies: numpy.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef df_shrink_dtypes(df, skip=[], obj2cat=True, int2uint=False):\n    \"Return any possible smaller data types for DataFrame columns. Allows `object`->`category`, `int`->`uint`, and exclusion.\"\n\n    # 1: Build column filter and typemap\n    excl_types, skip = {'category','datetime64[ns]','bool'}, set(skip)\n\n    typemap = {'int'   : [(np.dtype(x), np.iinfo(x).min, np.iinfo(x).max) for x in (np.int8, np.int16, np.int32, np.int64)],\n               'uint'  : [(np.dtype(x), np.iinfo(x).min, np.iinfo(x).max) for x in (np.uint8, np.uint16, np.uint32, np.uint64)],\n               'float' : [(np.dtype(x), np.finfo(x).min, np.finfo(x).max) for x in (np.float32, np.float64, np.longdouble)]\n              }\n    if obj2cat: typemap['object'] = 'category'  # User wants to categorify dtype('Object'), which may not always save space\n    else:       excl_types.add('object')\n\n    new_dtypes = {}\n    exclude = lambda dt: dt[1].name not in excl_types and dt[0] not in skip\n\n    for c, old_t in filter(exclude, df.dtypes.items()):\n        t = next((v for k,v in typemap.items() if old_t.name.startswith(k)), None)\n\n        if isinstance(t, list): # Find the smallest type that fits\n            if int2uint and t==typemap['int'] and df[c].min() >= 0: t=typemap['uint']\n            new_t = next((r[0] for r in t if r[1]<=df[c].min() and r[2]>=df[c].max()), None)\n            if new_t and new_t == old_t: new_t = None\n        else: new_t = t if isinstance(t, str) else None\n\n        if new_t: new_dtypes[c] = new_t\n    return new_dtypes\n```\n\n----------------------------------------\n\nTITLE: Getting Predictions on Test Set - Python\nDESCRIPTION: Uses the trained model to generate predictions on the test dataset DataLoaders. It returns the predictions and targets (which are empty for the test set).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\ntst_preds,_ = learn.get_preds(dl=test_dls.train)\n```\n\n----------------------------------------\n\nTITLE: Moving Batch to CUDA\nDESCRIPTION: This moves the input batch `xb` to the CUDA device for GPU acceleration.  This is necessary to utilize a GPU if available, to speed up training and inference.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nxb = xb.cuda()\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom U-Net Model with Flexible Architecture\nDESCRIPTION: This function constructs a U-Net architecture from a given backbone 'arch', with options for pretrained weights, custom cut, number of input channels, and output classes. It utilizes model metadata and supports dynamic weight loading depending on torchvision version. It then creates the encoder body and wraps it in a 'DynamicUnet'. Dependencies include fastai's models and vision modules. The function is key for flexible semantic segmentation models.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\n@delegates(models.unet.DynamicUnet.__init__)\ndef create_unet_model(arch, n_out, img_size, pretrained=True, weights=None, cut=None, n_in=3, **kwargs):\n    \"Create custom unet architecture\" \n    meta = model_meta.get(arch, _default_meta)\n    if parse(torchvision.__version__) >= parse('0.13') and 'weights' in meta:\n        if weights is not None and not pretrained:\n            warn(f'{pretrained=} but `weights` are set {weights=}. To randomly initialize set `pretrained=False` & `weights=None`')\n        model = arch(weights=meta['weights'] if (weights is None and pretrained) else weights)\n    else:\n        model = arch(pretrained=pretrained)\n    body = create_body(model, n_in, pretrained, ifnone(cut, meta['cut']))\n    model = models.unet.DynamicUnet(body, n_out, img_size, **kwargs)\n    return model\n```\n\n----------------------------------------\n\nTITLE: Map Models to Metadata for Splitting - Python\nDESCRIPTION: A dictionary mapping specific torchvision/fastai model architecture classes (e.g., models.resnet18, models.xresnet50) to their corresponding metadata dictionaries defined previously. This mapping allows fastai's model creation and training utilities to automatically apply the correct splitting function, cut point, and associated settings based on the chosen architecture.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#|export\nmodel_meta = {\n    models.xresnet.xresnet18 :{**_xresnet_meta}, models.xresnet.xresnet34: {**_xresnet_meta},\n    models.xresnet.xresnet50 :{**_xresnet_meta}, models.xresnet.xresnet101:{**_xresnet_meta},\n    models.xresnet.xresnet152:{**_xresnet_meta},\n\n    models.resnet18 :{**_resnet_meta}, models.resnet34: {**_resnet_meta},\n    models.resnet50 :{**_resnet_meta}, models.resnet101:{**_resnet_meta},\n    models.resnet152:{**_resnet_meta},\n\n    models.squeezenet1_0:{**_squeezenet_meta},\n    models.squeezenet1_1:{**_squeezenet_meta},\n\n    models.densenet121:{**_densenet_meta}, models.densenet169:{**_densenet_meta},\n    models.densenet201:{**_densenet_meta}, models.densenet161:{**_densenet_meta},\n    models.vgg11_bn:{**_vgg_meta}, models.vgg13_bn:{**_vgg_meta}, models.vgg16_bn:{**_vgg_meta}, models.vgg19_bn:{**_vgg_meta},\n    models.alexnet:{**_alexnet_meta}}\n```\n\n----------------------------------------\n\nTITLE: List Wheel File Contents (Python)\nDESCRIPTION: Uses the `namelist()` method of the `WheelFile` object (`fwhl`) to retrieve a list of all file and directory names contained within the Wheel archive.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfwhl.namelist()\n```\n\n----------------------------------------\n\nTITLE: Median Function Example\nDESCRIPTION: This code demonstrates the usage of the custom median function defined above. It creates a cuDF Series with missing values and calculates its median. It ensures missing values are handled correctly and returns the expected result.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ncol = cudf.Series([0,1,np.nan,1,2,3,4])\ntest_eq(col.median(), 1.5)\ncol = cudf.Series([np.nan,1,np.nan,1,2,3,4])\ntest_eq(col.median(), 2)\n```\n\n----------------------------------------\n\nTITLE: Importing Core Dependencies for fastai Text Models in Python\nDESCRIPTION: Imports future annotations and essential fastai text and data modules needed to define and implement text model architectures. These imports provide fundamental function, class, and constant definitions such as AWD_LSTM, data utilities, and core text processing components required throughout the file.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.data.all import *\nfrom fastai.text.core import *\nfrom fastai.text.models.awdlstm import *\n```\n\n----------------------------------------\n\nTITLE: Creating RNN Callback List with Optional Regularization in FastAI Python\nDESCRIPTION: Defines a helper function that returns a list of callback instances needed for RNN training, optionally including AR and TAR regularization callbacks depending on the specified alpha and beta parameters. It consolidates required callbacks (model resetting, output handling, and regularization) into a single list for easier integration into training loops. The function encapsulates callback creation logic for reuse and clarity.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/34_callback.rnn.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef rnn_cbs(alpha=0., beta=0.):\n    \"All callbacks needed for (optionally regularized) RNN training\"\n    reg = [RNNRegularizer(alpha=alpha, beta=beta)] if alpha or beta else []\n    return [ModelResetter(), RNNCallback()] + reg\n```\n\n----------------------------------------\n\nTITLE: Implementing Average Precision Score Wrapper for Multi-label Classification in Python\nDESCRIPTION: A wrapper function that adapts scikit-learn's average_precision_score for use in fastai's multi-label classification tasks, supporting customization of activation type and averaging method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef APScoreMulti(sigmoid=True, average='macro', pos_label=1, sample_weight=None):\n    \"Average Precision for multi-label classification problems\"\n    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n    return skm_to_fastai(skm.average_precision_score, activation=activation, flatten=False,\n                         average=average, pos_label=pos_label, sample_weight=sample_weight)\n```\n\n----------------------------------------\n\nTITLE: Training a Learner with Combined L1 and L2 Loss and Corresponding Metrics in fastai\nDESCRIPTION: Shows an example usage where a synthetic learner is created using the CombineL1L2 loss function, and metrics are set as LossMetrics for both 'l1' and 'l2'. The learner is then trained for 2 epochs demonstrating how combined loss and individual components can be tracked during training. This assumes the presence of a synth_learner factory function and fastai training loop infrastructure.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner(metrics=LossMetrics('l1,l2'))\nlearn.loss_func = CombineL1L2()\nlearn.fit(2)\n```\n\n----------------------------------------\n\nTITLE: Truncated Normal Initialization (Python)\nDESCRIPTION: Performs an in-place truncated normal initialization on the input tensor `x`. It approximates truncated normal distribution by drawing from a standard normal distribution, applying modulo 2, scaling by `std`, and adding the `mean`. This method is often used to initialize weights within a bounded range.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef trunc_normal_(x, mean=0., std=1.):\n    \"Truncated normal initialization (approximation)\"\n    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n    return x.normal_().fmod_(2).mul_(std).add_(mean)\n```\n\n----------------------------------------\n\nTITLE: Creating and visualizing MNIST datasets\nDESCRIPTION: Creates a Datasets object from the MNIST data and displays an example image, verifying the dataset structure and content.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndsets = mnist.datasets(untar_data(URLs.MNIST_TINY))\ntest_eq(dsets.vocab, ['3', '7'])\nx,y = dsets.train[0]\ntest_eq(x.size,(28,28))\nshow_at(dsets.train, 0, cmap='Greys', figsize=(2,2));\n```\n\n----------------------------------------\n\nTITLE: Implementing Dice Coefficient Metric for Binary Segmentation in PyTorch\nDESCRIPTION: Class that implements the Dice coefficient (F1 score) for binary segmentation tasks, measuring the overlap between prediction and target masks by tracking intersections and unions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nclass Dice(Metric):\n    \"Dice coefficient metric for binary target in segmentation\"\n    def __init__(self, axis=1): self.axis = axis\n    def reset(self): self.inter,self.union = 0,0\n    def accumulate(self, learn):\n        pred,targ = flatten_check(learn.pred.argmax(dim=self.axis), learn.y)\n        self.inter += (pred*targ).float().sum().item()\n        self.union += (pred+targ).float().sum().item()\n\n    @property\n    def value(self): return 2. * self.inter/self.union if self.union > 0 else None\n```\n\n----------------------------------------\n\nTITLE: Setup Colab Environment with fastai (Shell)\nDESCRIPTION: Installs or upgrades the fastai library specifically within a Google Colab environment by checking for the presence of the `/content` directory. This ensures the necessary library version is available for subsequent code execution.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Extending PyTorch Tensor Subclass with Metadata Support in Python\nDESCRIPTION: Defines TensorBase, a subclass of torch.Tensor that supports subclass pickling, metadata preservation during tensor casting and operations, and maintains metadata through overridden __torch_function__ and serialization methods. It includes methods for creating new tensors with the same subclass, cloning, and handling gradient requirements, with version-dependent adjustments for torch compatibility. Metadata such as image size is preserved and passed along operations, enhancing tensor functionality within fastai.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nclass TensorBase(Tensor):\n    \"A `Tensor` which support subclass pickling, and maintains metadata when casting or after methods\"\n    debug,_opt = False,defaultdict(list)\n    def __new__(cls, x, **kwargs):\n        res = cast(tensor(x), cls)\n        for k,v in kwargs.items(): setattr(res, k, v)\n        return res\n\n    @classmethod\n    def _before_cast(cls, x): return tensor(x)\n    def __repr__(self): return re.sub('tensor', self.__class__.__name__, super().__repr__())\n\n    def __reduce_ex__(self, proto):\n        if _torch_version >= _torch_20:\n            return super().__reduce_ex__(proto)\n        else:\n            torch.utils.hooks.warn_if_has_hooks(self)\n            args = (self.storage(), self.storage_offset(), tuple(self.size()), self.stride())\n            if self.is_quantized: args = args + (self.q_scale(), self.q_zero_point())\n            args = args + (self.requires_grad, OrderedDict())\n            f = torch._utils._rebuild_qtensor if self.is_quantized else  torch._utils._rebuild_tensor_v2\n            return (_rebuild_from_type, (f, type(self), args, self.__dict__))\n\n    @classmethod\n    def register_func(cls, func, *oks): cls._opt[func].append(oks)\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if cls.debug and func.__name__ not in ('__str__','__repr__'): print(func, types, args, kwargs)\n        if _torch_handled(args, cls._opt, func): types = (torch.Tensor,)\n        res = super().__torch_function__(func, types, args, ifnone(kwargs, {}))\n        dict_objs = _find_args(args) if args else _find_args(list(kwargs.values()))\n        if issubclass(type(res),TensorBase) and dict_objs: res.set_meta(dict_objs[0],as_copy=True)\n        elif dict_objs and is_listy(res): [r.set_meta(dict_objs[0],as_copy=True) for r in res if issubclass(type(r),TensorBase)]\n        return res\n\n    def new_tensor(self, size, dtype=None, device=None, requires_grad=False):\n        cls = type(self)\n        return self.as_subclass(Tensor).new_tensor(size, dtype=dtype, device=device, requires_grad=requires_grad).as_subclass(cls)\n\n    def new_ones(self, data, dtype=None, device=None, requires_grad=False):\n        cls = type(self)\n        return self.as_subclass(Tensor).new_ones(data, dtype=dtype, device=device, requires_grad=requires_grad).as_subclass(cls)\n\n    def new(self, x=None):\n        cls = type(self)\n        res = self.as_subclass(Tensor).new() if x is None else self.as_subclass(Tensor).new(x)\n        return res.as_subclass(cls)\n\n    def requires_grad_(self, requires_grad=True):\n        # Workaround https://github.com/pytorch/pytorch/issues/50219\n        self.requires_grad = requires_grad\n        return self\n\n    def clone(self, *, memory_format=None):\n        cls = type(self)\n        return self.as_subclass(Tensor).clone(memory_format=memory_format).as_subclass(cls)\n\n    def new_empty(self, size, *, dtype=None, layout=None, device=None, pin_memory=False, requires_grad=False):\n        cls = type(self)\n        if _torch_version < _torch_113 and layout is None:\n            layout = torch.strided\n        if _torch_version < _torch_112:\n            return super().new_empty(size, dtype=dtype, layout=layout, device=device, pin_memory=pin_memory, requires_grad=requires_grad)\n        return self.as_subclass(Tensor).new_empty(size, dtype=dtype, layout=layout, device=device, pin_memory=pin_memory, requires_grad=requires_grad).as_subclass(cls)\n\n    def new_empty(self, *size, dtype=None, layout=None, device=None, pin_memory=False, requires_grad=False):\n        cls = type(self)\n        if _torch_version < _torch_113 and layout is None:\n            layout = torch.strided\n        if _torch_version < _torch_112:\n            return super().new_empty(*size, dtype=dtype, layout=layout, device=device, pin_memory=pin_memory, requires_grad=requires_grad)\n        return self.as_subclass(Tensor).new_empty(*size, dtype=dtype, layout=layout, device=device, pin_memory=pin_memory, requires_grad=requires_grad).as_subclass(cls)\n```\n\n----------------------------------------\n\nTITLE: Testing Recorder Logging via Standard Output in fastai (Python)\nDESCRIPTION: This snippet constructs a synthetic fastai Learner with a simple metric (mean squared error), attaches the Recorder and training/validation callbacks, and tests that fitting prints logging output in the expected format. Parameters include 'n_train=5' and the custom metric function. The test checks log rows using a regular expression, confirming correct formatting and inclusion of time columns. Dependencies: fastai's synth_learner, TrainEvalCallback, Recorder, and custom test_stdout.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_69\n\nLANGUAGE: python\nCODE:\n```\n#Test printed output\ndef tst_metric(out, targ): return F.mse_loss(out, targ)\nlearn = synth_learner(n_train=5, metrics=tst_metric, default_cbs=False, cbs=[TrainEvalCallback, Recorder])\n# pat = r\"[tensor\\(\\d.\\d*\\), tensor\\(\\d.\\d*\\), tensor\\(\\d.\\d*\\), 'dd:dd']\"\npat = r\"\\[\\d, \\d+.\\d+, \\d+.\\d+, \\d+.\\d+, '\\d\\d:\\d\\d'\\]\"\ntest_stdout(lambda: learn.fit(1), pat, regex=True)\n```\n\n----------------------------------------\n\nTITLE: Creating pull request via GitHub CLI\nDESCRIPTION: Initiates a pull request from the current branch, optionally using the latest commit message as the PR title, streamlining contribution workflow.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/dev-setup.ipynb#_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\ngh pr create -f\n```\n\n----------------------------------------\n\nTITLE: Defining Data Augmentations Using fastai - Python\nDESCRIPTION: Specifies image augmentation transforms suitable for satellite imagery, including vertical flips, limited lighting and zoom, and no warp distortion, for a given image size. Used later in DataBlock configuration for training. Requires fastai installed. Inputs are hyperparameters for each augmentation type.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntfms = aug_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0., size=128)\n```\n\n----------------------------------------\n\nTITLE: Tabular Data Processing Configuration\nDESCRIPTION: This section defines the configuration for tabular data processing, including specifying categorical and continuous variable names, defining preprocessing steps (`Categorify`, `FillMissing`, `Normalize`), and setting up data splits using a `RandomSplitter`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ncat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\ncont_names = ['age', 'fnlwgt', 'education-num']\nprocs = [Categorify, FillMissing, Normalize]\n\nsplits = RandomSplitter()(range_of(df_trn))\n```\n\n----------------------------------------\n\nTITLE: Applying PCA to Movie Weights\nDESCRIPTION: This code applies Principal Component Analysis (PCA) to the movie weights to reduce their dimensionality.  It uses the `.pca(3)` method to reduce the number of dimensions to 3. This is an important step for visualization and simplification of the latent factors.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmovie_pca = movie_w.pca(3)\n```\n\n----------------------------------------\n\nTITLE: Load Classifier (First Layer Group)\nDESCRIPTION: Loads the saved model from the first training stage.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nlearn.load('first');\n```\n\n----------------------------------------\n\nTITLE: Tests: Instantiate Dynamic UNet and check output shape (Python)\nDESCRIPTION: Demonstrates instantiating the `DynamicUnet` with a ResNet34 encoder and a specified output channel count (5) for a 128x128 input image. It then passes a dummy tensor through the model and uses `test_eq` to assert that the output tensor has the expected shape, verifying the model's dimensions are correctly set up.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15a_vision.models.unet.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nm = resnet34()\nm = nn.Sequential(*list(m.children())[:-2])\ntst = DynamicUnet(m, 5, (128,128), norm_type=None)\nx = cast(torch.randn(2, 3, 128, 128), TensorImage)\ny = tst(x)\ntest_eq(y.shape, [2, 5, 128, 128])\n```\n\n----------------------------------------\n\nTITLE: Decoding Token IDs with GPT-2 Tokenizer (Python)\nDESCRIPTION: This code decodes previously encoded token IDs back into a human-readable string using the tokenizer's decode method. Expects a list of integer IDs as input and outputs a string equivalent. Useful for verifying tokenization and model outputs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntokenizer.decode(ids)\n```\n\n----------------------------------------\n\nTITLE: Executing Function in Rank-0 First with rank0_first - Python\nDESCRIPTION: Wraps a function call such that it is executed first on rank-0 (primary process) before being run in parallel on all ranks in distributed training. Facilitates safe shared operations (such as downloads or model construction) that must not be duplicated across processes. Dependencies: fastai Learner, DataLoaders, torch.nn, acceleration/distributed helpers. Returns the result of the called function.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\ndef rank0_first(func, *args, **kwargs):\n    \"Execute `func` in the Rank-0 process first, then in other ranks in parallel.\"\n    if args or kwargs: func = partial(func, *args, **kwargs)\n    dummy_l = Learner(DataLoaders(device='cpu'), nn.Linear(1,1), loss_func=lambda: 0)\n    with dummy_l.distrib_ctx():\n        if not rank_distrib(): res = func()\n        distrib_barrier()\n        if rank_distrib(): res = func()\n    return res\n```\n\n----------------------------------------\n\nTITLE: Hooks Class Definition\nDESCRIPTION: This code defines the `Hooks` class, which provides a way to create and manage multiple hooks on a list of modules. It takes a list of modules and a hook function as input, and creates a `Hook` object for each module. It provides methods for accessing the stored activations of each hook, removing the hooks, and using the `Hooks` object as a context manager.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@docs\nclass Hooks():\n    \"Create several hooks on the modules in `ms` with `hook_func`.\"\n    def __init__(self, ms, hook_func, is_forward=True, detach=True, cpu=False):\n        self.hooks = [Hook(m, hook_func, is_forward, detach, cpu) for m in ms]\n\n    def __getitem__(self,i): return self.hooks[i]\n    def __len__(self):       return len(self.hooks)\n    def __iter__(self):      return iter(self.hooks)\n    @property\n    def stored(self):        return L(o.stored for o in self)\n\n    def remove(self):\n        \"Remove the hooks from the model.\"\n        for h in self.hooks: h.remove()\n\n    def __enter__(self, *args): return self\n    def __exit__ (self, *args): self.remove()\n\n    _docs = dict(stored = \"The states saved in each hook.\",\n                 __enter__=\"Register the hooks\",\n                 __exit__=\"Remove the hooks\")\n```\n\n----------------------------------------\n\nTITLE: Pushing branch to remote origin\nDESCRIPTION: Pushes the current local branch to remote, setting upstream tracking, to prepare for creating a pull request.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/dev-setup.ipynb#_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\ngit push -u origin HEAD\n```\n\n----------------------------------------\n\nTITLE: Save Classifier (Second Layer Group)\nDESCRIPTION: Saves the trained model after the second training stage.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nlearn.save('second')\n```\n\n----------------------------------------\n\nTITLE: Implementing ROC AUC Score Wrapper for Multi-label Classification in Python\nDESCRIPTION: A wrapper function that adapts scikit-learn's roc_auc_score for use in fastai's multi-label binary classification tasks, with options for sigmoid activation and averaging method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndef RocAucMulti(sigmoid=True, average='macro', sample_weight=None, max_fpr=None):\n    \"Area Under the Receiver Operating Characteristic Curve for multi-label binary classification problems\"\n    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n    return skm_to_fastai(skm.roc_auc_score, activation=activation, flatten=False,\n                         average=average, sample_weight=sample_weight, max_fpr=max_fpr)\n```\n\n----------------------------------------\n\nTITLE: Applying AR and TAR Regularization with RNNRegularizer Callback in FastAI Python\nDESCRIPTION: This callback adds autoregressive (AR) and temporal autoregressive (TAR) regularization terms to the loss gradient during training of RNNs. It applies these penalties after loss computation to encourage smoother outputs and mitigate overfitting. Parameters alpha and beta control the magnitude of AR and TAR regularization respectively. The callback must be ordered to run after RNNCallback and disables running during validation. It assumes access to the model's output layers and uses PyTorch tensor operations for regularization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/34_callback.rnn.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass RNNRegularizer(Callback):\n    \"Add AR and TAR regularization\"\n    order,run_valid = RNNCallback.order+1,False\n    def __init__(self, alpha=0., beta=0.): store_attr()\n    def after_loss(self):\n        if not self.training: return\n        if self.alpha: self.learn.loss_grad += self.alpha * self.rnn.out.float().pow(2).mean()\n        if self.beta:\n            h = self.rnn.raw_out\n            if len(h)>1: self.learn.loss_grad += self.beta * (h[:,1:] - h[:,:-1]).float().pow(2).mean()\n```\n\n----------------------------------------\n\nTITLE: Set Batch Size\nDESCRIPTION: Sets the batch size for training. This variable can be adjusted based on available GPU memory.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nbs=128\n```\n\n----------------------------------------\n\nTITLE: Installing huggingface_hub with fastai Extras (Shell)\nDESCRIPTION: Shell command to install the `huggingface_hub` library along with specific extras required for fastai integration (`toml`, `fastai>=2.4`, `fastcore>=1.3.27`). This command sets up the necessary environment for using the integration functions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/74_huggingface.ipynb#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install huggingface_hub[\"fastai\"]\n```\n\n----------------------------------------\n\nTITLE: Splitting AWD-LSTM Classifier Model for Differential Learning Rates in Python\nDESCRIPTION: Implements a function to split an AWD-LSTM classifier model into parameter groups corresponding to the encoder layers (with dropout) and the RNN hidden layers. The function uses PyTorch's nn.Sequential and fastai's L container to organize the model parts, allowing for the assignment of different learning rates during training. It expects the model to have a nested module attribute structure typical in distributed or parallelized environments.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef awd_lstm_clas_split(model):\n    \"Split a RNN `model` in groups for differential learning rates.\"\n    groups = [nn.Sequential(model[0].module.encoder, model[0].module.encoder_dp)]\n    groups += [nn.Sequential(rnn, dp) for rnn, dp in zip(model[0].module.rnns, model[0].module.hidden_dps)]\n    groups = L(groups + [model[1]])\n    return groups.map(params)\n```\n\n----------------------------------------\n\nTITLE: Importing mixed precision dependencies and fastai modules (Python)\nDESCRIPTION: This snippet prepares the environment for using mixed precision training in fastai by importing future annotations, basic fastai utilities, progress callbacks, and PyTorch's autocast and gradient scaling modules. Inputs include no parameters, and the dependencies are fastai (for basics and callback.progress) and torch (particularly torch.cuda.amp). The output is simply loaded modules in the Python namespace. This is required before any mixed precision features are used.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.basics import *\nfrom fastai.callback.progress import *\n\nfrom torch.cuda.amp import GradScaler,autocast\nfrom torch.cuda.amp.grad_scaler import OptState\n\n```\n\n----------------------------------------\n\nTITLE: Monkey-Patching `Tensor.as_subclass` for Metadata Preservation in fastai\nDESCRIPTION: Adds or modifies the `as_subclass` method on the PyTorch `Tensor` class via monkey-patching (`@patch`). This patched version calls the standard `torch.as_subclass(self, typ)` function to perform the type cast but wraps the result with `retain_meta`. This ensures that any custom metadata stored in the original tensor's `__dict__` is transferred to the newly created subclass instance.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\ndef as_subclass(self:Tensor, typ):\n    \"Cast to `typ` and include `__dict__` and meta\"\n    return retain_meta(self, torch.as_subclass(self, typ))\n```\n\n----------------------------------------\n\nTITLE: Showing a Batch of Processed Data\nDESCRIPTION: Displays a sample batch of data from the created DataLoaders (`dls`). This allows visualization of the data format that will be fed into the model, including the dependent variable.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndls.show_batch()\n```\n\n----------------------------------------\n\nTITLE: Creating Random Tensor Fixtures for Testing - Python\nDESCRIPTION: Instantiates sample random tensors x1 and x2 using torch.randn for use as model predictions and targets respectively during metric unit tests. Dependencies: torch. No parameters. Used as dummy input data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nx1,x2 = torch.randn(20,5),torch.randn(20,5)\n```\n\n----------------------------------------\n\nTITLE: Testing all_batches Method with VerboseCallback in fastai Learner (Python)\nDESCRIPTION: Attaches VerboseCallback to a Learner and checks that all expected batch events are printed sequentially by the all_batches method, both for training and validation splits. This validates the Learner's iteration over batches and sequential event triggering. Requires test_stdout and fastai test functions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nlearn = synth_learner(n_train=5, cbs=VerboseCallback())\nlearn.opt = SGD(learn.parameters(), lr=learn.lr)\nwith redirect_stdout(io.StringIO()): \n    learn(_before_epoch)\n    learn.epoch,learn.dl = 0,learn.dls.train\n    learn('before_train')\ntest_stdout(learn.all_batches, '\\n'.join(batch_events * 5))\ntest_eq(learn.train_iter, 5)\n\nvalid_events = ['before_batch', 'after_pred', 'after_loss', 'after_batch']\nwith redirect_stdout(io.StringIO()): \n    learn.dl = learn.dls.valid\n    learn('before_validate')\ntest_stdout(learn.all_batches, '\\n'.join(valid_events * 2))\ntest_eq(learn.train_iter, 5)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Data Path and Listing Dataset Files with fastai Config\nDESCRIPTION: This snippet sets the data directory for the Rossmann dataset using the fastai Config class, then lists the files in that directory. Dependencies include fastai and a properly organized data folder. The function path.ls() outputs a list of available data files, which ensures all data components are accessible for loading.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npath = Config().data/'rossmann'\npath.ls()\n```\n\n----------------------------------------\n\nTITLE: Defining Array Split Utility - Python\nDESCRIPTION: This snippet defines a function `array_split` that takes an array-like object `arr` and a number of splits `n`. It calculates the size of each chunk by dividing the length of the array by `n` and then uses the `chunked` function to divide the array into chunks of that size. This provides an alternative way to split data, potentially used internally or for further experiments.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef array_split(arr, n): return chunked(arr, math.floor(len(arr)/n))\n```\n\n----------------------------------------\n\nTITLE: Testing Collate Error Handling for Inconsistent Batch Shapes - Python\nDESCRIPTION: Validates error messaging by creating a deliberately inconsistent batch of tensors and invoking fa_collate with collate_error handling. Uses torch, fastcore's ExceptionExpected utility, and expects a RuntimeError with a detailed mismatch description. This cell acts as a sanity check and developer smoke test.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nbatch = [torch.rand(3, 375, 500), torch.rand(3, 375, 500), torch.rand(3, 500, 333)]\nwith ExceptionExpected(RuntimeError, \"Mismatch found on axis 0 of the batch and is of type `Tensor`\"):\n    try:\n        fa_collate(batch)\n    except Exception as e:\n        collate_error(e, batch)\n```\n\n----------------------------------------\n\nTITLE: Defining OpenMask Transform (Python)\nDESCRIPTION: Creates `OpenMask`, a fastai `Transform` instance that wraps the `PILMask.create` classmethod. This allows `PILMask.create` to be easily used within fastai's data processing pipelines (e.g., `DataBlock`). It also assigns a default loss function (`CrossEntropyLossFlat`) suitable for segmentation tasks and replaces the original `PILMask.create` with this transform instance.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#|export\nOpenMask = Transform(PILMask.create)\nOpenMask.loss_func = CrossEntropyLossFlat(axis=1)\nPILMask.create = OpenMask\n```\n\n----------------------------------------\n\nTITLE: Saving Model After Full Fine-tuning - Python\nDESCRIPTION: Persists the model's updated parameters after fine-tuning to the label 'stage-2-rn50'. Allows checkpointing and later restoration. No inputs/outputs beyond saved file.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nlearn.save('stage-2-rn50')\n```\n\n----------------------------------------\n\nTITLE: Defining VerboseCallback for Event-tracing in fastai Learner (Python)\nDESCRIPTION: Defines VerboseCallback, a callback that prints out each event name when triggered, aiding in debugging and observing the callback sequence in training or validation. Requires the Callback base class and appropriate callback infrastructure within the Learner.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nclass VerboseCallback(Callback):\n    \"Callback that prints the name of each event called\"\n    def __call__(self, event_name):\n        print(event_name)\n        super().__call__(event_name)\n```\n\n----------------------------------------\n\nTITLE: Create Vision Learner (fastai)\nDESCRIPTION: This function creates a vision learner using a `DataLoader` and a specified architecture. It handles normalization, model creation (including TIMM models), splitting, and freezing pretrained layers. It accepts various arguments to customize the model and learner.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n@delegates(create_vision_model)\ndef vision_learner(dls, arch, normalize=True, n_out=None, pretrained=True, weights=None,\n        # learner args\n        loss_func=None, opt_func=Adam, lr=defaults.lr, splitter=None, cbs=None, metrics=None, path=None,\n        model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95,0.85,0.95),\n        # model & head args\n        cut=None, init=nn.init.kaiming_normal_, custom_head=None, concat_pool=True, pool=True,\n        lin_ftrs=None, ps=0.5, first_bn=True, bn_final=False, lin_first=False, y_range=None, **kwargs):\n    \"Build a vision learner from `dls` and `arch`\"\n    if n_out is None: n_out = get_c(dls)\n    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n    meta = model_meta.get(arch, _default_meta)\n    model_args = dict(init=init, custom_head=custom_head, concat_pool=concat_pool, pool=pool, lin_ftrs=lin_ftrs, ps=ps,\n                      first_bn=first_bn, bn_final=bn_final, lin_first=lin_first, y_range=y_range, **kwargs)\n    n_in = kwargs['n_in'] if 'n_in' in kwargs else 3\n    if isinstance(arch, str):\n        model,cfg = create_timm_model(arch, n_out, default_split, pretrained, **model_args)\n        if normalize: _timm_norm(dls, cfg, pretrained, n_in)\n    else:\n        if normalize: _add_norm(dls, meta, pretrained, n_in)\n        model = create_vision_model(arch, n_out, pretrained=pretrained, weights=weights, **model_args)\n\n    splitter = ifnone(splitter, meta['split'])\n    learn = Learner(dls=dls, model=model, loss_func=loss_func, opt_func=opt_func, lr=lr, splitter=splitter, cbs=cbs,\n                   metrics=metrics, path=path, model_dir=model_dir, wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn, moms=moms)\n    if pretrained: learn.freeze()\n    # keep track of args for loggers\n    store_attr('arch,normalize,n_out,pretrained', self=learn, **kwargs)\n    return learn\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Sequential Simple Tokenization - Jupyter\nDESCRIPTION: This snippet uses the `%%timeit` magic command in a Jupyter environment to measure the execution time of applying the `delim_tok` function sequentially to all strings in the `ss` list using the custom `apply` function. The results are stored globally in variable `t`. It runs the operation 2 times per loop (`-n 2`) for 3 loops (`-r 3`) and reports the average time.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%timeit -n 2 -r 3\nglobal t\nt = apply(delim_tok, ss)\n```\n\n----------------------------------------\n\nTITLE: Importing Core fastai Modules in Python\nDESCRIPTION: Imports necessary components for the AWD-LSTM implementation. It includes all modules from 'fastai.data.all' for data handling, core text processing utilities from 'fastai.text.core', and enables postponed evaluation of type annotations ('from __future__ import annotations') for modern Python compatibility.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.data.all import *\nfrom fastai.text.core import *\n```\n\n----------------------------------------\n\nTITLE: Testing DataLoader with different dataset types\nDESCRIPTION: This code tests `DataLoader` with different data types to ensure data loading behaves as expected with various inputs. Dependencies are `fastcore` and `torch` for the `L` and `tensor` functions, respectively.  The tests check `ds1` with `letters`, `ds2` with a `tensor` object and `ds3` and `ds4` with `t3` object, also including an `after_iter` lambda function, to ensure expected output type and results after iterations.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nds1 = DataLoader(letters)\ntest_eq(L(ds1), letters)\ntest_eq(len(ds1), 26)\n\ntest_shuffled(L(DataLoader(letters, shuffle=True)), letters)\n\nds1 = DataLoader(letters, indexed=False)\ntest_eq(L(ds1), letters)\ntest_eq(len(ds1), 26)\n\nt2 = L(tensor([0,1,2]),tensor([3,4,5]))\nds2 = DataLoader(t2)\ntest_eq_type(L(ds2), t2)\n\nt3 = L(array([0,1,2], dtype=np.int64),array([3,4,5], dtype=np.int64))\nds3 = DataLoader(t3)\ntest_eq_type(L(ds3), t3.map(tensor))\n\nds4 = DataLoader(t3, create_batch=noop, after_iter=lambda: setattr(t3, 'f', 1))\ntest_eq_type(L(ds4), t3)\ntest_eq(t3.f, 1)\n```\n\n----------------------------------------\n\nTITLE: Converting NumPy Array Functions to Accept and Return PyTorch Tensors in Python\nDESCRIPTION: Defines a decorator that wraps a numpy-based function to automatically convert input tensors to numpy arrays before function execution and then converts the numpy output back to a PyTorch tensor. This facilitates use of numpy functions as PyTorch tensor operations, compatible in deep learning workflows.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_70\n\nLANGUAGE: python\nCODE:\n```\ndef np_func(f):\n    \"Convert a function taking and returning numpy arrays to one taking and returning tensors\"\n    def _inner(*args, **kwargs):\n        nargs = [to_np(arg) if isinstance(arg,Tensor) else arg for arg in args]\n        return tensor(f(*nargs, **kwargs))\n    functools.update_wrapper(_inner, f)\n    return _inner\n```\n\n----------------------------------------\n\nTITLE: Testing Multi-label Accuracy Function with Various Parameters in Python\nDESCRIPTION: Comprehensive test cases for the accuracy_multi function, covering perfect match, complete mismatch, partial match scenarios, different threshold values, and operations with and without sigmoid activation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nx = torch.randn(4,5)\ny = (torch.sigmoid(x) >= 0.5).byte()\ntest_eq(accuracy_multi(x,y), 1)\ntest_eq(accuracy_multi(x,1-y), 0)\ny1 = change_1h_targ(y, 5)\ntest_eq(accuracy_multi(x,y1), 0.75)\n\n#Different thresh\ny = (torch.sigmoid(x) >= 0.2).byte()\ntest_eq(accuracy_multi(x,y, thresh=0.2), 1)\ntest_eq(accuracy_multi(x,1-y, thresh=0.2), 0)\ny1 = change_1h_targ(y, 5)\ntest_eq(accuracy_multi(x,y1, thresh=0.2), 0.75)\n\n#No sigmoid\ny = (x >= 0.5).byte()\ntest_eq(accuracy_multi(x,y, sigmoid=False), 1)\ntest_eq(accuracy_multi(x,1-y, sigmoid=False), 0)\ny1 = change_1h_targ(y, 5)\ntest_eq(accuracy_multi(x,y1, sigmoid=False), 0.75)\n```\n\n----------------------------------------\n\nTITLE: Adding and Testing Custom Transforms on DataLoaders (fastai, Python)\nDESCRIPTION: Creates a custom transform (_TestTfm) for batch normalization of all elements to ones. Demonstrates how to add transforms to specific DataLoaders using DataLoaders.add_tfms with both string-based and index-based selection of loaders. Finally, tests confirm the transforms run only on the designated DataLoader and yield correct results. Relies on fastai's DataLoaders, TfmdDL, and test utilities.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nclass _TestTfm(Transform):\n    def encodes(self, o):  return torch.ones_like(o)\n    def decodes(self, o):  return o\ntdl1,tdl2 = TfmdDL(start, bs=4),TfmdDL(start, bs=4)\ndls2 = DataLoaders(tdl1,tdl2)\ndls2.add_tfms([_TestTfm()],'after_batch',['valid'])\ndls2.add_tfms([_TestTfm()],'after_batch',[1])\ndls2.train.after_batch,dls2.valid.after_batch,\n```\n\nLANGUAGE: python\nCODE:\n```\n#|hide\ntest_eq(len(dls2.train.after_batch.fs),0)\ntest_eq(len(dls2.valid.after_batch.fs),2)\ntest_eq(next(iter(dls2.valid)),tensor([1,1,1,1]))\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Cat Layer in Python\nDESCRIPTION: This code snippet demonstrates how to use the `Cat` layer. It initializes a `Cat` layer with a list of `ConvLayer` instances. It then passes a random tensor through the `Cat` layer and tests that the output shape matches the expected shape (concatenation of channels) and that the output is equal to the manual concatenation of the outputs from applying each layer independently to the input.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_62\n\nLANGUAGE: python\nCODE:\n```\nlayers = [ConvLayer(2,4), ConvLayer(2,4), ConvLayer(2,4)] \nx = torch.rand(1,2,8,8) \ncat = Cat(layers) \ntest_eq(cat(x).shape, [1,12,8,8]) \ntest_eq(cat(x), torch.cat([l(x) for l in layers], dim=1))\n```\n\n----------------------------------------\n\nTITLE: Imports: nbdev showdoc (Python)\nDESCRIPTION: Imports the `showdoc` function from the `nbdev.showdoc` module. This is used for generating documentation within the notebook environment, typically rendering function or class signatures and docstrings.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15a_vision.models.unet.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Implementing Mean Absolute Error (MAE) for Regression in PyTorch\nDESCRIPTION: Function that calculates the mean absolute error between input and target tensors, using the absolute difference of values after flattening and checking the inputs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef mae(inp,targ):\n    \"Mean absolute error between `inp` and `targ`.\"\n    inp,targ = flatten_check(inp,targ)\n    return torch.abs(inp - targ).mean()\n```\n\n----------------------------------------\n\nTITLE: Creating an Annealing Decorator Function in Python\nDESCRIPTION: Defines a decorator named annealer that transforms a function f(start, end, pos) into a function that returns a partially applied _Annealer instance. This allows defining scheduling functions more abstractly, returning callable schedulers dependent only on position pos after fixing start and end values.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef annealer(f):\n    \"Decorator to make `f` return itself partially applied.\"\n    @functools.wraps(f)\n    def _inner(start, end): return _Annealer(f, start, end)\n    return _inner\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Distributed Training (Python)\nDESCRIPTION: Imports necessary modules from `fastai`, `fastai.distributed`, and `accelerate` to enable distributed training within a notebook. Includes standard fastai vision imports along with distributed utilities and the `notebook_launcher`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20b_tutorial.distributed.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\nfrom fastai.vision.all import *\nfrom fastai.distributed import *\nfrom fastai.vision.models.xresnet import *\n\nfrom accelerate import notebook_launcher\nfrom accelerate.utils import write_basic_config\n```\n\n----------------------------------------\n\nTITLE: Normalize TensorImage for Projector Visualization\nDESCRIPTION: Normalizes a batch of image tensors to be within the 0-1 range, facilitating visualization in TensorBoard.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n @dispatch\ndef _normalize_for_projector(x:TensorImage):\n    # normalize tensor to be between 0-1\n    img = x.clone()\n    sz = img.shape\n    img = img.view(x.size(0), -1)\n    img -= img.min(1, keepdim=True)[0]\n    img /= img.max(1, keepdim=True)[0]\n    img = img.view(*sz)\n    return img\n```\n\n----------------------------------------\n\nTITLE: Defining an Average Loss Metric Class (Python)\nDESCRIPTION: This snippet defines `AvgLoss`, a class that averages the losses taking into account potentially different batch sizes. It inherits from the base `Metric` class and implements `reset` and `accumulate` to keep track of the total loss and count, as well as value to return the average loss.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_62\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass AvgLoss(Metric):\n    \"Average the losses taking into account potential different batch sizes\"\n    def reset(self):           self.total,self.count = 0.,0\n    def accumulate(self, learn):\n        bs = find_bs(learn.yb)\n        self.total += learn.to_detach(learn.loss.mean())*bs\n        self.count += bs\n    @property\n    def value(self): return self.total/self.count if self.count != 0 else None\n    @property\n    def name(self):  return \"loss\"\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(AvgLoss, title_level=3)\n```\n\nLANGUAGE: python\nCODE:\n```\ntst = AvgLoss()\nt = torch.randn(100)\ntst.reset()\nfor i in range(0,100,25): \n    learn.yb,learn.loss = t[i:i+25],t[i:i+25].mean()\n    tst.accumulate(learn)\ntest_close(tst.value, t.mean())\n```\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#With varying batch size\ntst.reset()\nsplits = [0, 30, 50, 60, 100]\nfor i in range(len(splits )-1): \n    learn.yb,learn.loss = t[splits[i]:splits[i+1]],t[splits[i]:splits[i+1]].mean()\n    tst.accumulate(learn)\ntest_close(tst.value, t.mean())\n```\n\n----------------------------------------\n\nTITLE: Sigmoid Function with Clamping\nDESCRIPTION: This function, `sigmoid`, applies the sigmoid function to the input and clamps the output to the range `(eps, 1-eps)` to avoid numerical instability. `eps` defaults to 1e-7.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef sigmoid(input, eps=1e-7):\n    \"Same as `torch.sigmoid`, plus clamping to `(eps,1-eps)\"\n    return input.sigmoid().clamp(eps,1-eps)\n```\n\n----------------------------------------\n\nTITLE: Exporting Submission CSV for Kaggle Competition - Python\nDESCRIPTION: Saves the prediction results in CSV format at the specified path for upload to Kaggle. Output is a CSV file, columns: image_name, tags. Requires pandas. Output file is referenced in the Kaggle submission command.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\ndf.to_csv(path/'submission.csv', index=False)\n```\n\n----------------------------------------\n\nTITLE: Define and Check Package File Paths (Python)\nDESCRIPTION: Sets up `Path` objects pointing to a base directory (`~/git/spacy_conda/cymem`), an example Conda package (`cymem-2.0.2-py37_0.tar.bz2`), and an example Wheel package (`cymem-2.0.2-cp37-cp37m-manylinux1_x86_64.whl`). It then checks for the existence of the Conda and Wheel files.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npath = Path('~/git/spacy_conda/cymem').expanduser()\npkg = path/'pkg/cymem-2.0.2-py37_0.tar.bz2'\nwhl = path/'whl/cymem-2.0.2-cp37-cp37m-manylinux1_x86_64.whl'\npkg.exists(),whl.exists()\n```\n\n----------------------------------------\n\nTITLE: Defining BBoxLabeler Transform for Multi-Class Object Detection Labels - fastai (Python)\nDESCRIPTION: Implements a Transform for decoding bounding box and multi-category label pairs into LabeledBBox with human-readable labels from the DataLoader's vocabulary. Requires fastai's Transform, DataLoader, TensorBBox, TensorMultiCategory, and LabeledBBox types. Input is either a tensor or tuple, output is human-interpretable label structure.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nclass BBoxLabeler(Transform):\n    def setups(self, dl): self.vocab = dl.vocab\n\n    def decode (self, x, **kwargs):\n        self.bbox,self.lbls = None,None\n        return self._call('decodes', x, **kwargs)\n\n    def decodes(self, x:TensorMultiCategory):\n        self.lbls = [self.vocab[a] for a in x]\n        return x if self.bbox is None else LabeledBBox(self.bbox, self.lbls)\n\n    def decodes(self, x:TensorBBox):\n        self.bbox = x\n        return self.bbox if self.lbls is None else LabeledBBox(self.bbox, self.lbls)\n```\n\n----------------------------------------\n\nTITLE: Clipping and Removing Empty Bounding Boxes in fastai Vision Using Python\nDESCRIPTION: Defines `clip_remove_empty` which clips bounding box coordinates to the image border limits and removes any bounding boxes that become empty (zero or negative area) after clipping. The function takes tensors representing bounding boxes and their corresponding multi-category labels, clamps box coordinates within [-1, 1], identifies and removes empty boxes, and filters labels accordingly. This function is instrumental for robust bounding box processing pre-padding and training in object detection pipelines.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/08_vision.data.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef clip_remove_empty(\n    bbox:TensorBBox, # Coordinates of bounding boxes \n    label:TensorMultiCategory # Labels of the bounding boxes\n):\n    \"Clip bounding boxes with image border and remove empty boxes along with corresponding labels\"\n    bbox = torch.clamp(bbox, -1, 1)\n    empty = ((bbox[...,2] - bbox[...,0])*(bbox[...,3] - bbox[...,1]) <= 0.)\n    return (bbox[~empty], label[TensorBase(~empty)])\n```\n\n----------------------------------------\n\nTITLE: Creating Master FP32 Parameters from Model Parameters in fastai Python\nDESCRIPTION: The `get_master` function creates FP32 master copies of the model's parameters, organized by parameter groups. It handles both the standard list of parameter groups and an optional flattened version, which concatenates parameters within each group into a single tensor for efficiency.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef get_master(\n    opt:Optimizer, # Optimizer from which to retrieve model params\n    flat_master:bool=False, # Flatten fp32 params into a vector for better performance\n) -> list: # List of fp16 params, and list of fp32 params\n    \"Creates fp16 model params given an initialized `Optimizer`, also returning fp32 model params. \"\n    model_params = [[param for param in pg if getattr(param, 'requires_grad', False) and hasattr(param, 'data')] for pg in opt.param_lists]\n    if flat_master:\n        master_params = []\n        for pg in model_params:\n            mp = parameters_to_vector([param.data.float() for param in pg])\n            mp = nn.Parameter(mp, requires_grad=True)\n            if mp.grad is None: mp.grad = mp.new(*mp.size())\n            master_params.append([mp])\n    else:\n        master_params = [[nn.Parameter(param.data.clone().float().detach(), requires_grad=True) for param in pg] for pg in model_params]\n    return model_params, master_params\n```\n\n----------------------------------------\n\nTITLE: Instantiation of a U-Net Model Example\nDESCRIPTION: This code creates a U-Net model with a ResNet18 backbone, 10 output classes, input size of 24x24, and one input channel. It demonstrates flexible model instantiation for experimentation or testing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\ntst = create_unet_model(models.resnet18, 10, (24,24), True, n_in=1)\n```\n\n----------------------------------------\n\nTITLE: Documentation for create_unet_model Function\nDESCRIPTION: This line documents the 'create_unet_model' function, indicating it as a utility to create U-Net architectures with flexible options for architecture, input channels, output classes, and pretrained weights, leveraging model metadata for proper configuration.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\nshow_doc(create_unet_model)\n```\n\n----------------------------------------\n\nTITLE: Detecting Pooling Layers in PyTorch Models Using FastAI in Python\nDESCRIPTION: Defines a function '_is_pool_type' that returns True if a given layer's class name matches pooling pattern ends: Pool1d, Pool2d, or Pool3d. It uses regex matching to identify pooling layers prevalent in convolutional models. This utility facilitates dynamic model structuring by detecting where to cut pretrained models at pooling output points.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef _is_pool_type(l): return re.search(r'Pool[123]d$', l.__class__.__name__)\n```\n\n----------------------------------------\n\nTITLE: Defining BCELossFlat - Fastai/PyTorch\nDESCRIPTION: This function defines `BCELossFlat` within the fastai library. It acts as a wrapper around PyTorch's `nn.BCELoss`, designed to handle flattened input and target tensors automatically via the `BaseLoss` class. It supports optional arguments like `weight`, `reduction`, `axis`, and `floatify`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@use_kwargs_dict(weight=None, reduction='mean')\ndef BCELossFlat(\n    *args, \n    axis:int=-1, # Class axis\n    floatify:bool=True, # Convert `targ` to `float`\n    **kwargs\n):\n    \"Same as `nn.BCELoss`, but flattens input and target.\"\n    return BaseLoss(nn.BCELoss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Defining Filling Strategies - Python\nDESCRIPTION: This code defines a `FillStrategy` namespace, containing methods for filling missing values in a pandas Series.  It includes strategies for filling with the median, a constant value, and the mode.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nclass FillStrategy:\n    \"Namespace containing the various filling strategies.\"\n    def median  (c,fill): return c.median()\n    def constant(c,fill): return fill\n    def mode    (c,fill): return c.dropna().value_counts().idxmax()\n```\n\n----------------------------------------\n\nTITLE: Validating DataLoader Batches and Decode Logic for Object Detection - fastai (Python)\nDESCRIPTION: Obtains one batch from a DataLoader, checks proper normalization of bboxes and multi-category labels, and type correctness after decoding. Uses test_close, test_eq on batches containing tensors and label objects. Requires coco_tdl, bbox, tensor utils, and related test functions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\nx,y,z = coco_tdl.one_batch()\ntest_close(y[0], -1+tensor(bbox[0])/64)\ntest_eq(z[0], tensor([1,1,1]))\na,b,c = coco_tdl.decode_batch((x,y,z))[0]\ntest_close(b, tensor(bbox[0]).float())\ntest_eq(c.bbox, b)\ntest_eq(c.lbl, bbox[1])\n\n#Check types\ntest_eq(type(x), TensorImage)\ntest_eq(type(y), TensorBBox)\ntest_eq(type(z), TensorMultiCategory)\ntest_eq(type(a), TensorImage)\ntest_eq(type(b), TensorBBox)\ntest_eq(type(c), LabeledBBox)\ntest_eq(y.img_size, (128,128))\n```\n\n----------------------------------------\n\nTITLE: Testing DiceLoss Forward Pass and Decodes Method - Fastai/PyTorch\nDESCRIPTION: This snippet tests the `DiceLoss` class with sample model output and target tensors. It performs a forward pass to calculate the loss and verifies that the `decodes` method correctly converts the model output (logits) into the predicted class indices by finding the argmax along the class axis.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndl = DiceLoss()\nmodel_output = tensor([[[[2., 1.],\n                         [1., 5.]],\n                        [[1,  2.],\n                         [3., 1.]],\n                        [[3., 0],\n                         [4., 3.]]]])\ntarget       =  tensor([[[2, 1],\n                         [2, 0]]])\ndl_out = dl(model_output, target)\ntest_eq(dl.decodes(model_output), target)\n```\n\n----------------------------------------\n\nTITLE: Define CSVLogger Class in Python\nDESCRIPTION: Defines the `CSVLogger` class, a fastai `Callback` that logs training metrics (epoch, losses, metric values) to a CSV file. It takes an optional filename and an append flag. It hooks into `before_fit` to open the file and write headers, logs metrics after each epoch by intercepting the learner's logger, and closes the file in `after_fit`. Includes a `read_log` helper method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/16_callback.progress.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass CSVLogger(Callback):\n    \"Log the results displayed in `learn.path/fname`\"\n    order=60\n    def __init__(self, fname='history.csv', append=False):\n        self.fname,self.append = Path(fname),append\n\n    def read_log(self):\n        \"Convenience method to quickly access the log.\"\n        return pd.read_csv(self.path/self.fname)\n\n    def before_fit(self):\n        \"Prepare file with metric names.\"\n        if hasattr(self, \"gather_preds\"): return\n        self.path.parent.mkdir(parents=True, exist_ok=True)\n        self.file = (self.path/self.fname).open('a' if self.append else 'w')\n        self.file.write(','.join(self.recorder.metric_names) + '\\n')\n        self.old_logger,self.learn.logger = self.logger,self._write_line\n\n    def _write_line(self, log):\n        \"Write a line with `log` and call the old logger.\"\n        self.file.write(','.join([str(t) for t in log]) + '\\n')\n        self.file.flush()\n        os.fsync(self.file.fileno())\n        self.old_logger(log)\n\n    def after_fit(self):\n        \"Close the file and clean up.\"\n        if hasattr(self, \"gather_preds\"): return\n        self.file.close()\n        self.learn.logger = self.old_logger\n```\n\n----------------------------------------\n\nTITLE: Testing DistributedDL with TfmdDL - Python\nDESCRIPTION: Demonstrates how to instantiate and iterate over a DistributedDL data loader, wrapping a TfmdDL, and compares output batches to expected results using test_eq. Useful for verifying correctness of partitioning/gathering logic. Dependencies: torch, fastai's TfmdDL, DistributedDL, test_eq; requires multiple worker/distributed setup for proper operation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\ndl = TfmdDL(list(range(50)), bs=12, num_workers=2)\nfor i in range(4):\n    dl1 = DistributedDL(dl, i, 4)\n    test_eq(list(dl1), (torch.arange(i*13, i*13+12)%50,torch.tensor([i*13+12])%50))\n```\n\n----------------------------------------\n\nTITLE: Testing DataLoader with SleepyQueue\nDESCRIPTION: This code tests `DataLoader` with a `SleepyQueue` class to simulate data loading from a queue with varying latency.  It verifies that the shuffling behavior of the dataloader works as expected when data is retrieved from a queue.  It sets up a `Queue` object with data and checks that shuffling works correctly with the `SleepyQueue`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nclass SleepyQueue():\n    \"Simulate a queue with varying latency\"\n    def __init__(self, q): self.q=q\n    def __iter__(self):\n        while True:\n            time.sleep(random.random()/100)\n            try: yield self.q.get_nowait()\n            except queues.Empty: return\n\nq = Queue()\nfor o in range(30): q.put(o)\nit = SleepyQueue(q)\n\nif not (sys.platform == \"win32\" and IN_NOTEBOOK):\n    %time test_shuffled(L(DataLoader(it, num_workers=4)), L(range(30)))\n```\n\n----------------------------------------\n\nTITLE: Generate Text Predictions\nDESCRIPTION: Generates text predictions using the fine-tuned language model. It takes the initial text, the number of words to predict, and a temperature parameter to control the randomness of the predictions.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))\n```\n\n----------------------------------------\n\nTITLE: Creating an IPython Display Widget\nDESCRIPTION: Defines a function `widget` that takes any object displayable by IPython (like PIL Images, plots, HTML) and wraps it in an `ipywidgets.Output` widget. This allows embedding diverse content within widget layouts. Accepts layout arguments.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09c_vision.widgets.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef widget(im, *args, **layout) -> Output:\n    \"Convert anything that can be `display`ed by `IPython` into a widget\"\n    o = Output(layout=merge(*args, layout))\n    with o: display(im)\n    return o\n```\n\n----------------------------------------\n\nTITLE: Testing num_workers > 0 in scripts\nDESCRIPTION: This code tests running a python script and verifying the output. The intention is to verify that the num_workers functionality works correctly even when the python process is started using the spawn method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n# from subprocess import Popen, PIPE\n# # test num_workers > 0 in scripts works when python process start method is spawn\n# process = Popen([\"python\", \"dltest.py\"], stdout=PIPE)\n# _, err = process.communicate(timeout=15)\n# exit_code = process.wait()\n# test_eq(exit_code, 0)\n```\n\n----------------------------------------\n\nTITLE: Concatenating Torch Tensors or Fallback to Summing Lists in Python\nDESCRIPTION: Implements '_try_concat', which attempts to concatenate a list of torch tensors but falls back to manually summing elements if concatenation fails. Intended for internal data manipulation and safe handling of irregular tensor sets. Requires torch and fastcore's L utility for list extension.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef _try_concat(o):\n    try:    return torch.cat(o)\n    except: return sum([L(o_[i,:] for i in range_of(o_)) for o_ in o], L())\n```\n\n----------------------------------------\n\nTITLE: Adding FP32 Precision Method to Learner in fastai Python\nDESCRIPTION: This patch provides a `to_fp32` method for the `Learner`, allowing users to revert to standard FP32 precision training. It accomplishes this by removing the `MixedPrecision` callback from the learner if it exists.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\ndef to_fp32(self:Learner):\n    \"Set `Learner` to float32 precision\"\n    return self.remove_cb(MixedPrecision)\n```\n\n----------------------------------------\n\nTITLE: Setting Default Initialization for Sigmoid/Tanh\nDESCRIPTION: This code sets the `__default_init__` attribute of several sigmoid and tanh activation functions (both functional and module versions, as well as the custom sigmoid functions defined earlier) to `xavier_uniform_`. This allows these activation functions to be initialized using the Xavier uniform initialization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfor o in F.sigmoid,nn.Sigmoid,F.tanh,nn.Tanh,sigmoid,sigmoid_:\n    o.__default_init__ = xavier_uniform_\n```\n\n----------------------------------------\n\nTITLE: TensorBBox Class Python\nDESCRIPTION: The `TensorBBox` class is designed to represent a tensor of bounding boxes in an image. It inherits from `TensorPoint`. It includes a class method `create` to create an instance from a list or tensor of bounding box coordinates and a `show` method to visualize these boxes. It relies on the `_draw_rect` function defined previously. It handles creating the tensor and displays the boxes on a given context (e.g., an image).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass TensorBBox(TensorPoint):\n    \"Basic type for a tensor of bounding boxes in an image\"\n    @classmethod\n    def create(cls, x, img_size=None)->None: return cls(tensor(x).view(-1, 4).float(), img_size=img_size)\n\n    def show(self, ctx=None, **kwargs):\n        x = self.view(-1,4)\n        for b in x: _draw_rect(ctx, b, hw=False, **kwargs)\n        return ctx\n```\n\n----------------------------------------\n\nTITLE: Testing Labeled Bounding Box Visualization Python\nDESCRIPTION: This code demonstrates how to use the defined classes (`TensorBBox`, `LabeledBBox`) and the `get_annotations` function to load bounding boxes from the COCO Tiny dataset and display them on an image. It loads the COCO Tiny dataset, retrieves the images, associated bounding boxes and labels, and displays them on a matplotlib context.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ncoco = untar_data(URLs.COCO_TINY)\nimages, lbl_bbox = get_annotations(coco/'train.json')\nidx=2\ncoco_fn,bbox = coco/'train'/images[idx],lbl_bbox[idx]\ncoco_img = timg(coco_fn)\n\ntbbox = LabeledBBox(TensorBBox(bbox[0]), bbox[1])\nctx = coco_img.show(figsize=(3,3), cmap='Greys')\ntbbox.show(ctx=ctx);\n```\n\n----------------------------------------\n\nTITLE: Documentation for TensorBoardBaseCallback\nDESCRIPTION: Generates documentation for the TensorBoardBaseCallback class, detailing its methods and purpose in setting up and managing TensorBoard-related actions during training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nshow_doc(TensorBoardBaseCallback)\n```\n\n----------------------------------------\n\nTITLE: Setting up CUDA Device and cuDNN Benchmarking with PyTorch in Python\nDESCRIPTION: Defines the `setup_cuda` function that configures the main CUDA device used by PyTorch and sets the `cudnn.benchmark` flag for optimized GPU performance. The function checks for CUDA availability, adjusts the device based on an environment variable or default value, and toggles the benchmarking mode, which can improve runtime efficiency for fixed input sizes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef setup_cuda(benchmark=defaults.benchmark):\n    \"Sets the main cuda device and sets `cudnn.benchmark` to `benchmark`\"\n    if torch.cuda.is_available():\n        if torch.cuda.current_device()==0:\n            def_gpu = int(os.environ.get('DEFAULT_GPU') or 0)\n            if torch.cuda.device_count()>=def_gpu: torch.cuda.set_device(def_gpu)\n        torch.backends.cudnn.benchmark = benchmark\n```\n\n----------------------------------------\n\nTITLE: Defining PoolType Enum for Pooling Control - Python\nDESCRIPTION: Defines a simple PoolType class/enum with string values for selecting pooling type ('Avg', 'Max', 'Cat'). Used as control input in adaptive_pool and PoolFlatten. No dependencies except direct usage.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass PoolType: Avg,Max,Cat = 'Avg','Max','Cat'\n```\n\n----------------------------------------\n\nTITLE: Show Documentation for log_dataset function\nDESCRIPTION: This snippet uses the `show_doc` function to display the documentation for the `log_dataset` function. The `log_dataset` function is designed to log dataset information to Weights & Biases (wandb).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(log_dataset)\n```\n\n----------------------------------------\n\nTITLE: Implementing broadcast_vec Function for Tensor Normalization in Python\nDESCRIPTION: Creates a utility function that prepares tensors for broadcasting by reshaping them with unit dimensions. Used for normalization parameters like mean and standard deviation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef broadcast_vec(dim, ndim, *t, cuda=True):\n    \"Make a vector broadcastable over `dim` (out of `ndim` total) by prepending and appending unit axes\"\n    v = [1]*ndim\n    v[dim] = -1\n    f = to_device if cuda else noop\n    return [f(tensor(o).view(*v)) for o in t]\n```\n\n----------------------------------------\n\nTITLE: Recursively Concatenating Tensors in fastai\nDESCRIPTION: Defines the `to_concat` function designed to concatenate a list of items `xs` along a specified dimension `dim` (defaulting to 0). It handles nested structures (lists, tuples, dictionaries) by recursively calling itself on corresponding elements. If direct concatenation via `torch.cat` fails (e.g., tensors have incompatible shapes for the dimension), it attempts a fallback strategy by treating the input as a list of items to be combined into a flat list, preserving the original type.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef to_concat(xs, dim=0):\n    \"Concat the element in `xs` (recursively if they are tuples/lists of tensors)\"\n    if not xs: return xs\n    if is_listy(xs[0]): return type(xs[0])([to_concat([x[i] for x in xs], dim=dim) for i in range_of(xs[0])])\n    if isinstance(xs[0],dict):  return {k: to_concat([x[k] for x in xs], dim=dim) for k in xs[0].keys()}\n    #We may receive xs that are not concatenable (inputs of a text classifier for instance),\n    #   in this case we return a big list\n    try:    return retain_type(torch.cat(xs, dim=dim), xs[0])\n    except: return sum([L(retain_type(o_.index_select(dim, tensor(i)).squeeze(dim), xs[0])\n                          for i in range_of(o_)) for o_ in xs], L())\n```\n\n----------------------------------------\n\nTITLE: Create Synthetic Learner\nDESCRIPTION: Creates a fastai `Learner` object preconfigured with synthetic data, a linear regression model, and the mean squared error loss. It is used for quick testing purposes, simplifying the process of setting up a learner for common machine learning tasks.  It allows you to quickly validate training loops and callback behavior by providing default values for the parameters.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/97_test_utils.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@delegates(Learner.__init__)\ndef synth_learner(n_trn=10, n_val=2, cuda=False, lr=1e-3, data=None, model=None, **kwargs):\n    if data is None: data=synth_dbunch(n_train=n_trn,n_valid=n_val, cuda=cuda)\n    if model is None: model=RegModel()\n    return Learner(data, model, lr=lr, loss_func=MSELossFlat(),\n                   opt_func=partial(SGD, mom=0.9), **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Utility: Get size change indices (Python)\nDESCRIPTION: Identifies the indices within a list of tensor sizes where the spatial dimensions (height or width) change. This is crucial for dynamically building models like U-Nets, where skip connections or upsampling layers are needed at points where the feature map size changes due to downsampling in the encoder. It depends on the numpy library.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15a_vision.models.unet.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef _get_sz_change_idxs(sizes):\n    \"Get the indexes of the layers where the size of the activation changes.\"\n    feature_szs = [size[-1] for size in sizes]\n    sz_chg_idxs = list(np.where(np.array(feature_szs[:-1]) != np.array(feature_szs[1:]))[0])\n    return sz_chg_idxs\n```\n\n----------------------------------------\n\nTITLE: Creating and Displaying Datasets with Custom Point and Mask Pipelines - fastai (Python)\nDESCRIPTION: Defines a label function, builds a Datasets object for image and mask data, and displays an example using fastai's show_at utility. Needs Datasets, PILImage, ToTensor, PILMask, mask_fn, cam_fn, and show_at. Used for tuple pipelines typical to semantic segmentation tasks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\ndef _cam_lbl(x): return mask_fn\ncam_tds = Datasets([cam_fn], [[PILImage.create, ToTensor()], [_cam_lbl, PILMask.create, ToTensor()]])\nshow_at(cam_tds, 0);\n```\n\n----------------------------------------\n\nTITLE: Installing fastai Dependencies\nDESCRIPTION: This code snippet installs or upgrades the fastai library using pip. It checks if the `/content` directory exists (likely indicating a Google Colab environment) and then executes the pip install command. The `-Uqq` flags upgrade the library and suppress verbose output.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch Tensors to NumPy Arrays in fastai\nDESCRIPTION: Defines the `to_np` function which recursively converts PyTorch tensors within a potentially nested data structure `x` into NumPy arrays. It uses the `apply` utility to process elements. For each tensor, it accesses the underlying data (`.data`), moves it to the CPU (`.cpu()`), and then converts it to a NumPy array (`.numpy()`).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef to_np(x):\n    \"Convert a tensor to a numpy array.\"\n    return apply(lambda o: o.data.cpu().numpy(), x)\n```\n\n----------------------------------------\n\nTITLE: Text Classifier Creation\nDESCRIPTION: The `get_text_classifier` function creates a text classifier model using an encoder (like an RNN) and a pooling-based classifier head.  It takes an architecture `arch`, vocabulary size `vocab_sz`, number of classes `n_class`, sequence length, config, drop_mult to scale all the dropout probabilities,  hidden layers dimensions (`lin_ftrs`), dropout probabilities (`ps`), padding index, maximal output length, and the output range.  It returns a full text classification model and optionally applies initialization if specified in the config.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef get_text_classifier(\n    arch:Callable, # Function or class that can generate a language model architecture\n    vocab_sz:int, # Size of the vocabulary \n    n_class:int, # Number of classes\n    seq_len:int=72, # Backpropagation through time\n    config:dict=None, # Encoder configuration dictionary\n    drop_mult:float=1., # Multiplicative factor to scale all dropout probabilities in `config`\n    lin_ftrs:list=None, # List of hidden sizes for classifier head as `int`s\n    ps:list=None, # List of dropout probabilities for classifier head as `float`s\n    pad_idx:int=1, # Padding token id\n    max_len:int=72*20, # Maximal output length for `SentenceEncoder`\n    y_range:tuple=None # Tuple of (low, high) output value bounds\n):\n    \"Create a text classifier from `arch` and its `config`, maybe `pretrained`\"\n    meta = _model_meta[arch]\n    cfg = meta['config_clas'].copy()\n    cfg.update(ifnone(config, {}))\n    config = cfg\n    for k in config.keys():\n        if k.endswith('_p'): config[k] *= drop_mult\n    if lin_ftrs is None: lin_ftrs = [50]\n    if ps is None:  ps = [0.1]*len(lin_ftrs)\n    layers = [config[meta['hid_name']] * 3] + lin_ftrs + [n_class]\n    ps = [config.pop('output_p')] + ps\n    init = config.pop('init') if 'init' in config else None\n    encoder = SentenceEncoder(seq_len, arch(vocab_sz, **config), pad_idx=pad_idx, max_len=max_len)\n    model = SequentialRNN(encoder, PoolingLinearClassifier(layers, ps, bptt=seq_len, y_range=y_range))\n    return model if init is None else model.apply(init)\n```\n\n----------------------------------------\n\nTITLE: Combining All Texts into a Numpy Array (Python)\nDESCRIPTION: Concatenates the training and validation set text columns into a single numpy array for unified processing. Dependencies: numpy library. Expects DataFrames with at least one text column; output is a 1-d numpy array of text strings.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nall_texts = np.concatenate([df_train[0].values, df_valid[0].values])\n```\n\n----------------------------------------\n\nTITLE: Text Classifier - Padding Test\nDESCRIPTION: Tests whether the results of the text classifier remain consistent when padding is added to the input. It adds padding to the inputs and compares the outputs to verify that the model is robust to padding.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ntst.eval()\ny = tst(x)\nx1 = torch.cat([x, tensor([2,1,1,1,1,1,1,1,1,1])[:,None]], dim=1)\ny1 = tst(x1)\ntest_close(y[0][1:],y1[0][1:])\n```\n\n----------------------------------------\n\nTITLE: Comparing LabelSmoothingCrossEntropy and Flat Versions - Fastai/PyTorch\nDESCRIPTION: This snippet compares the standard `LabelSmoothingCrossEntropy` with its `Flat` counterpart. It demonstrates that, with appropriate input shaping for the standard version (transposing), both losses produce numerically close results when given inputs with additional dimensions, confirming the flattening functionality of `LabelSmoothingCrossEntropyFlat`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n#These two should always equal each other since the Flat version is just passing data through\nlmce = LabelSmoothingCrossEntropy()\nlmce_flat = LabelSmoothingCrossEntropyFlat()\noutput = torch.randn(32, 5, 10)\ntarget = torch.randint(0, 10, (32,5))\ntest_close(lmce(output.transpose(-1,-2), target), lmce_flat(output,target))\n```\n\n----------------------------------------\n\nTITLE: Getting a Batch of Data\nDESCRIPTION: This retrieves a batch of images (xb) and labels (yb) from the data loaders `dls`.  The shapes of the batch and labels are then printed to verify the data format.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nxb,yb = dls.one_batch()\nxb.shape,yb.shape\n```\n\n----------------------------------------\n\nTITLE: Drawing Utilities for Bounding Boxes Python\nDESCRIPTION: These functions, `_draw_outline` and `_draw_rect`, provide the basic utilities to draw a bounding box and its outline on a matplotlib `Axes` object. `_draw_outline` adds a black outline with a specified line width. `_draw_rect` draws a rectangle using matplotlib patches, potentially adding a text label to it.  Dependencies include `matplotlib.patches` and `matplotlib.patheffects`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef _draw_outline(o, lw):\n    o.set_path_effects([patheffects.Stroke(linewidth=lw, foreground='black'), patheffects.Normal()])\n\ndef _draw_rect(ax, b, color='white', text=None, text_size=14, hw=True, rev=False):\n    lx,ly,w,h = b\n    if rev: lx,ly,w,h = ly,lx,h,w\n    if not hw: w,h = w-lx,h-ly\n    patch = ax.add_patch(patches.Rectangle((lx,ly), w, h, fill=False, edgecolor=color, lw=2))\n    _draw_outline(patch, 4)\n    if text is not None:\n        patch = ax.text(lx,ly, text, verticalalignment='top', color=color, fontsize=text_size, weight='bold')\n        _draw_outline(patch,1)\n```\n\n----------------------------------------\n\nTITLE: Reassigning DataLoaders to Learner - Python\nDESCRIPTION: Updates the learner's DataLoaders to the new set created with higher-resolution images. Necessary for continued training with new augmentation/pipeline. Assumes learner and new DataLoaders are defined.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nlearn.dls = dls\n```\n\n----------------------------------------\n\nTITLE: Defining a Dataset with Multiple Transforms and Custom Splits\nDESCRIPTION: Defines a custom transform `_Tfm1` applied to subset 0, then creates a dataset with range 0-7 applying different transforms and specific splits. Validates the dataset's transformed output.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_67\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n#Add test set for inference\n\n# only transform subset 1\nclass _Tfm1(Transform):\n    split_idx=0\n    def encodes(self, x): return x*3\n\ndsets = Datasets(range(8), [[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]])\ntest_eq(dsets.train, [(3,),(6,),(15,),(21,)])\ntest_eq(dsets.valid, [(0,),(6,),(8,),(12,)])\n```\n\n----------------------------------------\n\nTITLE: Testing ReduceLROnPlateau with minimum learning rate constraint\nDESCRIPTION: Example demonstrating ReduceLROnPlateau with a specified minimum learning rate that prevents the learning rate from dropping below a certain threshold.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner(n_trn=2)\nlearn.fit(n_epoch=6, lr=5e-8, cbs=ReduceLROnPlateau(monitor='valid_loss', min_delta=0.1, patience=2, min_lr=1e-8))\n```\n\n----------------------------------------\n\nTITLE: Implementing Precision Score Wrapper for Multi-label Classification in Python\nDESCRIPTION: A wrapper function that adapts scikit-learn's precision_score for use in fastai's multi-label classification tasks, with customization options for threshold, activation, and averaging method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef PrecisionMulti(thresh=0.5, sigmoid=True, labels=None, pos_label=1, average='macro', sample_weight=None):\n    \"Precision for multi-label classification problems\"\n    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n    return skm_to_fastai(skm.precision_score, thresh=thresh, activation=activation, flatten=False,\n                         labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight)\n```\n\n----------------------------------------\n\nTITLE: Exiting script to prevent accidental execution\nDESCRIPTION: Immediately terminates script execution to avoid unintended running of subsequent commands, used as a safeguard in setup notebooks or scripts.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/dev-setup.ipynb#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nexit\n```\n\n----------------------------------------\n\nTITLE: Testing SaveModelCallback with synthetic learners\nDESCRIPTION: Examples demonstrating how SaveModelCallback works with various parameters including saving at the end of training, saving every epoch, and saving every N epochs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner(n_trn=2, path=Path.cwd()/'tmp')\nlearn.fit(n_epoch=2, cbs=SaveModelCallback())\nassert (Path.cwd()/'tmp/models/model.pth').exists()\nlearn = synth_learner(n_trn=2, path=Path.cwd()/'tmp')\nlearn.fit(n_epoch=2, cbs=SaveModelCallback(fname='end',at_end=True))\nassert (Path.cwd()/'tmp/models/end.pth').exists()\nlearn.fit(n_epoch=2, cbs=SaveModelCallback(every_epoch=True))\nfor i in range(2): assert (Path.cwd()/f'tmp/models/model_{i}.pth').exists()\nshutil.rmtree(Path.cwd()/'tmp')\nlearn.fit(n_epoch=4, cbs=SaveModelCallback(every_epoch=2))\nfor i in range(4): \n    if not i%2: assert (Path.cwd()/f'tmp/models/model_{i}.pth').exists()\n    else:       assert not (Path.cwd()/f'tmp/models/model_{i}.pth').exists()\nshutil.rmtree(Path.cwd()/'tmp')\n```\n\n----------------------------------------\n\nTITLE: Calculating elapsed time between events in Python\nDESCRIPTION: Helper function that calculates the time elapsed in days since the last occurrence of each event within groups defined by a base field.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef _get_elapsed(df,field_names, date_field, base_field, prefix):\n    for f in field_names:\n        day1 = np.timedelta64(1, 'D')\n        last_date,last_base,res = np.datetime64(),None,[]\n        for b,v,d in zip(df[base_field].values, df[f].values, df[date_field].values):\n            if last_base is None or b != last_base:\n                last_date,last_base = np.datetime64(),b\n            if v: last_date = d\n            res.append(((d-last_date).astype('timedelta64[D]') / day1))\n        df[prefix + f] = res\n    return df\n```\n\n----------------------------------------\n\nTITLE: Factory Functions for XResNet Variants with SE and ResNeXt Blocks in Python\nDESCRIPTION: Provides factory functions to create XResNet models configured with SE blocks and ResNeXt blocks, across different depths and layer configurations. Functions such as xse_resnet18, xse_resnext18, xresnext18, and others instantiate models with appropriate block types, output classes, and SE block parameters. These serve as ready-to-use constructors for commonly used XResNet architectures with enhanced modules.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/11_vision.models.xresnet.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef xse_resnet18(n_out=1000, pretrained=False, **kwargs):   return XResNet(SEBlock,  1, g0, n_out=n_out, **se_kwargs1, **kwargs)\ndef xse_resnext18(n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 1, g0, n_out=n_out, **se_kwargs2, **kwargs)\ndef xresnext18(n_out=1000, pretrained=False, **kwargs):     return XResNet(SEResNeXtBlock, 1, g0, n_out=n_out, **se_kwargs3, **kwargs)\ndef xse_resnet34(n_out=1000, pretrained=False, **kwargs):   return XResNet(SEBlock,  1, g1, n_out=n_out, **se_kwargs1, **kwargs)\ndef xse_resnext34(n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 1, g1, n_out=n_out, **se_kwargs2, **kwargs)\ndef xresnext34(n_out=1000, pretrained=False, **kwargs):     return XResNet(SEResNeXtBlock, 1, g1, n_out=n_out, **se_kwargs3, **kwargs)\ndef xse_resnet50(n_out=1000, pretrained=False, **kwargs):   return XResNet(SEBlock,  4, g1, n_out=n_out, **se_kwargs1, **kwargs)\ndef xse_resnext50(n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 4, g1, n_out=n_out, **se_kwargs2, **kwargs)\ndef xresnext50(n_out=1000, pretrained=False, **kwargs):     return XResNet(SEResNeXtBlock, 4, g1, n_out=n_out, **se_kwargs3, **kwargs)\ndef xse_resnet101(n_out=1000, pretrained=False, **kwargs):  return XResNet(SEBlock,  4, g2, n_out=n_out, **se_kwargs1, **kwargs)\ndef xse_resnext101(n_out=1000, pretrained=False, **kwargs): return XResNet(SEResNeXtBlock, 4, g2, n_out=n_out, **se_kwargs2, **kwargs)\ndef xresnext101(n_out=1000, pretrained=False, **kwargs):    return XResNet(SEResNeXtBlock, 4, g2, n_out=n_out, **se_kwargs3, **kwargs)\ndef xse_resnet152(n_out=1000, pretrained=False, **kwargs):  return XResNet(SEBlock,  4, g3, n_out=n_out, **se_kwargs1, **kwargs)\ndef xsenet154(n_out=1000, pretrained=False, **kwargs):\n    return XResNet(SEBlock, g3, groups=64, reduction=16, p=0.2, n_out=n_out)\ndef xse_resnext18_deep  (n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 1, g0+[1,1], n_out=n_out, **se_kwargs2, **kwargs)\ndef xse_resnext34_deep  (n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 1, g1+[1,1], n_out=n_out, **se_kwargs2, **kwargs)\ndef xse_resnext50_deep  (n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 4, g1+[1,1], n_out=n_out, **se_kwargs2, **kwargs)\ndef xse_resnext18_deeper(n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 1, [2,2,1,1,1,1,1,1], n_out=n_out, **se_kwargs2, **kwargs)\ndef xse_resnext34_deeper(n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 1, [3,4,4,2,2,1,1,1], n_out=n_out, **se_kwargs2, **kwargs)\ndef xse_resnext50_deeper(n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 4, [3,4,4,2,2,1,1,1], n_out=n_out, **se_kwargs2, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Defining a Resnet-ish Model\nDESCRIPTION: This defines a CNN model inspired by ResNet architectures, using the `ResBlock` class.  The model uses several `ResBlock` instances interleaved with convolutional layers. This allows for the creation of deeper networks and more complex feature extraction.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\nmodel = nn.Sequential(\n    conv2(1, 8),\n    ResBlock(8),\n    conv2(8, 16),\n    ResBlock(16),\n    conv2(16, 32),\n    ResBlock(32),\n    conv2(32, 16),\n    ResBlock(16),\n    conv2(16, 10),\n    Flatten()\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Text Prompt for Language Model Generation in Python\nDESCRIPTION: Creates a multi-line string variable `prompt` containing the initial text sequence to feed into the language model for text generation. The format includes newlines and equals signs, likely mimicking the structure observed in the training or validation data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"\\n = Unicorn = \\n \\n A unicorn is a magical creature with a rainbow tail and a horn\"\n```\n\n----------------------------------------\n\nTITLE: Preparing a Row for Inference - Python\nDESCRIPTION: This code prepares a single row from the DataFrame for prediction. This is usually a single row with values for the features the model has been trained on. This step sets up the input for the model's prediction function.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-tabular.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nrow = df.iloc[0]\n```\n\n----------------------------------------\n\nTITLE: Importing fastai Modules\nDESCRIPTION: This imports all necessary modules from the fastai library, specifically the text module, to provide access to pre-built functionality for text processing and model training, using the `*` wildcard for brevity.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.text.all import *\n```\n\n----------------------------------------\n\nTITLE: Find Collation Failure\nDESCRIPTION: This function attempts to identify why collation is failing within a `DataLoaders`. It checks if elements are collatable types and iterates through members, attempting default collation. If a collate error occurs, it gathers shapes and returns formatted diagnostics explaining why.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.data.load import _collate_types\n\ndef _find_fail_collate(s):\n    s = L(*s)\n    for x in s[0]:\n        if not isinstance(x, _collate_types): return f\"{type(x).__name__} is not collatable\"\n    for i in range_of(s[0]):\n        try: _ = default_collate(s.itemgot(i))\n        except:\n            shapes = [getattr(o[i], 'shape', None) for o in s]\n            return f\"Could not collate the {i}-th members of your tuples because got the following shapes\\n{','.join([str(s) for s in shapes])}\"\n```\n\n----------------------------------------\n\nTITLE: Applying NegTfm Transform with DataLoader\nDESCRIPTION: This snippet demonstrates the use of a custom transform NegTfm integrated into a DataLoader. It tests the effect of the split index on batch processing, producing either range or negated range tensors. Dependencies include NegTfm, TfmdLists, TfmdDL, and related test functions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\ntfm = NegTfm(split_idx=1)\ntds = TfmdLists(start, A())\ntdl = TfmdDL(tds, after_batch=tfm, bs=4)\nx = tdl.one_batch()\ntest_eq(x, torch.arange(4))\ntds.split_idx = 1\nx = tdl.one_batch()\ntest_eq(x, -torch.arange(4))\ntds.split_idx = 0\nx = tdl.one_batch()\ntest_eq(x, torch.arange(4))\n```\n\n----------------------------------------\n\nTITLE: Implementing teardown_distrib for Distributed Training\nDESCRIPTION: Creates a function that cleans up distributed training resources by destroying the process group if it was initialized, ensuring proper resource management.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef teardown_distrib():\n    \"Free distributed training resources\"\n    if torch.distributed.is_initialized(): torch.distributed.destroy_process_group()\n```\n\n----------------------------------------\n\nTITLE: Preparing and Joining Training and Test Data\nDESCRIPTION: Converts Date columns to proper date format and joins the training and test data with the enhanced store data containing all features.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmake_date(train, 'Date')\nmake_date(test, 'Date')\ntrain_fe = join_df(train, store, ['Store', 'Date'])\ntest_fe = join_df(test, store, ['Store', 'Date'])\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Normalize\nDESCRIPTION: This code shows how to use the `Normalize` processor with a `TabularGPU` object. It calculates the mean and standard deviation of the continuous variable and applies the normalization.  It also shows how to decode the normalized data back to the original values and includes test to ensure that calculations are close to correct.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf = cudf.from_pandas(pd.DataFrame({'a':[0,1,2,3,4]}))\nto = TabularGPU(df, Normalize, cont_names='a')\nnorm = to.procs.normalize\nx = np.array([0,1,2,3,4])\nm,s = x.mean(),x.std()\ntest_eq(norm.means['a'], m)\ntest_close(norm.stds['a'], s)\ntest_close(to.a.to_array(), (x-m)/s)\ndf1 = cudf.from_pandas(pd.DataFrame({'a':[5,6,7]}))\nto1 = to.new(df1)\nnorm(to1)\ntest_close(to1.a.to_array(), (np.array([5,6,7])-m)/s\n\nto2 = TabularPandas(to1.items.to_pandas(), None, cont_names='a')\nto2 = norm.decode(to2)\ntest_close(to2.a, [5,6,7])\n```\n\n----------------------------------------\n\nTITLE: Testing MSELossFlat Functionality - Fastai/PyTorch\nDESCRIPTION: This snippet tests the `MSELossFlat` function. It initializes the loss, performs a forward pass with sigmoid-activated output and binary integer targets, and confirms that the standard `nn.MSELoss` would fail with these shapes, validating the flattening behavior of `MSELossFlat`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ntst = MSELossFlat()\noutput = torch.sigmoid(torch.randn(32, 5, 10))\ntarget = torch.randint(0,2,(32, 5, 10))\n_ = tst(output, target)\ntest_fail(lambda x: nn.MSELoss()(output,target))\n```\n\n----------------------------------------\n\nTITLE: Adding singleton dimensions with optional multiple dims (Python)\nDESCRIPTION: The `unsqueeze` function adds one or more singleton dimensions to a tensor at the specified position. It can add `n` dimensions recursively in a loop, useful for reshaping tensors to match expected dimensions in operations.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\n#|export\ndef unsqueeze(x, dim=-1, n=1):\n    \"Same as `torch.unsqueeze` but can add `n` dims\"\n    for _ in range(n): x = x.unsqueeze(dim)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Testing tensor equality across different inputs (Python)\nDESCRIPTION: This snippet tests equality between tensors created via different methods: directly, from arrays, and with varying data types. It also verifies type equivalence for floating point tensors. Dependencies include PyTorch and NumPy. This ensures consistency in tensor creation and conversions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\ntest_eq(tensor(torch.tensor([1,2,3])), torch.tensor([1,2,3]))\ntest_eq(tensor(array([1,2,3])), torch.tensor([1,2,3]))\ntest_eq(tensor(1,2,3), torch.tensor([1,2,3]))\ntest_eq_type(tensor(1.0), torch.tensor(1.0))\n```\n\n----------------------------------------\n\nTITLE: Importing Core Dependencies for fastai Layers - Python\nDESCRIPTION: Imports necessary modules and functions from fastai and PyTorch, including wrappers for weight normalization and spectral normalization. Dependencies include fastai.core, fastai.torch_core, and torch.nn.utils. All custom layers in the file rely on these imports.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.imports import *\nfrom fastai.torch_imports import *\nfrom fastai.torch_core import *\nfrom torch.nn.utils import weight_norm, spectral_norm\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for Recorder Methods in fastai (Python)\nDESCRIPTION: Provides snippets to render in-notebook/method documentation (docstrings) for key Recorder methods such as before_fit, before_epoch, before_validate, after_batch, after_epoch, and plot_loss. These lines are intended for interactive environments and require the show_doc utility. Does not accept inputs or return outputs directly.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_73\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Recorder.before_fit)\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Recorder.before_epoch)\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Recorder.before_validate)\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Recorder.after_batch)\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Recorder.after_epoch)\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Recorder.plot_loss)\n```\n\n----------------------------------------\n\nTITLE: Importing nbdev showdoc\nDESCRIPTION: This code imports the `showdoc` function from the `nbdev.showdoc` module and the `math` module. The `showdoc` function is likely used to display documentation for fastai objects, while the math module is a standard python library.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev.showdoc import *\nimport math\n```\n\n----------------------------------------\n\nTITLE: Export Word Embeddings from Language Models\nDESCRIPTION: Extracts and exports word embeddings from language model layers for visualization in TensorBoard, allowing optional limits and starting indices.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n def projector_word_embeddings(learn=None, layer=None, vocab=None, limit=-1, start=0, log_dir=None):\n    \"Extracts and exports word embeddings from language models embedding layers\"\n    if not layer:\n        if   isinstance(learn, LMLearner):   layer = learn.model[0].encoder\n        elif isinstance(learn, TextLearner): layer = learn.model[0].module.encoder\n    emb = layer.weight\n    img = torch.full((len(emb),3,8,8), 0.7)\n    vocab = learn.dls.vocab[0] if vocab == None else vocab\n    vocab = list(map(lambda x: f'{x}_', vocab))\n    writer = SummaryWriter(log_dir=log_dir)\n    end = start + limit if limit >= 0 else -1\n    writer.add_embedding(emb[start:end], metadata=vocab[start:end], label_img=img[start:end])\n    writer.close()\n```\n\n----------------------------------------\n\nTITLE: Tests for _merge_tfms functionality\nDESCRIPTION: Test cases for the _merge_tfms function to verify it correctly handles various transformation types including instantiated classes, methods, functions, and lambdas.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n#|hide\ntfms = _merge_tfms([Categorize, MultiCategorize, Categorize(['dog', 'cat'])], Categorize(['a', 'b']))\n#If there are several instantiated versions, the last one is kept.\ntest_eq(len(tfms), 2)\ntest_eq(tfms[1].__class__, MultiCategorize)\ntest_eq(tfms[0].__class__, Categorize)\ntest_eq(tfms[0].vocab, ['a', 'b'])\n\ntfms = _merge_tfms([PILImage.create, PILImage.show])\n#Check methods are properly separated\ntest_eq(len(tfms), 2)\ntfms = _merge_tfms([show_image, set_trace])\n#Check functions are properly separated\ntest_eq(len(tfms), 2)\n\n_f = lambda x: 0\ntest_eq(len(_merge_tfms([_f,lambda x: 1])), 2)\ntest_eq(len(_merge_tfms([_f,_f])), 1)\n```\n\n----------------------------------------\n\nTITLE: Creating a Learner for Collaborative Filtering - fastai - Python\nDESCRIPTION: This code defines the `collab_learner` function, which creates a Learner object suitable for collaborative filtering.  It takes a `DataLoaders` object (`dls`), the number of factors (`n_factors`), and other optional parameters such as `use_nn` to choose between `EmbeddingDotBias` and `EmbeddingNN` models. It infers embedding sizes, sets the loss function to `MSELossFlat` if not provided, and allows customization of the model's configuration.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/45_collab.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@delegates(Learner.__init__)\ndef collab_learner(dls, n_factors=50, use_nn=False, emb_szs=None, layers=None, config=None, y_range=None, loss_func=None, **kwargs):\n    \"Create a Learner for collaborative filtering on `dls`.\"\n    emb_szs = get_emb_sz(dls, ifnone(emb_szs, {}))\n    if loss_func is None: loss_func = MSELossFlat()\n    if config is None: config = tabular_config()\n    if y_range is not None: config['y_range'] = y_range\n    if layers is None: layers = [n_factors]\n    if use_nn: model = EmbeddingNN(emb_szs=emb_szs, layers=layers, **config)\n    else:      model = EmbeddingDotBias.from_classes(n_factors, dls.classes, y_range=y_range)\n    return Learner(dls, model, loss_func=loss_func, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Logging Function for Image Predictions\nDESCRIPTION: Defines a function to log images and predictions to TensorBoard, displaying sample results with color-coded correctness indicators.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n@dispatch\ndef tensorboard_log(x:TensorImage, y: TensorCategory, samples, outs, writer, step):\n    fig,axs = get_grid(len(samples), return_fig=True)\n    for i in range(2):\n        axs = [b.show(ctx=c) for b,c in zip(samples.itemgot(i),axs)]\n    axs = [r.show(ctx=c, color='green' if b==r else 'red')\n            for b,r,c in zip(samples.itemgot(1),outs.itemgot(0),axs)]\n    writer.add_figure('Sample results', fig, step)\n```\n\n----------------------------------------\n\nTITLE: Partial ReLU Function with Inplace set in Python\nDESCRIPTION: This is a simple helper created using `functools.partial`. It sets the `inplace` argument of `nn.ReLU` to `True` by default. This partial function can be used as a convenient way to specify an in-place ReLU activation when defining network layers.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_68\n\nLANGUAGE: python\nCODE:\n```\n#|export\ninplace_relu = partial(nn.ReLU, inplace=True)\n```\n\n----------------------------------------\n\nTITLE: Defining a fastai Callback to Handle HuggingFace Model Output in Python\nDESCRIPTION: Creates a fastai `Callback` named `DropOutput` to modify the model's prediction output during the training loop. It specifically addresses HuggingFace models that return a tuple, selecting only the first element (actual predictions) for the loss function calculation by overriding the `after_pred` event. It uses the `self.pred` shortcut for read access and `self.learn.pred` for write access within the callback.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nclass DropOutput(Callback):\n    def after_pred(self): self.learn.pred = self.pred[0]\n```\n\n----------------------------------------\n\nTITLE: Creating Language Model DataLoaders\nDESCRIPTION: This creates `DataLoaders` specifically for language modeling, using `TextDataLoaders.from_folder`. It sets `is_lm=True` and specifies a validation percentage (`valid_pct`) to split the data.  This prepares data for fine-tuning a language model.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndls_lm = TextDataLoaders.from_folder(path, is_lm=True, valid_pct=0.1)\n```\n\n----------------------------------------\n\nTITLE: Resize Image Function - No Resize Test\nDESCRIPTION: This tests the resize_image function when max_size is None. The purpose is to verify that the function handles the case where no resizing is required correctly. It sets the source directory to 'images' and calls the function.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nfile = 'puppy.jpg'\ndest = Path('images')\nresize_image(file, dest, src=dest, max_size=None)\n```\n\n----------------------------------------\n\nTITLE: Squeeze and Excitation Module Creation in Python\nDESCRIPTION: This function creates a Squeeze and Excitation (SE) module using `SequentialEx`. It consists of an adaptive average pooling layer, followed by two `ConvLayer` instances with 1x1 kernels for dimension reduction and expansion (the 'squeeze' part), and finally a `ProdLayer` to scale the features by the learned attention weights (the 'excitation' part). The `reduction` parameter controls the bottleneck size.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_69\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef SEModule(ch, reduction, act_cls=defaults.activation):\n    nf = math.ceil(ch//reduction/8)*8\n    return SequentialEx(nn.AdaptiveAvgPool2d(1),\n                        ConvLayer(ch, nf, ks=1, norm_type=None, act_cls=act_cls),\n                        ConvLayer(nf, ch, ks=1, norm_type=None, act_cls=nn.Sigmoid),\n                        ProdLayer())\n```\n\n----------------------------------------\n\nTITLE: Upgrading fastai on Google Colab (Shell)\nDESCRIPTION: Shell command to conditionally upgrade the fastai library on a Google Colab environment if the `/content` directory exists. This ensures the latest version of fastai is used.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/74_huggingface.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Creating Normalization Layers\nDESCRIPTION: This function, `_get_norm`, creates normalization layers (BatchNorm or InstanceNorm) of specified dimension (`ndim`) based on the `prefix` provided ('BatchNorm' or 'InstanceNorm'). It initializes the bias to a small value and the weight to 1 or 0 depending on the `zero` parameter. It takes the number of features `nf` as input and accepts keyword arguments (`kwargs`) to be passed to the normalization layer constructor.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef _get_norm(prefix, nf, ndim=2, zero=False, **kwargs):\n    \"Norm layer with `nf` features and `ndim` initialized depending on `norm_type`.\"\n    assert 1 <= ndim <= 3\n    bn = getattr(nn, f\"{prefix}{ndim}d\")(nf, **kwargs)\n    if bn.affine:\n        bn.bias.data.fill_(1e-3)\n        bn.weight.data.fill_(0. if zero else 1.)\n    return bn\n```\n\n----------------------------------------\n\nTITLE: Example Prediction after Training in Python\nDESCRIPTION: Demonstrates how to make a prediction using the `predict` method of a trained `Learner` instance. It passes a single input tensor `torch.tensor([[0.1]])` to the model for inference.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/16_callback.progress.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nlearn.predict(torch.tensor([[0.1]]))\n```\n\n----------------------------------------\n\nTITLE: Getting Train and Test Text Files for Classification (Python)\nDESCRIPTION: Obtains .txt file paths from the 'train' and 'test' folders for use in classification tasks. Input: path variable; Output: texts list containing relevant files.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntexts = get_files(path, extensions=['.txt'], folders=['train', 'test'])\n```\n\n----------------------------------------\n\nTITLE: Open Conda Package with tarfile (Python)\nDESCRIPTION: Opens the specified Conda package file (`.tar.bz2`) in read mode with bzip2 decompression using the `tarfile` library. The resulting object `zpkg` can be used to inspect the archive's contents.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nzpkg = tarfile.open(pkg,\"r:bz2\")\n```\n\n----------------------------------------\n\nTITLE: Testing BCEWithLogitsLossFlat\nDESCRIPTION: This code tests the `BCEWithLogitsLossFlat` class.  It creates an instance of the loss function, generates random input and target tensors, and verifies that the flattened version works where the standard PyTorch version would fail due to shape or target dtype mismatch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntst = BCEWithLogitsLossFlat()\noutput = torch.randn(32, 5, 10)\ntarget = torch.randn(32, 5, 10)\n#nn.BCEWithLogitsLoss would fail with those two tensors, but not our flattened version.\n_ = tst(output, target)\ntest_fail(lambda x: nn.BCEWithLogitsLoss()(output,target))\noutput = torch.randn(32, 5)\ntarget = torch.randint(0,2,(32, 5))\n#nn.BCEWithLogitsLoss would fail with int targets but not our flattened version.\n_ = tst(output, target)\n```\n\n----------------------------------------\n\nTITLE: Recursively Finding Device of Tensor or Nested Containers in Python\nDESCRIPTION: Uses the recursive item_find function to locate a tensor nested anywhere within an input batch 'b' and returns its associated device (e.g., CPU or GPU). This aids in automatically detecting the device where the batch tensors reside regardless of nested structure complexity.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_68\n\nLANGUAGE: python\nCODE:\n```\ndef find_device(b):\n    \"Recursively search the device of `b`.\"\n    return item_find(b).device\n```\n\n----------------------------------------\n\nTITLE: Detailed Batch Collate Error Reporting Function - Python\nDESCRIPTION: Implements collate_error, which provides detailed error messages when batch collation fails due to tensor shape mismatches. Iterates through batch items to identify and report type and shape inconsistencies, suggesting the addition of shape-normalizing transforms. Enhances debugging for custom collation workflows; expects fastai's after_item transform pattern.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef collate_error(e:Exception, batch):\n    \"Raises error when the batch could not collate, stating what items in the batch are different sizes and their types\"\n    err = f'Error when trying to collate the data into batches with fa_collate, at least two tensors in the batch are not the same size.\\n\\n'\n    # we need to iterate through the entire batch and find a mismatch\n    length = len(batch[0])\n    for idx in range(length): # for each type in the batch\n        for i, item in enumerate(batch):\n            if i == 0: shape_a, type_a  = item[idx].shape, item[idx].__class__.__name__\n            elif item[idx].shape != shape_a:\n                shape_b = item[idx].shape\n                if shape_a != shape_b:\n                    err += f'Mismatch found on axis {idx} of the batch and is of type `{type_a}`:\\n\\tItem at index 0 has shape: {shape_a}\\n\\tItem at index {i} has shape: {shape_b}\\n\\nPlease include a transform in `after_item` that ensures all data of type {type_a} is the same size'\n                    e.args = [err]\n                    raise\n```\n\n----------------------------------------\n\nTITLE: Tabular Weighted Data Loader - Python\nDESCRIPTION: This class, `TabWeightedDL`, extends `TabDataLoader` to create a weighted data loader for tabular data. It allows assigning weights to each sample, enabling handling of imbalanced datasets. Samples are chosen based on these weights.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\n@delegates()\nclass TabWeightedDL(TabDataLoader):\n    \"A transformed `DataLoader` for Tabular Weighted data\"\n    def __init__(self, dataset, bs=16, wgts=None, shuffle=False, after_batch=None, num_workers=0, **kwargs):\n        wgts = np.array([1.]*len(dataset) if wgts is None else wgts)\n        self.wgts = wgts / wgts.sum()\n        super().__init__(dataset, bs=bs, shuffle=shuffle, after_batch=after_batch, num_workers=num_workers, **kwargs)\n        self.idxs = self.get_idxs()\n\n    def get_idxs(self):\n        if self.n == 0: return []\n        if not self.shuffle: return super().get_idxs()\n        return list(np.random.choice(self.n, self.n, p=self.wgts))\n\nTabularPandas._dl_type = TabWeightedDL\n```\n\n----------------------------------------\n\nTITLE: Example Wandb Integration\nDESCRIPTION: This section demonstrates how to integrate Weights & Biases (W&B) with fastai for experiment tracking. It shows how to initialize W&B, use the `WandbCallback` during training, and track datasets and models using the `log_model` and `log_dataset` functions. It also provides example code for logging only during one phase and continuously for all phases.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_21\n\n\n\n----------------------------------------\n\nTITLE: Tabular Normalization Setup and Encoding - Python\nDESCRIPTION: These functions define a `Normalize` transform for tabular data, computing and storing means/std deviations during setup and applying normalization during encoding/decoding. It calculates statistics on the training split (or entire dataset if no split is provided) and uses them to normalize continuous variables.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n@Normalize\ndef setups(self, to:Tabular):\n    store_attr(but='to', means=dict(getattr(to, 'train', to).conts.mean()),\n               stds=dict(getattr(to, 'train', to).conts.std(ddof=0)+1e-7))\n    return self(to)\n\n@Normalize\ndef encodes(self, to:Tabular):\n    to.conts = (to.conts-self.means) / self.stds\n    return to\n\n@Normalize\ndef decodes(self, to:Tabular):\n    to.conts = (to.conts*self.stds ) + self.means\n    return to\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch for Tensor Operations (Python)\nDESCRIPTION: This snippet imports the torch library, allowing the use of PyTorch tensors necessary for model input, especially when running inference and training steps with transformer models. No parameters required.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n```\n\n----------------------------------------\n\nTITLE: Wrapper for Timm Model Body - Python\nDESCRIPTION: A custom PyTorch module designed to wrap a model loaded from the `timm` library. It determines if the underlying timm model requires feature extraction (`forward_features`) based on its `default_cfg`. The `forward` method handles passing the input `x` through the wrapped model, using `forward_features` if necessary, otherwise calling the standard `forward`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass TimmBody(nn.Module):\n    def __init__(self, model, pretrained:bool=True, cut=None, n_in:int=3):\n        super().__init__()\n        self.needs_pool = model.default_cfg.get('pool_size', None) is not None\n        self.model = model if cut is None else cut_model(model, cut)\n    \n    def forward(self,x): return self.model.forward_features(x) if self.needs_pool else self.model(x)\n```\n\n----------------------------------------\n\nTITLE: Installing and Updating fastai\nDESCRIPTION: Installs or upgrades the fastai library on Google Colab if the environment is detected.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/45_collab.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#| eval: false\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Importing Vision Data Utilities\nDESCRIPTION: Imports vision data modules from fastai for image data processing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nfrom fastai.vision.data import *\n```\n\n----------------------------------------\n\nTITLE: Creating Small Sample DataFrame - Python\nDESCRIPTION: Subsets the main training DataFrame using the randomly selected indices to create a smaller DataFrame for faster experimentation.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsmall_df = train_df.iloc[idx]\n```\n\n----------------------------------------\n\nTITLE: Test `test_set` Function with Various Input Lengths and `rm_tfms`\nDESCRIPTION: Runs the `test_set` function on datasets with multiple transforms and splits, testing with different input lengths and specified removal transforms index or tuple, verifying batch transformations.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_70\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n#Test with various input lengths\ndsets = Datasets(range(8), [[_Tfm(),_Tfm1()],[_Tfm(),_Tfm1()],[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]])\ntst = test_set(dsets, [1,2,3])\ntest_eq(tst, [(2,2),(4,4),(6,6)])\n\ndsets = Datasets(range(8), [[_Tfm(),_Tfm1()],[_Tfm(),_Tfm1()],[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]], n_inp=1)\ntst = test_set(dsets, [1,2,3])\ntest_eq(tst, [(2,),(4,),(6,)])\n```\n\n----------------------------------------\n\nTITLE: Sorting and Analyzing PCA Components\nDESCRIPTION: These code snippets sort the movies by their scores on the first and second principal components. The first snippet sorts by the first component in reverse order. The second sorts by the first component in ascending order. The third and fourth do the same for the second component. These are used to analyze the learned latent factors.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nsorted(movie_comp, key=itemgetter(0), reverse=True)[:10]\nsorted(movie_comp, key=itemgetter(0))[:10]\n\nmovie_comp = [(f, i) for f,i in zip(fac1, top_movies)]\n\nsorted(movie_comp, key=itemgetter(0), reverse=True)[:10]\nsorted(movie_comp, key=itemgetter(0))[:10]\n```\n\n----------------------------------------\n\nTITLE: Installing fastai on Colab\nDESCRIPTION: This shell command checks if the code is running in a Colab environment (by checking for the existence of the `/content` directory) and, if so, upgrades the `fastai` library to the latest version using `pip`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#| eval: false\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Exporting Notebook with nbdev (Python)\nDESCRIPTION: A standard command used within the nbdev workflow to export code cells marked with `#|export` from the notebook into the corresponding Python library file. This snippet is part of the library's build process.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/30_text.core.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Import necessary modules\nDESCRIPTION: This section imports required modules from the fastai library and standard Python libraries.  It imports fastai.torch_basics for fundamental PyTorch utilities, fastdownload for streamlined file downloads, functools for LRU caching, and fastai.data, indicating that this module interacts with the fastai data pipeline.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/04_data.external.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom __future__ import annotations\nfrom fastai.torch_basics import *\nfrom fastdownload import FastDownload\nfrom functools import lru_cache\nimport fastai.data\n```\n\n----------------------------------------\n\nTITLE: Showing a Batch from Custom Dataset\nDESCRIPTION: This displays a batch of images from the custom data loader `dbunch1`. `figsize` sets the size of the plot, and `cmap` specifies the colormap for grayscale images. It allows for visualizing the processed data before training.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\ndbunch1.show_batch(figsize=(8,8), cmap='gray')\n```\n\n----------------------------------------\n\nTITLE: Defining AddMaskCodes Transform (Python)\nDESCRIPTION: Defines the `AddMaskCodes` fastai `Transform`. Its purpose is to attach vocabulary information (class names or codes) to a `TensorMask` during the decoding phase (when converting model output back to interpretable format). It stores the provided `codes` list and its length `c`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass AddMaskCodes(Transform):\n    \"Add the code metadata to a `TensorMask`\"\n    def __init__(self, codes=None):\n        self.codes = codes\n        if codes is not None: self.vocab,self.c = codes,len(codes)\n\n    def decodes(self, o:TensorMask):\n        if self.codes is not None: o.codes=self.codes\n        return o\n```\n\n----------------------------------------\n\nTITLE: Training a Collaborative Filtering Learner - fastai - Python\nDESCRIPTION: This snippet demonstrates how to train the `Learner` created for collaborative filtering using the `fit_one_cycle` method.  It trains the model for one epoch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/45_collab.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(1)\n```\n\n----------------------------------------\n\nTITLE: Exporting nbdev module\nDESCRIPTION: This code snippet uses `nbdev_export` to export the contents of the current notebook. The code utilizes nbdev to export the current notebook by calling the `nbdev_export()` function.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Defining BCEWithLogitsLossFlat in Fastai\nDESCRIPTION: This code defines the `BCEWithLogitsLossFlat` class, which inherits from `BaseLoss` and provides a flattened version of `nn.BCEWithLogitsLoss`. It includes methods for decoding the model output (thresholding) and applying the sigmoid activation function.  It uses `use_kwargs_dict` to pass arguments to the underlying `nn.BCEWithLogitsLoss` and sets `is_2d=False` during the call to `super().__init__`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@delegates()\nclass BCEWithLogitsLossFlat(BaseLoss):\n    \"Same as `nn.BCEWithLogitsLoss`, but flattens input and target.\"\n    @use_kwargs_dict(keep=True, weight=None, reduction='mean', pos_weight=None)\n    def __init__(self, \n        *args, \n        axis:int=-1, # Class axis\n        floatify:bool=True, # Convert `targ` to `float`\n        thresh:float=0.5, # The threshold on which to predict \n        **kwargs\n    ):\n        if kwargs.get('pos_weight', None) is not None and kwargs.get('flatten', None) is True:\n            raise ValueError(\"`flatten` must be False when using `pos_weight` to avoid a RuntimeError due to shape mismatch\")\n        if kwargs.get('pos_weight', None) is not None: kwargs['flatten'] = False\n        super().__init__(nn.BCEWithLogitsLoss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs)\n        self.thresh = thresh\n\n    def decodes(self, x:Tensor) -> Tensor:\n        \"Converts model output to target format\"\n        return x>self.thresh\n    \n    def activation(self, x:Tensor) -> Tensor:\n        \"`nn.BCEWithLogitsLoss`'s fused activation function applied to model output\"\n        return torch.sigmoid(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing AzureMLCallback class for logging training metrics\nDESCRIPTION: Defines the AzureMLCallback class extending fastai's Callback, enabling automatic logging of training metrics, loss, model summaries, and epoch info to AzureML. Also manages detection of AzureML environment and parent run sharing.\nSOURCE: https://github.com/fastai/fastai/blob/main/disabled/74_callback.azureml.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nclass AzureMLCallback(Callback):\n    \"\"\"\n    Log losses, metrics, model architecture summary to AzureML.\n\n    If `log_offline` is False, will only log if actually running on AzureML.\n    A custom AzureML `Run` class can be passed as `azurerun`.\n    If `log_to_parent` is True, will also log to the parent run, if exists (e.g. in AzureML pipelines).\n    \"\"\"\n    order = Recorder.order+1\n\n    def __init__(self, azurerun=None, log_to_parent=True):\n        if azurerun:\n            self.azurerun = azurerun\n        else:\n            try:\n                self.azurerun = Run.get_context(allow_offline=False)\n            except RunEnvironmentException:\n                # running locally\n                self.azurerun = None\n                warnings.warn(\"Not running on AzureML and no azurerun passed, AzureMLCallback will be disabled.\")\n        self.log_to_parent = log_to_parent\n\n    def before_fit(self):\n        self._log(\"n_epoch\", self.learn.n_epoch)\n        self._log(\"model_class\", str(type(self.learn.model)))\n\n        try:\n            summary_file = Path(\"outputs\") / 'model_summary.txt'\n            with summary_file.open(\"w\") as f:\n                f.write(repr(self.learn.model))\n        except:\n            print('Did not log model summary. Check if your model is PyTorch model.')\n\n    def after_batch(self):\n        # log loss and opt.hypers\n        if self.learn.training:\n            self._log('batch__loss', self.learn.loss.item())\n            self._log('batch__train_iter', self.learn.train_iter)\n            for i, h in enumerate(self.learn.opt.hypers):\n                for k, v in h.items():\n                    self._log(f'batch__opt.hypers.{k}', v)\n\n    def after_epoch(self):\n        # log metrics\n        for n, v in zip(self.learn.recorder.metric_names, self.learn.recorder.log):\n            if n not in ['epoch', 'time']:\n                self._log(f'epoch__{n}', v)\n            if n == 'time':\n                # split elapsed time string, then convert into 'seconds' to log\n                m, s = str(v).split(':')\n                elapsed = int(m)*60 + int(s)\n                self._log(f'epoch__{n}', elapsed)\n\n    def _log(self, metric, value):\n        if self.azurerun is not None:\n            self.azurerun.log(metric, value)\n            if self.log_to_parent and self.azurerun.parent is not None:\n                self.azurerun.parent.log(metric, value)\n```\n\n----------------------------------------\n\nTITLE: Declaring Default Nbdev Settings for Fastai Losses Module (Python)\nDESCRIPTION: This snippet sets nbdev-specific metadata for the current file, declaring it as part of the 'losses' export module and specifying the default class documentation level. This affects the documentation generation process and helps organize exported functions and classes. There are no dependencies, input, or output parameters for these configuration statements.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n#|default_exp losses\n#|default_cls_lvl 3\n```\n\n----------------------------------------\n\nTITLE: Masked Concat Pool\nDESCRIPTION: The `masked_concat_pool` function performs pooling on the output of a sentence encoder, and concatenates the results. It takes the `output` tensor from the encoder, a `mask` tensor (indicating padding), and `bptt`. It computes average pooling, max pooling, and extracts the last hidden state. It concatenates the last hidden state, max pool, and average pool results. This creates a single vector representation from the sequence output, which will then be fed into a classifier head.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef masked_concat_pool(\n    output:Tensor, # Output of sentence encoder\n    mask:Tensor, # Boolean mask as returned by sentence encoder\n    bptt:int # Backpropagation through time\n) -> Tensor: # Concatenation of [last_hidden, max_pool, avg_pool]\n    \"Pool `MultiBatchEncoder` outputs into one vector [last_hidden, max_pool, avg_pool]\"\n    lens = output.shape[1] - mask.long().sum(dim=1)\n    last_lens = mask[:,-bptt:].long().sum(dim=1)\n    avg_pool = output.masked_fill(mask[:, :, None], 0).sum(dim=1)\n    avg_pool.div_(lens.type(avg_pool.dtype)[:,None])\n    max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n    x = torch.cat([output[torch.arange(0, output.size(0)),-last_lens-1], max_pool, avg_pool], 1) #Concat pooling.\n    return x\n```\n\n----------------------------------------\n\nTITLE: Importing comet_ml\nDESCRIPTION: This imports the `comet_ml` library which is required for interacting with the Comet.ml platform.  This allows for creating experiments, logging metrics, and saving model artifacts. This is a core dependency for the functionality of the module.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70d_callback.comet.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport comet_ml\n```\n\n----------------------------------------\n\nTITLE: dummy_eval Function Definition\nDESCRIPTION: This code defines the `dummy_eval` function, which evaluates a given model `m` on a dummy input of a specified `size`.  It creates a random tensor of the appropriate size and channel count, sets `requires_grad_(False)` to prevent gradient calculation, and then performs a forward pass of the model in evaluation mode (`m.eval()`) with no gradient calculation (`torch.no_grad()`). It returns the output of the model.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef dummy_eval(m, size=(64,64)):\n    \"Evaluate `m` on a dummy input of a certain `size`\"\n    ch_in = in_channels(m)\n    x = one_param(m).new(1, ch_in, *size).requires_grad_(False).uniform_(-1.,1.)\n    with torch.no_grad(): return m.eval()(x)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom fastai Transform for HuggingFace Tokenizer (Python)\nDESCRIPTION: Defines a custom fastai Transform class (TransformersTokenizer) encapsulating a HuggingFace tokenizer. The encodes method tokenizes and converts a text string to token IDs (using PyTorch tensor); the decodes method converts tokens back to a human-readable string (with TitledStr for fastai display compatibility). Prerequisites: a compatible HuggingFace tokenizer instance. Used in fastai data block pipelines for lazy tokenization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass TransformersTokenizer(Transform):\n    def __init__(self, tokenizer): self.tokenizer = tokenizer\n    def encodes(self, x): \n        toks = self.tokenizer.tokenize(x)\n        return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))\n```\n\n----------------------------------------\n\nTITLE: Showing LMTensorText Batch - Python\nDESCRIPTION: This function implements `show_batch` for `LMTensorText` objects, specifically tailored for Language Model tasks. It truncates both input and target text sequences to the specified `trunc_at` length, ensuring that long texts are shortened for readability within the display. The function uses `get_show_batch_func(TensorText)` to reuse logic applicable to `TensorText` and returns its output.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@dispatch\ndef show_batch(x: LMTensorText, y, samples, ctxs=None, max_n=10, trunc_at=150, **kwargs):\n    samples = L((s[0].truncate(trunc_at), s[1].truncate(trunc_at)) for s in samples)\n    return get_show_batch_func(TensorText)(x, None, samples, ctxs=ctxs, max_n=max_n, trunc_at=None, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Extracting Data for Other Libraries\nDESCRIPTION: Extracts the processed training and validation data (features `xs` and flattened labels `ys`) from the `TabularPandas` object into separate variables. This prepares the data in a format suitable for use with machine learning libraries like Scikit-learn, XGBoost, or Random Forests.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nX_train, y_train = to.train.xs, to.train.ys.values.ravel()\nX_test, y_test = to.valid.xs, to.valid.ys.values.ravel()\n```\n\n----------------------------------------\n\nTITLE: Load and Concatenate DataFrames\nDESCRIPTION: This code reads the training and validation CSV files from the Wikitext dataset using pandas and concatenates them into a single DataFrame. It assumes the CSV files have no header.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/35_tutorial.wikitext.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf_train = pd.read_csv(path/'train.csv', header=None)\ndf_valid = pd.read_csv(path/'test.csv', header=None)\ndf_all = pd.concat([df_train, df_valid])\n```\n\n----------------------------------------\n\nTITLE: Load Adult Sample Dataset\nDESCRIPTION: This code snippet downloads and extracts the adult sample dataset, then reads it into a Pandas DataFrame.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/41_tabular.data.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.ADULT_SAMPLE)\ndf = pd.read_csv(path/'adult.csv', skipinitialspace=True)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Creating a Test DataLoader with Validation Transforms (`test_dl` Patch) \nDESCRIPTION: Patched method: Creates a test dataloader from test items, applying validation transforms of the dataset if applicable, otherwise returns test items directly. Uses the `valid` DataLoader for batch creation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_72\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n@patch\n@delegates(TfmdDL.__init__)\ndef test_dl(self:DataLoaders, \n    test_items, # Items in test dataset\n    rm_type_tfms=None, # Start index of `Transform`(s) from validation set in `dsets` to apply\n    with_labels:bool=False, # Whether the test items contain labels\n    **kwargs\n):\n    \"Create a test dataloader from `test_items` using validation transforms of `dsets`\"  \n    test_ds = test_set(self.valid_ds, test_items, rm_tfms=rm_type_tfms, with_labels=with_labels\n                      ) if isinstance(self.valid_ds, (Datasets, TfmdLists)) else test_items\n    return self.valid.new(test_ds, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders\nDESCRIPTION: This creates data loaders (`dls`) from the datasets, applying the defined transformations `tfms`, and other processing steps (`after_item`, `after_batch`) with a specified batch size `bs`.  Data loaders provide an efficient way to iterate over the dataset during training, applying the transformations and batching the data.  `IntToFloatTensor` converts integer tensor to float. `Normalize` normalizes the data.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ndls = dsets.dataloaders(bs=bs, after_item=tfms, after_batch=[IntToFloatTensor, Normalize])\n```\n\n----------------------------------------\n\nTITLE: Training the Refactored Model\nDESCRIPTION: This trains the refactored CNN model for 10 epochs using `fit_one_cycle`. This is similar to the training of the previous models.  It uses `dls`, the preprocessed dataset and the loss and accuracy metrics.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\nlearn.fit_one_cycle(10, lr_max=0.1)\n```\n\n----------------------------------------\n\nTITLE: Testing Gradient Accumulation Behavior\nDESCRIPTION: Validates that gradient accumulation maintains consistent gradients and loss values across varying batch sizes and accumulation steps using a custom test function.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18a_callback.training.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\nclass GetGrads(Callback):\n    run_valid,order = False,GradientAccumulation.order+1\n    def before_step(self): self.grads=to_detach(L([p.grad.clone() for p in self.model.parameters()]))\n\ndef _test_acc(bs,n,cbs=None,cuda=False):\n    with no_random(99): \n        db=synth_dbunch(bs=bs,n_train=n,n_valid=n,cuda=cuda)\n        learn = synth_learner(data=db,cbs=[GetGrads]+L(cbs))\n        learn.fit(1, lr=0.01)\n        train,valid = learn.recorder.values[-1]\n        return train,valid,learn.get_grads.grads\n\nacc_cb = GradientAccumulation(n_acc=8)\n\ntrain1,valid1,grads1 = _test_acc(8,1)\ntrain2,valid2,grads2 = _test_acc(1,8,acc_cb)\n\n#grads should be same, valid loss same, train loss different\ntest_close(grads2,grads1)\ntest_close(valid2,valid1)\ntest_ne(train2, train1)\n```\n\n----------------------------------------\n\nTITLE: Importing nbdev.showdoc for Documentation Rendering - Python\nDESCRIPTION: Imports showdoc utilities from nbdev, enabling generation and rendering of documentation for classes and functions directly from code. Dependency: nbdev.showdoc. No input parameters; this supports automatic documentation rendering but is not required for model training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Ensuring `torch.as_subclass` Compatibility in fastai\nDESCRIPTION: Provides backward compatibility for different PyTorch versions. It checks if the top-level `torch.as_subclass` function exists. If not (indicating an older PyTorch version), it assigns the method `torch.Tensor.as_subclass` to `torch.as_subclass`, ensuring that the subsequent patch to `Tensor.as_subclass` can reliably call `torch.as_subclass`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\n#|export\nif not hasattr(torch,'as_subclass'): torch.as_subclass = torch.Tensor.as_subclass\n```\n\n----------------------------------------\n\nTITLE: Function to Accumulate Embedding Features During Projection\nDESCRIPTION: Updates stored feature vectors and images with new hooks during embedding extraction, normalizing images. Stores labels if vocab is available.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n def _add_projector_features(learn, hook, feat):\n    img = _normalize_for_projector(learn.x)\n    first_epoch = True if learn.iter == 0 else False\n    feat['vec'] = hook.stored if first_epoch else torch.cat((feat['vec'], hook.stored),0)\n    feat['img'] = img           if first_epoch else torch.cat((feat['img'], img),0)\n    if getattr(learn.dl, 'vocab', None):\n        feat['lbl'] = learn.y if first_epoch else torch.cat((feat['lbl'], learn.y),0)\n    return feat\n```\n\n----------------------------------------\n\nTITLE: Save trained model\nDESCRIPTION: Saves the trained model to a file named 'dotprod'. This allows for later loading and reuse of the trained model without retraining.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nlearn.save('dotprod')\n```\n\n----------------------------------------\n\nTITLE: Gather tensors involved in distributed training (Python)\nDESCRIPTION: The `maybe_gather` function collects copies of a tensor (`x`) across all distributed processes if parallelism is active, concatenating results along the specified axis. If only one process is used, it returns the tensor unchanged. It facilitates synchronized data across multiple training nodes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\n#|export\ndef maybe_gather(x, axis=0):\n    \"Gather copies of `x` on `axis` (if training is distributed)\"\n    if num_distrib()<=1: return x\n    ndim = x.ndim\n    res = [x.new_zeros(*x.shape if ndim > 0 else (1,)) for _ in range(num_distrib())]\n    torch.distributed.all_gather(res, x.contiguous() if ndim > 0 else x[None])\n    return torch.cat(res, dim=axis) if ndim > 0 else torch.cat(res, dim=axis).mean()\n```\n\n----------------------------------------\n\nTITLE: Defining a Logistic Regression Model - PyTorch/Python\nDESCRIPTION: Defines a simple logistic regression model as a PyTorch `Module`. The model consists of a single linear layer (`nn.Linear`) that maps the 784 input features (28x28 pixels) to 10 output classes.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass Mnist_Logistic(Module):\n    def __init__(self): self.lin = nn.Linear(784, 10, bias=True)\n    def forward(self, xb): return self.lin(xb)\n```\n\n----------------------------------------\n\nTITLE: Defining a Base Metric Class (Python)\nDESCRIPTION: This snippet defines an abstract base class `Metric` in fastai. It outlines the basic structure required for creating custom metrics, including `reset`, `accumulate`, and `value` methods. The `reset` method is for resetting inner state, `accumulate` for updating the state with new results, and `value` for retrieving the metric's value.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_60\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@docs\nclass Metric():\n    \"Blueprint for defining a metric\"\n    def reset(self): pass\n    def accumulate(self, learn): pass\n    @property\n    def value(self): raise NotImplementedError\n\n    @property\n    def name(self): return class2attr(self, 'Metric')\n\n    _docs = dict(\n        reset=\"Reset inner state to prepare for new computation\",\n        name=\"Name of the `Metric`, camel-cased and with Metric removed\",\n        accumulate=\"Use `learn` to update the state with new results\",\n        value=\"The value of the metric\")\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Metric, title_level=3)\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Metric.reset)\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Metric.accumulate)\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Metric.value, name='Metric.value')\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Metric.name, name='Metric.name')\n```\n\n----------------------------------------\n\nTITLE: Example Usage of FillMissing\nDESCRIPTION: This code provides an example of how to use the `FillMissing` processor with different fill strategies (median, constant, mode). It creates a `cudf.DataFrame` with missing values, applies `FillMissing` with each strategy, and verifies the results. It tests for the correct fill value and presence of a missingness indicator column.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfill1,fill2,fill3 = (FillMissing(fill_strategy=s) \n                     for s in [FillStrategy.median, FillStrategy.constant, FillStrategy.mode])\ndf = cudf.from_pandas(pd.DataFrame({'a':[0,1,np.nan,1,2,3,4]}))\ndf1 = df.copy(); df2 = df.copy()\ntos = TabularGPU(df, fill1, cont_names='a'),TabularGPU(df1, fill2, cont_names='a'),TabularGPU(df2, fill3, cont_names='a')\n\ntest_eq(fill1.na_dict, {'a': 1.5})\ntest_eq(fill2.na_dict, {'a': 0})\ntest_eq(fill3.na_dict, {'a': 1.0})\n\nfor t in tos: test_eq(t.cat_names, ['a_na'])\n\nfor to_,v in zip(tos, [1.5, 0., 1.]):\n    test_eq(to_.a.to_array(), np.array([0, 1, v, 1, 2, 3, 4]))\n    test_eq(to_.a_na.to_array(), np.array([0, 0, 1, 0, 0, 0, 0]))\n```\n\n----------------------------------------\n\nTITLE: Resize Images Function\nDESCRIPTION: This function resizes multiple images within a given directory recursively. It iterates through all image files found within a directory structure, and for each image, it calls the `resize_image` function. The function uses parallel processing and provides parameters for controlling maximum workers, maximum size, image format, and other resize options. It supports resuming operations by checking the existing output files and skip resizing for them.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ndef resize_images(path, max_workers=defaults.cpus, max_size=None, recurse=False,\n                  dest=Path('.'), n_channels=3, ext=None, img_format=None, resample=BILINEAR,\n                  resume=None, **kwargs):\n    \"Resize files on path recursively to dest to max_size\"\n    path = Path(path)\n    if resume is None and dest != Path('.'): resume=False\n    os.makedirs(dest, exist_ok=True)\n    files = get_image_files(path, recurse=recurse)\n    files = [o.relative_to(path) for o in files]\n    parallel(resize_image, files, src=path, n_workers=max_workers, max_size=max_size, dest=dest, n_channels=n_channels, ext=ext,\n                   img_format=img_format, resample=resample, resume=resume, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Removing Zero Sales Records\nDESCRIPTION: Filters out rows with zero sales from the training data, which correspond to store closures.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ntrain_df = train_df[train_df.Sales != 0.]\n```\n\n----------------------------------------\n\nTITLE: Custom Sequential Module with Input Access in Python\nDESCRIPTION: This class provides a sequential container similar to `nn.Sequential` but built upon `nn.ModuleList`. It specifically adds functionality to temporarily store the original input (`x`) to the sequence in an attribute `orig` on the current result (`res`) before passing it to the next layer, enabling layers within the sequence to access the input of the *entire* `SequentialEx` block. It also includes methods (`__getitem__`, `append`, `extend`, `insert`) similar to `nn.ModuleList` for managing layers.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass SequentialEx(Module):\n    \"Like `nn.Sequential`, but with ModuleList semantics, and can access module input\"\n    def __init__(self, *layers): self.layers = nn.ModuleList(layers)\n\n    def forward(self, x):\n        res = x\n        for l in self.layers:\n            res.orig = x\n            nres = l(res)\n            # We have to remove res.orig to avoid hanging refs and therefore memory leaks\n            res.orig, nres.orig = None, None\n            res = nres\n        return res\n\n    def __getitem__(self,i): return self.layers[i]\n    def append(self,l):      return self.layers.append(l)\n    def extend(self,l):      return self.layers.extend(l)\n    def insert(self,i,l):    return self.layers.insert(i,l)\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Schedulers into a Composite Schedule in Python\nDESCRIPTION: Defines the combine_scheds function that takes a list of percentages (pcts) summing to 1 and a list of scheduling functions (scheds). It returns a single schedule function that applies scheds[i] in the subinterval defined by pcts. It divides [0,1] according to pcts and normalizes the position parameter to compute appropriate schedule output, enabling multi-phase scheduling.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef combine_scheds(pcts, scheds):\n    \"Combine `scheds` according to `pcts` in one function\"\n    assert sum(pcts) == 1.\n    pcts = tensor([0] + L(pcts))\n    assert torch.all(pcts >= 0)\n    pcts = torch.cumsum(pcts, 0)\n    pct_lim = len(pcts) - 2\n    def _inner(pos):\n        idx = min((pos >= pcts).nonzero().max(), pct_lim)\n        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])\n        return scheds[idx](actual_pos.item())\n    return _inner\n```\n\n----------------------------------------\n\nTITLE: Implementing Jaccard Coefficient (IoU) for Binary Segmentation in PyTorch\nDESCRIPTION: Class that implements the Jaccard coefficient (Intersection over Union) for binary segmentation tasks, extending the Dice class with a different calculation formula.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nclass JaccardCoeff(Dice):\n    \"Implementation of the Jaccard coefficient that is lighter in RAM\"\n    @property\n    def value(self): return self.inter/(self.union-self.inter) if self.union > 0 else None\n```\n\n----------------------------------------\n\nTITLE: Preprocessing All Texts Using HuggingFace Tokenizer and progress_bar (Python)\nDESCRIPTION: Defines a standalone function (tokenize) to batch-tokenize all texts using the HuggingFace tokenizer, then processes all_texts with a visual progress bar. Outputs a list of PyTorch tensors representing tokenized input sequences. Requires the fastprogress and HuggingFace tokenizer libraries.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef tokenize(text):\n    toks = tokenizer.tokenize(text)\n    return tensor(tokenizer.convert_tokens_to_ids(toks))\n\ntokenized = [tokenize(t) for t in progress_bar(all_texts)]\n```\n\n----------------------------------------\n\nTITLE: Rendering SkipItemException Documentation - Python\nDESCRIPTION: Displays documentation for the SkipItemException class using nbdev's show_doc utility. Intended for automated documentation generation in nbdev pipelines.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(SkipItemException, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Exporting Code using nbdev in Python\nDESCRIPTION: Imports necessary components from the `nbdev` library and invokes `nbdev_export()`. This function processes the notebook markdown and code cells to generate standard Python source files and documentation, as part of the `nbdev` workflow for software development in notebooks. Requires the `nbdev` library.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev import *\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Parsing PyTorch Version for Compatibility Checks in fastai\nDESCRIPTION: Parses the currently installed PyTorch version string (`torch.__version__`) using `packaging.version.parse`. It stores the parsed version and specific parsed version objects (`_torch_20`, `_torch_113`, `_torch_112`) as global variables. These are used elsewhere in the fastai library for conditional logic based on the PyTorch version.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n#|exporti\n# Parsed PyTorch versions for faster version checking\n_torch_version = parse(torch.__version__)\n_torch_20  = parse('2.0')\n_torch_113 = parse('1.13')\n_torch_112 = parse('1.12')\n```\n\n----------------------------------------\n\nTITLE: Creating Tabular Learner Model - Python\nDESCRIPTION: Initializes a fastai tabular learner model with specified dataloaders, hidden layer sizes, loss function (Mean Squared Error), model configuration (dropout probabilities, embedding dropout, output range), and evaluation metric (Root Mean Squared Percentage Error on the exponentiated prediction).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nlearn = tabular_learner(dls, layers=[1000,500], loss_func=MSELossFlat(),\n                        config=tabular_config(ps=[0.001,0.01], embed_p=0.04, y_range=y_range), \n                        metrics=exp_rmspe)\n```\n\n----------------------------------------\n\nTITLE: Processing TensorImage Data for Wandb Logging\nDESCRIPTION: This `wandb_process` function processes `TensorImage` data for logging to Weights & Biases (W&B). It iterates through the samples and outputs, creating W&B images for the input, prediction, and ground truth. It utilizes the `_make_plt` function to create Matplotlib figures for overlaying labels and predictions on the input image.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n@dispatch\ndef wandb_process(x:TensorImage, y, samples, outs, preds):\n    \"Process `sample` and `out` depending on the type of `x/y`\"\n    res_input, res_pred, res_label = [],[],[]\n    for s,o in zip(samples, outs):\n        img = s[0].permute(1,2,0)\n        res_input.append(wandb.Image(img, caption='Input_data'))\n        for t, capt, res in ((o[0], \"Prediction\", res_pred), (s[1], \"Ground_Truth\", res_label)):\n            fig, ax = _make_plt(img)\n            # Superimpose label or prediction to input image\n            ax = img.show(ctx=ax)\n            ax = t.show(ctx=ax)\n            res.append(wandb.Image(fig, caption=capt))\n            plt.close(fig)\n    return {\"Inputs\":res_input, \"Predictions\":res_pred, \"Ground_Truth\":res_label}\n```\n\n----------------------------------------\n\nTITLE: ArrayMask Subclass Representing Image Masks with Transparency in Python\nDESCRIPTION: The `ArrayMask` class extends `ArrayImageBase` and defines default visualization arguments for image masks. It sets a semi-transparent alpha, a categorical colormap (`tab20`), and nearest-neighbor interpolation for clear class boundaries when displayed. This class facilitates visualization of masks commonly used in segmentation tasks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass ArrayMask(ArrayImageBase):\n    \"An array representing an image mask\"\n    _show_args = {'alpha':0.5, 'cmap':'tab20', 'interpolation':'nearest'}\n```\n\n----------------------------------------\n\nTITLE: Saving Language Model State After Initial Finetuning (Python)\nDESCRIPTION: Saves the trained model checkpoint to disk for future reuse or staged training. Input: pre-trained learner; Output: serialized model files named 'stage1'.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlearn.save('stage1')\n```\n\n----------------------------------------\n\nTITLE: Installing nbdev Git Hooks (Bash)\nDESCRIPTION: Executes the nbdev utility command to install Git hooks in the repository. These hooks automate notebook cleaning to minimize merge conflicts by removing execution metadata.\nSOURCE: https://github.com/fastai/fastai/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nnbdev_install_hooks\n```\n\n----------------------------------------\n\nTITLE: Learner Summary Method for Model Details\nDESCRIPTION: A method added to the fastai Learner class that prints a comprehensive summary of the model architecture, optimizer, loss function, frozen layers, and attached callbacks. Provides a complete overview of the learning setup.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_40\n\nLANGUAGE: Python\nCODE:\n```\n@patch\ndef summary(self:Learner):\n    \"Print a summary of the model, optimizer and loss function.\"\n    xb = self.dls.train.one_batch()[:getattr(self.dls.train, \"n_inp\", 1)]\n    res = module_summary(self, *xb)\n    res += f\"Optimizer used: {self.opt_func}\\nLoss function: {self.loss_func}\\n\\n\"\n    if self.opt is not None:\n        res += f\"Model \" + (\"unfrozen\\n\\n\" if self.opt.frozen_idx==0 else f\"frozen up to parameter group #{self.opt.frozen_idx}\\n\\n\")\n    res += \"Callbacks:\\n\" + '\\n'.join(f\"  - {cb}\" for cb in self.cbs.sorted('order'))\n    return PrettyString(res)\n```\n\n----------------------------------------\n\nTITLE: Combine movie bias and mean ratings\nDESCRIPTION: Calculates the mean rating for each movie and combines the movie biases with the corresponding movie titles and mean ratings into a list of tuples. This allows for comparing the learned biases with the actual average ratings.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nmean_ratings = rating_movie.groupby('title')['rating'].mean()\nmovie_ratings = [(b, i, mean_ratings.loc[i]) for i,b in zip(top_movies,movie_bias)]\n```\n\n----------------------------------------\n\nTITLE: Freeze to Third Last Layer Group\nDESCRIPTION: Freezes all layers except the last three for the next stage of training.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3),moms=(0.8,0.7, 0.8))\n```\n\n----------------------------------------\n\nTITLE: Installing dependencies for fastai and nbdev\nDESCRIPTION: Installs necessary packages for fastai and nbdev using conda, ensuring the latest versions are fetched from specified channels. Adjustments are made for miniconda users.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/dev-setup.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nconda install -y -c fastai -c pytorch -c anaconda anaconda fastai nbdev\n```\n\n----------------------------------------\n\nTITLE: Preparing Tabular Data for fastai - Python\nDESCRIPTION: This snippet creates a `TabularPandas` object, which prepares the data for the fastai model. It applies the specified preprocessing steps to the data. The `cat_names` and `cont_names` parameters specify the categorical and continuous columns, while `y_names` specifies the target variable.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-tabular.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nto = TabularPandas(df, procs, cat_names, cont_names, y_names=\"salary\", splits=splits)\n```\n\n----------------------------------------\n\nTITLE: Extract PCA components\nDESCRIPTION: Extracts the first three principal components from the PCA result. These components are then used to analyze and visualize movie features.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nfac0,fac1,fac2 = movie_pca.t()\nmovie_comp = [(f, i) for f,i in zip(fac0, top_movies)]\n```\n\n----------------------------------------\n\nTITLE: Importing TensorDataset - PyTorch/Python\nDESCRIPTION: Imports the `TensorDataset` class from PyTorch's `utils.data` module. `TensorDataset` is used to wrap input and target tensors, providing a standard interface for datasets.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.data import TensorDataset\n```\n\n----------------------------------------\n\nTITLE: Example Usage of SequentialEx and MergeLayer in Python\nDESCRIPTION: This code demonstrates how to use `SequentialEx` and `MergeLayer` to build a simple residual block. It initializes a `SequentialEx` block with two convolutional layers and then appends a `MergeLayer`. It tests the forward pass with random input and verifies the output shape and value against the expected manual calculation, confirming the addition behavior of `MergeLayer`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_59\n\nLANGUAGE: python\nCODE:\n```\nres_block = SequentialEx(ConvLayer(16, 16), ConvLayer(16,16))\nres_block.append(MergeLayer()) # just to test append - normally it would be in init params\nx = torch.randn(32, 16, 8, 8)\ny = res_block(x)\ntest_eq(y.shape, [32, 16, 8, 8])\ntest_eq(y, x + res_block[1](res_block[0](x)))\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export Module for nbdev - Python\nDESCRIPTION: Marks the notebook cell's default export module for nbdev with the module path 'data.load'. This is meta-instructional and typically used in fastai/nbdev-driven projects.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp data.load\n```\n\n----------------------------------------\n\nTITLE: Defining Identity Layer using fastai module Decorator - Python\nDESCRIPTION: Implements an Identity layer, returning input unchanged. Uses the previously defined @module() decorator to generate a PyTorch nn.Module. No additional dependencies; takes a single input x and returns x.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@module()\ndef Identity(self, x):\n    \"Do nothing at all\"\n    return x\n```\n\n----------------------------------------\n\nTITLE: Importing nbdev showdoc\nDESCRIPTION: Imports the `showdoc` function from the `nbdev.showdoc` module. The `showdoc` function is used within nbdev to generate documentation for Python objects.  This import suggests that the file might include interactive documentation generation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70d_callback.comet.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Creating and Training Learner\nDESCRIPTION: This code creates and trains a collaborative filtering model using fastai's `collab_learner`.  It creates a model with 50 latent factors and sets the range for the target variable (ratings). It then fits the model for 5 epochs with a learning rate of 5e-3 and weight decay of 0.1 using the 1cycle policy.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n```\n\n----------------------------------------\n\nTITLE: Module Summary Function for PyTorch Models\nDESCRIPTION: Function that generates a detailed text summary of a PyTorch model, including layer types, output shapes, parameter counts, and trainability status. The summary is formatted in a table-like structure.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_39\n\nLANGUAGE: Python\nCODE:\n```\ndef module_summary(learn, *xb):\n    \"Print a summary of `model` using `xb`\"\n    #Individual parameters wrapped in ParameterModule aren't called through the hooks in `layer_info`,\n    #  thus are not counted inside the summary\n    #TODO: find a way to have them counted in param number somehow\n    infos = layer_info(learn, *xb)\n    n,bs = 76,find_bs(xb)\n    inp_sz = _print_shapes(apply(lambda x:x.shape, xb), bs)\n    res = f\"{type(learn.model).__name__} (Input shape: {inp_sz})\\n\"\n    res += \"=\" * n + \"\\n\"\n    res += f\"{'Layer (type)':<20} {'Output Shape':<20} {'Param #':<10} {'Trainable':<10}\\n\"\n    res += \"=\" * n\n    ps,trn_ps,j = 0,0,0\n    infos = [o for o in infos if o is not None] #see comment in previous cell\n    prev_sz = None\n    for typ,np,trn,sz,chnged in infos:\n        if sz is None: continue\n        if j == 0:\n            res += f'\\n{\"\"}:<20} {_print_shapes(sz, bs)[:19]:<20}' # to avoid a double line at the top\n        if not chnged and not prev_sz == sz and j > 0: res += \"\\n\" + \"_\" * n + \"\\n\" + f'{\"\"}:<20} {_print_shapes(sz, bs)[:19]:<20}'\n        j = 1\n        res += f\"\\n{typ:<20} {'':<20} {np:<10} {str(trn):<10}\"\n        if np != '':\n            ps += np\n            if trn: trn_ps += np\n        prev_sz = sz\n    res += \"\\n\" + \"_\" * n + \"\\n\"\n    res += f\"\\nTotal params: {ps:,}\\n\"\n    res += f\"Total trainable params: {trn_ps:,}\\n\"\n    res += f\"Total non-trainable params: {ps - trn_ps:,}\\n\\n\"\n    return PrettyString(res)\n```\n\n----------------------------------------\n\nTITLE: Extracting Text Vocabulary from DataLoaders - Python\nDESCRIPTION: Defines _get_text_vocab to extract the vocabulary used from Fastai DataLoaders. Takes a DataLoaders instance, handles possible types (L or list), and returns the base vocabulary. Dependency: fastai DataLoaders. Input: dls (DataLoaders). Output: list vocabulary. Limitation: expects fastai's structure for vocab.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef _get_text_vocab(dls:DataLoaders) -> list:\n    \"Get vocabulary from `DataLoaders`\"\n    vocab = dls.vocab\n    if isinstance(vocab, L): vocab = vocab[0]\n    return vocab\n```\n\n----------------------------------------\n\nTITLE: Defining the ImageClassifierCleaner Widget\nDESCRIPTION: Defines the `ImageClassifierCleaner` class, which integrates `ImagesCleaner` with a fastai CNN `Learner`. It provides dropdowns to select the dataset (Train/Valid) and category, displaying images sorted by loss for review and cleaning. It automatically fetches image paths, targets, and losses using `_get_iw_info`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09c_vision.widgets.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@delegates(ImagesCleaner)\nclass ImageClassifierCleaner(GetAttr):\n    \"A widget that provides an `ImagesCleaner` for a CNN `Learner`\"\n    def __init__(self, learn, **kwargs):\n        vocab = learn.dls.vocab\n        self.default = self.iw = ImagesCleaner(vocab, **kwargs)\n        self.dd_cats = Dropdown(options=vocab)\n        self.dd_ds   = Dropdown(options=('Train','Valid'))\n        self.iwis = _get_iw_info(learn,0),_get_iw_info(learn,1)\n        self.dd_ds.observe(self.on_change_ds, 'value')\n        self.dd_cats.observe(self.on_change_ds, 'value')\n        self.on_change_ds()\n        self.widget = VBox([self.dd_cats, self.dd_ds, self.iw.widget])\n\n    def _ipython_display_(self): display(self.widget)\n    def on_change_ds(self,change=None):\n        \"Toggle between training validation set view\"\n        info = L(o for o in self.iwis[self.dd_ds.index] if o[1]==self.dd_cats.value)\n        self.iw.set_fns(info.sorted(2, reverse=True).itemgot(0))\n```\n\n----------------------------------------\n\nTITLE: InstanceNorm Layer Creation\nDESCRIPTION: This function, `InstanceNorm`, creates an InstanceNorm layer with a specified number of features (`nf`) and dimension (`ndim`). It uses the `_get_norm` function internally. The `norm_type` parameter determines whether the weights are initialized to zero. The `affine` parameter determines whether to use affine parameters. The `delegates` decorator forwards arguments to `nn.InstanceNorm2d`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@delegates(nn.InstanceNorm2d)\ndef InstanceNorm(nf, ndim=2, norm_type=NormType.Instance, affine=True, **kwargs):\n    \"InstanceNorm layer with `nf` features and `ndim` initialized depending on `norm_type`.\"\n    return _get_norm('InstanceNorm', nf, ndim, zero=norm_type==NormType.InstanceZero, affine=affine, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Concatenate Layer Outputs in Python\nDESCRIPTION: This class, inheriting from `nn.ModuleList`, applies a list of layers sequentially to the same input tensor (`x`) and then concatenates their outputs along a specified dimension (`dim`, defaulting to 1). This is equivalent to Keras's `Concatenate` layer and is useful for architectures that merge features from parallel paths or different levels.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_61\n\nLANGUAGE: python\nCODE:\n```\n#|export \nclass Cat(nn.ModuleList):\n    \"Concatenate layers outputs over a given dim\"\n    def __init__(self, layers, dim=1):\n        self.dim=dim\n        super().__init__(layers)\n    def forward(self, x): return torch.cat([l(x) for l in self], dim=self.dim)\n```\n\n----------------------------------------\n\nTITLE: Create Language Model DataLoaders from DataFrame\nDESCRIPTION: Creates a `TextDataLoaders` object for language model training directly from a Pandas DataFrame. It specifies the text and label columns, path, and whether it's for a language model (is_lm=True). `valid_col` indicates which column specifies the validation set.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndbunch_lm = TextDataLoaders.from_df(df, text_col='text', label_col='label', path=path, is_lm=True, valid_col='is_valid')\n```\n\n----------------------------------------\n\nTITLE: Creating Custom File Getter Functions\nDESCRIPTION: Function factory that creates a partial function for file retrieval with predefined parameters. Allows customizing suffix paths, extensions, recursion behavior, and folder restrictions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef FileGetter(suf='', extensions=None, recurse=True, folders=None):\n    \"Create `get_files` partial function that searches path suffix `suf`, only in `folders`, if specified, and passes along args\"\n    def _inner(o, extensions=extensions, recurse=recurse, folders=folders):\n        return get_files(o/suf, extensions, recurse, folders)\n    return _inner\n```\n\n----------------------------------------\n\nTITLE: List Training Data Directory\nDESCRIPTION: Lists the contents of the 'train' directory within the IMDB dataset. This displays the structure of the training data (positive and negative reviews).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n(path/'train').ls()\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-label Accuracy Function in Python\nDESCRIPTION: A function to compute accuracy for multi-label classification when input and target tensors have the same size. It supports optional sigmoid activation and customizable threshold value.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n    \"Compute accuracy when `inp` and `targ` are the same size.\"\n    inp,targ = flatten_check(inp,targ)\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp>thresh)==targ.bool()).float().mean()\n```\n\n----------------------------------------\n\nTITLE: Testing SelfAttention Layer (Training)\nDESCRIPTION: Tests the `SelfAttention` layer's behavior during training by setting the `gamma` parameter to 1 and verifying the output shape.  It's a basic sanity check to confirm the forward pass works as expected with a non-zero `gamma`. Dependencies: `torch`, `fastai` (for Module, ConvLayer, etc.).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_44\n\nLANGUAGE: Python\nCODE:\n```\ntst.gamma.data.fill_(1.)\ny = tst(x)\ntest_eq(y.shape, [32,16,8,8])\n```\n\n----------------------------------------\n\nTITLE: Importing Core Dependencies in Python\nDESCRIPTION: Imports necessary modules for the Neptune callback implementation. It includes `annotations` for type hinting, `tempfile` for temporary file handling, core fastai basics (`fastai.basics`), and the base `Callback` class from `fastai.learner`.\nSOURCE: https://github.com/fastai/fastai/blob/main/disabled/70b_callback.neptune.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nimport tempfile\nfrom fastai.basics import *\nfrom fastai.learner import Callback\n```\n\n----------------------------------------\n\nTITLE: Visualizing Batches from LM DataLoader (Python)\nDESCRIPTION: Displays a sample batch from the language model DataLoader. Useful for verifying data preprocessing. Takes no input and outputs a visualization in Jupyter-compatible environments.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndbunch_lm.show_batch()\n```\n\n----------------------------------------\n\nTITLE: Merging Ratings and Movies\nDESCRIPTION: This merges the ratings and movies DataFrames based on the 'movie' column. The resulting DataFrame will have the user ratings and movie titles in a single structure.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nratings = ratings.merge(movies)\n```\n\n----------------------------------------\n\nTITLE: Creating Empty Dataset for DataFrame\nDESCRIPTION: Creates a dataset from a pandas DataFrame with specific attribute getters, then generates an empty dataset to validate the method works with dataframes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_66\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n#test it works for dataframes too\n df = pd.DataFrame({'a':[1,2,3,4,5], 'b':[6,7,8,9,10]})\ndsets = Datasets(df, [[attrgetter('a')], [attrgetter('b')]])\nempty = dsets.new_empty()\n```\n\n----------------------------------------\n\nTITLE: Example Forward Hook Registration\nDESCRIPTION: This snippet defines a simple forward hook function `example_forward_hook` that prints the module, input, and output of a layer. It registers this hook with a linear layer `tst_model`, performs a forward pass, and then removes the hook. This demonstrates the basic usage of forward hooks in PyTorch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntst_model = nn.Linear(5,3)\ndef example_forward_hook(m,i,o): print(m,i,o)\n    \nx = torch.randn(4,5)\nhook = tst_model.register_forward_hook(example_forward_hook)\ny = tst_model(x)\nhook.remove()\n```\n\n----------------------------------------\n\nTITLE: Implementing Brier Score Wrapper for Multi-label Classification in Python\nDESCRIPTION: A wrapper function that adapts scikit-learn's brier_score_loss for use in fastai's multi-label classification tasks, with options for threshold, sigmoid activation, and sample weighting.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef BrierScoreMulti(thresh=0.5, sigmoid=True, sample_weight=None, pos_label=None):\n    \"Brier score for multi-label classification problems\"\n    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n    return skm_to_fastai(skm.brier_score_loss, thresh=thresh, activation=activation, flatten=False,\n                         sample_weight=sample_weight, pos_label=pos_label)\n```\n\n----------------------------------------\n\nTITLE: hook_output CUDA Example\nDESCRIPTION: This snippet demonstrates using the `hook_output` function with the `cpu=True` argument to move the stored activations to the CPU. It creates a hook on a linear layer, moves the model and input to the GPU, performs a forward pass, and verifies that the stored activations are on the CPU.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#|cuda\nwith hook_output(tst_model, cpu=True) as h:\n    y = tst_model.cuda()(x.cuda())\n    test_eq(h.stored.device, torch.device('cpu'))\n```\n\n----------------------------------------\n\nTITLE: Implementing ICNR Initialization\nDESCRIPTION: Implements `icnr_init`, which initializes the weights of a convolutional layer used in PixelShuffle. It performs ICNR initialization, ensuring that each of the `r**2` output channels get the same weights to reduce checkerboard artifacts. The implementation involves reshaping and repeating the kernel weights. It requires `torch` for tensor operations.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_50\n\nLANGUAGE: Python\nCODE:\n```\ndef icnr_init(x, scale=2, init=nn.init.kaiming_normal_):\n    \"ICNR init of `x`, with `scale` and `init` function\"\n    ni,nf,h,w = x.shape\n    ni2 = int(ni/(scale**2))\n    k = init(x.new_zeros([ni2,nf,h,w])).transpose(0, 1)\n    k = k.contiguous().view(ni2, nf, -1)\n    k = k.repeat(1, 1, scale**2)\n    return k.contiguous().view([nf,ni,h,w]).transpose(0, 1)\n```\n\n----------------------------------------\n\nTITLE: hook_outputs CUDA Example\nDESCRIPTION: This snippet demonstrates using the `hook_outputs` function with the `cpu=True` argument to move the stored activations to the CPU. It creates a sequential model, moves the model and input to the GPU, performs a forward pass, and verifies that the stored activations are on the CPU.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n#|cuda\nwith hook_outputs(tst_model, cpu=True) as h:\n    y = tst_model.cuda()(x.cuda())\n    for s in h.stored: test_eq(s.device, torch.device('cpu'))\n```\n\n----------------------------------------\n\nTITLE: Loading COCO Annotations Python\nDESCRIPTION: This function, `get_annotations`, loads bounding box annotations from a COCO-style JSON file. It parses the JSON, extracts image filenames, and creates lists of bounding box coordinates and corresponding labels.  Dependencies include `json`, `collections`, and `ifnone` (likely from fastcore). It takes a filename and optional prefix for images. The output is a list of filenames and a list of tuples containing bounding boxes and labels.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef get_annotations(fname, prefix=None):\n    \"Open a COCO style json in `fname` and returns the lists of filenames (with maybe `prefix`) and labelled bboxes.\"\n    annot_dict = json.load(open(fname))\n    id2images, id2bboxes, id2cats = {}, collections.defaultdict(list), collections.defaultdict(list)\n    classes = {o['id']:o['name'] for o in annot_dict['categories']}\n    for o in annot_dict['annotations']:\n        bb = o['bbox']\n        id2bboxes[o['image_id']].append([bb[0],bb[1], bb[0]+bb[2], bb[1]+bb[3]])\n        id2cats[o['image_id']].append(classes[o['category_id']])\n    id2images = {o['id']:ifnone(prefix, '') + o['file_name'] for o in annot_dict['images'] if o['id'] in id2bboxes}\n    ids = list(id2images.keys())\n    return [id2images[k] for k in ids], [(id2bboxes[k], id2cats[k]) for k in ids]\n```\n\n----------------------------------------\n\nTITLE: Test SimpleCNN Layer Structure in Python\nDESCRIPTION: This test verifies the number of modules created by the `SimpleCNN` constructor and checks the input and output channel sizes for the initial convolutional layers. It confirms that the `SimpleCNN` correctly builds a sequence of `ConvLayer` instances based on the provided `filters` list.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_64\n\nLANGUAGE: python\nCODE:\n```\ntst = SimpleCNN([8,16,32])\nmods = list(tst.children())\ntest_eq(len(mods), 3)\ntest_eq([[m[0].in_channels, m[0].out_channels] for m in mods[:2]], [[8,16], [16,32]])\n```\n\n----------------------------------------\n\nTITLE: Defining PILBase Class (Python)\nDESCRIPTION: Defines `PILBase`, a base class inheriting from `PIL.Image.Image` using `BypassNewMeta`. It serves as a foundation for fastai's custom image types, providing a `create` classmethod to load images from various sources (path, Tensor, ndarray, bytes, PIL.Image) and a `show` method for displaying the image using `show_image`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass PILBase(Image.Image, metaclass=BypassNewMeta):\n    \"Base class for a Pillow `Image` that can show itself and convert to a Tensor\"\n    _bypass_type=Image.Image\n    _show_args = {'cmap':'viridis'}\n    _open_args = {'mode': 'RGB'}\n    @classmethod\n    def create(cls, fn:Path|str|Tensor|ndarray|bytes|Image.Image, **kwargs):\n        \"Return an Image from `fn`\"\n        if isinstance(fn,TensorImage): fn = fn.permute(1,2,0).type(torch.uint8)\n        if isinstance(fn,TensorMask): fn = fn.type(torch.uint8)\n        if isinstance(fn,Tensor): fn = fn.numpy()\n        if isinstance(fn,ndarray): return cls(Image.fromarray(fn))\n        if isinstance(fn,bytes): fn = io.BytesIO(fn)\n        if isinstance(fn,Image.Image): return cls(fn)\n        return cls(load_image(fn, **merge(cls._open_args, kwargs)))\n\n    def show(self, ctx=None, **kwargs):\n        \"Show image using `merge(self._show_args, kwargs)`\"\n        return show_image(self, ctx=ctx, **merge(self._show_args, kwargs))\n\n    def __repr__(self): return f'{self.__class__.__name__} mode={self.mode} size={\"x\".join([str(d) for d in self.size])}'\n```\n\n----------------------------------------\n\nTITLE: Test skipinitialspace feature\nDESCRIPTION: This code creates a test DataFrame with data containing leading spaces in the `workclass` column.  Then, it creates a test DataLoader and asserts that the `workclass` value for the first element in the test dataset is not 0, thus confirming the proper application of the `skipinitialspace` parameter of the `from_csv` method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/41_tabular.data.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntest_data = {\n    'age': [49], \n    'workclass': ['Private'], \n    'fnlwgt': [101320],\n    'education': ['Assoc-acdm'], \n    'education-num': [12.0],\n    'marital-status': ['Married-civ-spouse'], \n    'occupation': [''],\n    'relationship': ['Wife'],\n    'race': ['White'],\n}\ninput = pd.DataFrame(test_data)\ntdl = dls.test_dl(input)\n\ntest_ne(0, tdl.dataset.iloc[0]['workclass'])\n```\n\n----------------------------------------\n\nTITLE: Data Normalization Layer Checks\nDESCRIPTION: This code verifies that the normalization layer in the data loaders matches expected ImageNet statistics, ensuring proper data normalization for transfer learning. It compares the mean tensors from 'dls' to 'imagenet_stats'. Dependencies include fastai's utility functions. It supports validation of data preprocessing steps.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\ntest_eq(to_cpu(dls.after_batch[1].mean[0].squeeze()), tensor(imagenet_stats[0]))\ntest_eq(to_cpu(dls.valid.after_batch[1].mean[0].squeeze()), tensor(imagenet_stats[0]))\n```\n\n----------------------------------------\n\nTITLE: Calculating Time Series Split Index - Python\nDESCRIPTION: Determines the index in the training DataFrame that corresponds to the date matching the start date of the test set. This index serves as the cut-off point for a time-based validation split.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ncut = train_df['Date'][(train_df['Date'] == train_df['Date'][len(test_df)])].index.max()\ncut\n```\n\n----------------------------------------\n\nTITLE: Converting Functions to AvgMetric or Metric in Fastai (Python)\nDESCRIPTION: Defines 'mk_metric', which takes a metric function or class and returns a Metric-compatible object (AvgMetric or the given Metric itself). Ensures correct type for use in metrics lists, enabling seamless plug-in with training and evaluation loops. Relies on fastai's Metric and AvgMetric classes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef mk_metric(m):\n    \"Convert `m` to an `AvgMetric`, unless it's already a `Metric`\"\n    if isinstance(m,type): m = m()\n    return m if isinstance(m, Metric) else AvgMetric(m)\n```\n\n----------------------------------------\n\nTITLE: Define CollectDataCallback in fastai Python\nDESCRIPTION: Defines a fastai `Callback` that collects all batch data (inputs, targets, predictions, loss) during training or validation into a list called `self.data`. It is primarily intended for debugging and testing purposes to inspect the flow of data and model outputs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14a_callback.data.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass CollectDataCallback(Callback):\n    \"Collect all batches, along with `pred` and `loss`, into `self.data`. Mainly for testing\"\n    def before_fit(self): self.data = L()\n    def after_batch(self): \n        self.data.append(self.learn.to_detach((self.xb,self.yb,self.pred,self.loss)))\n```\n\n----------------------------------------\n\nTITLE: Define EarlyStoppingCallback\nDESCRIPTION: Defines a callback that stops training early if the monitored metric does not improve for a specified number of epochs (`patience`). It inherits from `TrackerCallback` and utilizes its `best` value tracking. The `after_epoch` method checks if the current epoch's value is a new best; if not, it increments the `wait` counter, and if `wait` exceeds `patience`, it raises `CancelFitException` to terminate training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass EarlyStoppingCallback(TrackerCallback):\n    \"A `TrackerCallback` that terminates training when monitored quantity stops improving.\"\n    order=TrackerCallback.order+3\n    def __init__(self,\n        monitor='valid_loss', # value (usually loss or metric) being monitored.\n        comp=None, # numpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n        min_delta=0., # minimum delta between the last monitor value and the best monitor value.\n        patience=1, # number of epochs to wait when training has not improved model.\n        reset_on_fit=True # before model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss).\n    ):\n        super().__init__(monitor=monitor, comp=comp, min_delta=min_delta, reset_on_fit=reset_on_fit)\n        self.patience = patience\n\n    def before_fit(self): self.wait = 0; super().before_fit()\n    def after_epoch(self):\n        \"Compare the value monitored to its best score and maybe stop training.\"\n        super().after_epoch()\n        if self.new_best: self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                print(f'No improvement since epoch {self.epoch-self.wait}: early stopping')\n                raise CancelFitException()\n```\n\n----------------------------------------\n\nTITLE: Debugger Layer for Model Inspection - Python\nDESCRIPTION: A layer that sets a breakpoint using set_trace() and returns its input. Useful for interactive model debugging in development sessions. Requires set_trace from Python debugging tools, input/output is passthrough tensor.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@module()\ndef Debugger(self,x):\n    \"A module to debug inside a model.\"\n    set_trace()\n    return x\n```\n\n----------------------------------------\n\nTITLE: Adding Context-Managed Callbacks\nDESCRIPTION: This code shows how to use the `added_cbs` context manager to temporarily add callbacks to the `Learner`. Callbacks are added within a `with` statement, and they are removed automatically once the `with` block completes. This simplifies the addition and automatic removal of callbacks. `test_eq` validates the callback count before and after the context.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nwith learn.added_cbs(TestTrainEvalCallback()):\n    test_eq(len(learn.cbs), 2)\n```\n\n----------------------------------------\n\nTITLE: Testing Softmax Activation in AccumMetric - Python\nDESCRIPTION: Tests AccumMetric on classification predictions processed with softmax, disabling flattening for compatibility. Constructs a metric, samples predictions and integer targets, and runs a test comparison with expected logic. Utilizes torch and fastai. Ensures softmax-based comparison behaves as intended.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ndef _l2_mean(x,y): return torch.sqrt((x.argmax(dim=-1).float()-y.float()).pow(2).mean())\nx1,x2 = torch.randn(20,5),torch.randint(0, 5, (20,))\ntst = AccumMetric(_l2_mean, dim_argmax=-1, flatten=False, activation=ActivationType.Softmax)\ntest_close(compute_val(tst, x1, x2), _l2_mean(F.softmax(x1, dim=-1), x2))\n```\n\n----------------------------------------\n\nTITLE: Refining Learning Rate for Fine-tuning - Python\nDESCRIPTION: Re-runs the learning rate finder on the unfrozen model to suggest new optimal rates for whole-model fine-tuning. Inputs and outputs same as previous lr_find.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nlearn.lr_find()\n```\n\n----------------------------------------\n\nTITLE: Importing Neptune Library in Python\nDESCRIPTION: Imports the main `neptune` library, providing access to its functions like `init`, `create_experiment`, and `get_experiment`.\nSOURCE: https://github.com/fastai/fastai/blob/main/disabled/70b_callback.neptune.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport neptune\n```\n\n----------------------------------------\n\nTITLE: Example of using URLs.path for PETS\nDESCRIPTION: This code snippet demonstrates the usage of the `URLs.path` method.  It fetches the local path for the PETS dataset and checks if the parent directory matches the archive location defined in the config. It uses the URL of PETS defined within the `URLs` class. The expected output is the location of the archive.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/04_data.external.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nurl = URLs.PETS\nlocal_path = URLs.path(url)\ntest_eq(local_path.parent, fastai_path('archive'))\nlocal_path\n```\n\n----------------------------------------\n\nTITLE: Defining Function for Spacy `pipe` in Worker - Python\nDESCRIPTION: This snippet defines a Python function `f` intended to be run in parallel workers. It takes a list of input items `its` and applies the globally initialized Spacy tokenizer's `pipe` method to them. The results are then converted to strings using `conv_sp` and returned. This function is designed to be used with fastai's parallel processing to leverage Spacy's `pipe` in a parallel setting.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndef f(its): return L(nlp.tokenizer.pipe(its)).map(conv_sp)\n```\n\n----------------------------------------\n\nTITLE: Loading PyTorch/Fastai Model and Optimizer State from File in Python\nDESCRIPTION: Implements 'load_model' for deserializing models and optimizers from files, supporting CPU/CUDA target device selection and strictness in key matching. Integrates PyTorch 2.5's safe serialization when available, warning if optimizer state restoration fails. Accepts custom loading kwargs and is compatible with different file targets.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef load_model(file, model, opt, with_opt=True, device=None, strict=True, **torch_load_kwargs):\n    \"Load `model` from `file` along with `opt` (if available, and if `with_opt`)\"\n    if isinstance(device, int): device = torch.device('cuda', device)\n    elif device is None: device = 'cpu'\n    if ismin_torch(\"2.5\"):\n        context = torch.serialization.safe_globals([L, np.core.multiarray.scalar, np.dtype, *[getattr(np.dtypes, dt) for dt in np.dtypes.__all__]])\n        torch_load_kwargs.setdefault(\"weights_only\", True)\n    else: context = nullcontext()\n    with context:\n        state = torch.load(file, map_location=device, **torch_load_kwargs)\n    hasopt = set(state)=={'model', 'opt'}\n    model_state = state['model'] if hasopt else state\n    get_model(model).load_state_dict(model_state, strict=strict)\n    if hasopt and with_opt:\n        try: opt.load_state_dict(state['opt'])\n        except:\n            if with_opt: warn(\"Could not load the optimizer state.\")\n    elif with_opt: warn(\"Saved file doesn't contain an optimizer state.\")\n```\n\n----------------------------------------\n\nTITLE: DataFrame Shrinking Example (Python)\nDESCRIPTION: This code snippet demonstrates the usage of `df_shrink_dtypes` function by creating a sample DataFrame with integer, float, boolean and object datatypes and then calling the function to find the smallest possible datatypes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'i': [-100, 0, 100], 'f': [-100.0, 0.0, 100.0], 'e': [True, False, True],\n                   'date':['2019-12-04','2019-11-29','2019-11-15',]})\ndf.dtypes\n```\n\nLANGUAGE: python\nCODE:\n```\ndt = df_shrink_dtypes(df)\ndt\n```\n\n----------------------------------------\n\nTITLE: Add TIMM Normalization Transform (fastai)\nDESCRIPTION: This function adds a normalization transform specifically for TIMM models to a `DataLoader`. It checks if a normalization transform is already present and, if not, creates one using the provided mean and standard deviation from the model configuration.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef _timm_norm(dls, cfg, pretrained, n_in=3):\n    if not pretrained: return\n    if n_in != len(cfg['mean']): return\n    if not dls.after_batch.fs.filter(risinstance(Normalize)):\n        tfm = Normalize.from_stats(cfg['mean'],cfg['std'])\n        dls.add_tfms([tfm],'after_batch')\n```\n\n----------------------------------------\n\nTITLE: Implementing _merge_grouper function for grouping transforms\nDESCRIPTION: Defines a function that determines how to group transformation objects based on their type, using function ID, class, qualname, or object class.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef _merge_grouper(o):\n    if isinstance(o, LambdaType): return id(o)\n    elif isinstance(o, type): return o\n    elif (isfunction(o) or ismethod(o)): return o.__qualname__\n    return o.__class__\n```\n\n----------------------------------------\n\nTITLE: Implementing Mixed Precision Callback in fastai Python\nDESCRIPTION: This is the core callback `MixedPrecision` for enabling automatic mixed precision training in fastai. It initializes PyTorch's `autocast` and `GradScaler` (for FP16), handles mode selection (FP16/BF16), manages precision switching during forward/backward passes, and interacts with the optimizer via a fake `step` method for gradient scaling.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@delegates(GradScaler)\nclass MixedPrecision(Callback):\n    \"Mixed precision training using Pytorch's Automatic Mixed Precision (AMP)\"\n    order = 10\n    def __init__(self,\n        amp_mode:str|AMPMode=AMPMode.FP16, # Mixed Precision training mode. Supports fp16 and bf16.\n        **kwargs\n    ):\n        amp_mode = AMPMode(amp_mode)\n        store_attr(names='amp_mode')\n        self.kwargs = kwargs\n\n    def before_fit(self):\n        if self.amp_mode == AMPMode.BF16:\n            if torch.cuda.is_available() and not torch.cuda.is_bf16_supported():\n                raise ValueError(\"Unsupported GPU for bfloat16 mixed precision training\")\n            dtype = torch.bfloat16\n        elif self.amp_mode == AMPMode.FP16:\n            dtype = torch.float16\n        else:\n            raise ValueError(f\"Unrecognized precision: {self.amp_mode}\")\n        # `GradScaler` is not needed for bfloat16 as fp32 and bf16 have the same range\n        self.kwargs['enabled'] = dtype == torch.float16\n        self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n\n    def before_batch(self): self.autocast.__enter__()\n    def after_pred(self):\n        self.learn.pred = to_float(self.pred)\n    def after_loss(self): self.autocast.__exit__(None, None, None)\n    def before_backward(self): self.learn.loss_grad = self.scaler.scale(self.loss_grad)\n    def before_step(self):\n        \"Use `self` as a fake optimizer. `self.skipped` will be set to True `after_step` if gradients overflow.\"\n        self.skipped=True\n        self.scaler.step(self)\n        if self.skipped: raise CancelStepException()\n        self.scales.append(self.scaler.get_scale())\n    def after_step(self): self.learn.scaler.update()\n    def after_fit(self): self.autocast,self.learn.scaler,self.scales = None,None,None\n\n    @property\n    def param_groups(self):\n        \"Pretend to be an optimizer for `GradScaler`\"\n        return self.opt.param_groups\n    def step(self, *args, **kwargs):\n        \"Fake optimizer step to detect whether this batch was skipped from `GradScaler`\"\n        self.skipped=False\n```\n\n----------------------------------------\n\nTITLE: Setting Up PyTorch Documentation Link Base\nDESCRIPTION: Defines a default export identifier for the PyTorch documentation base URL, which is used throughout the module to generate links to specific PyTorch documentation pages.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/99_pytorch_doc.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nPYTORCH_URL = 'https://pytorch.org/docs/stable/'\n```\n\n----------------------------------------\n\nTITLE: AWD-LSTM Test - PyTorch\nDESCRIPTION: This snippet tests the `AWD_LSTM` class. It initializes an `AWD_LSTM` model and creates dummy input data. It verifies that the shapes of the hidden states and the output are correct and that the hidden state matches the output of the last timestep in raw outputs. Also, it performs a forward pass in eval mode to verify the reset operation and calls again, ensuring the functionality as expected.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ntst = AWD_LSTM(100, 20, 10, 2, hidden_p=0.2, embed_p=0.02, input_p=0.1, weight_p=0.2)\nx = torch.randint(0, 100, (10,5))\nr = tst(x)\ntest_eq(tst.bs, 10)\ntest_eq(len(tst.hidden), 2)\ntest_eq([h_.shape for h_ in tst.hidden[0]], [[1,10,10], [1,10,10]])\ntest_eq([h_.shape for h_ in tst.hidden[1]], [[1,10,20], [1,10,20]])\n\ntest_eq(r.shape, [10,5,20])\ntest_eq(r[:,-1], tst.hidden[-1][0][0]) #hidden state is the last timestep in raw outputs\n\ntst.eval()\ntst.reset()\ntst(x);\ntst(x);\n```\n\n----------------------------------------\n\nTITLE: Defining fastai Configuration\nDESCRIPTION: This snippet defines the `fastai_cfg` function using `lru_cache` to create and return a `Config` object. This object contains default download paths for data, models, storage, and archives, based on the `FASTAI_HOME` environment variable or the default `~/.fastai` path.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/04_data.external.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@lru_cache(maxsize=None)\ndef fastai_cfg() -> Config: # Config that contains default download paths for `data`, `model`, `storage` and `archive`\n    \"`Config` object for fastai's `config.ini`\"\n    return Config(Path(os.getenv('FASTAI_HOME', '~/.fastai')), 'config.ini', create=dict(\n        data = 'data', archive = 'archive', storage = 'tmp', model = 'models'))\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Ratings\nDESCRIPTION: Calculates the mean rating for each movie using the original ratings data. This information is later used in comparison with the learned movie biases to demonstrate their correlation.  This helps in understanding the learned biases.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmean_ratings = ratings.groupby('title')['rating'].mean()\nmovie_ratings = [(b, i, mean_ratings.loc[i]) for i,b in zip(top_movies,movie_bias)]\n```\n\n----------------------------------------\n\nTITLE: Example Usage of `to_device` in fastai\nDESCRIPTION: Demonstrates a simple call to the `to_device` function. It passes a nested tuple containing integers and PyTorch tensors, which results in the tensors within the structure being moved to the default compute device (CUDA/MPS if available, otherwise CPU). The resulting structure `t` maintains its original nesting.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nt = to_device((3,(tensor(3),tensor(2))))\nt1,(t2,t3) = t\n```\n\n----------------------------------------\n\nTITLE: Module: UNet Decoder Block (Python)\nDESCRIPTION: Defines a single decoding block used within the UNet architecture. It combines upsampling (using `PixelShuffle_ICNR`), concatenation with a skip connection from the encoder (`self.hook.stored`), batch normalization, and two convolutional layers. It supports optional features like blur, self-attention, and customizable activation and normalization types.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15a_vision.models.unet.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass UnetBlock(Module):\n    \"A quasi-UNet block, using `PixelShuffle_ICNR upsampling`.\"\n    @delegates(ConvLayer.__init__)\n    def __init__(self, up_in_c, x_in_c, hook, final_div=True, blur=False, act_cls=defaults.activation,\n                 self_attention=False, init=nn.init.kaiming_normal_, norm_type=None, **kwargs):\n        self.hook = hook\n        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, act_cls=act_cls, norm_type=norm_type)\n        self.bn = BatchNorm(x_in_c)\n        ni = up_in_c//2 + x_in_c\n        nf = ni if final_div else ni//2\n        self.conv1 = ConvLayer(ni, nf, act_cls=act_cls, norm_type=norm_type, **kwargs)\n        self.conv2 = ConvLayer(nf, nf, act_cls=act_cls, norm_type=norm_type,\n                               xtra=SelfAttention(nf) if self_attention else None, **kwargs)\n        self.relu = act_cls()\n        apply_init(nn.Sequential(self.conv1, self.conv2), init)\n\n    def forward(self, up_in):\n        s = self.hook.stored\n        up_out = self.shuf(up_in)\n        ssh = s.shape[-2:]\n        if ssh != up_out.shape[-2:]:\n            up_out = F.interpolate(up_out, s.shape[-2:], mode='nearest')\n        cat_x = self.relu(torch.cat([up_out, self.bn(s)], dim=1))\n        return self.conv2(self.conv1(cat_x))\n```\n\n----------------------------------------\n\nTITLE: SE-enhanced ResNet Block Creation in Python\nDESCRIPTION: This function is a convenience wrapper around `ResBlock` that configures it specifically for a Squeeze and Excitation-enhanced block. It sets the `reduction` parameter and adjusts the intermediate channel sizes (`nh1`, `nh2`) based on the input and output feature map sizes (`ni`, `nf`) and the `expansion` factor. It delegates other arguments to the base `ResBlock` constructor.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_71\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef SEBlock(expansion, ni, nf, groups=1, reduction=16, stride=1, **kwargs):\n    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, reduction=reduction, nh1=nf*2, nh2=nf*expansion, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Triggering nbdev Code Export (Python)\nDESCRIPTION: This Python snippet explicitly calls the `nbdev_export()` function. This function, imported from the `nbdev` library, processes the notebook and exports code cells marked with `#|export` to the designated Python module file(s). The `#|hide` directive ensures this specific code block is not included in the exported library code itself, as it's meant for notebook execution.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/65_medical.text.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom nbdev import nbdev_export\nbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Initializing DataLoader with _rand_item\nDESCRIPTION: This snippet demonstrates initializing a `DataLoader` with a custom function `_rand_item`.  The purpose of this function is not detailed in the provided context. The code initializes a `DataLoader` with the result of `create_item=_rand_item`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef _rand_item(s):\n    r = random.random()\n    return r if r<0.95 else stop()\n\nL(DataLoader(create_item=_rand_item))\n```\n\n----------------------------------------\n\nTITLE: Upgrading fastai on Google Colab (Python Bash Magic)\nDESCRIPTION: This snippet checks if the current environment is Google Colab (by testing the existence of '/content') and, if so, upgrades the fastai library using pip with suppressed output. No explicit dependencies apart from pip are required. There are no input parameters; the output is silent unless pip complains. Should be run in notebook environments supporting shell magics, such as Jupyter.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary dependencies from fastai for collaborative filtering functionality.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/45_collab.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom __future__ import annotations\nfrom fastai.tabular.all import *\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Test and Additional Test Data - Python\nDESCRIPTION: Shell commands (commented out) to download and extract the Kaggle test set and 'additional' test set using kaggle-cli and 7zip, to the previously prepared data directory. Requires Kaggle API and 7zip installed.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n#! kaggle competitions download -c planet-understanding-the-amazon-from-space -f test-jpg.tar.7z -p {path}  \n#! 7za -bd -y -so x {path}/test-jpg.tar.7z | tar xf - -C {path}\n#! kaggle competitions download -c planet-understanding-the-amazon-from-space -f test-jpg-additional.tar.7z -p {path}  \n#! 7za -bd -y -so x {path}/test-jpg-additional.tar.7z | tar xf - -C {path}\n```\n\n----------------------------------------\n\nTITLE: Defining Common Image Extensions\nDESCRIPTION: Creates a set of file extensions that correspond to image file types based on the system's MIME type mappings.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimage_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('image/'))\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Parallel Spacy Tokenization (Batched, 2 Workers) - Jupyter\nDESCRIPTION: This snippet uses `%%timeit` to benchmark the execution time of applying the `spacy_tok` function in parallel using fastai's `parallel` with 2 workers on the `batches` data. Workers apply `spacy_tok` to their batch elements via a `partial` application of the `apply` function. This measures Spacy parallel performance with 2 workers. Results stored globally in `t`. Runs for 3 loops (`-r 3`).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n%%timeit -r 3\nglobal t\nt = parallel(partial(apply, spacy_tok), batches, progress=False, n_workers=2)\n```\n\n----------------------------------------\n\nTITLE: Defining Fastai Training Loop Event Sequence (Python)\nDESCRIPTION: This snippet initializes a list named `_loop` containing strings that represent the ordered events triggered during the fastai training and validation loops. This list serves as a reference or documentation for the sequence handled by the callback system.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n#|export\n_loop = ['Start Fit', 'before_fit', 'Start Epoch Loop', 'before_epoch', 'Start Train', 'before_train',\n         'Start Batch Loop', 'before_batch', 'after_pred', 'after_loss', 'before_backward', 'before_step',\n         'after_step', 'after_cancel_batch', 'after_batch','End Batch Loop','End Train',\n         'after_cancel_train', 'after_train', 'Start Valid', 'before_validate','Start Batch Loop',\n         '**CBs same as train batch**', 'End Batch Loop', 'End Valid', 'after_cancel_validate',\n         'after_validate', 'End Epoch Loop', 'after_cancel_epoch', 'after_epoch', 'End Fit',\n         'after_cancel_fit', 'after_fit']\n```\n\n----------------------------------------\n\nTITLE: Tabular Processing Pipeline Example\nDESCRIPTION: This code demonstrates how to create and use a tabular processing pipeline using `TabularGPU` with a sequence of processors: `Normalize`, `Categorify`, `FillMissing`, and `noop`. It creates a pipeline to process both categorical and continuous variables including handling missing values.  It also tests for the proper `classes` from the pipeline stages.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nprocs = [Normalize, Categorify, FillMissing, noop]\ndf = cudf.from_pandas(pd.DataFrame({'a':[0,1,2,1,1,2,0], 'b':[0,1,np.nan,1,2,3,4]}))\nto = TabularGPU(df, procs, cat_names='a', cont_names='b')\n\n#Test setup and apply on df_trn\ntest_eq(to.a.to_array(), [1,2,3,2,2,3,1])\ntest_eq(to.b_na.to_array(), [1,1,2,1,1,1,1])\nx = np.array([0,1,1.5,1,2,3,4])\nm,s = x.mean(),x.std()\ntest_close(to.b.to_array(), (x-m)/s)\ntest_eq(to.procs.classes, {'a': ['#na#','0','1','2'], 'b_na': ['#na#','False','True']})\n```\n\n----------------------------------------\n\nTITLE: Defining a Value Metric Class (Python)\nDESCRIPTION: This snippet defines `ValueMetric`, a class that allows including a pre-calculated metric value, for example, from a callback. The metric value is returned by a function.  The class stores the function and an optional metric name, and returns the result of calling the function in the `value` property.  The `name` property defaults to the function name if a specific name is not provided.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_64\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass ValueMetric(Metric):\n    \"Use to include a pre-calculated metric value (for instance calculated in a `Callback`) and returned by `func`\"\n    def __init__(self, func, metric_name=None): store_attr('func, metric_name')\n\n    @property\n    def value(self): return self.func()\n\n    @property\n    def name(self): return self.metric_name if self.metric_name else self.func.__name__\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(ValueMetric, title_level=3)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef metric_value_fn(): return 5e-3\n\npm = ValueMetric(metric_value_fn, 'custom_value_metric')\ntest_eq(vm.value, 5e-3)\ntest_eq(vm.name, 'custom_value_metric')\n\npm = ValueMetric(metric_value_fn)\ntest_eq(vm.name, 'metric_value_fn')\n```\n\n----------------------------------------\n\nTITLE: DataFrame Shrinking with ADULT_SAMPLE Dataset (Python)\nDESCRIPTION: This snippet demonstrates `df_shrink` with a larger dataset. It loads the ADULT_SAMPLE dataset, applies `df_shrink` with integer-to-unsigned integer conversion, and prints the initial and reduced memory usage. Dependencies: untar_data, URLs, pd.read_csv\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.ADULT_SAMPLE)\ndf = pd.read_csv(path/'adult.csv')\nnew_df = df_shrink(df, int2uint=True)\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(f'Initial Dataframe: {df.memory_usage().sum() / 1000000} megabytes')\nprint(f'Reduced Dataframe: {new_df.memory_usage().sum() / 1000000} megabytes')\n```\n\n----------------------------------------\n\nTITLE: Collecting BatchNorm and Bias Parameters from PyTorch Module in Python\nDESCRIPTION: Recursively scans a PyTorch module and its children to accumulate parameters associated with normalization layers (BatchNorm, InstanceNorm, LayerNorm) and optionally biases from other layers. Returns a concatenated list of these parameters, facilitating targeted optimization or regularization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_78\n\nLANGUAGE: python\nCODE:\n```\nnorm_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d, nn.LayerNorm)\n\ndef norm_bias_params(m, with_bias=True):\n    \"Return all bias and BatchNorm parameters\"\n    if isinstance(m, norm_types): return L(m.parameters())\n    res = L(m.children()).map(norm_bias_params, with_bias=with_bias).concat()\n    if with_bias and getattr(m, 'bias', None) is not None: res.append(m.bias)\n    return res\n```\n\n----------------------------------------\n\nTITLE: Example: Train with WeightedDL and CollectDataCallback in fastai Python\nDESCRIPTION: Sets up a synthetic dataset and a learner to demonstrate the usage of `WeightedDL` and `CollectDataCallback`. It creates a `WeightedDL` from a `Datasets` object and attaches the `CollectDataCallback` to the learner for later inspection of batch data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14a_callback.data.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nn = 160\ndsets = Datasets(torch.arange(n).float())\ndls = dsets.weighted_dataloaders(wgts=range(n), bs=16)\nlearn = synth_learner(data=dls, cbs=CollectDataCallback)\n```\n\n----------------------------------------\n\nTITLE: Add weighted_dataloaders to Datasets in fastai Python\nDESCRIPTION: Patches the `Datasets` class to add a `weighted_dataloaders` method. This convenient method simplifies the creation of `WeightedDL` instances directly from a `Datasets` object, applying the specified weights to the training split and optionally passing other arguments to the standard `dataloaders` method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14a_callback.data.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\n@delegates(Datasets.dataloaders)\ndef weighted_dataloaders(self:Datasets, wgts, bs=64, **kwargs):\n    \"Create a weighted dataloader `WeightedDL` with `wgts` for the training set\"\n    xtra_kwargs = [{}] * (self.n_subsets-1)\n    return self.dataloaders(bs=bs, dl_type=WeightedDL, dl_kwargs=({'wgts':wgts}, *xtra_kwargs), **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Defining the ImagesCleaner Widget\nDESCRIPTION: Defines the `ImagesCleaner` class, an ipywidget for reviewing images and assigning actions (like keep, delete, or reclassify). It displays image thumbnails in a carousel, each paired with a dropdown menu for user selection. Uses `parallel` processing for thumbnail generation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09c_vision.widgets.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass ImagesCleaner:\n    \"A widget that displays all images in `fns` along with a `Dropdown`\"\n    def __init__(self,\n        opts:tuple=(), # Options for the `Dropdown` menu\n        height:int=128, # Thumbnail Height\n        width:int=256, # Thumbnail Width\n        max_n:int=30 # Max number of images to display\n    ):\n        opts = ('<Keep>', '<Delete>')+tuple(opts)\n        store_attr('opts,height,width,max_n')\n        self.widget = carousel(width='100%')\n\n    def set_fns(self,\n        fns:list # Contains a path to each image \n    ):\n        \"Sets a `thumbnail` and a `Dropdown` menu for each `VBox`\"\n        self.fns = L(fns)[:self.max_n]\n        ims = parallel(_open_thumb, self.fns, h=self.height, w=self.width, progress=False,\n                       n_workers=min(len(self.fns)//10,defaults.cpus))\n        self.widget.children = [VBox([widget(im, height=f'{self.height}px'), Dropdown(\n            options=self.opts, layout={'width': 'max-content'})]) for im in ims]\n\n    def _ipython_display_(self): display(self.widget)\n    def values(self) -> list:\n        \"Current values of `Dropdown` for each `VBox`\"\n        return L(self.widget.children).itemgot(1).attrgot('value')\n    def delete(self) -> list:\n        \"Indices of items to delete\"\n        return self.values().argwhere(eq('<Delete>'))\n    def change(self) -> list:\n        \"Tuples of the form (index of item to change, new class)\"\n        idxs = self.values().argwhere(not_(in_(['<Delete>','<Keep>'])))\n        return idxs.zipwith(self.values()[idxs])\n```\n\n----------------------------------------\n\nTITLE: Counting PyTorch Module Parameters with Trainability (Python)\nDESCRIPTION: Defines the total_params function to compute the count of parameters in a module along with their trainable status (requires_grad). Returns a tuple of number of parameters and a boolean indicating trainability, or False if there are no parameters. Expects a torch.nn.Module as input. Useful for model introspection and summary generation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\n#|export\ndef total_params(m):\n    \"Give the number of parameters of a module and if it's trainable or not\"\n    params = sum([p.numel() for p in m.parameters()])\n    trains = [p.requires_grad for p in m.parameters()]\n    return params, (False if len(trains)==0 else trains[0])\n```\n\n----------------------------------------\n\nTITLE: Implementing Foreground Accuracy for Segmentation in PyTorch\nDESCRIPTION: Function that calculates non-background accuracy for multiclass segmentation by ignoring pixels with the background index (default 0) when computing accuracy.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndef foreground_acc(inp, targ, bkg_idx=0, axis=1):\n    \"Computes non-background accuracy for multiclass segmentation\"\n    targ = cast(targ.squeeze(1), TensorBase)\n    mask = targ != bkg_idx\n    return (inp.argmax(dim=axis)[mask]==targ[mask]).float().mean()\n```\n\n----------------------------------------\n\nTITLE: fit_flat_cos test\nDESCRIPTION: Trains a model using the `fit_flat_cos` method, which first trains with a constant learning rate and then with a cosine-annealed rate. The method calls `fit_flat_cos`, which applies the defined learning rate schedule.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner()\nlearn.fit_flat_cos(2)\n```\n\n----------------------------------------\n\nTITLE: Importing core FastAI modules in Python\nDESCRIPTION: This snippet imports several essential modules and functions from FastAI libraries such as torch_basics, data.core, data.load, data.external, and data.transforms. It sets up the environment with necessary components to work with tensors, datasets, dataloaders, and data transformation pipelines in FastAI.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.torch_basics import *\nfrom fastai.data.core import *\nfrom fastai.data.load import *\nfrom fastai.data.external import *\nfrom fastai.data.transforms import *\n```\n\n----------------------------------------\n\nTITLE: Exporting and Loading fastai Vision Learner (Python)\nDESCRIPTION: Demonstrates training, exporting, and loading a fastai `vision_learner`. A simple image classification model is trained on the PETS dataset, exported to a pickle file within a temporary directory, and then loaded back. Predictions from both the original and loaded learners are compared using `fastcore.test` functions (`test_eq`, `test_close`) to verify successful export and loading. Prerequisites include fastai, fastcore, and datasets like URLs.PETS.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/98_test_model_export.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|cuda\ndef label_func(f): return f[0].isupper()\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(32))\n\nwith TemporaryDirectory() as td:\n    learn = vision_learner(dls, resnet18, metrics=error_rate, path=td)\n    learn.fine_tune(1,base_lr=0.00001)\n    learn.export(\"model.pkl\")\n    \n    learn2 = load_learner(Path(td) / \"model.pkl\", cpu=False)\n\no1 = learn.predict(files[0])\no2 = learn2.predict(files[0])\n\ntest_eq(o1[:2],o2[:2])\ntest_close(o1[-1], o2[-1])\n```\n\n----------------------------------------\n\nTITLE: Displaying Tabular Learner Prediction Results in Python\nDESCRIPTION: Example snippet to display the predicted row and extracted classification result along with probabilities after calling the `predict` method on a `TabularLearner`. The row representation uses the `show` method while classification and probabilities are stored in variables for further inspection.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/43_tabular.learner.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nrow.show()\n```\n\nLANGUAGE: python\nCODE:\n```\nclas, probs\n```\n\n----------------------------------------\n\nTITLE: Initializing Spacy Tokenizer Function - Python\nDESCRIPTION: This code snippet initializes the Spacy English language model and directly gets its default tokenizer. It then defines a function `spacy_tok` that takes a string, applies the Spacy tokenizer to it, and converts the resulting Spacy tokens into a fastai `L` object of strings. This prepares a reusable tokenizer function for subsequent benchmarks.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nnlp = English()\nsp_tokenizer = nlp.Defaults.create_tokenizer(nlp)\ndef spacy_tok(s): return L(sp_tokenizer(str(s))).map(str)\n```\n\n----------------------------------------\n\nTITLE: Verify Images Function\nDESCRIPTION: This function takes a list of file paths and returns a list of image files that cannot be opened. It uses the `verify_image` function to check each file and utilizes the `parallel` function for concurrent verification. The output provides a list of images that are likely corrupted or unreadable.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ndef verify_images(fns):\n    \"Find images in `fns` that can't be opened\"\n    return L(fns[i] for i,o in enumerate(parallel(verify_image, fns)) if not o)\n```\n\n----------------------------------------\n\nTITLE: SequentialRNN Container with Reset Propagation in Python\nDESCRIPTION: Defines SequentialRNN, extending nn.Sequential to additionally pass a reset() call to all child modules that implement this method. This facilitates resetting internal states (such as RNN hidden states) across all contained modules uniformly.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass SequentialRNN(nn.Sequential):\n    \"A sequential module that passes the reset call to its children.\"\n    def reset(self):\n        for c in self.children(): getcallable(c, 'reset')()\n```\n\n----------------------------------------\n\nTITLE: Tests: Validate size change index utility (Python)\nDESCRIPTION: Provides unit tests for the `_get_sz_change_idxs` function using `fastai.testing.test_eq`. It checks several scenarios with different input size lists, including cases with multiple size changes, a single change, and no changes, to ensure the function correctly returns the expected indices.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15a_vision.models.unet.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntest_eq(_get_sz_change_idxs([[3,64,64], [16,64,64], [32,32,32], [16,32,32], [32,32,32], [16,16]]), [1,4])\ntest_eq(_get_sz_change_idxs([[3,64,64], [16,32,32], [32,32,32], [16,32,32], [32,16,16], [16,16]]), [0,3])\ntest_eq(_get_sz_change_idxs([[3,64,64]]), [])\ntest_eq(_get_sz_change_idxs([[3,64,64], [16,32,32]]), [0])\n```\n\n----------------------------------------\n\nTITLE: Suggesting Learning Rate via the Valley Method in Python\nDESCRIPTION: The valley function identifies the longest descending valley in the loss curve and suggests a learning rate approximately two-thirds into this valley. It uses dynamic programming to find the longest decreasing subsequence of losses and returns the corresponding learning rate and loss index. Originally developed by ESRI, this method is the default suggestion for Learner.lr_find.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndef valley(lrs:list, losses:list, num_it:int):\n    \"Suggests a learning rate from the longest valley and returns its index\"\n    n = len(losses)\n    max_start, max_end = 0,0\n\n    # find the longest valley\n    lds = [1]*n\n    for i in range(1,n):\n        for j in range(0,i):\n            if (losses[i] < losses[j]) and (lds[i] < lds[j] + 1):\n                lds[i] = lds[j] + 1\n            if lds[max_end] < lds[i]:\n                max_end = i\n                max_start = max_end - lds[max_end]\n    \n    sections = (max_end - max_start) / 3\n    idx = max_start + int(sections) + int(sections/2)\n\n    return float(lrs[idx]), (float(lrs[idx]), losses[idx])\n```\n\n----------------------------------------\n\nTITLE: TabularGPU Class Definition\nDESCRIPTION: This code defines the `TabularGPU` class, inheriting from `Tabular`. It provides a method `transform` to apply functions to specified columns.  It also overrides `__getattr__` to access columns directly from the underlying `cudf.DataFrame`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass TabularGPU(Tabular):\n    def transform(self, cols, f):\n        for c in cols: self[c] = f(self[c])\n\n    def __getattr__(self,k):\n        if isinstance(self.items, cudf.DataFrame) and k in self.items.columns: return self.items[k]\n        return super().__getattr__(k)\n```\n\n----------------------------------------\n\nTITLE: Installing fastai in Google Colab - Python\nDESCRIPTION: Installs or upgrades the fastai library when running inside Google Colab. No external dependencies except pip are required. The snippet checks whether the /content path exists (indicating Colab environment), and uses pip to install fastai. There are no input parameters and no outputs; it is just for setup.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Show Documentation for after_fit\nDESCRIPTION: Displays the documentation for the `after_fit` method of the `ParamScheduler` class, which saves hyper-parameters in the recorder.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(ParamScheduler.after_fit)\n```\n\n----------------------------------------\n\nTITLE: Get NVIDIA Memory Info\nDESCRIPTION: Retrieves memory information from NVIDIA GPUs using `nvidia-smi`. It calls the `nvidia-smi` command. This function strips the output, splits by newline, and is useful for inspecting GPU memory.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/97_test_utils.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef nvidia_mem():\n    try: mem = run(\"nvidia-smi --query-gpu=memory.total --format=csv,nounits,noheader\")\n    except: return None\n    return mem.strip().split('\\n')\n```\n\n----------------------------------------\n\nTITLE: Importing format_time Utility with fastai in Python\nDESCRIPTION: This snippet imports the 'format_time' utility function from the 'fastprogress.fastprogress' module, enabling easy conversion of time durations into human-readable strings for logging training durations within the Recorder. There are no parameters or outputs; it simply provides access to 'format_time' where needed. Ensure 'fastprogress' is installed and available in the Python environment.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_65\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom fastprogress.fastprogress import format_time\n```\n\n----------------------------------------\n\nTITLE: Show Batch for Tabular Data - Python\nDESCRIPTION: This function implements `show_batch` for `Tabular` data, delegating to the `Tabular.show` method to display the batch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\n@dispatch\ndef show_batch(x: Tabular, y, its, max_n=10, ctxs=None):\n    x.show()\n```\n\n----------------------------------------\n\nTITLE: Grouping Ratings by Title\nDESCRIPTION: This code groups the ratings DataFrame by movie title and counts the number of ratings for each movie. The result is a Series used later to identify the most rated movies. It's a preparatory step for analyzing movie biases and weights.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ng = ratings.groupby('title')['rating'].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\n```\n\n----------------------------------------\n\nTITLE: Testing Metadata Preservation with `Tensor.as_subclass` Patch in fastai\nDESCRIPTION: Demonstrates and tests the combined effect of the patched `Tensor.set_meta` and `Tensor.as_subclass` methods. It creates a simple tensor subclass `_T`, adds custom metadata (`img_size=1`) to a base tensor `t`, and sets `requires_grad_`. It then casts `t` to `_T` using the patched `as_subclass` and uses `test_eq` to verify that both the custom metadata (`img_size`) and standard tensor properties (`requires_grad`) are correctly preserved in the resulting instance `t2`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nclass _T(Tensor): pass\nt = tensor(1.).requires_grad_()\nt.img_size = 1\nt2 = t.as_subclass(_T)\ntest_eq(t.img_size, t2.img_size)\ntest_eq(t2.img_size, 1)\nassert(t2.requires_grad_)\n```\n\n----------------------------------------\n\nTITLE: Interpolating 1D Tensor Values Similar to NumPy interp in Python\nDESCRIPTION: Implements 1D linear interpolation on tensor 'x' points given breakpoints 'xp' and function values 'fp', mimicking the behavior of numpy.interp. It calculates linear slopes and intercepts between breakpoints and computes interpolated values at target points using PyTorch tensor operations.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_80\n\nLANGUAGE: python\nCODE:\n```\n@patch\ndef interp_1d(x:Tensor, xp, fp):\n    \"Same as `np.interp`\"\n    slopes = (fp[1:]-fp[:-1])/(xp[1:]-xp[:-1])\n    incx = fp[:-1] - (slopes*xp[:-1])\n    locs = (x[:,None]>=xp[None,:]).long().sum(1)-1\n    locs = locs.clamp(0,len(slopes)-1)\n    return slopes[locs]*x + incx[locs]\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Spacy Tokenizer Initialization - Jupyter\nDESCRIPTION: This snippet uses `%%timeit` to measure the time it takes to initialize the `SpTok` class, which in turn initializes the Spacy English language model and tokenizer. This benchmark quantifies the setup cost for using the Spacy tokenizer. Runs 2 times per loop, 3 loops.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n%%timeit -n 2 -r 3\nSpTok()\n```\n\n----------------------------------------\n\nTITLE: Dropout Mask Generation - PyTorch\nDESCRIPTION: This function generates a dropout mask. It takes a tensor, a size, and a dropout probability as input. It returns a tensor of the same type as the input, with the specified size, where elements are either 0 or a scaled value based on the dropout probability. This is used for implementing dropout in various neural network layers.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef dropout_mask(\n    x:Tensor, # Source tensor, output will be of the same type as `x` \n    sz:list, # Size of the dropout mask as `int`s\n    p:float # Dropout probability\n) -> Tensor: # Multiplicative dropout mask\n    \"Return a dropout mask of the same type as `x`, size `sz`, with probability `p` to cancel an element.\"\n    return x.new_empty(*sz).bernoulli_(1-p).div_(1-p)\n```\n\n----------------------------------------\n\nTITLE: Plotting Training Loss with recorder.plot_loss - Python\nDESCRIPTION: Displays a plot of the recorded training and validation losses across epochs for the current learning cycle. Output is a matplotlib plot. Inspect for overfitting or convergence issues.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nlearn.recorder.plot_loss()\n```\n\n----------------------------------------\n\nTITLE: WeightDropout Layer - PyTorch\nDESCRIPTION: The `WeightDropout` class wraps a PyTorch module, such as an LSTM, and applies dropout to its weights during training. It takes the module, a dropout probability, and a list of layer names to apply dropout to. It modifies the weights of the wrapped module by replacing them with dropout-applied versions and uses a forward method to apply the dropout during the forward pass.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nclass WeightDropout(Module):\n    \"A module that wraps another layer in which some weights will be replaced by 0 during training.\"\n\n    def __init__(self, \n        module:nn.Module, # Wrapped module\n        weight_p:float, # Weight dropout probability \n        layer_names:str|MutableSequence='weight_hh_l0' # Name(s) of the parameters to apply dropout to\n    ):\n        self.module,self.weight_p,self.layer_names = module,weight_p,L(layer_names)\n        for layer in self.layer_names:\n            #Makes a copy of the weights of the selected layers.\n            w = getattr(self.module, layer)\n            delattr(self.module, layer)\n            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n            setattr(self.module, layer, w.clone())\n            if isinstance(self.module, (nn.RNNBase, nn.modules.rnn.RNNBase)):\n                self.module.flatten_parameters = self._do_nothing\n\n    def _setweights(self):\n        \"Apply dropout to the raw weights.\"\n        for layer in self.layer_names:\n            raw_w = getattr(self, f'{layer}_raw')\n            if self.training: w = F.dropout(raw_w, p=self.weight_p)\n            else: w = raw_w.clone()\n            setattr(self.module, layer, w)\n\n    def forward(self, *args):\n        self._setweights()\n        with warnings.catch_warnings():\n            # To avoid the warning that comes because the weights aren't flattened.\n            warnings.simplefilter(\"ignore\", category=UserWarning)\n            return self.module(*args)\n\n    def reset(self):\n        for layer in self.layer_names:\n            raw_w = getattr(self, f'{layer}_raw')\n            setattr(self.module, layer, raw_w.clone())\n        if hasattr(self.module, 'reset'): self.module.reset()\n\n    def _do_nothing(self): pass\n```\n\n----------------------------------------\n\nTITLE: Tests: Test Dynamic UNet with non-standard input shape (Python)\nDESCRIPTION: Further tests the `DynamicUnet` by passing an input tensor with slightly different spatial dimensions (127x128) than the `img_size` specified during initialization. This implicitly checks how the model handles inputs that might require dynamic resizing or interpolation within its layers, although no specific output assertion is made.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15a_vision.models.unet.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntst = DynamicUnet(m, 5, (128,128), norm_type=None)\nx = torch.randn(2, 3, 127, 128)\ny = tst(x)\n```\n\n----------------------------------------\n\nTITLE: Demonstrate CSVLogger Usage in Python\nDESCRIPTION: Creates a synthetic `Learner`, adding `CSVLogger()` to its callbacks. It then trains the learner for 5 epochs using `fit`. This demonstrates how `CSVLogger` automatically logs the training history to the default `history.csv` file within the learner's path.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/16_callback.progress.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner(cbs=CSVLogger())\nlearn.fit(5)\n```\n\n----------------------------------------\n\nTITLE: Implementing PooledSelfAttention2d Layer\nDESCRIPTION: Defines a `PooledSelfAttention2d` layer, which applies self-attention to 2D data. It utilizes max-pooling and 1x1 convolutions for feature transformation and attention computation, similar to the original `SelfAttention`, but with pooling to reduce the spatial dimensions for computational efficiency. The `gamma` parameter is again used for the residual connection.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_47\n\nLANGUAGE: Python\nCODE:\n```\nclass PooledSelfAttention2d(Module):\n    \"Pooled self attention layer for 2d.\"\n    def __init__(self, n_channels):\n        self.n_channels = n_channels\n        self.query,self.key,self.value = [self._conv(n_channels, c) for c in (n_channels//8,n_channels//8,n_channels//2)]\n        self.out   = self._conv(n_channels//2, n_channels)\n        self.gamma = nn.Parameter(tensor([0.]))\n\n    def _conv(self,n_in,n_out):\n        return ConvLayer(n_in, n_out, ks=1, norm_type=NormType.Spectral, act_cls=None, bias=False)\n\n    def forward(self, x):\n        n_ftrs = x.shape[2]*x.shape[3]\n        f = self.query(x).view(-1, self.n_channels//8, n_ftrs)\n        g = F.max_pool2d(self.key(x),   [2,2]).view(-1, self.n_channels//8, n_ftrs//4)\n        h = F.max_pool2d(self.value(x), [2,2]).view(-1, self.n_channels//2, n_ftrs//4)\n        beta = F.softmax(torch.bmm(f.transpose(1, 2), g), -1)\n        o = self.out(torch.bmm(h, beta.transpose(1,2)).view(-1, self.n_channels//2, x.shape[2], x.shape[3]))\n        return self.gamma * o + x\n```\n\n----------------------------------------\n\nTITLE: Dataset and Normalization Verification\nDESCRIPTION: This code snippet creates datasets with normalization transforms applied. It verifies that the normalized data has mean zero and std one, and that internal states (`m`, `s`) are correctly set. It uses `zip`, `tensor`, and test assertions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_43\n\nLANGUAGE: Python\nCODE:\n```\nitems = [1,2,3,4]\n nrm = Norm()\ndsets = Datasets(items, [[neg_tfm,int2f_tfm], [neg_tfm,nrm]])\n\nx,y = zip(*dsets)\ntest_close(tensor(y).mean(), 0)\ntest_close(tensor(y).std(), 1)\ntest_eq(x, (-1,-2,-3,-4,))\ntest_eq(nrm.m, -2.5)\ntest_stdout(lambda:show_at(dsets, 1), '-2')\n\ntest_eq(dsets.m, nrm.m)\ntest_eq(dsets.norm.m, nrm.m)\ntest_eq(dsets.train.norm.m, nrm.m)\n```\n\n----------------------------------------\n\nTITLE: Exporting fastai Notebook Code to Module - nbdev_export (Python)\nDESCRIPTION: Runs nbdev_export to convert the notebook code into a reusable module. Should be called as the final step after implementing fastai transforms. Requires nbdev imported. No standard input/output, run for side-effect.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_40\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Importing nbdev Documentation Tool in Python\nDESCRIPTION: Imports the 'showdoc' function from the 'nbdev.showdoc' module. This function is commonly used within nbdev projects to display documentation for Python objects directly within notebooks. The '#|hide' directive ensures this import statement is not included in the final exported Python module.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Define range for target variable\nDESCRIPTION: Defines the range for the target variable (ratings) using `y_range`. This is used to clip the predicted ratings to a reasonable range.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ny_range = [0,5.5]\n```\n\n----------------------------------------\n\nTITLE: Setting Module Export Configuration\nDESCRIPTION: Configures the module export name and default class level for the nbdev documentation system.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/45_collab.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp collab\n#default_class_lvl 3\n```\n\n----------------------------------------\n\nTITLE: Setting Sample Batch Size and Letter List - Python\nDESCRIPTION: Defines sample batch size variable and generates a list of lowercase letters using the string module. Used as toy data or configuration for examples and tests.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nbs = 4\nletters = list(string.ascii_lowercase)\n```\n\n----------------------------------------\n\nTITLE: Creating config object\nDESCRIPTION: This snippet creates a `cfg` object by calling the `fastai_cfg()` function, which returns the configuration object. It then accesses and prints the `data` path defined by `cfg` and the directory of data using `cfg.path('data')`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/04_data.external.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncfg = fastai_cfg()\ncfg.data,cfg.path('data')\n```\n\n----------------------------------------\n\nTITLE: Get Vocabulary Length\nDESCRIPTION: Prints the length of the vocabulary used by the `DataLoaders` object. This is the number of unique tokens in the dataset.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nlen(dbunch_lm.vocab)\n```\n\n----------------------------------------\n\nTITLE: Parse Wheel METADATA Content (Python)\nDESCRIPTION: Parses the string content of the `METADATA` file (stored in the `meta` variable) using `email.parser.Parser`. It then retrieves the parsed metadata items (key-value pairs) using the `.items()` method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nm = Parser().parsestr(meta)\nm.items()\n```\n\n----------------------------------------\n\nTITLE: Defining NeptuneCallback Class in Python\nDESCRIPTION: Defines the `NeptuneCallback` class, inheriting from `fastai.learner.Callback`. This callback logs losses, metrics, model architecture summary, and optionally model weights to Neptune.ai during the fastai training loop. It requires `neptune.init()` and `neptune.create_experiment()` to be called before fitting.\nSOURCE: https://github.com/fastai/fastai/blob/main/disabled/70b_callback.neptune.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass NeptuneCallback(Callback):\n    \"Log losses, metrics, model weights, model architecture summary to neptune\"\n    order = Recorder.order+1\n    def __init__(self, log_model_weights=True, keep_experiment_running=False):\n        self.log_model_weights = log_model_weights\n        self.keep_experiment_running = keep_experiment_running\n        self.experiment = None\n\n        if neptune.project is None:\n            raise ValueError('You did not initialize project in neptune.\\n',\n                             'Please invoke `neptune.init(\"USERNAME/PROJECT_NAME\")` before this callback.')\n\n    def before_fit(self):\n        try:\n            self.experiment = neptune.get_experiment()\n        except ValueError:\n            print('No active experiment. Please invoke `neptune.create_experiment()` before this callback.')\n\n        try:\n            self.experiment.set_property('n_epoch', str(self.learn.n_epoch))\n            self.experiment.set_property('model_class', str(type(self.learn.model)))\n        except: print(f'Did not log all properties. Check properties in the {neptune.get_experiment()}.')\n\n        try:\n            with tempfile.NamedTemporaryFile(mode='w') as f:\n                with open(f.name, 'w') as g: g.write(repr(self.learn.model))\n                self.experiment.log_artifact(f.name, 'model_summary.txt')\n        except: print('Did not log model summary. Check if your model is PyTorch model.')\n\n        if self.log_model_weights and not hasattr(self.learn, 'save_model'):\n            print('Unable to log model to Neptune.\\n',\n                  'Use \"SaveModelCallback\" to save model checkpoints that will be logged to Neptune.')\n\n    def after_batch(self):\n        # log loss and opt.hypers\n        if self.learn.training:\n            self.experiment.log_metric('batch__smooth_loss', self.learn.smooth_loss)\n            self.experiment.log_metric('batch__loss', self.learn.loss)\n            self.experiment.log_metric('batch__train_iter', self.learn.train_iter)\n            for i, h in enumerate(self.learn.opt.hypers):\n                for k, v in h.items(): self.experiment.log_metric(f'batch__opt.hypers.{k}', v)\n\n    def after_epoch(self):\n        # log metrics\n        for n, v in zip(self.learn.recorder.metric_names, self.learn.recorder.log):\n            if n not in ['epoch', 'time']: self.experiment.log_metric(f'epoch__{n}', v)\n            if n == 'time': self.experiment.log_text(f'epoch__{n}', str(v))\n\n        # log model weights\n        if self.log_model_weights and hasattr(self.learn, 'save_model'):\n            if self.learn.save_model.every_epoch:\n                _file = join_path_file(f'{self.learn.save_model.fname}_{self.learn.save_model.epoch}',\n                                       self.learn.path / self.learn.model_dir, ext='.pth')\n            else:\n                _file = join_path_file(self.learn.save_model.fname,\n                                       self.learn.path / self.learn.model_dir, ext='.pth')\n            self.experiment.log_artifact(_file)\n\n    def after_fit(self):\n        if not self.keep_experiment_running:\n            try: self.experiment.stop()\n            except: print('No neptune experiment to stop.')\n        else:\n            print(f'Your experiment (id: {self.experiment.id}, name: {self.experiment.name}) is left in the running state.\\n',\n                  'You can log more data to it, like this: `neptune.log_metric()`')\n```\n\n----------------------------------------\n\nTITLE: DataFrame Memory Usage Comparison (Python)\nDESCRIPTION: This snippet compares the memory usage of an original DataFrame and a reduced DataFrame using `df_shrink`. It prints the memory usage in bytes for both DataFrames, demonstrating the memory savings achieved by the `df_shrink` function.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nprint(f'Initial Dataframe: {df.memory_usage().sum()} bytes')\nprint(f'Reduced Dataframe: {df2.memory_usage().sum()} bytes')\n```\n\n----------------------------------------\n\nTITLE: Downloading Data with untar_data\nDESCRIPTION: The `untar_data` function is a utility that downloads and extracts a file from a URL. It wraps `FastDownload.get` from the `fastdownload` library.  It defaults to downloading data into subdirectories of `~/.fastai`. It accepts parameters for the URL, optional override for `archive`, `data` keys, c_key, and a `force_download` flag to overwrite existing files. The return value is the Path to the extracted data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/04_data.external.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef untar_data(\n    url:str, # File to download\n    archive:Path=None, # Optional override for `Config`'s `archive` key\n    data:Path=None, # Optional override for `Config`'s `data` key\n    c_key:str='data', # Key in `Config` where to extract file\n    force_download:bool=False, # Setting to `True` will overwrite any existing copy of data\n    base:str=None # Directory containing config file and base of relative paths\n) -> Path: # Path to extracted file(s)\n    \"Download `url` using `FastDownload.get`\"\n    cfg = None\n    if base is None:\n        cfg = fastai_cfg()\n        # A base must be provided as FastDownload initializes a Path with it even\n        # though the config provided is ultimately used instead.\n        base = '~/.fastai'\n    d = FastDownload(cfg, module=fastai.data, archive=archive, data=data, base=base)\n    return d.get(url, force=force_download, extract_key=c_key)\n```\n\n----------------------------------------\n\nTITLE: PoolingLinearClassifier - Example Usage\nDESCRIPTION: This code shows how to use the `SentenceEncoder` and `PoolingLinearClassifier` together in a minimal example.  It creates an embedding layer, feeds data through the `SentenceEncoder` and shows how the output of the sentence encoder is used as input for the PoolingLinearClassifier.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nmod = nn.Embedding(5, 10)\ntst = SentenceEncoder(5, mod, pad_idx=0)\nx = torch.randint(1, 5, (3, 15))\nx[2,:5]=0\nout,mask = tst(x)\n\ntest_eq(out[:1], mod(x)[:1])\ntest_eq(out[2,5:], mod(x)[2,5:])\ntest_eq(mask, x==0)\n```\n\n----------------------------------------\n\nTITLE: Testing FP16 Mixed Precision Training in fastai Python\nDESCRIPTION: This snippet provides a test case for FP16 mixed precision training using a synthetic learner. It initializes a simple model, adds the `MixedPrecision` and `FP16TestCallback`, and runs a fit cycle. The assertion checks if the final loss is lower than the initial loss, indicating successful training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#|cuda\nset_seed(99, True)\nlearn = synth_learner(cbs=[MixedPrecision,FP16TestCallback], cuda=True)\nlearn.model = nn.Sequential(nn.Linear(1,1), nn.Linear(1,1)).cuda()\nlearn.opt_func = partial(SGD, mom=0.)\nlearn.splitter = lambda m: [list(m[0].parameters()), list(m[1].parameters())]\nlearn.fit(3)\nassert learn.recorder.values[-1][-1]<learn.recorder.values[0][-1]\n```\n\n----------------------------------------\n\nTITLE: Implementing Core Scheduling Functions and Wrappers in Python\nDESCRIPTION: Defines fundamental scheduling functions for linear, cosine, constant, and exponential annealing interpolations, parameterized by start, end, and pos. Corresponding Sched* functions wrap these with the _Annealer class. These schedules map the position within [0,1] to interpolated hyperparameter values for training dynamic adjustment.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef sched_lin(start, end, pos): return start + pos*(end-start)\ndef sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\ndef sched_no (start, end, pos): return start\ndef sched_exp(start, end, pos): return start * (end/start) ** pos\n\ndef SchedLin(start, end): return _Annealer(sched_lin, start, end)\ndef SchedCos(start, end): return _Annealer(sched_cos, start, end)\ndef SchedNo (start, end): return _Annealer(sched_no,  start, end)\ndef SchedExp(start, end): return _Annealer(sched_exp, start, end)\n\nSchedLin.__doc__ = \"Linear schedule function from `start` to `end`\"\nSchedCos.__doc__ = \"Cosine schedule function from `start` to `end`\"\nSchedNo .__doc__ = \"Constant schedule function with `start` value\"\nSchedExp.__doc__ = \"Exponential schedule function from `start` to `end`\"\n```\n\n----------------------------------------\n\nTITLE: Show Installation Information\nDESCRIPTION: Prints system setup information to the console, including Python version, fastai versions, torch version, NVIDIA driver information (if available), CUDA and cuDNN versions, and platform details. It also attempts to retrieve conda environment and python path and provides information if GPUs are detected. It optionally includes nvidia-smi output and suggests commands for missing dependencies if necessary. This function provides useful information for debugging.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/97_test_utils.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef show_install(show_nvidia_smi:bool=False):\n    \"Print user's setup information\"\n\n    import fastai, platform, fastprogress, fastcore\n\n    rep = []\n    opt_mods = []\n\n    rep.append([\"=== Software ===\", None])\n    rep.append([\"python\", platform.python_version()])\n    rep.append([\"fastai\", fastai.__version__])\n    rep.append([\"fastcore\", fastcore.__version__])\n    rep.append([\"fastprogress\", fastprogress.__version__])\n    rep.append([\"torch\",  torch.__version__])\n\n    # nvidia-smi\n    smi = nvidia_smi()\n    if smi:\n        match = re.findall(r'Driver Version: +(\\d+\\.\\d+)', smi)\n        if match: rep.append([\"nvidia driver\", match[0]])\n\n    available = \"available\" if torch.cuda.is_available() else \"**Not available** \"\n    rep.append([\"torch cuda\", f\"{torch.version.cuda} / is {available}\"])\n\n    # no point reporting on cudnn if cuda is not available, as it\n    # seems to be enabled at times even on cpu-only setups\n    if torch.cuda.is_available():\n        enabled = \"enabled\" if torch.backends.cudnn.enabled else \"**Not enabled** \"\n        rep.append([\"torch cudnn\", f\"{torch.backends.cudnn.version()} / is {enabled}\"])\n\n    rep.append([\"\\n=== Hardware ===\", None])\n\n    gpu_total_mem = []\n    nvidia_gpu_cnt = 0\n    if smi:\n        mem = nvidia_mem()\n        nvidia_gpu_cnt = len(ifnone(mem, []))\n\n    if nvidia_gpu_cnt: rep.append([\"nvidia gpus\", nvidia_gpu_cnt])\n\n    torch_gpu_cnt = torch.cuda.device_count()\n    if torch_gpu_cnt:\n        rep.append([\"torch devices\", torch_gpu_cnt])\n        # information for each gpu\n        for i in range(torch_gpu_cnt):\n            rep.append([f\"  - gpu{i}\", (f\"{gpu_total_mem[i]}MB | \" if gpu_total_mem else \"\") + torch.cuda.get_device_name(i)])\n    else:\n        if nvidia_gpu_cnt:\n            rep.append([f\"Have {nvidia_gpu_cnt} GPU(s), but torch can't use them (check nvidia driver)\", None])\n        else:\n            rep.append([f\"No GPUs available\", None])\n\n\n    rep.append([\"\\n=== Environment ===\", None])\n\n    rep.append([\"platform\", platform.platform()])\n\n    if platform.system() == 'Linux':\n        distro = try_import('distro')\n        if distro:\n            # full distro info\n            rep.append([\"distro\", ' '.join(distro.linux_distribution())])\n        else:\n            opt_mods.append('distro');\n            # partial distro info\n            rep.append([\"distro\", platform.uname().version])\n\n    rep.append([\"conda env\", get_env('CONDA_DEFAULT_ENV')])\n    rep.append([\"python\", sys.executable])\n    rep.append([\"sys.path\", \"\\n\".join(sys.path)])\n\n    print(\"\\n\\n```text\")\n\n    keylen = max([len(e[0]) for e in rep if e[1] is not None])\n    for e in rep:\n        print(f\"{e[0]:{keylen}}\", (f\": {e[1]}\" if e[1] is not None else \"\"))\n\n    if smi:\n        if show_nvidia_smi: print(f\"\\n{smi}\")\n    else:\n        if torch_gpu_cnt: print(\"no nvidia-smi is found\")\n        else: print(\"no supported gpus found on this system\")\n\n    print(\"```\\n\")\n\n    print(\"Please make sure to include opening/closing ``` when you paste into forums/github to make the reports appear formatted as code sections.\\n\")\n\n    if opt_mods:\n        print(\"Optional package(s) to enhance the diagnostics can be installed with:\")\n        print(f\"pip install {' '.join(opt_mods)}\")\n        print(\"Once installed, re-run this utility to get the additional information\")\n```\n\n----------------------------------------\n\nTITLE: Dummy Test for Training Set - Fastai\nDESCRIPTION: This snippet is a dummy test to ensure the code can run on the training set. It creates a new `DataLoader` from the training set, iterates through the batches, collects input, target, and output tensors. It then uses the `Interpretation` object (`interp`) to obtain the inputs, predictions, targets, decoded values, and losses.  Finally, it compares the input, target, and loss with calculations from the model and dataset.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#dummy test to ensure we can run on the training set\ninterp = Interpretation.from_learner(test_learner, ds_idx=0)\nx, y, out = [], [], []\nfor batch in test_learner.dls.train.new(drop_last=False, shuffle=False):\n    x += batch[0]\n    y += batch[1]\n    out += test_learner.model(batch[0])\nx,y,out = torch.stack(x), torch.stack(y, dim=0), torch.stack(out, dim=0)\ninps, preds, targs, decoded, losses = interp[:]\ntest_eq(inps, to_cpu(x))\ntest_eq(targs, to_cpu(y))\nloss = torch.stack([test_learner.loss_func(p,t) for p,t in zip(out,y)], dim=0)\ntest_close(losses, to_cpu(loss))\n```\n\n----------------------------------------\n\nTITLE: Plotting Training Losses (NN, Manual SGD) - Python/Matplotlib\nDESCRIPTION: Plots the list of training losses collected during the manual training loop using the neural network and the custom SGD update function with weight decay.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nplt.plot(losses);\n```\n\n----------------------------------------\n\nTITLE: Testing Zip Data with DistributedDL and TfmdDL - Python\nDESCRIPTION: Tests batch handling of zipped datasets through TfmdDL wrapped by DistributedDL, ensuring each distributed rank receives and processes its appropriate data partition. Verifies both input and target values are partitioned and gathered correctly. Requires torch, fastai's TfmdDL and DistributedDL, test_eq, and zipped input dataset.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ndl = TfmdDL(list(zip(range(50),range(100,150))), bs=12, num_workers=4)\nfor i in range(4):\n    dl1 = DistributedDL(dl, i, 4)\n    test_eq(list(dl1), [(torch.arange(i*13, i*13+12)%50,100+torch.arange(i*13, i*13+12)%50),\n                        ((torch.tensor([i*13+12])%50),100+torch.tensor([i*13+12])%50)])\n```\n\n----------------------------------------\n\nTITLE: Import necessary modules\nDESCRIPTION: This code snippet imports the required modules from the fastai library.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/41_tabular.data.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom __future__ import annotations\nfrom fastai.torch_basics import *\nfrom fastai.data.all import *\nfrom fastai.tabular.core import *\n```\n\n----------------------------------------\n\nTITLE: Examining State Holiday Values\nDESCRIPTION: Checks the unique values in the StateHoliday column to understand how holidays are encoded before converting to boolean.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nall_ftrs['StateHoliday'].unique()\n```\n\n----------------------------------------\n\nTITLE: Importing Core fastai and Sklearn Modules (Python)\nDESCRIPTION: Imports essential components from `fastai.data`, `fastai.optimizer`, `fastai.learner`, `fastai.tabular`, and `sklearn.metrics`. These imports are necessary for the definition and functionality of the `Interpretation` class and related utilities within the fastai library.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.data.all import *\nfrom fastai.optimizer import *\nfrom fastai.learner import *\nfrom fastai.tabular.core import *\nimport sklearn.metrics as skm\n```\n\n----------------------------------------\n\nTITLE: Listing Files in MNIST Dataset\nDESCRIPTION: This snippet lists the files within the MNIST dataset directory using `path.ls()`. This lists the files and directories present in the location downloaded in the previous step.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npath.ls()\n```\n\n----------------------------------------\n\nTITLE: Show a Batch of Data\nDESCRIPTION: This code displays a batch of data from the dataloaders, showing the tokenized and numericalized text sequences. `max_n` specifies the number of examples to show.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/35_tutorial.wikitext.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndls.show_batch(max_n=3)\n```\n\n----------------------------------------\n\nTITLE: Importing Modules\nDESCRIPTION: This code imports necessary modules and functions from various libraries like `fastai.torch_basics`, `fastai.data.all`, `fastai.vision.core`, and `fastdownload`. These imports provide the required functionalities for image processing, data handling, and downloading.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom __future__ import annotations\nimport uuid\nfrom fastai.torch_basics import *\nfrom fastai.data.all import *\nfrom fastai.vision.core import *\nfrom fastdownload import download_url\nfrom pathlib import Path\n```\n\n----------------------------------------\n\nTITLE: Asserting Module Parameter Presence (Python)\nDESCRIPTION: Performs assertions on standard PyTorch modules (Linear, LSTM, ReLU) to test the has_params functions correctness. Ensures that has_params returns True for parameterized layers and False for stateless activations. Requires torch.nn (PyTorch) imported as nn and the has_params function defined above.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\nassert has_params(nn.Linear(3,4))\nassert has_params(nn.LSTM(4,5,2))\nassert not has_params(nn.ReLU())\n```\n\n----------------------------------------\n\nTITLE: Documentation for most_confused method\nDESCRIPTION: This snippet generates documentation for the `most_confused` method of the `ClassificationInterpretation` class, covering its functionality, parameters, and the information it returns, such as identifying the classes most frequently confused by the model.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(ClassificationInterpretation.most_confused, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Read Specific File from Wheel using zipfile (Python)\nDESCRIPTION: Selects a specific file from the Wheel archive members list (`ls`) by its index (11). It then reads the content of this file using `zip_ref.read()`, decodes it as a string, prints the content, and finally prints the filename of the selected member.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfn = ls[11]\nf = zip_ref.read(fn)\nprint(f.decode())\nfn.filename\n```\n\n----------------------------------------\n\nTITLE: Implementing Pad_Chunk Transform for Type-aware Chunk Padding\nDESCRIPTION: A transform class that applies chunk-based padding with support for encoding, decoding, and type preservation. It calculates maximum length automatically and handles tensor type retention throughout the transformation process.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass Pad_Chunk(DisplayedTransform):\n    \"Pad `samples` by adding padding by chunks of size `seq_len`\"\n    def __init__(self, pad_idx=1, pad_first=True, seq_len=72,decode=True,**kwargs):\n        store_attr('pad_idx, pad_first, seq_len,seq_len')\n        super().__init__(**kwargs)\n    def before_call(self, b):\n        \"Set `self.max_len` before encodes\" \n        self.max_len = max([x.shape[0] for xs in b for x in xs if isinstance(x,TensorText)])\n    def __call__(self, b, **kwargs):\n        self.before_call(b)\n        return super().__call__(tuple(b), **kwargs)\n    def encodes(self, x:TensorText):\n        return pad_chunk(x,pad_idx=self.pad_idx, pad_first=self.pad_first, seq_len=self.seq_len, pad_len=self.max_len)\n    def decodes(self, o:TensorText):\n        return o[o != self.pad_idx] if self.decode else o\n```\n\n----------------------------------------\n\nTITLE: Define categorical and continuous columns, and processors\nDESCRIPTION: This code defines the categorical and continuous column names, as well as the preprocessors to be used for the tabular data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/41_tabular.data.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\ncont_names = ['age', 'fnlwgt', 'education-num']\nprocs = [Categorify, FillMissing, Normalize]\n```\n\n----------------------------------------\n\nTITLE: Module: Dynamic UNet Model (Python)\nDESCRIPTION: Constructs a U-Net model dynamically based on a given encoder architecture. It analyzes the encoder to find layers where the feature map size changes, uses hooks to capture their outputs for skip connections, and builds corresponding `UnetBlock` decoder layers. The model includes a middle convolutional block, UNet blocks for upsampling and combining with skips, and final layers to produce the desired output channels and size. It supports various configurations including blur, self-attention, and range output.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15a_vision.models.unet.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass DynamicUnet(SequentialEx):\n    \"Create a U-Net from a given architecture.\"\n    def __init__(self, encoder, n_out, img_size, blur=False, blur_final=True, self_attention=False,\n                 y_range=None, last_cross=True, bottle=False, act_cls=defaults.activation,\n                 init=nn.init.kaiming_normal_, norm_type=None, **kwargs):\n        imsize = img_size\n        sizes = model_sizes(encoder, size=imsize)\n        sz_chg_idxs = list(reversed(_get_sz_change_idxs(sizes)))\n        self.sfs = hook_outputs([encoder[i] for i in sz_chg_idxs], detach=False)\n        x = dummy_eval(encoder, imsize).detach()\n\n        ni = sizes[-1][1]\n        middle_conv = nn.Sequential(ConvLayer(ni, ni*2, act_cls=act_cls, norm_type=norm_type, **kwargs),\n                                    ConvLayer(ni*2, ni, act_cls=act_cls, norm_type=norm_type, **kwargs)).eval()\n        x = middle_conv(x)\n        layers = [encoder, BatchNorm(ni), nn.ReLU(), middle_conv]\n\n        for i,idx in enumerate(sz_chg_idxs):\n            not_final = i!=len(sz_chg_idxs)-1\n            up_in_c, x_in_c = int(x.shape[1]), int(sizes[idx][1])\n            do_blur = blur and (not_final or blur_final)\n            sa = self_attention and (i==len(sz_chg_idxs)-3)\n            unet_block = UnetBlock(up_in_c, x_in_c, self.sfs[i], final_div=not_final, blur=do_blur, self_attention=sa,\n                                   act_cls=act_cls, init=init, norm_type=norm_type, **kwargs).eval()\n            layers.append(unet_block)\n            x = unet_block(x)\n\n        ni = x.shape[1]\n        if imsize != sizes[0][-2:]: layers.append(PixelShuffle_ICNR(ni, act_cls=act_cls, norm_type=norm_type))\n        layers.append(ResizeToOrig())\n        if last_cross:\n            layers.append(MergeLayer(dense=True))\n            ni += in_channels(encoder)\n            layers.append(ResBlock(1, ni, ni//2 if bottle else ni, act_cls=act_cls, norm_type=norm_type, **kwargs))\n        layers += [ConvLayer(ni, n_out, ks=1, act_cls=None, norm_type=norm_type, **kwargs)]\n        apply_init(nn.Sequential(layers[3], layers[-2]), init)\n        #apply_init(nn.Sequential(layers[2]), init)\n        if y_range is not None: layers.append(SigmoidRange(*y_range))\n        layers.append(ToTensorBase())\n        super().__init__(*layers)\n\n    def __del__(self):\n        if hasattr(self, \"sfs\"): self.sfs.remove()\n```\n\n----------------------------------------\n\nTITLE: Importing fastai Core Dependencies - Python\nDESCRIPTION: This block imports essential modules from the fastai library, including data loading, optimizers, and core learning utilities. It also ensures Python 3 compatibility with future annotations. Dependencies are fastai, which should be installed. There are no parameters, and this enables downstream code to use fastai interfaces.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.data.all import *\nfrom fastai.optimizer import *\nfrom fastai.learner import *\n```\n\n----------------------------------------\n\nTITLE: Composing Data Pipelines for Point Data with PointScaler and ToTensor - fastai (Python)\nDESCRIPTION: Assembles a Datasets and TfmdDL pipeline for point-based image data using custom image opening and labeling. Integrates PointScaler and ToTensor in data loading. Requires Datasets, PILImage, TensorPoint, TfmdDL, PointScaler, and ToTensor. Useful for keypoint detection pipelines.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\ndef _pnt_lbl(x): return TensorPoint.create(pnts)\ndef _pnt_open(fn): return PILImage(PILImage.create(fn).resize((28,35)))\npnt_tds = Datasets([mnist_fn], [_pnt_open, [_pnt_lbl]])\npnt_tdl = TfmdDL(pnt_tds, bs=1, after_item=[PointScaler(), ToTensor()])\n```\n\n----------------------------------------\n\nTITLE: Create Verbose Callback\nDESCRIPTION: Defines a custom callback that prints the name of each event triggered during the training process. This callback helps with debugging by explicitly showing when different events are called within the training loop.  The callback inherits from the fastai `Callback` class.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/97_test_utils.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass VerboseCallback(Callback):\n    \"Callback that prints the name of each event called\"\n    def __call__(self, event_name):\n        print(event_name)\n        super().__call__(event_name)\n```\n\n----------------------------------------\n\nTITLE: Adding Date Features to Google Trends Data\nDESCRIPTION: Uses fastai's add_datepart function to expand the Date column into multiple temporal features while preserving the original date column.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ngoogletrend = add_datepart(googletrend, 'Date', drop=False)\n```\n\n----------------------------------------\n\nTITLE: Testing DataLoader with IterableDataset\nDESCRIPTION: This snippet tests `DataLoader` with a custom `IterableDataset`. It verifies the correct operation of data loading, including multiple passes and the `drop_last` parameter.  It creates a `DummyIterableDataset` class to yield data and checks that the data is loaded correctly in multiple passes, and drop_last works correctly.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nclass DummyIterableDataset(IterableDataset):\n    def __iter__(self):\n        yield from range(11)\n\nds1 = DataLoader(DummyIterableDataset(), bs=4)\n# Check it yields fine, and check we can do multiple passes\nfor i in range(3):\n    test_eq_type(L(ds1), L(tensor([0,1,2,3]),tensor([4,5,6,7]),tensor([8,9,10])))\n\n# Check `drop_last` works fine (with multiple passes, since this will prematurely terminate the iterator)\nds1 = DataLoader(DummyIterableDataset(), bs=4, drop_last=True)\nfor i in range(3):\n    test_eq_type(L(ds1), L(tensor([0,1,2,3]),tensor([4,5,6,7])))\n```\n\n----------------------------------------\n\nTITLE: Preparing Datasets for Classification with Transforms and Label Encoding (Python)\nDESCRIPTION: Applies tokenization and numericalization (using language model vocab) to inputs, and parent_label with Categorize transform to targets. Uses SortedDL for DataLoader type. Inputs: texts and transforms; Output: classification dsets object ready for DataLoader construction.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nx_tfms = [Tokenizer.from_folder(path), Numericalize(vocab=dbunch_lm.vocab)]\ndsets = Datasets(texts, [x_tfms, [parent_label, Categorize()]], splits=splits, dl_type=SortedDL)\n```\n\n----------------------------------------\n\nTITLE: Displaying Sample Data Batch - Python\nDESCRIPTION: Shows a sample batch of data from the DataLoaders, useful for verifying that the data has been processed and batched correctly before training.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndls.show_batch()\n```\n\n----------------------------------------\n\nTITLE: Setting Default Module Export Name\nDESCRIPTION: Configuration cell that sets the default export module name to 'distributed', which determines the namespace where exported functions will be placed in the library.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp distributed\n```\n\n----------------------------------------\n\nTITLE: Displaying Dataset Information\nDESCRIPTION: Calls the `show` method of a dataset to display its contents or structure for inspection purposes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_63\n\nLANGUAGE: Python\nCODE:\n```\nshow_doc(Datasets.show)\n```\n\n----------------------------------------\n\nTITLE: Installing and Upgrading fastai on Colab\nDESCRIPTION: Command to check if running in Colab and upgrade the fastai library if needed. This is hidden from normal execution.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Setting default fastai export for callback.channelslast\nDESCRIPTION: This is a marker directive indicating the module's default export, facilitating automatic documentation or module setup within fastai's infrastructure.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18c_callback.channelslast.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n#|default_exp callback.channelslast\n```\n\n----------------------------------------\n\nTITLE: Show Documentation for before_fit\nDESCRIPTION: Displays the documentation for the `before_fit` method of the `ParamScheduler` class, which is used to initialize the hyperparameter container.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(ParamScheduler.before_fit)\n```\n\n----------------------------------------\n\nTITLE: Documentation for confusion_matrix method\nDESCRIPTION: This snippet uses the `show_doc` function to generate documentation for the `confusion_matrix` method of the `ClassificationInterpretation` class. The documentation likely includes a description of the method's functionality, parameters, and return value.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(ClassificationInterpretation.confusion_matrix, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Show a batch of data\nDESCRIPTION: Displays a batch of data from the collaborative data loaders to visualize the input data format.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndls.show_batch()\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary fastai Modules\nDESCRIPTION: This code imports necessary modules from the fastai library. Specifically, it imports all modules from `fastai.basics` and enables annotations for forward compatibility.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom __future__ import annotations\nfrom fastai.basics import *\n```\n\n----------------------------------------\n\nTITLE: Using before_batch_cb Decorator\nDESCRIPTION: This code utilizes the `before_batch_cb` decorator to create a callback function that modifies the data before each batch. The decorated `cb` function adds 1000 to `xb` and subtracts 1000 from `yb`. This example provides a more convenient method for data modification using the decorator.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\n@before_batch_cb\ndef cb(self, xb, yb): return xb+1000,yb-1000\n```\n\n----------------------------------------\n\nTITLE: Padding Tensor\nDESCRIPTION: This function pads a given tensor `t` with zero tensors to match a specified batch size `bs`. It checks if the tensor's first dimension (sequence length) is less than the batch size. If it is, it concatenates the tensor with a zero tensor to achieve the desired batch size. The input is a tensor `t` and batch size `bs` and the output is padded tensor.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef _pad_tensor(t:Tensor, bs:int) -> Tensor:\n    if t.size(0) < bs: return torch.cat([t, t.new_zeros(bs-t.size(0), *t.shape[1:])])\n    return t\n```\n\n----------------------------------------\n\nTITLE: Calculating Embedding Size Rule in Python\nDESCRIPTION: Defines a heuristic function to determine the embedding size for a categorical variable based on its cardinality `n_cat`. The rule returns the minimum of 600 or 1.6 times the cardinality raised to the power 0.56. This approach offers a good baseline embedding dimension size for categorical variables used in tabular models in FastAI, promoting a balance between underfitting and overfitting.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/42_tabular.model.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef emb_sz_rule(\n    n_cat:int # Cardinality of a category\n) -> int:\n    \"Rule of thumb to pick embedding size corresponding to `n_cat`\"\n    return min(600, round(1.6 * n_cat**0.56))\n```\n\n----------------------------------------\n\nTITLE: Implementing Hamming Loss Wrapper for Multi-label Classification in Python\nDESCRIPTION: A wrapper function that adapts scikit-learn's hamming_loss for use in fastai's multi-label classification tasks, providing threshold customization and sigmoid activation options.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef HammingLossMulti(thresh=0.5, sigmoid=True, labels=None, sample_weight=None):\n    \"Hamming loss for multi-label classification problems\"\n    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n    return skm_to_fastai(skm.hamming_loss, thresh=thresh, activation=activation, flatten=False,\n                         sample_weight=sample_weight)\n```\n\n----------------------------------------\n\nTITLE: Creating TensorImageBase Subclass with Custom Show Method in Python\nDESCRIPTION: Defines TensorImageBase as a subclass of TensorBase specialized for image tensors, incorporating default image display parameters from ArrayImageBase. It overrides the show method to display images using the fastai show_image utility with customizable context and keyword arguments.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nclass TensorImageBase(TensorBase):\n    _show_args = ArrayImageBase._show_args\n    def show(self, ctx=None, **kwargs):\n        return show_image(self, ctx=ctx, **{**self._show_args, **kwargs})\n```\n\n----------------------------------------\n\nTITLE: Implementing Explained Variance Score for Regression from Scikit-learn\nDESCRIPTION: Function that creates a fastai metric using scikit-learn's explained_variance_score, adapting it to fastai's metric interface for regression tasks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndef ExplainedVariance(sample_weight=None):\n    \"Explained variance between predictions and targets\"\n    return skm_to_fastai(skm.explained_variance_score, is_class=False, sample_weight=sample_weight)\n```\n\n----------------------------------------\n\nTITLE: Getting Files with Optional Recursion and Extension Filtering\nDESCRIPTION: Function to retrieve files from a path with support for recursive searching, extension filtering, and folder restrictions. Returns a list of file paths that match the specified criteria.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef get_files(path, extensions=None, recurse=True, folders=None, followlinks=True):\n    \"Get all the files in `path` with optional `extensions`, optionally with `recurse`, only in `folders`, if specified.\"\n    path = Path(path)\n    folders=L(folders)\n    extensions = setify(extensions)\n    extensions = {e.lower() for e in extensions}\n    if recurse:\n        res = []\n        for i,(p,d,f) in enumerate(os.walk(path, followlinks=followlinks)): # returns (dirpath, dirnames, filenames)\n            if len(folders) !=0 and i==0: d[:] = [o for o in d if o in folders]\n            else:                         d[:] = [o for o in d if not o.startswith('.')]\n            if len(folders) !=0 and i==0 and '.' not in folders: continue\n            res += _get_files(p, f, extensions)\n    else:\n        f = [o.name for o in os.scandir(path) if o.is_file()]\n        res = _get_files(path, f, extensions)\n    return L(res)\n```\n\n----------------------------------------\n\nTITLE: Extracting Movie Weights\nDESCRIPTION: This code extracts the weights (latent factors) for the `top_movies` using the trained model. The `learn.model.weight()` method retrieves the learned weights for the specified movies, crucial for understanding the model's learned representations. The `is_item=True` indicates retrieving item weights.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmovie_w = learn.model.weight(top_movies, is_item=True)\n```\n\n----------------------------------------\n\nTITLE: Defining BaseLoss Class in Fastai\nDESCRIPTION: This code defines the `BaseLoss` class, which serves as a base class for custom loss functions in fastai. It flattens input and target tensors, converts target tensors to float if specified, and allows specifying an axis for losses like softmax. The class takes a PyTorch-compatible loss function as input and provides methods for setting the reduction style and moving the loss function to a specified device.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass BaseLoss():\n    \"Same as `loss_cls`, but flattens input and target.\"\n    activation=decodes=noops\n    def __init__(self, \n        loss_cls, # Uninitialized PyTorch-compatible loss\n        *args,\n        axis:int=-1, # Class axis\n        flatten:bool=True, # Flatten `inp` and `targ` before calculating loss\n        floatify:bool=False, # Convert `targ` to `float`\n        is_2d:bool=True, # Whether `flatten` keeps one or two channels when applied\n        **kwargs\n    ):\n        store_attr(\"axis,flatten,floatify,is_2d\")\n        self.func = loss_cls(*args,**kwargs)\n        self.func.__annotations__ = typing.get_type_hints(self.func, globalns=globals(), localns=locals()) # used to prevent unpicklable loss functions (https://github.com/fastai/fastai/issues/3901)\n        functools.update_wrapper(self, self.func)\n\n    def __repr__(self) -> str: return f\"FlattenedLoss of {self.func}\"\n    \n    @property\n    def reduction(self) -> str: return self.func.reduction\n    \n    @reduction.setter\n    def reduction(self, v:str):\n        \"Sets the reduction style (typically 'mean', 'sum', or 'none')\" \n        self.func.reduction = v\n\n    def _contiguous(self, x:Tensor) -> TensorBase:\n        \"Move `self.axis` to the last dimension and ensure tensor is contigous for `Tensor` otherwise just return\"\n        return TensorBase(x.transpose(self.axis,-1).contiguous()) if isinstance(x,torch.Tensor) else x\n\n    def __call__(self, \n        inp:Tensor|MutableSequence, # Predictions from a `Learner`\n        targ:Tensor|MutableSequence, # Actual y label\n        **kwargs\n    ) -> TensorBase: # `loss_cls` calculated on `inp` and `targ`\n        inp,targ  = map(self._contiguous, (inp,targ))\n        if self.floatify and targ.dtype!=torch.float16: targ = targ.float()\n        if targ.dtype in [torch.int8, torch.int16, torch.int32]: targ = targ.long()\n        if self.flatten: inp = inp.view(-1,inp.shape[-1]) if self.is_2d else inp.view(-1)\n        return self.func.__call__(inp, targ.view(-1) if self.flatten else targ, **kwargs)\n    \n    def to(self, device:torch.device):\n        \"Move the loss function to a specified `device`\"\n        if isinstance(self.func, nn.Module): self.func.to(device)\n```\n\n----------------------------------------\n\nTITLE: Defining TensorPointCreate Transform (Python)\nDESCRIPTION: Creates `TensorPointCreate`, a fastai `Transform` instance that wraps the `TensorPoint.create` classmethod. This integrates point creation into fastai data pipelines. It assigns `MSELossFlat` as a suitable default loss function for point-based regression tasks and replaces the original `TensorPoint.create` with this transform instance.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n#|export\nTensorPointCreate = Transform(TensorPoint.create)\nTensorPointCreate.loss_func = MSELossFlat()\nTensorPoint.create = TensorPointCreate\n```\n\n----------------------------------------\n\nTITLE: SentenceEncoder - Example Usage\nDESCRIPTION: This code demonstrates the usage of `SentenceEncoder` with a simple embedding layer as the inner module. It creates an embedding layer, initializes a `SentenceEncoder` instance, generates random input, and then runs a forward pass through the encoder. The results are compared to ensure the output of the `SentenceEncoder` is aligned with the expected output of the module used within it, particularly in regions with no padding and the padding.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmod = nn.Embedding(5, 10)\ntst = SentenceEncoder(5, mod, pad_idx=0)\nx = torch.randint(1, 5, (3, 15))\nx[2,:5]=0\nout,mask = tst(x)\n\ntest_eq(out[:1], mod(x)[:1])\ntest_eq(out[2,5:], mod(x)[2,5:])\ntest_eq(mask, x==0)\n```\n\n----------------------------------------\n\nTITLE: Upgrading fastai on Colab (Python/Shell)\nDESCRIPTION: Installs or upgrades the fastai library using pip, specifically checking if running in a Google Colab environment using a shell command executed from Python. This ensures the latest version is available.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20b_tutorial.distributed.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n#| eval: false\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Sequential Spacy Tokenization - Jupyter\nDESCRIPTION: This snippet uses `%%timeit` to benchmark the execution time of applying the `spacy_tok` function sequentially to all strings in the `ss` list using the custom `apply` function. This measures the baseline performance of Spacy tokenization without parallelism. Results stored globally in `t`. Runs for 3 loops (`-r 3`). Note: `-n` is not specified, implying default iterations per loop.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n%%timeit -r 3\nglobal t\nt = apply(spacy_tok, ss)\n```\n\n----------------------------------------\n\nTITLE: Creating time-based features for event sequences in Python\nDESCRIPTION: Adds elapsed time features for specified events, calculating time before and after events and including 7-day rolling sums forward and backward.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef add_elapsed_times(df, field_names, date_field, base_field):\n    \"Add in `df` for each event in `field_names` the elapsed time according to `date_field` grouped by `base_field`\"\n    field_names = list(L(field_names))\n    #Make sure date_field is a date and base_field a bool\n    df[field_names] = df[field_names].astype('bool')\n    make_date(df, date_field)\n\n    work_df = df[field_names + [date_field, base_field]]\n    work_df = work_df.sort_values([base_field, date_field])\n    work_df = _get_elapsed(work_df, field_names, date_field, base_field, 'After')\n    work_df = work_df.sort_values([base_field, date_field], ascending=[True, False])\n    work_df = _get_elapsed(work_df, field_names, date_field, base_field, 'Before')\n\n    for a in ['After' + f for f in field_names] + ['Before' + f for f in field_names]:\n        work_df[a] = work_df[a].fillna(0).astype(int)\n\n    for a,s in zip([True, False], ['_bw', '_fw']):\n        work_df = work_df.set_index(date_field)\n        tmp = (work_df[[base_field] + field_names].sort_index(ascending=a)\n                      .groupby(base_field).rolling(7, min_periods=1).sum())\n        if base_field in tmp: tmp.drop(base_field, axis=1,inplace=True)\n        tmp.reset_index(inplace=True)\n        work_df.reset_index(inplace=True)\n        work_df = work_df.merge(tmp, 'left', [date_field, base_field], suffixes=['', s])\n    work_df.drop(field_names, axis=1, inplace=True)\n    return df.merge(work_df, 'left', [date_field, base_field])\n```\n\n----------------------------------------\n\nTITLE: Save Language Model Encoder\nDESCRIPTION: Saves the encoder part of the fine-tuned language model. The encoder is responsible for creating and updating the hidden state, which captures information about the sentence up to that point. This encoder will be used for the classifier.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nlearn.save_encoder('fine_tuned_enc')\n```\n\n----------------------------------------\n\nTITLE: Defining Callback Name\nDESCRIPTION: Shows how a callback's name is generated based on the class name.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntest_eq(TstCallback().name, 'tst')\nclass ComplicatedNameCallback(Callback): pass\ntest_eq(ComplicatedNameCallback().name, 'complicated_name')\n```\n\n----------------------------------------\n\nTITLE: Set default export target\nDESCRIPTION: This code snippet sets the default export target for the nbdev library.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/41_tabular.data.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp tabular.data\n```\n\n----------------------------------------\n\nTITLE: Printing Continuous and Categorical Column Names (Python)\nDESCRIPTION: This snippet prints the names of continuous and categorical columns, assuming they have been previously identified and stored in the `cont_names` and `cat_names` variables respectively. It's intended for debugging and inspecting the results of feature engineering or data preprocessing steps.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(f'cont_names: {cont_names}\\ncat_names: {cat_names}')\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Callback Subclassing\nDESCRIPTION: Demonstrates one way to define a callback by subclassing the Callback class and implementing event-specific methods. This allows custom behavior to be injected into the training loop.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass _T(Callback):\n    def call_me(self): return \"maybe\"\ntest_eq(_T()(\"call_me\"), \"maybe\")\n```\n\n----------------------------------------\n\nTITLE: ArrayImageBW Subclass for Black-and-White Images in Python\nDESCRIPTION: Subclass `ArrayImageBW` extends `ArrayImage` with a grayscale colormap default (`Greys`) for visualization. This allows automatic grayscale rendering when the `show` method is called on such arrays, useful for black and white or single-channel images.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass ArrayImageBW(ArrayImage):\n    \"An array representing an image\"\n    _show_args = {'cmap':'Greys'}\n```\n\n----------------------------------------\n\nTITLE: Testing `chunked` Utility Function - Python\nDESCRIPTION: This snippet contains multiple `test_eq` assertions using fastai's testing utilities. It verifies the correctness of the `chunked` function (likely imported from fastai.core or similar) by checking if it correctly divides an input range into a specified number of chunks, handling cases where the division is not exact. This ensures the function used for creating batches is working as expected.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ntest_eq(chunked(range(12),n_chunks=4), [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]])\ntest_eq(chunked(range(11),n_chunks=4), [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]])\ntest_eq(chunked(range(10),n_chunks=4), [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]])\ntest_eq(chunked(range( 9),n_chunks=3), [[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n```\n\n----------------------------------------\n\nTITLE: Testing Picklability of fastai Loss Functions in Python\nDESCRIPTION: Tests whether various loss function classes from `fastai` and custom ones like `CombinedLoss` can be successfully serialized using Python's `pickle` module. This is crucial for saving and loading models or learners that include these loss functions. Requires Python's `pickle` module and the respective loss classes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\n# Tests to catch future changes to pickle which cause some loss functions to be 'unpicklable'.\n# This causes problems with `Learner.export` as the model can't be pickled with these particular loss funcitons.\n\nlosses_picklable = [\n (BCELossFlat(), True),\n (BCEWithLogitsLossFlat(), True),\n (CombinedLoss(), True),\n (CrossEntropyLossFlat(), True),\n (DiceLoss(), True),\n (FocalLoss(), True),\n (FocalLossFlat(), True),\n (L1LossFlat(), True),\n (LabelSmoothingCrossEntropyFlat(), True),\n (LabelSmoothingCrossEntropy(), True),\n (MSELossFlat(), True),\n]\n\nfor loss, picklable in losses_picklable:\n    try:\n        pickle.dumps(loss, protocol=2)\n    except (pickle.PicklingError, TypeError) as e:\n        if picklable:\n            # Loss was previously picklable but isn't currently\n            raise e\n```\n\n----------------------------------------\n\nTITLE: Displaying a Batch of Data\nDESCRIPTION: This uses the `show_batch` method of the `DataLoaders` object (`dls`) to display a sample of the data.  It is used to visualize how the text data is preprocessed (tokenized) before training, which provides insight into special tokens added during tokenization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndls.show_batch()\n```\n\n----------------------------------------\n\nTITLE: Displaying Decoded Train and Validation Examples Using show_at (Python)\nDESCRIPTION: Displays the decoded text for the first training and validation examples using fastai's show_at utility. Useful for data inspection and verifying transformation reversibility.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nshow_at(tls.train, 0)\n```\n\n----------------------------------------\n\nTITLE: Import Necessary Modules\nDESCRIPTION: Imports modules from fastai and other libraries for use in the testing utilities, including functionality for data handling, optimization, learner creation, callbacks, and tensor datasets. This sets up the necessary dependencies for the functions defined later in the file.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/97_test_utils.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.imports import *\nfrom fastai.data.all import *\nfrom fastai.optimizer import *\nfrom fastai.learner import *\nfrom fastai.callback.core import *\nfrom torch.utils.data import TensorDataset\n```\n\n----------------------------------------\n\nTITLE: Loading SegmentationDataLoaders from label function using fastai in Python\nDESCRIPTION: Example of using SegmentationDataLoaders.from_label_func to create segmentation DataLoaders by providing image file names, a label function to fetch masks, and segmentation codes mapping labels to indices. The validation set is generated randomly by valid_pct for training/validation split. Requires fastai and external dependencies like numpy for codes loading and path utilities. Outputs a segmentation DataLoader prepared for modeling segmentation tasks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/08_vision.data.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.CAMVID_TINY)\nfnames = get_image_files(path/'images')\ndef label_func(x): return path/'labels'/f'{x.stem}_P{x.suffix}'\ncodes = np.loadtxt(path/'codes.txt', dtype=str)\n    \ndls = SegmentationDataLoaders.from_label_func(path, fnames, label_func, codes=codes)\n```\n\n----------------------------------------\n\nTITLE: Testing BCELossFlat Functionality - Fastai/PyTorch\nDESCRIPTION: This snippet tests the `BCELossFlat` function. It initializes the loss, performs a forward pass with sigmoid-activated output and binary integer targets, and confirms that the standard `nn.BCELoss` would fail with these shapes, validating the flattening behavior of `BCELossFlat`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ntst = BCELossFlat()\noutput = torch.sigmoid(torch.randn(32, 5, 10))\ntarget = torch.randint(0,2,(32, 5, 10))\n_ = tst(output, target)\ntest_fail(lambda x: nn.BCELoss()(output,target))\n```\n\n----------------------------------------\n\nTITLE: Installing fastai for Development (Bash)\nDESCRIPTION: These commands clone the fastai repository and install it in editable mode using pip, including development dependencies. This setup is used for contributing to or developing on the fastai library itself, requiring prior PyTorch installation.\nSOURCE: https://github.com/fastai/fastai/blob/main/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/fastai/fastai\npip install -e \"fastai[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Creating Average Pooling Layer (Python)\nDESCRIPTION: Returns a PyTorch Average Pooling layer (`nn.AvgPool1d`, `nn.AvgPool2d`, or `nn.AvgPool3d`) based on the specified number of dimensions (`ndim`). Parameters include kernel size (`ks`), stride, padding, and ceil mode. Asserts that `ndim` is between 1 and 3.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef AvgPool(ks=2, stride=None, padding=0, ndim=2, ceil_mode=False):\n    \"nn.AvgPool layer for `ndim`\"\n    assert 1 <= ndim <= 3\n    return getattr(nn, f\"AvgPool{ndim}d\")(ks, stride=stride, padding=padding, ceil_mode=ceil_mode)\n```\n\n----------------------------------------\n\nTITLE: Define FakeRecords and TestTracker\nDESCRIPTION: This code defines two helper callbacks for testing `TrackerCallback`. `FakeRecords` injects predefined values for the monitored metric into the recorder. `TestTracker` records the best values and new best flags during training for later verification.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nclass FakeRecords(Callback):\n    order=51\n    def __init__(self, monitor, values): self.monitor,self.values = monitor,values\n        \n    def before_fit(self):   self.idx = list(self.recorder.metric_names[1:]).index(self.monitor)\n    def after_epoch(self): self.recorder.values[-1][self.idx] = self.values[self.epoch]\n        \nclass TestTracker(Callback):\n    order=61\n    def before_fit(self): self.bests,self.news = [],[]\n    def after_epoch(self): \n        self.bests.append(self.tracker.best)\n        self.news.append(self.tracker.new_best)\n```\n\n----------------------------------------\n\nTITLE: Displaying fastai Installation Details (Python)\nDESCRIPTION: Imports the `fastai.test_utils` module and calls `show_install(1)` to print detailed information about the current fastai library version and system environment. This output is crucial when reporting bugs to help diagnose installation-specific issues.\nSOURCE: https://github.com/fastai/fastai/blob/main/CONTRIBUTING.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport fastai.test_utils; fastai.test_utils.show_install(1)\n```\n\n----------------------------------------\n\nTITLE: DataLoader Aware `to_detach` Method in Learner (Python)\nDESCRIPTION: This snippet shows how the `to_detach` method in `Learner` is designed to use the `to_detach` method of the current `DataLoader` if available. If the `DataLoader` does not have the method, the default `to_detach` is used.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_59\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Learner.to_detach)\n```\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nlearn = synth_learner()\ntest_eq(learn.to_detach(Tensor([123])),Tensor([123]))\nlearn.dl = learn.dls[0]\ntest_eq(learn.to_detach(Tensor([123])),Tensor([123]))\nlearn.dl.to_detach = lambda b,cpu,gather: b-100\ntest_eq(learn.to_detach(Tensor([123.])),Tensor([23.]))\n```\n\n----------------------------------------\n\nTITLE: Importing Documentation Tools\nDESCRIPTION: Imports showdoc utility from nbdev for documentation purposes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Re-instantiating Neural Network Model for New Training - PyTorch/Python\nDESCRIPTION: Creates another new instance of the `Mnist_NN` model and moves it to the GPU. This resets the model's weights and biases, preparing it for a new training run with a different optimization method.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nmodel = Mnist_NN().cuda()\n```\n\n----------------------------------------\n\nTITLE: Import Libraries\nDESCRIPTION: Imports necessary libraries from the fastai.text and nbdev modules. fastai.text provides tools for NLP tasks, and nbdev.showdoc is used for displaying documentation.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.text.all import *\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Alternative Log Function for TensorImage and Spatial Data\nDESCRIPTION: Logs images, points, and bounding boxes to TensorBoard, displaying both the original samples and model outputs for comparison.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n@dispatch\ndef tensorboard_log(x:TensorImage, y: TensorImageBase|TensorPoint|TensorBBox, samples, outs, writer, step):\n    fig,axs = get_grid(len(samples), return_fig=True, double=True)\n    for i in range(2):\n        axs[::2] = [b.show(ctx=c) for b,c in zip(samples.itemgot(i),axs[::2])]\n    for x in [samples,outs]:\n        axs[1::2] = [b.show ctx=c for b,c in zip(x.itemgot(0),axs[1::2])]\n```\n\n----------------------------------------\n\nTITLE: Using ShortEpochCallback in model training\nDESCRIPTION: Demonstrates the use of ShortEpochCallback during model training with the fastai learner, with one instance stopping early in training and another disabling early validation stopping.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18a_callback.training.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nlearn = synth_learner()\nlearn.fit(1, cbs=ShortEpochCallback())\n```\n\nLANGUAGE: Python\nCODE:\n```\nlearn = synth_learner()\nlearn.fit(1, cbs=ShortEpochCallback(short_valid=False))\n```\n\n----------------------------------------\n\nTITLE: Separable Convolution Block Creation in Python\nDESCRIPTION: This function is a wrapper around `ResBlock` configured to create a block utilizing depthwise separable convolutions. It sets the `reduction` parameter and the intermediate channel size `nh2`. Crucially, it sets the `dw=True` argument to enable depthwise convolutions within the `ResBlock`'s convolutional path. Other keyword arguments are passed through to the `ResBlock` constructor.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_73\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef SeparableBlock(expansion, ni, nf, reduction=16, stride=1, base_width=4, **kwargs):\n    return ResBlock(expansion, ni, nf, stride=stride, reduction=reduction, nh2=nf*2, dw=True, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Base Callback Class for TensorBoard Integration\nDESCRIPTION: Defines a base class for TensorBoard callbacks, managing logging setup, feature extraction hooks, and embedding export during training or inference. Handles invoking hooks, writing embeddings, and clean-up.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nclass TensorBoardBaseCallback(Callback):\n    order = Recorder.order+1\n    \"Base class for tensorboard callbacks\"\n    def __init__(self): self.run_projector = False\n        \n    def after_pred(self):\n        if self.run_projector: self.feat = _add_projector_features(self.learn, self.h, self.feat)\n    \n    def after_validate(self):\n        if not self.run_projector: return\n        self.run_projector = False\n        self._remove()\n        _write_projector_embedding(self.learn, self.writer, self.feat)\n            \n    def after_fit(self): \n        if self.run: self.writer.close()\n        \n    def _setup_projector(self):\n        self.run_projector = True\n        self.h = hook_output(self.learn.model[1][1] if not self.layer else self.layer)\n        self.feat = {}\n        \n    def _setup_writer(self): self.writer = SummaryWriter(log_dir=self.log_dir)\n    def __del__(self): self._remove()\n    def _remove(self):\n        if getattr(self, 'h', None): self.h.remove()\n```\n\n----------------------------------------\n\nTITLE: Downloading LSUN Bedroom Dataset using fastai in Python\nDESCRIPTION: Downloads and extracts a sample of the LSUN bedroom images dataset via fastai's `untar_data` utility using a predefined URL identifier. The dataset path is stored for subsequent data loading.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-wgan.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.LSUN_BEDROOMS)\n```\n\n----------------------------------------\n\nTITLE: Defining TensorPoint Class (Python)\nDESCRIPTION: Defines `TensorPoint`, a class inheriting from `fastai.torch_basics.TensorBase`. It represents point coordinates associated with an image, stored as a floating-point Tensor of shape `(n, 2)`. It provides a `create` classmethod to construct instances from lists or arrays and a `show` method for visualizing the points on a plot context (matplotlib axes), using scatter plot defaults.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass TensorPoint(TensorBase):\n    \"Basic type for points in an image\"\n    _show_args = dict(s=10, marker='.', c='r')\n\n    @classmethod\n    def create(cls, t, img_size=None)->None:\n        \"Convert an array or a list of points `t` to a `Tensor`\"\n        return cls(tensor(t).view(-1, 2).float(), img_size=img_size)\n\n    def show(self, ctx=None, **kwargs):\n        if 'figsize' in kwargs: del kwargs['figsize']\n        x = self.view(-1,2)\n        ctx.scatter(x[:, 0], x[:, 1], **{**self._show_args, **kwargs})\n        return ctx\n```\n\n----------------------------------------\n\nTITLE: Converting State Holiday to Boolean\nDESCRIPTION: Transforms the StateHoliday column from categorical values to a boolean indicator for any holiday type.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nall_ftrs.StateHoliday = all_ftrs.StateHoliday!='0'\n```\n\n----------------------------------------\n\nTITLE: Installing fastai on Colab\nDESCRIPTION: This snippet installs or upgrades the fastai library using pip.  It checks if the current environment is a Google Colab instance (by checking for the existence of the `/content` directory) and then installs fastai. The `-Uqq` flag ensures a quiet upgrade.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#| eval: false\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Plotting SGDR Schedules with plot_sched\nDESCRIPTION: After performing SGDR training, this snippet uses `plot_sched` to visualize the learning rate schedule. This provides a visual confirmation that the cosine annealing schedule is correctly applied over the SGDR cycles.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nlearn.recorder.plot_sched()\n```\n\n----------------------------------------\n\nTITLE: PoolFlatten Sequential Layer - Python\nDESCRIPTION: Creates a composite sequential module that performs adaptive pooling (based on pool_type) followed by flattening. Constructor argument pool_type chooses the pooling variant. Depends on nn.Sequential, adaptive_pool, and Flatten layer defined above. Input is 4D tensor, output is 2D feature vector for dense head layers.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nclass PoolFlatten(nn.Sequential):\n    \"Combine `nn.AdaptiveAvgPool2d` and `Flatten`.\"\n    def __init__(self, pool_type=PoolType.Avg): super().__init__(adaptive_pool(pool_type)(1), Flatten())\n```\n\n----------------------------------------\n\nTITLE: Implementing Spearman Correlation Coefficient for Regression in PyTorch\nDESCRIPTION: Function that creates a fastai metric for Spearman correlation coefficient using scipy's spearmanr function, wrapped in AccumMetric for batch computation in regression tasks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n@delegates(AccumMetric)\ndef SpearmanCorrCoef(dim_argmax=None, axis=0, nan_policy='propagate', **kwargs):\n    \"Spearman correlation coefficient for regression problem\"\n    def spearmanr(a,b=None,**kwargs): return scs.spearmanr(a,b,**kwargs)[0]\n    return AccumMetric(partial(spearmanr, axis=axis, nan_policy=nan_policy),\n                       invert_arg=False, dim_argmax=dim_argmax, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Importing Visualization Components in Python\nDESCRIPTION: Imports visualization functionality from the fastai vision core module for displaying transformed batches.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n#Just for visuals\nfrom fastai.vision.core import *\n```\n\n----------------------------------------\n\nTITLE: Initializing Fastai Learner (Python)\nDESCRIPTION: This snippet demonstrates how to initialize a fastai `Learner` object, which is the core class for training models in fastai. It first creates a `DataLoaders` object from existing PyTorch `train_loader` and `test_loader`, and then passes this along with a PyTorch model (`Net`), a PyTorch loss function (`F.nll_loss`), a PyTorch optimizer (`Adam`), and a fastai metric (`accuracy`) to the `Learner` constructor. This setup replaces the need for a manual training loop, offloading epoch iteration, gradient updates, and metric calculation to fastai.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_pytorch.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndata = DataLoaders(train_loader, test_loader)\nlearn = Learner(data, Net(), loss_func=F.nll_loss, opt_func=Adam, metrics=accuracy)\n```\n\n----------------------------------------\n\nTITLE: Installing fastai in Google Colab - Python\nDESCRIPTION: Checks if running in the Colab environment and installs/upgrades fastai using pip if necessary. Designed for use in notebook and cloud environments to ensure the correct fastai version. No function definitions here; it's a cell-based command invoking shell and pip.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Splitting a Tabular Data\nDESCRIPTION: This test case demonstrates splitting a tabular dataset into training and validation sets and running the same TabularGPU pipeline. It checks the datatypes of categorical variables, existence of missing values indicators.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nprocs = [Normalize, Categorify, FillMissing, noop]\ndf = cudf.from_pandas(pd.DataFrame({'a':[0,1,2,1,1,2,0], 'b':[0,np.nan,1,1,2,3,4], 'c': ['b','a','b','a','a','b','a']}))\nto = TabularGPU(df, procs, cat_names='a', cont_names='b', y_names='c', splits=[[0,1,4,6], [2,3,5]])\n\ntest_eq(to.cat_names, ['a', 'b_na'])\ntest_eq(to.a.to_array(), [1,2,2,1,0,2,0])\ntest_eq(to.a.dtype,int)\ntest_eq(to.b_na.to_array(), [1,2,1,1,1,1,1])\ntest_eq(to.c.to_array(), [1,0,0,0,1,0,1])\n```\n\n----------------------------------------\n\nTITLE: Registering Tensor Operation Functions for Metadata Consistency in Python\nDESCRIPTION: Registers multiple PyTorch tensor operations (indexing, comparison, arithmetic, matrix multiplication) with TensorBase for specialized handling when operands include TensorMask and TensorImageBase subclasses. Additional einsum registrations ensure that operations between TensorMask and TensorImageBase produce consistent subclass outputs, preserving metadata and subtype behavior.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nfor o in Tensor.__getitem__, Tensor.__ne__,Tensor.__eq__,Tensor.add,Tensor.sub,Tensor.mul,Tensor.div,Tensor.__rsub__,Tensor.__radd__,Tensor.matmul,Tensor.bmm:\n    TensorBase.register_func(o, TensorMask, TensorImageBase)\n    TensorBase.register_func(o, TensorImageBase, TensorMask)\n\nTensorMask.register_func(torch.einsum, str, TensorImageBase, TensorMask)\nTensorMask.register_func(torch.einsum, str, TensorMask, TensorImageBase)\n```\n\n----------------------------------------\n\nTITLE: Defining Events with L.split and mk_class\nDESCRIPTION: Defines all possible events using `L.split` for string splitting, and `mk_class` to create attributes for tab completion and typo-proofing. It also creates the `event` class for easy use of the events.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#|export\n_events = L.split('after_create before_fit before_epoch before_train before_batch after_pred after_loss \\\n    before_backward after_cancel_backward after_backward before_step after_cancel_step after_step \\\n    after_cancel_batch after_batch after_cancel_train after_train before_validate after_cancel_validate \\\n    after_validate after_cancel_epoch after_epoch after_cancel_fit after_fit')\n\nmk_class('event', **_events.map_dict(),\n         doc=\"All possible events as attributes to get tab-completion and typo-proofing\")\n```\n\n----------------------------------------\n\nTITLE: Load Fine-tuned Language Model (Full)\nDESCRIPTION: Loads the fully fine-tuned language model.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nlearn.load('fine_tuned');\n```\n\n----------------------------------------\n\nTITLE: Load Fine-tuned Language Model Head\nDESCRIPTION: Loads the previously saved head of the fine-tuned language model.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nlearn.load('fit_head');\n```\n\n----------------------------------------\n\nTITLE: Upgrading Fastai on Colab via Shell Command in Python\nDESCRIPTION: Executes a shell command to upgrade the Fastai library on Google Colab instances if the environment is detected. Requires network access and appropriate pip permissions. Outputs standard installation progress or errors to the notebook cell output.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Setting the nbdev Export Module Name\nDESCRIPTION: This line specifies the default export module name for the nbdev library.  It indicates that the code in this file should be exported under the module `callback.hook`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp callback.hook\n```\n\n----------------------------------------\n\nTITLE: Fit One Cycle (Classifier - First Layer Group)\nDESCRIPTION: Fine-tunes the classifier for one cycle. The first stage only trains the last layers.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7, 0.8))\n```\n\n----------------------------------------\n\nTITLE: Importing TensorBoard and Torch Utilities\nDESCRIPTION: Imports necessary modules for TensorBoard operations and tensorboard callback functionalities, such as SummaryWriter from torch and specific fastai hooks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nimport tensorboard\nfrom torch.utils.tensorboard import SummaryWriter\nfrom fastai.callback.fp16 import ModelToHalf\nfrom fastai.callback.hook import hook_output\n```\n\n----------------------------------------\n\nTITLE: Importing Base fastai Dependencies\nDESCRIPTION: Imports necessary components from the `fastai.basics` and `fastai.learner` modules, which are fundamental to the fastai library.  Specifically, it imports `*` from `fastai.basics` and the `Callback` class from `fastai.learner`. These dependencies are necessary for defining and using custom callbacks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70d_callback.comet.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nimport tempfile\n\nfrom fastai.basics import *\nfrom fastai.learner import Callback\n```\n\n----------------------------------------\n\nTITLE: Creating _round_to_multiple Helper Function\nDESCRIPTION: Implements a utility function that rounds a number up to the nearest multiple, which is used for ensuring batches are evenly divisible across distributed workers.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef _round_to_multiple(number,multiple): return int(math.ceil(number/multiple)*multiple)\n```\n\n----------------------------------------\n\nTITLE: Testing CrossEntropyLossFlat with axis=1\nDESCRIPTION: This code tests the `CrossEntropyLossFlat` class with the `axis` parameter set to 1, which is useful for segmentation tasks where the softmax should be applied over the channel dimension. It creates an instance of the class, generates random input and target tensors, and verifies that the loss function works correctly. It also tests the associated activation (softmax) and decoding (argmax) methods.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#In a segmentation task, we want to take the softmax over the channel dimension\ntst = CrossEntropyLossFlat(axis=1)\noutput = torch.randn(32, 5, 128, 128)\ntarget = torch.randint(0, 5, (32, 128, 128))\n_ = tst(output, target)\n\ntest_eq(tst.activation(output), F.softmax(output, dim=1))\ntest_eq(tst.decodes(output), output.argmax(dim=1))\n```\n\n----------------------------------------\n\nTITLE: Alias for SentencePieceTokenizer (Python)\nDESCRIPTION: Creates a simple alias, `SubwordTokenizer`, that points to the `SentencePieceTokenizer` class. This provides an alternative name for the SentencePiece implementation, potentially offering a more generic term for subword tokenization within the library's interface.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/30_text.core.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#|export\nSubwordTokenizer = SentencePieceTokenizer\n```\n\n----------------------------------------\n\nTITLE: Upgrading FastAI library in a Python environment\nDESCRIPTION: This snippet upgrades the fastai library using pip if running in a Google Colab environment. It checks for the existence of the /content directory, typical in Colab, before attempting the upgrade. This ensures that the latest fastai version is used during runtime, avoiding compatibility issues.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Importing Fastai and nbdev Modules - Python\nDESCRIPTION: Imports essential modules from fastai.vision.all and nbdev.showdoc for deep learning and documentation. Prerequisites: fastai and nbdev installed. These imports provide access to fastai vision utilities and documentation tools, facilitating code samples and docs integration.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.vision.all import *\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Reversing Text Tokens with a Reverse Transform in Python\nDESCRIPTION: Defines a utility function to reverse a tensor representing a sequence of tokens. This is intended for use in sequence ensemble models where reversed input can enhance accuracy. It takes a PyTorch tensor and returns its reversed version along dimension 0. The function expects input tensor-like data and outputs a tensor with the order of elements flipped.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef reverse_text(x): return x.flip(0)\n```\n\n----------------------------------------\n\nTITLE: Plotting Training Losses (NN, Adam) - Python/Matplotlib\nDESCRIPTION: Plots the list of training losses collected during the manual training loop using the neural network and the Adam optimizer update function.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nplt.plot(losses);\n```\n\n----------------------------------------\n\nTITLE: Integrating PointScaler with TensorBBox for Normalization/Denormalization - fastai (Python)\nDESCRIPTION: Extends PointScaler to handle encoding and decoding of TensorBBox by reshaping as TensorPoint, processing, and casting back. This enables bbox coordinates to be scaled for data augmentation. Requires PointScaler, TensorBBox, cast utility, and the encodes/decodes methods provided. Input/output: TensorBBox objects.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n@PointScaler\ndef encodes(self, x:TensorBBox):\n    pnts = self.encodes(cast(x.view(-1,2), TensorPoint))\n    return cast(pnts.view(-1, 4), TensorBBox)\n\n@PointScaler\ndef decodes(self, x:TensorBBox):\n    pnts = self.decodes(cast(x.view(-1,2), TensorPoint))\n    return cast(pnts.view(-1, 4), TensorBBox)\n```\n\n----------------------------------------\n\nTITLE: Install fastai on Colab Python\nDESCRIPTION: This snippet provides a shell command to upgrade the fastai library when running in a Google Colab environment. It checks if the '/content' directory exists, which is typical in Colab, before executing the pip install command.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14a_callback.data.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#| eval: false\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Data Transformations\nDESCRIPTION: Imports necessary modules from fastai and other libraries for data manipulation, including torch basics, data core functionality, loading utilities, and scikit-learn's train_test_split.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.torch_basics import *\nfrom fastai.data.core import *\nfrom fastai.data.load import *\nfrom fastai.data.external import *\n\nfrom sklearn.model_selection import train_test_split\n\nimport posixpath\n```\n\n----------------------------------------\n\nTITLE: Call nvidia_smi\nDESCRIPTION: Executes the `nvidia_smi()` function. The result of the call is assigned to `res`, which is used later. The `nvidia_smi` is used to retrieve nvidia driver information.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/97_test_utils.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nres = nvidia_smi()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Parallel Simple Tokenization - Jupyter\nDESCRIPTION: This snippet uses the `%%timeit` magic command to benchmark the execution time of applying the `delim_tok` function in parallel using fastai's `parallel` function with 2 workers. The input `ss` list is processed in parallel. Progress reporting is disabled (`progress=False`). The results are stored globally in variable `t`. It runs the operation 2 times per loop (`-n 2`) for 3 loops (`-r 3`).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n%%timeit -n 2 -r 3\nparallel(delim_tok, ss, n_workers=2, progress=False)\n```\n\n----------------------------------------\n\nTITLE: Import Core fastai Libraries Python\nDESCRIPTION: Imports necessary components from `fastai.basics` required for the definitions and functionalities provided in this file. This includes core fastai classes and functions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14a_callback.data.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom __future__ import annotations\nfrom fastai.basics import *\n```\n\n----------------------------------------\n\nTITLE: Adding Reset Method to DataParallel\nDESCRIPTION: Patches the DataParallel class to include a reset method that delegates to the wrapped module. This ensures consistent behavior between wrapped and unwrapped models during training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@patch\ndef reset(self: DataParallel):\n    \"Patch required `reset` call into `DataParallel`\"\n    if hasattr(self.module, 'reset'): self.module.reset()\n```\n\n----------------------------------------\n\nTITLE: Function to Unwrap PyTorch Model from Parallel Wrappers in Python\nDESCRIPTION: Returns the original model inside a DistributedDataParallel or DataParallel wrapper if provided, otherwise returns the model as is. This permits access to the underlying module without concerns regarding parallelization wrappers.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_73\n\nLANGUAGE: python\nCODE:\n```\ndef get_model(model):\n    \"Return the model maybe wrapped inside `model`.\"\n    return model.module if isinstance(model, (DistributedDataParallel, nn.DataParallel)) else model\n```\n\n----------------------------------------\n\nTITLE: Accessing Model Layer - PyTorch/Python\nDESCRIPTION: Accesses the linear layer (`self.lin`) attribute of the `model` object. This allows direct inspection or manipulation of the layer's properties, such as weights and biases.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmodel.lin\n```\n\n----------------------------------------\n\nTITLE: EmbeddingDropout Layer - PyTorch\nDESCRIPTION: The `EmbeddingDropout` module applies dropout to the embedding weights of an embedding layer. It takes an embedding layer and a dropout probability as input. During training, it creates a dropout mask and applies it to the embedding weights. It uses `F.embedding` to compute the embedding lookup, with the option to scale the embeddings. It also handles the padding index appropriately.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nclass EmbeddingDropout(Module):\n    \"Apply dropout with probability `embed_p` to an embedding layer `emb`.\"\n\n    def __init__(self,\n        emb:nn.Embedding, # Wrapped embedding layer\n        embed_p:float # Embdedding layer dropout probability \n    ):\n        self.emb,self.embed_p = emb,embed_p\n\n    def forward(self, words, scale=None):\n        if self.training and self.embed_p != 0:\n            size = (self.emb.weight.size(0),1)\n            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n            masked_embed = self.emb.weight * mask\n        else: masked_embed = self.emb.weight\n        if scale: masked_embed.mul_(scale)\n        return F.embedding(words, masked_embed, ifnone(self.emb.padding_idx, -1), self.emb.max_norm,\n                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)\n```\n\n----------------------------------------\n\nTITLE: Documenting TfmdDL.one_batch Method (fastai, Python)\nDESCRIPTION: Uses show_doc to render API documentation for the TfmdDL.one_batch method. This is primarily a docstring or documentation rendering utility from fastai. No input or output aside from the display output.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TfmdDL.one_batch)\n```\n\n----------------------------------------\n\nTITLE: Example: Create PartialDL from Datasets in fastai Python\nDESCRIPTION: Demonstrates creating a `PartialDL` instance using the patched `partial_dataloaders` method on a `Datasets` object. It sets the total number of items to sample per epoch to 32 with a batch size of 16.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14a_callback.data.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndls = dsets.partial_dataloaders(partial_n=32, bs=16)\n```\n\n----------------------------------------\n\nTITLE: ResizeBatch Layer for Batch-Preserving Tensor Reshape - Python\nDESCRIPTION: Implements a Module that reshapes input x to (batch_size, *size), where size is provided at initialization. Useful for keeping batch dimension intact while reshaping per-item dimensions. Expects tensor x, outputs reshaped tensor, depends on torch and fastai.Module.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass ResizeBatch(Module):\n    \"Reshape `x` to `size`, keeping batch dim the same size\"\n    def __init__(self, *size): self.size = size\n    def forward(self, x): return x.view((x.size(0),) + self.size)\n```\n\n----------------------------------------\n\nTITLE: Creating and Testing Recorder with Different Configurations in fastai (Python)\nDESCRIPTION: Defines a function to return a test Learner with the Recorder and custom TestRecorderCallback, and runs a series of assertive tests to check that Recorders metric_names attribute correctly reflects its configuration (with/without train metrics and timing). Inputs are various Recorder configurations toggled via the Learners attributes. Dependencies include synth_learner, TrainEvalCallback, Recorder, TestRecorderCallback, and test_eq.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_71\n\nLANGUAGE: python\nCODE:\n```\n#|hide\ndef _get_learn():\n    return synth_learner(n_train=5, metrics = tst_metric, default_cbs=False, cbs=[TrainEvalCallback, Recorder, TestRecorderCallback])\n\nlearn = _get_learn()\nlearn.fit(1)\ntest_eq(learn.recorder.metric_names, ['epoch', 'train_loss', 'valid_loss', 'tst_metric', 'time'])\n\nlearn = _get_learn()\nlearn.recorder.train_metrics=True\nlearn.fit(1)\ntest_eq(learn.recorder.metric_names, \n        ['epoch', 'train_loss', 'train_tst_metric', 'valid_loss', 'valid_tst_metric', 'time'])\n\nlearn = _get_learn()\nlearn.recorder.add_time=False\nlearn.fit(1)\ntest_eq(learn.recorder.metric_names, ['epoch', 'train_loss', 'valid_loss', 'tst_metric'])\n```\n\n----------------------------------------\n\nTITLE: Exporting Notebook\nDESCRIPTION: This code snippet uses the `nbdev_export` function from the `nbdev` library to export the current notebook as a Python module.  This makes the code reusable in other projects.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Parallel Minimal Return (Batched) - Jupyter\nDESCRIPTION: This snippet uses `%%timeit` to benchmark the execution time of applying the `f` function (which returns only `1`) using fastai's `parallel` with 2 workers on the `batches` data. This scenario tests parallel processing with low computational cost and minimal data transfer back from workers. Results stored globally in `t`. Runs 2 times per loop, 3 loops.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n%%timeit -n 2 -r 3\nglobal t\nt = parallel(f, batches, progress=False, n_workers=2)\n```\n\n----------------------------------------\n\nTITLE: Extract and Print Conda about.json (Python)\nDESCRIPTION: Extracts the `info/about.json` file from the opened Conda archive (`zpkg`), reads its content, decodes it from bytes to a string (assuming UTF-8), and prints it. The `fn.name` line appears extraneous in this context.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nf = (zpkg.extractfile('info/about.json').read().decode())\nprint(f)\nfn.name\n```\n\n----------------------------------------\n\nTITLE: Installing fastai in Colab Environment\nDESCRIPTION: A conditional installation command that upgrades fastai only if running in a Google Colab environment. This ensures the latest version is available without reinstalling unnecessarily.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Selecting Embedding Size for a Category in Python\nDESCRIPTION: Function to select an embedding size for a category variable `n` based on its classes and an optional override dictionary `sz_dict`. If an embedding size for the category is not provided in `sz_dict`, it applies the default `emb_sz_rule`. Returns a tuple of the number of unique classes and the embedding dimension size. This function supports customization and fallback sizing within tabular embedding size calculations.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/42_tabular.model.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef _one_emb_sz(classes, n, sz_dict=None):\n    \"Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`.\"\n    sz_dict = ifnone(sz_dict, {})\n    n_cat = len(classes[n])\n    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb\n    return n_cat,sz\n```\n\n----------------------------------------\n\nTITLE: Loading and Using Trained Model (Python)\nDESCRIPTION: Shows the steps to perform after distributed training is complete. It loads the learner state that was exported by the `train` function using `load_learner`, gets a list of image files, and then uses the loaded learner to make a prediction on the first image.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20b_tutorial.distributed.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nimgs = get_image_files(path)\nlearn = load_learner(path/'pets')\nlearn.predict(imgs[0])\n```\n\n----------------------------------------\n\nTITLE: Displaying Multiple Images as Subplots using fastai in Python\nDESCRIPTION: Defines `show_images` to display a list of images in a grid, allocating rows and columns automatically or via arguments. Accepts optional titles per image and supports figure-level titles (suptitle). Uses fastai's `show_image` to render individual images in each subplot. It supports all parameters applicable to matplotlib subplots, allowing flexible visualization layouts.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@delegates(subplots)\ndef show_images(ims, nrows=1, ncols=None, titles=None, **kwargs):\n    \"Show all images `ims` as subplots with `rows` using `titles`.\"\n    if ncols is None: ncols = int(math.ceil(len(ims)/nrows))\n    if titles is None: titles = [None]*len(ims)\n    axs = subplots(nrows, ncols, **kwargs)[1].flat\n    for im,t,ax in zip(ims, titles, axs): show_image(im, ax=ax, title=t)\n```\n\n----------------------------------------\n\nTITLE: Performing Image Preprocessing and Prediction on Single Image Using PyTorch Model in Python\nDESCRIPTION: Loads a sample MNIST image from fastai's MNIST_SAMPLE dataset, opens it with PIL, applies the same transforms used in validation, and prepares a batch tensor for inference. Then moves the tensor and model to GPU if available, performs prediction with the loaded PyTorch model with no gradient computation, and obtains class prediction by taking the argmax of the output logits. This exemplifies how to run raw PyTorch inference on a single image using preprocessing consistent with training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_pytorch_verbose.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.data.external import untar_data, URLs\n\ndata_path = untar_data(URLs.MNIST_SAMPLE)\n```\n\nLANGUAGE: python\nCODE:\n```\nsingle_image = data_path/'valid'/'3'/'8483.png'\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\n```\n\nLANGUAGE: python\nCODE:\n```\nim = Image.open(single_image)\nim.load();\n```\n\nLANGUAGE: python\nCODE:\n```\ntfmd_im = tfms(im); tfmd_im.shape\n```\n\nLANGUAGE: python\nCODE:\n```\ntfmd_im = tfmd_im.unsqueeze(0)\n```\n\nLANGUAGE: python\nCODE:\n```\nwith torch.no_grad():\n    new_net.cuda()\n    tfmd_im = tfmd_im.cuda()\n    preds = new_net(tfmd_im)\n```\n\nLANGUAGE: python\nCODE:\n```\npreds.argmax(dim=-1)\n```\n\n----------------------------------------\n\nTITLE: Combined Model Testing\nDESCRIPTION: This is an end-to-end test of a model consisting of a `SentenceEncoder` and a `PoolingLinearClassifier`. It creates an embedding layer, then creates a `SentenceEncoder`, a `PoolingLinearClassifier`, and then combines those two in a `nn.Sequential` to create a complete model.  Tests are performed to check shapes and outputs of various layers.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nmod = nn.Embedding(5, 10)\ntst = nn.Sequential(SentenceEncoder(5, mod, pad_idx=0), PoolingLinearClassifier([10*3,4], [0.], 5))\n\nx = torch.randint(1, 5, (3, 14))\nx[2,:5] = 0\nres,raw,out = tst(x) \n\ntest_eq(raw[:1], mod(x)[:1])\ntest_eq(raw[2,5:], mod(x)[2,5:])\ntest_eq(out[:1], mod(x)[:1])\ntest_eq(out[2,5:], mod(x)[2,5:])\ntest_eq(res.shape, [3,4])\n\nx1 = torch.cat([x, tensor([0,0,0])[:,None]], dim=1)\nres1,raw1,out1 = tst(x1) \ntest_eq(res, res1)\n```\n\n----------------------------------------\n\nTITLE: Export Module using nbdev\nDESCRIPTION: This snippet exports the current module using the `nbdev_export` function from the `nbdev` library. This is typically used to automatically generate Python modules from Jupyter notebooks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Using tabular_learner with ADULT_SAMPLE Dataset in Python\nDESCRIPTION: Demonstrates preparing a `TabularDataLoaders` from the ADULT_SAMPLE dataset including specifying categorical and continuous feature names, preprocessing steps (Categorify, FillMissing, Normalize), and creating a validation split by index. Then initializes a `TabularLearner` using the previously defined tabular_learner function with default parameters, illustrating typical end-to-end usage for tabular model training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/43_tabular.learner.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.ADULT_SAMPLE)\ndf = pd.read_csv(path/'adult.csv')\ncat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\ncont_names = ['age', 'fnlwgt', 'education-num']\nprocs = [Categorify, FillMissing, Normalize]\ndls = TabularDataLoaders.from_df(df, path, procs=procs, cat_names=cat_names, cont_names=cont_names, \n                                 y_names=\"salary\", valid_idx=list(range(800,1000)), bs=64)\nlearn = tabular_learner(dls)\n```\n\n----------------------------------------\n\nTITLE: Implementing Normalize Class for Tensor Normalization in Python\nDESCRIPTION: Defines a transformation class for normalizing and denormalizing batches of TensorImages using mean and standard deviation. Can calculate statistics from a DataLoader if not provided explicitly.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n@docs\nclass Normalize(DisplayedTransform):\n    \"Normalize/denorm batch of `TensorImage`\"\n    parameters,order = L('mean', 'std'),99\n    def __init__(self, mean=None, std=None, axes=(0,2,3)): store_attr()\n\n    @classmethod\n    def from_stats(cls, mean, std, dim=1, ndim=4, cuda=True): return cls(*broadcast_vec(dim, ndim, mean, std, cuda=cuda))\n\n    def setups(self, dl:DataLoader):\n        if self.mean is None or self.std is None:\n            x,*_ = dl.one_batch()\n            self.mean,self.std = x.mean(self.axes, keepdim=True),x.std(self.axes, keepdim=True)+1e-7\n    def encodes(self, x:TensorImage): return (x-self.mean) / self.std\n    def decodes(self, x:TensorImage):\n        f = to_cpu if x.device.type=='cpu' else noop\n        return (x*f(self.std) + f(self.mean))\n\n    _docs=dict(encodes=\"Normalize batch\", decodes=\"Denormalize batch\", setups=\"Calculate mean/std statistics from DataLoader if not provided\")\n```\n\n----------------------------------------\n\nTITLE: Freeze to Second Last Layer Group\nDESCRIPTION: Freezes all layers except the last two for the next stage of training.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7, 0.8))\n```\n\n----------------------------------------\n\nTITLE: Extracting Image Info from Fastai Learner\nDESCRIPTION: Internal helper function `_get_iw_info` to retrieve information about images from a fastai `Learner`'s dataloader (`dls`). For a specified dataset index (0 for train, 1 for valid), it gets predictions, targets, and losses, then returns a zipped list containing image file paths, decoded targets (class names), and losses for each image.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09c_vision.widgets.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef _get_iw_info(\n    learn,\n    ds_idx:int=0 # Index in `learn.dls`\n) -> list:\n    \"For every image in `dls` `zip` it's `Path`, target and loss\"\n    dl = learn.dls[ds_idx].new(shuffle=False, drop_last=False)\n    probs,targs,preds,losses = learn.get_preds(dl=dl, with_input=False, with_loss=True, with_decoded=True)\n    targs = [dl.vocab[t] for t in targs]\n    return L([dl.dataset.items,targs,losses]).zip()\n```\n\n----------------------------------------\n\nTITLE: Testing PointScaler Internal State After Application - fastai (Python)\nDESCRIPTION: Runs a quick check to confirm PointScaler captures and stores image dimensions after processing an item. Uses assertion testing (test_eq) and manipulates as_item. Expects PointScaler, pnt_tds, and test_eq to be defined. No output unless assertion fails.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n#Check the size was grabbed by PointScaler and added to y\ntfm = PointScaler()\ntfm.as_item=False\nx,y = tfm(pnt_tds[0])\ntest_eq(tfm.sz, x.size)\ntest_eq(y.img_size, x.size)\n```\n\n----------------------------------------\n\nTITLE: DataLoader batch visualization with NegTfm\nDESCRIPTION: This example shows how a DataLoader's `show_batch` method displays batch data, verifying the dataset's batching and transformations (NegTfm). It confirms correct batching, sample selection, and the effect of the transform, using the show_batch utility.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\ntds = TfmdLists(start, A())\ntdl = TfmdDL(tds, after_batch=NegTfm(), bs=4)\ntest_eq(tdl.dataset[0], start[0])\ntest_eq(len(tdl), (len(tds)-1)//4+1)\ntest_eq(tdl.bs, 4)\ntest_stdout(tdl.show_batch, '0\\n1\\n2\\n3')\n```\n\n----------------------------------------\n\nTITLE: Gradual Unfreezing and Fine-tuning Classifier (Unfreeze to -3) (Python)\nDESCRIPTION: Unfreezes all layers except the three deepest, halves the learning rate again, and trains another epoch with deeper gradual learning rate. Input: classifier Learner, previous lr; Output: more robust model adaptation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nlearn.freeze_to(-3)\nlr /= 2\nlearn.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7,0.8), wd=0.1)\n```\n\n----------------------------------------\n\nTITLE: Implementing Mean Squared Logarithmic Error (MSLE) for Regression in PyTorch\nDESCRIPTION: Function that calculates the mean squared logarithmic error between input and target tensors, applying log transformation to both inputs after adding 1 to avoid log(0) issues.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndef msle(inp, targ):\n    \"Mean squared logarithmic error between `inp` and `targ`.\"\n    inp,targ = flatten_check(inp,targ)\n    return F.mse_loss(torch.log(1 + inp), torch.log(1 + targ))\n```\n\n----------------------------------------\n\nTITLE: Open Wheel with zipfile and List Members (Python)\nDESCRIPTION: Opens the Wheel file (`.whl`) using the standard `zipfile.ZipFile` class in read mode. It then retrieves the list of `ZipInfo` objects representing the archive members using the `filelist` attribute and displays their indices and filenames.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nzip_ref = zipfile.ZipFile(whl,\"r\")\nls = zip_ref.filelist\n[(i,o.filename) for i,o in enumerate(ls)]\n```\n\n----------------------------------------\n\nTITLE: Testing DataBlock validation with incorrect get_y configuration\nDESCRIPTION: Tests that DataBlock correctly validates the number of getter functions in get_y matches the expected number of targets.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntest_fail(lambda: DataBlock((ImageBlock, ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(),\n                  get_y=[parent_label, noop],\n                  n_inp=2), msg='get_y contains 2 functions, but must contain 1 (one for each output)')\n```\n\n----------------------------------------\n\nTITLE: Import wheel.metadata Function (Python)\nDESCRIPTION: Imports the `pkginfo_to_metadata` function from the `wheel.metadata` module. This function is typically used for converting package information into METADATA format, although it is not used in the subsequent provided snippets.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom wheel.metadata import pkginfo_to_metadata\n```\n\n----------------------------------------\n\nTITLE: Documenting and Testing Decode and Decode_batch Methods (fastai, Python)\nDESCRIPTION: Uses show_doc for TfmdDL.decode and TfmdDL.decode_batch, then tests that decoding the output of a batch correctly inverts earlier transforms. Expects that the dls.decode and dls.decode_batch methods map numerical values accordingly. These tests ensure transforms can be reversed for interpreting data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TfmdDL.decode)\n```\n\nLANGUAGE: python\nCODE:\n```\ntest_eq(tdl.decode(b), tensor(0,1,2,3))\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TfmdDL.decode_batch)\n```\n\nLANGUAGE: python\nCODE:\n```\ntest_eq(tdl.decode_batch(b), [0,1,2,3])\n```\n\n----------------------------------------\n\nTITLE: Sort movies by first PCA component (lowest)\nDESCRIPTION: Sorts the movies by the values of the first principal component in ascending order and displays the top 10 movies. This reveals movies that are negatively associated with the first component.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nsorted(movie_comp, key=itemgetter(0))[:10]\n```\n\n----------------------------------------\n\nTITLE: Using RandDL with Batching and drop_last in Python\nDESCRIPTION: Demonstrates using the custom `RandDL` class with batching enabled. An instance is created with a batch size (`bs`) of 4 and `drop_last=True` to discard any final batch smaller than 4. The `L()` function collects all generated batches, and `.map(len)` is applied to check the size of each batch, verifying they are all of size 4.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nL(RandDL(bs=4, drop_last=True)).map(len)\n```\n\n----------------------------------------\n\nTITLE: Implementing TensorMask Subclass with Custom Show Method in Python\nDESCRIPTION: Defines TensorMask subclass of TensorImageBase for segmentation masks or label tensors. It uses ArrayMask's default visualization parameters and overrides the show method to adjust the visualization range based on optional 'codes' metadata, enabling meaningful display of categorical mask data with proper color scaling.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nclass TensorMask(TensorImageBase):\n    _show_args = ArrayMask._show_args\n\n    def show(self, ctx=None, **kwargs):\n        codes = getattr(self, 'codes', None)\n        if codes is not None: kwargs = merge({'vmin': 0, 'vmax': len(codes)}, kwargs)\n        return super().show(ctx=ctx, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: LinBnDrop Module\nDESCRIPTION: This class, `LinBnDrop`, is a sequential module that combines a BatchNorm layer, a Dropout layer, and a Linear layer. The order of the layers can be controlled using the `lin_first` parameter. The `bn` and `p` parameters determine whether the BatchNorm and Dropout layers are included, respectively. An activation function can be optionally added after the linear layer using the `act` parameter.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass LinBnDrop(nn.Sequential):\n    \"Module grouping `BatchNorm1d`, `Dropout` and `Linear` layers\"\n    def __init__(self, n_in, n_out, bn=True, p=0., act=None, lin_first=False):\n        layers = [BatchNorm(n_out if lin_first else n_in, ndim=1)] if bn else []\n        if p != 0: layers.append(nn.Dropout(p))\n        lin = [nn.Linear(n_in, n_out, bias=not bn)]\n        if act is not None: lin.append(act)\n        layers = lin+layers if lin_first else layers+lin\n        super().__init__(*layers)\n```\n\n----------------------------------------\n\nTITLE: FillMissing Setup and Encoding for TabularGPU\nDESCRIPTION: This snippet defines the `setups` and `encodes` methods for the `FillMissing` processor tailored for `TabularGPU`. `setups` determines the fill value for each column based on the specified strategy. `encodes` fills missing values with the determined fill values and optionally adds a boolean column indicating missingness.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@FillMissing\ndef setups(self, to: TabularGPU):\n    self.na_dict = {}\n    for n in to.cont_names:\n        col = to.iloc[:, n]\n        if col.isnull().any(): self.na_dict[n] = self.fill_strategy(col, self.fill_vals[n])\n\n@FillMissing\ndef encodes(self, to: TabularGPU):\n    for n in to.cont_names:\n        if n in self.na_dict:\n            if self.add_col:\n                to.items[n+'_na'] = to[n].isnull()\n                if n+'_na' not in to.cat_names: to.cat_names.append(n+'_na')\n            to[n] = to[n].fillna(self.na_dict[n])\n        elif df[n].isnull().any():\n            raise Exception(f\"nan values in `{n}` but not in setup training set\")\n```\n\n----------------------------------------\n\nTITLE: Creating Plot from Image with Matplotlib\nDESCRIPTION: This function `_make_plt` creates a Matplotlib figure and axes object from an image. It calculates the figure size based on the image dimensions and DPI, then it configures the axes to remove padding and turn off axis labels. It returns both the figure and axes objects.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef _make_plt(img):\n    \"Make plot to image resolution\"\n    # from https://stackoverflow.com/a/13714915\n    my_dpi = 100\n    fig = plt.figure(frameon=False, dpi=my_dpi)\n    h, w = img.shape[:2]\n    fig.set_size_inches(w / my_dpi, h / my_dpi)\n    ax = plt.Axes(fig, [0., 0., 1., 1.])\n    ax.set_axis_off()\n    fig.add_axes(ax)\n    return fig, ax\n```\n\n----------------------------------------\n\nTITLE: Recursive Element Finder for Arbitrary Nested Objects in Python\nDESCRIPTION: Recursively navigates nested lists, tuples, or dictionaries to retrieve an element at the specified index or key. It supports multi-level nested structures by repeatedly indexing into inner components until a non-list/dict object is reached. Useful for deep data extraction.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_67\n\nLANGUAGE: python\nCODE:\n```\ndef item_find(x, idx=0):\n    \"Recursively takes the `idx`-th element of `x`\"\n    if is_listy(x): return item_find(x[idx])\n    if isinstance(x,dict):\n        key = list(x.keys())[idx] if isinstance(idx, int) else idx\n        return item_find(x[key])\n    return x\n```\n\n----------------------------------------\n\nTITLE: Importing fastai Tabular Modules - Python\nDESCRIPTION: This snippet imports necessary modules from the fastai library's tabular module. It imports all functionalities that are related to tabular data processing, including data preparation, preprocessing, and model building. It serves as the foundation for using fastai for tabular data tasks.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-tabular.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.tabular.all import *\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for TextDataLoaders.from_csv\nDESCRIPTION: Uses the `show_doc` function to display the documentation for the `TextDataLoaders.from_csv` method. This is useful for understanding the parameters and usage of the method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TextDataLoaders.from_csv)\n```\n\n----------------------------------------\n\nTITLE: End-to-End Batch Validation with PointScaler/TensorPoint in DataLoader - fastai (Python)\nDESCRIPTION: Obtains a batch from the DataLoader, verifies correct scaling, decoding, and tensor types for points. Uses test_close, test_eq, and works with input images and manually scaled points. Suitable for integration/unit tests. Requires PointScaler, TfmdDL, and test utilities.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\nx,y = pnt_tdl.one_batch()\n#Scaling and flipping properly done\n#NB: we added a point earlier at (9,17); formula below scales to (-1,1) coords\ntest_close(y[0], tensor([[-1., -1.], [-1.,  1.], [1.,  -1.], [1., 1.], [9/14-1, 17/17.5-1]]))\na,b = pnt_tdl.decode_batch((x,y))[0]\ntest_eq(b, tensor(pnts).float())\n#Check types\ntest_eq(type(x), TensorImage)\ntest_eq(type(y), TensorPoint)\ntest_eq(type(a), TensorImage)\ntest_eq(type(b), TensorPoint)\ntest_eq(b.img_size, (28,35)) #Automatically picked the size of the input\n```\n\n----------------------------------------\n\nTITLE: Creating Helper Function for Testing Multi-label Metrics in Python\nDESCRIPTION: A utility function that modifies target tensors for testing purposes by flipping a specified number of labels. Used to validate the accuracy of multi-label classification metrics.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef change_1h_targ(targ, n):\n    idx = torch.randperm(targ.numel())[:n]\n    res = targ.clone().view(-1)\n    for i in idx: res[i] = 1-res[i]\n    return res.view(targ.shape)\n```\n\n----------------------------------------\n\nTITLE: Test for Segmentation Learner\nDESCRIPTION: This line performs a test to verify that the data loader's callback sequence or batch transforms are correctly set up for the segmentation learner, ensuring proper data processing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\ntest_ne(learn.cbs, None)\n```\n\n----------------------------------------\n\nTITLE: Extracting Base Filenames for Submission - Python\nDESCRIPTION: Strips '.jpg' from each test file's name to yield base image names for CSV submission. Outputs a list of strings. Inputs: list of Path objects or filenames. Output matches submission format required by Kaggle.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nfnames = [f.name[:-4] for f in test_items]\n```\n\n----------------------------------------\n\nTITLE: Testing Gradient Copying to Master Parameters in fastai Python\nDESCRIPTION: This test snippet verifies the functionality of `to_master_grads`. It performs a forward and backward pass to generate gradients in the model parameters, then uses `to_master_grads` to copy them to the FP32 master parameters (both standard and flattened). It asserts that the gradients match after the copy.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#|cuda\nxb,yb = learn.dls.one_batch()\npred = learn.model.cuda()(xb.cuda().half())\nloss = F.mse_loss(pred, yb.cuda().half())\nloss.backward()\nto_master_grads(model_p, master_p)\nto_master_grads(model_pf, master_pf, flat_master=True)\ntest_eq([[p.grad.float() for p in pg] for pg in model_p],\n        [[p.grad for p in pg] for pg in master_p])\ntest_eq([[p.grad.float().squeeze() for p in pg] for pg in model_pf], \n        [[p for p in pg[0].grad] for pg in master_pf])\nxb.shape\n```\n\n----------------------------------------\n\nTITLE: Importing Core Dependencies for Vision Widgets\nDESCRIPTION: Imports necessary modules from fastai (torch_basics, data.all, vision.core), fastcore (parallel), and ipywidgets. These dependencies provide the foundation for creating interactive widgets and handling image data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09c_vision.widgets.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom __future__ import annotations\nfrom fastai.torch_basics import *\nfrom fastai.data.all import *\nfrom fastai.vision.core import *\nfrom fastcore.parallel import *\nfrom ipywidgets import HBox,VBox,widgets,Button,Checkbox,Dropdown,Layout,Box,Output,Label,FileUpload\n```\n\n----------------------------------------\n\nTITLE: Categorify Setup and Encoding for TabularGPU\nDESCRIPTION: This snippet defines the `setups` and `encodes` methods for the `Categorify` processor, adapted for `TabularGPU`. `setups` creates category mappings using `nvcategory`. `encodes` transforms categorical columns to numerical representations using the created mappings.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@Categorify\ndef setups(self, to: TabularGPU):\n    self.lbls = {n: nvcategory.from_strings(_to_str(to.iloc[:,n]).data).keys() for n in to.all_cat_names}\n    self.classes = {n: CategoryMap(_remove_none(c.to_host()), add_na=(n in to.cat_names)) for n,c in self.lbls.items()}\n\n@patch\ndef _apply_cats_gpu(self: Categorify, c):\n    return cudf.Series(nvcategory.from_strings(_to_str(c).data).set_keys(self.lbls[c.name]).values()).add(add)\n\n@Categorify\ndef encodes(self, to: TabularGPU):\n    def _apply_cats_gpu(add, c):\n        return cudf.Series(nvcategory.from_strings(_to_str(c).data).set_keys(self.lbls[c.name]).values()).add(add)\n    to.transform(to.cat_names, partial(_apply_cats_gpu, 1))\n    to.transform(L(to.cat_y),  partial(_apply_cats_gpu, 0))\n```\n\n----------------------------------------\n\nTITLE: Training keypoint detection model for head pose using notebook_launcher\nDESCRIPTION: Defines a function to train a model for detecting head pose keypoints on the BIWI dataset using ResNet18, then launches it across 2 GPUs using notebook_launcher.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/distributed_app_examples.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.BIWI_HEAD_POSE)\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1,c2])\n\nimg_files = get_image_files(path)\ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\n\n\ndef train():\n    biwi = DataBlock(\n            blocks=(ImageBlock, PointBlock),\n            get_items=get_image_files,\n            get_y=get_ctr,\n            splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n            batch_tfms=[*aug_transforms(size=(240,320)), \n                        Normalize.from_stats(*imagenet_stats)])\n    dls = biwi.dataloaders(path)\n    learn = vision_learner(dls, resnet18, y_range=(-1,1))\n    with learn.distrib_ctx(in_notebook=True, sync_bn=False):\n        learn.fine_tune(1)\n        \nnotebook_launcher(train, num_processes=2)\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders with Image Resizing and Normalization in fastai Python\nDESCRIPTION: Defines a function `get_dls` that returns dataloaders for batches of bedroom images paired with noise inputs. It includes transformations: cropping-resizing images to a specified size and normalizing them with mean and stddev of 0.5 for all channels. The batch size (`bs`) and image size (`size`) are configurable parameters.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-wgan.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_dls(bs, size):\n    dblock = DataBlock(blocks = (TransformBlock, ImageBlock),\n                   get_x = generate_noise,\n                   get_items = get_image_files,\n                   splitter = IndexSplitter([]),\n                   item_tfms=Resize(size, method=ResizeMethod.Crop),\n                   batch_tfms = Normalize.from_stats(torch.tensor([0.5,0.5,0.5]), torch.tensor([0.5,0.5,0.5])))\n    return dblock.dataloaders(path, path=path, bs=bs)\n```\n\n----------------------------------------\n\nTITLE: Checking Type Compatibility for PyTorch Operations in fastai\nDESCRIPTION: Defines the internal helper function `_torch_handled`. This function is likely used for dispatching or determining how to handle specific PyTorch operations (`func`) based on the types of the input arguments (`args`). It checks if the types of `args` match any of the predefined valid type combinations specified in the `opt` dictionary for the given `func`. Returns `True` if a matching type signature is found, `False` otherwise.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef _torch_handled(args, opt, func):\n    if func not in opt: return False\n    for oks in opt[func]:\n        if all(isinstance(arg,ok) for arg,ok in zip(args,oks) if ok): return True\n```\n\n----------------------------------------\n\nTITLE: Get Wheel dist-info Path (Python)\nDESCRIPTION: Retrieves the path to the `.dist-info` directory within the opened Wheel file (`fwhl`). This directory contains essential package metadata.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfwhl.dist_info_path\n```\n\n----------------------------------------\n\nTITLE: Export module definition\nDESCRIPTION: This code defines the export scope for the module, indicating that everything within this module should be exported when the library is built.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/04_data.external.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp data.external\n```\n\n----------------------------------------\n\nTITLE: Importing Fastai Vision Utilities (Python)\nDESCRIPTION: This snippet imports essential modules and functions from the fastai library's vision application. It uses the `*` wildcard import to bring all components from `fastai.vision.all` into the current namespace, providing access to various vision-related functionalities required for subsequent steps, such as defining a `Learner` for image data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_pytorch.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom fastai.vision.all import *\n```\n\n----------------------------------------\n\nTITLE: Exporting Notebook Changes to Library (Bash)\nDESCRIPTION: Runs the nbdev export command to synchronize code from modified notebook cells into the corresponding Python library files. This is necessary after making code changes directly in the notebooks.\nSOURCE: https://github.com/fastai/fastai/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nnbdev_export\n```\n\n----------------------------------------\n\nTITLE: Finding Learning Rate for Resnet-ish model\nDESCRIPTION: This finds the optimal learning rate for the ResNet-ish model using `learn.lr_find()`.  It helps to identify a good learning rate range for efficient training of the ResNet-ish model.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\nlearn.lr_find(end_lr=100)\n```\n\n----------------------------------------\n\nTITLE: CrossEntropyLossFlat CUDA test\nDESCRIPTION: This code snippet tests the `CrossEntropyLossFlat` class on a CUDA device if available.  It initializes the loss function with a weight tensor, moves it to the CUDA device, generates random input and target tensors on the device, and calculates the loss.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#|cuda\ntst = CrossEntropyLossFlat(weight=torch.ones(10))\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntst.to(device)\noutput = torch.randn(32, 10, device=device)\ntarget = torch.randint(0, 10, (32,), device=device)\n_ = tst(output, target)\n```\n\n----------------------------------------\n\nTITLE: Creating Kaggle Credentials Directory and Moving kaggle.json - Python\nDESCRIPTION: Shell command (commented out) for Unix systems to create the .kaggle directory in the user's home folder and move the Kaggle API token (kaggle.json) into it. Required for authenticated access to Kaggle datasets. Windows version included in comments. No Python-side output generated.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# ! mkdir -p ~/.kaggle/\n# ! mv kaggle.json ~/.kaggle/\n\n# For Windows, uncomment these two commands\n# ! mkdir %userprofile%\\.kaggle\n# ! move kaggle.json %userprofile%\\.kaggle\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Head for Inspection (Python, pandas)\nDESCRIPTION: This code calls the .head() method on a DataFrame (train, test, or other) to output a sample of the first five records. This requires pandas and outputs a small preview of the table for quick visual inspection of columns and data types.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain.head()\n```\n\nLANGUAGE: python\nCODE:\n```\ntest.head()\n```\n\nLANGUAGE: python\nCODE:\n```\nstore.head().T\n```\n\nLANGUAGE: python\nCODE:\n```\nstore_states.head()\n```\n\nLANGUAGE: python\nCODE:\n```\nstate_names.head()\n```\n\nLANGUAGE: python\nCODE:\n```\nweather.head().T\n```\n\nLANGUAGE: python\nCODE:\n```\ngoogletrend.head()\n```\n\n----------------------------------------\n\nTITLE: Saving the Language Model State\nDESCRIPTION: This saves the state of the language model, specifically the model weights, to a file named '1epoch.pth' in the 'models' subdirectory of the learner's path.  This allows the user to save the progress of training to resume later.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nlearn.save('1epoch')\n```\n\n----------------------------------------\n\nTITLE: Importing fastai modules\nDESCRIPTION: This code imports necessary modules from the fastai library. It imports all modules for vision, text, collaborative filtering, and tabular data, providing access to functionalities for handling various data types and model architectures within the fastai framework. These modules are essential for the subsequent examples.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/quick_start.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.vision.all import *\nfrom fastai.text.all import *\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\n```\n\n----------------------------------------\n\nTITLE: AdaptiveConcatPool1d Layer - Python\nDESCRIPTION: A composite pooling layer that concatenates the outputs of AdaptiveAvgPool1d and AdaptiveMaxPool1d along the channel axis. Depends on nn.AdaptiveAvgPool1d, nn.AdaptiveMaxPool1d, and torch.cat. Initialized with optional size (default 1); input is 3D tensor, output doubles channel dimension. Useful for sequence models requiring both pooling types.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass AdaptiveConcatPool1d(Module):\n    \"Layer that concats `AdaptiveAvgPool1d` and `AdaptiveMaxPool1d`\"\n    def __init__(self, size=None):\n        self.size = size or 1\n        self.ap = nn.AdaptiveAvgPool1d(self.size)\n        self.mp = nn.AdaptiveMaxPool1d(self.size)\n    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n```\n\n----------------------------------------\n\nTITLE: Defining conv_and_res\nDESCRIPTION: This defines `conv_and_res`, which is a utility function combining a convolutional layer (`conv2`) and a `ResBlock`.  This is used to simplify the construction of the ResNet-ish model, allowing for combining the convolutional layers and residual blocks.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\ndef conv_and_res(ni,nf): return nn.Sequential(conv2(ni, nf), ResBlock(nf))\n```\n\n----------------------------------------\n\nTITLE: Decoding Special Tokens in FastAI NLP\nDESCRIPTION: Processes a list of string tokens, interpreting and applying special FastAI tokens (like 'xxmaj', 'xxup', 'xxrep', 'xxwrep') for text formatting such as capitalization, uppercasing, and word repetition. It iterates through the input tokens, maintaining state to handle rules and arguments for repetition.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef decode_spec_tokens(tokens):\n    \"Decode the special tokens in `tokens`\"\n    new_toks,rule,arg = [],None,None\n    for t in tokens:\n        if t in [TK_MAJ, TK_UP, TK_REP, TK_WREP]: rule = t\n        elif rule is None: new_toks.append(t)\n        elif rule == TK_MAJ:\n            new_toks.append(t[:1].upper() + t[1:].lower())\n            rule = None\n        elif rule == TK_UP:\n            new_toks.append(t.upper())\n            rule = None\n        elif arg is None:\n            try:    arg = int(t)\n            except: rule = None\n        else:\n            if rule == TK_REP: new_toks.append(t * arg)\n            else:              new_toks += [t] * arg\n    return new_toks\n```\n\n----------------------------------------\n\nTITLE: Testing BCEWithLogitsLoss Failure Case - PyTorch\nDESCRIPTION: This snippet demonstrates a failure case when using the standard PyTorch `nn.BCEWithLogitsLoss` with potentially incompatible input/target shapes, highlighting the need for flattened versions or specific data preparation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntest_fail(lambda x: nn.BCEWithLogitsLoss()(output,target))\n```\n\n----------------------------------------\n\nTITLE: Importing fastai Tabular Components - Python\nDESCRIPTION: Imports all necessary modules and functions from the fastai tabular library, providing access to tabular data processing, modeling, and utilities.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.tabular.all import *\n```\n\n----------------------------------------\n\nTITLE: Comparing Imports for Distributed Training (Diff)\nDESCRIPTION: Highlights the additional Python imports needed from `fastai.distributed` and `accelerate` when setting up distributed training compared to a standard fastai script. Specifically, it shows the addition of `fastai.distributed`, `accelerate.notebook_launcher`, and `accelerate.utils.write_basic_config`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20b_tutorial.distributed.ipynb#_snippet_2\n\nLANGUAGE: Diff\nCODE:\n```\n+ from fastai.distributed import *\nfrom fastai.vision.all import *\nfrom fastai.vision.models.xresnet import *\n\n+ from accelerate import notebook_launcher\n+ from accelerate.utils import write_basic_config\n```\n\n----------------------------------------\n\nTITLE: Creating Adaptive Average Pooling Layer (Python)\nDESCRIPTION: Returns a PyTorch Adaptive Average Pooling layer (`nn.AdaptiveAvgPool1d`, `nn.AdaptiveAvgPool2d`, or `nn.AdaptiveAvgPool3d`) based on the specified number of dimensions (`ndim`). The target output size `sz` defaults to 1. Asserts that `ndim` is between 1 and 3.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef AdaptiveAvgPool(sz=1, ndim=2):\n    \"nn.AdaptiveAvgPool layer for `ndim`\"\n    assert 1 <= ndim <= 3\n    return getattr(nn, f\"AdaptiveAvgPool{ndim}d\")(sz)\n```\n\n----------------------------------------\n\nTITLE: Merging Store, Store States, and Weather DataFrames (pandas, Python)\nDESCRIPTION: This code demonstrates sequential joins of store data with store states and then with weather data using the join_df helper. It relies on proper DataFrame structures and expects the 'State' field to exist after merging. It then checks the joins validity by looking for missing Mean_TemperatureC values. Outputs include an augmented store DataFrame and a nullity check for climate data fields.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nstore = join_df(store, store_states, 'Store')\nstore = join_df(store, weather, 'State')\n```\n\nLANGUAGE: python\nCODE:\n```\nlen(store[store.Mean_TemperatureC.isnull()])\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Parallel Spacy Tokenization (Worker Init, 8 Workers) - Jupyter\nDESCRIPTION: This snippet uses `%%timeit` to benchmark the execution time of applying the `f` function (which initializes Spacy per worker) using fastai's `parallel` with 8 workers on the `batches8` data. This measures the performance when Spacy's setup cost is incurred within each parallel process, potentially avoiding serialization issues. Results stored globally in `t`. Runs for 3 loops (`-r 3`).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n%%timeit -r 3\nglobal t\nt = parallel(f, batches8, progress=False, n_workers=8)\n```\n\n----------------------------------------\n\nTITLE: AdaptiveConcatPool2d Layer - Python\nDESCRIPTION: A composite 2D pooling layer that concatenates AdaptiveAvgPool2d and AdaptiveMaxPool2d outputs along the channel dimension. Dependencies: nn.AdaptiveAvgPool2d, nn.AdaptiveMaxPool2d, torch.cat. Takes input shape (bs, nf, h, w); output is (bs, 2*nf, size, size). Useful for image tasks needing both max and avg pooled features.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass AdaptiveConcatPool2d(Module):\n    \"Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`\"\n    def __init__(self, size=None):\n        self.size = size or 1\n        self.ap = nn.AdaptiveAvgPool2d(self.size)\n        self.mp = nn.AdaptiveMaxPool2d(self.size)\n    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n```\n\n----------------------------------------\n\nTITLE: Importing necessary fastai modules\nDESCRIPTION: This imports required modules from the fastai library, including basics, progress tracking, and mixed precision training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom __future__ import annotations\nfrom fastai.basics import *\nfrom fastai.callback.progress import *\nfrom fastai.callback.fp16 import MixedPrecision\n```\n\n----------------------------------------\n\nTITLE: Testing Dataset Serialization with Pickle\nDESCRIPTION: This snippet serializes and deserializes the dataset using pickle, then tests that the train and validation datasets remain consistent, ensuring that dataset persistence mechanisms are functioning correctly.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_55\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n#Test Datasets pickles\ndsrc1 = pickle.loads(pickle.dumps(dsets))\ntest_eq(dsets.train, dsrc1.train)\ntest_eq(dsets.valid, dsrc1.valid)\n```\n\n----------------------------------------\n\nTITLE: Defining a DataBlock for Noise to Image Mapping using fastai in Python\nDESCRIPTION: Creates a DataBlock that pairs random noise vectors (generated by `generate_noise`) as inputs with bedroom images as targets, using fastai's data block API. No validation set is created (via `IndexSplitter([])`), and images are retrieved with `get_image_files` from the dataset path.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-wgan.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndblock = DataBlock(blocks = (TransformBlock, ImageBlock),\n                   get_x = generate_noise,\n                   get_items = get_image_files,\n                   splitter = IndexSplitter([]))\n```\n\n----------------------------------------\n\nTITLE: Set Text and Prediction Parameters\nDESCRIPTION: Sets the initial text and parameters for generating text predictions with the language model. `N_WORDS` specifies the number of words to predict, and `N_SENTENCES` specifies the number of sentences to generate.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nTEXT = \"I liked this movie because\"\nN_WORDS = 40\nN_SENTENCES = 2\n```\n\n----------------------------------------\n\nTITLE: Loading and Reading Tabular Data with Pandas - Python\nDESCRIPTION: This code loads the adult sample dataset from a URL and reads it into a Pandas DataFrame. The `untar_data` function downloads and extracts the data from the provided URL. The `pd.read_csv` function reads the CSV file into a DataFrame, making it ready for further processing with fastai.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-tabular.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.ADULT_SAMPLE)\ndf = pd.read_csv(path/'adult.csv')\n```\n\n----------------------------------------\n\nTITLE: Load ratings data into a pandas DataFrame\nDESCRIPTION: Loads the `ratings.csv` file from the sample Movielens dataset into a pandas DataFrame named `ratings`. The `head()` method is then called to display the first few rows of the DataFrame.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nratings = pd.read_csv(path/'ratings.csv')\nratings.head()\n```\n\n----------------------------------------\n\nTITLE: Creating and fine-tuning a segmentation model\nDESCRIPTION: This code creates a U-Net learner with a ResNet34 backbone and fine-tunes it for 8 epochs.  It uses the data loaders created in the previous step, allowing for pixel-level image analysis. This step trains the model using the prepared Camvid data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/quick_start.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(8)\n```\n\n----------------------------------------\n\nTITLE: Testing FillMissing with Multiple Columns - Python\nDESCRIPTION: This code tests the `FillMissing` transform when applied to a DataFrame with multiple continuous columns.  It verifies that the missing values are filled in the correct column ('a'), a boolean column indicating the missing values is added ('a_na'), and that the other columns remain unchanged.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nfill = FillMissing()\ndf = pd.DataFrame({'a':[0,1,np.nan,1,2,3,4], 'b': [0,1,2,3,4,5,6]})\nto = TabularPandas(df, fill, cont_names=['a', 'b'])\ntest_eq(fill.na_dict, {'a': 1.5})\ntest_eq(to.cat_names, ['a_na'])\ntest_eq(to['a'].values, np.array([0, 1, 1.5, 1, 2, 3, 4]))\ntest_eq(to['a_na'].values, np.array([0, 0, 1, 0, 0, 0, 0]))\ntest_eq(to['b'].values, np.array([0,1,2,3,4,5,6]))\n```\n\n----------------------------------------\n\nTITLE: Printing Model Summary\nDESCRIPTION: This prints a summary of the model, including the layers, their parameters, and the output shapes.  This allows for inspection of the model's architecture and verifying the correct number of parameters.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nprint(learn.summary())\n```\n\n----------------------------------------\n\nTITLE: Importing fastai Vision and GAN Modules in Python\nDESCRIPTION: Imports all necessary classes and functions from fastai's vision and GAN submodules, enabling image processing, data loading, model creation, and GAN training routines.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-wgan.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.vision.all import *\nfrom fastai.vision.gan import *\n```\n\n----------------------------------------\n\nTITLE: Download Full IMDB Dataset\nDESCRIPTION: Downloads the full IMDB dataset using `untar_data` from the `fastai.data.external` module and lists the contents of the extracted directory. This prepares the full dataset for language model fine-tuning and classifier training.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.IMDB)\npath.ls()\n```\n\n----------------------------------------\n\nTITLE: Generating Text using the Fine-tuned Model's generate Method in Python (HuggingFace/fastai)\nDESCRIPTION: Uses the `generate` method of the underlying HuggingFace model (accessed via `learn.model`) to generate text based on the prepared input tensor (`inp`). Key parameters specified include `max_length` (maximum length of the generated sequence), `num_beams` (number of beams for beam search decoding), and `temperature` (controls the randomness of the output). The generated token IDs are stored in the `preds` variable.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\npreds = learn.model.generate(inp, max_length=40, num_beams=5, temperature=1.5)\n```\n\n----------------------------------------\n\nTITLE: Testing `test_set` with `rm_tfms` Parameter for Transform Removal\nDESCRIPTION: Applies `test_set` with specific `rm_tfms` values including tuples to remove certain transforms from datasets, validating the batch transformation output after removal.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_71\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n#Test with rm_tfms\ndsets = Datasets(range(8), [[_Tfm(),_Tfm()]], splits=[[1,2,5,7],[0,3,4,6]])\ntst = test_set(dsets, [1,2,3])\ntest_eq(tst, [(4,),(8,),(12,)])\n\ndsets = Datasets(range(8), [[_Tfm(),_Tfm()]], splits=[[1,2,5,7],[0,3,4,6]])\ntst = test_set(dsets, [1,2,3], rm_tfms=1)\ntest_eq(tst, [(2,),(4,),(6,)])\n\ndsets = Datasets(range(8), [[_Tfm(),_Tfm()], [_Tfm(),_Tfm()]], splits=[[1,2,5,7],[0,3,4,6]], n_inp=2)\ntst = test_set(dsets, [1,2,3], rm_tfms=(1,0))\ntest_eq(tst, [(2,4),(4,8),(6,12)])\n```\n\n----------------------------------------\n\nTITLE: Demonstration of seed setting for reproducible randomness across libraries (Python)\nDESCRIPTION: This example shows how to use the `set_seed` function to reset RNG states and verify the reproducibility of random outputs from NumPy, Torch, and the Python `random` module after reseeding. It highlights the practical application for reproducible experiments.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nset_seed(2*33)\na1 = np.random.random()\na2 = torch.rand(())\na3 = random.random()\nset_seed(2*33)\nb1 = np.random.random()\nb2 = torch.rand(())\nb3 = random.random()\nprint('a\\'s: {0:3.3f} {1:3.3f} {2:3.3f}'.format(a1,a2,a3))\nprint('b\\'s: {0:3.3f} {1:3.3f} {2:3.3f}'.format(b1,b2,a3))\n```\n\n----------------------------------------\n\nTITLE: Defining Parallel Spacy Tokenization Function with Worker Initialization - Python\nDESCRIPTION: This snippet defines a Python function `f` intended to be run in parallel workers. Inside `f`, a new instance of the `SpTok` class (which initializes the Spacy tokenizer) is created. It then iterates through the input items `its`, tokenizes each using the newly created tokenizer, and returns the results as a list of lists of strings. This tests the approach of initializing Spacy within each parallel worker.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef f(its):\n    tok = SpTok()\n    return [[str(o) for o in tok(p)] for p in its]\n```\n\n----------------------------------------\n\nTITLE: Creating CategoryBlock for single-label categorical targets in FastAI Python\nDESCRIPTION: This function returns a `TransformBlock` configured for single-label categorical target variables. It uses the `Categorize` transform with options to specify a vocabulary (`vocab`), sorting order (`sort`), and whether to add a missing category tag (`add_na`). It simplifies common classification tasks by wrapping these transforms as a reusable block.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef CategoryBlock(\n    vocab:MutableSequence|pd.Series=None, # List of unique class names\n    sort:bool=True, # Sort the classes alphabetically\n    add_na:bool=False, # Add `#na#` to `vocab`\n):\n    \"`TransformBlock` for single-label categorical targets\"\n    return TransformBlock(type_tfms=Categorize(vocab=vocab, sort=sort, add_na=add_na))\n```\n\n----------------------------------------\n\nTITLE: Configuring SortedDL for Fastai DataLoaders - Python\nDESCRIPTION: This snippet demonstrates how to configure and use `SortedDL` as the primary `dl_type` when creating DataLoaders from existing Datasets. It shows how to use `partial` to pre-set parameters like `res` (text lengths) for resource tracking and how to pass additional, per-split keyword arguments via `dl_kwargs`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n# Pass the training dataset text lengths to SortedDL\nsrtd_dl=partial(SortedDL, res = train_text_lens)\n\n# Pass the validation dataset text lengths \ndl_kwargs = [{},{'val_res': val_text_lens}]\n\n# init our Datasets \ndsets = Datasets(...)   \n\n# init our Dataloaders\ndls = dsets.dataloaders(...,dl_type = srtd_dl, dl_kwargs = dl_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Constructing and Configuring the Language Model Learner (Python)\nDESCRIPTION: Creates a Learner configured for language modeling using AWD_LSTM architecture, Adam optimizer with weight decay, and accuracy/perplexity metrics. Then converts the learner to mixed precision with to_fp16 for faster training. Requires fastai's language_model_learner, partial, Adam, and metric functions. Inputs: DataLoaders, architecture, optimizer, metrics; Output: initialized Learner object.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nopt_func = partial(Adam, wd=0.1)\nlearn = language_model_learner(dbunch_lm, AWD_LSTM, opt_func=opt_func, metrics=[accuracy, Perplexity()], path=path)\nlearn = learn.to_fp16()\n```\n\n----------------------------------------\n\nTITLE: Create Text Classifier Learner\nDESCRIPTION: Creates a text classifier learner using the `text_classifier_learner` function from FastAI. It specifies the `DataLoaders` object, the architecture (AWD_LSTM), dropout multiplier, and metrics (accuracy). The encoder from the language model is loaded, and the model is moved to fp16 precision.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nlearn = text_classifier_learner(dbunch_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16()\nlearn.load_encoder('fine_tuned_enc')\n```\n\n----------------------------------------\n\nTITLE: Importing showdoc from nbdev for documentation purposes\nDESCRIPTION: Python import statement for using showdoc from nbdev library, potentially for inline documentation or display within notebooks.\nSOURCE: https://github.com/fastai/fastai/blob/main/disabled/74_callback.azureml.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Converting Data to PyTorch Tensors - Python/PyTorch\nDESCRIPTION: Converts the loaded NumPy arrays (x_train, y_train, x_valid, y_valid) into PyTorch tensors using the `map` function. It then extracts the number of samples (n) and features (c) from the training data shape and prints the training data shape and the min/max values of the training labels.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nx_train,y_train,x_valid,y_valid = map(torch.tensor, (x_train,y_train,x_valid,y_valid))\nn,c = x_train.shape\nx_train.shape, y_train.min(), y_train.max()\n```\n\n----------------------------------------\n\nTITLE: Install fastai on Colab\nDESCRIPTION: This shell command checks if the code is running in a Google Colab environment and, if so, installs or upgrades the fastai library using pip.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/35_tutorial.wikitext.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Copying Model Gradients to Master Gradients in fastai Python\nDESCRIPTION: The `to_master_grads` function copies gradients from the model's parameters (typically FP16/BF16) to the corresponding master parameters (FP32). This step is crucial before the optimizer takes a step, which is performed in FP32 using the master parameters.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n#|export \ndef to_master_grads( \n    model_pgs:list, # Fp16 model parameters to copy gradients from\n    master_pgs:list, # Fp32 model parameters to copy gradients to\n    flat_master:bool=False, # Whether or not fp32 parameters were previously flattened\n):\n    \"Move fp16 model gradients to fp32 master gradients\"\n    for (model_params,master_params) in zip(model_pgs,master_pgs):\n        model_grads_to_master_grads(model_params, master_params, flat_master=flat_master)\n```\n\n----------------------------------------\n\nTITLE: Get the length of the ratings DataFrame\nDESCRIPTION: Calculates the number of rows in the `ratings` DataFrame, representing the total number of ratings.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nlen(ratings)\n```\n\n----------------------------------------\n\nTITLE: First Training Cycle of Language Model (Python)\nDESCRIPTION: Performs one training epoch of the LM learner using the 1cycle policy and specific learning rates/momentum values. Input: initialized learner learn; Output: training progress with updated model weights. Fastai's fit_one_cycle required.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7,0.8))\n```\n\n----------------------------------------\n\nTITLE: Joining and Cleaning Weather and State Data (pandas, Python)\nDESCRIPTION: These snippets join weather and state name tables, verify the join, and clean up unnecessary columns in pandas DataFrames. The helper join_df must be defined previously. Outputs include a merged DataFrame and validation (via checking for nulls) that join operations succeeded before columns are dropped.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nweather = join_df(weather, state_names, \"file\", \"StateName\")\nweather[['file', 'Date', 'State', 'StateName']].head()\n```\n\nLANGUAGE: python\nCODE:\n```\nlen(weather[weather.State.isnull()])\n```\n\nLANGUAGE: python\nCODE:\n```\nweather.drop(columns=['file', 'StateName'], inplace=True)\n```\n\n----------------------------------------\n\nTITLE: Importing fastai Text Module (Python)\nDESCRIPTION: Imports all modules from fastai.text.all, providing access to fastai's text data utilities, learners, and data loaders used later for data preparation and model training. Must be imported before using fastai-specific classes or functions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.text.all import *\n```\n\n----------------------------------------\n\nTITLE: Displaying Text Classification Results in FastAI (TensorText)\nDESCRIPTION: Provides a `show_results` implementation for general text tasks (like classification) where the input is `TensorText`. It truncates the input text samples to `trunc_at` characters for display clarity before calling the base `show_results` method and presenting the formatted input, target, and prediction in a Pandas DataFrame.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@dispatch\ndef show_results(x: TensorText, y, samples, outs, ctxs=None, max_n=10, trunc_at=150, **kwargs):\n    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n    samples = L((s[0].truncate(trunc_at),*s[1:]) for s in samples)\n    ctxs = show_results[object](x, y, samples, outs, ctxs=ctxs, max_n=max_n, **kwargs)\n    display_df(pd.DataFrame(ctxs))\n    return ctxs\n```\n\n----------------------------------------\n\nTITLE: Installing fastai on Google Colab\nDESCRIPTION: This code snippet checks if the code is running in a Google Colab environment and, if so, upgrades the fastai library to the latest version. This ensures that the user has the most up-to-date features and bug fixes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/04_data.external.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Generate Synthetic DataLoaders\nDESCRIPTION: Generates a `DataLoaders` object with synthetic data for training and validation. It creates a linear regression dataset with random input and target values, allowing for testing of training loops and callbacks without relying on external data. This function is useful for quickly creating and training models for quick testing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/97_test_utils.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef synth_dbunch(a=2, b=3, bs=16, n_train=10, n_valid=2, cuda=False):\n    def get_data(n):\n        x = torch.randn(bs*n, 1)\n        return TensorDataset(x, a*x + b + 0.1*torch.randn(bs*n, 1))\n    train_ds = get_data(n_train)\n    valid_ds = get_data(n_valid)\n    device = default_device() if cuda else None\n    train_dl = TfmdDL(train_ds, bs=bs, shuffle=True, num_workers=0)\n    valid_dl = TfmdDL(valid_ds, bs=bs, num_workers=0)\n    return DataLoaders(train_dl, valid_dl, device=device)\n```\n\n----------------------------------------\n\nTITLE: Defining a Combined L1 and L2 Loss Module in PyTorch\nDESCRIPTION: Introduces the CombineL1L2 module class which computes the sum of L1 loss and mean squared error loss between the output and target tensors. It stores the individual loss values as attributes and returns their sum for combined training loss. This module depends on PyTorch's functional loss implementations (F.l1_loss and F.mse_loss) and can be used as a custom loss function in neural network training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nclass CombineL1L2(Module):\n    def forward(self, out, targ):\n        self.l1 = F.l1_loss(out, targ)\n        self.l2 = F.mse_loss(out, targ)\n        return self.l1+self.l2\n```\n\n----------------------------------------\n\nTITLE: Importing Language Model Utilities from fastai\nDESCRIPTION: Imports classes relevant to language modeling with fastai, including LMLearner and TextLearner, enabling word embedding exports.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nfrom fastai.text.all import LMLearner, TextLearner\n```\n\n----------------------------------------\n\nTITLE: Save Fine-tuned Language Model Head\nDESCRIPTION: Saves the head (last layers) of the fine-tuned language model. This allows for loading it later without retraining the entire model.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nlearn.save('fit_head')\n```\n\n----------------------------------------\n\nTITLE: Writing Accelerate Basic Config in Notebook Launcher - Python\nDESCRIPTION: Calls write_basic_config from accelerate.utils to generate an Accelerate configuration automatically. Should be executed in the notebook before launching distributed jobs with HuggingFace accelerate. Prerequisite: accelerate library installed.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Initializing a fastai Learner for Language Model Fine-Tuning in Python\nDESCRIPTION: Initializes a fastai `Learner` object for language model training. It takes the data loaders (`dls`), the HuggingFace model (`model`), a flattened cross-entropy loss function (`CrossEntropyLossFlat`), the custom `DropOutput` callback (`cbs=[DropOutput]`), and perplexity (`Perplexity()`) as a metric. Mixed precision (`to_fp16()`) is enabled for potentially faster training and reduced GPU memory usage.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity()).to_fp16()\n```\n\n----------------------------------------\n\nTITLE: Create Short Tensor Representation\nDESCRIPTION: This function creates a short string representation of a given object, particularly useful for tensors. It handles tuples and lists recursively, and provides a concise summary for tensors including their size if they have more than 20 elements or are multi-dimensional. It returns the string representation of the object.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef _short_repr(x):\n    if isinstance(x, tuple): return f'({\", \".join([_short_repr(y) for y in x])})'\n    if isinstance(x, list): return f'[{\", \".join([_short_repr(y) for y in x])}]'\n    if not isinstance(x, Tensor): return str(x)\n    if x.numel() <= 20 and x.ndim <=1: return str(x)\n    return f'{x.__class__.__name__} of size {\"x\".join([str(d) for d in x.shape])}'\n```\n\n----------------------------------------\n\nTITLE: Loading Multiple CSV Tables with pandas in Python\nDESCRIPTION: This code defines a list of relevant table names and loads each as a pandas DataFrame from the specified path. The approach relies on the pandas library and the existence of all named CSV files in the Rossmann dataset folder. Expected input is a directory with the required CSV files; output is a set of DataFrames for further analysis and processing.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntable_names = ['train', 'store', 'store_states', 'state_names', 'googletrend', 'weather', 'test']\ntables = [pd.read_csv(path/f'{fname}.csv', low_memory=False) for fname in table_names]\ntrain, store, store_states, state_names, googletrend, weather, test = tables\n```\n\n----------------------------------------\n\nTITLE: Benchmarking fastai `parallel_chunks` - Jupyter\nDESCRIPTION: This snippet uses `%%timeit` to benchmark the execution time of fastai's `parallel_chunks` function with 8 workers. This function is designed to handle chunking and parallel processing internally. It applies the previously defined `f` function (which uses Spacy's `pipe`) to the `ss` list, automatically dividing it into chunks for the workers. Results stored globally in `t`. Runs for 3 loops (`-r 3`).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n%%timeit -r 3\nglobal t\nt = parallel_chunks(f, ss, n_workers=8, progress=False)\n```\n\n----------------------------------------\n\nTITLE: Running Manual Training Loop (Manual SGD with NN) - PyTorch/Python\nDESCRIPTION: Executes the previously defined manual `update` function (which includes weight decay and manual parameter updates) for every batch in the training DataLoader, but now using the newly instantiated `Mnist_NN` model (due to `update` using the global `model` variable). This trains the neural network for one epoch manually. The loss for each batch is collected in the `losses` list.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nlosses = [update(x,y,lr) for x,y in dls.train]\n```\n\n----------------------------------------\n\nTITLE: Creating a Vision Learner for Multi-Weight Models using String Architecture\nDESCRIPTION: This snippet shows how initializing a data loader and then creating a vision learner with a string specifying the architecture (e.g., 'convnext_tiny') triggers the use of the 'timm' library to load the model. Dependencies include fastai, timm, and torch. This enables flexible architecture specification via string identifiers and supports new models from external libraries.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\ndls = ImageDataLoaders.from_name_re(path, fnames, pat, item_tfms=Resize(224))\nlearn = vision_learner(dls, 'convnext_tiny', loss_func=CrossEntropyLossFlat(), ps=0.25)\n```\n\n----------------------------------------\n\nTITLE: Add weighted_dataloaders to DataBlock in fastai Python\nDESCRIPTION: Patches the `DataBlock` class to add a `weighted_dataloaders` method. This allows creating weighted dataloaders directly from a `DataBlock` definition and a source. It handles creating the `Datasets` internally and applying the correct weights to the training split based on the block's splitter.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14a_callback.data.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\n@delegates(Datasets.weighted_dataloaders)\ndef weighted_dataloaders(self:DataBlock, source, wgts, bs=64, verbose:bool=False, **kwargs):\n    \"Create a weighted dataloader `WeightedDL` with `wgts` for the dataset\"\n    dss = self.datasets(source, verbose=verbose)\n    if not hasattr(wgts, '__array__'): wgts = np.array(wgts)\n    trn_wgts = wgts[dss.splits[0]]\n    return dss.weighted_dataloaders(trn_wgts, bs=bs, after_batch=self.batch_tfms, after_item=self.item_tfms, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: model_sizes Function Definition\nDESCRIPTION: This code defines the `model_sizes` function, which passes a dummy input through a model `m` to determine the shapes of the activations at various layers. It uses the `hook_outputs` function to attach hooks to all layers of the model and store their outputs. It then calls the `dummy_eval` function to perform a forward pass with a dummy input. Finally, it returns a list of the shapes of the stored activations.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef model_sizes(m, size=(64,64)):\n    \"Pass a dummy input through the model `m` to get the various sizes of activations.\"\n    with hook_outputs(m) as hooks:\n        _ = dummy_eval(m, size=size)\n        return [o.stored.shape for o in hooks]\n```\n\n----------------------------------------\n\nTITLE: Example: Create WeightedDL from Datasets in fastai Python\nDESCRIPTION: Demonstrates how to create a weighted dataloader using the patched `weighted_dataloaders` method on a `Datasets` object. It sets up a simple `DataBlock`, creates a `Datasets` object, defines weights, and then generates dataloaders with a batch size of 1 and item transformations.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14a_callback.data.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlbls = np.random.randint(0, 2, size=(10)) # Dataset of size 10 (train=8, valid=2)\nis_valid = lambda i: i >= 8\ndblock = DataBlock(blocks=[CategoryBlock], \n    getters=[lambda i: lbls[i]], splitter=FuncSplitter(is_valid))\ndset = dblock.datasets(list(range(10)))\nitem_tfms = [ToTensor()] \nwgts = range(8) # len(wgts) == 8\ndls = dset.weighted_dataloaders(bs=1, wgts=wgts, after_item=item_tfms)\n```\n\n----------------------------------------\n\nTITLE: Creating Test DataLoader\nDESCRIPTION: Prepares a DataLoader for making predictions on new, unseen data (a 'test set'). It creates a copy of the original DataFrame, drops the target variable ('salary'), and uses the learned data transformations to create a test DataLoader.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\ntest_df = df.copy()\ntest_df.drop(['salary'], axis=1, inplace=True)\ndl = learn.dls.test_dl(test_df)\n```\n\n----------------------------------------\n\nTITLE: Opening and Thumbnailing an Image\nDESCRIPTION: Internal helper function `_open_thumb` that takes an image file path, desired height, and width. It opens the image using PIL, creates a thumbnail respecting aspect ratio, converts it to RGBA format, and returns the `PIL.Image` object. Used by `ImagesCleaner`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09c_vision.widgets.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef _open_thumb(\n    fn:Path|str, # A path of an image\n    h:int, # Thumbnail Height\n    w:int # Thumbnail Width\n) -> Image: # `PIL` image to display\n    \"Opens an image path and returns the thumbnail of the image\"\n    return Image.open(fn).to_thumb(h, w).convert('RGBA')\n```\n\n----------------------------------------\n\nTITLE: Gathering Learner Arguments with fastai\nDESCRIPTION: This calls the `gather_args` method of a `Learner` object and displays its documentation using `show_doc`. The `gather_args` method is intended to collect and return relevant arguments used during the learner's initialization. The `show_doc` function displays documentation for the method.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Learner.gather_args)\n```\n\n----------------------------------------\n\nTITLE: Importing Fastai Loss Function Dependencies (Python)\nDESCRIPTION: This snippet imports future annotation support and several core Fastai modules essential for building loss functions, including base imports, Torch utilities, core classes, and neural network layers. Dependencies include the fastai library and its submodules. This snippet prepares the environment for defining and utilizing custom loss functions, with no direct inputs or outputs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.imports import *\nfrom fastai.torch_imports import *\nfrom fastai.torch_core import *\nfrom fastai.layers import *\n```\n\n----------------------------------------\n\nTITLE: Inspect Language Model Dataset\nDESCRIPTION: Inspects the first element of the training dataset in the `DataLoaders` object. This allows for verification of the data format and numericalization.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndbunch_lm.train_ds[0]\n```\n\n----------------------------------------\n\nTITLE: Import Conda Function for Reading Records (Python)\nDESCRIPTION: Imports the `read_python_record` function from the `conda` library's gateway modules. This function is specifically designed to read and parse the RECORD file often found in Python package distributions, including wheels.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom conda.gateways.disk.read import read_python_record\n```\n\n----------------------------------------\n\nTITLE: Defining Default Export Name for NBDev - Python\nDESCRIPTION: Specifies the default module export name for nbdev build process. Sets this .py file as default for exports relating to text learner utilities in fastai. No dependencies or functional output, required only for codebase structure organization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp text.learner\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Parallel Spacy Tokenization (Batched, 8 Workers) - Jupyter\nDESCRIPTION: This snippet uses `%%timeit` to benchmark the execution time of applying the `spacy_tok` function in parallel using fastai's `parallel` with 8 workers on the `batches8` data (data split into 8 batches). Workers apply `spacy_tok` to their batch elements via a `partial` application of the `apply` function. This measures Spacy parallel performance with increased parallelism. Results stored globally in `t`. Runs for 3 loops (`-r 3`).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n%%timeit -r 3\nglobal t\nt = parallel(partial(apply, spacy_tok), batches8, progress=False, n_workers=8)\n```\n\n----------------------------------------\n\nTITLE: Generating Layer-Wise Model Info with layer_info in fastai (Python)\nDESCRIPTION: Implements the layer_info function, which hooks into the layers of a fastai learner's model and extracts per-layer statistics, including type, parameter count, trainability, output shapes, and whether input/output shapes match. Optionally supports parameterless layers. Designed for batch-first inputs and relies on fastai hooks, flatten_model, apply, and supporting utilities. Returns a list of tuples for each layer for further reporting or inspection.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\n#|export\ndef layer_info(learn, *xb):\n    \"Return layer infos of `model` on `xb` (only support batch first inputs)\"\n    def _track(m, i, o): \n        params, trainable, shape = '', '', ''\n        same = any((isinstance(x[0], torch.Tensor) and x[0].shape[1:] == x[1].shape for x in zip(i, o)))\n        shape = apply(lambda x: x.shape, o)\n        if hasattr(m, 'weight'): # non activation layer\n            params, trainable = total_params(m)\n        return (type(m).__name__, params, trainable, shape, same)\n            \n    with Hooks(flatten_model(learn.model), _track) as h:\n        batch = apply(lambda o:o[:1], xb)\n        train_only_cbs = [cb for cb in learn.cbs if hasattr(cb, '_only_train_loop')]\n        with learn.removed_cbs(train_only_cbs), learn.no_logging(), learn as l:\n            r = l.get_preds(dl=[batch], inner=True, reorder=False)\n        return h.stored\n```\n\n----------------------------------------\n\nTITLE: BatchNorm1dFlat Class\nDESCRIPTION: This class, `BatchNorm1dFlat`, is a subclass of `nn.BatchNorm1d` that flattens the leading dimensions of the input before applying batch normalization. This allows it to be used with inputs of dimension greater than 2.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass BatchNorm1dFlat(nn.BatchNorm1d):\n    \"`nn.BatchNorm1d`, but first flattens leading dimensions\"\n    def forward(self, x):\n        if x.dim()==2: return super().forward(x)\n        *f,l = x.shape\n        x = x.contiguous().view(-1,l)\n        return super().forward(x).view(*f,l)\n```\n\n----------------------------------------\n\nTITLE: Initializing Wasserstein GAN Learner with fastai in Python\nDESCRIPTION: Creates a GANLearner using fastai's WGAN implementation with the previously defined dataloaders, generator, and critic. The optimizer function is Adam with zero momentum (`mom=0.`), suitable for GAN training to reduce oscillations.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-wgan.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlearn = GANLearner.wgan(dls, generator, critic, opt_func = partial(Adam, mom=0.))\n```\n\n----------------------------------------\n\nTITLE: Checking Output Shape of Model\nDESCRIPTION: This passes the input batch through the model and checks the output shape.  This validates the forward pass and ensures that the model is producing the expected output dimensions.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nmodel(xb).shape\n```\n\n----------------------------------------\n\nTITLE: Loading and Decompressing IMDb Data\nDESCRIPTION: This snippet downloads and decompresses the IMDb dataset using `untar_data` and assigns the data's path to the variable `path`. The `URLs.IMDB` constant specifies the location of the dataset.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.IMDB)\n```\n\n----------------------------------------\n\nTITLE: Base Class for Image Arrays with Display Capability in Python\nDESCRIPTION: Defines `ArrayImageBase`, subclass of `ArrayBase`, representing image arrays with default visualization settings. It includes a `show` method that calls fastai's `show_image`, applying a default colormap and accepting customization parameters. This allows image arrays to self-render easily in notebooks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass ArrayImageBase(ArrayBase):\n    \"Base class for arrays representing images\"\n    _show_args = {'cmap':'viridis'}\n    def show(self, ctx=None, **kwargs):\n        return show_image(self, ctx=ctx, **{**self._show_args, **kwargs})\n```\n\n----------------------------------------\n\nTITLE: Preparing Text Data List - Python\nDESCRIPTION: This snippet extracts the 'text' column from the loaded pandas DataFrame `df` and converts it into a fastai `L` object (a list-like collection). It then prints the first element of this list, which is a sample IMDB review, to verify the data extraction.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nss = L(list(df.text))\nss[0]\n```\n\n----------------------------------------\n\nTITLE: Normalize Setup and Encoding for TabularGPU\nDESCRIPTION: This snippet defines the `setups` and `encodes` methods for the `Normalize` processor, specifically for `TabularGPU`. `setups` computes the mean and standard deviation for each continuous column. `encodes` applies the normalization transformation using the calculated statistics.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@Normalize\ndef setups(self, to: TabularGPU):\n    self.means = {n: to.iloc[:,n].mean()           for n in to.cont_names}\n    self.stds  = {n: to.iloc[:,n].std(ddof=0)+1e-7 for n in to.cont_names}\n\n@Normalize\ndef encodes(self, to: TabularGPU):\n    to.transform(to.cont_names, lambda c: (c-self.means[c.name])/self.stds[c.name])\n```\n\n----------------------------------------\n\nTITLE: Recursive application of a function to nested structures (Python)\nDESCRIPTION: The `apply` function recursively traverses nested collections (`list`, `dict`, etc.) and applies a provided `func` to each element. It preserves the container types and handles `None` values gracefully. Useful for batch processing or data transformations on complex structures.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\n#|export\ndef apply(func, x, *args, **kwargs):\n    \"Apply `func` recursively to `x`, passing on args\"\n    if is_listy(x): return type(x)([apply(func, o, *args, **kwargs) for o in x])\n    if isinstance(x,(dict,MutableMapping)): return {k: apply(func, v, *args, **kwargs) for k,v in x.items()}\n    res = func(x, *args, **kwargs)\n    return res if x is None else retain_type(res, x)\n```\n\n----------------------------------------\n\nTITLE: Showing the Training Loop\nDESCRIPTION: This code snippet shows the training loop using `show_training_loop`. It is presented to help with debugging, with callbacks shown in order at each step of the process.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner()\nlearn.show_training_loop()\n```\n\n----------------------------------------\n\nTITLE: Verifying Raw DataLoader GPU Transition in fastai Learner (Python)\nDESCRIPTION: This tests that raw PyTorch DataLoaders (i.e., when tfmdDL=False) are also transferred to the GPU during Learner training, in conjunction with a test callback. This ensures consistent device movement for custom data input pipelines. Relies on CUDA support and the fastai librarys callback and Learner internal implementation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#|cuda\n#Check that raw DataLoaders are put on the GPU\nlearn = synth_learner(cbs=TestTrainEvalCallback, tfmdDL=False)\nlearn.fit(1)\n```\n\n----------------------------------------\n\nTITLE: Cutting a PyTorch Model at Specific Layer or Function Using FastAI in Python\nDESCRIPTION: Function 'cut_model' takes a model and a 'cut' parameter, which can be an integer index or a callable. If integer, returns a sequential container of layers up to that index; if callable, returns the result of applying it to the model. This allows flexible and dynamic truncation of pretrained models for fine-tuning or feature extraction.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef cut_model(model, cut):\n    \"Cut an instantiated model\"\n    if   isinstance(cut, int): return nn.Sequential(*list(model.children())[:cut])\n    elif callable(cut): return cut(model)\n    raise NameError(\"cut must be either integer or a function\")\n```\n\n----------------------------------------\n\nTITLE: Testing DiceLoss One-Hot Encoding Helper - Fastai/PyTorch\nDESCRIPTION: This snippet tests the static `_one_hot` helper method within the `DiceLoss` class. It verifies that a non-one-hot encoded target tensor is correctly converted into a one-hot encoded tensor representation based on the specified number of classes and axis.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndl = DiceLoss()\n_x         = tensor( [[[1, 0, 2],\n                       [2, 2, 1]]])\n_one_hot_x = tensor([[[[0, 1, 0],\n                       [0, 0, 0]],\n                      [[1, 0, 0],\n                       [0, 0, 1]],\n                      [[0, 0, 1],\n                       [1, 1, 0]]]])\ntest_eq(dl._one_hot(_x, 3), _one_hot_x)\n```\n\n----------------------------------------\n\nTITLE: Importing fastai and Testing Libraries (Python)\nDESCRIPTION: Imports necessary modules from `tempfile`, `fastai.vision.all`, and `fastcore.test`. These libraries provide utilities for creating temporary directories, accessing fastai vision components, and performing test assertions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/98_test_model_export.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom tempfile import TemporaryDirectory\nfrom fastai.vision.all import *\nfrom fastcore.test import *\n```\n\n----------------------------------------\n\nTITLE: Importing Boxes and Points Data Types\nDESCRIPTION: Imports data types related to bounding boxes and points for visualization or detection tasks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nfrom fastai.vision.core import TensorPoint,TensorBBox\n```\n\n----------------------------------------\n\nTITLE: Filtering Dataset to Training Set\nDESCRIPTION: This code checks that only specified splits (training set) are considered by setting splits explicitly. It verifies the vocabulary and dataset contents accordingly, relying on TfmdLists and test_eq for validation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\n#Check only the training set is taken into account for setup\n tl = TfmdLists(fns, [tcat,_lbl], splits=[[0,4], [1,2,3]])\ntest_eq(tcat.vocab, ['dog'])\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Parallel Computation vs. Return Cost - Jupyter\nDESCRIPTION: This snippet uses `%%timeit` to benchmark the execution time of applying the `f` function (which computes tokens but returns the original strings) using fastai's `parallel` with 2 workers on the `batches` data. This benchmark helps determine how much of the parallel execution time is spent on the actual tokenization computation within the worker versus the overhead of sending the results back to the main process. Results stored globally in `t`. Runs 2 times per loop, 3 loops.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n%%timeit -n 2 -r 3\nglobal t\nt = parallel(f, batches, progress=False, n_workers=2)\n```\n\n----------------------------------------\n\nTITLE: Testing TextDataLoaders.from_df with tok_text_col - Python\nDESCRIPTION: This hidden snippet tests the `tok_text_col` parameter of `TextDataLoaders.from_df`. It modifies the DataFrame column names to ensure that the DataLoader correctly identifies the text column for tokenization and processing when `tok_text_col` is explicitly provided, verifying the numerical output.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\npath = untar_data(URLs.IMDB_SAMPLE)\ndf = pd.read_csv(path/\"texts.csv\")\ndf.columns = ['label', 'text_col', 'is_valid'] # to test tok_text_col is working properly\ndls = TextDataLoaders.from_df(df, path=path, text_col='text_col', label_col='label', valid_col='is_valid')\ndl = dls.test_dl([\"This movie was bad\"])\nx, = dl.one_batch()\ntest_eq(x.cpu(), TensorText([[2,8,21,29,25,97]]))\n```\n\n----------------------------------------\n\nTITLE: Detaching tensors, optionally gathering across distributed processes and moving to CPU (Python)\nDESCRIPTION: The `to_detach` function recursively detaches tensors within nested structures, optionally gathering tensors from distributed processes and moving them to CPU memory. It provides a consistent way to prepare tensors for evaluation or inference without gradient tracking.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\n#|export\ndef to_detach(b, cpu=True, gather=True):\n    \"Recursively detach lists of tensors in `b `; put them on the CPU if `cpu=True`.\"\n    def _inner(x, cpu=True, gather=True):\n        if not isinstance(x,Tensor): return x\n        x = x.detach()\n        if gather: x = maybe_gather(x)\n        return x.cpu() if cpu else x\n    return apply(_inner, b, cpu=cpu, gather=gather)\n```\n\n----------------------------------------\n\nTITLE: Preparing Datasets for Language Modeling with Tokenization and Numericalization (Python)\nDESCRIPTION: Splits available text files into training/validation sets using RandomSplitter, configures transforms for tokenization and numericalization, and constructs a Datasets object with LMDataLoader type. Dependencies: fastai Datasets, Tokenizer, Numericalize, and RandomSplitter. Inputs: texts list; Outputs: dsets object ready for DataLoader creation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsplits = RandomSplitter(valid_pct=0.1)(texts)\ntfms = [Tokenizer.from_folder(path), Numericalize()]\ndsets = Datasets(texts, [tfms], splits=splits, dl_type=LMDataLoader)\n```\n\n----------------------------------------\n\nTITLE: Setting up Model Output for All Background Prediction Example - PyTorch\nDESCRIPTION: This snippet sets up a sample model output tensor representing a prediction where the model predicts only the background class (class 0) for all pixels. This is used as a test case to show how a loss function like DiceLoss would penalize such a model even if background is the majority class.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nmodel_output_all_background = torch.zeros(3, 100,100)\n# assign probability 1 to class 0 everywhere\n# to get probability 1, we just need a high model output before softmax gets applied\nmodel_output_all_background[0,:,:] = 100\n```\n\n----------------------------------------\n\nTITLE: Testing fa_collate with Nested Tuples - Python\nDESCRIPTION: Runs assertions to test fa_collate with various nested tuple structures, ensuring both value and type match with PyTorch's default_collate. Uses fastcore's test_eq and L utilities for type and value validation. These cells are primarily for developer validation and internal testing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nt = [(1,(2,3)),(1,(2,3))]\ntest_eq(fa_collate(t), default_collate(t))\ntest_eq(L(fa_collate(t)).map(type), [Tensor,tuple])\n\nt = [(1,(2,(3,4))),(1,(2,(3,4)))]\ntest_eq(fa_collate(t), default_collate(t))\ntest_eq(L(fa_collate(t)).map(type), [Tensor,tuple])\ntest_eq(L(fa_collate(t)[1]).map(type), [Tensor,tuple])\n```\n\n----------------------------------------\n\nTITLE: Filtering Datasets with Custom Transform\nDESCRIPTION: This example tests filtering in Datasets by applying a custom transform `add1` that modifies data during encoding/decoding. It verifies the correctness of resulting data after splits, including train and valid subsets, using a custom `Transform` subclass.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_44\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n#Check filtering is properly applied\nclass B(Transform):\n    def encodes(self, x)->None:  return int(x+1)\n    def decodes(self, x):        return TitledInt(x-1)\nadd1 = B(split_idx=1)\n\ndsets = Datasets(items, [neg_tfm, [neg_tfm,int2f_tfm,add1]], splits=[[3],[0,1,2]])\ntest_eq(dsets[1], [-2,-2])\ntest_eq(dsets.valid[1], [-2,-1])\ntest_eq(dsets.valid[[1,1]], [[-2,-1], [-2,-1]])\ntest_eq(dsets.train[0], [-4,-4])\n```\n\n----------------------------------------\n\nTITLE: TabularGPU Pipeline Validation\nDESCRIPTION: This code verifies the correct preprocessing steps within a TabularGPU pipeline. It involves setting up a pipeline including normalization, categorization, and handling missing values, then validating the processed data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nprocs = [Normalize, Categorify, FillMissing, noop]\ndf = cudf.from_pandas(pd.DataFrame({'a':[0,1,2,1,1,2,0], 'b':[0,1,np.nan,1,2,3,4], 'c': ['b','a','b','a','a','b','a']}))\nto = TabularGPU(df, procs, cat_names='a', cont_names='b', y_names='c')\n\ntest_eq(to.cat_names, ['a', 'b_na'])\ntest_eq(to.a.to_array(), [1,2,3,2,2,3,1])\ntest_eq(to.a.dtype,int)\ntest_eq(to.b_na.to_array(), [1,1,2,1,1,1,1])\ntest_eq(to.c.to_array(), [1,0,1,0,0,1,0])\n```\n\n----------------------------------------\n\nTITLE: Importing nbdev.showdoc Utility - Python\nDESCRIPTION: Imports the showdoc function from nbdev.showdoc, which is used for generating documentation in fastai/nbdev workflows. No computational logic, strictly for documentation rendering.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Importing fastai Test Utilities and nbdev ShowDoc (Python)\nDESCRIPTION: This snippet imports internal test utilities from fastai and a helper for generating notebook documentation from nbdev. Prerequisites include fastai's 'test_utils' and nbdev's 'showdoc'. Inputs involve no arguments; output is the availability of test and doc utilities for downstream code or documentation cells.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.test_utils import *\nfrom nbdev.showdoc import *\n\n```\n\n----------------------------------------\n\nTITLE: Checking Shape of Exponentiated Predictions - Python\nDESCRIPTION: Applies the exponential function (`np.exp`) to the log-transformed predictions to get them back into the original 'Sales' scale. It then checks the shape of the resulting numpy array after transposing.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nnp.exp(tst_preds.numpy()).T.shape\n```\n\n----------------------------------------\n\nTITLE: Loading and preparing segmentation data\nDESCRIPTION: This code snippet downloads a subset of the Camvid dataset and creates `DataLoaders` for segmentation. It specifies a function to create labels for the images and then creates a `SegmentationDataLoaders` object. This prepares the data for training a segmentation model.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/quick_start.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.CAMVID_TINY)\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs=8, fnames = get_image_files(path/\"images\"),\n    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n    codes = np.loadtxt(path/'codes.txt', dtype=str)\n)\n```\n\n----------------------------------------\n\nTITLE: Importing nbdev Documentation Utilities (Python)\nDESCRIPTION: This Python snippet imports all functions from `nbdev.showdoc`. These utilities are typically used within a `nbdev` notebook to display documentation generated from code cells, facilitating interactive development and documentation. The `#|hide` directive prevents this import statement from being exported into the final Python library code.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/65_medical.text.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Processing TensorImage with Mask Data for Wandb\nDESCRIPTION: This `wandb_process` function processes `TensorImage` data with `TensorMask` labels for logging to Weights & Biases (W&B). It creates a W&B table with columns for the input image, ground truth masks, and prediction masks. It determines the class labels from the output or defaults to integers. It adds data to the table using W&B images with mask overlays.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n@dispatch\ndef wandb_process(x:TensorImage, y:TensorMask, samples, outs, preds):\n    res = []\n    codes = getattr(outs[0][0], 'codes', None)\n    if codes is not None:\n        class_labels = [{'name': name, 'id': id}  for id, name in enumerate(codes)] \n    else:\n        class_labels = [{'name': i, 'id': i} for i in range(preds.shape[1])]\n    table = wandb.Table(columns=[\"Input Image\", \"Ground_Truth\", \"Predictions\"])\n    for (image, label), pred_label in zip(samples, outs):\n        img = image.permute(1,2,0)\n        table.add_data(wandb.Image(img),\n                       wandb.Image(img, masks={\"Ground_Truth\": {'mask_data': label.numpy().astype(np.uint8)}}, classes=class_labels), \n                       wandb.Image(img, masks={\"Prediction\":   {'mask_data': pred_label[0].numpy().astype(np.uint8)}}, classes=class_labels) \n                      )\n    return {\"Prediction_Samples\": table}\n```\n\n----------------------------------------\n\nTITLE: Showing Results for Specific Indices (Python)\nDESCRIPTION: Provides the `show_results` method within the `Interpretation` class. Similar to `Learner.show_results`, this method retrieves inputs, targets, and decoded predictions for a given list of indices and uses the dataloader's `show_results` method to display them.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef show_results(self,\n        idxs:list, # Indices of predictions and targets\n        **kwargs\n    ):\n        \"Show predictions and targets of `idxs`\"\n        if isinstance(idxs, Tensor): idxs = idxs.tolist()\n        if not is_listy(idxs): idxs = [idxs]\n        inps, _, targs, decoded, _ = self[idxs]\n        b = tuplify(inps)+tuplify(targs)\n        self.dl.show_results(b, tuplify(decoded), max_n=len(idxs), **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Get movie biases\nDESCRIPTION: Retrieves the bias values for the top movies from the model. These bias values represent the inherent preference for each movie, independent of user preferences. `is_item=True` specifies that we want biases for items (movies).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nmovie_bias = learn.model.bias(top_movies, is_item=True)\nmovie_bias.shape\n```\n\n----------------------------------------\n\nTITLE: Example using URLs.path with 'model' c_key\nDESCRIPTION: This demonstrates downloading a dataset into the model directory using `URLs.path`, by overriding the default `archive` c_key with `model`.  It checks the parent directory of the local path to ensure it corresponds to the model location in the config, thus changing the download location. The expected output is a model path.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/04_data.external.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nlocal_path = URLs.path(url, c_key='model')\ntest_eq(local_path.parent, fastai_path('model'))\nlocal_path\n```\n\n----------------------------------------\n\nTITLE: Call nvidia_mem\nDESCRIPTION: Invokes the `nvidia_mem()` function. The function's output, which is a list of strings, will be available for further usage. This function call gathers the total memory available on the nvidia GPUs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/97_test_utils.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nnvidia_mem()\n```\n\n----------------------------------------\n\nTITLE: Testing After-Batch Transform and Device Transfer (fastai, Python)\nDESCRIPTION: Tests the use of an after_batch transform within a TfmdDL DataLoader, checking the device placement before and after moving the dataloader to the default device. Utilizes PyTorch device handling for CPU and accelerator compatibility. Assumes existence of TensorImage, B (transform), and fastai test_eq/assert utilities.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntdl = TfmdDL([(TensorImage([1]),)] * 4, after_batch=B(), bs=4)\ntest_eq(tdl.after_batch.fs[0].a.device, torch.device('cpu'))\ntdl.to(default_device())\ntest_eq(tdl.after_batch.fs[0].a.device, default_device())\n```\n\n----------------------------------------\n\nTITLE: Defining show_at Helper Function in fastai (Python)\nDESCRIPTION: Defines a utility function `show_at` that takes an object `o` (like a `TfmdLists`), an index `idx`, and optional keyword arguments `**kwargs`. It retrieves the transformed item at the index (`o[idx]`) and then displays it using the object's `show` method, passing along any extra arguments for customized visualization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n#|exports\ndef show_at(o, idx, **kwargs):\n    \"Show item at `idx`\",\n    return o.show(o[idx], **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Testing LabelSmoothingCrossEntropy with Different Input Shapes - Fastai/PyTorch\nDESCRIPTION: This snippet tests the `LabelSmoothingCrossEntropy` class to ensure it produces the same result when inputs are flattened or transposed according to its expected input format (N, C, *) vs (N, *, C). It compares the loss calculated on flattened tensors with that calculated on transposed tensors.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nlmce = LabelSmoothingCrossEntropy()\noutput = torch.randn(32, 5, 10)\ntarget = torch.randint(0, 10, (32,5))\ntest_close(lmce(output.flatten(0,1), target.flatten()), lmce(output.transpose(-1,-2), target))\n```\n\n----------------------------------------\n\nTITLE: Creating Validation Split for Classification with GrandparentSplitter (Python)\nDESCRIPTION: Splits the classification dataset into training and validation sets based on the grandparent folder name ('test'). Input: texts list; Output: splits tuple indicating indices for train/validation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nsplits = GrandparentSplitter(valid_name='test')(texts)\n```\n\n----------------------------------------\n\nTITLE: Converting Text Data to NumPy Array - Python\nDESCRIPTION: This snippet converts the fastai `L` object `ss` containing the sample text reviews into a NumPy array `sarr`. This is done to explore if NumPy's built-in string processing functions offer better performance for tokenization or related operations compared to standard Python list processing.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nsarr = np.array(ss)\n```\n\n----------------------------------------\n\nTITLE: Committing changes with message\nDESCRIPTION: Records staged file changes with a descriptive message, preparing the commit for pushing to remote repository.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/dev-setup.ipynb#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\ngit commit -am \"just testing\"\n```\n\n----------------------------------------\n\nTITLE: Resize Image Function\nDESCRIPTION: This function resizes a single image file, potentially converting it to RGB, based on size constraints and other parameters. It opens the image, resizes it if the height or width exceeds `max_size` or the number of channels does not match `n_channels`, and saves the resized image to a destination. It handles file copying if no resizing is needed. It includes optional parameters for resampling, image format, and resume functionality to avoid reprocessing files.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ndef resize_image(file, dest, src='.', max_size=None, n_channels=3, ext=None,\n                 img_format=None, resample=BILINEAR, resume=False, **kwargs ):\n    \"Resize file to dest to max_size\"\n    dest = Path(dest)\n    \n    dest_fname = dest/file\n    dest_fname.parent.mkdir(exist_ok=True, parents=True)\n    file = Path(src)/file\n    if resume and dest_fname.exists(): return\n    if not verify_image(file): return\n\n    img = Image.open(file)\n    imgarr = np.array(img)\n    img_channels = 1 if len(imgarr.shape) == 2 else imgarr.shape[2]\n    if ext is not None: dest_fname=dest_fname.with_suffix(ext)\n    if (max_size is not None and (img.height > max_size or img.width > max_size)) or img_channels != n_channels:\n        if max_size is not None:\n            new_sz = resize_to(img, max_size)\n            img = img.resize(new_sz, resample=resample)\n        if n_channels == 3: img = img.convert(\"RGB\")\n        img.save(dest_fname, img_format, **kwargs)\n    elif file != dest_fname : shutil.copy2(file, dest_fname)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Spacy `pipe` Tokenization - Jupyter\nDESCRIPTION: This snippet uses `%%timeit` to benchmark the performance of Spacy's optimized `pipe` method for tokenizing a list of strings sequentially. The `nlp.tokenizer.pipe` method processes inputs in batches and is generally faster than tokenizing strings one by one. The results are converted to strings using `conv_sp` and stored globally in `t`. Runs for 3 loops (`-r 3`).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n%%timeit -r 3\nglobal t\nt = L(nlp.tokenizer.pipe(ss)).map(conv_sp)\n```\n\n----------------------------------------\n\nTITLE: Defining TstLearner for Metric Testing - Python\nDESCRIPTION: Defines a toy subclass of Learner called TstLearner for testing metric accumulation. It overrides __init__ to initialize pred, xb, and yb as None and accepts dls or model parameters. No special dependencies except fastai. Used for metric test cycles where real data loaders are not required.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n@delegates()\nclass TstLearner(Learner):\n    def __init__(self,dls=None,model=None,**kwargs): self.pred,self.xb,self.yb = None,None,None\n```\n\n----------------------------------------\n\nTITLE: Plotting Recorded Schedules - fastai/Python\nDESCRIPTION: Plots the schedules of hyperparameters (typically learning rate and momentum) that were used during the training process, as recorded by the `Learner`. This visualization is useful for understanding the dynamics of the training process, especially with policies like 'one-cycle'.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nlearn.recorder.plot_sched()\n```\n\n----------------------------------------\n\nTITLE: Find top movies\nDESCRIPTION: Groups the `rating_movie` DataFrame by movie title and counts the number of ratings for each movie. Then, it sorts the movies by rating count in descending order and selects the top 1000 movies. This is used to focus analysis on the most popular movies.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ng = rating_movie.groupby('title')['rating'].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_movies[:10]\n```\n\n----------------------------------------\n\nTITLE: Defining Example Transforms for TfmdLists Usage in fastai (Python)\nDESCRIPTION: Defines two simple example `Transform` objects for demonstrating `TfmdLists`. `_IntFloatTfm` encodes input to `TitledInt` and decodes back to `TitledFloat`. `neg_tfm` uses a function `_neg` to negate the input during both encoding and decoding.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nclass _IntFloatTfm(Transform):\n    def encodes(self, o):  return TitledInt(o)\n    def decodes(self, o):  return TitledFloat(o)\nint2f_tfm=_IntFloatTfm()\n\ndef _neg(o): return -o\nneg_tfm = Transform(_neg, _neg)\n```\n\n----------------------------------------\n\nTITLE: Accessing Processed Features (TabularPandas)\nDESCRIPTION: Shows the first three rows of the preprocessed feature data (`xs`) stored within the `TabularPandas` object. This demonstrates how to access the data after fastai's processing, useful for integrating with other libraries.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nto.xs[:3]\n```\n\n----------------------------------------\n\nTITLE: Test SequentialEx Original Input Cleanup in Python\nDESCRIPTION: This test verifies that the `orig` attribute is correctly set back to `None` on the output tensor after the `SequentialEx` forward pass. This is crucial for preventing memory leaks by ensuring that the original input tensor (`x`) is not held by a reference within the output (`y`), allowing it to be garbage collected when no longer needed.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_60\n\nLANGUAGE: python\nCODE:\n```\nx = TensorBase(torch.randn(32, 16, 8, 8))\ny = res_block(x)\ntest_is(y.orig, None)\n```\n\n----------------------------------------\n\nTITLE: Installing nbdev Hooks (Bash)\nDESCRIPTION: This command installs necessary git and Jupyter hooks provided by nbdev. These hooks automate tasks like notebook cleaning, trusting, and handling merge conflicts, simplifying the contribution workflow and should be run after cloning the repository.\nSOURCE: https://github.com/fastai/fastai/blob/main/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnbdev_install_hooks\n```\n\n----------------------------------------\n\nTITLE: Find Learning Rate (Classifier)\nDESCRIPTION: Runs the learning rate finder to identify the optimal learning rate for training the text classifier.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nlearn.lr_find()\n```\n\n----------------------------------------\n\nTITLE: Extracting Movie Bias\nDESCRIPTION: This code extracts the bias for a set of movies using the trained collaborative filtering model. The `learn.model.bias()` method retrieves the bias values for the specified `top_movies` (most rated movies). The `is_item=True` parameter indicates that we're retrieving the item biases, essential for analysis.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmovie_bias = learn.model.bias(top_movies, is_item=True)\n```\n\n----------------------------------------\n\nTITLE: Printing Model Architecture - PyTorch/Python\nDESCRIPTION: Prints a string representation of the `model` object, showing its architecture. In this case, it will display the `Mnist_Logistic` module with its single linear layer.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmodel\n```\n\n----------------------------------------\n\nTITLE: Defining Custom DataLoader Exception SkipItemException - Python\nDESCRIPTION: Implements a custom exception used by the DataLoader to signal that a specific dataset item should be skipped during iteration or batch creation. This enables robust error-tolerant iteration over datasets with potentially problematic entries. No dependencies beyond standard inheritance.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass SkipItemException(Exception):\n    \"Raised to notify `DataLoader` to skip an item\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: ArrayImage Subclass Representing a Generic Image Array in Python\nDESCRIPTION: The `ArrayImage` subclass inherits from `ArrayImageBase` without modification, representing a standard image array. It inherits visualization and casting behavior from parent classes, providing a convenient fastai-compatible numpy ndarray subtype for colored image data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass ArrayImage(ArrayImageBase):\n    \"An array representing an image\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: In-place tensor unsqueezing with multiple dimensions (Python)\nDESCRIPTION: The `unsqueeze_` function performs in-place addition of singleton dimensions to a tensor. It modifies the original tensor by repeatedly calling `unsqueeze_` `n` times at the specified dimension. It is useful for reshaping tensors without creating new objects.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\n#|export\ndef unsqueeze_(x, dim=-1, n=1):\n    \"Same as `torch.unsqueeze_` but can add `n` dims\"\n    for _ in range(n): x.unsqueeze_(dim)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Testing Annotations Loading Python\nDESCRIPTION: This code tests the `get_annotations` function using the COCO Tiny dataset. It downloads and extracts the dataset, then calls `get_annotations` to load annotations. It compares the loaded filenames with the dataset's expected image filenames and validates the bounding box labels. It defines a helper function `bbox_lbls` to retrieve bounding boxes and labels by filename and utilizes `test_eq` for assertions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ncoco = untar_data(URLs.COCO_TINY)\ntest_images, test_lbl_bbox = get_annotations(coco/'train.json')\nannotations = json.load(open(coco/'train.json'))\ncategories, images, annots = map(lambda x:L(x),annotations.values())\n\ntest_eq(test_images, images.attrgot('file_name'))\n\ndef bbox_lbls(file_name):\n    img = images.filter(lambda img:img['file_name']==file_name)[0]\n    bbs = annots.filter(lambda a:a['image_id'] == img['id'])\n    i2o = {k['id']:k['name'] for k in categories}\n    lbls = [i2o[cat] for cat in bbs.attrgot('category_id')]\n    bboxes = [[bb[0],bb[1], bb[0]+bb[2], bb[1]+bb[3]] for bb in bbs.attrgot('bbox')]\n    return [bboxes, lbls]\n\nfor idx in random.sample(range(len(images)),5): \n    test_eq(test_lbl_bbox[idx], bbox_lbls(test_images[idx]))\n```\n\n----------------------------------------\n\nTITLE: DataFrame Shrinking Tests (Python)\nDESCRIPTION: These test cases verify the functionality of the `df_shrink_dtypes` function. They check if the function correctly identifies smaller data types for integer and float columns, handles object columns by converting them to categories by default and also provides option to turn off the categorifying of 'object' type columns using obj2cat parameter. Dependencies: test_eq\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntest_eq(df['i'].dtype, 'int64')\ntest_eq(dt['i'], 'int8')\n\ntest_eq(df['f'].dtype, 'float64')\ntest_eq(dt['f'], 'float32')\n\n# Default ignore 'object' and 'boolean' columns\ntest_eq(df['date'].dtype, 'object')\ntest_eq(dt['date'], 'category')\n\n# Test categorifying 'object' type\ndt2 = df_shrink_dtypes(df, obj2cat=False)\ntest_eq('date' not in dt2, True)\n```\n\n----------------------------------------\n\nTITLE: Defining synth_learner: Synthetic Learner Factory for fastai (Python)\nDESCRIPTION: This snippet defines the 'synth_learner' factory method for creating a simple Learner instance, which by default trains a linear regression model on synthetic data. It requires 'synth_dbunch', 'Learner', 'RegModel', and 'MSELossFlat' as dependencies. Parameters allow customization of data sizes, device, dataloader type, learning rate, and other keyword arguments.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n#|hide\ndef synth_learner(n_train=10, n_valid=2, cuda=False, tfmdDL=True, lr=defaults.lr, **kwargs):\n    data = synth_dbunch(n_train=n_train,n_valid=n_valid, cuda=cuda, tfmdDL=tfmdDL)\n    return Learner(data, RegModel(), loss_func=MSELossFlat(), lr=lr, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Testing DiceLoss with Half Precision on CUDA in PyTorch\nDESCRIPTION: Tests the `DiceLoss` function's compatibility with half-precision floating-point tensors and CUDA (GPU) if available. It creates random half-precision tensors for output and target and calls the loss function to ensure it runs without error. Requires PyTorch and CUDA.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#|cuda\n#Test DicceLoss work in half precision\nif torch.cuda.is_available():\n    output = torch.randn(32, 4, 5, 10).half().cuda()\n    target = torch.randint(0,2,(32, 5, 10)).half().cuda()\n    _ = dl(output, target)\n```\n\n----------------------------------------\n\nTITLE: Displaying Titled Image using fastai's show_image in Python\nDESCRIPTION: The `show_titled_image` function is a convenience wrapper that takes a tuple `(image, title)` and calls `show_image` with these arguments. It extracts the image and title from the tuple and passes additional parameters, enabling easy rendering of images accompanied by descriptive titles.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@delegates(show_image, keep=True)\ndef show_titled_image(o, **kwargs):\n    \"Call `show_image` destructuring `o` to `(img,title)`\"\n    show_image(o[0], title=str(o[1]), **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Creating a Config Dictionary for TabularModel Initialization in Python\nDESCRIPTION: Defines a convenience function `tabular_config` that delegates keyword arguments to the `TabularModel` constructor, returning a configuration dictionary. This facilitates passing and managing configuration parameters for the model, such as dropout probabilities or batch normalization flags, in a centralized and structured way.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/42_tabular.model.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@delegates(TabularModel.__init__)\ndef tabular_config(**kwargs):\n    \"Convenience function to easily create a config for `TabularModel`\"\n    return kwargs\n```\n\n----------------------------------------\n\nTITLE: One-Hot Encoding Tensor or Iterable Labels with Specified Classes in Python\nDESCRIPTION: Computes a one-hot encoded tensor of specified class size 'c' from input 'x', which can be a tensor or iterable of indices. It returns a byte tensor with 1s at positions indicated by 'x' and zeros elsewhere, supporting both empty and multi-class inputs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_74\n\nLANGUAGE: python\nCODE:\n```\ndef one_hot(x, c):\n    \"One-hot encode `x` with `c` classes.\"\n    res = torch.zeros(c, dtype=torch.uint8)\n    if isinstance(x, Tensor) and x.numel()>0: res[x] = 1.\n    else: res[list(L(x, use_list=None))] = 1.\n    return res\n```\n\n----------------------------------------\n\nTITLE: Conditional Import of TorchVision Weights API (Version >= 0.13)\nDESCRIPTION: This snippet checks the installed torchvision version, and if >= 0.13, imports 'ResNet34_Weights' to use the multi-weight API for model initialization. It sets 'weights' accordingly for use in model creation. Dependencies include torchvision and packaging, useful for dynamic weight assignment based on version.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\nif parse(torchvision.__version__) >= parse('0.13'):\n    from torchvision.models import ResNet34_Weights\n    weights = ResNet34_Weights.IMAGENET1K_V1\nelse:\n    weights = None\n```\n\n----------------------------------------\n\nTITLE: Creating a Horizontal Scrolling Carousel Widget\nDESCRIPTION: Defines a function `carousel` that creates an `ipywidgets.Box` configured as a horizontally scrolling container. It takes a sequence of child widgets and optional layout parameters. It observes changes to its children to apply default flex styling using `_update_children`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09c_vision.widgets.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef carousel(\n    children:tuple|MutableSequence=(), # `Box` objects to display in a carousel\n    **layout\n) -> Box: # An `ipywidget`'s carousel\n    \"A horizontally scrolling carousel\"\n    def_layout = dict(overflow='scroll hidden', flex_flow='row', display='flex')\n    res = Box([], layout=merge(def_layout, layout))\n    res.observe(_update_children, names='children')\n    res.children = children\n    return res\n```\n\n----------------------------------------\n\nTITLE: Creating Max Pooling Layer (Python)\nDESCRIPTION: Returns a PyTorch Max Pooling layer (`nn.MaxPool1d`, `nn.MaxPool2d`, or `nn.MaxPool3d`) based on the specified number of dimensions (`ndim`). Parameters include kernel size (`ks`), stride, padding, and ceil mode. Asserts that `ndim` is between 1 and 3.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef MaxPool(ks=2, stride=None, padding=0, ndim=2, ceil_mode=False):\n    \"nn.MaxPool layer for `ndim`\"\n    assert 1 <= ndim <= 3\n    return getattr(nn, f\"MaxPool{ndim}d\")(ks, stride=stride, padding=padding)\n```\n\n----------------------------------------\n\nTITLE: Defining a test callback to verify channels last format post-prediction\nDESCRIPTION: Implements a callback that asserts the output tensor is in channels last format immediately after the model's prediction. Useful for validating the effectiveness of channels last conversion during training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18c_callback.channelslast.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\nclass ChannelsLastTest(Callback):\n    \"Asserts that predictions are in channels last format\"\n    order = MixedPrecision.order-1\n    def after_pred(self):\n        assert self.pred.is_contiguous(memory_format=torch.channels_last), \"Model and/or output isn't channels last\"\n```\n\n----------------------------------------\n\nTITLE: Defining L1LossFlat - Fastai/PyTorch\nDESCRIPTION: This function defines `L1LossFlat` within the fastai library. It wraps PyTorch's `nn.L1Loss` using the `BaseLoss` class, providing automatic flattening of input and target tensors. It supports standard `L1Loss` arguments like `reduction`, along with fastai-specific ones like `axis` and `floatify`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@use_kwargs_dict(reduction='mean')\ndef L1LossFlat(\n    *args, \n    axis=-1, # Class axis\n    floatify=True, # Convert `targ` to `float`\n    **kwargs\n):\n    \"Same as `nn.L1Loss`, but flattens input and target.\"\n    return BaseLoss(nn.L1Loss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Installing fastai Library on Colab\nDESCRIPTION: Shell command to upgrade fastai library in a Google Colab environment by installing or updating the package if the /content directory exists.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Tokenizing DataFrame with SentencePieceTokenizer Directly (Python)\nDESCRIPTION: Demonstrates using the `SentencePieceTokenizer` in conjunction with fastai's lower-level `tokenize_df` function. This shows how to apply the SentencePiece tokenization directly to a pandas DataFrame column, passing specific configuration like `vocab_sz` to the tokenizer instance. Requires pandas and fastai text processing functions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/30_text.core.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntexts = [f\"This is an example of text {i}\" for i in range(10)]\ndf = pd.DataFrame({'text': texts, 'label': list(range(10))}, columns=['text', 'label'])\nout,cnt = tokenize_df(df, text_cols='text', tok=SentencePieceTokenizer(vocab_sz=34), n_workers=1)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Learning Rate Suggestion Functions in Python\nDESCRIPTION: These code snippets describe the expected interface for creating custom learning rate suggestion functions compatible with the LR Finder. Functions must accept learning rates, losses, and iteration count parameters and return a tuple containing a suggested learning rate and its details. The use of functools.partial is shown to handle extra parameters cleanly.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndef myfunc(lrs:list, losses:list, num_it:int) -> tuple(float, tuple(float,int)):\n    ...\n    return suggestion, (suggestion,loss_idx)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef myfunc(lrs:list, losses:list, num_it:int, pct_reduction:float) -> tuple(float, tuple(float,int)):\n    ...\n    return suggestion, (suggestion,loss_idx)\n```\n\nLANGUAGE: python\nCODE:\n```\nf = partial(myfunc, pct_reduction=.2)\n```\n\n----------------------------------------\n\nTITLE: Training a Lightning Model with fastai Learner in Python\nDESCRIPTION: Initializes a fastai `Learner` instance. It takes the `DataLoaders` (`data`), the Lightning model (`model`), `F.cross_entropy` as the loss function, `Adam` as the optimizer, and `accuracy` as the evaluation metric. Subsequently, it trains the model for one epoch using the `fit_one_cycle` method with a maximum learning rate of 0.001.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_lightning.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlearn = Learner(data, model, loss_func=F.cross_entropy, opt_func=Adam, metrics=accuracy)\nlearn.fit_one_cycle(1, 0.001)\n```\n\n----------------------------------------\n\nTITLE: Importing nbdev Show Documentation Utility - Python\nDESCRIPTION: Imports nbdev.showdoc to assist with generating API documentation in notebooks. No effect on layer execution; purely for documentation workflows.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Freezing All Model Layers for New Cycle - Python\nDESCRIPTION: Freezes all layers for subsequent head-only training phase on new DataLoaders. Ensures only classifier top layers are trained. Used before initializing new learning rate search.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nlearn.freeze()\n```\n\n----------------------------------------\n\nTITLE: Installing Neptune Client via Conda\nDESCRIPTION: Alternative shell command to install the `neptune-client` library using Conda from the conda-forge channel.\nSOURCE: https://github.com/fastai/fastai/blob/main/disabled/70b_callback.neptune.ipynb#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nconda install neptune-client -c conda-forge\n```\n\n----------------------------------------\n\nTITLE: Testing Test Data Loader with Custom Transform and Multiple Inputs\nDESCRIPTION: This test creates a test dataloader for specific test items, verifies the number of inputs, and confirms the data batch reflects proper transformation, including the ability to apply additional item transforms like `add1`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_74\n\nLANGUAGE: Python\nCODE:\n```\ndsets = Datasets(range(8), [[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]])\ndls = dsets.dataloaders(bs=4, device=torch.device('cpu'))\ntst_dl = dls.test_dl([2,3,4,5])\ntest_eq(tst_dl._n_inp, 1)\ntest_eq(list(tst_dl), [(tensor([ 4,  6,  8, 10]),)])\n#Test you can change transforms\ntst_dl = dls.test_dl([2,3,4,5], after_item=add1)\ntest_eq(list(tst_dl), [(tensor([ 5,  7,  9, 11]),)])\n```\n\n----------------------------------------\n\nTITLE: Setting random seeds for reproducibility in PyTorch, NumPy, and random (Python)\nDESCRIPTION: The `set_seed` function initializes random number generators for `random`, `torch`, and `numpy` to ensure reproducibility. It accepts a seed value and a `reproducible` flag that, when True, enforces determinism in CUDA operations. It is used prior to stochastic operations to guarantee consistent results across runs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n#|export\ndef set_seed(s, reproducible=False):\n    \"Set random seed for `random`, `torch`, and `numpy` (where available)\"\n    try: torch.manual_seed(s)\n    except NameError: pass\n    try: torch.cuda.manual_seed_all(s)\n    except NameError: pass\n    try: np.random.seed(s%(2**32-1))\n    except NameError: pass\n    random.seed(s)\n    if reproducible:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n```\n\n----------------------------------------\n\nTITLE: Setting up Segmentation Target with Class Imbalance Example - PyTorch\nDESCRIPTION: This snippet sets up a sample target tensor representing a segmentation mask with class imbalance. It creates a 100x100 image where most pixels are class 0 (background), with thin lines representing class 1 (river) and class 2 (road), illustrating a common scenario for segmentation datasets.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ntarget = torch.zeros(100,100)\ntarget[:,5] = 1\ntarget[:,50] = 2\nplt.imshow(target);\n```\n\n----------------------------------------\n\nTITLE: Defining Full Variable Lists - Python\nDESCRIPTION: Defines the dependent variable ('Sales') and comprehensive lists of categorical and continuous feature names used for the full dataset.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndep_var = 'Sales'\ncat_names = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'StoreType', 'Assortment', \n    'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear', 'State', 'Week', 'Events', 'Promo_fw', \n    'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw', 'SchoolHoliday_fw', 'SchoolHoliday_bw']\n\ncont_names = ['CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC', \n    'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', 'Mean_Wind_SpeedKm_h', \n    'CloudCover', 'trend', 'trend_DE', 'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday']\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders from Datasets with Transforms\nDESCRIPTION: This snippet converts a dataset with range 0-7 and transforms into dataloaders with batch size 4. The train and validation dataloaders are checked for correct batching and transformations, and the number of input features is verified.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_59\n\nLANGUAGE: Python\nCODE:\n```\n# only transform subset 1\nclass _Tfm(Transform):\n    split_idx=1\n    def encodes(self, x): return x*2\n\ndsets = Datasets(range(8), [None], splits=[[1,2,5,7],[0,3,4,6]])\ndls = dsets.dataloaders(bs=4, after_batch=_Tfm(), shuffle=False, device=torch.device('cpu'))\ntest_eq(dls.train, [(tensor([1,2,5, 7]),)])\ntest_eq(dls.valid, [(tensor([0,6,8,12]),)])\ntest_eq(dls.n_inp, 1)\n```\n\n----------------------------------------\n\nTITLE: Training tabular model on Adult dataset using notebook_launcher\nDESCRIPTION: Defines a function to train a tabular model on the Adult Census dataset, processing categorical and continuous features, then launches it across 2 GPUs using notebook_launcher.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/distributed_app_examples.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.ADULT_SAMPLE)\ndf = pd.read_csv(path/'adult.csv')\n\n\ndef train():\n    dls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n            cat_names = ['workclass', 'education', 'marital-status', 'occupation',\n                         'relationship', 'race'],\n            cont_names = ['age', 'fnlwgt', 'education-num'],\n            procs = [Categorify, FillMissing, Normalize])\n\n    learn = tabular_learner(dls, metrics=accuracy)\n    with learn.distrib_ctx(in_notebook=True):\n        learn.fit_one_cycle(3)\n        \nnotebook_launcher(train, num_processes=2)\n```\n\n----------------------------------------\n\nTITLE: Read Wheel RECORD using Conda Function (Python)\nDESCRIPTION: Uses the imported `read_python_record` function from the `conda` library to parse the `RECORD` file of the Wheel. It requires the parent directory of the wheel file, the path to the `RECORD` file within the wheel, and the target Python version ('3.7'). It then accesses the `paths` attribute of the resulting `paths_data` object.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nread_python_record(whl.parent, fwhl.record_path, '3.7')['paths_data'].paths\n```\n\n----------------------------------------\n\nTITLE: Importing FP16 Utility Functions from fastai Python\nDESCRIPTION: This import statement brings in necessary utility functions from the `fastai.fp16_utils` module. These functions, originally adapted from the APEX library, are used for tasks like converting model weights and handling gradient copies between different precision types.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#|export \nfrom fastai.fp16_utils import convert_network, model_grads_to_master_grads, master_params_to_model_params\n```\n\n----------------------------------------\n\nTITLE: Load and Inspect CSV Data\nDESCRIPTION: Loads the `texts.csv` file from the specified path into a Pandas DataFrame using `pd.read_csv` and displays the first few rows using `df.head()`. This allows for quick inspection of the structure of the data.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(path/'texts.csv')\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Defining Function Creating Tokens Without Returning Them - Python\nDESCRIPTION: This snippet defines a Python function `f` that takes a list of `items`. Inside the function, it iterates through the items and performs a string split operation, similar to tokenization, but it stores the results in a local list `o`. However, the function *returns* a different list containing the original items, not the tokens. This is designed to benchmark the cost of computation within the worker process versus the cost of returning data.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef f(items):\n    o = [s.split(' ') for s in items]\n    return [s for s in items]\n```\n\n----------------------------------------\n\nTITLE: Running nbdev Tests (Bash)\nDESCRIPTION: Launches the nbdev test runner, which executes tests defined within the notebooks, often in parallel. This is a quick way to verify changes locally before submitting a pull request.\nSOURCE: https://github.com/fastai/fastai/blob/main/CONTRIBUTING.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nnbdev_test\n```\n\n----------------------------------------\n\nTITLE: Perform PCA on movie weights\nDESCRIPTION: Performs Principal Component Analysis (PCA) on the movie weight vectors to reduce the dimensionality to 3 components. This simplifies the representation of movie features and allows for visualization.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nmovie_pca = movie_w.pca(3)\nmovie_pca.shape\n```\n\n----------------------------------------\n\nTITLE: Implementing DistributedDL for Distributed Data Loading\nDESCRIPTION: Creates a specialized DataLoader for distributed training that splits batches across workers. Handles batch creation, broadcasting of indices, padding for even distribution, and proper gathering of results across processes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass DistributedDL(TfmdDL):\n    \"A `TfmdDL` which splits a batch into equal size pieces for each worker\"\n    def __init__(self,dl,rank=None,world_size=None,device=None):\n        if rank is None: rank=rank_distrib()\n        if world_size is None: world_size=num_distrib()\n        store_attr()\n        if type(dl) == torch.utils.data.DataLoader:\n            shuffle = True if eq(type(dl.sampler), torch.utils.data.RandomSampler) else False\n            self.dl = DataLoader(dataset=dl.dataset, bs=dl.batch_size, num_workers=dl.num_workers, \\\n                pin_memory=dl.pin_memory, timeout=dl.timeout, shuffle=shuffle, drop_last=dl.drop_last, persistent_workers=dl.persistent_workers)\n        self.bs,self.drop_last,self.dataset,fake,self.num_workers,self.offs,self.pin_memory = \\\n            attrgetter('bs','drop_last','dataset','fake_l','num_workers','offs','pin_memory')(self.dl)\n        if device is None: self.device = self.dl.device\n        self.fake_l = _FakeLoader(self, fake.pin_memory, fake.num_workers, fake.timeout, \n                                  persistent_workers=fake.persistent_workers, \n                                  pin_memory_device=fake.pin_memory_device)\n        \n    def _broadcast(self,t,rank):\n        \"Broadcasts t from rank `rank` to all other ranks. Returns t so t is same for all ranks after call.\"\n        t = LongTensor(t).cuda() # nccl only works with cuda tensors\n        torch.distributed.broadcast(t,rank)\n        return t.cpu().tolist()\n\n    def _to_detach(self,b,cpu=True,gather=True): return to_detach(b,cpu,gather) # member func so we can override for test\n    def __len__(self): return _round_to_multiple(len(self.dl),self.world_size)//self.world_size\n    def get_idxs(self):\n        idxs = list(self.dl.get_idxs()) # compute get_idxs in all ranks (we'll only use rank 0 but size must be consistent)\n        idxs = self._broadcast(idxs,0)  # broadcast and receive it from rank 0 to all\n        self.n = len(idxs)              # we assumed n was dl.n but we really care about number of idxs\n        # add extra samples to make it evenly divisible\n        self.n_padded = _round_to_multiple(self.n,self.world_size)\n        idxs += (idxs * (self.n_padded//self.n))[:self.n_padded-self.n] # idx needs to be repeated when n_padded>>n\n        # slice padded idxs so that each rank gets self.n_padded//self.world_size tensors\n        return idxs[self.rank*self.n_padded//self.world_size:(self.rank+1)*self.n_padded//self.world_size]\n\n    def before_iter(self):\n        self.i = 0\n        self.dl.before_iter()\n\n    def randomize(self): self.dl.randomize()\n    def after_batch(self,b):\n        self.i += find_bs(b)\n        return self.dl.after_batch(b)\n\n    def after_iter(self):  self.dl.after_iter()\n    def create_batches(self,samps): return self.dl.create_batches(samps)\n    def to_detach(self,b, cpu=True, gather=True):\n        b = self._to_detach(b, cpu, gather)\n        def _inner(b):\n            if b.ndim>0:\n                # for each rank, compute overflow of read idxs vs self.n and accumulate them to unpad totals after gathering\n                n = sum([min(0,max(-len(b)//self.world_size,\n                                   self.n-(self.i+r*self.n_padded//self.world_size))) for r in range(self.world_size)])\n                b = b[:n or None]\n            return b\n        return apply(_inner,b) if gather and all(hasattr(self,o) for o in ('i','n','n_padded')) else b\n```\n\n----------------------------------------\n\nTITLE: Defining Semantic Tensor Types for Text Data in fastai (Python)\nDESCRIPTION: Creates two subclassed tensor types: TensorText and LMTensorText, both derived from TensorBase. These are semantic markers used in fastai to indicate tensors representing text data, with LMTensorText specifically for language modeling tasks. This helps with type-based dispatching and clearer code semantics in the fastai framework. There is no additional functionality beyond the superclass inheritance.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass TensorText(TensorBase):   pass\nclass LMTensorText(TensorText): pass\n\nTensorText.__doc__ = \"Semantic type for a tensor representing text\"\nLMTensorText.__doc__ = \"Semantic type for a tensor representing text in language modeling\"\n```\n\n----------------------------------------\n\nTITLE: Defining Polynomial Scheduling Function in Python\nDESCRIPTION: Creates a polynomial schedule function that returns a callable mapping position pos to a value interpolated between start and end with pos raised to a given power. This facilitates custom-shaped annealing schedules controlled by the power parameter.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef SchedPoly(start, end, power):\n    \"Polynomial schedule (of `power`) function from `start` to `end`\"\n    def _inner(pos): return start + (end - start) * pos ** power\n    return _inner\n```\n\n----------------------------------------\n\nTITLE: Defining SE and ResNeXt Block Configuration Dictionaries and Layer Groupings in Python\nDESCRIPTION: Sets up dictionaries for parameters configuring SE blocks with group and reduction values. Defines lists g0 through g3 corresponding to layer configurations used in XResNet variants, which specify the number of residual units per layer group. These constants are used when instantiating models with SE or ResNeXt blocks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/11_vision.models.xresnet.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nse_kwargs1 = dict(groups=1 , reduction=16)\nse_kwargs2 = dict(groups=32, reduction=16)\nse_kwargs3 = dict(groups=32, reduction=0)\ng0 = [2,2,2,2]\ng1 = [3,4,6,3]\ng2 = [3,4,23,3]\ng3 = [3,8,36,3]\n```\n\n----------------------------------------\n\nTITLE: Defining nbdev Export Module (nbdev)\nDESCRIPTION: This `nbdev` directive specifies the default Python module name to which the code cells marked for export (`#|export`) in this notebook should be saved. It tells `nbdev` to export relevant code into a file structure corresponding to `fastai.medical.text`, organizing the project code based on the notebook structure.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/65_medical.text.ipynb#_snippet_1\n\nLANGUAGE: nbdev\nCODE:\n```\n#|default_exp medical.text\n```\n\n----------------------------------------\n\nTITLE: TensorBoard Callback for Monitoring and Logging\nDESCRIPTION: Creates a callback class extending the base, responsible for saving model graph, logging losses, metrics, and predictions to TensorBoard during training and validation, with options for model tracing and feature projection.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nclass TensorBoardCallback(TensorBoardBaseCallback):\n    \"Saves model topology, losses & metrics for tensorboard and tensorboard projector during training\"\n    def __init__(self, log_dir=None, trace_model=True, log_preds=True, n_preds=9, projector=False, layer=None):\n        super().__init__()\n        store_attr()\n\n    def before_fit(self):\n        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\") and rank_distrib()==0\n        if not self.run: return\n        self._setup_writer()\n        if self.trace_model:\n            if hasattr(self.learn, 'mixed_precision'):\n                raise Exception(\"Can't trace model in mixed precision, pass `trace_model=False` or don't use FP16.\")\n            b = self.dls.one_batch()\n            self.learn._split(b)\n            self.writer.add_graph(self.model, *self.xb)\n\n    def after_batch(self):\n        self.writer.add_scalar('train_loss', self.smooth_loss, self.train_iter)\n        for i,h in enumerate(self.opt.hypers):\n            for k,v in h.items(): self.writer.add_scalar(f'{k}_{i}', v, self.train_iter)\n\n    def after_epoch(self):\n        for n,v in zip(self.recorder.metric_names[2:-1], self.recorder.log[2:-1]):\n            self.writer.add_scalar(n, v, self.train_iter)\n        if self.log_preds:\n            b = self.dls.valid.one_batch()\n            self.learn.one_batch(0, b)\n            preds = getcallable(self.loss_func, 'activation')(self.pred)\n            out = getcallable(self.loss_func, 'decodes')(preds)\n            x,y,its,outs = self.dls.valid.show_results(b, out, show=False, max_n=self.n_preds)\n            tensorboard_log(x, y, its, outs, self.writer, self.train_iter)\n            \n    def before_validate(self):\n        if self.projector: self._setup_projector()\n```\n\n----------------------------------------\n\nTITLE: Model Metadata Dictionary for AWD_LSTM Architecture in Python\nDESCRIPTION: Defines a dictionary containing metadata for the AWD_LSTM architecture including hyperparameter names, URLs for pretrained weights, and configurations for language modeling and classification tasks. This dictionary centralizes architecture-specific settings used later for model creation and configuration.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n_model_meta = {AWD_LSTM: {'hid_name':'emb_sz', 'url':URLs.WT103_FWD, 'url_bwd':URLs.WT103_BWD,\n                          'config_lm':awd_lstm_lm_config, 'split_lm': awd_lstm_lm_split,\n                          'config_clas':awd_lstm_clas_config, 'split_clas': awd_lstm_clas_split},}\n```\n\n----------------------------------------\n\nTITLE: Manual Training Update with Adam Optimizer - PyTorch/Python\nDESCRIPTION: Defines a new function `update` that performs a single training step using the `torch.optim.Adam` optimizer. It calculates the loss, computes gradients using `loss.backward()`, updates model parameters using `opt.step()`, and zeros the gradients using `opt.zero_grad()`. It returns the loss value.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndef update(x,y,lr):\n    opt = torch.optim.Adam(model.parameters(), lr)\n    y_hat = model(x)\n    loss = loss_func(y_hat, y)\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n    return loss.item()\n```\n\n----------------------------------------\n\nTITLE: ndarray Subclass for Arrays with Specialized Casting Behavior in Python\nDESCRIPTION: The `ArrayBase` class subclasses numpy's ndarray to modify or extend casting behavior. It defines a classmethod `_before_cast` which ensures input is an ndarray or converts it. This forms a foundation for image array subclasses that include display methods and metadata handling.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass ArrayBase(ndarray):\n    \"An `ndarray` that can modify casting behavior\"\n    @classmethod\n    def _before_cast(cls, x): return x if isinstance(x,ndarray) else array(x)\n```\n\n----------------------------------------\n\nTITLE: Formatting Configuration Value Recursively\nDESCRIPTION: The `_format_config_value` function recursively formats a configuration value for logging.  If the value is a list, it processes each item. If the value has a `__stored_args__` attribute, it formats those arguments using `_format_config` and adds a `_name` key with the value itself. Otherwise, it returns the value as is.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef _format_config_value(v):\n    if isinstance(v, list):\n        return [_format_config_value(item) for item in v]\n    elif hasattr(v, '__stored_args__'):\n        return {**_format_config(v.__stored_args__), '_name': v}\n    return v\n```\n\n----------------------------------------\n\nTITLE: Importing fastai Tabular Module in Python\nDESCRIPTION: This snippet imports all relevant components from the fastai tabular module, which is used for manipulating and modeling structured/tabular data. The dependency is the fastai Python library. Importing this module allows access to key functions and classes needed for the remaining data processing and modeling steps.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.tabular.all import *\n```\n\n----------------------------------------\n\nTITLE: Pushing a fastai Learner to Hugging Face Hub (Python)\nDESCRIPTION: Python code snippet showing how to upload a trained fastai `Learner` object to a specified repository on the Hugging Face Hub using the `push_to_hub_fastai` function. Requires a logged-in Hugging Face user (via CLI, `notebook_login`, or token argument) with write access to the target repository.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/74_huggingface.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import push_to_hub_fastai\n\n# repo_id = \"YOUR_USERNAME/YOUR_LEARNER_NAME\"\nrepo_id = \"espejelomar/identify-my-cat\"\n\npush_to_hub_fastai(learner=learn, repo_id=repo_id)\n```\n\n----------------------------------------\n\nTITLE: Unpacking tar.7z Data with 7za and tar - Python\nDESCRIPTION: Shell command (commented out) to extract the contents of the compressed train-jpg.tar.7z archive directly to the dataset path using 7za and tar utilities. Requires 7zip and tar installed on the system. Processes may take several minutes depending on file size.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#! 7za -bd -y -so x {path}/train-jpg.tar.7z | tar xf - -C {path.as_posix()}\n```\n\n----------------------------------------\n\nTITLE: Notebook Default Export Directive (Python)\nDESCRIPTION: This snippet is a special nbdev notebook directive indicating the default export module for generated code. It is not executable Python logic but helps nbdev organize exported source code modules. The directive expects a string path (e.g., 'callback.fp16') and does not produce a runtime output.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp callback.fp16\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Test Callback for Recorder in fastai (Python)\nDESCRIPTION: Implements the 'TestRecorderCallback' to thoroughly test Recorder's interaction with fastai's training process. It validates the proper updating of metrics, loss, and logging, performs state resets, and uses assertions for checking variable values at various stages. The custom 'test_log' function inspects log content post-epoch. Dependencies include fastai's Callback/Metric classes and test utilities such as test_eq/test_close.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_70\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nclass TestRecorderCallback(Callback):\n    order=51\n    def before_fit(self): \n        self.train_metrics,self.add_time = self.recorder.train_metrics,self.recorder.add_time\n        self.beta = self.recorder.smooth_loss.beta\n        for m in self.metrics: assert isinstance(m, Metric)\n        test_eq(self.recorder.smooth_loss.val, 0.)\n        #To test what the recorder logs, we use a custom logger function.\n        self.learn.logger = self.test_log\n        self.old_smooth,self.count = tensor(0.),0\n    \n    def after_batch(self):\n        if self.training:\n            self.count += 1\n            test_eq(len(self.recorder.lrs), self.count)\n            test_eq(self.recorder.lrs[-1], self.opt.hypers[-1]['lr'])\n            test_eq(len(self.recorder.losses), self.count)\n            smooth = (1 - self.beta**(self.count-1)) * self.old_smooth * self.beta + self.loss * (1-self.beta)\n            smooth /= 1 - self.beta**self.count\n            test_close(self.recorder.losses[-1], smooth, eps=1e-4)\n            test_close(self.smooth_loss, smooth, eps=1e-4)\n            self.old_smooth = self.smooth_loss\n        self.bs += find_bs(self.yb)\n        if not self.training: test_eq(self.recorder.loss.count, self.bs)\n        if self.train_metrics or not self.training: \n            for m in self.metrics: test_eq(m.count, self.bs)\n        self.losses.append(self.loss.detach().cpu())\n    \n    def before_epoch(self): \n        if self.add_time: self.start_epoch = time.time()\n        self.log = [self.epoch]\n\n    def before_train(self):\n        self.bs = 0\n        self.losses = []\n        for m in self.recorder._train_mets: test_eq(m.count, self.bs)\n            \n    def after_train(self):\n        mean = tensor(self.losses).mean()\n        self.log += [self.smooth_loss, mean] if self.train_metrics else [self.smooth_loss]\n        test_close(self.log, self.recorder.log)\n        self.losses = []\n    \n    def before_validate(self):\n        self.bs = 0\n        self.losses = []\n        for m in [self.recorder.loss] + self.metrics: test_eq(m.count, self.bs)\n    \n    def test_log(self, log):\n        res = tensor(self.losses).mean()\n        self.log += [res, res]\n        if self.add_time: self.log.append(format_time(time.time() - self.start_epoch))\n        test_close(log[:-1], self.log[:-1])\n        test_eq(log[-1], self.log[-1])\n```\n\n----------------------------------------\n\nTITLE: Mapping PyTorch Modules to Documentation Pages\nDESCRIPTION: Provides a function to determine the appropriate documentation page name for a given PyTorch module, such as 'tensors.html' for Tensor or 'nn.html' for neural network modules. It simplifies the process of linking to relevant documentation pages based on module types.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/99_pytorch_doc.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef _mod2page(\n    mod:ModuleType, # A PyTorch module\n) -> str:\n    \"Get the webpage name for a PyTorch module\"\n    if mod == Tensor: return 'tensors.html'\n    name = mod.__name__\n    name = name.replace('torch.', '').replace('utils.', '')\n    if name.startswith('nn.modules'): return 'nn.html'\n    return f'{name}.html'\n```\n\n----------------------------------------\n\nTITLE: List Conda Package Info Members (Python)\nDESCRIPTION: Retrieves a list of all members (files and directories) within the opened Conda tar archive (`zpkg`). It then filters and displays the index and name of members located within the `info/` directory, which typically contains Conda metadata.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlpkg = zpkg.getmembers()\n[(i,o.name) for i,o in enumerate(lpkg) if o.name.startswith('info/')]\n```\n\n----------------------------------------\n\nTITLE: Declaring Public API Symbols for Fastai Learner Module in Python\nDESCRIPTION: Defines a list '_all_' indicating which symbols (primarily exception classes) are public exports from this module. This affects import * behavior and helps downstream users discover available exceptions relevant to the training loop. No dependencies.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n_all_ = ['CancelBackwardException', 'CancelStepException','CancelFitException','CancelEpochException',\n         'CancelTrainException','CancelValidException','CancelBatchException']\n```\n\n----------------------------------------\n\nTITLE: Testing DiceLoss with Incorrect Output in PyTorch\nDESCRIPTION: Tests the `DiceLoss` (`dl`) function with a synthetic `model_output_all_background` and `target`. The output is designed to be mostly incorrect, demonstrating that the loss is approximately 0.67, consistent with a Dice score of 1/3 as explained in the surrounding text. Requires PyTorch and a `DiceLoss` instance.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ntest_close(dl(model_output_all_background, target), 0.67, eps=0.01)\n```\n\n----------------------------------------\n\nTITLE: Removing a Callback Type from Learner\nDESCRIPTION: This snippet shows removing all instances of a specific callback type using `remove_cb`.  It demonstrates that the same callback class can be used as input, removing all instances of that class. This can be used for cleaning up a training process. The assertions ensure the correct callbacks are removed.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner()\nlearn.add_cbs([TestTrainEvalCallback(), TestTrainEvalCallback()])\nlearn.remove_cb(TestTrainEvalCallback)\ntest_eq(len(learn.cbs), 1)\nassert not getattr(learn,'test_train_eval',None)\n```\n\n----------------------------------------\n\nTITLE: Test SimpleCNN Custom Kernel Sizes in Python\nDESCRIPTION: This test checks if the `SimpleCNN` correctly applies custom kernel sizes specified during initialization. It creates a `SimpleCNN` with a list of filters and a list of desired kernel sizes, then verifies that the kernel sizes of the first two convolutional layers in the resulting model match the provided values.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_65\n\nLANGUAGE: python\nCODE:\n```\ntst = SimpleCNN([8,16,32], kernel_szs=[1,3])\nmods = list(tst.children())\ntest_eq([m[0].kernel_size for m in mods[:2]], [(1,1), (3,3)])\n```\n\n----------------------------------------\n\nTITLE: Downloading IMDB Dataset using fastai (Python)\nDESCRIPTION: Downloads and extracts the IMDB movie review dataset using fastai's untar_data function and standardized URLs.IMDB constant. Requires an active internet connection and writes data to the configured path variable. Input/output: path contains the dataset directory after execution, or raises error if download fails.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.IMDB)\n```\n\n----------------------------------------\n\nTITLE: Training text classification model with AWD_LSTM using notebook_launcher\nDESCRIPTION: Defines a function to train a text classifier on the IMDB dataset using AWD_LSTM architecture, then launches it across 2 GPUs using notebook_launcher.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/distributed_app_examples.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.IMDB_SAMPLE)\ndf = pd.read_csv(path/'texts.csv')\n\ndef train():\n    imdb_clas = DataBlock(blocks=(TextBlock.from_df('text', seq_len=72), CategoryBlock),\n                      get_x=ColReader('text'), get_y=ColReader('label'), splitter=ColSplitter())\n    dls = imdb_clas.dataloaders(df, bs=64)\n    learn = rank0_first(lambda: text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy))\n    with learn.distrib_ctx(in_notebook=True):\n        learn.fine_tune(4, 1e-2)\n        \nnotebook_launcher(train, num_processes=2)\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Neural Network Model - PyTorch/Python\nDESCRIPTION: Defines a simple feedforward neural network model as a PyTorch `Module`. It has two linear layers: an input layer mapping 784 features to 50 hidden units, followed by a ReLU activation, and an output layer mapping 50 hidden units to 10 output classes.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nclass Mnist_NN(Module):\n    def __init__(self):\n        self.lin1 = nn.Linear(784, 50, bias=True)\n        self.lin2 = nn.Linear(50, 10, bias=True)\n\n    def forward(self, xb):\n        x = self.lin1(xb)\n        x = F.relu(x)\n        return self.lin2(x)\n```\n\n----------------------------------------\n\nTITLE: Loading MNIST Dataset - Python\nDESCRIPTION: Loads the MNIST dataset from a gzipped pickle file. The data is expected to be in a tuple format containing training data (x_train, y_train), validation data (x_valid, y_valid), and potentially test data (ignored here), unpacked using tuple assignment. The 'latin-1' encoding is specified for compatibility.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith gzip.open(path/'mnist.pkl.gz', 'rb') as f:\n    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n```\n\n----------------------------------------\n\nTITLE: Verify Image Function\nDESCRIPTION: This function checks if an image file can be opened and read. It attempts to open an image using `Image.open`, attempts to create a draft and load its content, and returns `True` if successful, `False` otherwise. It is used to verify that an image file is not corrupted or damaged.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndef verify_image(fn):\n    \"Confirm that `fn` can be opened\"\n    try:\n        im = Image.open(fn)\n        im.draft(im.mode, (32,32))\n        im.load()\n        return True\n    except: return False\n```\n\n----------------------------------------\n\nTITLE: Installing/updating Kaggle API Using pip - Python\nDESCRIPTION: Shell command (commented out) to install or update the Kaggle API Python package using the system's Python interpreter. Requires pip and internet connectivity. Used for interacting with Kaggle programmatically to download datasets. Often run in Jupyter or shell; not executed as a regular Python statement.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# ! {sys.executable} -m pip install kaggle --upgrade\n```\n\n----------------------------------------\n\nTITLE: Adding documentation to TfmdDL methods\nDESCRIPTION: Adds documentation strings to various methods of the TfmdDL class using the add_docs utility function, providing explanations for decode, decode_batch, new, show_batch, show_results, before_iter, and to methods.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nadd_docs(TfmdDL,\n         decode=\"Decode `b` using `tfms`\",\n         decode_batch=\"Decode `b` entirely\",\n         new=\"Create a new version of self with a few changed attributes\",\n         show_batch=\"Show `b` (defaults to `one_batch`), a list of lists of pipeline outputs (i.e. output of a `DataLoader`)\",\n         show_results=\"Show each item of `b` and `out`\",\n         before_iter=\"override\",\n         to=\"Put self and its transforms state on `device`\")\n```\n\n----------------------------------------\n\nTITLE: Cleaning State Dict Keys for Backward Compatibility - Python\nDESCRIPTION: Defines clean_raw_keys to remove redundant keys (ending in '_raw' or containing '.module') from a model state dict, enhancing backward compatibility between different fastai versions. Input: wgts (dict). Output: cleaned dict. Used for model loading, particularly when PyTorch multi-GPU checkpoints are present.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/37_text.learner.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef clean_raw_keys(wgts:dict):\n    keys = list(wgts.keys())\n    for k in keys:\n        t = k.split('.module')\n        if f'{_rm_module(k)}_raw' in keys: del wgts[k]\n    return wgts\n```\n\n----------------------------------------\n\nTITLE: Show Results for Multi-Channel or Multidimensional Masks\nDESCRIPTION: This visualization arranges multiple images, targets, and predictions in a grid with a 3-column layout labeled 'Input/Target/Prediction'. It helps assess the quality of multi-channel masks or multidimensional outputs, including prediction confidence scores.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_42\n\nLANGUAGE: Python\nCODE:\n```\n@dispatch\ndef show_results(x:TensorImage, y:TensorImage, samples, outs, ctxs=None, max_n=10, figsize=None, **kwargs):\n    if ctxs is None: ctxs = get_grid(3*min(len(samples), max_n), ncols=3, figsize=figsize, title='Input/Target/Prediction')\n    for i in range(2):\n        ctxs[i::3] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs[i::3],range(max_n))]\n    ctxs[2::3] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(outs.itemgot(0),ctxs[2::3],range(max_n))]\n    return ctxs\n```\n\n----------------------------------------\n\nTITLE: Calculating Figure Bounds for Image Display in Python\nDESCRIPTION: Utility `_fig_bounds` calculates suitable figure dimension bounds based on image dimension input, implementing a heuristic that clamps the value between 1 and 5 after integer division by 32. This helps scale figure sizes proportionally to image size for plotting.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef _fig_bounds(x):\n    r = x//32\n    return min(5, max(1,r))\n```\n\n----------------------------------------\n\nTITLE: Defining Dataset Splits for Training and Validation (Python)\nDESCRIPTION: Creates two lists of indices corresponding to the training and validation splits. Used for splitting the data when constructing TfmdLists for fastai. Expects df_train to be defined; provides correct indices for supervised dataset creation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nsplits = [range_of(df_train), list(range(len(df_train), len(all_texts)))]\ntls = TfmdLists(all_texts, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)\n```\n\n----------------------------------------\n\nTITLE: Instantiating DataLoaders for Training with Batch Size and Image Size in fastai Python\nDESCRIPTION: Creates dataloaders for the GAN training using a batch size of 128 and images resized to 64x64 pixels. This prepares the data pipeline for training the generator and critic models.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-wgan.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndls = get_dls(128, 64)\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders from Ignite Module in Python\nDESCRIPTION: This code creates a `DataLoaders` object using the `get_data_loaders` function from the imported module. It passes batch sizes of 64 and 128 and then calls `.cuda()` to move the data to the GPU. The returned object will be used to feed into the learner.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_ignite.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndata = DataLoaders(*get_data_loaders(64, 128)).cuda()\n```\n\n----------------------------------------\n\nTITLE: Testing fa_convert with Nested Sequences - Python\nDESCRIPTION: Validates fa_convert on nested list and tuple inputs with arrays, comparing against default_convert and ensuring returned types are preserved. Appropriately uses fastcore's L class for type checking.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nt0 = array([1,2])\nt = [t0,(t0,t0)]\n\ntest_eq(fa_convert(t), default_convert(t))\ntest_eq(L(fa_convert(t)).map(type), [Tensor,tuple])\n```\n\n----------------------------------------\n\nTITLE: Showing Validation Example Using show_at (Python)\nDESCRIPTION: Displays the first validation example (decoded) using fastai's show_at function. Ensures that text data is displayed accurately after transformation. Used for data exploration and validation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nshow_at(tls.valid, 0)\n```\n\n----------------------------------------\n\nTITLE: Retrieving PyTorch Convolution Function (Python)\nDESCRIPTION: Returns the appropriate PyTorch convolutional function (Conv1d, Conv2d, Conv3d, ConvTranspose1d, ConvTranspose2d, or ConvTranspose3d) based on the specified number of dimensions (`ndim`) and whether it should be a transposed convolution (`transpose`). Asserts that `ndim` is between 1 and 3.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef _conv_func(ndim=2, transpose=False):\n    \"Return the proper conv `ndim` function, potentially `transposed`.\"\n    assert 1 <= ndim <=3\n    return getattr(nn, f'Conv{\"Transpose\" if transpose else \"\"}{ndim}d')\n```\n\n----------------------------------------\n\nTITLE: SGDR Test\nDESCRIPTION: Tests the implementation of the `fit_sgdr` method to ensure it generates the expected number of epochs and that the learning rate schedule is correctly applied. It uses a synthetic learner and verifies the learning rates over the training cycles.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n#|slow\nlearn = synth_learner()\nwith learn.no_logging(): learn.fit_sgdr(3, 1)\ntest_eq(learn.n_epoch, 7)\niters = [k * len(learn.dls.train) for k in [0,1,3,7]]\nfor i in range(3):\n    n = iters[i+1]-iters[i]\n    #The start of a cycle can be mixed with the 0 of the previous cycle with rounding errors, so we test at +1\n    test_close(learn.recorder.lrs[iters[i]+1:iters[i+1]], [SchedCos(learn.lr, 0)(k/n) for k in range(1,n)])\n```\n\n----------------------------------------\n\nTITLE: Retrieving Text Files from Dataset with get_files (Python)\nDESCRIPTION: Retrieves all .txt files from specified folders ('unsup', 'train', 'test') within the dataset path. Uses fastai's get_files utility for file collection. Input: dataset directory; Output: a list of file paths stored in texts.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntexts = get_files(path, extensions=['.txt'], folders=['unsup', 'train', 'test'])\nlen(texts)\n```\n\n----------------------------------------\n\nTITLE: Running Manual Training Loop (Manual SGD) - PyTorch/Python\nDESCRIPTION: Executes the manual `update` function for every batch in the training DataLoader (`dls.train`). This performs one full epoch of training using the manually implemented SGD with weight decay update step defined previously. The loss for each batch is collected in the `losses` list.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nlosses = [update(x,y,lr) for x,y in dls.train]\n```\n\n----------------------------------------\n\nTITLE: Installing 7zip for Extracting tar.7z Files - Python\nDESCRIPTION: Shell command (commented out) to install the 'eidl7zip' package via conda for extracting .7z archives. Required when working with compressed tar.7z files. Needs conda environment and internet access. Only required once per environment.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# ! conda install --yes --prefix {sys.prefix} -c haasad eidl7zip\n```\n\n----------------------------------------\n\nTITLE: Download Images Function\nDESCRIPTION: This function downloads multiple images from a list of URLs, as specified in a text file or given directly. It downloads images concurrently using the `parallel` function.  It creates the destination directory if it doesn't exist and handles file name preservation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef download_images(dest, url_file=None, urls=None, max_pics=1000, n_workers=8, timeout=4, preserve_filename=False):\n    \"Download images listed in text file `url_file` to path `dest`, at most `max_pics`\"\n    if urls is None: urls = url_file.read_text().strip().split(\"\\n\")[:max_pics]\n    dest = Path(dest)\n    dest.mkdir(exist_ok=True)\n    parallel(partial(_download_image_inner, dest, timeout=timeout, preserve_filename=preserve_filename),\n             list(enumerate(urls)), n_workers=n_workers, threadpool=True)\n```\n\n----------------------------------------\n\nTITLE: Create TabularDataLoaders from CSV\nDESCRIPTION: This creates a `TabularDataLoaders` from a CSV file. It uses the previously defined categorical and continuous column names, the processors, the y variable name, and sets the validation indices and batch size.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/41_tabular.data.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ncat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\ncont_names = ['age', 'fnlwgt', 'education-num']\nprocs = [Categorify, FillMissing, Normalize]\ndls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, procs=procs, cat_names=cat_names, cont_names=cont_names, \n                                  y_names=\"salary\", valid_idx=list(range(800,1000)), bs=64)\n```\n\n----------------------------------------\n\nTITLE: Configuration: Export notebook code (Python)\nDESCRIPTION: Executes the `nbdev_export()` function. This command processes the notebook and exports all code cells marked with `#|export` into standard Python files, following the module path specified by the `#|default_exp` directive. This is part of the nbdev workflow for converting notebooks into production code.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15a_vision.models.unet.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom nbdev import *\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Installing fastai on Colab Using Shell Command in Python\nDESCRIPTION: Runs a shell command to check if the environment is Colab and, if so, upgrades fastai installation quietly. This snippet is intended to ensure the latest version of fastai is installed prior to running the rest of the tutorial. It requires a Jupyter/Colab environment with bash support.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_pytorch_verbose.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Creating fastai Optimizer Wrapper around PyTorch Adam Optimizer Using Python\nDESCRIPTION: Uses functools.partial to create an optimizer function compatible with fastai by wrapping the native PyTorch Adam optimizer within fastai's OptimWrapper. This enables seamless usage of PyTorch optimizers inside the fastai Learner without explicit layer grouping or configuration.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_pytorch_verbose.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.optimizer import OptimWrapper\n\nfrom torch import optim\nfrom functools import partial\n\nopt_func = partial(OptimWrapper, opt=optim.Adam)\n```\n\n----------------------------------------\n\nTITLE: Reading File Contents as List of Words (Python)\nDESCRIPTION: Defines a function read_file that splits the content of a text file into a list of words, using fastcore's L for fastai pipeline compatibility. Dependency: fastcore L class must be imported. Input: file path f; Output: list-like sequence of words.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef read_file(f): return L(f.read_text().split(' '))\n```\n\n----------------------------------------\n\nTITLE: Norm Transform for Normalization\nDESCRIPTION: The `Norm` class defines a normalization transform with `encodes` and `decodes` methods for data normalization, computing mean and std during setup. It requires dependencies on `Transform` and tensor operations. Used for feature scaling in datasets.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_42\n\nLANGUAGE: Python\nCODE:\n```\nclass Norm(Transform):\n    def encodes(self, o): return (o-self.m)/self.s\n    def decodes(self, o): return (o*self.s)+self.m\n    def setups(self, items):\n        its = tensor(items).float()\n        self.m,self.s = its.mean(),its.std()\n```\n\n----------------------------------------\n\nTITLE: Try to Import Module\nDESCRIPTION: Attempts to import a Python module and returns the module object if successful; otherwise, it returns `None`. This function is used to optionally import modules without crashing if they are not available. This is important for platform dependent operations. It supports graceful fallback in case of missing dependencies.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/97_test_utils.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef try_import(module):\n    \"Try to import `module`. Returns module's object on success, None on failure\"\n    try: return importlib.import_module(module)\n    except: return None\n```\n\n----------------------------------------\n\nTITLE: Training image segmentation model with UNet using notebook_launcher\nDESCRIPTION: Defines a function to train a segmentation model on the CAMVID dataset using UNet with ResNet34 backbone, then launches it across 2 GPUs using notebook_launcher.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/distributed_app_examples.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.CAMVID_TINY)\n\ndef train():\n    dls = SegmentationDataLoaders.from_label_func(\n        path, bs=8, fnames = get_image_files(path/\"images\"),\n        label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n        codes = np.loadtxt(path/'codes.txt', dtype=str)\n    )\n    learn = unet_learner(dls, resnet34)\n    with learn.distrib_ctx(in_notebook=True, sync_bn=False):\n        learn.fine_tune(8)\n        \nnotebook_launcher(train, num_processes=2)\n```\n\n----------------------------------------\n\nTITLE: Defining a Convolutional Layer\nDESCRIPTION: This defines a function `conv` that creates a 2D convolutional layer. It takes the number of input and output filters (`ni` and `nf`) and returns a `nn.Conv2d` layer with a kernel size of 3, stride of 2, and padding of 1.  This function is used to create the convolutional layers in the model.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\ndef conv(ni,nf): return nn.Conv2d(ni, nf, kernel_size=3, stride=2, padding=1)\n```\n\n----------------------------------------\n\nTITLE: Loading Test Image Filepaths - Python\nDESCRIPTION: Collects all test and additional test image filenames from respective directories, concatenating the resulting lists. Output is a list of file objects; input uses fastai's get_image_files. Assumes test images are extracted to expected folders.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ntest_items = get_image_files(path/'test-jpg') + get_image_files(path/'test-jpg-additional')\nlen(test_items)\n```\n\n----------------------------------------\n\nTITLE: Constructing Learner with Multi-label Metrics - Python\nDESCRIPTION: Defines custom metrics (accuracy_multi with threshold, FBetaMulti) for multi-label classification and instantiates a fastai Learner with a resnet50 architecture. Prerequisites: DataLoaders ready, fastai and its vision module imported. Allows for advanced evaluation suited for multi-label outcomes.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nacc_02 = partial(accuracy_multi, thresh=0.2)\nf_score = FBetaMulti(2, thresh=0.2, average='samples')\nlearn = vision_learner(dls, arch, metrics=[acc_02, f_score])\n```\n\n----------------------------------------\n\nTITLE: Helper Functions for Shape Printing in Model Summary\nDESCRIPTION: Utility functions that handle the formatting of tensor shapes for display in model summaries. Supports different input types including torch.Size, tuples, and collections of tensors.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_38\n\nLANGUAGE: Python\nCODE:\n```\ndef _get_shapes(o, bs): \n    inp = o[first(o)] if (isinstance(o, dict)) else o\n    return ' x '.join([str(bs)] + [str(t) for t in inp[1:]])\n\ndef _print_shapes(o, bs):\n    if isinstance(o, torch.Size): return _get_shapes(o, bs)\n    elif isinstance(o, tuple): return _get_shapes(o[0], bs)\n    else: return str([_print_shapes(x, bs) for x in o])\n```\n\n----------------------------------------\n\nTITLE: Importing Basic FastAI Modules (Python)\nDESCRIPTION: This Python snippet imports essential components for the `fastai.medical.text` module. It enables future annotations for type hinting and imports all basic utilities, data types, and functions from `fastai.basics`. This provides the foundational elements required for building the medical NLP functionalities.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/65_medical.text.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.basics import *\n```\n\n----------------------------------------\n\nTITLE: Adding Batch Dimension using PyTorch\nDESCRIPTION: Adds a singleton dimension (batch dimension) to the input tensors `model_output_all_background` and `target`. This is a common preprocessing step to make individual tensors compatible with loss functions or models expecting batched inputs. Requires PyTorch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n# add a batch dimension\nmodel_output_all_background = torch.unsqueeze(model_output_all_background,0)\ntarget = torch.unsqueeze(target,0)\n```\n\n----------------------------------------\n\nTITLE: Extracting date features from datetime columns in Python\nDESCRIPTION: Creates multiple derived features from a date column, including Year, Month, Week, Day, and various boolean flags for month/quarter/year start and end.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef add_datepart(df, field_name, prefix=None, drop=True, time=False):\n    \"Helper function that adds columns relevant to a date in the column `field_name` of `df`.\"\n    make_date(df, field_name)\n    field = df[field_name]\n    prefix = ifnone(prefix, re.sub('[Dd]ate$', '', field_name))\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Is_month_end', 'Is_month_start',\n            'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time: attr = attr + ['Hour', 'Minute', 'Second']\n    # Pandas removed `dt.week` in v1.1.10\n    week = field.dt.isocalendar().week.astype(field.dt.day.dtype) if hasattr(field.dt, 'isocalendar') else field.dt.week\n    for n in attr: df[prefix + n] = getattr(field.dt, n.lower()) if n != 'Week' else week\n    mask = ~field.isna()\n    df[prefix + 'Elapsed'] = np.where(mask,field.values.astype(np.int64) // 10 ** 9,np.nan)\n    if drop: df.drop(field_name, axis=1, inplace=True)\n    return df\n```\n\n----------------------------------------\n\nTITLE: Defining LabelSmoothingCrossEntropy Loss Class - Fastai/PyTorch\nDESCRIPTION: This class implements a Label Smoothing Cross Entropy loss function. It applies log_softmax to the model output and blends the standard negative log-likelihood loss with a loss calculated against a uniform distribution across classes, controlled by the `eps` parameter. It includes `activation` and `decodes` methods for use within the fastai Learner.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass LabelSmoothingCrossEntropy(Module):\n    y_int = True # y interpolation\n    def __init__(self, \n        eps:float=0.1, # The weight for the interpolation formula\n        weight:Tensor=None, # Manual rescaling weight given to each class passed to `F.nll_loss`\n        reduction:str='mean' # PyTorch reduction to apply to the output\n    ): \n        store_attr()\n\n    def forward(self, output:Tensor, target:Tensor) -> Tensor:\n        \"Apply `F.log_softmax` on output then blend the loss/num_classes(`c`) with the `F.nll_loss`\"\n        c = output.size()[1]\n        log_preds = F.log_softmax(output, dim=1)\n        if self.reduction=='sum': loss = -log_preds.sum()\n        else:\n            loss = -log_preds.sum(dim=1) #We divide by that size at the return line so sum and not mean\n            if self.reduction=='mean':  loss = loss.mean()\n        return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), weight=self.weight, reduction=self.reduction)\n\n    def activation(self, out:Tensor) -> Tensor: \n        \"`F.log_softmax`'s fused activation function applied to model output\"\n        return F.softmax(out, dim=-1)\n    \n    def decodes(self, out:Tensor) -> Tensor:\n        \"Converts model output to target format\"\n        return out.argmax(dim=-1)\n```\n\n----------------------------------------\n\nTITLE: Creating and fine-tuning a vision model\nDESCRIPTION: This snippet creates a vision learner using the prepared `DataLoaders`, a ResNet34 architecture, and the error rate metric. Then it fine-tunes the model for one epoch. This builds on the previous step by creating the model and starting the training process on the dataset that was prepared.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/quick_start.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n```\n\n----------------------------------------\n\nTITLE: Pipelining PIL Image Creation and ToTensor Transform - fastai (Python)\nDESCRIPTION: Demonstrates the construction of a fastai Pipeline combining image creation and tensor conversion, then applies it to a file and displays the result. Requires Pipeline, PILImageBW, ToTensor, test_eq, and mnist_fn. Shows output as an image with matplotlib.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\npipe_img = Pipeline([PILImageBW.create, ToTensor()])\nimg = pipe_img(mnist_fn)\ntest_eq(type(img), TensorImageBW)\npipe_img.show(img, figsize=(1,1));\n```\n\n----------------------------------------\n\nTITLE: Formatting Metadata for Artifacts\nDESCRIPTION: The `_format_metadata` function formats metadata associated with artifacts by converting all values to strings. It iterates through the key-value pairs in the metadata dictionary and converts each value to a string using `str(v)`. This ensures that the metadata is suitable for logging.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef _format_metadata(metadata):\n    \"Format metadata associated to artifacts\"\n    for k,v in metadata.items(): metadata[k] = str(v)\n```\n\n----------------------------------------\n\nTITLE: Loading Pretrained Language Model Encoder into Classifier (Python)\nDESCRIPTION: Loads the previously finetuned encoder weights into the classifier Learner, and converts it to mixed precision for efficiency with optional gradient clipping. Input: encoder filename ('finetuned1'); Output: classifier model updated with pretrained representation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nlearn = learn.load_encoder('finetuned1')\nlearn = learn.to_fp16(clip=0.1)\n```\n\n----------------------------------------\n\nTITLE: Documenting DataLoader Methods in Python\nDESCRIPTION: Uses the `add_docs` utility function to associate detailed docstrings with various methods of the `DataLoader` class. This enhances understanding by explaining the purpose of methods like `get_idxs`, `sample`, `create_batches`, `new`, `do_item`, `create_item`, `create_batch`, callback methods (`before_iter`, `after_item`, etc.), and utility methods.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nadd_docs(DataLoader, \"API compatible with PyTorch DataLoader, with a lot more callbacks and flexibility\",\n         get_idxs       = \"Return a list of indices to reference the dataset. Calls `shuffle_fn` internally if `shuffle=True`.\",\n         sample         = \"Same as `get_idxs` but returns a generator of indices to reference the dataset.\",\n         create_batches = \"Takes output of `sample` as input, and returns batches of data. Does not apply `after_batch`.\",\n         new            = \"Create a new `DataLoader` with given arguments keeping remaining arguments same as original `DataLoader`.\",\n         prebatched     = \"Check if `bs` is None.\",\n         do_item        = \"Combines `after_item` and `create_item` to get an item from dataset by providing index as input.\",\n         chunkify       = \"Used by `create_batches` to turn generator of items (`b`) into batches.\",\n         shuffle_fn     = \"Returns a random permutation of `idxs`.\",\n         randomize      = \"Set's `DataLoader` random number generator state.\",\n         retain         = \"Cast each item of `res` to type of matching item in `b` if its a superclass.\",\n         create_item    = \"Subset of the dataset containing the index values of sample if exists, else next iterator.\",\n         create_batch   = \"Collate a list of items into a batch.\",\n         do_batch       = \"Combines `create_batch` and `before_batch` to get a batch of items. Input is a list of items to collate.\",\n         to             = \"Sets `self.device=device`.\",\n         one_batch      = \"Return one batch from `DataLoader`.\",\n         wif            = \"See pytorch `worker_init_fn` for details.\", \n         before_iter    = \"Called before `DataLoader` starts to read/iterate over the dataset.\",\n         after_item     = \"Takes output of `create_item` as input and applies this function on it.\",\n         before_batch   = \"It is called before collating a list of items into a batch. Input is a list of items.\",\n         after_batch    = \"After collating mini-batch of items, the mini-batch is passed through this function.\",\n         after_iter     = \"Called after `DataLoader` has fully read/iterated over the dataset.\")\n```\n\n----------------------------------------\n\nTITLE: Inspecting New StateHoliday Features\nDESCRIPTION: Lists all columns related to StateHoliday to examine the new elapsed time features that were created.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n[c for c in all_ftrs.columns if 'StateHoliday' in c]\n```\n\n----------------------------------------\n\nTITLE: Implementing Recall Score Wrapper for Multi-label Classification in Python\nDESCRIPTION: A wrapper function that adapts scikit-learn's recall_score for use in fastai's multi-label classification tasks, with options for threshold, activation, and averaging method customization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndef RecallMulti(thresh=0.5, sigmoid=True, labels=None, pos_label=1, average='macro', sample_weight=None):\n    \"Recall for multi-label classification problems\"\n    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n    return skm_to_fastai(skm.recall_score, thresh=thresh, activation=activation, flatten=False,\n                         labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight)\n```\n\n----------------------------------------\n\nTITLE: Displaying Batches of Images/Bounding Boxes - fastai (Python)\nDESCRIPTION: Invokes the DataLoader's 'show_batch' method to display samples with bounding boxes. Requires fastai DataLoader (coco_tdl). No input/output except graphical plot.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_38\n\nLANGUAGE: Python\nCODE:\n```\ncoco_tdl.show_batch();\n```\n\n----------------------------------------\n\nTITLE: Upgrading Fastai in Google Colab (Shell)\nDESCRIPTION: This snippet checks if the execution environment is Google Colab by looking for the /content directory and, if present, upgrades Fastai using pip to the latest version. It ensures compatibility with Fastai code in Colab sessions. The command requires internet access and appropriate permissions. There is no input or output other than side effects (updating the package).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Defining TitledTensorScalar Subclass with Show Method in Python\nDESCRIPTION: Defines TitledTensorScalar as a TensorBase subclass representing a scalar tensor with an enhanced show method that displays the scalar value as a title. It utilizes the show_title utility to present the scalar prominently, supporting visualization of single values with context.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_61\n\nLANGUAGE: python\nCODE:\n```\nclass TitledTensorScalar(TensorBase):\n    \"A tensor containing a scalar that has a `show` method\"\n    def show(self, **kwargs): show_title(self.item(), **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Defining LabelSmoothingCrossEntropyFlat Class - Fastai/PyTorch\nDESCRIPTION: This class defines `LabelSmoothingCrossEntropyFlat`, a version of `LabelSmoothingCrossEntropy` that automatically handles flattening of input and target tensors using the `BaseLoss` class. It ensures the loss functions correctly regardless of the original tensor dimensions (beyond the batch and class dimensions). It retains the `activation` and `decodes` methods.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@delegates()\nclass LabelSmoothingCrossEntropyFlat(BaseLoss):\n    \"Same as `LabelSmoothingCrossEntropy`, but flattens input and target.\"\n    y_int = True\n    @use_kwargs_dict(keep=True, eps=0.1, reduction='mean')\n    def __init__(self, \n        *args, \n        axis:int=-1, # Class axis\n        **kwargs\n    ): \n        super().__init__(LabelSmoothingCrossEntropy, *args, axis=axis, **kwargs)\n    def activation(self, out:Tensor) -> Tensor: \n        \"`LabelSmoothingCrossEntropy`'s fused activation function applied to model output\"\n        return F.softmax(out, dim=-1)\n    \n    def decodes(self, out:Tensor) -> Tensor:\n        \"Converts model output to target format\"\n        return out.argmax(dim=-1)\n```\n\n----------------------------------------\n\nTITLE: Testing DistributedDL with drop_last in TfmdDL - Python\nDESCRIPTION: Validates proper batch partitioning with drop_last=True in the input TfmdDL, ensuring incomplete batches are dropped and only full batches are distributed and gathered. Essential for validating behavior with partial datasets. Dependencies: fastai TfmdDL, DistributedDL, test_eq, torch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\ndl = TfmdDL(list(range(50)), bs=12, num_workers=2,drop_last=True)\nfor i in range(4):\n    dl1 = DistributedDL(dl, i, 4)\n    test_eq(list(dl1), [torch.arange(i*13, i*13+12)%50])\n```\n\n----------------------------------------\n\nTITLE: Checking Record Counts in Train and Test DataFrames (Python, pandas)\nDESCRIPTION: This snippet prints the size (row count) of the train and test DataFrames using Python's built-in len() function. pandas is required, and the train/test variables must already reference loaded DataFrames. The output is a tuple containing the number of records in each subset, helpful for understanding dataset scale.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlen(train), len(test)\n```\n\n----------------------------------------\n\nTITLE: Copying FP32 Master Parameters to FP16 Model Parameters in fastai (Python)\nDESCRIPTION: Copies the updated FP32 master parameters back to the FP16 model parameters after a gradient step. This function iterates through corresponding parameter groups (model_pgs and master_pgs) and utilizes an underlying `master_params_to_model_params` function (presumably from Apex or a similar library). It handles both standard and potentially flattened master parameter structures via the `flat_master` flag.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18_callback.fp16.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n#|export \ndef to_model_params(\n    model_pgs:list, # Fp16 model params to copy to\n    master_pgs:list, # Fp32 master params to copy from\n    flat_master:bool=False # Whether master_pgs was previously flattened\n)->None:\n    \"Copy updated fp32 master params to fp16 model params after gradient step. \" \n    for (model_params,master_params) in zip(model_pgs,master_pgs):\n        master_params_to_model_params(model_params, master_params, flat_master=flat_master)\n```\n\n----------------------------------------\n\nTITLE: Displaying Pre-Tokenized Batches Using show_batch (Python)\nDESCRIPTION: Displays batches of tokenized data (inputs and targets) from DataLoaders created with pre-tokenized inputs, for verification of data pipeline integrity. Operates similarly to prior show_batch usage; used after switching to pre-tokenized dataset approach.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndls.show_batch(max_n=2)\n```\n\n----------------------------------------\n\nTITLE: ToTensorBase Layer for Type Conversion - Python\nDESCRIPTION: Defines a layer that converts input tensors to a specified base class (default: TensorBase). Uses @module for flexible Tensor class injection. Requires tensor_cls argument, input is any compatible tensor, output is tensor cast to tensor_cls, useful for type enforcement in fastai pipelines.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@module(tensor_cls=TensorBase)\ndef ToTensorBase(self, x):\n    \"Convert x to TensorBase class\"\n    return self.tensor_cls(x)\n```\n\n----------------------------------------\n\nTITLE: Rebuilding Pickled Tensors in Python\nDESCRIPTION: Defines a helper function to rebuild tensor subclasses during unpickling by applying a provided constructor function and reassigning the original subclass type and dictionary attributes. This supports correct restoration of tensor subclasses with custom metadata after serialization.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\ndef _rebuild_from_type(func, type, args, dict):\n    ret = func(*args).as_subclass(type)\n    ret.__dict__ = dict\n    return ret\n```\n\n----------------------------------------\n\nTITLE: Defining PILMask Class (Python)\nDESCRIPTION: Defines the `PILMask` class, inheriting from `PILBase`. This class represents a segmentation mask as a Pillow Image, typically in 'L' (grayscale) mode. It overrides `_open_args` to default to 'L' mode and `_show_args` for appropriate visualization (alpha blending, colormap), intended for conversion to `TensorMask`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass PILMask(PILBase):\n    \"A Pillow `Image` Mask that can show itself and converts to `TensorMask`\"\n    _open_args,_show_args = {'mode':'L'},{'alpha':0.5, 'cmap':'tab20'}\n```\n\n----------------------------------------\n\nTITLE: Testing channels last training on modern GPU hardware\nDESCRIPTION: Setup code that initializes a fastai Learner with combinations of mixed precision and channels last callbacks, trains a simple model to validate performance and correctness of the channels last configuration on supported hardware (Volta, Turing or newer).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18c_callback.channelslast.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n#|cuda\n# Test must be ran on modern hardware (Volta, Turning, or newer)\nwith no_random():\n    learn = synth_learner(cbs=[MixedPrecision,ChannelsLast,ChannelsLastTest], cuda=True, data=synth_dbunch())\n    class ConvModel(Module):\n        def __init__(self): self.conv = nn.Conv2d(3, 32, 1)\n        def forward(self,x): return self.conv(x)\n    def fakeloss(): pass\n    learn.model = ConvModel()\n    learn.opt_func = partial(SGD, mom=0.)\n    learn.loss_func=fakeloss\n    learn.fit(3)\n```\n\n----------------------------------------\n\nTITLE: Define Learner.no_bar Context Manager in Python\nDESCRIPTION: Patches the `Learner` class with a context manager named `no_bar`. This context manager temporarily removes the `ProgressCallback` (if present) before executing the code within the `with` block and adds it back afterwards, effectively disabling progress bars for the operations within the context.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/16_callback.progress.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\n@contextmanager\ndef no_bar(self:Learner):\n    \"Context manager that deactivates the use of progress bars\"\n    has_progress = hasattr(self, 'progress')\n    if has_progress: self.remove_cb(self.progress)\n    try: yield self\n    finally:\n        if has_progress: self.add_cb(ProgressCallback())\n```\n\n----------------------------------------\n\nTITLE: Get Downloaded Image Filename\nDESCRIPTION: This function generates a unique filename for downloaded images, preventing conflicts when multiple images with the same base name are downloaded to the same destination. It checks if a file with the same name already exists and appends a numerical suffix to avoid overwriting files.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef _get_downloaded_image_filename(dest, name, suffix):\n    start_index = 1\n    candidate_name = name\n\n    while (dest/f\"{candidate_name}{suffix}\").is_file():\n        candidate_name = f\"{candidate_name}{start_index}\"\n        start_index += 1\n\n    return candidate_name\n```\n\n----------------------------------------\n\nTITLE: Example: Visualize collected data distribution in fastai Python\nDESCRIPTION: Executes one epoch of training using the learner set up with `WeightedDL` and `CollectDataCallback`. After training, it retrieves the collected input data from the callback and plots a histogram to visualize the distribution of sampled data points, demonstrating the effect of the applied weights.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14a_callback.data.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit(1)\nt = concat(*learn.collect_data.data.itemgot(0,0))\nplt.hist(t.numpy());\n```\n\n----------------------------------------\n\nTITLE: Rebuilding tensors using internal utilities (Python)\nDESCRIPTION: These functions wrap internal PyTorch utilities to reconstruct tensors (`_rebuild_tensor_v2`) and quantized tensors (`_rebuild_qtensor`) from stored parameters. They facilitate tensor serialization/deserialization or custom tensor creation workflows.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\n#|export\ndef _fa_rebuild_tensor (cls, *args, **kwargs): return cls(torch._utils._rebuild_tensor_v2(*args, **kwargs))\ndef _fa_rebuild_qtensor(cls, *args, **kwargs): return cls(torch._utils._rebuild_qtensor  (*args, **kwargs))\n```\n\n----------------------------------------\n\nTITLE: Defining Function Returning Constant - Python\nDESCRIPTION: This snippet defines a simple Python function `f` that takes any input `x` and always returns the integer `1`. This function is used in the following benchmark to test the performance of parallel processing when the worker function performs minimal computation and returns very little data.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef f(x): return 1\n```\n\n----------------------------------------\n\nTITLE: Distributed Training Utilities for Process Number and Rank in Python\nDESCRIPTION: Provides helper functions to retrieve environment variables indicating the number of distributed training processes ('WORLD_SIZE') and the rank of the current process ('RANK'), defaulting to zero if not set. These utilities assist in managing and identifying distributed training state.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_83\n\nLANGUAGE: python\nCODE:\n```\ndef num_distrib():\n    \"Return the number of processes in distributed training (if applicable).\"\n    return int(os.environ.get('WORLD_SIZE', 0))\n```\n\nLANGUAGE: python\nCODE:\n```\ndef rank_distrib():\n    \"Return the distributed rank of this process (if applicable).\"\n    return int(os.environ.get('RANK', 0))\n```\n\n----------------------------------------\n\nTITLE: Configuration: Set nbdev export path (Python)\nDESCRIPTION: Sets the default module path for exporting code using nbdev. This directive specifies that functions and classes marked with `#|export` should be saved into a Python file corresponding to the path `vision.models.unet` when the nbdev export process is run.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15a_vision.models.unet.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp vision.models.unet\n```\n\n----------------------------------------\n\nTITLE: Installing fastai in Google Colab - Python\nDESCRIPTION: Checks if the environment is Google Colab and upgrades fastai via pip if so. Requires internet access in Colab and the ability to execute shell commands. This snippet is intended for interactive notebook contexts and does not affect program logic directly.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Adult Dataset Integration\nDESCRIPTION: This code demonstrates the integration of the `TabularGPU` pipeline with a real-world dataset. It downloads the Adult dataset, loads it into cuDF DataFrames, splits it into training and test sets, and prepares it for further analysis and modeling.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.ADULT_SAMPLE)\ndf = cudf.from_pandas(pd.read_csv(path/'adult.csv'))\ndf_trn,df_tst = df.iloc[:10000].copy(),df.iloc[10000:].copy()\ndf_trn.head()\n```\n\n----------------------------------------\n\nTITLE: Introspecting `chunked` Function - Jupyter\nDESCRIPTION: This snippet uses the Jupyter/IPython introspection command `??` to display the source code and documentation for the `chunked` function. This is done to understand how the function, which is used later for splitting data into batches for parallel processing, works internally.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nchunked??\n```\n\n----------------------------------------\n\nTITLE: Mapping floating point tensors to float32 recursively (Python)\nDESCRIPTION: The `to_float` function traverses nested collections and converts all floating point tensors back to float32. This is useful for consistency or preparing tensors for operations requiring higher precision.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\n#|export\ndef to_float(b):\n    \"Recursively map floating point tensors in `b ` to float.\"\n    return apply(lambda x: x.float() if torch.is_floating_point(x) else x, b)\n```\n\n----------------------------------------\n\nTITLE: Wrapping PyTorch DataLoaders with fastai DataLoaders Class in Python\nDESCRIPTION: Imports fastai DataLoaders class and creates a fastai DataLoaders object from the previously defined PyTorch DataLoaders. This adapts native PyTorch DataLoaders for compatibility with fastai's Learner and training pipeline, enabling enhanced callbacks and training utilities.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_pytorch_verbose.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.data.core import DataLoaders\n\ndls = DataLoaders(train_loader, test_loader)\n```\n\n----------------------------------------\n\nTITLE: RNNDropout with Sequence - PyTorch\nDESCRIPTION: This demonstrates that `RNNDropout` can handle a sequence of images, where the time dimension is the first axis. It creates a random tensor with the appropriate dimensions and passes it through an `RNNDropout` instance. The underscore indicates that the output is not used, implying that the code serves for demonstration and verification of functionality.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n_ = dp(torch.rand(4,10,3,32,32))\n```\n\n----------------------------------------\n\nTITLE: Install/Upgrade fastai on Google Colab\nDESCRIPTION: Checks if the environment is Google Colab (`/content` exists) and installs or upgrades the `fastai` library using pip. The `-Uqq` flags ensure an upgrade (`-U`) and quiet output (`-qq`). This snippet is typically used at the beginning of Colab notebooks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/16_callback.progress.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n#|hide\n#| eval: false\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Defining Standard Image Statistics (Python)\nDESCRIPTION: Defines common image normalization statistics (mean and standard deviation) for popular datasets: ImageNet, CIFAR, and MNIST. These tuples are often used in data normalization transforms.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#|export\nimagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\ncifar_stats    = ([0.491, 0.482, 0.447], [0.247, 0.243, 0.261])\nmnist_stats    = ([0.131], [0.308])\n```\n\n----------------------------------------\n\nTITLE: Testing FocalLossFlat gamma=0 vs CrossEntropy\nDESCRIPTION: This code compares the output of `FocalLossFlat` with `gamma=0` to `CrossEntropyLossFlat`. Since gamma=0 makes FocalLoss equivalent to CrossEntropy, the results should be very close. The test also ensures FocalLoss with gamma>0 gives different results than standard CrossEntropy.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#Compare focal loss with gamma = 0 to cross entropy\nfl = FocalLossFlat(gamma=0)\nce = CrossEntropyLossFlat()\noutput = torch.randn(32, 5, 10)\ntarget = torch.randint(0, 10, (32,5))\ntest_close(fl(output, target), ce(output, target))\n#Test focal loss with gamma > 0 is different than cross entropy\nfl = FocalLossFlat(gamma=2)\ntest_ne(fl(output, target), ce(output, target))\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for FilteredBase.dataloaders in fastai (Python)\nDESCRIPTION: Uses the `show_doc` function, likely from the fastai library or a similar documentation utility, to display the documentation (docstring and signature) for the `dataloaders` method of the `FilteredBase` class. This is a common practice in fastai notebooks for inspecting and understanding library components.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(FilteredBase().dataloaders)\n```\n\n----------------------------------------\n\nTITLE: Defining Image Transformations\nDESCRIPTION: This sets up a list of image transformations (`tfms`) to be applied to the data. These include converting the image to a tensor, padding, and random cropping. This prepares the images for input to a neural network, often reducing variability and increasing consistency.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ntfms = [ToTensor(), CropPad(size=34, pad_mode=PadMode.Zeros), RandomCrop(size=28)]\nbs = 128\n```\n\n----------------------------------------\n\nTITLE: Configuring nbdev Export Directives\nDESCRIPTION: Sets configuration directives for the nbdev development environment. '#|default_exp text.models.awdlstm' specifies the default export module path for code defined in this notebook. '#|default_cls_lvl 3' sets the default documentation level for classes, controlling their visibility in generated documentation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_3\n\nLANGUAGE: Plaintext\nCODE:\n```\n#|default_exp text.models.awdlstm\n#|default_cls_lvl 3\n```\n\n----------------------------------------\n\nTITLE: Creating ParallelTrainer Callback for DataParallel\nDESCRIPTION: Defines a callback that automatically wraps a model in DataParallel for training and unwraps it after completion. It handles the lifecycle of parallel model execution with specific ordering relative to other callbacks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass ParallelTrainer(Callback):\n    \"Wrap a model `DataParallel` automatically\"\n    run_after,run_before = TrainEvalCallback,Recorder\n    def __init__(self, device_ids): self.device_ids = device_ids\n    def before_fit(self): self.learn.model = DataParallel(self.learn.model, device_ids=self.device_ids)\n    def after_fit(self): self.learn.model = self.learn.model.module\n```\n\n----------------------------------------\n\nTITLE: Dataset and Dataloader Setup\nDESCRIPTION: This snippet creates Datasets and a TabDataLoader for the tabular data. This step is crucial for feeding the data into a machine learning model, as it structures the data in a way that is compatible with the training process.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nsplits = [list(range(len(splits[0]))), list(range(len(splits[0]), 10000))]\ndsets = Datasets(to, splits=splits, tfms=[None])\ndl = TabDataLoader(to.valid, bs=64, num_workers=0)\n```\n\n----------------------------------------\n\nTITLE: Applying Log Transformation to Target - Python\nDESCRIPTION: Applies a natural logarithm transformation to the 'Sales' column in the training DataFrame. This is common practice for skewed target variables like sales to make their distribution more normal.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ntrain_df[dep_var] = np.log(train_df[dep_var])\n```\n\n----------------------------------------\n\nTITLE: Checking Model Output Shape - PyTorch/Python\nDESCRIPTION: Performs a forward pass through the `model` using a sample batch `x` obtained from the DataLoaders and prints the shape of the output tensor. This verifies that the model produces output with the expected dimensions for the given input batch.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel(x).shape\n```\n\n----------------------------------------\n\nTITLE: Defining ChannelsLast callback class for channels last training\nDESCRIPTION: Implements a fastai Callback that converts the model to PyTorch's channels last memory format before training. It enhances training throughput when used with compatible models and hardware.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18c_callback.channelslast.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nclass ChannelsLast(Callback):\n    \"Channels last training using PyTorch's Channels Last Memory Format (beta)\"\n    order = -1 # Needs to run before any model modification callbacks occur\n    def before_fit(self):\n        self.learn.model.to(memory_format=torch.channels_last)\n```\n\n----------------------------------------\n\nTITLE: Redefining Time Series Data Splits - Python\nDESCRIPTION: Redefines the train/validation splits based on the previously calculated `cut` index. This snippet is identical to a previous one and likely redundant, confirming the split strategy after potential data modifications.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nsplits = (list(range(cut, len(train_df))),list(range(cut)))\n```\n\n----------------------------------------\n\nTITLE: Creating Attribute Getter Transform\nDESCRIPTION: Transform class that extracts an attribute with a specified name from objects, with an optional default value if the attribute doesn't exist.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass AttrGetter(ItemTransform):\n    \"Creates a proper transform that applies `attrgetter(nm)` (even on a tuple)\"\n    _retain = False\n    def __init__(self, nm, default=None): store_attr()\n    def encodes(self, x): return getattr(x, self.nm, self.default)\n```\n\n----------------------------------------\n\nTITLE: Installing FastAI on Colab (Shell)\nDESCRIPTION: This snippet uses a shell command executed from a Python environment (likely a Jupyter notebook or similar) to upgrade the fastai library on a Google Colab environment. It checks if the `/content` directory exists (typical in Colab) before running `pip install -Uqq fastai` for a quiet, quick upgrade. This ensures the latest version of fastai is used for development or execution.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/65_medical.text.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Defining the fastai DataLoader Class in Python\nDESCRIPTION: Defines the `DataLoader` class, inheriting from `GetAttr`. It initializes attributes for dataset handling, batch size, workers, shuffling, device placement, and callbacks. It provides methods for iteration (`__iter__`), length calculation (`__len__`), index generation (`get_idxs`), batch creation (`create_batches`, `create_batch`), item creation (`create_item`), and various hooks for customization throughout the loading process. It aims for compatibility with PyTorch's DataLoader while adding flexibility.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@funcs_kwargs\nclass DataLoader(GetAttr):\n    _noop_methods = 'wif before_iter after_item before_batch after_batch after_iter'.split()\n    for o in _noop_methods: exec(f\"def {o}(self, x=None, *args, **kwargs): return x\")\n    _methods = _noop_methods + 'create_batches create_item create_batch retain \\\n        get_idxs sample shuffle_fn do_batch create_batch'.split()\n    _default = 'dataset'\n    def __init__(self, dataset=None, bs=None, num_workers=0, pin_memory=False, timeout=0, batch_size=None,\n                 shuffle=False, drop_last=False, indexed=None, n=None, device=None, persistent_workers=False,\n                 pin_memory_device='', **kwargs):\n        if batch_size is not None: bs = batch_size # PyTorch compatibility\n        assert not (bs is None and drop_last)\n        if indexed is None: indexed = (hasattr(dataset,'__getitem__')\n                                       and not isinstance(dataset, IterableDataset))\n        if not indexed and shuffle: raise ValueError(\"Can only shuffle an indexed dataset (not an iterable one).\")\n        if n is None:\n            try: n = len(dataset)\n            except TypeError: pass\n        store_attr('dataset,bs,shuffle,drop_last,indexed,n,pin_memory,timeout,device')\n        self.rng,self.num_workers,self.offs = random.Random(random.randint(0,2**32-1)),1,0\n        if sys.platform == \"win32\" and IN_NOTEBOOK and num_workers > 0: num_workers = 0       \n        if sys.platform == \"darwin\" and num_workers > 0: num_workers = 0       \n        self.fake_l = _FakeLoader(self, pin_memory, num_workers, timeout, persistent_workers=persistent_workers,\n                                  pin_memory_device=pin_memory_device)\n\n    def __len__(self):\n        if self.n is None: raise TypeError\n        if self.bs is None: return self.n\n        return self.n//self.bs + (0 if self.drop_last or self.n%self.bs==0 else 1)\n\n    def get_idxs(self):\n        idxs = Inf.count if self.indexed else Inf.nones\n        if self.n is not None: idxs = list(itertools.islice(idxs, self.n))\n        if self.shuffle: idxs = self.shuffle_fn(idxs)\n        return idxs\n    \n    def sample(self): \n        return (b for i,b in enumerate(self.__idxs) if i//(self.bs or 1)%self.num_workers==self.offs)\n\n    def __iter__(self):\n        self.randomize()\n        self.before_iter()\n        self.__idxs=self.get_idxs() # called in context of main process (not workers/subprocesses)\n        for b in _loaders[self.fake_l.num_workers==0](self.fake_l):\n            # pin_memory causes tuples to be converted to lists, so convert them back to tuples\n            if self.pin_memory and type(b) == list: b = tuple(b)\n            if self.device is not None: b = to_device(b, self.device)\n            yield self.after_batch(b)\n        self.after_iter()\n        if hasattr(self, 'it'): del(self.it)\n\n    def create_batches(self, samps):\n        if self.dataset is not None: self.it = iter(self.dataset)\n        res = filter(lambda o:o is not None, map(self.do_item, samps))\n        yield from map(self.do_batch, self.chunkify(res))\n\n    def new(self, dataset=None, cls=None, **kwargs):\n        if dataset is None: dataset = self.dataset\n        if cls is None: cls = type(self)\n        cur_kwargs = dict(dataset=dataset, num_workers=self.fake_l.num_workers, pin_memory=self.pin_memory, timeout=self.timeout,\n                          bs=self.bs, shuffle=self.shuffle, drop_last=self.drop_last, indexed=self.indexed, device=self.device)\n        for n in self._methods:\n            o = getattr(self, n)\n            if not isinstance(o, MethodType): cur_kwargs[n] = o\n        return cls(**merge(cur_kwargs, kwargs))\n\n    @property\n    def device(self) -> torch.device|None:\n        return self._device\n\n    @device.setter\n    def device(self, device:int|str|torch.device|None):\n        self._device, *_ = torch._C._nn._parse_to(device=device)\n        if hasattr(self, 'after_batch') and hasattr(self.after_batch, 'fs'):\n            for tfm in self.after_batch.fs:\n                # Check that tfm.to is callable as TabularPandas & transforms set tfm.to as an object\n                if hasattr(tfm, 'to') and callable(tfm.to): tfm.to(device)\n                else:\n                    for a in L(getattr(tfm, 'parameters', None)):\n                        if hasattr(getattr(tfm, a), 'to'): setattr(tfm, a, getattr(tfm, a).to(device))\n\n    @property\n    def prebatched(self): return self.bs is None\n    def do_item(self, s):\n        try: return self.after_item(self.create_item(s))\n        except SkipItemException: return None\n    def chunkify(self, b): return b if self.prebatched else chunked(b, self.bs, self.drop_last)\n    def shuffle_fn(self, idxs): return self.rng.sample(idxs, len(idxs))\n    def randomize(self): self.rng = random.Random(self.rng.randint(0,2**32-1))\n    def retain(self, res, b):  return retain_types(res, b[0] if is_listy(b) else b)\n    def create_item(self, s):\n        if self.indexed: return self.dataset[s or 0]\n        elif s is None:  return next(self.it)\n        else: raise IndexError(\"Cannot index an iterable dataset numerically - must use `None`.\")\n    def create_batch(self, b): \n        try: return (fa_collate,fa_convert)[self.prebatched](b)\n        except Exception as e: \n            if not self.prebatched: collate_error(e,b)\n            raise\n    def do_batch(self, b): return self.retain(self.create_batch(self.before_batch(b)), b)\n    def to(self, device): self.device = device\n    def one_batch(self):\n        if self.n is not None and len(self)==0: raise ValueError(f'This DataLoader does not contain any batches')\n        with self.fake_l.no_multiproc(): res = first(self)\n        if hasattr(self, 'it'): delattr(self, 'it')\n        return res\n```\n\n----------------------------------------\n\nTITLE: Testing DiceLoss with Correct Output in PyTorch\nDESCRIPTION: Tests the `DiceLoss` (`dl`) function using the synthetic `correct_model_output` and `target` tensors. It asserts that the loss is zero when the model output is perfectly correct, as expected for a proper loss function. Requires PyTorch and a `DiceLoss` instance.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ntest_close(dl(correct_model_output, target), 0)\n```\n\n----------------------------------------\n\nTITLE: Processing Test Data with TabularPandas - Python\nDESCRIPTION: Creates a new `TabularPandas` object specifically for the test dataset, reusing the statistics (like mean, std, category mappings) learned from the training data during processing. This ensures consistent preprocessing.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ntest_to = to.new(test_df)\ntest_to.process()\n```\n\n----------------------------------------\n\nTITLE: Testing Module to Page Mapping\nDESCRIPTION: Includes test cases that verify the correctness of the '_mod2page' function across various PyTorch modules such as Tensor, torch.nn, F, torch.optim, and torch.utils.data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/99_pytorch_doc.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ntest_eq(_mod2page(Tensor), 'tensors.html')\ntest_eq(_mod2page(torch.nn), 'nn.html')\ntest_eq(_mod2page(inspect.getmodule(nn.Conv2d)), 'nn.html')\ntest_eq(_mod2page(F), 'nn.functional.html')\ntest_eq(_mod2page(torch.optim), 'optim.html')\ntest_eq(_mod2page(torch.utils.data), 'data.html')\n```\n\n----------------------------------------\n\nTITLE: Implementing Matthews Correlation Coefficient for Multi-label Classification in Python\nDESCRIPTION: A wrapper function that adapts scikit-learn's matthews_corrcoef for use in fastai's multi-label classification tasks, supporting threshold customization and sigmoid activation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef MatthewsCorrCoefMulti(thresh=0.5, sigmoid=True, sample_weight=None):\n    \"Matthews correlation coefficient for multi-label classification problems\"\n    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n    return skm_to_fastai(skm.matthews_corrcoef, thresh=thresh, activation=activation, flatten=False, sample_weight=sample_weight)\n```\n\n----------------------------------------\n\nTITLE: Installing fastai on Colab\nDESCRIPTION: This code snippet installs the fastai library using pip. It checks if a directory named '/content' exists and, if so, upgrades fastai on the Google Colaboratory environment. This is typically used at the start of a Colab notebook to ensure the latest version of fastai is available.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/quick_start.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Full Model with fit_one_cycle - Python\nDESCRIPTION: Trains the whole model for five epochs with a scheduled slice of learning rates, targeting fine-tuning performance. Inputs: learning rate slice. Outputs: improvement in validation metrics and potential reduction in loss.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(5, slice(1e-5, lr/5))\n```\n\n----------------------------------------\n\nTITLE: Defining a Dataset with Transform Subset 1 applied to range 0-7\nDESCRIPTION: Defines a custom transform `_Tfm` with split index 1, and creates a dataset with data range 0-7 applying this transform only to subset 1, with specified splits. No further processing shown.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_58\n\nLANGUAGE: Python\nCODE:\n```\n# only transform subset 1\nclass _Tfm(Transform):\n    split_idx=1\n    def encodes(self, x): return x*2\n\ndsets = Datasets(range(8), [None], splits=[[1,2,5,7],[0,3,4,6]])\n```\n\n----------------------------------------\n\nTITLE: Visualizing segmentation results\nDESCRIPTION: This code visualizes the results of the segmentation model. It plots the predicted segmentation masks on the images using the `show_results` method, displaying up to six examples. This allows for a qualitative assessment of the model's performance. Also, using the `SegmentationInterpretation` class, it plots the top losses.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/quick_start.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlearn.show_results(max_n=6, figsize=(7,8))\n```\n\nLANGUAGE: python\nCODE:\n```\ninterp = SegmentationInterpretation.from_learner(learn)\ninterp.plot_top_losses(k=2)\n```\n\n----------------------------------------\n\nTITLE: Reading Labels CSV into DataFrame - Python\nDESCRIPTION: Loads the labels file (train_v2.csv) into a pandas DataFrame for analysis and preparation. Requires pandas library and that the path to the CSV is correct. Typical input is a file path; output is a DataFrame with image names and labels.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(path/'train_v2.csv')\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Evaluating the Pre-trained Language Model with fastai in Python\nDESCRIPTION: Runs the validation loop using the initialized fastai `Learner` (`learn`) to evaluate the performance of the language model on the validation dataset before any fine-tuning. This calculates and displays the validation loss and specified metrics (e.g., perplexity).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nlearn.validate()\n```\n\n----------------------------------------\n\nTITLE: Training collaborative filtering model on MovieLens dataset using notebook_launcher\nDESCRIPTION: Defines a function to train a collaborative filtering model on the MovieLens dataset for movie recommendations, then launches it across 2 GPUs using notebook_launcher.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/distributed_app_examples.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.ML_SAMPLE)\ndf = pd.read_csv(path/'ratings.csv')\n\ndef train():\n    dls = CollabDataLoaders.from_df(df)\n    learn = collab_learner(dls, y_range=(0.5,5.5))\n    with learn.distrib_ctx(in_notebook=True):\n        learn.fine_tune(6)\n        \nnotebook_launcher(train, num_processes=2)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Classification DataLoader Batch (Python)\nDESCRIPTION: Displays a sample of up to two batches from the classification DataLoader, useful for debugging transform settings. Input: dls classification DataLoaders; Output: visual display in notebook or compatible interface.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndls.show_batch(max_n=2)\n```\n\n----------------------------------------\n\nTITLE: Example Plotting First 9 Top Losses (Python)\nDESCRIPTION: Demonstrates how to create an `Interpretation` instance from a learner and plot the top 9 losses using the `plot_top_losses` method. This is a common use case for quickly visualizing the most problematic predictions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ninterp = Interpretation.from_learner(learn)\ninterp.plot_top_losses(9)\n```\n\n----------------------------------------\n\nTITLE: Creating Dataloaders Utility Function for Validation Sets\nDESCRIPTION: This code demonstrates exporting the `dataloaders` method from fastai using a patch and delegates, enabling the creation of multiple data loaders with optional validation-specific transformations.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_61\n\nLANGUAGE: Python\nCODE:\n```\n#| echo: false\n_dsrc = Datasets([1,2])\nshow_doc(_dsrc.dataloaders, name=\"Datasets.dataloaders\")\n```\n\n----------------------------------------\n\nTITLE: fit_flat_cos method\nDESCRIPTION: Defines a `fit_flat_cos` method, which first trains the model with a flat learning rate, and then gradually anneals the learning rate using a cosine schedule. It takes parameters like `lr`, `div_final`, and `pct_start` to define the flat learning rate phase and the cosine annealing phase. It utilizes `ParamScheduler` for managing the schedule.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\ndef fit_flat_cos(self:Learner, n_epoch, lr=None, div_final=1e5, pct_start=0.75, wd=None,\n                 cbs=None, reset_opt=False, start_epoch=0):\n    \"Fit `self.model` for `n_epoch` at flat `lr` before a cosine annealing.\"\n    if self.opt is None: self.create_opt()\n    self.opt.set_hyper('lr', self.lr if lr is None else lr)\n    lr = np.array([h['lr'] for h in self.opt.hypers])\n    scheds = {'lr': combined_cos(pct_start, lr, lr, lr/div_final)}\n    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=0)\n```\n\n----------------------------------------\n\nTITLE: Joining Google Trends with Store Data\nDESCRIPTION: Extracts relevant columns from Google trends and joins them with store data after adding date features to the store dataset.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ngoogletrend = googletrend[['trend', 'State', 'trend_DE', 'Week', 'Year']]\nstore = add_datepart(store, 'Date', drop=False)\nstore = join_df(store, googletrend, ['Week', 'Year', 'State'])\n```\n\n----------------------------------------\n\nTITLE: Create Learner\nDESCRIPTION: This code creates a fastai Learner object for training the language model. It specifies the dataloaders, model, loss function (CrossEntropyLossFlat), optimizer function, callbacks, and evaluation metrics (accuracy and perplexity).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/35_tutorial.wikitext.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), opt_func=opt_func, cbs=cbs, metrics=[accuracy, Perplexity()])\n```\n\n----------------------------------------\n\nTITLE: Creating fastai DataLoaders from Lightning Dataloaders in Python\nDESCRIPTION: Constructs a fastai `DataLoaders` object by calling the `train_dataloader()` and `val_dataloader()` methods on the `LitModel` instance (`model`). The resulting `DataLoaders` object is then moved to the GPU using `.cuda()`, assuming a CUDA-enabled environment.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_lightning.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata = DataLoaders(model.train_dataloader(), model.val_dataloader()).cuda()\n```\n\n----------------------------------------\n\nTITLE: Install/Upgrade fastai on Colab (Bash)\nDESCRIPTION: Checks if the environment is Google Colab (`/content` exists) and, if so, uses pip to install or upgrade the `fastai` library quietly. This is often used as a setup step in Colab notebooks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Saving Finetuned Language Model Encoder for Classification (Python)\nDESCRIPTION: Saves only the encoder part of the language model to a file, enabling transfer to a downstream classifier. Input: fully finetuned learner; Output: model encoder weights stored as 'finetuned1'.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nlearn.save_encoder('finetuned1')\n```\n\n----------------------------------------\n\nTITLE: Defining a Resnet-ish Model with conv_and_res\nDESCRIPTION: This defines another ResNet-ish CNN model using the `conv_and_res` function and `nn.Sequential`. This version uses `conv_and_res` to simplify the model definition.  This helps to improve the code's organization and readability.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\nmodel = nn.Sequential(\n    conv_and_res(1, 8),\n    conv_and_res(8, 16),\n    conv_and_res(16, 32),\n    conv_and_res(32, 16),\n    conv2(16, 10),\n    Flatten()\n)\n```\n\n----------------------------------------\n\nTITLE: Defining URLs Class\nDESCRIPTION: The `URLs` class is a container for various URLs of datasets and models used within the fastai library. It includes constants for local paths and common URL prefixes, followed by specific dataset and model URLs. It also has a `path` method which returns the local path where data should be downloaded and saved, based on the provided `c_key`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/04_data.external.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass URLs():\n    \"Global constants for dataset and model URLs.\"\n    LOCAL_PATH = Path.cwd()\n    MDL = 'http://files.fast.ai/models/'\n    GOOGLE = 'https://storage.googleapis.com/'\n    S3  = 'https://s3.amazonaws.com/fast-ai-'\n    URL = f'{S3}sample/'\n\n    S3_IMAGE    = f'{S3}imageclas/'\n    S3_IMAGELOC = f'{S3}imagelocal/'\n    S3_AUDI     = f'{S3}audio/'\n    S3_NLP      = f'{S3}nlp/'\n    S3_COCO     = f'{S3}coco/'\n    S3_MODEL    = f'{S3}modelzoo/'\n\n    # main datasets\n    ADULT_SAMPLE        = f'{URL}adult_sample.tgz'\n    BIWI_SAMPLE         = f'{URL}biwi_sample.tgz'\n    CIFAR               = f'{URL}cifar10.tgz'\n    COCO_SAMPLE         = f'{S3_COCO}coco_sample.tgz'\n    COCO_TINY           = f'{S3_COCO}coco_tiny.tgz'\n    HUMAN_NUMBERS       = f'{URL}human_numbers.tgz'\n    IMDB                = f'{S3_NLP}imdb.tgz'\n    IMDB_SAMPLE         = f'{URL}imdb_sample.tgz'\n    ML_SAMPLE           = f'{URL}movie_lens_sample.tgz'\n    ML_100k             = 'https://files.grouplens.org/datasets/movielens/ml-100k.zip'\n    MNIST_SAMPLE        = f'{URL}mnist_sample.tgz'\n    MNIST_TINY          = f'{URL}mnist_tiny.tgz'\n    MNIST_VAR_SIZE_TINY = f'{S3_IMAGE}mnist_var_size_tiny.tgz'\n    PLANET_SAMPLE       = f'{URL}planet_sample.tgz'\n    PLANET_TINY         = f'{URL}planet_tiny.tgz'\n    IMAGENETTE          = f'{S3_IMAGE}imagenette2.tgz'\n    IMAGENETTE_160      = f'{S3_IMAGE}imagenette2-160.tgz'\n    IMAGENETTE_320      = f'{S3_IMAGE}imagenette2-320.tgz'\n    IMAGEWOOF           = f'{S3_IMAGE}imagewoof2.tgz'\n    IMAGEWOOF_160       = f'{S3_IMAGE}imagewoof2-160.tgz'\n    IMAGEWOOF_320       = f'{S3_IMAGE}imagewoof2-320.tgz'\n    IMAGEWANG           = f'{S3_IMAGE}imagewang.tgz'\n    IMAGEWANG_160       = f'{S3_IMAGE}imagewang-160.tgz'\n    IMAGEWANG_320       = f'{S3_IMAGE}imagewang-320.tgz'\n\n    # kaggle competitions download dogs-vs-cats -p {DOGS.absolute()}\n    DOGS = f'{URL}dogscats.tgz'\n\n    # image classification datasets\n    CALTECH_101  = f'{S3_IMAGE}caltech_101.tgz'\n    CARS         = f'{S3_IMAGE}stanford-cars.tgz'\n    CIFAR_100    = f'{S3_IMAGE}cifar100.tgz'\n    CUB_200_2011 = f'{S3_IMAGE}CUB_200_2011.tgz'\n    FLOWERS      = f'{S3_IMAGE}oxford-102-flowers.tgz'\n    FOOD         = f'{S3_IMAGE}food-101.tgz'\n    MNIST        = f'{S3_IMAGE}mnist_png.tgz'\n    PETS         = f'{S3_IMAGE}oxford-iiit-pet.tgz'\n\n    # NLP datasets\n    AG_NEWS                 = f'{S3_NLP}ag_news_csv.tgz'\n    AMAZON_REVIEWS          = f'{S3_NLP}amazon_review_full_csv.tgz'\n    AMAZON_REVIEWS_POLARITY = f'{S3_NLP}amazon_review_polarity_csv.tgz'\n    DBPEDIA                 = f'{S3_NLP}dbpedia_csv.tgz'\n    MT_ENG_FRA              = f'{S3_NLP}giga-fren.tgz'\n    SOGOU_NEWS              = f'{S3_NLP}sogou_news_csv.tgz'\n    WIKITEXT                = f'{S3_NLP}wikitext-103.tgz'\n    WIKITEXT_TINY           = f'{S3_NLP}wikitext-2.tgz'\n    YAHOO_ANSWERS           = f'{S3_NLP}yahoo_answers_csv.tgz'\n    YELP_REVIEWS            = f'{S3_NLP}yelp_review_full_csv.tgz'\n    YELP_REVIEWS_POLARITY   = f'{S3_NLP}yelp_review_polarity_csv.tgz'\n\n    # Image localization datasets\n    BIWI_HEAD_POSE     = f\"{S3_IMAGELOC}biwi_head_pose.tgz\"\n    CAMVID             = f'{S3_IMAGELOC}camvid.tgz'\n    CAMVID_TINY        = f'{URL}camvid_tiny.tgz'\n    LSUN_BEDROOMS      = f'{S3_IMAGE}bedroom.tgz'\n    PASCAL_2007        = f'{S3_IMAGELOC}pascal_2007.tgz'\n    PASCAL_2012        = f'{S3_IMAGELOC}pascal_2012.tgz'\n\n    # Audio classification datasets\n    MACAQUES           = f'{GOOGLE}ml-animal-sounds-datasets/macaques.zip'\n    ZEBRA_FINCH        = f'{GOOGLE}ml-animal-sounds-datasets/zebra_finch.zip'\n\n    # Medical Imaging datasets\n    #SKIN_LESION        = f'{S3_IMAGELOC}skin_lesion.tgz'\n    SIIM_SMALL         = f'{S3_IMAGELOC}siim_small.tgz'\n    TCGA_SMALL         = f'{S3_IMAGELOC}tcga_small.tgz'\n\n    #Pretrained models\n    OPENAI_TRANSFORMER = f'{S3_MODEL}transformer.tgz'\n    WT103_FWD          = f'{S3_MODEL}wt103-fwd.tgz'\n    WT103_BWD          = f'{S3_MODEL}wt103-bwd.tgz'\n\n    def path(\n        url:str='.', # File to download\n        c_key:str='archive' # Key in `Config` where to save URL\n    ) -> Path:\n        \"Local path where to download based on `c_key`\"\n        fname = url.split('/')[-1]\n        local_path = URLs.LOCAL_PATH/('models' if c_key=='model' else 'data')/fname\n        if local_path.exists(): return local_path\n        return fastai_path(c_key)/fname\n```\n\n----------------------------------------\n\nTITLE: Defining Module Public Interface (`_all_`)\nDESCRIPTION: Specifies the list of names (`HBox`, `VBox`, `widgets`, etc.) from `ipywidgets` that are intended to be publicly available when importing from this module using `import *`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09c_vision.widgets.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|export\n_all_ = ['HBox','VBox','widgets','Button','Checkbox','Dropdown','Layout','Box','Output','Label','FileUpload']\n```\n\n----------------------------------------\n\nTITLE: Installing fastai in Google Colab Environment\nDESCRIPTION: This shell script checks if the '/content' directory exists and, if so, upgrades fastai in the Colab environment using pip. It ensures the latest version is installed without verbose output, facilitating quick setup in a cloud notebook setting.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/index.ipynb#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n#|hide\n#| eval: false\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Defining PILImage Class (Python)\nDESCRIPTION: Defines the `PILImage` class, inheriting from `PILBase`. This class represents a standard RGB Pillow Image within the fastai ecosystem, intended to be convertible to a `TensorImage`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass PILImage(PILBase): \n    \"A RGB Pillow `Image` that can show itself and converts to `TensorImage`\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Viewing Enhanced Google Trends Data\nDESCRIPTION: Displays the transposed version of the first few rows of the googletrend dataframe after adding date features to visualize the newly created columns.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ngoogletrend.head().T\n```\n\n----------------------------------------\n\nTITLE: Documentation for unet_learner\nDESCRIPTION: This line documents the 'unet_learner' function, explaining it as a builder for U-Net models in FastAI, supporting architecture flexibility, normalization, pretraining, and other training configurations suitable for image segmentation tasks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\nshow_doc(unet_learner)\n```\n\n----------------------------------------\n\nTITLE: Define TerminateOnNaNCallback\nDESCRIPTION: Defines a callback that terminates training if the loss becomes NaN or infinite. The `after_batch` method checks for NaN or infinite loss and raises `CancelFitException` to stop the training loop.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass TerminateOnNaNCallback(Callback):\n    \"A `Callback` that terminates training if loss is NaN.\"\n    order=-9\n    def after_batch(self):\n        \"Test if `last_loss` is NaN and interrupts training.\"\n        if torch.isinf(self.loss) or torch.isnan(self.loss): raise CancelFitException\n```\n\n----------------------------------------\n\nTITLE: Loading and preparing recommendation data\nDESCRIPTION: This code loads the MovieLens sample dataset and creates `CollabDataLoaders`. It uses a CSV file as input and prepares data for collaborative filtering. The dataset will then be used for training a recommendation system.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/quick_start.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.ML_SAMPLE)\ndls = CollabDataLoaders.from_csv(path/'ratings.csv')\n```\n\n----------------------------------------\n\nTITLE: Defining Manual Data Splits - Python\nDESCRIPTION: Manually creates lists of indices to define distinct training and validation sets based on a fixed split point (indices 0-999 for train, 1000-1999 for validation).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nsplits = [list(range(1000)),list(range(1000,2000))]\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns for DataFrame Copy - Python\nDESCRIPTION: Creates a new DataFrame containing only the specified categorical, continuous, and dependent variables, plus the 'Date' column, and makes a copy to avoid modifying the original DataFrame.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndf = train_df[cat_names + cont_names + [dep_var,'Date']].copy()\n```\n\n----------------------------------------\n\nTITLE: Implementing Mean Squared Error (MSE) for Regression in PyTorch\nDESCRIPTION: Function that calculates the mean squared error between input and target tensors. It uses PyTorch's F.mse_loss function after flattening and checking the inputs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef mse(inp,targ):\n    \"Mean squared error between `inp` and `targ`.\"\n    return F.mse_loss(*flatten_check(inp,targ))\n```\n\n----------------------------------------\n\nTITLE: Training the Fully Unfrozen Model (Python)\nDESCRIPTION: Unfreezes all layers of the model using `unfreeze()`. Trains the entire model for two epochs using `fit_one_cycle` with discriminative learning rates (`slice(1e-3/(2.6**4),1e-3)`) to fine-tune all parameters.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))\n```\n\n----------------------------------------\n\nTITLE: Export Notebook Cells Python\nDESCRIPTION: This command is specific to the nbdev development environment. It exports the code marked with `#|export` from the notebook cells into the corresponding Python modules, making the code available for use as a library.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14a_callback.data.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Exporting the Learner for Production Inference - Python\nDESCRIPTION: Command (commented out) exports the full learner, including architecture and weights, for production or submission. No runtime effect unless uncommented. Output: .pkl file for deployment.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n#learn.export()\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders for Dataset with Transformations\nDESCRIPTION: Creates datasets with range data applying multiple transforms, then constructs data loaders with batch size 4 using CPU device for processing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_73\n\nLANGUAGE: Python\nCODE:\n```\ndsets = Datasets(range(8), [[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]])\ndls = dsets.dataloaders(bs=4, device=torch.device('cpu'))\n```\n\n----------------------------------------\n\nTITLE: masked_concat_pool - Padding Independence Test\nDESCRIPTION: This tests the independence of the `masked_concat_pool` function from the padding values by modifying the padded part of the `out` tensor. It creates a new tensor `out1` which clones the non-padded parts from `out`. Then, the `masked_concat_pool` function is called again. The results of both function calls should be the same and this is tested.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nout1 = torch.randn(2,4,5)\nout1[0,2:] = out[0,2:].clone()\nout1[1,:3] = out[1,:3].clone()\nx1 = masked_concat_pool(out1, mask, 2)\ntest_eq(x, x1)\n```\n\n----------------------------------------\n\nTITLE: Importing Documentation Utilities - Python\nDESCRIPTION: Imports functions from nbdev.showdoc to enable automatic documentation generation within Jupyter or nbsphinx. The only dependency is nbdev. No parameters are required and the outputs are mainly for inline documentation rendering.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Suggesting Learning Rate via Steepest Slope in Python\nDESCRIPTION: The steep function computes the gradient of the loss with respect to the logarithm of the learning rate and suggests the learning rate corresponding to the steepest negative slope, indicating the point of most rapid loss decrease. It returns this learning rate and the associated loss index, enabling an informed choice during learning rate finder runs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndef steep(lrs:list, losses:list, num_it:int) -> (float, tuple):\n    \"Suggests a learning rate when the slope is the steepest and returns its index\"\n    grads = (losses[1:]-losses[:-1]) / (lrs[1:].log()-lrs[:-1].log())\n    lr_steep = lrs[grads.argmin()].item()\n    loss_idx = losses[min(range(len(lrs)), key=lambda i: abs(lrs[i]-lr_steep))]\n    return lr_steep, (lr_steep, loss_idx)\n```\n\n----------------------------------------\n\nTITLE: Running Learning Rate Finder - fastai/Python\nDESCRIPTION: Executes fastai's learning rate finder method (`learn.lr_find()`). This trains the model for a few epochs, increasing the learning rate exponentially, and records the loss. It helps determine an optimal learning rate range for training.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nlearn.lr_find()\n```\n\n----------------------------------------\n\nTITLE: Exporting using nbdev\nDESCRIPTION: This code uses the `nbdev_export` function from the `nbdev` library.  It's a utility to export the current notebook as a Python module, creating the corresponding `.py` files for the documented code.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/04_data.external.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Sort movie ratings by bias (highest)\nDESCRIPTION: Sorts the `movie_ratings` list by the bias values (the first element of each tuple) in descending order and displays the first 15 movies. This shows the movies with the highest learned biases.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nsorted(movie_ratings, key=lambda o: o[0], reverse=True)[:15]\n```\n\n----------------------------------------\n\nTITLE: Adding Elapsed Time Features\nDESCRIPTION: Uses fastai's add_elapsed_times function to calculate days before/after events like promotions and holidays for each store.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nall_ftrs = add_elapsed_times(all_ftrs, ['Promo', 'StateHoliday', 'SchoolHoliday'], \n                             date_field='Date', base_field='Store')\n```\n\n----------------------------------------\n\nTITLE: Visualizing GAN Generated Images in fastai Python\nDESCRIPTION: Displays up to 16 generated bedroom images from the GAN learner after training, using a figure size of 8x8. This allows qualitative evaluation of the generator's output quality.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-wgan.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#learn.gan_trainer.switch(gen_mode=True)\nlearn.show_results(max_n=16, figsize=(8,8), ds_idx=0)\n```\n\n----------------------------------------\n\nTITLE: Call show_install\nDESCRIPTION: This code calls the `show_install` function, which prints the user's setup information, including software and hardware details. The `True` argument indicates that NVIDIA SMI information should be included.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/97_test_utils.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nshow_install(True)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setup - Python\nDESCRIPTION: Imports necessary libraries for plotting, fastai basics, and file compression handling. The `%matplotlib inline` command is used in Jupyter environments to display plots directly below the code cells.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\nfrom fastai.basics import *\nimport gzip\n```\n\n----------------------------------------\n\nTITLE: Reading Pickled DataFrame - Python\nDESCRIPTION: Loads a pre-processed training DataFrame from a pickle file stored at the specified path. This assumes data cleaning and feature engineering have already been performed.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_df = pd.read_pickle(path/'train_clean')\n```\n\n----------------------------------------\n\nTITLE: Splitting Enhanced Data Back into Training and Test Sets\nDESCRIPTION: Separates the combined dataset back into training and test datasets after all features have been added.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntrain_df = all_ftrs.iloc[:len(train_fe)]\ntest_df  = all_ftrs.iloc[len(train_fe):]\n```\n\n----------------------------------------\n\nTITLE: Scatter plot of PCA components\nDESCRIPTION: Generates a scatter plot of the first and third principal components for a random subset of 50 movies. The plot displays the movie titles as text annotations, allowing for visual exploration of movie relationships in the reduced feature space.  The movies are selected by index rather than randomly to ensure deterministic output for this documentation example. Requires `matplotlib.pyplot`.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nidxs = np.random.choice(len(top_movies), 50, replace=False)\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(15,15))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Testing SkipToEpoch Callback\nDESCRIPTION: This snippet tests a custom callback named `TestSkipToEpoch`. The `after_train` event of the callback is triggered after the training.  It tests `start_epoch` parameter, showing that the training process skips epochs as specified. The assertion `assert self.epoch >= 2` checks for the expected behavior of the `SkipToEpoch` callback. The second part checks that the model parameters remain unchanged.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nclass TestSkipToEpoch(Callback):\n    def after_train(self):\n        assert self.epoch >= 2\nlearn = synth_learner(cbs=TestSkipToEpoch())\nlearn.fit(4, start_epoch=2)\n\nlearn = synth_learner()\np0_pre = first(learn.model.parameters()).data.clone()\nlearn.fit(3, start_epoch=3)\np0 = first(learn.model.parameters()).data\ntest_eq(p0_pre, p0)\n```\n\n----------------------------------------\n\nTITLE: Loading Saved Language Model State (Python)\nDESCRIPTION: Restores the model weights from a previously saved checkpoint ('stage1'). Input: model checkpoint must exist; Output: learner now contains weights as loaded from storage.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nlearn.load('stage1');\n```\n\n----------------------------------------\n\nTITLE: Training the Resnet-ish model\nDESCRIPTION: This trains the ResNet-ish model for 12 epochs using `fit_one_cycle` with a specified maximum learning rate.  The model is trained using the `dls` data loaders, the defined loss function, and the accuracy metric.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\nlearn.fit_one_cycle(12, lr_max=0.05)\n```\n\n----------------------------------------\n\nTITLE: Implementing ReduceLROnPlateau in FastAI\nDESCRIPTION: A TrackerCallback that automatically reduces the learning rate when a monitored metric stops improving for a specified number of epochs. It includes settings for patience, reduction factor, and minimum learning rate.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass ReduceLROnPlateau(TrackerCallback):\n    \"A `TrackerCallback` that reduces learning rate when a metric has stopped improving.\"\n    order=TrackerCallback.order+2\n    def __init__(self, \n        monitor='valid_loss', # value (usually loss or metric) being monitored.\n        comp=None, # numpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n        min_delta=0., # minimum delta between the last monitor value and the best monitor value.\n        patience=1, # number of epochs to wait when training has not improved model.\n        factor=10., # the denominator to divide the learning rate by, when reducing the learning rate.\n        min_lr=0, # the minimum learning rate allowed; learning rate cannot be reduced below this minimum.\n        reset_on_fit=True # before model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss).\n    ):\n        super().__init__(monitor=monitor, comp=comp, min_delta=min_delta, reset_on_fit=reset_on_fit)\n        self.patience,self.factor,self.min_lr = patience,factor,min_lr\n\n    def before_fit(self): self.wait = 0; super().before_fit()\n    def after_epoch(self):\n        \"Compare the value monitored to its best score and reduce LR by `factor` if no improvement.\"\n        super().after_epoch()\n        if self.new_best: self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                old_lr = self.opt.hypers[-1]['lr']\n                for h in self.opt.hypers: h['lr'] = max(h['lr'] / self.factor, self.min_lr)\n                self.wait = 0\n                if self.opt.hypers[-1][\"lr\"] < old_lr:\n                    print(f'Epoch {self.epoch}: reducing lr to {self.opt.hypers[-1][\"lr\"]}')\n```\n\n----------------------------------------\n\nTITLE: Extending pandas DataFrame Initialization with Tensor Support in Python\nDESCRIPTION: Modifies the pandas DataFrame __init__ method to accept tensors as input by converting them to numpy arrays before initialization. It also preserves the original __init__ method to allow fallback. This enables seamless creation of DataFrames from PyTorch tensors without manual conversion.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_62\n\nLANGUAGE: python\nCODE:\n```\nif not hasattr(pd.DataFrame,'_old_init'): pd.DataFrame._old_init = pd.DataFrame.__init__\n```\n\nLANGUAGE: python\nCODE:\n```\n@patch\ndef __init__(self:pd.DataFrame, data=None, index=None, columns=None, dtype=None, copy=None):\n    if data is not None and isinstance(data, Tensor): data = to_np(data)\n    self._old_init(data, index=index, columns=columns, dtype=dtype, copy=copy)\n```\n\n----------------------------------------\n\nTITLE: Implementing pad_chunk Function for Chunk-based Padding\nDESCRIPTION: A function that pads tensor inputs in chunks of specified length. It can pad at the beginning (if pad_first=True) or end, and ensures proper type retention. This is optimized for use with sequential models.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/31_text.data.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef pad_chunk(x,pad_idx=1, pad_first=True, seq_len=72, pad_len=10):\n    \"Pad `x` by adding padding by chunks of size `seq_len`\"\n    l = pad_len - x.shape[0]\n    pad_chunk = x.new_zeros((l//seq_len) * seq_len) + pad_idx\n    pad_res   = x.new_zeros(l % seq_len) + pad_idx\n    x1 = torch.cat([pad_chunk, x, pad_res]) if pad_first else torch.cat([x, pad_chunk, pad_res])\n    return retain_type(x1, x)\n```\n\n----------------------------------------\n\nTITLE: Converting NumPy Arrays to PyTorch Tensors with Custom Behavior in Python\nDESCRIPTION: The `_array2tensor` function converts numpy arrays to PyTorch tensors addressing dtype conversions such as UINT16 to float32, and on Windows converts default integers to int64 to fix known issues. Supports additional options like `requires_grad` and `pin_memory` to fit tensor use cases in PyTorch pipelines.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef _array2tensor(x, requires_grad=False, pin_memory=False, **kwargs):\n    if x.dtype==np.uint16: x = x.astype(np.float32)\n    # windows default numpy int dtype is int32, while torch tensor default int dtype is int64\n    # https://github.com/numpy/numpy/issues/9464\n    if sys.platform == \"win32\" and x.dtype==int: x = x.astype(np.int64)\n    t = torch.as_tensor(x, **kwargs)\n    t.requires_grad_(requires_grad)\n    if pin_memory: t.pin_memory()\n    return t\n```\n\n----------------------------------------\n\nTITLE: Testing FocalLossFlat with axis=1\nDESCRIPTION: Tests `FocalLossFlat` with `axis=1` which is appropriate for segmentation tasks. The test verifies that the activation and decoding functions behave as expected and compares the output to `CrossEntropyLossFlat`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n#In a segmentation task, we want to take the softmax over the channel dimension\nfl = FocalLossFlat(gamma=0, axis=1)\nce = CrossEntropyLossFlat(axis=1)\noutput = torch.randn(32, 5, 128, 128)\ntarget = torch.randint(0, 5, (32, 128, 128))\ntest_close(fl(output, target), ce(output, target), eps=1e-4)\ntest_eq(fl.activation(output), F.softmax(output, dim=1))\ntest_eq(fl.decodes(output), output.argmax(dim=1))\n```\n\n----------------------------------------\n\nTITLE: Loading Movie Titles\nDESCRIPTION: This snippet loads the movie titles data from the 'u.item' file. It uses a pipe ('|') delimiter, latin-1 encoding, specifies which columns to use (movie, title), and assigns column names.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmovies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1',\n                     usecols=(0,1), names=('movie','title'), header=None)\n```\n\n----------------------------------------\n\nTITLE: Configuring GitHub SSH authentication\nDESCRIPTION: Generates SSH keys, copies public key content to GitHub, and sets up personal access tokens for secure GitHub operations, enabling seamless repository access.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/dev-setup.ipynb#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\necho -e \"github.com:\\n  user: $GH_USER\\n  oauth_token: $TOKEN\\\" > ~/.config/gh/hosts.yml\n```\n\nLANGUAGE: Shell\nCODE:\n```\ngh config set git_protocol ssh\n```\n\n----------------------------------------\n\nTITLE: Sort movies by second PCA component (highest)\nDESCRIPTION: Sorts the movies by the values of the second principal component in descending order and displays the top 10 movies. This reveals movies that are highly associated with the second component.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nmovie_comp = [(f, i) for f,i in zip(fac1, top_movies)]\nsorted(movie_comp, key=itemgetter(0), reverse=True)[:10]\n```\n\n----------------------------------------\n\nTITLE: Installing fastai with Conda (Bash)\nDESCRIPTION: This command installs the fastai library using the conda package manager from the fastai channel. It is the recommended installation method for Linux and Windows users after installing PyTorch via conda.\nSOURCE: https://github.com/fastai/fastai/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda install fastai::fastai\n```\n\n----------------------------------------\n\nTITLE: Setting Learning Rate Based on Batch Size for Classifier Training (Python)\nDESCRIPTION: Computes initial learning rate proportionally to batch size for classifier training. Input: current bs; Output: lr float to be used in fit_one_cycle.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/ulmfit.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nlr = 1e-1 * bs/128\n```\n\n----------------------------------------\n\nTITLE: Creating fastai Learner - fastai/Python\nDESCRIPTION: Initializes a fastai `Learner` object. The `Learner` encapsulates the DataLoaders (`dls`), the model (`Mnist_NN`), the loss function (`loss_func`), and metrics (`accuracy`), streamlining the training process.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nlearn = Learner(dls, Mnist_NN(), loss_func=loss_func, metrics=accuracy)\n```\n\n----------------------------------------\n\nTITLE: Viewing Tail of Sample DataFrame Section - Python\nDESCRIPTION: Displays the first few rows of the small experimental DataFrame starting from index 1000. This is used to inspect the start of the potential validation split.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsmall_df.iloc[1000:].head()\n```\n\n----------------------------------------\n\nTITLE: Example using fastai_path for the Archive\nDESCRIPTION: This is an example of calling the `fastai_path` function with the argument 'archive' to retrieve the local path to the archive directory as configured by `fastai_cfg`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/04_data.external.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfastai_path('archive')\n```\n\n----------------------------------------\n\nTITLE: Export Notebook - nbdev - Python\nDESCRIPTION: This code snippet exports the current notebook using the `nbdev_export` function from the `nbdev` library.  It's used to create a Python module from the notebook.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_77\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Saving Final Model State for Competition Submission - Python\nDESCRIPTION: Saves the fully trained network weights with the identifier 'stage-2-256-rn50' for later inference or submission. No output except updated model files.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nlearn.save('stage-2-256-rn50')\n```\n\n----------------------------------------\n\nTITLE: Idxmax Patch for cuDF Series\nDESCRIPTION: This code adds an `idxmax` method to `cudf.Series` objects.  It returns the index of the first occurrence of the maximum value in the series.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\ndef idxmax(self:cudf.Series):\n    \"Return the index of the first occurrence of the max in `self`\"\n    return self.argsort(ascending=False).index[0]\n```\n\n----------------------------------------\n\nTITLE: Combining Training and Test Data for Feature Engineering\nDESCRIPTION: Appends test data to training data to create a complete time series for calculating elapsed time features across the entire dataset.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nall_ftrs = train_fe.append(test_fe, sort=False)\n```\n\n----------------------------------------\n\nTITLE: Instantiating Neural Network Model and Moving to GPU - PyTorch/Python\nDESCRIPTION: Creates a new instance of the `Mnist_NN` model and moves it to the GPU. This replaces the previous `model` (the logistic regression model) with the new neural network.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nmodel = Mnist_NN().cuda()\n```\n\n----------------------------------------\n\nTITLE: Define user, item, and title column names\nDESCRIPTION: Defines variables for the column names representing user ID, movie ID, and movie title. These variables are used throughout the notebook to reference the corresponding columns in the DataFrames.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nuser,item,title = 'userId','movieId','title'\n```\n\n----------------------------------------\n\nTITLE: Finding Optimal Learning Rate - Python\nDESCRIPTION: Runs fastai's learning rate finder utility, which trains the model for a few epochs with exponentially increasing learning rates to help identify a suitable learning rate for training.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nlearn.lr_find()\n```\n\n----------------------------------------\n\nTITLE: Train collaborative filtering model on Movielens 100k\nDESCRIPTION: Trains the collaborative filtering model using the `fit_one_cycle` method with 5 epochs, a maximum learning rate of 5e-3, and a weight decay of 1e-1.  Weight decay helps to prevent overfitting.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(5, 5e-3,  wd=1e-1)\n```\n\n----------------------------------------\n\nTITLE: Export fastai module\nDESCRIPTION: Exports the content to the appropriate location for documentation generation and/or deployment in a python package. This will likely be implemented with the nbdev library.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Creating synthetic dataset for testing channels last format\nDESCRIPTION: Generates a dummy dataset with random tensors for input images, wrapped in a DataLoader. Designed for testing purposes with GPU acceleration, simulating training data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18c_callback.channelslast.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\nfrom torch.utils.data import TensorDataset\n```\n\n----------------------------------------\n\nTITLE: Importing fastai Callbacks - fastai/Python\nDESCRIPTION: Imports all available callbacks from the fastai callback module. Callbacks are used to modify or extend the training loop behavior, such as logging, scheduling hyperparamters, or early stopping.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.callback.all import *\n```\n\n----------------------------------------\n\nTITLE: Creating and training a tabular model\nDESCRIPTION: This code creates a tabular learner and trains it. It uses a previously prepared `DataLoaders` object, the accuracy metric, and trains for 2 epochs. This trains a model for tabular data analysis.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/quick_start.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nlearn = tabular_learner(dls, metrics=accuracy)\nlearn.fit_one_cycle(2)\n```\n\n----------------------------------------\n\nTITLE: Creating a Learner for the Resnet-ish Model\nDESCRIPTION: This creates a `Learner` object for the ResNet-ish CNN model. This initializes the training process with the data loaders, model, loss function, and accuracy metric.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\nlearn = Learner(dls, model, loss_func = nn.CrossEntropyLoss(), metrics=accuracy)\n```\n\n----------------------------------------\n\nTITLE: Examining First Five Predicted Labels - Python\nDESCRIPTION: Prints or displays the first five predicted label strings from the test output. Used for sanity checking the label formatting and output prior to submission.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nlabelled_preds[:5]\n```\n\n----------------------------------------\n\nTITLE: Installing and setup commands for fastai and environment preparation\nDESCRIPTION: Shell commands to upgrade fastai in a Colab environment and a placeholder for additional setup code. The primary purpose is environment setup before running FastAI code with AzureML integration.\nSOURCE: https://github.com/fastai/fastai/blob/main/disabled/74_callback.azureml.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Sort movies by first PCA component (highest)\nDESCRIPTION: Sorts the movies by the values of the first principal component in descending order and displays the top 10 movies. This reveals movies that are highly associated with the first component.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nsorted(movie_comp, key=itemgetter(0), reverse=True)[:10]\n```\n\n----------------------------------------\n\nTITLE: Creating Datasets with Multiple Transforms and Validating\nDESCRIPTION: Creates a dataset with range 0-4, applies multiple transforms, and specifies splits. It verifies train and validation data after applying the transformations.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_56\n\nLANGUAGE: Python\nCODE:\n```\ndsets = Datasets(range(5), [_Tfm(),noop], splits=[[1,2],[0,3,4]])\ntest_eq(dsets.train,[(1,1),(2,2)])\ntest_eq(dsets.valid,[(0,0),(6,3),(8,4)])\n```\n\n----------------------------------------\n\nTITLE: Configuring GAN Learner Recorder Metrics in fastai Python\nDESCRIPTION: Enables training metrics and disables validation metrics recording in the GAN learner's recorder object. This focuses the logging on training progress only, as no validation set is used.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-wgan.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlearn.recorder.train_metrics=True\nlearn.recorder.valid_metrics=False\n```\n\n----------------------------------------\n\nTITLE: Setting Learning Rate for 256x256 Training Cycle - Python\nDESCRIPTION: Assigns a learning rate value (1e-2/2) for the new head-training phase with higher-resolution images. Variable used in subsequent training call.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nlr=1e-2/2\n```\n\n----------------------------------------\n\nTITLE: Installing psutil for Hardware Monitoring\nDESCRIPTION: Shell command to install the `psutil` library using pip. This library enables Neptune to monitor and log hardware usage (CPU, memory) during the experiment.\nSOURCE: https://github.com/fastai/fastai/blob/main/disabled/70b_callback.neptune.ipynb#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip install psutil\n```\n\n----------------------------------------\n\nTITLE: Training the Model\nDESCRIPTION: This trains the model for 3 epochs using the `fit_one_cycle` method with a specified maximum learning rate.  It trains the model using the `dls` DataLoaders and the defined loss function and metrics.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\nlearn.fit_one_cycle(3, lr_max=0.1)\n```\n\n----------------------------------------\n\nTITLE: Saving Submission File - Python\nDESCRIPTION: Selects the 'Id' and 'Sales' columns from the test DataFrame, ensures they are integer type, and saves them to a CSV file named 'rossmann_submission.csv' in the format required for the competition, excluding the pandas index.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\ntest_df[[\"Id\",\"Sales\"]] = test_df[[\"Id\",\"Sales\"]].astype(\"int\")\ntest_df[[\"Id\",\"Sales\"]].to_csv(\"rossmann_submission.csv\",index=False)\n```\n\n----------------------------------------\n\nTITLE: Sort movies by second PCA component (lowest)\nDESCRIPTION: Sorts the movies by the values of the second principal component in ascending order and displays the top 10 movies. This reveals movies that are negatively associated with the second component.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nsorted(movie_comp, key=itemgetter(0))[:10]\n```\n\n----------------------------------------\n\nTITLE: Managing FastAI Installation in Colab Using Shell Command in Python\nDESCRIPTION: This snippet conditionally upgrades the fastai library on Colab if the '/content' directory exists. It uses a shell command inside Python with suppression of output and error messages. This ensures fastai is up-to-date when running in the Google Colab environment.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Checking Proper Weight Decay Handling for BatchNorm/Bias in fastai Learner (Python)\nDESCRIPTION: Defines '_TstModel' (with Linear and BatchNorm layers) and '_PutGrad' (a custom Callback that sets gradients). A Learner is configured with decoupled weight decay, then single-epoch training is run and parameter updates are asserted to confirm that weight decay is not inappropriately applied to BatchNorm and bias terms if 'wd_bn_bias=False'. Dependencies: torch, nn.Module, partial, SGD, and fastais test utility functions.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#Check wd is not applied on bn/bias when option wd_bn_bias=False\nclass _TstModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a,self.b = nn.Parameter(torch.randn(1)),nn.Parameter(torch.randn(1))\n        self.tst = nn.Sequential(nn.Linear(4,5), nn.BatchNorm1d(3))\n        self.tst[0].bias.data,self.tst[1].bias.data = torch.randn(5),torch.randn(3) \n    def forward(self, x): return x * self.a + self.b\n    \nclass _PutGrad(Callback):\n    def before_step(self):\n        for p in self.learn.tst.parameters():\n            p.grad = torch.ones_like(p.data)\n    \nlearn = synth_learner(n_train=5, opt_func = partial(SGD, wd=1, decouple_wd=True), cbs=_PutGrad)\nlearn.model = _TstModel()\ninit = [p.clone() for p in learn.tst.parameters()]\nlearn.fit(1, lr=1e-2)\nend = list(learn.tst.parameters())\nassert not torch.allclose(end[0]-init[0], -0.05 * torch.ones_like(end[0]))\nfor i in [1,2,3]: test_close(end[i]-init[i], -0.05 * torch.ones_like(end[i]))\n```\n\n----------------------------------------\n\nTITLE: Create collaborative data loaders from DataFrame\nDESCRIPTION: Creates collaborative data loaders (`CollabDataLoaders`) from the `ratings` DataFrame using the `from_df` method.  Sets the batch size to 64 and the random seed to 42 for reproducibility.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndls = CollabDataLoaders.from_df(ratings, bs=64, seed=42)\n```\n\n----------------------------------------\n\nTITLE: Download and set path for the sample Movielens dataset\nDESCRIPTION: Downloads the sample Movielens dataset using `untar_data` and stores the path to the dataset in the `path` variable. This path is used for loading the data into pandas DataFrames.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npath = untar_data(URLs.ML_SAMPLE)\npath\n```\n\n----------------------------------------\n\nTITLE: Learning Rate Finder for New Data Pipeline - Python\nDESCRIPTION: Runs learning rate search again to determine suitable rates for the new dataloader setup. Inputs/outputs same as previous lr_find calls.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nlearn.lr_find()\n```\n\n----------------------------------------\n\nTITLE: Train the collaborative filtering model\nDESCRIPTION: Trains the collaborative filtering model using the `fit_one_cycle` method with 3 epochs and a maximum learning rate of 5e-3. This training method uses a learning rate schedule that increases then decreases.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlearn.fit_one_cycle(3, 5e-3)\n```\n\n----------------------------------------\n\nTITLE: Updating Notebooks from Library Changes (Bash)\nDESCRIPTION: Uses the nbdev update command to export code changes from library files back to the corresponding notebook cells. This is useful when modifications are made outside of the notebooks.\nSOURCE: https://github.com/fastai/fastai/blob/main/CONTRIBUTING.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\nnbdev_update\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Access to Attributes\nDESCRIPTION: Shows how to access existing attributes of the learner and how to define callbacks via subclassing and the impact that has in attributes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmk_class('TstLearner', 'a')\n\nclass TstCallback(Callback):\n    def batch_begin(self): print(self.a)\n\nlearn,cb = TstLearner(1),TstCallback()\ncb.learn = learn\ntest_stdout(lambda: cb('batch_begin'), \"1\")\n```\n\n----------------------------------------\n\nTITLE: Viewing Transposed DataFrame Head - Python\nDESCRIPTION: Displays the first few rows of the training DataFrame with rows and columns transposed. This is useful for quickly inspecting many columns simultaneously.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_df.head().T\n```\n\n----------------------------------------\n\nTITLE: Instantiating a PyTorch Lightning Model in Python\nDESCRIPTION: Creates an instance of the `LitModel` class. This class is assumed to be a PyTorch Lightning `LightningModule` defined in the imported `migrating_lightning` module or available in the current scope.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_lightning.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = LitModel()\n```\n\n----------------------------------------\n\nTITLE: Documentation for TensorBoardCallback\nDESCRIPTION: Creates documentation for the TensorBoardCallback class, highlighting its methods for logging model details, losses, metrics, and predictions, and enabling feature projection during training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nshow_doc(TensorBoardCallback)\n```\n\n----------------------------------------\n\nTITLE: Exporting with nbdev\nDESCRIPTION: This code uses `nbdev_export` to export the current notebook as a Python module, making the defined functions available for external use.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Importing AzureML core and exception classes\nDESCRIPTION: Imports necessary classes from AzureML SDK to interact with AzureML runs and handle environment exceptions, foundational for the callback's functionality.\nSOURCE: https://github.com/fastai/fastai/blob/main/disabled/74_callback.azureml.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom azureml.core.run import Run\nfrom azureml.exceptions import RunEnvironmentException\nimport warnings\n```\n\n----------------------------------------\n\nTITLE: Testing LinearDecoder Integration with AWD_LSTM Encoder in Python\nDESCRIPTION: Demonstrates instantiation of the AWD_LSTM encoder with specified vocabulary size and hidden dimensions, generation of a random input tensor, and passing it through the encoder and LinearDecoder. Also tests correct shape of outputs and weight tying functionality between decoder and encoder embeddings, ensuring the LinearDecoder behaves as expected within a language model pipeline.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nenc = AWD_LSTM(100, 20, 10, 2)\nx = torch.randint(0, 100, (10,5))\nr = enc(x)\n\ntst = LinearDecoder(100, 20, 0.1)\ny = tst(r)\ntest_eq(y[1], r)\ntest_eq(y[2].shape, r.shape)\ntest_eq(y[0].shape, [10, 5, 100])\n\ntst = LinearDecoder(100, 20, 0.1, tie_encoder=enc.encoder)\ntest_eq(tst.decoder.weight, enc.encoder.weight)\n```\n\n----------------------------------------\n\nTITLE: Selecting Random Sample Indices - Python\nDESCRIPTION: Generates a random permutation of indices and selects the first 2000 to create a small sample for experimentation. The indices are then sorted.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nidx = np.random.permutation(range(n))[:2000]\nidx.sort()\n```\n\n----------------------------------------\n\nTITLE: Installing fastai package on Colab\nDESCRIPTION: Checks if the /content directory exists and installs or upgrades the fastai library silently on Google Colab environment to ensure the latest version is used.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/dev-setup.ipynb#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai\n```\n\n----------------------------------------\n\nTITLE: Listing Files in the Data Path\nDESCRIPTION: These snippets list the contents of the directory specified by the variable `path`, and specifically the 'train' subfolder. This is useful to understand the dataset's structure.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npath.ls()\n```\n\nLANGUAGE: python\nCODE:\n```\n(path/'train').ls()\n```\n\n----------------------------------------\n\nTITLE: Read Wheel METADATA File (Python)\nDESCRIPTION: Reads the content of the `METADATA` file located within the `.dist-info` directory of the opened Wheel file (`fwhl`) and decodes it into a string.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmeta = fwhl.read(f'{fwhl.dist_info_path}/METADATA').decode()\n```\n\n----------------------------------------\n\nTITLE: Getting Data Path - fastai/Python\nDESCRIPTION: Retrieves the configured data path for the 'mnist' dataset using fastai's configuration system. This path is where the MNIST dataset file is expected to be located.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npath = Config().data/'mnist'\n```\n\n----------------------------------------\n\nTITLE: Displaying a Batch of DataLoaders Samples in fastai Python\nDESCRIPTION: Shows a batch of 16 image samples from the dataloaders to visually confirm the input data quality and preprocessing correctness before training.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-wgan.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndls.show_batch(max_n=16)\n```\n\n----------------------------------------\n\nTITLE: Saving Processed Datasets\nDESCRIPTION: Pickles the cleaned and feature-enhanced training and test datasets to avoid repeating the preprocessing steps.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ntrain_df.to_pickle(path/'train_clean')\ntest_df.to_pickle(path/'test_clean')\n```\n\n----------------------------------------\n\nTITLE: Unlisting elements of length 1\nDESCRIPTION: This function `_unlist` get element of lists of length 1, useful to simplify the table view on W&B\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef _unlist(l):\n    \"get element of lists of lenght 1\"\n    if isinstance(l, (list, tuple)):\n        if len(l) == 1: return l[0]\n    else: return l\n```\n\n----------------------------------------\n\nTITLE: Plotting Loss History - Python\nDESCRIPTION: Plots the training and validation loss curves recorded during the training process. `skip_start` parameter skips the initial iterations in the plot.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nlearn.recorder.plot_loss(skip_start=1000)\n```\n\n----------------------------------------\n\nTITLE: Loading Data into Pandas DataFrame\nDESCRIPTION: Reads the downloaded 'adult.csv' file into a pandas DataFrame. It then displays the first few rows of the DataFrame to inspect the raw data structure.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndf = pd.read_csv(path/'adult.csv')\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Importing Core Libraries and Handling Pillow Compatibility (Python)\nDESCRIPTION: Imports essential modules from fastai (`torch_basics`, `data.all`) and the Python Imaging Library (PIL). It also handles potential `AttributeError` for different Pillow versions when accessing `Image.Resampling`, ensuring compatibility by falling back to the older `PIL.Image` attributes if necessary.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom __future__ import annotations\nfrom fastai.torch_basics import *\nfrom fastai.data.all import *\n\nfrom PIL import Image\n\ntry: BILINEAR,NEAREST = Image.Resampling.BILINEAR,Image.Resampling.NEAREST\nexcept AttributeError: from PIL.Image import BILINEAR,NEAREST\n```\n\n----------------------------------------\n\nTITLE: Defining Categorical and Continuous Features and Preprocessing - Python\nDESCRIPTION: This snippet defines the dependent variable ('salary'), categorical features, continuous features, and the preprocessing steps to be applied. `Categorify` converts categorical variables to numerical codes, `FillMissing` handles missing values, and `Normalize` scales the numerical features. These steps are essential for preparing the data for the model.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-tabular.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndep_var = 'salary'\ncat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\ncont_names = ['age', 'fnlwgt', 'education-num']\nprocs = [Categorify, FillMissing, Normalize]\n```\n\n----------------------------------------\n\nTITLE: Testing DataLoader with to_device and pop\nDESCRIPTION: This code tests `DataLoader` with `after_batch` and the ability to delegate unknown attributes to the underlying dataset. The code initializes the custom class A with a tensor, passes it to the `DataLoader` and verifies that the `to_device` function operates correctly and that `pop()` also works.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nclass A(TensorBase): pass\nt = A(tensor(1,2))\n\ntdl = DataLoader([t,t,t,t,t,t,t,t], bs=4, num_workers=2, after_batch=to_device)\nb = first(tdl)\ntest_eq(type(b), A)\n\n# Unknown attributes are delegated to `dataset`\ntest_eq(tdl.pop(), tensor(1,2))\n```\n\n----------------------------------------\n\nTITLE: Testing DistributedDL with torch DataLoader - Python\nDESCRIPTION: Wraps a standard torch.utils.data.DataLoader with DistributedDL for partitioned distributed loading. Uses test_eq to assert partitioned output matches expectations. Designed for debugging or verifying custom distributed dataloader implementations. Dependencies: torch, torch.utils.data, fastai.DistributedDL, test_eq.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\ndl = torch.utils.data.DataLoader(list(range(50)), batch_size=12, num_workers=2)\nfor i in range(4):\n    dl1 = DistributedDL(dl, i, 4)\n    test_eq(list(dl1), (torch.arange(i*13, i*13+12)%50,torch.tensor([i*13+12])%50))\n```\n\n----------------------------------------\n\nTITLE: Transposing Batches to Samples for Nested Tensor Structures in Python\nDESCRIPTION: Transforms a batch 'b' containing tensors or nested collections by 'transposing' it into a list of samples up to a maximum specified number. It handles simple tensors or recursively processes lists of tensors to return samples retaining original types and nested structure, enabling easier item-wise processing of batch data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_79\n\nLANGUAGE: python\nCODE:\n```\ndef batch_to_samples(b, max_n=10):\n    \"'Transposes' a batch to (at most `max_n`) samples\"\n    if isinstance(b, Tensor): return retain_types(list(b[:max_n]), [b])\n    else:\n        res = L(b).map(partial(batch_to_samples,max_n=max_n))\n        return retain_types(res.zip(), [b])\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face Hub from a Notebook (Python)\nDESCRIPTION: Python code demonstrating how to log into the Hugging Face Hub interactively within a Python notebook using the `notebook_login` function from the `huggingface_hub` library. This function prompts the user for their Hugging Face API token.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/74_huggingface.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\n----------------------------------------\n\nTITLE: Clearing Metrics for Inference - Python\nDESCRIPTION: Removes any evaluation metrics from the learner object. This is often done before performing inference to focus solely on prediction generation without calculating metrics.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nlearn.metrics=[]\n```\n\n----------------------------------------\n\nTITLE: Skipping Training Epochs with a Specialized Fastai Callback (Python)\nDESCRIPTION: Defines the 'SkipToEpoch' callback that halts training for all epochs before a specified index by raising a CancelEpochException. Used for resuming training from a given epoch. Requires integration with the Fastai Callback system.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass SkipToEpoch(Callback):\n    \"Skip training up to `epoch`\"\n    order = 70\n    \n    def __init__(self, epoch:int):\n        self._skip_to = epoch\n\n    def before_epoch(self):\n        if self.epoch < self._skip_to:\n            raise CancelEpochException\n```\n\n----------------------------------------\n\nTITLE: Temporarily Replacing an Attribute using a Context Manager in Python\nDESCRIPTION: Implements 'replacing_yield', a generator-based context manager for temporarily replacing an attribute value on an object. Restores the original value after exit, even on exceptions. Requires use within a contextmanager-decorated function.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef replacing_yield(o, attr, val):\n    \"Context manager to temporarily replace an attribute\"\n    old = getattr(o,attr)\n    try:     yield setattr(o,attr,val)\n    finally: setattr(o,attr,old)\n```\n\n----------------------------------------\n\nTITLE: Defining and Testing Custom Training Callback Order in fastai Learner (Python)\nDESCRIPTION: Introduces 'TestTrainEvalCallback', a custom callback to assert proper training/validation state, event order, and batch counter values during training. It attaches the callback to a synthetic Learner and runs a fit operation, also reversing the callback list to test event order flexibility. Requires Callback base class, TrainEvalCallback, and associated Learner machinery.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nclass TestTrainEvalCallback(Callback):\n    run_after,run_valid = TrainEvalCallback,False\n    def before_fit(self): \n        test_eq([self.pct_train,self.train_iter], [0., 0])\n        self.old_pct_train,self.old_train_iter = self.pct_train,self.train_iter\n    \n    def before_batch(self): test_eq(next(self.parameters()).device, find_device(self.xb))\n    \n    def after_batch(self):\n        assert self.training\n        test_eq(self.pct_train , self.old_pct_train+1/(self.n_iter*self.n_epoch))\n        test_eq(self.train_iter, self.old_train_iter+1)\n        self.old_pct_train,self.old_train_iter = self.pct_train,self.train_iter\n    \n    def before_train(self):\n        assert self.training and self.model.training\n        test_eq(self.pct_train, self.epoch/self.n_epoch)\n        self.old_pct_train = self.pct_train\n    \n    def before_validate(self):\n        assert not self.training and not self.model.training\n        \nlearn = synth_learner(cbs=TestTrainEvalCallback)\nlearn.fit(1)\n#Check order is properly taken into account\nlearn.cbs = L(reversed(learn.cbs))\n```\n\n----------------------------------------\n\nTITLE: Setting Default CUDA Usage Behavior in fastai\nDESCRIPTION: Sets the global default behavior for CUDA device usage within the fastai library using `defaults.use_cuda`. Setting it to `None` enables auto-detection based on availability, `True` strictly requires CUDA (raising an error if unavailable), and `False` forces the use of the CPU.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n# None: True if available; True: error if not available; False: use CPU\ndefaults.use_cuda = None\n```\n\n----------------------------------------\n\nTITLE: Sort movie ratings by bias (lowest)\nDESCRIPTION: Sorts the `movie_ratings` list by the bias values (the first element of each tuple) in ascending order and displays the first 15 movies. This shows the movies with the lowest learned biases.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nsorted(movie_ratings, key=item0)[:15]\n```\n\n----------------------------------------\n\nTITLE: Extracting and Cleaning Google Trends Data in Python/pandas\nDESCRIPTION: A set of pandas snippets to extract, transform, and clean state and date information from the Google Trends DataFrame, including splitting fields and handling country-wide trends separately. These snippets require pandas and an appropriately formatted googletrend file. Inputs include string columns to be split; outputs include added or remapped columns and row/column filtering for joining.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ngoogletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0]\ngoogletrend['State'] = googletrend.file.str.split('_', expand=True)[2]\ngoogletrend.head()\n```\n\nLANGUAGE: python\nCODE:\n```\nstore['State'].unique(),googletrend['State'].unique()\n```\n\nLANGUAGE: python\nCODE:\n```\ngoogletrend.loc[googletrend.State=='NI', \"State\"] = 'HB,NI'\n```\n\nLANGUAGE: python\nCODE:\n```\ntrend_de = googletrend[googletrend.file == 'Rossmann_DE'][['Date', 'trend']]\n```\n\nLANGUAGE: python\nCODE:\n```\ngoogletrend = join_df(googletrend, trend_de, 'Date', suffix='_DE')\n```\n\nLANGUAGE: python\nCODE:\n```\ngoogletrend.drop(columns=['file', 'week'], axis=1, inplace=True)\ngoogletrend = googletrend[~googletrend['State'].isnull()]\n```\n\n----------------------------------------\n\nTITLE: Pool Type-Based Adaptive Pooling Factory Function - Python\nDESCRIPTION: Provides a function mapping a string pool type to the appropriate adaptive pooling layer (Avg, Max, or Concat of both). Inputs: pool_type (string: 'Avg', 'Max', 'Cat'); output: pooling layer class. Used to abstract over pooling behavior in composite modules.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef adaptive_pool(pool_type):\n    return nn.AdaptiveAvgPool2d if pool_type=='Avg' else nn.AdaptiveMaxPool2d if pool_type=='Max' else AdaptiveConcatPool2d\n```\n\n----------------------------------------\n\nTITLE: Updating local branch with upstream changes\nDESCRIPTION: Synchronizes the local fork with the upstream main repository's master branch, ensuring the fork is up-to-date before further development.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/dev-setup.ipynb#_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\ngit pull upstream master\n```\n\nLANGUAGE: Shell\nCODE:\n```\ngit checkout master\n```\n\n----------------------------------------\n\nTITLE: Creating a new branch for pull request\nDESCRIPTION: Creates and switches to a new Git branch for development or PRs, enabling isolated changes without affecting the main branch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/dev-setup.ipynb#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\ngit checkout -b test-pr\n```\n\n----------------------------------------\n\nTITLE: Testing Model Instantiation and Forward Passes for Various XResNet Variants in Python\nDESCRIPTION: Several test snippets instantiate different XResNet models with various configurations such as 2D, 1D, and 3D inputs, different input channel counts, kernel sizes, and strides. They create random tensors matching input shapes and pass them through the models to verify forward pass behavior. This serves as example usage and basic sanity checks for the defined architectures.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/11_vision.models.xresnet.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntst = xse_resnext18()\nx = torch.randn(64, 3, 128, 128)\ny = tst(x)\n```\n\nLANGUAGE: python\nCODE:\n```\ntst = xresnext18()\nx = torch.randn(64, 3, 128, 128)\ny = tst(x)\n```\n\nLANGUAGE: python\nCODE:\n```\ntst = xse_resnet50()\nx = torch.randn(8, 3, 64, 64)\ny = tst(x)\n```\n\nLANGUAGE: python\nCODE:\n```\ntst = xresnet18(ndim=1, c_in=1, ks=15)\nx = torch.randn(64, 1, 128)\ny = tst(x)\n```\n\nLANGUAGE: python\nCODE:\n```\ntst = xresnext50(ndim=1, c_in=2, ks=31, stride=4)\nx = torch.randn(8, 2, 128)\ny = tst(x)\n```\n\nLANGUAGE: python\nCODE:\n```\ntst = xresnet18(ndim=3, c_in=3, ks=3)\nx = torch.randn(8, 3, 32, 32, 32)\ny = tst(x)\n```\n\n----------------------------------------\n\nTITLE: Show Batch of Language Model Data\nDESCRIPTION: Displays a batch of data from the `DataLoaders` object.  This is useful for understanding the structure of the batches and verifying the data pipeline.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndbunch_lm.show_batch()\n```\n\n----------------------------------------\n\nTITLE: Importing inspection utilities in fastai\nDESCRIPTION: Imports the isfunction and ismethod functions from the inspect module, which are used to identify function and method objects.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom inspect import isfunction,ismethod\n```\n\n----------------------------------------\n\nTITLE: Plotting Recorded Loss - fastai/Python\nDESCRIPTION: Plots the training and validation losses recorded by the `Learner` during training. This helps visualize the training progress and check for overfitting.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nlearn.recorder.plot_loss()\n```\n\n----------------------------------------\n\nTITLE: Cloning and installing fastcore repository\nDESCRIPTION: Clones the fastcore repository from GitHub, installs it in editable mode so changes are reflected immediately, and forks the repository for contribution.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/dev-setup.ipynb#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\ngh repo clone fastai/fastcore\n```\n\nLANGUAGE: Shell\nCODE:\n```\ncd fastcore\n```\n\nLANGUAGE: Shell\nCODE:\n```\npip install -qe .\n```\n\nLANGUAGE: Shell\nCODE:\n```\ngh repo fork --remote\n```\n\n----------------------------------------\n\nTITLE: Installing nbdev hooks for fastcore\nDESCRIPTION: Sets up nbdev hooks in the fastcore repository to ensure notebooks are clean, trusted, and exported correctly when pushing to GitHub.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/dev-setup.ipynb#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nnbdev_install_hooks\n```\n\n----------------------------------------\n\nTITLE: Importing Utility Modules\nDESCRIPTION: Imports the 'importlib' module, which is used for dynamic importing of modules based on their string names, facilitating flexible URL mappings.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/99_pytorch_doc.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport importlib\n```\n\n----------------------------------------\n\nTITLE: Importing Showdoc\nDESCRIPTION: This code imports `showdoc` from the `nbdev.showdoc` module, which is used for displaying documentation within Jupyter notebooks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Import fastai modules\nDESCRIPTION: This code imports necessary modules from the fastai library for basic functionalities, callback management, and text processing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/35_tutorial.wikitext.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.basics import *\nfrom fastai.callback.all import *\nfrom fastai.text.all import *\n```\n\n----------------------------------------\n\nTITLE: Assigning Predictions to Test DataFrame - Python\nDESCRIPTION: Assigns the exponentiated predictions back to the 'Sales' column in the test DataFrame, replacing any existing values.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\ntest_df[\"Sales\"]=np.exp(tst_preds.numpy()).T[0]\n```\n\n----------------------------------------\n\nTITLE: Getting Image Files\nDESCRIPTION: The snippet gets the image files from the dataset using `get_image_files`.  It takes the path of the dataset as input and returns a list of paths to image files.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nitems = get_image_files(path)\n```\n\n----------------------------------------\n\nTITLE: Documentation for projector_word_embeddings\nDESCRIPTION: Provides documentation for exporting word embeddings from language models, supporting models from fastai and transformers, with options for vocab and limit.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nshow_doc(projector_word_embeddings)\n```\n\n----------------------------------------\n\nTITLE: Retrieve Embedding Layer from Model\nDESCRIPTION: Fetches the embedding weights or a specified layer from a model, defaulting to the encoder if no layer is provided.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n def _get_embeddings(model, layer):\n    layer = model[0].encoder if layer == None else layer\n    return layer.weight\n```\n\n----------------------------------------\n\nTITLE: Setting Neptune API Token Environment Variable\nDESCRIPTION: Shell command to export the Neptune API token as an environment variable. This is required for the Neptune client library to authenticate.\nSOURCE: https://github.com/fastai/fastai/blob/main/disabled/70b_callback.neptune.ipynb#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nexport NEPTUNE_API_TOKEN='YOUR_LONG_API_TOKEN'\n```\n\n----------------------------------------\n\nTITLE: Access Text Data\nDESCRIPTION: Accesses and prints the text of the second row in the 'text' column of the DataFrame. This helps to understand the content of the text data.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf['text'][1]\n```\n\n----------------------------------------\n\nTITLE: Exporting Constants and Types (Python)\nDESCRIPTION: Defines the `_all_` variable to control which names are exported when the module is imported using `from ... import *`. This snippet specifically exports the `BILINEAR` and `NEAREST` constants obtained from PIL and later exports `Image` and `ToTensor`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|export\n_all_ = ['BILINEAR','NEAREST']\n```\n\nLANGUAGE: python\nCODE:\n```\n#|export\n_all_ = ['Image','ToTensor']\n```\n\n----------------------------------------\n\nTITLE: Importing base and callback modules in fastai\nDESCRIPTION: Python code for importing core fastai modules to access basic functionalities and callback class, which are essential for training and callback customization.\nSOURCE: https://github.com/fastai/fastai/blob/main/disabled/74_callback.azureml.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom fastai.basics import *\nfrom fastai.learner import Callback\n```\n\n----------------------------------------\n\nTITLE: SigmoidRange Layer for Custom Output Bounds - Python\nDESCRIPTION: Defines a Parametric layer with trainable (or fixed) 'low' and 'high' attributes, applies sigmoid_range in forward pass. Uses @module with 'low' and 'high' arguments. Inputs: tensor x; outputs: sigmoid-mapped tensor in (low, high) range. Enhances activations for regression tasks where target range is constrained.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@module('low','high')\ndef SigmoidRange(self, x):\n    \"Sigmoid module with range `(low, high)`\"\n    return sigmoid_range(x, self.low, self.high)\n```\n\n----------------------------------------\n\nTITLE: Displaying Test Set Date Range - Python\nDESCRIPTION: Shows the minimum and maximum dates present in the test dataset, important for defining a time-based validation split.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ntest_df['Date'].min(), test_df['Date'].max()\n```\n\n----------------------------------------\n\nTITLE: Testing DataLoader with custom TensorBase class\nDESCRIPTION: This code tests `DataLoader` with a custom `TensorBase` class and multiple workers, verifying that the type of the output batches is preserved. The code initializes a custom `TensorBase` class `A` and a `DataLoader` with this class type, with several workers to test type preservation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nclass A(TensorBase): pass\n\nfor nw in (0,2):\n    t = A(tensor([1,2]))\n    dl = DataLoader([t,t,t,t,t,t,t,t], bs=4, num_workers=nw)\n    b = first(dl)\n    test_eq(type(b), A)\n\n    t = (A(tensor([1,2])),)\n    dl = DataLoader([t,t,t,t,t,t,t,t], bs=4, num_workers=nw)\n    b = first(dl)\n    test_eq(type(b[0]), A)\n```\n\n----------------------------------------\n\nTITLE: Deleting local feature branch after PR closure\nDESCRIPTION: Removes the local branch used for the PR after it has been merged or closed, maintaining branch hygiene.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/dev-setup.ipynb#_snippet_12\n\nLANGUAGE: Shell\nCODE:\n```\ngit branch -d test-pr\n```\n\n----------------------------------------\n\nTITLE: Defining Sample Continuous Variables - Python\nDESCRIPTION: Defines a list of column names representing continuous variables chosen for the small sample experiment.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsmall_cont_vars = ['CompetitionDistance', 'Mean_Humidity']\n```\n\n----------------------------------------\n\nTITLE: Displaying Single Prediction Output (fastai tabular)\nDESCRIPTION: Displays the predicted class label and the probabilities associated with each class for the single row prediction performed in the previous step.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nclas, probs\n```\n\n----------------------------------------\n\nTITLE: Testing PixelShuffle_ICNR Layer (norm_type=Spectral)\nDESCRIPTION: Tests `PixelShuffle_ICNR` layer, with `norm_type` set to Spectral. This version validates that spectral normalization works correctly with the pixel shuffle module. Checks output shape and validates ICNR initialization properties. Dependencies: `torch`, `fastai`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_55\n\nLANGUAGE: Python\nCODE:\n```\npsfl = PixelShuffle_ICNR(16, norm_type=NormType.Spectral)\nx = torch.randn(64, 16, 8, 8)\ny = psfl(x)\ntest_eq(y.shape, [64, 16, 16, 16])\n#ICNR init makes every 2x2 window (stride 2) have the same elements\nfor i in range(0,16,2):\n    for j in range(0,16,2):\n        test_eq(y[:,:,i,j],y[:,:,i+1,j])\n        test_eq(y[:,:,i,j],y[:,:,i  ,j+1])\n        test_eq(y[:,:,i,j],y[:,:,i+1,j+1])\n```\n\n----------------------------------------\n\nTITLE: Displaying Full DataFrame Lengths - Python\nDESCRIPTION: Prints the number of rows in both the full training and test DataFrames.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nlen(train_df),len(test_df)\n```\n\n----------------------------------------\n\nTITLE: WeightDropout Test - PyTorch\nDESCRIPTION: This tests the `WeightDropout` module. It initializes an LSTM module and wraps it with `WeightDropout`. It tests forward pass with input and hidden state, calculates the loss, and backpropagates. It validates the original and the modified weights' properties. It also asserts that the raw weights' gradients exist and are not None. Finally, it checks if the dropped weights are set to zero within an expected range.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nmodule = nn.LSTM(5,7)\ndp_module = WeightDropout(module, 0.4)\nwgts = dp_module.module.weight_hh_l0\ntst_inp = torch.randn(10,20,5)\nh = torch.zeros(1,20,7), torch.zeros(1,20,7)\ndp_module.reset()\nx,h = dp_module(tst_inp,h)\nloss = x.sum()\nloss.backward()\nnew_wgts = getattr(dp_module.module, 'weight_hh_l0')\ntest_eq(wgts, getattr(dp_module, 'weight_hh_l0_raw'))\nassert 0.2 <= (new_wgts==0).sum().float()/new_wgts.numel() <= 0.6\nassert dp_module.weight_hh_l0_raw.requires_grad\nassert dp_module.weight_hh_l0_raw.grad is not None\nassert ((dp_module.weight_hh_l0_raw.grad == 0.) & (new_wgts == 0.)).any()\n```\n\n----------------------------------------\n\nTITLE: Testing Gradient Accumulation with Mixed Precision and CUDA\nDESCRIPTION: Tests the gradient accumulation implementation under mixed precision training and GPU acceleration, comparing gradients and losses to ensure consistency.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18a_callback.training.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n#|cuda\nfp16_cb = MixedPrecision(init_scale=1024)\ntrain1,valid1,grads1 = _test_acc(8,1, fp16_cb, cuda=True)\ntrain2,valid2,grads2 = _test_acc(1,8, [acc_cb,fp16_cb], cuda=True)\ntest_close(grads2,grads1, eps=0.01)\ntest_close(valid2,valid1)\ntest_ne(train2, train1)\n```\n\n----------------------------------------\n\nTITLE: Documentation for WandbCallback Class and Utility Functions\nDESCRIPTION: Provides explanations for the WandbCallback class methods and utility functions, including how they log model information, metrics, predictions, and datasets to wandb during fastai training. Emphasizes customization options such as dataset logging, model checkpoints, prediction sampling, and configuration gathering for reproducibility.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n#|export\n@patch\ndef gather_args(self:Learner):\n    \"Gather config parameters accessible to the learner\"\n    # args stored by `store_attr`\n    cb_args = {f'{cb}':getattr(cb,'__stored_args__',True) for cb in self.cbs}\n    args = {'Learner':self, **cb_args}\n    # input dimensions\n    try:\n        n_inp = self.dls.train.n_inp\n        args['n_inp'] = n_inp\n        xb = self.dls.valid.one_batch()[:n_inp]\n        args.update({f'input {n+1} dim {i+1}':d for n in range(n_inp) for i,d in enumerate(list(detuplify(xb[n]).shape))})\n    except: print(f'Could not gather input dimensions')\n    # other useful information\n    with ignore_exceptions():\n        args['batch size'] = self.dls.bs\n        args['batch per epoch'] = len(self.dls.train)\n        args['model parameters'] = total_params(self.model)[0]\n        args['device'] = self.dls.device.type\n        args['frozen'] = bool(self.opt.frozen_idx)\n        args['frozen idx'] = self.opt.frozen_idx\n        args['dataset.tfms'] = f'{self.dls.dataset.tfms}'\n        args['dls.after_item'] = f'{self.dls.after_item}'\n        args['dls.before_batch'] = f'{self.dls.before_batch}'\n\n```\n\n----------------------------------------\n\nTITLE: Loading Ratings Data\nDESCRIPTION: This code reads the ratings data from the 'u.data' file. It specifies the tab delimiter and the columns to keep, along with their names (user, movie, rating). It uses `pd.read_csv` to read the file into a pandas DataFrame.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n                      usecols=(0,1,2), names=['user','movie','rating'])\n```\n\n----------------------------------------\n\nTITLE: Customizing Backward Hooks via HookCallback Subclass (Python)\nDESCRIPTION: Implements an advanced TstCallback subclass which attaches backward hooks (is_forward=False) and checks that stored gradients match computed values after each training batch. Built for debugging and validating backward pass mechanics. Requires previously defined HookCallback, test_eq, and synth_learner, with PyTorch modules capable of gradients.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\nclass TstCallback(HookCallback):\n    def __init__(self, modules=None, remove_end=True, detach=True, cpu=False):\n        super().__init__(modules, None, remove_end, False, detach, cpu)\n    def hook(self, m, i, o): return o\n    def after_batch(self):\n        if self.training:\n            test_eq(self.hooks.stored[0][0], 2*(self.pred-self.y)/self.pred.shape[0])\n        \nlearn = synth_learner(n_trn=5, cbs = TstCallback())\nlearn.fit(1)\n```\n\n----------------------------------------\n\nTITLE: Creating Synthetic Correct Model Output in PyTorch\nDESCRIPTION: Constructs a synthetic PyTorch tensor `correct_model_output` representing a multi-class model prediction where the prediction is perfectly correct for the corresponding `target`. This tensor is used to test the expected behavior of a loss function when the model is ideal. Requires PyTorch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ncorrect_model_output = torch.zeros(3, 100,100)\ncorrect_model_output[0,:,:] = 100\ncorrect_model_output[0,:,5] = 0\ncorrect_model_output[0,:,50] = 0\ncorrect_model_output[1,:,5] = 100\ncorrect_model_output[2,:,50] = 100\ncorrect_model_output = torch.unsqueeze(correct_model_output, 0)\n```\n\n----------------------------------------\n\nTITLE: Viewing Head of Sample DataFrame - Python\nDESCRIPTION: Displays the first few rows of the small experimental DataFrame.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsmall_df.head()\n```\n\n----------------------------------------\n\nTITLE: Setting up Matplotlib Inline Plotting in Python\nDESCRIPTION: Enables inline plotting for Jupyter notebooks using Matplotlib. This is a prerequisite for visualizing training outputs and images within the notebook environment.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-wgan.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Testing Normalization with TabularPandas - Python\nDESCRIPTION: This code tests the `Normalize` transform with `TabularPandas`. It creates a DataFrame, applies normalization using `TabularPandas`, and checks if the calculated means, standard deviations, and normalized values are correct.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nnorm = Normalize()\ndf = pd.DataFrame({'a':[0,1,2,3,4]})\nto = TabularPandas(df, norm, cont_names='a')\nx = np.array([0,1,2,3,4])\nm,s = x.mean(),x.std()\ntest_eq(norm.means['a'], m)\ntest_close(norm.stds['a'], s)\ntest_close(to['a'].values, (x-m)/s)\n```\n\n----------------------------------------\n\nTITLE: Load saved model\nDESCRIPTION: Loads a previously saved model named 'dotprod'. This allows the model to be used without retraining.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nlearn.load('dotprod');\n```\n\n----------------------------------------\n\nTITLE: Inspecting Datasets\nDESCRIPTION: This allows for inspecting the created `Datasets` object, returning information about the Datasets.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndsets\n```\n\n----------------------------------------\n\nTITLE: Save Classifier (First Layer Group)\nDESCRIPTION: Saves the trained model after the first training stage.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nlearn.save('first')\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Image Getter Functions\nDESCRIPTION: Function factory that creates a partial function specifically for image file retrieval with predefined parameters.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef ImageGetter(suf='', recurse=True, folders=None):\n    \"Create `get_image_files` partial that searches suffix `suf` and passes along `kwargs`, only in `folders`, if specified\"\n    def _inner(o, recurse=recurse, folders=folders): return get_image_files(o/suf, recurse, folders)\n    return _inner\n```\n\n----------------------------------------\n\nTITLE: Testing Loss Functions in Half Precision on CUDA - Fastai/PyTorch\nDESCRIPTION: This snippet tests the `BCELossFlat` and `MSELossFlat` functions for compatibility with half precision tensors when a CUDA-enabled GPU is available. It performs forward passes with `.half().cuda()` tensors.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#|cuda\n#Test losses work in half precision\nif torch.cuda.is_available():\n    output = torch.sigmoid(torch.randn(32, 5, 10)).half().cuda()\n    target = torch.randint(0,2,(32, 5, 10)).half().cuda()\n    for tst in [BCELossFlat(), MSELossFlat()]: _ = tst(output, target)\n```\n\n----------------------------------------\n\nTITLE: Export module using nbdev\nDESCRIPTION: This code snippet exports the current module using the nbdev library.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/41_tabular.data.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Show a batch of data\nDESCRIPTION: This line of code displays a batch of data from the `TabularDataLoaders` object.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/41_tabular.data.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndls.show_batch()\n```\n\n----------------------------------------\n\nTITLE: Showing Classification Results\nDESCRIPTION: This uses `show_results` on the trained `Learner` to visualize the model's performance on a batch of validation data. It is used to evaluate the classification accuracy of the model and understand how well the model generalizes to new data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/38_tutorial.text.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlearn.show_results()\n```\n\n----------------------------------------\n\nTITLE: Predict Sentiment\nDESCRIPTION: Predicts the sentiment of a given text using the trained classifier.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nlearn.predict(\"I really loved that movie , it was awesome !\")\n```\n\n----------------------------------------\n\nTITLE: Documenting HookCallback's after_fit Method (Python)\nDESCRIPTION: Utilizes show_doc to exhibit documentation for the HookCallback.after_fit method, which manages post-training cleanup, e.g., removing hooks. Requires fastai's show_doc utility and the HookCallback class from previous examples.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\nshow_doc(HookCallback.after_fit)\n```\n\n----------------------------------------\n\nTITLE: Dataloader Show Batch Example\nDESCRIPTION: This shows a batch of data from a dataloader.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndl.show_batch()\n```\n\n----------------------------------------\n\nTITLE: Accessing the Validation Dataset\nDESCRIPTION: This accesses the validation dataset from the `dsets` object.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ndsets.valid\n```\n\n----------------------------------------\n\nTITLE: Load Classifier (Third Layer Group)\nDESCRIPTION: Loads the saved model from the third training stage.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nlearn.load('third');\n```\n\n----------------------------------------\n\nTITLE: Exporting Core Events\nDESCRIPTION: Defines and exports the events that a Callback can respond to in the training loop.  These are a set of strings representing the possible events.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#|export\n_all_ = ['CancelStepException','CancelBackwardException','CancelFitException','CancelEpochException','CancelTrainException','CancelValidException','CancelBatchException']\n```\n\n----------------------------------------\n\nTITLE: Testing PixelShuffle_ICNR Layer (norm_type=None)\nDESCRIPTION: Tests the `PixelShuffle_ICNR` layer with `norm_type` set to None.  It checks the output shape and, similarly to the previous test, validates the ICNR initialization. Dependencies: `torch`, `fastai`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_54\n\nLANGUAGE: Python\nCODE:\n```\npsfl = PixelShuffle_ICNR(16, norm_type=None)\nx = torch.randn(64, 16, 8, 8)\ny = psfl(x)\ntest_eq(y.shape, [64, 16, 16, 16])\n#ICNR init makes every 2x2 window (stride 2) have the same elements\nfor i in range(0,16,2):\n    for j in range(0,16,2):\n        test_eq(y[:,:,i,j],y[:,:,i+1,j])\n        test_eq(y[:,:,i,j],y[:,:,i  ,j+1])\n        test_eq(y[:,:,i,j],y[:,:,i+1,j+1])\n```\n\n----------------------------------------\n\nTITLE: Read Tokenized File\nDESCRIPTION: Defines a function to read a tokenized text file and split it into a list of tokens. This function will be used in the DataBlock to create the classification dataset.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef read_tokenized_file(f): return L(f.read_text().split(' '))\n```\n\n----------------------------------------\n\nTITLE: Configuring accelerate Automatically (Python)\nDESCRIPTION: Shows how to use `accelerate.utils.write_basic_config` to automatically create a basic accelerate configuration file. This function checks `torch.cuda.device_count`. The surrounding text notes that this only needs to be run once and requires a notebook restart afterward. The example code is commented out.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20b_tutorial.distributed.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n#from accelerate.utils import write_basic_config\n#write_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Pandas Helper Function for Table Joins with Suffix Handling (Python)\nDESCRIPTION: This snippet defines a reusable Python function that uses pandas merge to join two DataFrames based on specified keys, handling optional suffix arguments for overlapping columns. Required dependencies are the pandas library and properly structured DataFrames. Inputs include two DataFrames and merge key names; output is a merged DataFrame, preserving left-hand rows and attaching right-hand columns as needed.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/rossman_data_clean.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef join_df(left, right, left_on, right_on=None, suffix='_y'):\n    if right_on is None: right_on = left_on\n    return left.merge(right, how='left', left_on=left_on, right_on=right_on, \n                      suffixes=(\"\", suffix))\n```\n\n----------------------------------------\n\nTITLE: Import fastai libraries for collaborative filtering\nDESCRIPTION: Imports necessary modules from the fastai library for collaborative filtering and tabular data processing. It includes modules for creating collaborative data loaders and building collaborative filtering models.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\n```\n\n----------------------------------------\n\nTITLE: Testing Recorder with NumPy-Based Metric Output in fastai (Python)\nDESCRIPTION: This snippet tests Recorder's compatibility with metric functions returning NumPy arrays, ensuring proper logging and aggregation. It defines a test metric that returns a NumPy float via .numpy() on a torch Tensor and creates a Learner using it, running a fit to confirm no errors and correct handling. Dependencies: fastai Learner, Recorder, torch.nn.functional as F, and NumPy.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_72\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#Test numpy metric\ndef tst_metric_np(out, targ): return F.mse_loss(out, targ).numpy()\nlearn = synth_learner(n_train=5, metrics=tst_metric_np)\nlearn.fit(1)\n```\n\n----------------------------------------\n\nTITLE: Add partial_dataloaders to FilteredBase in fastai Python\nDESCRIPTION: Patches the `FilteredBase` class (the base class for `Datasets` and `DataBlock`) to add a `partial_dataloaders` method. This allows creating `PartialDL` instances conveniently from `Datasets` or `DataBlock` objects by specifying the desired number of items (`partial_n`) to use per epoch for the training split.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14a_callback.data.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\n@delegates(Datasets.dataloaders)\ndef partial_dataloaders(self:FilteredBase, partial_n, bs=64, **kwargs):\n    \"Create a partial dataloader `PartialDL` for the training set\"\n    xtra_kwargs = [{}] * (self.n_subsets-1)\n    return self.dataloaders(bs=bs, dl_type=PartialDL, dl_kwargs=({'partial_n':partial_n}, *xtra_kwargs), **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Save Fine-tuned Language Model (Full)\nDESCRIPTION: Saves the fully fine-tuned language model.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-imdb.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nlearn.save('fine_tuned')\n```\n\n----------------------------------------\n\nTITLE: Importing Nbdev Documentation Utilities (Python)\nDESCRIPTION: This snippet imports documentation utility functions from nbdev, specifically nbdev.showdoc. This enables the use of automatic documentation generation and display within notebooks and rendered docs. The dependency is nbdev, and there are no input parameters or direct outputs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01a_losses.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Extending Tabular Wrapper with Transform Method in Python\nDESCRIPTION: The TabularPandas subclass extends Tabular by adding a transform method that applies a function to specified columns. Optionally, the transformation respects whether all columns should be considered or only those present in the DataFrame. This enables in-place transformation for feature engineering or preprocessing stages in a tabular ML workflow. It assumes columns are present and the function is applied via pandas DataFrame.transform.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nclass TabularPandas(Tabular):\n    \"A `Tabular` object with transforms\"\n    def transform(self, cols, f, all_col=True):\n        if not all_col: cols = [c for c in cols if c in self.items.columns]\n        if len(cols) > 0: self[cols] = self[cols].transform(f)\n```\n\n----------------------------------------\n\nTITLE: Resize Image Function - Test\nDESCRIPTION: This code tests the `resize_image` function. It defines a filename and a destination path, calls the function to resize an image (assumed to be in an 'images' directory relative to the current working directory), and then verifies that the image has been resized correctly. The tests check to ensure the image has a certain shape after the resizing operation. The file is then unlinked after testing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09b_vision.utils.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nfile = 'puppy.jpg'\ndest = Path('.')\nresize_image(file, dest, src='images', max_size=400)\nim = Image.open(dest/file)\ntest_eq(im.shape[1],400)\n(dest/file).unlink()\n```\n\n----------------------------------------\n\nTITLE: Importing WeightedDL for Custom DistributedDL Usage - Python\nDESCRIPTION: Simple import statement for WeightedDL from the fastai.callback.data module, used for later demonstration/testing of WeightedDL and DistributedDL interoperability. Prerequisite: fastai installed.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nfrom fastai.callback.data import WeightedDL\n```\n\n----------------------------------------\n\nTITLE: Conditional Fastai Installation\nDESCRIPTION: This code snippet conditionally installs or upgrades the fastai library on Google Colab environments. It checks for the existence of the '/content' directory (a characteristic of Colab) before attempting the installation using pip.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#| eval: false\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Testing SelfAttention Layer (Initialization)\nDESCRIPTION: Tests the `SelfAttention` layer by creating an instance and feeding a random input tensor. It verifies that the output is unchanged initially due to the trainable parameter `gamma` being initialized to zero. Dependencies: `torch`, `fastai` (for Module, ConvLayer, etc.).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_43\n\nLANGUAGE: Python\nCODE:\n```\ntst = SelfAttention(16)\nx = torch.randn(32, 16, 8, 8)\ntest_eq(tst(x),x)\n```\n\n----------------------------------------\n\nTITLE: Defining NormType Enum\nDESCRIPTION: This code defines an enumeration called `NormType` using the `Enum` class. This enum is used to specify the type of normalization to be applied.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n#|export\nNormType = Enum('NormType', 'Batch BatchZero Weight Spectral Instance InstanceZero')\n```\n\n----------------------------------------\n\nTITLE: Installing fastai on Colab\nDESCRIPTION: This code snippet installs or upgrades the fastai library using pip. It first checks if a directory named `/content` exists, which indicates a Google Colab environment.  If it exists, it then uses `pip install -Uqq fastai` to upgrade fastai.  The `qq` flag reduces the output verbosity during installation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70d_callback.comet.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Test Learner.validate Method in Python\nDESCRIPTION: Tests the `validate` method of a fastai `Learner` without prior training. It creates a synthetic learner with a specific metric (`F.mse_loss`), runs validation, and captures the predictions and targets. This checks if the validation loop functions correctly even before any training occurs.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/16_callback.progress.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#Check validate works without any training\ndef tst_metric(out, targ): return F.mse_loss(out, targ)\nlearn = synth_learner(n_trn=5, metrics=tst_metric)\npreds,targs = learn.validate()\n```\n\n----------------------------------------\n\nTITLE: Defining Simple Delimiter Tokenizer - Python\nDESCRIPTION: This code defines a basic Python function `delim_tok` that takes a string `s` and an optional delimiter `delim` (defaulting to a space). It splits the string by the delimiter and returns a fastai `L` object containing the resulting tokens. It then demonstrates its usage on a sample string.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef delim_tok(s, delim=' '): return L(s.split(delim))\ns = ss[0]\ndelim_tok(s)\n```\n\n----------------------------------------\n\nTITLE: Mocking _broadcast Method for Testing\nDESCRIPTION: Creates a patched version of the _broadcast method that uses file-based communication instead of actual distributed communication, enabling testing of DistributedDL without a proper distributed setup.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n_tmp_file = tempfile.NamedTemporaryFile().name\n# patch _broadcast with a mocked version so we can test DistributedDL w/o a proper DDP setup\n@patch\ndef _broadcast(self:DistributedDL,t,rank):\n    t = LongTensor(t)\n    if rank == self.rank: torch.save(t,_tmp_file)\n    else:                 t.data = torch.load(_tmp_file)\n    return t.tolist()\n```\n\n----------------------------------------\n\nTITLE: Exporting Event Class\nDESCRIPTION: Exports the `event` class to make it accessible outside the module.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#|export\n_all_ = ['event']\n```\n\n----------------------------------------\n\nTITLE: Run System Command\nDESCRIPTION: Executes a shell command using the `run` function.  The `nvidia_smi` function captures the output from the command. It also handles `OSError` by returning `None` in case of failure (e.g., if nvidia-smi is not installed).  It relies on an external command-line utility.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/97_test_utils.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef nvidia_smi(cmd = \"nvidia-smi\"):\n    try: res = run(cmd)\n    except OSError as e: return None\n    return res\n```\n\n----------------------------------------\n\nTITLE: Dropout Mask Test - PyTorch\nDESCRIPTION: This code snippet tests the `dropout_mask` function. It creates a random tensor, calls `dropout_mask`, and asserts that the output tensor has the expected shape and that elements are either 0 or the scaled value. This ensures the dropout mask is generated correctly.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/32_text.models.awdlstm.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nt = dropout_mask(torch.randn(3,4), [4,3], 0.25)\ntest_eq(t.shape, [4,3])\nassert ((t == 4/3) + (t==0)).all()\n```\n\n----------------------------------------\n\nTITLE: Importing nbdev showdoc - Python\nDESCRIPTION: This imports the `showdoc` function from the `nbdev` library.  This likely facilitates documentation generation or display within the project. It is used internally for documenting fastai's components and does not have any explicit parameters or expected outputs in this context.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Creating Batches for Parallel Processing - Python\nDESCRIPTION: This code prepares the input text data (`ss`) by splitting it into smaller batches using `np.array_split`. It creates lists of batches for 32, 8, and 2 workers, respectively. Each batch is then converted to a fastai `L` object and its elements are mapped to strings, ensuring consistent data types for parallel processing.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nbatches32 = [L(list(o)).map(str) for o in np.array_split(ss, 32)]\nbatches8  = [L(list(o)).map(str) for o in np.array_split(ss, 8 )]\nbatches   = [L(list(o)).map(str) for o in np.array_split(ss, 2 )]\n```\n\n----------------------------------------\n\nTITLE: Testing DataLoader Performance with SleepyDL\nDESCRIPTION: This code tests the performance of `DataLoader` with `num_workers` > 0.  It introduces a `SleepyDL` class which simulates latency during data loading. It then tests the performance of `DataLoader` with varying numbers of workers (0, 2, and 4) and verifies that the behavior is as expected, checking the execution time of a dataloader with various number of workers.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nclass SleepyDL(list):\n    def __getitem__(self,i):\n        time.sleep(random.random()/50)\n        return super().__getitem__(i)\n\nt = SleepyDL(letters)\n\n%time test_eq(DataLoader(t, num_workers=0), letters)\n%time test_eq(DataLoader(t, num_workers=2), letters)\n%time test_eq(DataLoader(t, num_workers=4), letters)\n\ndl = DataLoader(t, shuffle=True, num_workers=1)\ntest_shuffled(L(dl), letters)\ntest_shuffled(L(dl), L(dl))\nL(dl)\n```\n\n----------------------------------------\n\nTITLE: Obtaining First Parameter from PyTorch Module in Python\nDESCRIPTION: Provides a helper function that returns the first parameter tensor of a PyTorch model or module. This is useful for accessing or inspecting the initial parameter, often weights of the first layer, directly from the model object.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_66\n\nLANGUAGE: python\nCODE:\n```\ndef one_param(m):\n    \"First parameter in `m`\"\n    return first(m.parameters())\n```\n\n----------------------------------------\n\nTITLE: Importing Required Dependencies for Distributed Training\nDESCRIPTION: Imports necessary modules from fastai and PyTorch required for distributed training, including parallel processing utilities. The code also handles conditional import of the Accelerate library for multi-GPU training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.basics import *\nfrom fastai.callback.progress import ProgressCallback\nfrom torch.nn.parallel import DistributedDataParallel, DataParallel\nfrom fastai.data.load import _FakeLoader,_loaders\nfrom fastai.optimizer import OptimWrapper\ntry: from accelerate import Accelerator\nexcept ModuleNotFoundError: pass\n```\n\n----------------------------------------\n\nTITLE: Creating and displaying an image\nDESCRIPTION: This snippet creates and shows a grayscale image from a file path using fastai's `PILImageBW.create` and `.show()` methods.  It takes an image file path as input and displays the image. It uses PIL for image manipulation.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nim = PILImageBW.create(items[0])\nim.show();\n```\n\n----------------------------------------\n\nTITLE: InterruptCallback Class\nDESCRIPTION: Defines an `InterruptCallback` class.  This is a custom callback that enables interrupting the training process at a specific epoch. It's intended to facilitate testing checkpointing by simulating an early termination of training.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nclass InterruptCallback(Callback):\n    def __init__(self, epoch):\n        self._interupt_before = epoch\n    def before_epoch(self):\n        if self.epoch == self._interupt_before:\n            raise CancelFitException\n```\n\n----------------------------------------\n\nTITLE: Displaying pandas DataFrames in Notebooks or Console in Python\nDESCRIPTION: Attempts to display a given pandas DataFrame using IPython's HTML display capabilities inside Jupyter notebooks. If IPython is not available, it falls back to printing the DataFrame to the console. This ensures appropriate visualization across different environments.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_64\n\nLANGUAGE: python\nCODE:\n```\ndef display_df(df):\n    \"Display `df` in a notebook or defaults to print\"\n    try: from IPython.display import display, HTML\n    except: return print(df)\n    display(HTML(df.to_html()))\n```\n\n----------------------------------------\n\nTITLE: Importing Model Feature Counting Utility from FastAI Callback Hooks in Python\nDESCRIPTION: Imports 'num_features_model' from 'fastai.callback.hook', a utility to count the number of features produced by a model. It is commonly used in Learner setup to automatically infer feature sizes for heads.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.callback.hook import num_features_model\n```\n\n----------------------------------------\n\nTITLE: Displaying Fastai Learner Class Documentation - Python\nDESCRIPTION: This snippet uses the fastai function `show_doc` to display the documentation associated with the `Learner` class. This function is commonly used in interactive environments like Jupyter notebooks to quickly view the documentation for a class or function that has been documented using `add_docs` or standard Python docstrings.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Learner)\n```\n\n----------------------------------------\n\nTITLE: Defining PILImageBW Class (Python)\nDESCRIPTION: Defines the `PILImageBW` class, inheriting from `PILImage`. This class is specifically for Black & White (grayscale) Pillow Images. It overrides `_show_args` to use a 'Greys' colormap and `_open_args` to default to 'L' mode when loading, intended for conversion to `TensorImageBW`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#|export\nclass PILImageBW(PILImage):\n    \"A BW Pillow `Image` that can show itself and converts to `TensorImageBW`\"\n    _show_args,_open_args = {'cmap':'Greys'},{'mode': 'L'}\n```\n\n----------------------------------------\n\nTITLE: Testing DataBlock defaults and transforms with MNIST\nDESCRIPTION: Tests the default transforms applied by the DataBlock to MNIST data, showing the type, item, and batch transforms automatically included.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntest_eq(mnist.type_tfms[0], [PILImageBW.create])\ntest_eq(mnist.type_tfms[1].map(type), [Categorize])\ntest_eq(mnist.default_item_tfms.map(type), [ToTensor])\ntest_eq(mnist.default_batch_tfms.map(type), [IntToFloatTensor])\n```\n\n----------------------------------------\n\nTITLE: Importing DataLoader Dependencies and Definitions - Python\nDESCRIPTION: Imports key modules needed for DataLoader customization, including future annotations, fastai basics, and detailed PyTorch DataLoader internals. The _loaders tuple enables referencing multiprocessing and single-processing DataLoader iterators for later enhancement.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.torch_basics import *\nfrom torch.utils.data.dataloader import _MultiProcessingDataLoaderIter,_SingleProcessDataLoaderIter,_DatasetKind\n_loaders = (_MultiProcessingDataLoaderIter,_SingleProcessDataLoaderIter)\n```\n\n----------------------------------------\n\nTITLE: Initializing Tensor Class Bindings for PIL Image Types - fastai (Python)\nDESCRIPTION: Assigns fastai tensor classes (TensorImage, TensorImageBW, TensorMask) to corresponding PIL image wrapper classes for seamless type conversion in later transforms. No external dependencies beyond fastai are required, but assumes PILImage, PILImageBW, and PILMask are defined and imported. Input is class types only, no function input/output.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\n#|export\nPILImage  ._tensor_cls = TensorImage\nPILImageBW._tensor_cls = TensorImageBW\nPILMask   ._tensor_cls = TensorMask\n```\n\n----------------------------------------\n\nTITLE: Importing fastai and Lightning Helper Modules in Python\nDESCRIPTION: Imports all components from the `migrating_lightning` helper module (assumed to contain the Lightning model definition and potentially data loading logic) and the `fastai.vision.all` module, which provides core fastai vision functionalities needed for subsequent steps.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/examples/migrating_lightning.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom migrating_lightning import *\n\nfrom fastai.vision.all import *\n```\n\n----------------------------------------\n\nTITLE: Testing DataBlock with invalid arguments\nDESCRIPTION: Tests that the DataBlock constructor correctly fails when passed invalid keyword arguments.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/06_data.block.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntest_fail(lambda: DataBlock(wrong_kwarg=42, wrong_kwarg2='foo'))\n```\n\n----------------------------------------\n\nTITLE: Importing fastai Text Modules - Python\nDESCRIPTION: This snippet imports all necessary components from the `fastai.text.all` module. This is a common practice in fastai notebooks to quickly access the entire text processing library.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.text.all import *\n```\n\n----------------------------------------\n\nTITLE: Running fastai Tests (Bash)\nDESCRIPTION: This command executes the test suite for the fastai library in parallel. It utilizes `nbdev_test`, part of the nbdev framework, and requires the installation of development dependencies for all tests to pass.\nSOURCE: https://github.com/fastai/fastai/blob/main/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnbdev_test\n```\n\n----------------------------------------\n\nTITLE: Testing TrackerCallback Functionality\nDESCRIPTION: This set of code snippets tests the functionality of the `TrackerCallback`. It uses `FakeRecords` to simulate metric values and `TestTracker` to record the `best` and `new_best` attributes of the `TrackerCallback` during training.  It tests scenarios with and without a `min_delta` and verifies that the `comp` parameter is correctly used for metrics.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nlearn = synth_learner(n_trn=2, cbs=TestTracker())\ncbs=[TrackerCallback(monitor='valid_loss'), FakeRecords('valid_loss', [0.2,0.1])]\nwith learn.no_logging(): learn.fit(2, cbs=cbs)\ntest_eq(learn.test_tracker.bests, [0.2, 0.1])\ntest_eq(learn.test_tracker.news,  [True,True])\n\n#With a min_delta\ncbs=[TrackerCallback(monitor='valid_loss', min_delta=0.15), FakeRecords('valid_loss', [0.2,0.1])]\nwith learn.no_logging(): learn.fit(2, cbs=cbs)\ntest_eq(learn.test_tracker.bests, [0.2, 0.2])\ntest_eq(learn.test_tracker.news,  [True,False])\n```\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#By default metrics have to be bigger at each epoch.\ndef tst_metric(out,targ): return F.mse_loss(out,targ)\nlearn = synth_learner(n_trn=2, cbs=TestTracker(), metrics=tst_metric)\ncbs=[TrackerCallback(monitor='tst_metric'), FakeRecords('tst_metric', [0.2,0.1])]\nwith learn.no_logging(): learn.fit(2, cbs=cbs)\ntest_eq(learn.test_tracker.bests, [0.2, 0.2])\ntest_eq(learn.test_tracker.news,  [True,False])\n\n#This can be overwritten by passing `comp=np.less`.\nlearn = synth_learner(n_trn=2, cbs=TestTracker(), metrics=tst_metric)\ncbs=[TrackerCallback(monitor='tst_metric', comp=np.less), FakeRecords('tst_metric', [0.2,0.1])]\nwith learn.no_logging(): learn.fit(2, cbs=cbs)\ntest_eq(learn.test_tracker.bests, [0.2, 0.1])\ntest_eq(learn.test_tracker.news,  [True,True])\n```\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#Setting reset_on_fit=True will maintain the \"best\" value over subsequent calls to fit\nlearn = synth_learner(n_val=2, cbs=TrackerCallback(monitor='tst_metric', reset_on_fit=False), metrics=tst_metric)\ntracker_cb = learn.cbs.filter(lambda cb: isinstance(cb, TrackerCallback))[0]\nwith learn.no_logging(): learn.fit(1)\nfirst_best = tracker_cb.best\nwith learn.no_logging(): learn.fit(1)\ntest_eq(tracker_cb.best, first_best)\n```\n\n----------------------------------------\n\nTITLE: Declaring FastAI callback for AzureML integration\nDESCRIPTION: Specifies the module export for the AzureMLCallback class, initializing it with parameters to enable logging to AzureML during model training. Sets the stage for logging metrics, model summaries, and training info directly to AzureML.\nSOURCE: https://github.com/fastai/fastai/blob/main/disabled/74_callback.azureml.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n#default_exp callback.azureml\n```\n\n----------------------------------------\n\nTITLE: Testing SelfAttention Layer (Output Validation)\nDESCRIPTION: Calculates the attention weights (`beta`) and output (`out`) from the self-attention mechanism, verifying the shapes and comparing the final output `y` to the expected result based on `x`, `beta`, and `h`. It uses `test_close` to handle potential floating-point differences.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_46\n\nLANGUAGE: Python\nCODE:\n```\nbeta = F.softmax(torch.bmm(f, g.transpose(1,2)), dim=1)\ntest_eq(beta.shape, [32, 64, 64])\nout = torch.bmm(h.transpose(1,2), beta)\ntest_eq(out.shape, [32, 16, 64])\ntest_close(y, x + out.view(32, 16, 8, 8), eps=1e-4)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Preprocessed Data (TabularPandas)\nDESCRIPTION: Displays the first two rows of the preprocessed feature data (`xs`) stored within the `TabularPandas` object. This shows the result of applying the defined preprocessing steps.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nto.xs.iloc[:2]\n```\n\n----------------------------------------\n\nTITLE: Testing DataLoader and multi-worker processing\nDESCRIPTION: This snippet tests multi-worker data loading with shuffling using `DataLoader`. The code loads a range of numbers into a `DataLoader` with a defined batch size, shuffling, and num_workers parameter and checks that it works as expected.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/02_data.load.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nlist(DataLoader(list(range(50)),bs=32,shuffle=True,num_workers=3))\n```\n\n----------------------------------------\n\nTITLE: Categorify Decode Example\nDESCRIPTION: This code tests the decode functionality of the `Categorify` processor. It takes a `Tabular` object with encoded categorical variables, decodes them back to their original values, and compares the result to the expected output.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#Test decode\nto2 = TabularPandas(to1.items.to_pandas(), None, 'a')\nto2 = cat.decode(to2)\ntest_eq(to2.a, np.array(['1','0','#na#','#na#','2']))\n```\n\n----------------------------------------\n\nTITLE: Importing Initialization Functions\nDESCRIPTION: This code imports several initialization functions from `torch.nn.init`: `kaiming_uniform_`, `uniform_`, `xavier_uniform_`, and `normal_`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom torch.nn.init import kaiming_uniform_,uniform_,xavier_uniform_,normal_\n```\n\n----------------------------------------\n\nTITLE: Accessing the Training Dataset\nDESCRIPTION: This code accesses the training dataset from the `dsets` object.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndsets.train\n```\n\n----------------------------------------\n\nTITLE: Testing PixelShuffle_ICNR Layer\nDESCRIPTION: Tests the `PixelShuffle_ICNR` layer with standard weight normalization and the default activation function, confirming the shape of the output tensor and verifying that ICNR initialization is correctly applied by ensuring that the expected values within each 2x2 window are equal. Dependencies: `torch`, `fastai`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_53\n\nLANGUAGE: Python\nCODE:\n```\npsfl = PixelShuffle_ICNR(16)\nx = torch.randn(64, 16, 8, 8)\ny = psfl(x)\ntest_eq(y.shape, [64, 16, 16, 16])\n#ICNR init makes every 2x2 window (stride 2) have the same elements\nfor i in range(0,16,2):\n    for j in range(0,16,2):\n        test_eq(y[:,:,i,j],y[:,:,i+1,j])\n        test_eq(y[:,:,i,j],y[:,:,i  ,j+1])\n        test_eq(y[:,:,i,j],y[:,:,i+1,j+1])\n```\n\n----------------------------------------\n\nTITLE: Verifying Training with Raw torch DataLoaders in fastai Learner (Python)\nDESCRIPTION: This snippet reproduces the prior loss-improvement test but disables fastai's transformed DataLoaders, working directly with raw PyTorch DataLoaders. It confirms that the training process remains valid under these settings, ensuring CPU state and proper use of iterators. Prerequisites include 'synth_learner' and appropriate fastai and PyTorch setup.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#Ensure we can train with raw torch\nlearn = synth_learner(lr=0.1, tfmdDL=False)\nlearn(_before_epoch)\nlearn.model = learn.model.cpu()\nxb,yb = next(iter(learn.dls[0]))\ninit_loss = learn.loss_func(learn.model(xb), yb)\nlearn.fit(10)\nxb,yb = next(iter(learn.dls[0]))\nlearn.model = learn.model.cpu() # Ensure we're still on CPU even in a CUDA environment\nfinal_loss = learn.loss_func(learn.model(xb), yb)\nassert final_loss < init_loss, (final_loss,init_loss)\n```\n\n----------------------------------------\n\nTITLE: ReadTabBatch Encodings\nDESCRIPTION: This defines the encodes method for the `ReadTabBatch` transform. It converts the categorical, continuous, and target variables to PyTorch tensors using DLPack for zero-copy transfer from cuDF to PyTorch.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom torch.utils.dlpack import from_dlpack\n\n@ReadTabBatch\ndef encodes(self, to: TabularGPU):\n    return from_dlpack(to.cats.to_dlpack()).long(),from_dlpack(to.conts.to_dlpack()).float(), from_dlpack(to.targ.to_dlpack()).long()\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Continuous Features - Python\nDESCRIPTION: Retrieves and displays the count of continuous variables present in the training dataset handled by the DataLoaders.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nlen(dls.train_ds.cont_names)\n```\n\n----------------------------------------\n\nTITLE: Pipeline Processing with Y Variable\nDESCRIPTION: This code extends the pipeline example to include a target variable (`y_names`). The target variable 'c' is categorized as part of the processing. This showcases how the tabular pipeline handles dependent variables along with independent variables.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n#Test apply on y_names\nprocs = [Normalize, Categorify, FillMissing, noop]\ndf = cudf.from_pandas(pd.DataFrame({'a':[0,1,2,1,1,2,0], 'b':[0,1,np.nan,1,2,3,4], 'c': ['b','a','b','a','a','b','a']}))\nto = TabularGPU(df, procs, cat_names='a', cont_names='b', y_names='c')\n\ntest_eq(to.cat_names, ['a', 'b_na'])\ntest_eq(to.a.to_array(), [1,2,3,2,2,3,1])\ntest_eq(to.b_na.to_array(), [1,1,2,1,1,1,1])\ntest_eq(to.c.to_array(), [1,0,1,0,0,1,0])\nx = np.array([0,1,1.5,1,2,3,4])\nm,s = x.mean(),x.std()\ntest_close(to.b.to_array(), (x-m)/s)\ntest_eq(to.procs.classes, {'a': ['#na#','0','1','2'], 'b_na': ['#na#','False','True'], 'c': ['a','b']})\n```\n\n----------------------------------------\n\nTITLE: Monkey-Patching `Tensor.set_meta` for Metadata Preservation in fastai\nDESCRIPTION: Adds a `set_meta` method to the PyTorch `Tensor` class via monkey-patching (`@patch`). This method allows copying the `__dict__` (which often holds custom metadata in fastai) from another object `x` to the tensor instance `self`. The `as_copy` parameter determines whether a shallow copy (`copy.copy`) or a direct reference assignment of the dictionary is performed.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\ndef set_meta(self:Tensor, x, as_copy=False):\n    \"Set all metadata in `__dict__`\"\n    if not hasattr(x,'__dict__'): return\n    # XXX: change to `deepcopy` once PyTorch 1.7.1 is out, and check nb 23 segmentation fit works\n    self.__dict__ = copy(x.__dict__) if as_copy else x.__dict__\n```\n\n----------------------------------------\n\nTITLE: Displaying the First Entry of the Validation DataFrame in Python (pandas)\nDESCRIPTION: Displays the first row of the validation DataFrame (`df_valid`) using the pandas `head()` method. This is likely done to inspect the format or content of the validation data, potentially to create a suitable prompt for text generation. Assumes `df_valid` is a pandas DataFrame object.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndf_valid.head(1)\n```\n\n----------------------------------------\n\nTITLE: Helper Functions for String Conversion and None Removal\nDESCRIPTION: These functions are helper methods used in categorical variable processing. `_to_str` converts cudf series dtype to string if it is object type else it casts it to string. `_remove_none` removes `None` from a list.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef _to_str(c): return c if c.dtype == \"object\" else c.astype(\"str\")\ndef _remove_none(c):\n    if None in c: c.remove(None)\n    return c\n```\n\n----------------------------------------\n\nTITLE: Inspecting Shape of Predictions - Python\nDESCRIPTION: Checks the shape (number of samples and classes) of the prediction tensor to ensure correct output size before submission formatting. Output: tuple of dimensions. No parameters; input is prediction tensor.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\npreds.shape\n```\n\n----------------------------------------\n\nTITLE: Defining a Safe Tensor Item Extractor with fastai in Python\nDESCRIPTION: Defines the helper function '_maybe_item' to safely extract a scalar value from a possibly tensor-like object. It first attempts to call .item(), falling back to returning the value unchanged if the call fails (useful for inputs not supporting .item()). No external dependencies are needed. Input is expected to be an object with a 'value' attribute.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_66\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef _maybe_item(t):\n    t = t.value\n    try: return t.item()\n    except: return t\n```\n\n----------------------------------------\n\nTITLE: Testing IntToFloatTensor Class in Python\nDESCRIPTION: Tests the IntToFloatTensor transformation by applying it to different tensor types and validating the outputs maintain correct types and values.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nt = (TensorImage(tensor(1)),tensor(2).long(),TensorMask(tensor(3)))\ntfm = IntToFloatTensor()\nft = tfm(t)\ntest_eq(ft, [1./255, 2, 3])\ntest_eq(type(ft[0]), TensorImage)\ntest_eq(type(ft[2]), TensorMask)\ntest_eq(ft[0].type(),'torch.FloatTensor')\ntest_eq(ft[1].type(),'torch.LongTensor')\ntest_eq(ft[2].type(),'torch.LongTensor')\n```\n\n----------------------------------------\n\nTITLE: Helper Function for File Filtering\nDESCRIPTION: Internal helper function that filters files based on provided extensions and excludes hidden files (starting with '.').\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef _get_files(p, fs, extensions=None):\n    p = Path(p)\n    res = [p/f for f in fs if not f.startswith('.')\n           and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)]\n    return res\n```\n\n----------------------------------------\n\nTITLE: Median Patch for cuDF Series\nDESCRIPTION: This code adds a `median` method to `cudf.Series`.  It calculates the median of the series, handling missing values by dropping them first.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\ndef median(self:cudf.Series):\n    \"Get the median of `self`\"\n    col = self.dropna().reset_index(drop=True).sort_values()\n    return col[len(col)//2] if len(col)%2 != 0 else (col[len(col)//2]+col[len(col)//2-1])/2\n```\n\n----------------------------------------\n\nTITLE: Generating Multiple Empty Rows of a pandas DataFrame in Python\nDESCRIPTION: Defines a utility function to create 'n' empty rows in a pandas DataFrame and returns a list of individual row Series objects. Useful for initializing or padding data structures within data workflows where empty rows are required.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_63\n\nLANGUAGE: python\nCODE:\n```\ndef get_empty_df(n):\n    \"Return `n` empty rows of a dataframe\"\n    df = pd.DataFrame(index = range(n))\n    return [df.iloc[i] for i in range(n)]\n```\n\n----------------------------------------\n\nTITLE: Removing DistributedTrainer from Learner with detach_distributed - Python\nDESCRIPTION: Removes the DistributedTrainer callback from a Learner instance, restoring single-process training or validation. May re-add ProgressCallback when exiting distributed mode, unless a specific Learner property exists. Intended for cleanup after distributed experiments. No outputs except modified Learner. Dependencies: fastai.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\n@patch\ndef detach_distributed(self: Learner):\n    \"Remove `DistributedTrainer` from a learner\"\n    if num_distrib() <=1: return self\n    self.remove_cb(DistributedTrainer)\n    if rank_distrib() and not hasattr(self, 'progress'): self.add_cb(ProgressCallback())\n    return self\n```\n\n----------------------------------------\n\nTITLE: Getting Local Path from Config\nDESCRIPTION: The `fastai_path` function takes a folder name as input and returns the local path of that folder within the fastai configuration.  It utilizes the `fastai_cfg` to determine the base path and then joins it with the specified folder.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/04_data.external.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#|export\ndef fastai_path(folder:str) -> Path: \n    \"Local path to `folder` in `Config`\"\n    return fastai_cfg().path(folder)\n```\n\n----------------------------------------\n\nTITLE: Testing `to_device` Functionality with CUDA in fastai\nDESCRIPTION: Contains unit tests using `test_eq_type` and `test_eq` specifically for scenarios where CUDA is available. These tests verify that the `to_device` function correctly moves tensors within a nested structure to the CUDA device and that the resulting tensors have the expected CUDA tensor type (e.g., `torch.cuda.LongTensor`).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n#|cuda\nif torch.cuda.is_available():\n    test_eq_type(t,(3,(tensor(3).cuda(),tensor(2).cuda())))\n    test_eq(t2.type(), \"torch.cuda.LongTensor\")\n    test_eq(t3.type(), \"torch.cuda.LongTensor\")\n```\n\n----------------------------------------\n\nTITLE: Testing BBox/Label Decoding Order in Reversed Detection Dataset Pipeline - fastai (Python)\nDESCRIPTION: Checks that the pipeline works correctly regardless of input order, by switching label and bounding box transforms for the Datasets and rerunning validation on DataLoader output. Ensures invariance to order. Requires Datasets, TfmdDL, test utilities, coco_fn, bbox, and MultiCategorize.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/07_vision.core.ipynb#_snippet_39\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\n#test other direction works too\ncoco_tds = Datasets([coco_fn], [PILImage.create, [_coco_lbl, MultiCategorize(add_na=True)], [_coco_bb]])\ncoco_tdl = TfmdDL(coco_tds, bs=1, after_item=[BBoxLabeler(), PointScaler(), ToTensor()])\n\nx,y,z = coco_tdl.one_batch()\ntest_close(z[0], -1+tensor(bbox[0])/64)\ntest_eq(y[0], tensor([1,1,1]))\na,b,c = coco_tdl.decode_batch((x,y,z))[0]\ntest_eq(b, bbox[1])\ntest_close(c.bbox, tensor(bbox[0]).float())\ntest_eq(c.lbl, b)\n\n#Check types\ntest_eq(type(x), TensorImage)\ntest_eq(type(y), TensorMultiCategory)\ntest_eq(type(z), TensorBBox)\ntest_eq(type(a), TensorImage)\ntest_eq(type(b), MultiCategory)\ntest_eq(type(c), LabeledBBox)\ntest_eq(z.img_size, (128,128))\n```\n\n----------------------------------------\n\nTITLE: Importing Fastai, Contextlib, and Utility Dependencies in Python\nDESCRIPTION: Imports core Fastai modules (data, optimizer, callbacks), context manager utilities, and standard libraries for use in subsequent code. Ensures required types and functions are available for learner, model, and data handling. All dependencies must be pre-installed in the environment.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.data.all import *\nfrom fastai.optimizer import *\nfrom fastai.callback.core import *\nfrom contextlib import nullcontext\nimport cloudpickle,pickle,threading\nfrom collections.abc import MutableSequence\n```\n\n----------------------------------------\n\nTITLE: Finding Arguments with Metadata Attributes in Python\nDESCRIPTION: Implements a utility function to extract elements from a nested list that possess a __dict__ attribute, indicating presence of metadata. This function assists in identifying tensors or objects that carry metadata within collections of arguments.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\ndef _find_args(x):\n    x0 = x[0] if is_listy(x[0]) and x[0] else x\n    return [a for a in x0 if hasattr(a,'__dict__')]\n```\n\n----------------------------------------\n\nTITLE: Importing Core fastai Libraries\nDESCRIPTION: Imports necessary components from the fastai library, including basic functionalities required for defining and using callbacks and learners.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18b_callback.preds.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.basics import *\n```\n\n----------------------------------------\n\nTITLE: Testing Learner.create_opt with Default and Custom Optimizers (Python)\nDESCRIPTION: Demonstrates the creation of optimizers for a Learner using both default and custom (e.g., Adam) wrappers, testing that the optimizer is established correctly and the learning rate is set as expected. Includes an example where the optimizer's learning rate is overridden. Depends on partial, OptimWrapper, torch.optim.Adam, and test_eq.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner(n_train=5, cbs=VerboseCallback())\nassert learn.opt is None\nlearn.create_opt()\nassert learn.opt is not None\ntest_eq(learn.opt.hypers[0]['lr'], learn.lr)\n```\n\nLANGUAGE: python\nCODE:\n```\nlearn = synth_learner(n_train=5, cbs=VerboseCallback(), opt_func=partial(OptimWrapper, opt=torch.optim.Adam))\nassert learn.opt is None\nlearn.create_opt()\nassert learn.opt is not None\ntest_eq(learn.opt.hypers[0]['lr'], learn.lr)\n```\n\nLANGUAGE: python\nCODE:\n```\nwrapper_lr = 1\nlearn = synth_learner(n_train=5, cbs=VerboseCallback(), opt_func=partial(OptimWrapper, opt=torch.optim.Adam, lr=wrapper_lr))\nassert learn.opt is None\nlearn.create_opt()\nassert learn.opt is not None\ntest_eq(learn.opt.hypers[0]['lr'], wrapper_lr)\n```\n\n----------------------------------------\n\nTITLE: Installing and Upgrading FastAI on Google Colab\nDESCRIPTION: A hidden cell that checks if running in Google Colab environment and automatically installs or upgrades the FastAI library. This ensures users have the latest version when running the notebooks.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#| eval: false\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Testing `default_device` Functionality in fastai\nDESCRIPTION: Contains unit tests using `test_eq` to verify the behavior of the `default_device` function under different conditions. It checks if the correct device (CUDA or CPU) is returned based on CUDA availability and the `use` parameter (-1 for auto, True for required, False for CPU).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n#|cuda\nif torch.cuda.is_available():\n    _td = torch.device(torch.cuda.current_device())\n    test_eq(default_device(-1), _td)\n    test_eq(default_device(True), _td)\nelse:\n    test_eq(default_device(False), torch.device('cpu'))\ndefault_device(-1);\n```\n\n----------------------------------------\n\nTITLE: Testing Shuffle and Detachment with DistributedDL - Python\nDESCRIPTION: Tests correct batch shuffling and detachment with DistributedDL wrapping a shuffled TfmdDL. Ensures batch slices are correctly mapped post-detachment, especially when there are more distributed processes than batch chunks. Dependencies: torch, fastai TfmdDL and DistributedDL, test_eq.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\ndl = TfmdDL(list(range(10)), bs=4, num_workers=2, shuffle=True)\nres = []\nfor i in range(3):\n    dl1 = DistributedDL(dl, i, 3)\n    b  = list(dl1)[0]\n    bd = dl1.to_detach(b)\n    test_eq(b[:None if i<2 else 2],bd[4*i:4*(i+1)])\n```\n\n----------------------------------------\n\nTITLE: Importing fastai Modules\nDESCRIPTION: This snippet imports necessary modules from the fastai library. Specifically, it imports all modules from the 'tabular' and 'collab' subpackages. These modules provide functionalities related to tabular data and collaborative filtering models, respectively.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/46_tutorial.collab.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.tabular.all import *\nfrom fastai.collab import *\n```\n\n----------------------------------------\n\nTITLE: Setup: Upgrade fastai on Colab (Shell)\nDESCRIPTION: This snippet conditionally upgrades the fastai library using pip. It checks if the code is running in a Google Colab environment by verifying the existence of the `/content` directory. If found, it executes the `pip install -Uqq fastai` command to ensure the latest version of fastai is installed.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15a_vision.models.unet.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Testing Series Equality with Target Values - Python\nDESCRIPTION: This code snippet tests the equality of a pandas Series against a list of expected values. It initializes Categorify, creates a DataFrame, constructs a TabularPandas object with specified splits and target column, and asserts that the vocab attribute matches the expected list of categories ['a', 'b'].\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ntest_series(to2['b'], ['a', 'b', 'a', 'b', 'b'])\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Data from Kaggle - Python\nDESCRIPTION: Shell commands (commented out) to download the training images (train-jpg.tar.7z) and labels (train_v2.csv) from Kaggle, and unzip the CSV file. Intended for use in notebook or shell after authenticating Kaggle API. Commands are not executed in this form.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#! kaggle competitions download -c planet-understanding-the-amazon-from-space -f train-jpg.tar.7z -p {path}  \n#! kaggle competitions download -c planet-understanding-the-amazon-from-space -f train_v2.csv -p {path}  \n#! unzip -q -n {path}/train_v2.csv.zip -d {path}\n```\n\n----------------------------------------\n\nTITLE: Export Notebook with nbdev\nDESCRIPTION: This code snippet uses the `nbdev_export` function from the `nbdev` library to export the current notebook. This is a common practice in nbdev projects to convert the notebook to a Python module.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20_interpret.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Export Notebook Module Using nbdev_export - Python\nDESCRIPTION: Calls nbdev_export from nbdev to export notebook contents as a Python module/script. Should be at the end of the notebook for standard nbdev workflows. Dependency: nbdev.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20a_distributed.ipynb#_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Example of using unsqueeze to add multiple singleton dimensions (Python)\nDESCRIPTION: This code demonstrates applying `unsqueeze` to add two singleton dimensions to a tensor, resulting in a 3D tensor with shape (1, 1, 1). It verifies the functionality of the `unsqueeze` helper for dimension expansion.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/00_torch_core.ipynb#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nt = tensor([1])\nt2 = unsqueeze(t, n=2)\ntest_eq(t2,t[:,None,None])\n```\n\n----------------------------------------\n\nTITLE: Testing Column Categorization (Python)\nDESCRIPTION: This snippet tests the output of the `cont_cat_split` function. It asserts that the function correctly identifies continuous and categorical columns in a DataFrame, based on predefined expectations. The test utilizes the `test_eq` function.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncont, cat = cont_cat_split(df, max_card=0)\ntest_eq((cont, cat), (\n    ['ui32', 'i64', 'f16', 'd1_Year', 'd1_Month', 'd1_Week', 'd1_Day', 'd1_Dayofweek', 'd1_Dayofyear', 'd1_Elapsed'],\n    ['cat1', 'd1_date', 'd1_Is_month_end', 'd1_Is_month_start', 'd1_Is_quarter_end', 'd1_Is_quarter_start', 'd1_Is_year_end', 'd1_Is_year_start']\n    ))\n```\n\n----------------------------------------\n\nTITLE: Installing fastai on Colab\nDESCRIPTION: This code snippet upgrades the fastai library on Google Colaboratory if it detects that the code is running within a Colab environment. It uses a shell command to check for the existence of the `/content` directory (which is present in Colab) and then uses `pip` to upgrade fastai.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#|hide\n#| eval: false\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Installing FastAI in Colab Environment\nDESCRIPTION: Checks if the '/content' directory exists and upgrades FastAI in Google Colab to ensure the latest version is installed. This setup step is essential for maintaining an up-to-date environment for running FastAI code.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/99_pytorch_doc.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai\n```\n\n----------------------------------------\n\nTITLE: Testing Load Without Optimizer State\nDESCRIPTION: This code tests the behavior of `load` when the model has been saved without the optimizer state, by using `with_opt=False` during saving.  It creates, trains, and saves a model without the optimizer, then reloads the model. The assertion confirms that the loaded model parameters are the same but the optimizer states differ using `test_ne`.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nwith tempfile.TemporaryDirectory() as d:\n    learn = synth_learner(path=d)\n    learn.fit(1)\n    learn.save('tmp', with_opt=False)\n    learn1 = synth_learner(path=d)\n    learn1 = learn1.load('tmp', with_opt=False)\n    test_eq(learn.a, learn1.a)\n    test_eq(learn.b, learn1.b)\n    test_ne(learn.opt.state_dict(), learn1.opt.state_dict())\n```\n\n----------------------------------------\n\nTITLE: Testing DataFrame Copies in TabularPandas - Python\nDESCRIPTION: This code snippet checks if the `TabularPandas` object stores copies of the original DataFrame. It initializes `Categorify`, creates a DataFrame, then instantiates `TabularPandas`.  The `test_eq` assertion verifies that the 'to' attribute does not exist within the `categorify` processor, ensuring no copies are made.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ncat = Categorify()\ndf = pd.DataFrame({'a':[0,1,2,3,4]})\nto = TabularPandas(df, cat, cont_names='a', splits=[[0,1,2],[3,4]])\ntest_eq(hasattr(to.categorify, 'to'), False)\n```\n\n----------------------------------------\n\nTITLE: Installing Neptune Client via Pip\nDESCRIPTION: Shell command to install the `neptune-client` library using pip, which is necessary for interacting with the Neptune.ai service.\nSOURCE: https://github.com/fastai/fastai/blob/main/disabled/70b_callback.neptune.ipynb#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npip install neptune-client\n```\n\n----------------------------------------\n\nTITLE: Defining a Constant-Returning Callable in Python\nDESCRIPTION: Implements _ConstantFunc, a callable class returning a fixed value regardless of input parameters. Used for injecting constant values or stubs in training flows or parameter injection. Pure Python and no external dependencies.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass _ConstantFunc():\n    \"Returns a function that returns `o`\"\n    def __init__(self, o): self.o = o\n    def __call__(self, *args, **kwargs): return self.o\n```\n\n----------------------------------------\n\nTITLE: DataFrame Shrinking and Validation (Python)\nDESCRIPTION: These tests validate the `df_shrink` function. They assert that the function correctly changes the data types of columns to smaller types, that memory usage is reduced, that integer columns are correctly converted to unsigned integers when applicable, and that specified columns can be excluded from shrinking. Relies on the test_eq function.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/40_tabular.core.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntest_eq(df['i'].dtype=='int64' and df2['i'].dtype=='int8', True)\ntest_eq(df['f'].dtype=='float64' and df2['f'].dtype=='float32', True)\ntest_eq(df['u'].dtype=='int64' and df2['u'].dtype=='int16', True)\ntest_eq(df2['date'].dtype, 'object')\n\ntest_eq(df2.memory_usage().sum() < df.memory_usage().sum(), True)\n\n# Test int => uint (when col.min() >= 0)\ndf3 = df_shrink(df, int2uint=True)\ntest_eq(df3['u'].dtype, 'uint8')  # int64 -> uint8 instead of int16\n\n# Test excluding columns\ndf4 = df_shrink(df, skip=['i','u'])\ntest_eq(df['i'].dtype, df4['i'].dtype)\ntest_eq(df4['u'].dtype, 'int64')\n```\n\n----------------------------------------\n\nTITLE: Helper function for retrieving show_batch implementation based on input types\nDESCRIPTION: A utility function that manually resolves the appropriate show_batch function implementation based on the provided input, target, and samples types, using the type dispatch system.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_show_batch_func(x_typ=Any, y_typ=Any, samples_typ=Any):\n    \"Helper function to manually get show_batch function for given input types.\"\n    return show_batch._resolve_method_with_cache((x_typ, y_typ, samples_typ))[0]\n```\n\n----------------------------------------\n\nTITLE: Define a lambda to extract the first element\nDESCRIPTION: Defines a lambda function `item0` that takes an object `o` and returns its first element. This is used as the key function for sorting movie ratings based on bias.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson4-collab.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nitem0 = lambda o:o[0]\n```\n\n----------------------------------------\n\nTITLE: Testing _do_epoch_train and _do_epoch_validate Event Firing in fastai Learner (Python)\nDESCRIPTION: Ensures that the internal methods _do_epoch_train and _do_epoch_validate of the Learner emit the expected sequences of events before, during, and after an epoch for both training and validation. Uses test_stdout and previous definitions for event lists and VerboseCallback.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nlearn.opt = SGD(learn.parameters(), lr=learn.lr)\nlearn.epoch = 0\ntest_stdout(lambda: learn._do_epoch_train(), '\\n'.join(['before_train'] + batch_events * 5 + ['after_train']))\n```\n\nLANGUAGE: python\nCODE:\n```\n#|hide\ntest_stdout(learn._do_epoch_validate, '\\n'.join(['before_validate'] + valid_events * 2+ ['after_validate']))\n```\n\n----------------------------------------\n\nTITLE: Accessing a specific Image Path\nDESCRIPTION: This accesses the path of the first item in the list of images retrieved in the previous step, `items`. This is used to display a single image.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson7-resnet-mnist.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nitems[0]\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch Dataset Utilities for Testing (Fastai) in Python\nDESCRIPTION: Imports TensorDataset and DataLoader from torch.utils.data to facilitate the creation of synthetic datasets and data loaders for testing training workflow. Intended for internal or test use; requires torch installed.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.data import TensorDataset, DataLoader as TorchDL\n```\n\n----------------------------------------\n\nTITLE: Defining Learning Rate - Python\nDESCRIPTION: Sets the value for the learning rate (`lr`), a hyperparameter controlling the step size during the gradient descent optimization process. A value of 2e-2 (0.02) is chosen.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nlr=2e-2\n```\n\n----------------------------------------\n\nTITLE: Testing layer_info on a Sequential Model with BN and Activation (Python)\nDESCRIPTION: Uses layer_info with a custom sequential model comprising Linear, ReLU, and BatchNorm1d layers. Validates extracted info with test_eq on expected output tuples representing layer types, parameter counts, shapes, and grouping logic. Assumes synth_learner, layer_info, and torch.nn modules are available, and tests consistent with batch input.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\ndef _m(): return nn.Sequential(nn.Linear(1,50), nn.ReLU(), nn.BatchNorm1d(50), nn.Linear(50, 1))\nsample_input = torch.randn((16, 1))\ntest_eq(layer_info(synth_learner(model=_m()), sample_input), [\n    ('Linear', 100, True, [1, 50], False),\n    ('ReLU', '', '', [1,50], True),\n    ('BatchNorm1d', 100, True, [1, 50], True),\n    ('Linear', 51, True, [1, 1], False)\n])\n```\n\n----------------------------------------\n\nTITLE: Installing HuggingFace transformers Library (Bash)\nDESCRIPTION: This command upgrades or installs the HuggingFace transformers library using pip. Required for accessing pretrained transformer models and tokenizer classes. Internet connectivity is needed; should be executed in a shell or notebook cell prior to importing transformer classes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/39_tutorial.transformers.ipynb#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n!pip install -Uq transformers\n```\n\n----------------------------------------\n\nTITLE: Parse Wheel Filename Components (Python)\nDESCRIPTION: Uses the `parsed_filename` attribute of the `WheelFile` object (`fwhl`) to access a dictionary containing the parsed components of the Wheel filename, such as package name, version, build tag, and compatibility tags.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfwhl.parsed_filename.groupdict()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Parallel No-Operation (Batched) - Jupyter\nDESCRIPTION: This snippet uses `%%timeit` to measure the overhead of fastai's `parallel` function itself when using 2 workers on the `batches` data, by executing a function that does nothing (`noop`). This helps isolate the time spent on process creation, communication, and management from the actual computation time. Results stored globally in `t`. Runs 2 times per loop, 3 loops.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n%%timeit -n 2 -r 3\nglobal t\nt = parallel(noop, batches, progress=False, n_workers=2)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Parallel Spacy `pipe` (Batched, 8 Workers) - Jupyter\nDESCRIPTION: This snippet uses `%%timeit` to benchmark the execution time of applying the `f` function (which uses Spacy's `pipe`) using fastai's `parallel` with 8 workers on the `batches8` data. This measures the combined performance of fastai parallelism and Spacy's efficient `pipe` method when each worker processes a batch. Results stored globally in `t`. Runs for 3 loops (`-r 3`).\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n%%timeit -r 3\nglobal t\nt = parallel(f, batches8, progress=False, n_workers=8)\n```\n\n----------------------------------------\n\nTITLE: Importing nbdev and test_utils\nDESCRIPTION: This imports `showdoc` from `nbdev` for documentation and `test_utils` from `fastai` for testing purposes. These are hidden from the exported documentation.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/17_callback.tracker.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev.showdoc import *\nfrom fastai.test_utils import *\n```\n\n----------------------------------------\n\nTITLE: Testing skm_to_fastai with Classification and Regression Metrics - Python\nDESCRIPTION: Demonstrates using skm_to_fastai to wrap precision_score and r2_score for both single-label and multi-label scenarios. Samples predictions and targets, computes metric both via fastai's mechanism and directly via sklearn for equivalence testing. Used for validating sklearn metric integration. Requires sklearn, torch, fastai, and numpy.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ntst_single = skm_to_fastai(skm.precision_score)\nx1,x2 = torch.randn(20,2),torch.randint(0, 2, (20,))\ntest_close(compute_val(tst_single, x1, x2), skm.precision_score(x2, x1.argmax(dim=-1)))\n```\n\nLANGUAGE: Python\nCODE:\n```\ntst_multi = skm_to_fastai(skm.precision_score, thresh=0.2)\nx1,x2 = torch.randn(20),torch.randint(0, 2, (20,))\ntest_close(compute_val(tst_multi, x1, x2), skm.precision_score(x2, torch.sigmoid(x1) >= 0.2))\n\ntst_multi = skm_to_fastai(skm.precision_score, thresh=0.2, activation=ActivationType.No)\nx1,x2 = torch.randn(20),torch.randint(0, 2, (20,))\ntest_close(compute_val(tst_multi, x1, x2), skm.precision_score(x2, x1 >= 0.2))\n```\n\nLANGUAGE: Python\nCODE:\n```\ntst_reg = skm_to_fastai(skm.r2_score, is_class=False)\nx1,x2 = torch.randn(20,5),torch.randn(20,5)\ntest_close(compute_val(tst_reg, x1, x2), skm.r2_score(x2.view(-1).numpy(), x1.view(-1).numpy()))\n```\n\nLANGUAGE: Python\nCODE:\n```\ntest_close(tst_reg(x1, x2), skm.r2_score(x2.view(-1).numpy(), x1.view(-1).numpy()))\n```\n\n----------------------------------------\n\nTITLE: Patching `ipywidgets.Box` for Index Access\nDESCRIPTION: Adds `__getitem__` method to the `ipywidgets.Box` class, allowing access to its child widgets using standard integer indexing (e.g., `my_box[0]`).\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/09c_vision.widgets.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#|export\n@patch\ndef __getitem__(self:Box, i): return self.children[i]\n```\n\n----------------------------------------\n\nTITLE: Submitting Results to Kaggle Competition via CLI - Python\nDESCRIPTION: Shell command (uncomment to run) for submitting the generated CSV to the Kaggle competition using the official CLI. Takes submission file path and message as arguments. Requires active Kaggle API authentication and CLI tools.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\n! kaggle competitions submit planet-understanding-the-amazon-from-space -f {path/'submission.csv'} -m \"My submission\"\n```\n\n----------------------------------------\n\nTITLE: Export with nbdev\nDESCRIPTION: This code uses `nbdev_export()` to export the content of the current notebook. This function is part of the nbdev library and is used for converting the notebook into Python modules, ensuring that the code is available in a standard form.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/97_test_utils.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev import *\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking NumPy String Split - Jupyter\nDESCRIPTION: This snippet uses `%%timeit` to benchmark the performance of NumPy's built-in character array split function (`np.char.split`) applied to the `sarr` NumPy array. This measures how fast NumPy can perform the simple string splitting operation across an array of strings. Results stored globally in `t`. Runs 2 times per loop, 3 loops.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/explorations/tokenizing.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n%%timeit -n 2 -r 3\nglobal t\nt = np.char.split(sarr)\n```\n\n----------------------------------------\n\nTITLE: Importing necessary modules from fastai and PyTorch\nDESCRIPTION: This snippet imports core modules from fastai and PyTorch required for training, mixed precision, and callback functionalities. It sets up the environment for subsequent channels last training enhancements.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18c_callback.channelslast.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.basics import *\nfrom fastai.callback.fp16 import AMPMode, MixedPrecision\nfrom torch.cuda.amp import GradScaler\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export Module for nbdev in Python\nDESCRIPTION: Specifies the default export module name for use with nbdev-based workflows. This declares the current module as 'learner' for exported symbols, affecting export behavior during documentation and packaging. Requires nbdev setup.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp learner\n```\n\n----------------------------------------\n\nTITLE: Installing fastai in Colab Shell Command\nDESCRIPTION: This shell command checks if the code is running in a Colab environment ('/content' directory exists) and installs or upgrades the fastai package quietly and quickly. It ensures the fastai library is up to date for running subsequent Python code.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/33_text.models.core.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Documenting DataLoaders Methods and Properties (fastai, Python)\nDESCRIPTION: Renders API documentation with show_doc for several DataLoaders methods/properties, notably __getitem__, train, valid, train_ds, and valid_ds. These provide indexed loader access, property-based access to train/valid loaders and datasets. No runtime input/output.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(DataLoaders.__getitem__)\n```\n\nLANGUAGE: python\nCODE:\n```\nx2\n```\n\nLANGUAGE: python\nCODE:\n```\nx2 = dls[0].one_batch()\ntest_eq(x,x2)\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(DataLoaders.train, name=\"DataLoaders.train\")\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(DataLoaders.valid, name=\"DataLoaders.valid\")\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(DataLoaders.train_ds, name=\"DataLoaders.train_ds\")\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(DataLoaders.valid_ds, name=\"DataLoaders.valid_ds\")\n```\n\n----------------------------------------\n\nTITLE: Export command for fastai project documentation\nDESCRIPTION: Triggers the export of current module contents to nbdev to facilitate documentation, versioning, or deployment workflows.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18c_callback.channelslast.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\nfrom nbdev import *\nbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Unit Testing AccumMetric Functionalities - Python\nDESCRIPTION: Runs multiple test cases demonstrating AccumMetric's flexibility: standard metric, activation handling (argmax, thresh, sigmoid), conversion to numpy, and argument inversion. Uses fastai test helpers like test_close and test_eq and constructs metrics with relevant parameters. Dependencies: torch, numpy, fastai's testing utilities. No return value unless assertions fail.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ntst = AccumMetric(_l2_mean)\ntest_close(compute_val(tst, x1, x2), _l2_mean(x1, x2))\ntest_eq(torch.cat(tst.preds), x1.view(-1))\ntest_eq(torch.cat(tst.targs), x2.view(-1))\n\n#test argmax\nx1,x2 = torch.randn(20,5),torch.randint(0, 5, (20,))\ntst = AccumMetric(_l2_mean, dim_argmax=-1)\ntest_close(compute_val(tst, x1, x2), _l2_mean(x1.argmax(dim=-1), x2))\n\n#test thresh\nx1,x2 = torch.randn(20,5),torch.randint(0, 2, (20,5)).bool()\ntst = AccumMetric(_l2_mean, thresh=0.5)\ntest_close(compute_val(tst, x1, x2), _l2_mean((x1 >= 0.5), x2))\n\n#test sigmoid\nx1,x2 = torch.randn(20,5),torch.randn(20,5)\ntst = AccumMetric(_l2_mean, activation=ActivationType.Sigmoid)\ntest_close(compute_val(tst, x1, x2), _l2_mean(torch.sigmoid(x1), x2))\n\n#test to_np\nx1,x2 = torch.randn(20,5),torch.randn(20,5)\ntst = AccumMetric(lambda x,y: isinstance(x, np.ndarray) and isinstance(y, np.ndarray), to_np=True)\nassert compute_val(tst, x1, x2)\n\n#test invert_arg\nx1,x2 = torch.randn(20,5),torch.randn(20,5)\ntst = AccumMetric(lambda x,y: torch.sqrt(x.pow(2).mean()))\ntest_close(compute_val(tst, x1, x2), torch.sqrt(x1.pow(2).mean()))\ntst = AccumMetric(lambda x,y: torch.sqrt(x.pow(2).mean()), invert_arg=True)\ntest_close(compute_val(tst, x1, x2), torch.sqrt(x2.pow(2).mean()))\n```\n\n----------------------------------------\n\nTITLE: Documenting HookCallback's before_fit Method (Python)\nDESCRIPTION: Calls show_doc on the HookCallback.before_fit method for documentation purposes. This displays detailed documentation of this lifecycle event in the fastai callback, which typically registers hooks at the start of training. Assumes show_doc and HookCallback are imported.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\nshow_doc(HookCallback.before_fit)\n```\n\n----------------------------------------\n\nTITLE: Installing fastai Development Dependencies (Bash)\nDESCRIPTION: This command installs the necessary development dependencies specified in the `settings.ini` file. It uses pip's editable install feature to install the current directory's package and its `dev` dependency group, essential for running tests and contributing.\nSOURCE: https://github.com/fastai/fastai/blob/main/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .[dev]\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export and Class Level - fastai nbdev - Python\nDESCRIPTION: Specifies nbdev metadata for the default export module and class nesting level. Used for auto-generating documentation and controlling notebook export behavior. No runtime effect except in nbdev-based workflows.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/01_layers.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp layers\n#|default_cls_lvl 3\n```\n\n----------------------------------------\n\nTITLE: Display DataFrame head\nDESCRIPTION: This displays the first few rows of the concatenated DataFrame, allowing for a quick inspection of the data.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/35_tutorial.wikitext.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf_all.head()\n```\n\n----------------------------------------\n\nTITLE: Importing fastai test_utils\nDESCRIPTION: This line imports all the functions and classes from the `fastai.test_utils` module, which are likely used for testing the functionality of the model hooking system.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15_callback.hook.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.test_utils import *\n```\n\n----------------------------------------\n\nTITLE: Getting DataFrame Length - Python\nDESCRIPTION: Calculates and displays the number of rows in the training DataFrame.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson6-rossmann.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nn = len(train_df); n\n```\n\n----------------------------------------\n\nTITLE: Importing Documentation Utility from nbdev\nDESCRIPTION: This snippet imports the 'showdoc' utility from nbdev, likely used for automating documentation within notebooks. It facilitates inline documentation and demonstration of code functionalities.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Importing test utilities from fastai and showdoc for documentation\nDESCRIPTION: Imports utility functions used for testing and documentation purposes, supporting the development and validation of the channels last callback.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/18c_callback.channelslast.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\nfrom fastai.test_utils import *\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Importing nbdev Showdoc Utility in Python\nDESCRIPTION: Imports the 'showdoc' utility from the nbdev documentation toolchain, facilitating function and class documentation display. This helps with notebook-based documentation in fastai or nbdev projects. Requires nbdev installed.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13a_learner.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Cloning and installing fastai repository\nDESCRIPTION: Clones the fastai repository, installs it in development mode with additional dependencies, and forks it for contribution purposes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/dev-setup.ipynb#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\ncd ..\n```\n\nLANGUAGE: Shell\nCODE:\n```\ngh repo clone fastai/fastai\n```\n\nLANGUAGE: Shell\nCODE:\n```\ncd fastai\n```\n\nLANGUAGE: Shell\nCODE:\n```\npip install -qe .[dev]\n```\n\nLANGUAGE: Shell\nCODE:\n```\ngh repo fork --remote\n```\n\nLANGUAGE: Shell\nCODE:\n```\nnbdev_install_hooks\n```\n\n----------------------------------------\n\nTITLE: Importing Documentation Utilities\nDESCRIPTION: Imports the `showdoc` function from the `nbdev.showdoc` module for generating documentation and displaying information about functions and classes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Helper Functions for Metric Testing and Aggregation - Python\nDESCRIPTION: Defines a helper function _l2_mean to compute per-element L2 error and a compute_val function that simulates batch-wise accumulation and computes the final metric value via AccumMetric. Parameters are metric instance, prediction tensors, and target tensors. Outputs metric value after batch accumulation. Used for internal testing.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13b_metrics.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef _l2_mean(x,y): return torch.sqrt((x.float()-y.float()).pow(2).mean())\n\n#Go through a fake cycle with various batch sizes and computes the value of met\ndef compute_val(met, x1, x2):\n    met.reset()\n    vals = [0,6,15,20]\n    learn = TstLearner()\n    for i in range(3):\n        learn.pred,learn.yb = x1[vals[i]:vals[i+1]],(x2[vals[i]:vals[i+1]],)\n        met.accumulate(learn)\n    return met.value\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation Links for PyTorch Modules, Classes, or Functions\nDESCRIPTION: Defines a function that takes the name of a PyTorch module, class, or function, and returns the URL to its documentation page on the PyTorch website. It handles special cases such as functional API 'F' and tensor types, performing dynamic imports to determine the correct link.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/99_pytorch_doc.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef pytorch_doc_link(\n    name:str # Name of a PyTorch module, class or function\n) -> (str, None):\n    \"Get the URL to the documentation of a PyTorch module, class or function\"\n    if name.startswith('F'): name = 'torch.nn.functional' + name[1:]\n    if not name.startswith('torch.'): name = 'torch.' + name\n    if name == 'torch.Tensor': return f'{PYTORCH_URL}tensors.html'\n    try:\n        mod = importlib.import_module(name)\n        return f'{PYTORCH_URL}{_mod2page(mod)}'\n    except: pass\n    splits = name.split('.')\n    mod_name,fname = '.'.join(splits[:-1]),splits[-1]\n    if mod_name == 'torch.Tensor': return f'{PYTORCH_URL}tensors.html#{name}'\n    try:\n        mod = importlib.import_module(mod_name)\n        page = _mod2page(mod)\n        return f'{PYTORCH_URL}{page}#{name}'\n    except: return None\n```\n\n----------------------------------------\n\nTITLE: Import Core Libraries for Package Handling (Python)\nDESCRIPTION: Imports essential Python libraries for working with package files and metadata. It includes `fastai.torch_basics` (likely for utility functions like `Path`), `zipfile` for ZIP archives (like .whl), `tarfile` for tar archives (like .tar.bz2), `email.parser` for parsing METADATA files, and `wheel.wheelfile` for specific Wheel file operations.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.torch_basics import *\nimport zipfile,tarfile\nfrom email.parser import Parser\nfrom wheel import wheelfile\n```\n\n----------------------------------------\n\nTITLE: Configuring Matplotlib for Inline Display - Python\nDESCRIPTION: Sets the matplotlib backend to allow plots to display inline in Jupyter notebooks. No dependencies required beyond Jupyter and matplotlib. No parameters are taken; this is a magic command. Outputs inline plots in notebook environments only.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson3-planet.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Verifying Documentation Link Generation\nDESCRIPTION: Runs a series of tests to confirm that the 'pytorch_doc_link' function generates the correct URLs for various PyTorch modules and functions such as Tensor, Tensor.sqrt, torch.zeros_like, nn.Module, nn.Linear, and F.cross_entropy.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/99_pytorch_doc.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ntest_links = {\n    'Tensor': 'https://pytorch.org/docs/stable/tensors.html',\n    'Tensor.sqrt': 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor.sqrt',\n    'torch.zeros_like': 'https://pytorch.org/docs/stable/torch.html#torch.zeros_like',\n    'nn.Module': 'https://pytorch.org/docs/stable/nn.html#torch.nn.Module',\n    'nn.Linear': 'https://pytorch.org/docs/stable/nn.html#torch.nn.Linear',\n    'F.cross_entropy': 'https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy'\n}\nfor f,l in test_links.items(): test_eq(pytorch_doc_link(f), l)\n```\n\n----------------------------------------\n\nTITLE: Deprecated Function create_cnn_model\nDESCRIPTION: This function offloads the creation of vision models to 'create_vision_model' but is deprecated with a warning. It serves as a backward compatibility shim for legacy code referring to 'create_cnn_model'.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/21_vision.learner.ipynb#_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\n#|export\ndef create_cnn_model(*args, **kwargs):\n    \"Deprecated name for `create_vision_model` -- do not use\"\n    warn(\"`create_cnn_model` has been renamed to `create_vision_model` -- please update your code\")\n    return create_vision_model(*args, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Updating fastai on Colab\nDESCRIPTION: Checks if the environment is Google Colab (`/content` directory exists) and, if so, updates the fastai library to the latest version silently and quietly.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/44_tutorial.tabular.ipynb#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n```\n\n----------------------------------------\n\nTITLE: Imports: Core fastai modules (Python)\nDESCRIPTION: Imports necessary modules from `fastai.torch_basics` and `fastai.callback.hook`. These imports provide fundamental PyTorch-related utilities, base classes like `Module`, and mechanisms for capturing layer outputs required for skip connections in the U-Net.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/15a_vision.models.unet.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom fastai.torch_basics import *\nfrom fastai.callback.hook import *\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary Modules\nDESCRIPTION: Imports necessary modules from the fastai library and other dependencies, like data handling, optimizers, and loss functions. These imports provide access to fundamental building blocks for creating and training models.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/13_callback.core.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#|export\nfrom __future__ import annotations\nfrom fastai.data.all import *\nfrom fastai.optimizer import *\nfrom fastai.losses import BaseLoss\n```\n\n----------------------------------------\n\nTITLE: Show Documentation for after_batch\nDESCRIPTION: Displays the documentation for the `after_batch` method of the `ParamScheduler` class, which is responsible for recording hyperparameters.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(ParamScheduler.after_batch)\n```\n\n----------------------------------------\n\nTITLE: Import Fastai Test Utilities\nDESCRIPTION: Imports utilities from `fastai.test_utils`, likely including functions like `synth_learner` for creating synthetic data and learners for testing purposes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/16_callback.progress.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.test_utils import *\n```\n\n----------------------------------------\n\nTITLE: Listing Directory Contents - Python\nDESCRIPTION: Lists the contents of the directory specified by the `path` variable. This is useful for verifying that the required dataset file exists in the expected location.\nSOURCE: https://github.com/fastai/fastai/blob/main/dev_nbs/course/lesson5-sgd-mnist.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npath.ls()\n```\n\n----------------------------------------\n\nTITLE: Installing accelerate Library (Bash)\nDESCRIPTION: Installs the Hugging Face accelerate library using pip. This library is required to facilitate distributed training setups.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/20b_tutorial.distributed.ipynb#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\npip install accelerate -U\n```\n\n----------------------------------------\n\nTITLE: Importing nbdev CLI - Python\nDESCRIPTION: This snippet imports the necessary functions from the `nbdev.cli` module.  This import is likely used for command-line interface related functionality within the broader project. There are no specific parameters, inputs, or outputs mentioned in this context.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom nbdev.cli import *\n```\n\n----------------------------------------\n\nTITLE: Show Documentation for before_batch\nDESCRIPTION: Displays the documentation for the `before_batch` method of the `ParamScheduler` class, responsible for setting hyperparameters in the optimizer.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(ParamScheduler.before_batch)\n```\n\n----------------------------------------\n\nTITLE: Import show_doc Utility from nbdev\nDESCRIPTION: Imports the show_doc utility from nbdev to generate documentation or help views for classes and functions, aiding in code understanding.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70a_callback.tensorboard.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom nbdev import show_doc\n```\n\n----------------------------------------\n\nTITLE: Import showdoc function\nDESCRIPTION: This code snippet imports the `showdoc` function from the `nbdev.showdoc` module, which is used to display documentation for functions and classes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/41_tabular.data.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev.showdoc import *\n```\n\n----------------------------------------\n\nTITLE: Define all\nDESCRIPTION: The `_all_` list defines the public API of the module, exporting the `wandb_process` function.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/70_callback.wandb.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n_all_ = ['wandb_process']\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export Module Name\nDESCRIPTION: Specifies the default export module name for the notebook as 'data.transforms'.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/05_data.transforms.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp data.transforms\n```\n\n----------------------------------------\n\nTITLE: Preparing fastai for Contribution (Bash)\nDESCRIPTION: This command, part of the nbdev framework, prepares the fastai library codebase after changes have been made. It performs necessary updates and checks before running tests and finalizing contributions.\nSOURCE: https://github.com/fastai/fastai/blob/main/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nnbdev_prepare\n```\n\n----------------------------------------\n\nTITLE: Exporting fastai Modules\nDESCRIPTION: Exports the current notebook or module for fastai library, enabling reuse and deployment of the code in other contexts.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/03_data.core.ipynb#_snippet_75\n\nLANGUAGE: Python\nCODE:\n```\n#|hide\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Exporting notebooks for fastai package using nbdev in Python\nDESCRIPTION: Invokes nbdev_export() from the nbdev package to export notebooks into a Python package for release or distribution. This snippet is used to compile and hide code not intended for regular execution but for packaging. Requires nbdev installed and the project structured for nbdev usage.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/08_vision.data.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#|hide\nfrom nbdev import nbdev_export\nnbdev_export()\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export\nDESCRIPTION: This code defines the default export target for the module as `tabular.rapids` using the `nbdev` library.  It specifies which module will contain the exported functions and classes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_42_tabular_rapids.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#|default_exp tabular.rapids\n```\n\n----------------------------------------\n\nTITLE: Get Wheel RECORD Path (Python)\nDESCRIPTION: Retrieves the path to the `RECORD` file within the opened Wheel file (`fwhl`). The `RECORD` file lists all files included in the wheel along with their hashes and sizes.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/_whl2conda.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfwhl.record_path\n```\n\n----------------------------------------\n\nTITLE: Testing LRFinder Behaviour with Synthetic Learner - FastAI Python\nDESCRIPTION: Uses a temporary directory to instantiate a synthetic learner for testing the LRFinder callback. Checks that the number of recorded learning rates and losses does not exceed the prescribed iteration count and asserts various conditions on loss divergence and learning rate schedule correctness. It verifies no validation data is present and that the model parameters and optimizer state reset properly after fitting with LRFinder. This snippet ensures robustness and expected behavior of the learning rate finder in controlled test environments.\nSOURCE: https://github.com/fastai/fastai/blob/main/nbs/14_callback.schedule.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n#|slow\nwith tempfile.TemporaryDirectory() as d:\n    learn = synth_learner(path=Path(d))\n    init_a,init_b = learn.model.a,learn.model.b\n    with learn.no_logging(): learn.fit(20, cbs=LRFinder(num_it=100))\n    assert len(learn.recorder.lrs) <= 100\n    test_eq(len(learn.recorder.lrs), len(learn.recorder.losses))\n    #Check stop if diverge\n    if len(learn.recorder.lrs) < 100: assert learn.recorder.losses[-1] > 4 * min(learn.recorder.losses)\n    #Test schedule\n    test_eq(learn.recorder.lrs, [SchedExp(1e-7, 10)(i/100) for i in range_of(learn.recorder.lrs)])\n    #No validation data\n    test_eq([len(v) for v in learn.recorder.values], [1 for _ in range_of(learn.recorder.values)])\n    #Model loaded back properly\n    test_eq(learn.model.a, init_a)\n    test_eq(learn.model.b, init_b)\n    test_eq(learn.opt.state_dict()['state'], [{}, {}])\n```"
  }
]
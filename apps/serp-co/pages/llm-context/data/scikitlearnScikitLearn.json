[
  {
    "owner": "scikit-learn",
    "repo": "scikit-learn",
    "content": "TITLE: Training MLPClassifier for Binary Classification in Python\nDESCRIPTION: This snippet demonstrates how to create, train, and use an MLPClassifier for binary classification. It includes initializing the classifier with specific parameters, fitting it to training data, and making predictions on new samples.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neural_networks_supervised.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.neural_network import MLPClassifier\n>>> X = [[0., 0.], [1., 1.]]\n>>> y = [0, 1]\n>>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n...                     hidden_layer_sizes=(5, 2), random_state=1)\n...\n>>> clf.fit(X, y)\nMLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n              solver='lbfgs')\n\n>>> clf.predict([[2., 2.], [-1., -2.]])\narray([1, 0])\n```\n\n----------------------------------------\n\nTITLE: Using Pipeline for Consistent Preprocessing in scikit-learn\nDESCRIPTION: Demonstrates how to use a Pipeline to chain transformations with estimators, which reduces the possibility of forgetting a transformation step and ensures consistent preprocessing.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/common_pitfalls.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.pipeline import make_pipeline\n\n>>> model = make_pipeline(StandardScaler(), LinearRegression())\n>>> model.fit(X_train, y_train)\nPipeline(steps=[('standardscaler', StandardScaler()),\n                    ('linearregression', LinearRegression())])\n>>> mean_squared_error(y_test, model.predict(X_test))\n0.90...\n```\n\n----------------------------------------\n\nTITLE: Calculating Precision, Recall, and F-measures in Binary Classification\nDESCRIPTION: Shows how to calculate precision, recall, F1-score, and F-beta scores for binary classification tasks using scikit-learn metrics. This example illustrates the basic usage with binary classification predictions.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import metrics\ny_pred = [0, 1, 0, 0]\ny_true = [0, 1, 0, 1]\nmetrics.precision_score(y_true, y_pred)\n1.0\nmetrics.recall_score(y_true, y_pred)\n0.5\nmetrics.f1_score(y_true, y_pred)\n0.66...\nmetrics.fbeta_score(y_true, y_pred, beta=0.5)\n0.83...\nmetrics.fbeta_score(y_true, y_pred, beta=1)\n0.66...\nmetrics.fbeta_score(y_true, y_pred, beta=2)\n0.55...\nmetrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5)\n(array([0.66..., 1.        ]), array([1. , 0.5]), array([0.71..., 0.83...]), array([2, 2]))\n```\n\n----------------------------------------\n\nTITLE: Generating a classification report in Python with scikit-learn\nDESCRIPTION: Example showing how to create a comprehensive text report of classification metrics using scikit-learn. The report includes precision, recall, F1-score, and support for each class as well as aggregate metrics.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics import classification_report\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 1, 0]\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_true, y_pred, target_names=target_names))\n             precision    recall  f1-score   support\n\n    class 0       0.67      1.00      0.80         2\n    class 1       0.00      0.00      0.00         1\n    class 2       1.00      0.50      0.67         2\n\n   accuracy                           0.60         5\n  macro avg       0.56      0.50      0.49         5\nweighted avg       0.67      0.60      0.59         5\n\n```\n\n----------------------------------------\n\nTITLE: Basic StandardScaler Usage in Python with Scikit-learn\nDESCRIPTION: Demonstrates how to use StandardScaler to standardize a dataset by removing the mean and scaling to unit variance. Shows creation of scaler object, fitting to training data, and transforming data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import preprocessing\nimport numpy as np\nX_train = np.array([[ 1., -1.,  2.],\n                   [ 2.,  0.,  0.],\n                   [ 0.,  1., -1.]])\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_scaled = scaler.transform(X_train)\n```\n\n----------------------------------------\n\nTITLE: Basic Classification with DecisionTreeClassifier\nDESCRIPTION: Demonstrates how to create and train a basic decision tree classifier using scikit-learn with binary input data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/tree.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn import tree\n>>> X = [[0, 0], [1, 1]]\n>>> Y = [0, 1]\n>>> clf = tree.DecisionTreeClassifier()\n>>> clf = clf.fit(X, Y)\n```\n\n----------------------------------------\n\nTITLE: Finding Nearest Neighbors Using NearestNeighbors Class\nDESCRIPTION: Demonstrates how to use the NearestNeighbors class to find nearest neighbors between data points using the ball_tree algorithm. Shows initialization, fitting, and querying for neighbors with distances and indices.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neighbors.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.neighbors import NearestNeighbors\nimport numpy as np\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\nnbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\ndistances, indices = nbrs.kneighbors(X)\n```\n\n----------------------------------------\n\nTITLE: Fitting GradientBoostingClassifier in Python\nDESCRIPTION: This code demonstrates how to fit a gradient boosting classifier using the GradientBoostingClassifier from scikit-learn. It leverages the `make_hastie_10_2` dataset for both binary and multi-class classification tasks. Key parameters like `n_estimators` and `learning_rate` control the number of weak learners and model overfitting, respectively.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import make_hastie_10_2\n>>> from sklearn.ensemble import GradientBoostingClassifier\n\n>>> X, y = make_hastie_10_2(random_state=0)\n>>> X_train, X_test = X[:2000], X[2000:]\n>>> y_train, y_test = y[:2000], y[2000:]\n\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n...     max_depth=1, random_state=0).fit(X_train, y_train)\n>>> clf.score(X_test, y_test)\n```\n\n----------------------------------------\n\nTITLE: Calculating Precision, Recall, and F1 Score in Python using scikit-learn\nDESCRIPTION: This snippet demonstrates how to use scikit-learn's metrics module to calculate precision, recall, and F1 score for a multi-class classification problem. It shows different averaging methods and the use of the fbeta_score function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import metrics\ny_true = [0, 1, 2, 0, 1, 2]\ny_pred = [0, 2, 1, 0, 0, 1]\nmetrics.precision_score(y_true, y_pred, average='macro')\nmetrics.recall_score(y_true, y_pred, average='micro')\nmetrics.f1_score(y_true, y_pred, average='weighted')\nmetrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5)\nmetrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5, average=None)\n```\n\n----------------------------------------\n\nTITLE: Using OneHotEncoder for Categorical Features in Python\nDESCRIPTION: Demonstrates using OneHotEncoder to transform categorical features into binary features using one-hot encoding. This converts each categorical value into a new column with binary values.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n>>> enc = preprocessing.OneHotEncoder()\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n>>> enc.fit(X)\nOneHotEncoder()\n>>> enc.transform([['female', 'from US', 'uses Safari'],\n...                ['male', 'from Europe', 'uses Safari']]).toarray()\narray([[1., 0., 0., 1., 0., 1.],\n       [0., 1., 1., 0., 0., 1.]])\n```\n\n----------------------------------------\n\nTITLE: Creating a ColumnTransformer with StandardScaler and OneHotEncoder - Python\nDESCRIPTION: This snippet demonstrates creating a ColumnTransformer that scales numeric features with StandardScaler and encodes categorical features using OneHotEncoder. It selects the columns to be transformed based on data types using make_column_selector.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.compose import make_column_selector\\nct = ColumnTransformer([\\n      ('scale', StandardScaler(),\\n      make_column_selector(dtype_include=np.number)),\\n      ('onehot',\\n      OneHotEncoder(),\\n      make_column_selector(pattern='city', dtype_include=object))])\\nct.fit_transform(X)\\narray([[ 0.904...,  0.      ,  1. ,  0. ,  0. ],\\n         [-1.507...,  1.414...,  1. ,  0. ,  0. ],\\n         [-0.301...,  0.      ,  0. ,  1. ,  0. ],\\n         [ 0.904..., -1.414...,  0. ,  0. ,  1. ]])\n```\n\n----------------------------------------\n\nTITLE: Incorrect Feature Selection Causing Data Leakage in scikit-learn\nDESCRIPTION: Shows how performing feature selection on the entire dataset before splitting leads to data leakage and artificially inflated performance metrics.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/common_pitfalls.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.feature_selection import SelectKBest\n>>> from sklearn.ensemble import HistGradientBoostingClassifier\n>>> from sklearn.metrics import accuracy_score\n\n>>> # Incorrect preprocessing: the entire data is transformed\n>>> X_selected = SelectKBest(k=25).fit_transform(X, y)\n\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X_selected, y, random_state=42)\n>>> gbc = HistGradientBoostingClassifier(random_state=1)\n>>> gbc.fit(X_train, y_train)\nHistGradientBoostingClassifier(random_state=1)\n\n>>> y_pred = gbc.predict(X_test)\n>>> accuracy_score(y_test, y_pred)\n0.76\n```\n\n----------------------------------------\n\nTITLE: Tracking Feature Names in scikit-learn Pipeline\nDESCRIPTION: Shows how to use the get_feature_names_out() method to track feature names through a Pipeline, including using custom feature names for input data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.feature_selection import SelectKBest\n>>> iris = load_iris()\n>>> pipe = Pipeline(steps=[\n...    ('select', SelectKBest(k=2)),\n...    ('clf', LogisticRegression())])\n>>> pipe.fit(iris.data, iris.target)\nPipeline(steps=[('select', SelectKBest(...)), ('clf', LogisticRegression())])\n>>> pipe[:-1].get_feature_names_out()\narray(['x2', 'x3'], ...)\n>>> pipe[:-1].get_feature_names_out(iris.feature_names)\narray(['petal length (cm)', 'petal width (cm)'], ...)\n```\n\n----------------------------------------\n\nTITLE: Calculating Specificity for Multilabel Classification using Confusion Matrix in Python\nDESCRIPTION: This snippet shows how to calculate specificity (true negative rate) for each class in a multilabel classification problem using the multilabel_confusion_matrix function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n>>> tn / (tn + fp)\narray([1. , 0. , 0.5])\n```\n\n----------------------------------------\n\nTITLE: Creating a Cached Pipeline with PCA and SVM\nDESCRIPTION: Demonstrates creating a scikit-learn pipeline with PCA dimensionality reduction and SVM classifier, using memory caching for improved performance\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncachedir = mkdtemp()\npca2 = PCA(n_components=10)\nsvm2 = SVC()\ncached_pipe = Pipeline([('reduce_dim', pca2), ('clf', svm2)],\n                        memory=cachedir)\ncached_pipe.fit(X_digits, y_digits)\ncached_pipe.named_steps['reduce_dim'].components_.shape\nrmtree(cachedir)\n```\n\n----------------------------------------\n\nTITLE: Using TimeSeriesSplit for Cross-Validation of Time Series Data in Python\nDESCRIPTION: Shows how to use TimeSeriesSplit, a variation of k-fold cross-validation suitable for time series data. It ensures that training data is always prior to test data, which is crucial for time-dependent samples.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/cross_validation.rst#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import TimeSeriesSplit\n\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([1, 2, 3, 4, 5, 6])\ntscv = TimeSeriesSplit(n_splits=3)\nprint(tscv)\nfor train, test in tscv.split(X):\n    print(\"%s %s\" % (train, test))\n```\n\n----------------------------------------\n\nTITLE: Creating a custom scorer with make_scorer in scikit-learn\nDESCRIPTION: This code snippet demonstrates how to create a custom scorer using the `make_scorer` function in scikit-learn. It defines a custom loss function (`my_custom_loss_func`), creates a scorer object with `make_scorer`, trains a `DummyClassifier`, and evaluates it using both the custom loss function and the created scorer. The `greater_is_better` parameter is set to `False` because the custom function represents a loss.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> # score will negate the return value of my_custom_loss_func,\n>>> # which will be np.log(2), 0.693, given the values for X\n>>> # and y defined below.\n>>> score = make_scorer(my_custom_loss_func, greater_is_better=False)\n>>> X = [[1], [1]]\n>>> y = [0, 1]\n>>> from sklearn.dummy import DummyClassifier\n>>> clf = DummyClassifier(strategy='most_frequent', random_state=0)\n>>> clf = clf.fit(X, y)\n>>> my_custom_loss_func(y, clf.predict(X))\n0.69...\n>>> score(clf, X, y)\n-0.69...\n```\n\n----------------------------------------\n\nTITLE: Data Preprocessing with Train-Test Split and Pipeline in Python\nDESCRIPTION: This snippet shows how to properly implement data preprocessing in a machine learning workflow by applying StandardScaler to training data and transforming test data accordingly. It also demonstrates using Pipeline for simplified preprocessing during cross-validation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/cross_validation.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn import preprocessing\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.4, random_state=0)\n>>> scaler = preprocessing.StandardScaler().fit(X_train)\n>>> X_train_transformed = scaler.transform(X_train)\n>>> clf = svm.SVC(C=1).fit(X_train_transformed, y_train)\n>>> X_test_transformed = scaler.transform(X_test)\n>>> clf.score(X_test_transformed, y_test)\n0.9333...\n\n>>> from sklearn.pipeline import make_pipeline\n>>> clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))\n>>> cross_val_score(clf, X, y, cv=cv)\narray([0.977..., 0.933..., 0.955..., 0.933..., 0.977...])\n```\n\n----------------------------------------\n\nTITLE: Pipeline Creation with StandardScaler and LogisticRegression\nDESCRIPTION: Shows how to create a scikit-learn pipeline that combines StandardScaler with LogisticRegression for proper data preprocessing in machine learning workflow.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nX, y = make_classification(random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\npipe = make_pipeline(StandardScaler(), LogisticRegression())\npipe.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Using Pipeline with Cross-Validation in scikit-learn\nDESCRIPTION: Demonstrates how to use a Pipeline with cross-validation to ensure proper data handling during the evaluation process and prevent data leakage.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/common_pitfalls.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.model_selection import cross_val_score\n>>> scores = cross_val_score(pipeline, X, y)\n>>> print(f\"Mean accuracy: {scores.mean():.2f}+/-{scores.std():.2f}\")\nMean accuracy: 0.43+/-0.05\n```\n\n----------------------------------------\n\nTITLE: Implementing DBSCAN Clustering Algorithm\nDESCRIPTION: DBSCAN (Density-Based Spatial Clustering of Applications with Noise) identifies clusters as areas of high density separated by areas of low density. It requires two main parameters: min_samples and eps, which define the density threshold for forming clusters.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Quantile Transformation: Mapping to Normal Distribution\nDESCRIPTION: This snippet demonstrates the use of QuantileTransformer to map data to a normal distribution. It initializes a QuantileTransformer with the output_distribution set to 'normal', applies it to the iris dataset, and then prints the learned quantiles.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> quantile_transformer = preprocessing.QuantileTransformer(\n...     output_distribution='normal', random_state=0)\n>>> X_trans = quantile_transformer.fit_transform(X)\n>>> quantile_transformer.quantiles_\narray([[4.3, 2. , 1. , 0.1],\n       [4.4, 2.2, 1.1, 0.1],\n       [4.4, 2.2, 1.2, 0.1],\n       ...,\n       [7.7, 4.1, 6.7, 2.5],\n       [7.7, 4.2, 6.7, 2.5],\n       [7.9, 4.4, 6.9, 2.5]])\n```\n\n----------------------------------------\n\nTITLE: Implementing a Voting Classifier in scikit-learn\nDESCRIPTION: Shows how to create a majority voting classifier using the VotingClassifier class, combining LogisticRegression, RandomForestClassifier, and GaussianNB models. The example includes evaluating the individual models and the ensemble using cross-validation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn import datasets\n>>> from sklearn.model_selection import cross_val_score\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.ensemble import VotingClassifier\n\n>>> iris = datasets.load_iris()\n>>> X, y = iris.data[:, 1:3], iris.target\n\n>>> clf1 = LogisticRegression(random_state=1)\n>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n>>> clf3 = GaussianNB()\n\n>>> eclf = VotingClassifier(\n...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n...     voting='hard')\n\n>>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n...     scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n...     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\nAccuracy: 0.95 (+/- 0.04) [Logistic Regression]\n```\n\n----------------------------------------\n\nTITLE: Specifying Grid Search Parameter Grid in Python\nDESCRIPTION: Demonstrates how to specify a parameter grid for GridSearchCV, including multiple parameter combinations for different kernels in an SVM.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/grid_search.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nparam_grid = [\n  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n]\n```\n\n----------------------------------------\n\nTITLE: Using ColumnTransformer for Heterogeneous Data Preprocessing\nDESCRIPTION: Demonstrates preprocessing different column types in a DataFrame using ColumnTransformer, applying OneHotEncoder and CountVectorizer to categorical and text columns\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nX = pd.DataFrame({\n    'city': ['London', 'London', 'Paris', 'Sallisaw'],\n    'title': [\"His Last Bow\", \"How Watson Learned the Trick\",\n             \"A Moveable Feast\", \"The Grapes of Wrath\"],\n    'expert_rating': [5, 3, 4, 5],\n    'user_rating': [4, 5, 4, 3]})\n\ncolumn_trans = ColumnTransformer(\n    [('categories', OneHotEncoder(dtype='int'), ['city']),\n     ('title_bow', CountVectorizer(), 'title')],\n    remainder='drop', verbose_feature_names_out=False)\n\ncolumn_trans.fit(X)\ncolumn_trans.transform(X).toarray()\n```\n\n----------------------------------------\n\nTITLE: Using GroupKFold for Cross-Validation with Grouped Data in Python\nDESCRIPTION: This code shows how to use GroupKFold to ensure that groups of samples (e.g., from the same subject) are not split between training and testing sets. The example demonstrates splitting 10 samples from 3 different subjects into 3 folds while keeping each subject's samples together.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/cross_validation.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import GroupKFold\n\nX = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\ny = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"d\", \"d\", \"d\"]\ngroups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n\ngkf = GroupKFold(n_splits=3)\nfor train, test in gkf.split(X, y, groups=groups):\n    print(\"%s %s\" % (train, test))\n[0 1 2 3 4 5] [6 7 8 9]\n[0 1 2 6 7 8 9] [3 4 5]\n[3 4 5 6 7 8 9] [0 1 2]\n```\n\n----------------------------------------\n\nTITLE: Plotting ROC Curve with Scikit-learn's SVC\nDESCRIPTION: This Python snippet demonstrates how to plot a ROC curve using a fitted Support Vector Machine (SVM) classifier and Scikit-learn's `RocCurveDisplay.from_estimator` method. Dependencies include `sklearn.model_selection.train_test_split`, `sklearn.svm.SVC`, `sklearn.metrics.RocCurveDisplay`, and `sklearn.datasets.load_wine`. The snippet splits a wine dataset into training and testing sets, fits an SVM to the training data, and plots a ROC curve based on the test data predictions. Key parameters include the test data `X_test` and `y_test`. Outputs include the `svc_disp` display object with computed ROC AUC, false positive rate (FPR), and true positive rate (TPR).\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/visualizations.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import RocCurveDisplay\nfrom sklearn.datasets import load_wine\n\nX, y = load_wine(return_X_y=True)\ny = y == 2  # make binary\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nsvc = SVC(random_state=42)\nsvc.fit(X_train, y_train)\n\nsvc_disp = RocCurveDisplay.from_estimator(svc, X_test, y_test)\n```\n\n----------------------------------------\n\nTITLE: Hyperparameter Tuning with RandomizedSearchCV in scikit-learn\nDESCRIPTION: Example of using RandomizedSearchCV to automatically search for optimal hyperparameters for a RandomForestRegressor on the California housing dataset. Shows parameter space definition and evaluation of the best model.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/getting_started.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import fetch_california_housing\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> from sklearn.model_selection import RandomizedSearchCV\n>>> from sklearn.model_selection import train_test_split\n>>> from scipy.stats import randint\n...\n>>> X, y = fetch_california_housing(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n...\n>>> # define the parameter space that will be searched over\n>>> param_distributions = {'n_estimators': randint(1, 5),\n...                        'max_depth': randint(5, 10)}\n...\n>>> # now create a searchCV object and fit it to the data\n>>> search = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0),\n...                             n_iter=5,\n...                             param_distributions=param_distributions,\n...                             random_state=0)\n>>> search.fit(X_train, y_train)\nRandomizedSearchCV(estimator=RandomForestRegressor(random_state=0), n_iter=5,\n                     param_distributions={'max_depth': ...,\n                                          'n_estimators': ...},\n                     random_state=0)\n>>> search.best_params_\n{'max_depth': 9, 'n_estimators': 4}\n\n>>> # the search object now acts like a normal random forest estimator\n>>> # with max_depth=9 and n_estimators=4\n>>> search.score(X_test, y_test)\n0.73...\n```\n\n----------------------------------------\n\nTITLE: Quantile Transformation: Percentile Verification\nDESCRIPTION: This snippet verifies that after quantile transformation, the landmark values closely approach the percentiles previously defined, confirming the effect of mapping to a uniform distribution. It builds on the previous example, checking percentiles after the transformation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])\n... # doctest: +SKIP\narray([ 0.00... ,  0.24...,  0.49...,  0.73...,  0.99... ])\n```\n\n----------------------------------------\n\nTITLE: Calculating ROC AUC Score for Binary Classification in Python using scikit-learn\nDESCRIPTION: This example shows how to compute the ROC AUC score for a binary classification problem using scikit-learn. It uses the breast cancer dataset and logistic regression classifier.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.metrics import roc_auc_score\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> clf = LogisticRegression(solver=\"liblinear\").fit(X, y)\n>>> clf.classes_\narray([0, 1])\n\n>>> y_score = clf.predict_proba(X)[:, 1]\n>>> roc_auc_score(y, y_score)\n0.99...\n\n>>> roc_auc_score(y, clf.decision_function(X))\n0.99...\n```\n\n----------------------------------------\n\nTITLE: Loading and Splitting Iris Dataset for SVM Classification in Python\nDESCRIPTION: This code snippet demonstrates how to load the Iris dataset, split it into training and test sets using train_test_split, and train a linear SVM classifier to evaluate its performance.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/cross_validation.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn import datasets\n>>> from sklearn import svm\n\n>>> X, y = datasets.load_iris(return_X_y=True)\n>>> X.shape, y.shape\n((150, 4), (150,))\n\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.4, random_state=0)\n\n>>> X_train.shape, y_train.shape\n((90, 4), (90,))\n>>> X_test.shape, y_test.shape\n((60, 4), (60,))\n\n>>> clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n>>> clf.score(X_test, y_test)\n0.96...\n```\n\n----------------------------------------\n\nTITLE: MinMaxScaler Implementation Example\nDESCRIPTION: Demonstrates using MinMaxScaler to scale features to a specific range (typically [0,1]). Shows fitting the scaler and transforming both training and test data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nX_train = np.array([[ 1., -1.,  2.],\n                   [ 2.,  0.,  0.],\n                   [ 0.,  1., -1.]])\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_train_minmax = min_max_scaler.fit_transform(X_train)\nX_test = np.array([[-3., -1.,  4.]])\nX_test_minmax = min_max_scaler.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Feature Scaling with StandardScaler in scikit-learn\nDESCRIPTION: Example showing how to properly scale features using StandardScaler for MLP training and testing data. Demonstrates the correct workflow of fitting on training data and applying the same transformation to test data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neural_networks_supervised.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# Don't cheat - fit only on training data\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\n# apply same transformation to test data\nX_test = scaler.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Using d2_absolute_error_score in Python with scikit-learn\nDESCRIPTION: Demonstrates the usage of d2_absolute_error_score function from scikit-learn to evaluate regression models based on mean absolute error.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_54\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.metrics import d2_absolute_error_score\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nd2_absolute_error_score(y_true, y_pred)\n0.764...\ny_true = [1, 2, 3]\ny_pred = [1, 2, 3]\nd2_absolute_error_score(y_true, y_pred)\n1.0\ny_true = [1, 2, 3]\ny_pred = [2, 2, 2]\nd2_absolute_error_score(y_true, y_pred)\n0.0\n```\n\n----------------------------------------\n\nTITLE: Using CalibratedClassifierCV in Scikit-Learn\nDESCRIPTION: This code snippet illustrates how to use the CalibratedClassifierCV class to calibrate a classifier. It uses cross-validation to ensure unbiased data for fitting the calibrator.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/calibration.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.calibration import CalibratedClassifierCV\n\ncalibrated_clf = CalibratedClassifierCV(base_estimator=clf, cv=5, method='sigmoid')\ncalibrated_clf.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Creating and Using a Pipeline in scikit-learn\nDESCRIPTION: Example of creating a machine learning pipeline that combines StandardScaler and LogisticRegression. The pipeline is fitted on training data from the Iris dataset and evaluated on test data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/getting_started.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.metrics import accuracy_score\n...\n>>> # create a pipeline object\n>>> pipe = make_pipeline(\n...     StandardScaler(),\n...     LogisticRegression()\n... )\n...\n>>> # load the iris dataset and split it into train and test sets\n>>> X, y = load_iris(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n...\n>>> # fit the whole pipeline\n>>> pipe.fit(X_train, y_train)\nPipeline(steps=[('standardscaler', StandardScaler()),\n                  ('logisticregression', LogisticRegression())])\n>>> # we can now use it like any other estimator\n>>> accuracy_score(pipe.predict(X_test), y_test)\n0.97...\n```\n\n----------------------------------------\n\nTITLE: Creating a Polynomial Regression Pipeline in Python\nDESCRIPTION: This code creates a Pipeline object that combines PolynomialFeatures transformation with LinearRegression. It fits the model to third-degree polynomial data and retrieves the learned coefficients.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\nmodel = Pipeline([('poly', PolynomialFeatures(degree=3)),\n                  ('linear', LinearRegression(fit_intercept=False))])\n# fit to an order-3 polynomial data\nx = np.arange(5)\ny = 3 - 2 * x + x ** 2 - x ** 3\nmodel = model.fit(x[:, np.newaxis], y)\nmodel.named_steps['linear'].coef_\n```\n\n----------------------------------------\n\nTITLE: Calculating Miss Rate for Multilabel Classification using Confusion Matrix in Python\nDESCRIPTION: This snippet shows how to calculate miss rate (false negative rate) for each class in a multilabel classification problem using the multilabel_confusion_matrix function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n>>> fn / (fn + tp)\narray([0. , 0.5, 1. ])\n```\n\n----------------------------------------\n\nTITLE: Tree-based Feature Selection with ExtraTreesClassifier\nDESCRIPTION: Demonstrates how to use ExtraTreesClassifier with SelectFromModel to perform feature selection based on feature importance scores. The example loads the iris dataset, fits the classifier, and transforms the data to keep only the most important features.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_selection.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.ensemble import ExtraTreesClassifier\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.feature_selection import SelectFromModel\n>>> X, y = load_iris(return_X_y=True)\n>>> X.shape\n(150, 4)\n>>> clf = ExtraTreesClassifier(n_estimators=50)\n>>> clf = clf.fit(X, y)\n>>> clf.feature_importances_  # doctest: +SKIP\narray([ 0.04...,  0.05...,  0.4...,  0.4...])\n>>> model = SelectFromModel(clf, prefit=True)\n>>> X_new = model.transform(X)\n>>> X_new.shape               # doctest: +SKIP\n(150, 2)\n```\n\n----------------------------------------\n\nTITLE: Using Pipeline for Feature Selection and Model Training\nDESCRIPTION: Shows how to use a Pipeline to combine feature selection with model training, ensuring that test data is only used for evaluation and not for fitting preprocessing steps.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/common_pitfalls.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.pipeline import make_pipeline\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=42)\n>>> pipeline = make_pipeline(SelectKBest(k=25),\n...                          HistGradientBoostingClassifier(random_state=1))\n>>> pipeline.fit(X_train, y_train)\nPipeline(steps=[('selectkbest', SelectKBest(k=25)),\n                    ('histgradientboostingclassifier',\n                     HistGradientBoostingClassifier(random_state=1))])\n\n>>> y_pred = pipeline.predict(X_test)\n>>> accuracy_score(y_test, y_pred)\n0.5\n```\n\n----------------------------------------\n\nTITLE: Weighted Scoring and Fitting with Metadata Routing (Python)\nDESCRIPTION: Demonstrates using cross_validate with LogisticRegressionCV and a custom scorer, both configured to use sample weights and groups passed as metadata.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/metadata_routing.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> weighted_acc = make_scorer(accuracy_score).set_score_request(sample_weight=True)\n>>> lr = LogisticRegressionCV(\n...     cv=GroupKFold(),\n...     scoring=weighted_acc\n... ).set_fit_request(sample_weight=True)\n>>> cv_results = cross_validate(\n...     lr,\n...     X,\n...     y,\n...     params={\"sample_weight\": my_weights, \"groups\": my_groups},\n...     cv=GroupKFold(),\n...     scoring=weighted_acc,\n... )\n```\n\n----------------------------------------\n\nTITLE: Using cross_val_score for K-fold Cross-Validation in Python\nDESCRIPTION: This snippet shows how to implement K-fold cross-validation using scikit-learn's cross_val_score function. It evaluates a linear SVM on the Iris dataset using 5-fold cross-validation and computes accuracy and F1 scores.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/cross_validation.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.model_selection import cross_val_score\n>>> clf = svm.SVC(kernel='linear', C=1, random_state=42)\n>>> scores = cross_val_score(clf, X, y, cv=5)\n>>> scores\narray([0.96..., 1. , 0.96..., 0.96..., 1. ])\n\n>>> print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n0.98 accuracy with a standard deviation of 0.02\n\n>>> from sklearn import metrics\n>>> scores = cross_val_score(\n...     clf, X, y, cv=5, scoring='f1_macro')\n>>> scores\narray([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline in scikit-learn\nDESCRIPTION: Demonstrates how to create a Pipeline object by combining PCA and SVC estimators. The Pipeline allows chaining multiple estimators that can be treated as a single estimator.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.svm import SVC\n>>> from sklearn.decomposition import PCA\n>>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n>>> pipe = Pipeline(estimators)\n>>> pipe\nPipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])\n```\n\n----------------------------------------\n\nTITLE: Using OneHotEncoder with drop Parameter in Python\nDESCRIPTION: Demonstrates how to use OneHotEncoder with the drop parameter to avoid co-linearity by dropping one category for each feature, useful for some machine learning algorithms like non-regularized regression.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n>>> X = [['male', 'from US', 'uses Safari'],\n...      ['female', 'from Europe', 'uses Firefox']]\n>>> drop_enc = preprocessing.OneHotEncoder(drop='first').fit(X)\n>>> drop_enc.categories_\n[array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object),\n array(['uses Firefox', 'uses Safari'], dtype=object)]\n>>> drop_enc.transform(X).toarray()\narray([[1., 1., 1.],\n       [0., 0., 0.]])\n```\n\n----------------------------------------\n\nTITLE: Discretizing Features with KBinsDiscretizer in Python\nDESCRIPTION: This snippet demonstrates how to use KBinsDiscretizer to discretize continuous features into bins. It shows the transformation of a sample input array into ordinal encoded bins.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n>>> X = np.array([[ -3., 5., 15 ],\n...               [  0., 6., 14 ],\n...               [  6., 3., 11 ]])\n>>> est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal').fit(X)\n>>> est.transform(X)                      # doctest: +SKIP\narray([[ 0., 1., 1.],\n       [ 1., 1., 1.],\n       [ 2., 0., 0.]])\n```\n\n----------------------------------------\n\nTITLE: Converting Dictionary Features to Vectors using DictVectorizer in Python\nDESCRIPTION: This example demonstrates how to use DictVectorizer to convert dictionary-based feature representations into numerical arrays suitable for machine learning. It shows one-hot encoding of categorical features alongside numerical features.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmeasurements = [\n    {'city': 'Dubai', 'temperature': 33.},\n    {'city': 'London', 'temperature': 12.},\n    {'city': 'San Francisco', 'temperature': 18.},\n]\n\nfrom sklearn.feature_extraction import DictVectorizer\nvec = DictVectorizer()\n\nvec.fit_transform(measurements).toarray()\narray([[ 1.,  0.,  0., 33.],\n       [ 0.,  1.,  0., 12.],\n       [ 0.,  0.,  1., 18.]])\n\nvec.get_feature_names_out()\narray(['city=Dubai', 'city=London', 'city=San Francisco', 'temperature'], ...)\n```\n\n----------------------------------------\n\nTITLE: Imputing Missing Values with SimpleImputer in Python\nDESCRIPTION: This snippet demonstrates how to use SimpleImputer to replace missing values (encoded as np.nan) with the mean value of each column containing missing values.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/impute.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp.fit([[1, 2], [np.nan, 3], [7, 6]])\nX = [[np.nan, 2], [6, np.nan], [7, 6]]\nprint(imp.transform(X))\n```\n\n----------------------------------------\n\nTITLE: Calculating Hinge Loss for Multiclass SVM Classification in Python\nDESCRIPTION: This example shows how to use the hinge_loss function with a LinearSVC classifier for a multiclass classification problem. It includes model fitting, decision function calculation, and hinge loss computation with specified labels.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n>>> X = np.array([[0], [1], [2], [3]])\n>>> Y = np.array([0, 1, 2, 3])\n>>> labels = np.array([0, 1, 2, 3])\n>>> est = svm.LinearSVC()\n>>> est.fit(X, Y)\nLinearSVC()\n>>> pred_decision = est.decision_function([[-1], [2], [3]])\n>>> y_true = [0, 2, 3]\n>>> hinge_loss(y_true, pred_decision, labels=labels)\n0.56...\n```\n\n----------------------------------------\n\nTITLE: Calculating ROC Curve in Python using scikit-learn\nDESCRIPTION: This snippet demonstrates how to use the roc_curve function from scikit-learn to compute the receiver operating characteristic curve. It requires true binary values and target scores as inputs.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn.metrics import roc_curve\n>>> y = np.array([1, 1, 2, 2])\n>>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)\n>>> fpr\narray([0. , 0. , 0.5, 0.5, 1. ])\n>>> tpr\narray([0. , 0.5, 0.5, 1. , 1. ])\n>>> thresholds\narray([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\n```\n\n----------------------------------------\n\nTITLE: OneHotEncoder with Missing Values in Python\nDESCRIPTION: Shows how OneHotEncoder treats missing values (None or np.nan) as additional categories, allowing for encoding of datasets with missing categorical values.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n>>> X = [['male', 'Safari'],\n...      ['female', None],\n...      [np.nan, 'Firefox']]\n>>> enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\n>>> enc.categories_\n[array(['female', 'male', nan], dtype=object),\narray(['Firefox', 'Safari', None], dtype=object)]\n>>> enc.transform(X).toarray()\narray([[0., 1., 0., 0., 1., 0.],\n      [1., 0., 0., 0., 0., 1.]])\n```\n\n----------------------------------------\n\nTITLE: Ridge Regression with Cross-Validation in scikit-learn\nDESCRIPTION: This snippet demonstrates how to use Ridge regression with built-in cross-validation to find the optimal alpha parameter. It shows how to instantiate a RidgeCV model, fit it to data, and access the optimal alpha value selected by cross-validation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn import linear_model\n>>> reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n>>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\nRidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n          1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))\n>>> reg.alpha_\nnp.float64(0.01)\n```\n\n----------------------------------------\n\nTITLE: Using Pipeline with GridSearchCV in Scikit-learn\nDESCRIPTION: Demonstrates how to create a pipeline combining feature selection and a model, then performing grid search over the pipeline parameters. Shows nested parameter specification using double underscores.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/grid_search.rst#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest\npipe = Pipeline([\n   ('select', SelectKBest()),\n   ('model', calibrated_forest)])\nparam_grid = {\n   'select__k': [1, 2],\n   'model__estimator__max_depth': [2, 4, 6, 8]}\nsearch = GridSearchCV(pipe, param_grid, cv=5).fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Creating Scaled SVM Classifier with Pipeline\nDESCRIPTION: Demonstrates how to create an SVM classifier with automatic data scaling using scikit-learn's Pipeline. Shows the recommended approach for preprocessing data before training an SVM model to ensure scale invariance.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/svm.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.svm import SVC\n>>> clf = make_pipeline(StandardScaler(), SVC())\n```\n\n----------------------------------------\n\nTITLE: Ridge Regression with scikit-learn\nDESCRIPTION: This snippet demonstrates how to use Ridge regression with scikit-learn.  It shows how to instantiate a Ridge model, fit it to data, and access the coefficients and intercept. The `alpha` parameter controls the strength of the regularization.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn import linear_model\n>>> reg = linear_model.Ridge(alpha=.5)\n>>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\nRidge(alpha=0.5)\n>>> reg.coef_\narray([0.34545455, 0.34545455])\n>>> reg.intercept_\nnp.float64(0.13636...)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multioutput Regression with Scikit-Learn Python\nDESCRIPTION: This code snippet illustrates how to perform multioutput regression using Scikit-Learn's `MultiOutputRegressor` with `GradientBoostingRegressor`. It demonstrates the creation of a regression dataset with multiple targets and how to fit and predict using the regressor. The required dependencies include Scikit-Learn for regression and dataset creation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/multiclass.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.multioutput import MultiOutputRegressor\n>>> from sklearn.ensemble import GradientBoostingRegressor\n>>> X, y = make_regression(n_samples=10, n_targets=3, random_state=1)\n>>> MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X, y).predict(X)\narray([[-154.75474165, -147.03498585,  -50.03812219],\n         [   7.12165031,    5.12914884,  -81.46081961],\n         [-187.8948621 , -100.44373091,   13.88978285],\n         [-141.62745778,   95.02891072, -191.48204257],\n         [  97.03260883,  165.34867495,  139.52003279],\n         [ 123.92529176,   21.25719016,   -7.84253   ],\n         [-122.25193977,  -85.16443186, -107.12274212],\n         [ -30.170388  ,  -94.80956739,   12.16979946],\n         [ 140.72667194,  176.50941682,  -17.50447799],\n         [ 149.37967282,  -81.15699552,   -5.72850319]])\n```\n\n----------------------------------------\n\nTITLE: Basic Cross Validation Scoring Example\nDESCRIPTION: Demonstrates using string-based scoring with cross validation on the iris dataset using an SVM classifier.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import cross_val_score\nX, y = datasets.load_iris(return_X_y=True)\nclf = svm.SVC(random_state=0)\ncross_val_score(clf, X, y, cv=5, scoring='recall_macro')\n```\n\n----------------------------------------\n\nTITLE: Using R² Score in Python\nDESCRIPTION: Example demonstrating how to use the r2_score function to compute the coefficient of determination, which represents the proportion of variance explained by the model. Various examples show different multioutput handling options.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_43\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.metrics import r2_score\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> r2_score(y_true, y_pred)\n0.948...\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> r2_score(y_true, y_pred, multioutput='variance_weighted')\n0.938...\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> r2_score(y_true, y_pred, multioutput='uniform_average')\n0.936...\n>>> r2_score(y_true, y_pred, multioutput='raw_values')\narray([0.965..., 0.908...])\n>>> r2_score(y_true, y_pred, multioutput=[0.3, 0.7])\n0.925...\n>>> y_true = [-2, -2, -2]\n>>> y_pred = [-2, -2, -2]\n>>> r2_score(y_true, y_pred)\n1.0\n>>> r2_score(y_true, y_pred, force_finite=False)\nnan\n>>> y_true = [-2, -2, -2]\n>>> y_pred = [-2, -2, -2 + 1e-8]\n>>> r2_score(y_true, y_pred)\n0.0\n>>> r2_score(y_true, y_pred, force_finite=False)\n-inf\n```\n\n----------------------------------------\n\nTITLE: Accessing Feature Importance in Gradient Boosting Classifier\nDESCRIPTION: This snippet demonstrates how to create a GradientBoostingClassifier model, fit it on synthetic data, and then access the feature importance scores through the feature_importances_ property.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import make_hastie_10_2\n>>> from sklearn.ensemble import GradientBoostingClassifier\n\n>>> X, y = make_hastie_10_2(random_state=0)\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n...     max_depth=1, random_state=0).fit(X, y)\n>>> clf.feature_importances_\narray([0.10..., 0.10..., 0.11..., ...\n```\n\n----------------------------------------\n\nTITLE: Feature Binarization with Binarizer in Python\nDESCRIPTION: This snippet illustrates the use of Binarizer to threshold numerical features into boolean values. It shows how to apply binarization with default and custom thresholds.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n>>> X = [[ 1., -1.,  2.],\n...      [ 2.,  0.,  0.],\n...      [ 0.,  1., -1.]]\n\n>>> binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\n>>> binarizer\nBinarizer()\n\n>>> binarizer.transform(X)\narray([[1., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.]])\n\n>>> binarizer = preprocessing.Binarizer(threshold=1.1)\n>>> binarizer.transform(X)\narray([[0., 0., 1.],\n       [1., 0., 0.],\n       [0., 0., 0.]])\n```\n\n----------------------------------------\n\nTITLE: Splitting Train and Test Sets Using GroupShuffleSplit in Python\nDESCRIPTION: Demonstrates how to use GroupShuffleSplit to split a dataset into training and testing subsets while respecting group structure. This is particularly useful when standard train_test_split cannot account for groups.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/cross_validation.rst#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom sklearn.model_selection import GroupShuffleSplit\n\nX = np.array([0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001])\ny = np.array([\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"a\"])\ngroups = np.array([1, 1, 2, 2, 3, 3, 4, 4])\ntrain_indx, test_indx = next(\n    GroupShuffleSplit(random_state=7).split(X, y, groups)\n)\nX_train, X_test, y_train, y_test = \\\n    X[train_indx], X[test_indx], y[train_indx], y[test_indx]\nX_train.shape, X_test.shape\nnp.unique(groups[train_indx]), np.unique(groups[test_indx])\n```\n\n----------------------------------------\n\nTITLE: Feature Selection Pipeline Implementation\nDESCRIPTION: Shows how to integrate feature selection into a scikit-learn pipeline using LinearSVC for feature selection followed by RandomForestClassifier for classification. This approach combines feature selection as a preprocessing step with the actual learning algorithm.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_selection.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclf = Pipeline([\n  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))),\n  ('classification', RandomForestClassifier())\n])\nclf.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Computing Precision-Recall Curve and Average Precision Score\nDESCRIPTION: Demonstrates how to calculate precision-recall curve points and average precision score from prediction scores in scikit-learn. This is useful for evaluating classifier performance at different decision thresholds.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\ny_true = np.array([0, 0, 1, 1])\ny_scores = np.array([0.1, 0.4, 0.35, 0.8])\nprecision, recall, threshold = precision_recall_curve(y_true, y_scores)\nprecision\narray([0.5       , 0.66..., 0.5       , 1.        , 1.        ])\nrecall\narray([1. , 1. , 0.5, 0.5, 0. ])\nthreshold\narray([0.1 , 0.35, 0.4 , 0.8 ])\naverage_precision_score(y_true, y_scores)\n0.83...\n```\n\n----------------------------------------\n\nTITLE: Using HalvingRandomSearchCV and HalvingGridSearchCV in Python\nDESCRIPTION: New parameter search estimators implementing Successive Halving, which can be used as drop-in replacements for RandomizedSearchCV and GridSearchCV.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.model_selection import HalvingRandomSearchCV, HalvingGridSearchCV\n```\n\n----------------------------------------\n\nTITLE: Correctly Applying Preprocessing to Test Data in scikit-learn\nDESCRIPTION: Shows the correct approach of transforming the test data with the same scaler that was fit on the training data, resulting in better model performance.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/common_pitfalls.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> X_test_transformed = scaler.transform(X_test)\n>>> mean_squared_error(y_test, model.predict(X_test_transformed))\n0.90...\n```\n\n----------------------------------------\n\nTITLE: Basic SVM Classification Example with scikit-learn\nDESCRIPTION: Demonstrates how to create and train a basic SVM classifier using scikit-learn's SVC class. Shows initialization, fitting with training data, and making predictions.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/svm.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import svm\nX = [[0, 0], [1, 1]]\ny = [0, 1]\nclf = svm.SVC()\nclf.fit(X, y)\n\n```\n\nLANGUAGE: python\nCODE:\n```\nclf.predict([[2., 2.]])\n```\n\n----------------------------------------\n\nTITLE: Multivariate Imputation with IterativeImputer in Python\nDESCRIPTION: This example shows how to use IterativeImputer for multivariate imputation, which models each feature with missing values as a function of other features.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/impute.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=10, random_state=0)\nimp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\nX_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\nprint(np.round(imp.transform(X_test)))\n```\n\n----------------------------------------\n\nTITLE: Creating ColumnTransformer Easily with make_column_transformer - Python\nDESCRIPTION: This snippet demonstrates using the make_column_transformer utility to create a ColumnTransformer object more easily. It automatically names the transformers and allows for concise syntax.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.compose import make_column_transformer\\ncolumn_trans = make_column_transformer(\\n     (OneHotEncoder(), ['city']),\\n     (CountVectorizer(), 'title'),\\n     remainder=MinMaxScaler())\\ncolumn_trans\\nColumnTransformer(remainder=MinMaxScaler(),\\n                  transformers=[('onehotencoder', OneHotEncoder(), ['city']),\\n                                ('countvectorizer', CountVectorizer(),\\n                                 'title')])\n```\n\n----------------------------------------\n\nTITLE: Comparing SVC and DummyClassifier Performance in Python\nDESCRIPTION: This code snippet compares the performance of a Support Vector Machine classifier with a linear kernel against a dummy classifier that always predicts the most frequent class. It demonstrates how to create, train, and evaluate both models on the same dataset.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.svm import SVC\nclf = SVC(kernel='linear', C=1).fit(X_train, y_train)\nclf.score(X_test, y_test)\n0.63...\nclf = DummyClassifier(strategy='most_frequent', random_state=0)\nclf.fit(X_train, y_train)\nDummyClassifier(random_state=0, strategy='most_frequent')\nclf.score(X_test, y_test)\n0.57...\n```\n\n----------------------------------------\n\nTITLE: Using VarianceThreshold for Feature Selection in Python\nDESCRIPTION: This example demonstrates how to use VarianceThreshold to remove features with low variance. In this case, it removes boolean features that are either 0 or 1 in more than 80% of samples by setting a threshold based on Bernoulli variance formula.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_selection.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.feature_selection import VarianceThreshold\nX = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\nsel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nsel.fit_transform(X)\n```\n\n----------------------------------------\n\nTITLE: Caching Transformers in scikit-learn Pipeline\nDESCRIPTION: Shows how to enable caching for transformers in a Pipeline to avoid repeated computation. This is useful for expensive transformations, especially in grid searches.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from tempfile import mkdtemp\n>>> from shutil import rmtree\n>>> from sklearn.decomposition import PCA\n>>> from sklearn.svm import SVC\n>>> from sklearn.pipeline import Pipeline\n>>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n>>> cachedir = mkdtemp()\n>>> pipe = Pipeline(estimators, memory=cachedir)\n>>> pipe\nPipeline(memory=...,\n         steps=[('reduce_dim', PCA()), ('clf', SVC())])\n>>> # Clear the cache directory when you don't need it anymore\n>>> rmtree(cachedir)\n```\n\n----------------------------------------\n\nTITLE: Searching Nested Estimator Parameters with GridSearchCV in scikit-learn\nDESCRIPTION: Example demonstrating how to search over parameters of nested estimators like CalibratedClassifierCV using the <estimator>__<parameter> syntax in the param_grid.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/grid_search.rst#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.model_selection import GridSearchCV\n>>> from sklearn.calibration import CalibratedClassifierCV\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.datasets import make_moons\n>>> X, y = make_moons()\n>>> calibrated_forest = CalibratedClassifierCV(\n...    estimator=RandomForestClassifier(n_estimators=10))\n>>> param_grid = {\n...    'estimator__max_depth': [2, 4, 6, 8]}\n>>> search = GridSearchCV(calibrated_forest, param_grid, cv=5)\n>>> search.fit(X, y)\nGridSearchCV(cv=5,\n               estimator=CalibratedClassifierCV(estimator=RandomForestClassifier(n_estimators=10)),\n               param_grid={'estimator__max_depth': [2, 4, 6, 8]})\n```\n\n----------------------------------------\n\nTITLE: Generating a confusion matrix in Python with scikit-learn\nDESCRIPTION: Example demonstrating how to compute a confusion matrix using scikit-learn. A confusion matrix shows the counts of true vs. predicted class assignments, with rows corresponding to true classes and columns to predicted classes.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics import confusion_matrix\ny_true = [2, 0, 2, 2, 0, 1]\ny_pred = [0, 0, 2, 2, 0, 2]\nconfusion_matrix(y_true, y_pred)\narray([[2, 0, 0],\n       [0, 0, 1],\n       [1, 0, 2]])\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline with StandardScaler and SGDClassifier in Python\nDESCRIPTION: Example demonstrating how to use scikit-learn's pipeline functionality to combine feature scaling and SGD classification. This approach is more elegant and prevents errors in the preprocessing workflow.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/sgd.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Or better yet: use a pipeline!\nfrom sklearn.pipeline import make_pipeline\nest = make_pipeline(StandardScaler(), SGDClassifier())\nest.fit(X_train)\nest.predict(X_test)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-layer StackingRegressor in Python with scikit-learn\nDESCRIPTION: This snippet demonstrates how to create and use a multi-layer StackingRegressor for ensemble learning. It includes defining estimators for different layers, fitting the model, and evaluating its performance using R2 score.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfinal_layer_rfr = RandomForestRegressor(\n    n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)\nfinal_layer = StackingRegressor(\n    estimators=[('rf', final_layer_rfr),\n                ('gbrt', final_layer_gbr)],\n    final_estimator=RidgeCV()\n    )\nmulti_layer_regressor = StackingRegressor(\n    estimators=[('ridge', RidgeCV()),\n                ('lasso', LassoCV(random_state=42)),\n                ('knr', KNeighborsRegressor(n_neighbors=20,\n                                            metric='euclidean'))],\n    final_estimator=final_layer\n)\nmulti_layer_regressor.fit(X_train, y_train)\nStackingRegressor(...)\nprint('R2 score: {:.2f}'\n      .format(multi_layer_regressor.score(X_test, y_test)))\nR2 score: 0.53\n```\n\n----------------------------------------\n\nTITLE: Prediction Using Trained Decision Tree Classifier\nDESCRIPTION: Shows how to use a trained decision tree classifier to make predictions on new data and get probability estimates.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/tree.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> clf.predict([[2., 2.]])\narray([1])\n>>> clf.predict_proba([[2., 2.]])\narray([[0., 1.]])\n```\n\n----------------------------------------\n\nTITLE: Initializing and fitting SGDClassifier\nDESCRIPTION: This snippet demonstrates how to initialize and fit an SGDClassifier with example data. It imports the SGDClassifier class, defines sample data X and y, initializes the classifier with hinge loss, L2 penalty, and a maximum of 5 iterations, and then fits the model to the data. The model is then ready to be used for prediction.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/sgd.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.linear_model import SGDClassifier\n>>> X = [[0., 0.], [1., 1.]]\n>>> y = [0, 1]\n>>> clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n>>> clf.fit(X, y)\nSGDClassifier(max_iter=5)\n```\n\n----------------------------------------\n\nTITLE: Calling LogisticRegression with LBFGS Solver in Python\nDESCRIPTION: Example of using LogisticRegression with the LBFGS solver, which now has improved convergence and precision in scikit-learn 1.4.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.4.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nLogisticRegression(solver=\"lbfgs\", tol=1e-5).fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Specifying Randomized Search Parameter Distributions in Python\nDESCRIPTION: Shows how to specify parameter distributions for RandomizedSearchCV, including continuous distributions from scipy.stats and discrete choices.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/grid_search.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n{'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),\n  'kernel': ['rbf'], 'class_weight':['balanced', None]}\n```\n\n----------------------------------------\n\nTITLE: K-Nearest Neighbors Imputation with KNNImputer in Python\nDESCRIPTION: This snippet demonstrates how to use KNNImputer to replace missing values using the k-Nearest Neighbors approach, with the mean feature value of the two nearest neighbors.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/impute.rst#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nnan = np.nan\nX = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\nimputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\nimputer.fit_transform(X)\n```\n\n----------------------------------------\n\nTITLE: Using Precomputed Kernels with Gram Matrix-Python\nDESCRIPTION: This code shows how to use a precomputed kernel by passing a Gram matrix to SVM's 'fit' and 'predict' methods using scikit-learn. It illustrates kernel computation for training and test data sets. Critical dependencies include 'numpy' for matrix operations, and 'svm.SVC()' to specify 'precomputed' as the 'kernel' type.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/svm.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn import svm\n>>> X, y = make_classification(n_samples=10, random_state=0)\n>>> X_train , X_test , y_train, y_test = train_test_split(X, y, random_state=0)\n>>> clf = svm.SVC(kernel='precomputed')\n>>> # linear kernel computation\n>>> gram_train = np.dot(X_train, X_train.T)\n>>> clf.fit(gram_train, y_train)\nSVC(kernel='precomputed')\n>>> # predict on training examples\n>>> gram_test = np.dot(X_test, X_train.T)\n>>> clf.predict(gram_test)\narray([0, 1, 0])\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using a Soft Voting Classifier in Python\nDESCRIPTION: This snippet demonstrates how to create and use a VotingClassifier with soft voting in scikit-learn. It combines a Decision Tree, K-Nearest Neighbors, and Support Vector Machine classifiers.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom itertools import product\nfrom sklearn.ensemble import VotingClassifier\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0, 2]]\ny = iris.target\n\n# Training classifiers\nclf1 = DecisionTreeClassifier(max_depth=4)\nclf2 = KNeighborsClassifier(n_neighbors=7)\nclf3 = SVC(kernel='rbf', probability=True)\neclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],\n                        voting='soft', weights=[2, 1, 2])\n\nclf1 = clf1.fit(X, y)\nclf2 = clf2.fit(X, y)\nclf3 = clf3.fit(X, y)\neclf = eclf.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Basic Histogram Gradient Boosting Classification Example\nDESCRIPTION: Demonstrates how to use HistGradientBoostingClassifier for binary classification using the make_hastie_10_2 dataset. Shows basic model initialization, training and evaluation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.ensemble import HistGradientBoostingClassifier\n>>> from sklearn.datasets import make_hastie_10_2\n\n>>> X, y = make_hastie_10_2(random_state=0)\n>>> X_train, X_test = X[:2000], X[2000:]\n>>> y_train, y_test = y[:2000], y[2000:]\n\n>>> clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n>>> clf.score(X_test, y_test)\n0.8965\n```\n\n----------------------------------------\n\nTITLE: Tuning Decision Threshold with Custom Scorer in Python\nDESCRIPTION: This example demonstrates how to use TunedThresholdClassifierCV with a custom scorer to optimize the decision threshold. It uses LogisticRegression as the base model and tunes the threshold to maximize the F1 score for a specific positive label.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/classification_threshold.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.model_selection import TunedThresholdClassifierCV\n>>> from sklearn.metrics import make_scorer, f1_score\n>>> X, y = make_classification(\n...   n_samples=1_000, weights=[0.1, 0.9], random_state=0)\n>>> pos_label = 0\n>>> scorer = make_scorer(f1_score, pos_label=pos_label)\n>>> base_model = LogisticRegression()\n>>> model = TunedThresholdClassifierCV(base_model, scoring=scorer)\n>>> scorer(model.fit(X, y), X, y)\n0.88...\n>>> # compare it with the internal score found by cross-validation\n>>> model.best_score_\nnp.float64(0.86...)\n```\n\n----------------------------------------\n\nTITLE: Initializing SVR Regression Model in Scikit-learn\nDESCRIPTION: Demonstrates how to create and fit a Support Vector Regression (SVR) model with a simple dataset. Shows the basic workflow of creating an SVR instance, fitting it with training data, and performing a prediction.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/svm.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn import svm\n>>> X = [[0, 0], [2, 2]]\n>>> y = [0.5, 2.5]\n>>> regr = svm.SVR()\n>>> regr.fit(X, y)\nSVR()\n>>> regr.predict([[1, 1]])\narray([1.5])\n```\n\n----------------------------------------\n\nTITLE: Calculating Accuracy Score in Scikit-learn\nDESCRIPTION: This example demonstrates how to calculate the accuracy score using the `accuracy_score` function from `sklearn.metrics`. The example shows calculating the accuracy score both as a fraction (default) and as a count (by setting `normalize=False`).\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn.metrics import accuracy_score\n>>> y_pred = [0, 2, 1, 3]\n>>> y_true = [0, 1, 2, 3]\n>>> accuracy_score(y_true, y_pred)\n0.5\n>>> accuracy_score(y_true, y_pred, normalize=False)\n2.0\n```\n\n----------------------------------------\n\nTITLE: Using FeatureHasher with Custom Feature Generator in Python\nDESCRIPTION: This snippet shows how to use FeatureHasher with a custom feature generator function. It demonstrates lazy feature extraction using generator comprehensions and transformation to sparse matrices for efficient processing.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nraw_X = (token_features(tok, pos_tagger(tok)) for tok in corpus)\n\nhasher = FeatureHasher(input_type='string')\nX = hasher.transform(raw_X)\n```\n\n----------------------------------------\n\nTITLE: Using SimpleImputer with Sparse Matrices in Python\nDESCRIPTION: This example shows how to use SimpleImputer with sparse matrices, replacing missing values (encoded as -1) with the mean value of each column.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/impute.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport scipy.sparse as sp\nX = sp.csc_matrix([[1, 2], [0, -1], [8, 4]])\nimp = SimpleImputer(missing_values=-1, strategy='mean')\nimp.fit(X)\nX_test = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]])\nprint(imp.transform(X_test).toarray())\n```\n\n----------------------------------------\n\nTITLE: TfidfVectorizer Usage Example\nDESCRIPTION: Shows how to use TfidfVectorizer, which combines CountVectorizer and TfidfTransformer functionality in a single class for text feature extraction.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> vectorizer = TfidfVectorizer()\n>>> vectorizer.fit_transform(corpus)\n<Compressed Sparse...dtype 'float64'\n  with 19 stored elements and shape (4, 9)>\n```\n\n----------------------------------------\n\nTITLE: Feature Scaling with StandardScaler for SGD Models in Python\nDESCRIPTION: Example showing how to properly scale features for SGD models using StandardScaler. The code demonstrates fitting the scaler on training data only and applying the same transformation to test data to avoid data leakage.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/sgd.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)  # Don't cheat - fit only on training data\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)  # apply same transformation to test data\n```\n\n----------------------------------------\n\nTITLE: Training and evaluating a Multinomial Naive Bayes classifier on the 20 newsgroups dataset\nDESCRIPTION: This snippet demonstrates how to train a Multinomial Naive Bayes classifier on the 20 newsgroups dataset and evaluate its performance using the F1-score metric. It includes loading the test set and transforming it into TF-IDF vectors.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/twenty_newsgroups.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.naive_bayes import MultinomialNB\n>>> from sklearn import metrics\n>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n0.88213...\n```\n\n----------------------------------------\n\nTITLE: Using validation_curve in scikit-learn to evaluate hyperparameter impact\nDESCRIPTION: This snippet demonstrates how to use validation_curve to assess the impact of the C parameter on SVM performance with the Iris dataset. It shows how to generate training and validation scores across a range of hyperparameter values.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/learning_curve.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.datasets import load_iris\nfrom sklearn.svm import SVC\n\nnp.random.seed(0)\nX, y = load_iris(return_X_y=True)\nindices = np.arange(y.shape[0])\nnp.random.shuffle(indices)\nX, y = X[indices], y[indices]\n\ntrain_scores, valid_scores = validation_curve(\n    SVC(kernel=\"linear\"), X, y, param_name=\"C\", param_range=np.logspace(-7, 3, 3),\n)\n```\n\n----------------------------------------\n\nTITLE: L1-based Feature Selection with LinearSVC in Python\nDESCRIPTION: This example demonstrates how to perform feature selection using L1 regularization with LinearSVC classifier. It uses SelectFromModel to automatically select non-zero coefficient features, reducing the dimensionality of the Iris dataset.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_selection.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.svm import LinearSVC\nfrom sklearn.datasets import load_iris\nfrom sklearn.feature_selection import SelectFromModel\nX, y = load_iris(return_X_y=True)\nX.shape\nlsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(X)\nX_new.shape\n```\n\n----------------------------------------\n\nTITLE: Creating Classification Datasets with make_classification in Python\nDESCRIPTION: Demonstrates the use of make_classification function to create three different multiclass datasets with varying parameters. It visualizes the results using matplotlib subplots.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/datasets/sample_generators.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\nfig, axs = plt.subplots(1, 3, figsize=(12, 4), sharey=True, sharex=True)\ntitles = [\"Two classes,\\none informative feature,\\none cluster per class\",\n         \"Two classes,\\ntwo informative features,\\ntwo clusters per class\",\n         \"Three classes,\\ntwo informative features,\\none cluster per class\"]\nparams = [\n    {\"n_informative\": 1, \"n_clusters_per_class\": 1, \"n_classes\": 2},\n    {\"n_informative\": 2, \"n_clusters_per_class\": 2, \"n_classes\": 2},\n    {\"n_informative\": 2, \"n_clusters_per_class\": 1, \"n_classes\": 3}\n]\n\nfor i, param in enumerate(params):\n    X, Y = make_classification(n_features=2, n_redundant=0, random_state=1, **param)\n    axs[i].scatter(X[:, 0], X[:, 1], c=Y)\n    axs[i].set_title(titles[i])\n\nplt.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Using Custom Levenshtein Distance with DBSCAN in Python\nDESCRIPTION: This code snippet demonstrates how to implement the DBSCAN clustering algorithm using a custom distance metric based on Levenshtein distance for string data. The snippet imports necessary libraries, defines a custom metric function, and applies DBSCAN using a distance matrix from a list of DNA sequences.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/faq.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n>>> import numpy as np\n>>> from leven import levenshtein  # doctest: +SKIP\n>>> from sklearn.cluster import dbscan\n>>> data = [\"ACCTCCTAGAAG\", \"ACCTACTAGAAGTT\", \"GAATATTAGGCCGA\"]\n>>> def lev_metric(x, y):\n...     i, j = int(x[0]), int(y[0])  # extract indices\n...     return levenshtein(data[i], data[j])\n...\n>>> X = np.arange(len(data)).reshape(-1, 1)\n>>> X\narray([[0],\n       [1],\n       [2]])\n>>> # We need to specify algorithm='brute' as the default assumes\n>>> # a continuous feature space.\n>>> dbscan(X, metric=lev_metric, eps=5, min_samples=2, algorithm='brute')  # doctest: +SKIP\n(array([0, 1]), array([ 0,  0, -1]))\n```\n\n----------------------------------------\n\nTITLE: Handling Unknown Categories with drop Parameter in OneHotEncoder in Python\nDESCRIPTION: Shows the behavior of OneHotEncoder when both handle_unknown='ignore' and drop parameters are set, demonstrating how unknown categories are encoded as all zeros.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n>>> drop_enc = preprocessing.OneHotEncoder(drop='first',\n...                                        handle_unknown='ignore').fit(X)\n>>> X_test = [['unknown', 'America', 'IE']]\n>>> drop_enc.transform(X_test).toarray()\narray([[0., 0., 0., 0., 0.]])\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline with OrdinalEncoder and SimpleImputer in Python\nDESCRIPTION: Shows how to create a pipeline combining OrdinalEncoder with SimpleImputer to handle missing values in categorical features, equivalent to using OrdinalEncoder with encoded_missing_value parameter.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.impute import SimpleImputer\n>>> enc = Pipeline(steps=[\n...     (\"encoder\", preprocessing.OrdinalEncoder()),\n...     (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=-1)),\n... ])\n>>> enc.fit_transform(X)\narray([[ 1.],\n       [ 0.],\n       [-1.],\n       [ 0.]])\n```\n\n----------------------------------------\n\nTITLE: Using HalvingGridSearchCV with Aggressive Elimination in scikit-learn\nDESCRIPTION: Example showing how aggressive_elimination=True forces HalvingGridSearchCV to reduce the number of candidates to approximately factor candidates in the final iteration.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/grid_search.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\n...                            factor=2,\n...                            max_resources=40,\n...                            aggressive_elimination=True,\n...                            ).fit(X, y)\n>>> sh.n_resources_\n[20, 20, 40]\n>>> sh.n_candidates_\n[6, 3, 2]\n```\n\n----------------------------------------\n\nTITLE: Univariate Feature Selection Using SelectKBest in Python\nDESCRIPTION: This example shows how to perform univariate feature selection using the SelectKBest class with an F-test scoring function. It selects the two best features from the Iris dataset based on statistical significance.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_selection.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import load_iris\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nX, y = load_iris(return_X_y=True)\nX.shape\nX_new = SelectKBest(f_classif, k=2).fit_transform(X, y)\nX_new.shape\n```\n\n----------------------------------------\n\nTITLE: Using callable scoring returning multiple metrics in GridSearchCV\nDESCRIPTION: Support for using a callable scoring function that returns a dictionary of multiple metric names/values in GridSearchCV, RandomizedSearchCV, and cross_validate.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n\ndef custom_scoring(estimator, X, y):\n    return {'metric1': score1, 'metric2': score2}\n\nGridSearchCV(estimator, param_grid, scoring=custom_scoring)\n```\n\n----------------------------------------\n\nTITLE: Using LabelEncoder for Non-Numeric Label Transformation in Python\nDESCRIPTION: Shows how to use LabelEncoder to transform non-numerical (string) labels into numerical format and back. Useful for converting categorical string labels to numeric values.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing_targets.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> le = preprocessing.LabelEncoder()\n>>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\nLabelEncoder()\n>>> list(le.classes_)\n[np.str_('amsterdam'), np.str_('paris'), np.str_('tokyo')]\n>>> le.transform([\"tokyo\", \"tokyo\", \"paris\"])\narray([2, 2, 1])\n>>> list(le.inverse_transform([2, 2, 1]))\n[np.str_('tokyo'), np.str_('tokyo'), np.str_('paris')]\n```\n\n----------------------------------------\n\nTITLE: Using Log-uniform Distributions for Randomized Search in Python\nDESCRIPTION: Demonstrates how to use log-uniform distributions for continuous parameters in RandomizedSearchCV using scikit-learn's loguniform function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/grid_search.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.utils.fixes import loguniform\n{'C': loguniform(1e0, 1e3),\n 'gamma': loguniform(1e-4, 1e-3),\n 'kernel': ['rbf'],\n 'class_weight':['balanced', None]}\n```\n\n----------------------------------------\n\nTITLE: Accuracy Score for Multilabel Classification in Scikit-learn\nDESCRIPTION: This code snippet demonstrates the use of `accuracy_score` in a multilabel classification scenario. It showcases how `accuracy_score` calculates subset accuracy, where a sample is considered correctly predicted only if all predicted labels exactly match the true labels.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n0.5\n```\n\n----------------------------------------\n\nTITLE: Custom Scorer with GridSearchCV\nDESCRIPTION: Shows how to create a custom scorer using make_scorer for fbeta_score and use it with GridSearchCV.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics import fbeta_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import LinearSVC\nftwo_scorer = make_scorer(fbeta_score, beta=2)\ngrid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n                    scoring=ftwo_scorer, cv=5)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Classifier in Python\nDESCRIPTION: Shows how to create a custom classifier that inherits from BaseEstimator and ClassifierMixin, implementing the required fit and predict methods.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/develop.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn.base import BaseEstimator, ClassifierMixin\n>>> from sklearn.utils.validation import validate_data, check_is_fitted\n>>> from sklearn.utils.multiclass import unique_labels\n>>> from sklearn.metrics import euclidean_distances\n>>> class TemplateClassifier(ClassifierMixin, BaseEstimator):\n...\n...     def __init__(self, demo_param='demo'):\n...         self.demo_param = demo_param\n...\n...     def fit(self, X, y):\n...\n...         # Check that X and y have correct shape, set n_features_in_, etc.\n...         X, y = validate_data(self, X, y)\n...         # Store the classes seen during fit\n...         self.classes_ = unique_labels(y)\n...\n...         self.X_ = X\n...         self.y_ = y\n...         # Return the classifier\n...         return self\n...\n...     def predict(self, X):\n...\n...         # Check if fit has been called\n...         check_is_fitted(self)\n...\n...         # Input validation\n...         X = validate_data(self, X, reset=False)\n...\n...         closest = np.argmin(euclidean_distances(X, self.X_), axis=1)\n...         return self.y_[closest]\n```\n\n----------------------------------------\n\nTITLE: Multiple Metric Evaluation with Dictionary in Scikit-learn\nDESCRIPTION: This example demonstrates another way to specify multiple scoring metrics for the scoring parameter in Scikit-learn, using a dictionary mapping scorer names to scoring functions or predefined metric strings. The dict values can be scorer functions created with `make_scorer`, or one of the predefined metric strings.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.metrics import accuracy_score\n>>> from sklearn.metrics import make_scorer\n>>> scoring = {'accuracy': make_scorer(accuracy_score),\n...            'prec': 'precision'}\n```\n\n----------------------------------------\n\nTITLE: Implementing NCA with KNeighborsClassifier in Python\nDESCRIPTION: This code demonstrates how to use NeighborhoodComponentsAnalysis with KNeighborsClassifier in a pipeline for classification on the Iris dataset. The example loads the dataset, splits it into training and test sets, creates and trains the pipeline, and evaluates the accuracy.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neighbors.rst#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.neighbors import (NeighborhoodComponentsAnalysis,\nKNeighborsClassifier)\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y,\nstratify=y, test_size=0.7, random_state=42)\nnca = NeighborhoodComponentsAnalysis(random_state=42)\nknn = KNeighborsClassifier(n_neighbors=3)\nnca_pipe = Pipeline([('nca', nca), ('knn', knn)])\nnca_pipe.fit(X_train, y_train)\nPipeline(...)\nprint(nca_pipe.score(X_test, y_test))\n0.96190476...\n```\n\n----------------------------------------\n\nTITLE: Building Analyzer for n-grams\nDESCRIPTION: This snippet demonstrates building and using the analyzer of a `CountVectorizer` configured to extract bigrams. The analyzer identifies sequences of words, reflecting the specified n-gram range.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> analyze = bigram_vectorizer.build_analyzer()\n>>> analyze('Bi-grams are cool!') == (\n...     ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])\nTrue\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Probability Estimates and Class Predictions in Python\nDESCRIPTION: This snippet shows how to use a DecisionTreeClassifier to make probability estimates and class predictions on a sample dataset. It illustrates the relationship between conditional probability estimates and class labels.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/classification_threshold.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> X, y = make_classification(random_state=0)\n>>> classifier = DecisionTreeClassifier(max_depth=2, random_state=0).fit(X, y)\n>>> classifier.predict_proba(X[:4])\narray([[0.94     , 0.06     ],\n       [0.94     , 0.06     ],\n       [0.0416..., 0.9583...],\n       [0.0416..., 0.9583...]])\n>>> classifier.predict(X[:4])\narray([0, 0, 1, 1])\n```\n\n----------------------------------------\n\nTITLE: Calculating Matthews Correlation Coefficient for Binary Classification in Python\nDESCRIPTION: This example shows how to use the matthews_corrcoef function to compute the Matthews correlation coefficient for a binary classification problem. It demonstrates the usage with true labels and predicted labels.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.metrics import matthews_corrcoef\n>>> y_true = [+1, +1, +1, -1]\n>>> y_pred = [+1, -1, +1, +1]\n>>> matthews_corrcoef(y_true, y_pred)\n-0.33...\n```\n\n----------------------------------------\n\nTITLE: Multiple Metric Evaluation with String List in Scikit-learn\nDESCRIPTION: This example demonstrates one way to specify multiple scoring metrics for the scoring parameter in Scikit-learn using a list of string metrics. The example shows that we want to score based on `accuracy` and `precision`.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> scoring = ['accuracy', 'precision']\n```\n\n----------------------------------------\n\nTITLE: Fitting GradientBoostingRegressor in Python\nDESCRIPTION: This snippet demonstrates the use of GradientBoostingRegressor to perform regression analysis using the `make_friedman1` dataset. It shows how to compute the mean squared error of predictions and how parameters like `n_estimators`, `learning_rate`, and `max_depth` affect model training.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn.metrics import mean_squared_error\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.ensemble import GradientBoostingRegressor\n\n>>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n>>> X_train, X_test = X[:200], X[200:]\n>>> y_train, y_test = y[:200], y[200:]\n>>> est = GradientBoostingRegressor(\n...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n...     loss='squared_error'\n... ).fit(X_train, y_train)\n>>> mean_squared_error(y_test, est.predict(X_test))\n```\n\n----------------------------------------\n\nTITLE: Using partial_fit for Out-of-core Naive Bayes Training\nDESCRIPTION: Method for incrementally training Naive Bayes models (MultinomialNB, BernoulliNB, GaussianNB) on large datasets that don't fit in memory. The partial_fit method requires passing all expected class labels on first call and supports sample weighting.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/naive_bayes.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npartial_fit\n```\n\n----------------------------------------\n\nTITLE: Implementing One-vs-Rest Multiclass Classification with scikit-learn\nDESCRIPTION: This example demonstrates multiclass learning using the OneVsRestClassifier with a LinearSVC base estimator on the Iris dataset.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/multiclass.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn import datasets\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nX, y = datasets.load_iris(return_X_y=True)\nOneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)\n```\n\n----------------------------------------\n\nTITLE: Transforming counts to tf-idf\nDESCRIPTION: This snippet transforms a matrix of counts into a TF-IDF representation using the `TfidfTransformer`. It first defines a count matrix, then fits and transforms it to obtain the TF-IDF values.  The IDF values are computed based on the document frequencies of each term.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n>>> counts = [[3, 0, 1],\n...           [2, 0, 0],\n...           [3, 0, 0],\n...           [4, 0, 0],\n...           [3, 2, 0],\n...           [3, 0, 2]]\n...\n>>> tfidf = transformer.fit_transform(counts)\n>>> tfidf\n```\n\n----------------------------------------\n\nTITLE: Categorical Features Configuration in HistGradientBoosting\nDESCRIPTION: Shows different ways to specify categorical features in HistGradientBoosting models using boolean masks, integer indices, or column names with DataFrames.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ngbdt = HistGradientBoostingClassifier(categorical_features=[True, False])\n\n# Using integer indices\ngbdt = HistGradientBoostingClassifier(categorical_features=[0])\n\n# Using column names with DataFrame\ngbdt = HistGradientBoostingClassifier(categorical_features=[\"site\", \"manufacturer\"])\n```\n\n----------------------------------------\n\nTITLE: Extracting Character N-grams with CountVectorizer in Python\nDESCRIPTION: Demonstrates how to use CountVectorizer to extract character 2-grams from text, showing the difference between 'char_wb' and 'char' analyzers. The 'char_wb' analyzer creates n-grams only from characters inside word boundaries, while 'char' spans across words.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n>>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n>>> counts = ngram_vectorizer.fit_transform(['words', 'wprds'])\n>>> ngram_vectorizer.get_feature_names_out()\narray([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'], ...)\n>>> counts.toarray().astype(int)\narray([[1, 1, 1, 0, 1, 1, 1, 0],\n       [1, 1, 0, 1, 1, 1, 0, 1]])\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))\n>>> ngram_vectorizer.fit_transform(['jumpy fox'])\n<Compressed Sparse...dtype 'int64'\n  with 4 stored elements and shape (1, 4)>\n\n>>> ngram_vectorizer.get_feature_names_out()\narray([' fox ', ' jump', 'jumpy', 'umpy '], ...)\n\n>>> ngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(5, 5))\n>>> ngram_vectorizer.fit_transform(['jumpy fox'])\n<Compressed Sparse...dtype 'int64'\n  with 5 stored elements and shape (1, 5)>\n>>> ngram_vectorizer.get_feature_names_out()\narray(['jumpy', 'mpy f', 'py fo', 'umpy ', 'y fox'], ...)\n```\n\n----------------------------------------\n\nTITLE: Quantile Transformation: Mapping to Uniform Distribution\nDESCRIPTION: This snippet demonstrates how to use QuantileTransformer to map data to a uniform distribution between 0 and 1. It uses the iris dataset, splits it into training and testing sets, and applies the quantile transformation. The code then calculates and prints the percentiles of the original and transformed data to show the effect of the transformation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = load_iris(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n>>> quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\n>>> X_train_trans = quantile_transformer.fit_transform(X_train)\n>>> X_test_trans = quantile_transformer.transform(X_test)\n>>> np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) # doctest: +SKIP\narray([ 4.3,  5.1,  5.8,  6.5,  7.9])\n```\n\n----------------------------------------\n\nTITLE: Multi-class SVM Classification with One-vs-One Approach\nDESCRIPTION: Shows how to implement multi-class classification using SVM with the one-versus-one approach. Demonstrates shape configuration of the decision function and classifier training.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/svm.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nX = [[0], [1], [2], [3]]\nY = [0, 1, 2, 3]\nclf = svm.SVC(decision_function_shape='ovo')\nclf.fit(X, Y)\ndec = clf.decision_function([[1]])\ndec.shape[1] # 6 classes: 4*3/2 = 6\nclf.decision_function_shape = \"ovr\"\ndec = clf.decision_function([[1]])\ndec.shape[1] # 4 classes\n```\n\n----------------------------------------\n\nTITLE: Using learning_curve in scikit-learn to evaluate model performance with varying training sizes\nDESCRIPTION: This snippet shows how to use learning_curve to generate data for assessing how model performance changes with different training set sizes. It demonstrates the function with an SVM classifier on the Iris dataset using cross-validation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/learning_curve.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.svm import SVC\n\ntrain_sizes, train_scores, valid_scores = learning_curve(\n    SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5)\n```\n\n----------------------------------------\n\nTITLE: Configuring OneHotEncoder to Handle Unknown Categories as Infrequent in Python\nDESCRIPTION: This example demonstrates how to set up OneHotEncoder to treat unknown categories as infrequent. It uses handle_unknown='infrequent_if_exist' and min_frequency=6.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n>>> enc = preprocessing.OneHotEncoder(\n...    handle_unknown='infrequent_if_exist', sparse_output=False, min_frequency=6)\n>>> enc = enc.fit(X)\n>>> enc.transform(np.array([['dragon']]))\narray([[0., 0., 1.]])\n```\n\n----------------------------------------\n\nTITLE: Different Scoring and Fitting Weights with Metadata Routing (Python)\nDESCRIPTION: Shows how to use aliases to pass different weights for scoring and fitting to different consumers in cross_validate.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/metadata_routing.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> weighted_acc = make_scorer(accuracy_score).set_score_request(\n...    sample_weight=\"scoring_weight\"\n... )\n>>> lr = LogisticRegressionCV(\n...     cv=GroupKFold(), scoring=weighted_acc,\n... ).set_fit_request(sample_weight=\"fitting_weight\")\n>>> cv_results = cross_validate(\n...     lr,\n...     X,\n...     y,\n...     cv=GroupKFold(),\n...     params={\n...         \"scoring_weight\": my_weights,\n...         \"fitting_weight\": my_other_weights,\n...         \"groups\": my_groups,\n...     },\n...     scoring=weighted_acc,\n... )\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Transformer with FunctionTransformer in scikit-learn\nDESCRIPTION: This example shows how to convert a Python function (np.log1p) into a transformer using FunctionTransformer. The transformer applies the log transformation to a numpy array in a pipeline-compatible way.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn.preprocessing import FunctionTransformer\n>>> transformer = FunctionTransformer(np.log1p, validate=True)\n>>> X = np.array([[0, 1], [2, 3]])\n>>> # Since FunctionTransformer is no-op during fit, we can call transform directly\n>>> transformer.transform(X)\narray([[0.        , 0.69314718],\n       [1.09861229, 1.38629436]])\n```\n\n----------------------------------------\n\nTITLE: Encoding Categorical Features Using OrdinalEncoder in Python\nDESCRIPTION: Demonstrates how to use OrdinalEncoder to transform categorical features into integer codes. The example shows encoding categorical variables like gender and location into numerical values for machine learning models.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> enc = preprocessing.OrdinalEncoder()\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n>>> enc.fit(X)\nOrdinalEncoder()\n>>> enc.transform([['female', 'from US', 'uses Safari']])\narray([[0., 1., 1.]])\n```\n\n----------------------------------------\n\nTITLE: Creating a normalized confusion matrix in Python with scikit-learn\nDESCRIPTION: Example showing how to create a normalized confusion matrix, which reports ratios instead of counts. The matrix can be normalized by predictions, true values, or overall total.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ny_true = [0, 0, 0, 1, 1, 1, 1, 1]\ny_pred = [0, 1, 0, 1, 0, 1, 0, 1]\nconfusion_matrix(y_true, y_pred, normalize='all')\narray([[0.25 , 0.125],\n       [0.25 , 0.375]])\n```\n\n----------------------------------------\n\nTITLE: Computing Permutation Importance with Multiple Scorers\nDESCRIPTION: Shows how to calculate permutation importance using multiple scoring metrics simultaneously for computational efficiency.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/permutation_importance.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nscoring = ['r2', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error']\nr_multi = permutation_importance(\n    model, X_val, y_val, n_repeats=30, random_state=0, scoring=scoring)\nfor metric in r_multi:\n    print(f\"{metric}\")\n    r = r_multi[metric]\n    for i in r.importances_mean.argsort()[::-1]:\n        if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n            print(f\"    {diabetes.feature_names[i]:<8}\"\n                  f\"{r.importances_mean[i]:.3f}\"\n                  f\" +/- {r.importances_std[i]:.3f}\")\n```\n\n----------------------------------------\n\nTITLE: Using scoring Parameter in scikit-learn\nDESCRIPTION: Documentation for the scoring parameter which specifies metrics for evaluation in scikit-learn. It can be a string, a callable scorer, None (uses the estimator's score method), a list of strings, or a dictionary of callables.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/glossary.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Example usage patterns for scoring parameter:\n\n# String metric name\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(estimator, X, y, scoring='accuracy')\n\n# None uses estimator's score method\ncross_val_score(estimator, X, y, scoring=None)\n\n# Multiple metrics as list\nfrom sklearn.model_selection import GridSearchCV\nGridSearchCV(estimator, param_grid, scoring=['accuracy', 'precision'])\n\n# Multiple metrics as dictionary\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score\nscorers = {\n    'accuracy': make_scorer(accuracy_score),\n    'precision': make_scorer(precision_score)\n}\nGridSearchCV(estimator, param_grid, scoring=scorers)\n```\n\n----------------------------------------\n\nTITLE: Using MinMaxScaler in ColumnTransformer - Python\nDESCRIPTION: This snippet illustrates how to use MinMaxScaler to transform the remaining columns in a ColumnTransformer. The transformed values are appended to the final output along with the encoded and vectorized columns.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.preprocessing import MinMaxScaler\\ncolumn_trans = ColumnTransformer(\\n     [('city_category', OneHotEncoder(), ['city']),\\n      ('title_bow', CountVectorizer(), 'title')],\\n     remainder=MinMaxScaler())\\ncolumn_trans.fit_transform(X)[:, -2:]\\narray([[1. , 0.5],\\n         [0. , 1. ],\\n         [0.5, 0.5],\\n         [1. , 0. ]])\n```\n\n----------------------------------------\n\nTITLE: Tokenizing and Counting Word Occurrences\nDESCRIPTION: This snippet demonstrates how to use `CountVectorizer` to tokenize and count word occurrences in a corpus of text documents. It first defines a corpus of four short documents, then fits and transforms this corpus using `CountVectorizer` to obtain a sparse matrix representing word frequencies.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> corpus = [\n...     'This is the first document.',\n...     'This is the second second document.',\n...     'And the third one.',\n...     'Is this the first document?',\n... ]\n>>> X = vectorizer.fit_transform(corpus)\n>>> X\n<Compressed Sparse...dtype 'int64'\n    with 19 stored elements and shape (4, 9)>\n```\n\n----------------------------------------\n\nTITLE: Early Stopping in Gradient Boosting Models (Python)\nDESCRIPTION: Adds early stopping functionality to GradientBoostingClassifier and GradientBoostingRegressor via n_iter_no_change, validation_fraction and tol parameters. This allows stopping training early if performance on a validation set stops improving.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nn_iter_no_change, validation_fraction, tol\n```\n\n----------------------------------------\n\nTITLE: Pipeline with KNeighborsTransformer and Isomap\nDESCRIPTION: This code snippet demonstrates how to create a pipeline using KNeighborsTransformer and Isomap for dimensionality reduction. It shows how to use a temporary directory for caching and then fit and transform the data using the pipeline.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neighbors.rst#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> import tempfile\n>>> from sklearn.manifold import Isomap\n>>> from sklearn.neighbors import KNeighborsTransformer\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.datasets import make_regression\n>>> cache_path = tempfile.gettempdir()  # we use a temporary folder here\n>>> X, _ = make_regression(n_samples=50, n_features=25, random_state=0)\n>>> estimator = make_pipeline(\n...     KNeighborsTransformer(mode='distance'),\n...     Isomap(n_components=3, metric='precomputed'),\n...     memory=cache_path)\n>>> X_embedded = estimator.fit_transform(X)\n>>> X_embedded.shape\n(50, 3)\n```\n\n----------------------------------------\n\nTITLE: Creating an Imbalanced Dataset with Iris Data in Python\nDESCRIPTION: This code snippet demonstrates how to load the Iris dataset, create a binary classification problem by setting one class vs. all others, and split the data into training and test sets.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nX, y = load_iris(return_X_y=True)\ny[y != 1] = -1\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n```\n\n----------------------------------------\n\nTITLE: Power Transformation: Box-Cox Example\nDESCRIPTION: This snippet demonstrates the use of PowerTransformer with the Box-Cox method to map samples from a lognormal distribution to a normal distribution. It initializes a PowerTransformer with the specified method and standardization setting, generates a lognormal dataset, and applies the transformation. The transformed dataset is then printed.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> pt = preprocessing.PowerTransformer(method='box-cox', standardize=False)\n>>> X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))\n>>> X_lognormal\narray([[1.28..., 1.18..., 0.84...],\n      [0.94..., 1.60..., 0.38...],\n      [1.35..., 0.21..., 1.09...]])\n>>> pt.fit_transform(X_lognormal)\narray([[ 0.49...,  0.17..., -0.15...],\n      [-0.05...,  0.58..., -0.57...],\n      [ 0.69..., -0.84...,  0.10...]])\n```\n\n----------------------------------------\n\nTITLE: Decision Tree Regression Example\nDESCRIPTION: Demonstrates how to create and use a decision tree for regression tasks with numerical output values.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/tree.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn import tree\n>>> X = [[0, 0], [2, 2]]\n>>> y = [0.5, 2.5]\n>>> clf = tree.DecisionTreeRegressor()\n>>> clf = clf.fit(X, y)\n>>> clf.predict([[1, 1]])\narray([0.5])\n```\n\n----------------------------------------\n\nTITLE: Using MultiLabelBinarizer for Multilabel Classification in Python\nDESCRIPTION: Shows how to use MultiLabelBinarizer to convert collections of label sets into binary indicator matrix format. This is useful for multilabel classification where each sample can have multiple labels.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing_targets.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.preprocessing import MultiLabelBinarizer\n>>> y = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]]\n>>> MultiLabelBinarizer().fit_transform(y)\narray([[0, 0, 1, 1, 1],\n       [0, 0, 1, 0, 0],\n       [1, 1, 0, 1, 0],\n       [1, 1, 1, 1, 1],\n       [1, 1, 1, 0, 0]])\n```\n\n----------------------------------------\n\nTITLE: Implementing XOR with Perceptron and Polynomial Features\nDESCRIPTION: Creates a binary classification model using Perceptron and polynomial feature transformation to solve the XOR problem. The code transforms 2D boolean inputs into polynomial features and trains a linear classifier that perfectly separates the XOR operation results.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.linear_model import Perceptron\n>>> from sklearn.preprocessing import PolynomialFeatures\n>>> import numpy as np\n>>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n>>> y = X[:, 0] ^ X[:, 1]\n>>> y\narray([0, 1, 1, 0])\n>>> X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)\n>>> X\narray([[1, 0, 0, 0],\n       [1, 0, 1, 0],\n       [1, 1, 0, 0],\n       [1, 1, 1, 1]])\n>>> clf = Perceptron(fit_intercept=False, max_iter=10, tol=None,\n...                  shuffle=False).fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Using LeaveOneGroupOut for Cross-Validation in Python\nDESCRIPTION: Demonstrates how to use LeaveOneGroupOut cross-validation, which creates splits by holding out samples belonging to one specific group. This is useful for scenarios like multiple experiments or time-based splits.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/cross_validation.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import LeaveOneGroupOut\n\nX = [1, 5, 10, 50, 60, 70, 80]\ny = [0, 1, 1, 2, 2, 2, 2]\ngroups = [1, 1, 2, 2, 3, 3, 3]\nlogo = LeaveOneGroupOut()\nfor train, test in logo.split(X, y, groups=groups):\n    print(\"%s %s\" % (train, test))\n```\n\n----------------------------------------\n\nTITLE: Extracting binary classification metrics from confusion matrix in Python\nDESCRIPTION: Example showing how to extract true negatives, false positives, false negatives, and true positives from a confusion matrix for binary classification problems.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ny_true = [0, 0, 0, 1, 1, 1, 1, 1]\ny_pred = [0, 1, 0, 1, 0, 1, 0, 1]\ntn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel().tolist()\ntn, fp, fn, tp\n(2, 1, 2, 3)\n```\n\n----------------------------------------\n\nTITLE: Probability Estimation with MLPClassifier in Python\nDESCRIPTION: This example illustrates how to use the predict_proba method of MLPClassifier to obtain probability estimates for class predictions. It shows the output format for multi-class classification.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neural_networks_supervised.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> clf.predict_proba([[2., 2.], [1., 2.]])\narray([[1.967...e-04, 9.998...-01],\n       [1.967...e-04, 9.998...-01]])\n```\n\n----------------------------------------\n\nTITLE: Using LabelBinarizer for Multiclass Label Transformation in Python\nDESCRIPTION: Demonstrates how to use LabelBinarizer to convert multiclass labels into a label indicator matrix format. The transformer fits numeric labels and transforms them into a binary matrix representation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing_targets.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn import preprocessing\n>>> lb = preprocessing.LabelBinarizer()\n>>> lb.fit([1, 2, 6, 4, 2])\nLabelBinarizer()\n>>> lb.classes_\narray([1, 2, 4, 6])\n>>> lb.transform([1, 6])\narray([[1, 0, 0, 0],\n       [0, 0, 0, 1]])\n```\n\n----------------------------------------\n\nTITLE: Using PredictionErrorDisplay for regression diagnostics in Python\nDESCRIPTION: Shows how to use the new PredictionErrorDisplay class to create plots for assessing regressor behavior, including residuals vs predicted and actual vs predicted plots.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.metrics import PredictionErrorDisplay\n\nPredictionErrorDisplay.from_estimator(regressor, X, y)\n```\n\n----------------------------------------\n\nTITLE: Calculating Calinski-Harabasz Index in Python using scikit-learn\nDESCRIPTION: This snippet demonstrates how to compute the Calinski-Harabasz Index using scikit-learn's metrics module. It shows loading the Iris dataset, performing K-means clustering, and then calculating the Calinski-Harabasz score.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn import metrics\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn import datasets\nX, y = datasets.load_iris(return_X_y=True)\n\nimport numpy as np\nfrom sklearn.cluster import KMeans\nkmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\nlabels = kmeans_model.labels_\nmetrics.calinski_harabasz_score(X, labels)\n561.59...\n```\n\n----------------------------------------\n\nTITLE: Fitting a Random Forest Classifier in scikit-learn\nDESCRIPTION: Example of initializing a RandomForestClassifier with a random state, and fitting it to a simple dataset with two samples and three features. This demonstrates the basic pattern of model initialization and training.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/getting_started.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> clf = RandomForestClassifier(random_state=0)\n>>> X = [[ 1,  2,  3],  # 2 samples, 3 features\n...      [11, 12, 13]]\n>>> y = [0, 1]  # classes of each sample\n>>> clf.fit(X, y)\nRandomForestClassifier(random_state=0)\n```\n\n----------------------------------------\n\nTITLE: Implementing Stacking Regressor in Python with scikit-learn\nDESCRIPTION: This example shows how to implement a Stacking Regressor using scikit-learn, combining Ridge, Lasso, and K-Nearest Neighbors regressors with a Gradient Boosting final estimator.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.linear_model import RidgeCV, LassoCV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\nestimators = [('ridge', RidgeCV()),\n              ('lasso', LassoCV(random_state=42)),\n              ('knr', KNeighborsRegressor(n_neighbors=20,\n                                          metric='euclidean'))]\n\nfinal_estimator = GradientBoostingRegressor(\n    n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1,\n    random_state=42)\n\nreg = StackingRegressor(\n    estimators=estimators,\n    final_estimator=final_estimator)\n\nX, y = load_diabetes(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    random_state=42)\nreg.fit(X_train, y_train)\n\ny_pred = reg.predict(X_test)\nprint('R2 score: {:.2f}'.format(r2_score(y_test, y_pred)))\n```\n\n----------------------------------------\n\nTITLE: Log loss criterion usage in DecisionTrees\nDESCRIPTION: Shows how to use the new log_loss criterion in DecisionTreeClassifier which is equivalent to entropy.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.1.rst#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ncriterion=\"log_loss\"\n```\n\n----------------------------------------\n\nTITLE: Setting newton-cholesky solver for LogisticRegressionCV in Python\nDESCRIPTION: Demonstrates setting the new newton-cholesky solver for LogisticRegressionCV and other linear models. This solver uses Cholesky decomposition of the Hessian matrix and can converge faster for certain problems.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.linear_model import LogisticRegressionCV\n\nmodel = LogisticRegressionCV(solver=\"newton-cholesky\")\n```\n\n----------------------------------------\n\nTITLE: Using VotingClassifier with GridSearchCV in Python\nDESCRIPTION: This example shows how to use VotingClassifier with GridSearchCV to tune hyperparameters of individual estimators in scikit-learn.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import GridSearchCV\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\neclf = VotingClassifier(\n    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n    voting='soft'\n)\n\nparams = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid = grid.fit(iris.data, iris.target)\n```\n\n----------------------------------------\n\nTITLE: Using make_pipeline for Shorthand Pipeline Creation\nDESCRIPTION: Shows how to use the make_pipeline utility function as a shorthand for constructing pipelines. It automatically names the steps based on the estimator class names.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.pipeline import make_pipeline\n>>> make_pipeline(PCA(), SVC())\nPipeline(steps=[('pca', PCA()), ('svc', SVC())])\n```\n\n----------------------------------------\n\nTITLE: Pipeline Integration with Missing Value Handling in Python\nDESCRIPTION: Demonstrates how to integrate missing value handling into a machine learning pipeline using FeatureUnion and SimpleImputer with a DecisionTreeClassifier.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/impute.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.impute import SimpleImputer, MissingIndicator\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.pipeline import FeatureUnion, make_pipeline\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> X, y = load_iris(return_X_y=True)\n>>> mask = np.random.randint(0, 2, size=X.shape).astype(bool)\n>>> X[mask] = np.nan\n>>> X_train, X_test, y_train, _ = train_test_split(X, y, test_size=100,\n...                                                random_state=0)\n\n>>> transformer = FeatureUnion(\n...     transformer_list=[\n...         ('features', SimpleImputer(strategy='mean')),\n...         ('indicators', MissingIndicator())])\n>>> transformer = transformer.fit(X_train, y_train)\n>>> results = transformer.transform(X_test)\n>>> results.shape\n(100, 8)\n\n>>> clf = make_pipeline(transformer, DecisionTreeClassifier())\n>>> clf = clf.fit(X_train, y_train)\n>>> results = clf.predict(X_test)\n>>> results.shape\n(100,)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Pipeline Components in scikit-learn\nDESCRIPTION: Demonstrates how to inspect individual components of a Pipeline without caching enabled. Note that enabling caching triggers cloning of transformers, which affects direct inspection.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import load_digits\n>>> X_digits, y_digits = load_digits(return_X_y=True)\n>>> pca1 = PCA(n_components=10)\n>>> svm1 = SVC()\n>>> pipe = Pipeline([('reduce_dim', pca1), ('clf', svm1)])\n>>> pipe.fit(X_digits, y_digits)\nPipeline(steps=[('reduce_dim', PCA(n_components=10)), ('clf', SVC())])\n>>> # The pca instance can be inspected directly\n>>> pca1.components_.shape\n(10, 64)\n```\n\n----------------------------------------\n\nTITLE: Using Median Absolute Error in Python with scikit-learn\nDESCRIPTION: This example demonstrates the median_absolute_error function which is robust to outliers as it calculates the median of all absolute differences between targets and predictions, rather than the mean.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics import median_absolute_error\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nmedian_absolute_error(y_true, y_pred)\n0.5\n```\n\n----------------------------------------\n\nTITLE: Computing Silhouette Coefficient in Python with scikit-learn\nDESCRIPTION: This code snippet shows how to calculate the Silhouette Coefficient using scikit-learn's metrics module. It demonstrates loading the Iris dataset, performing K-means clustering, and then computing the Silhouette score.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn import metrics\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn import datasets\nX, y = datasets.load_iris(return_X_y=True)\n\nimport numpy as np\nfrom sklearn.cluster import KMeans\nkmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\nlabels = kmeans_model.labels_\nmetrics.silhouette_score(X, labels, metric='euclidean')\n0.55...\n```\n\n----------------------------------------\n\nTITLE: Accessing Dataset Information and Metadata from OpenML\nDESCRIPTION: Shows how to access descriptive information and metadata about a dataset downloaded from OpenML using the DESCR and details attributes. This provides information about the dataset's source, authors, and other characteristics.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/datasets/loading_other_datasets.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(mice.DESCR)\nmice.details\n```\n\n----------------------------------------\n\nTITLE: Using Chi-squared Kernel with SVM in Python\nDESCRIPTION: Shows how to use the chi-squared kernel with Support Vector Machines (SVM) in scikit-learn. The example demonstrates two approaches: precomputing the kernel and passing it to SVC, and directly using the kernel function in SVC.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/metrics.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.svm import SVC\nfrom sklearn.metrics.pairwise import chi2_kernel\nX = [[0, 1], [1, 0], [.2, .8], [.7, .3]]\ny = [0, 1, 0, 1]\nK = chi2_kernel(X, gamma=.5)\nsvm = SVC(kernel='precomputed').fit(K, y)\nsvm.predict(K)\n\nsvm = SVC(kernel=chi2_kernel).fit(X, y)\nsvm.predict(X)\n```\n\n----------------------------------------\n\nTITLE: Implementing MiniBatch NMF in scikit-learn\nDESCRIPTION: This snippet describes the MiniBatchNMF class, which implements a faster version of non-negative matrix factorization better suited for large datasets. Key parameters include 'batch_size' to control mini-batch size and 'forget_factor' to adjust the importance of past data batches.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/decomposition.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass MiniBatchNMF:\n    # Implements fast and scalable NMF\n    def __init__(self, batch_size=100, forget_factor=0.5):\n        self.batch_size = batch_size\n        self.forget_factor = forget_factor\n        # Additional initialization code...\n```\n\n----------------------------------------\n\nTITLE: Calculating Jaccard Similarity Score in Python using scikit-learn\nDESCRIPTION: These examples demonstrate how to use the jaccard_score function from scikit-learn to compute Jaccard similarity for binary, 2D, multilabel, and multiclass classification problems. It shows different averaging methods and handling of various input formats.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom sklearn.metrics import jaccard_score\ny_true = np.array([[0, 1, 1],\n                   [1, 1, 0]])\ny_pred = np.array([[1, 1, 1],\n                   [1, 0, 0]])\njaccard_score(y_true[0], y_pred[0])\njaccard_score(y_true, y_pred, average=\"micro\")\njaccard_score(y_true, y_pred, average='samples')\njaccard_score(y_true, y_pred, average='macro')\njaccard_score(y_true, y_pred, average=None)\n\ny_pred = [0, 2, 1, 2]\ny_true = [0, 1, 2, 2]\njaccard_score(y_true, y_pred, average=None)\njaccard_score(y_true, y_pred, average='macro')\njaccard_score(y_true, y_pred, average='micro')\n```\n\n----------------------------------------\n\nTITLE: Using Mean Squared Error in Python with scikit-learn\nDESCRIPTION: This example shows how to use the mean_squared_error function to calculate the MSE between true and predicted values in scikit-learn. The example includes both single-output and multi-output scenarios.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics import mean_squared_error\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nmean_squared_error(y_true, y_pred)\n0.375\ny_true = [[0.5, 1], [-1, 1], [7, -6]]\ny_pred = [[0, 2], [-1, 2], [8, -5]]\nmean_squared_error(y_true, y_pred)\n0.7083...\n```\n\n----------------------------------------\n\nTITLE: Calculating Adjusted Mutual Information Score with scikit-learn in Python\nDESCRIPTION: This snippet demonstrates how to calculate the Adjusted Mutual Information (AMI) score using `sklearn.metrics.adjusted_mutual_info_score`. It measures the agreement between true labels (`labels_true`) and predicted labels (`labels_pred`), correcting for chance. AMI is normalized against chance and provides a baseline for random labelings.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn import metrics\n>>> labels_true = [0, 0, 0, 1, 1, 1]\n>>> labels_pred = [0, 0, 1, 1, 2, 2]\n\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP\n0.22504...\n```\n\n----------------------------------------\n\nTITLE: Vectorizing Large Text Corpus with HashingVectorizer in Python\nDESCRIPTION: Shows how to use HashingVectorizer to vectorize text data without storing the vocabulary in memory. This is useful for large datasets where memory usage is a concern. The example demonstrates basic usage and discusses the trade-offs of using different numbers of features.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.feature_extraction.text import HashingVectorizer\n>>> hv = HashingVectorizer(n_features=10)\n>>> hv.transform(corpus)\n<Compressed Sparse...dtype 'float64'\n  with 16 stored elements and shape (4, 10)>\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> hv = HashingVectorizer()\n>>> hv.transform(corpus)\n<Compressed Sparse...dtype 'float64'\n  with 19 stored elements and shape (4, 1048576)>\n```\n\n----------------------------------------\n\nTITLE: Implementing Agglomerative Clustering with Connectivity Constraints\nDESCRIPTION: Agglomerative clustering with connectivity constraints allows merging only adjacent clusters through a connectivity matrix. This can be particularly useful for structured data like images or graphs where only neighboring elements should be clustered together.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_3\n\n\n\n----------------------------------------\n\nTITLE: Specifying Different SVM Kernels in Scikit-learn - Python\nDESCRIPTION: This code demonstrates how to specify different kernel functions in a support vector machine model using scikit-learn's SVC. It shows how to define a linear and an RBF (Radial Basis Function) kernel. The main parameters are 'kernel', which selects the kernel type, and 'SVC()', the model constructor. The example expects no outputs other than the defined kernel types.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/svm.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> linear_svc = svm.SVC(kernel='linear')\n>>> linear_svc.kernel\n'linear'\n>>> rbf_svc = svm.SVC(kernel='rbf')\n>>> rbf_svc.kernel\n'rbf'\n```\n\n----------------------------------------\n\nTITLE: Custom Cross-Validation with ShuffleSplit and Custom Iterators in Python\nDESCRIPTION: This code demonstrates advanced cross-validation techniques using ShuffleSplit for random partitioning and a custom iterator for defining specific train-test splits in scikit-learn.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/cross_validation.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.model_selection import ShuffleSplit\n>>> n_samples = X.shape[0]\n>>> cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n>>> cross_val_score(clf, X, y, cv=cv)\narray([0.977..., 0.977..., 1.  ..., 0.955..., 1.        ])\n\n>>> def custom_cv_2folds(X):\n...     n = X.shape[0]\n...     i = 1\n...     while i <= 2:\n...         idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)\n...         yield idx, idx\n...         i += 1\n...\n>>> custom_cv = custom_cv_2folds(X)\n>>> cross_val_score(clf, X, y, cv=custom_cv)\narray([1.        , 0.973...])\n```\n\n----------------------------------------\n\nTITLE: Linear SVM Multi-class Classification\nDESCRIPTION: Demonstrates using LinearSVC for multi-class classification with the one-vs-rest strategy. Shows model initialization, training and decision function shape.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/svm.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlin_clf = svm.LinearSVC()\nlin_clf.fit(X, Y)\ndec = lin_clf.decision_function([[1]])\ndec.shape[1]\n```\n\n----------------------------------------\n\nTITLE: Limiting Working Memory in scikit-learn Operations\nDESCRIPTION: Example of using scikit-learn's config_context to limit temporary working memory for chunked operations, which helps prevent system memory exhaustion during calculations.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/computing/computational_performance.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> import sklearn\n>>> with sklearn.config_context(working_memory=128):\n...     pass  # do chunked work here\n```\n\n----------------------------------------\n\nTITLE: Using AdaBoostClassifier for Iris Dataset Classification in Python\nDESCRIPTION: This example shows how to use AdaBoostClassifier from scikit-learn to classify the Iris dataset. It demonstrates loading the dataset, creating an AdaBoost classifier with 100 estimators, and evaluating its performance using cross-validation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import AdaBoostClassifier\n\nX, y = load_iris(return_X_y=True)\nclf = AdaBoostClassifier(n_estimators=100)\nscores = cross_val_score(clf, X, y, cv=5)\nscores.mean()\nnp.float64(0.9...)\n```\n\n----------------------------------------\n\nTITLE: Performing Cross-Validation in scikit-learn\nDESCRIPTION: Example of using cross_validate to perform 5-fold cross-validation on a LinearRegression model with synthetic regression data. Shows how to retrieve and interpret cross-validation scores.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/getting_started.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.linear_model import LinearRegression\n>>> from sklearn.model_selection import cross_validate\n...\n>>> X, y = make_regression(n_samples=1000, random_state=0)\n>>> lr = LinearRegression()\n...\n>>> result = cross_validate(lr, X, y)  # defaults to 5-fold CV\n>>> result['test_score']  # r_squared score is high because dataset is easy\narray([1., 1., 1., 1., 1.])\n```\n\n----------------------------------------\n\nTITLE: Computing Top-k Accuracy Score in Python\nDESCRIPTION: This snippet introduces the new top_k_accuracy_score function, which generalizes accuracy_score to consider a prediction correct if the true label is among the k highest predicted scores.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.metrics import top_k_accuracy_score\n\n# Example usage (actual parameters may vary)\nscore = top_k_accuracy_score(y_true, y_score, k=5)\n```\n\n----------------------------------------\n\nTITLE: Sample Weight Support Example\nDESCRIPTION: Shows how to use sample weights with HistGradientBoostingClassifier using a simple toy dataset.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> X = [[1, 0],\n...      [1, 0],\n...      [1, 0],\n...      [0, 1]]\n>>> y = [0, 0, 1, 0]\n```\n\n----------------------------------------\n\nTITLE: Computing Cohen's kappa score in Python with scikit-learn\nDESCRIPTION: Example showing how to calculate Cohen's kappa statistic with scikit-learn. This metric measures agreement between two annotators on a classification problem and ranges from -1 to 1, with scores above 0.8 generally considered good agreement.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics import cohen_kappa_score\nlabeling1 = [2, 0, 2, 2, 0, 1]\nlabeling2 = [0, 0, 2, 2, 0, 2]\ncohen_kappa_score(labeling1, labeling2)\n0.4285714285714286\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Multiclass Classification Target Format in Python\nDESCRIPTION: This snippet shows valid multiclass representations for the type_of_target function, including a 1d vector and binary matrices (dense and sparse) for multiclass classification.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/multiclass.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\ny = np.array(['apple', 'pear', 'apple', 'orange'])\nprint(y)\n```\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nfrom sklearn.preprocessing import LabelBinarizer\ny = np.array(['apple', 'pear', 'apple', 'orange'])\ny_dense = LabelBinarizer().fit_transform(y)\nprint(y_dense)\nfrom scipy import sparse\ny_sparse = sparse.csr_matrix(y_dense)\nprint(y_sparse)\n```\n\n----------------------------------------\n\nTITLE: Implementing Bisecting K-Means Clustering\nDESCRIPTION: Bisecting K-Means is an iterative variant of K-Means that creates clusters by repeatedly splitting existing clusters. It offers two strategies: largest_cluster and biggest_inertia for selecting which cluster to split next.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_4\n\n\n\n----------------------------------------\n\nTITLE: One-vs-One Multiclass Classification with Linear SVC\nDESCRIPTION: Demonstrates multiclass learning using OneVsOneClassifier with LinearSVC on the Iris dataset, creating classifiers for each pair of classes\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/multiclass.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import datasets\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.svm import LinearSVC\nX, y = datasets.load_iris(return_X_y=True)\nOneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)\n```\n\n----------------------------------------\n\nTITLE: Computing Davies-Bouldin Index in Python with scikit-learn\nDESCRIPTION: This code snippet illustrates how to calculate the Davies-Bouldin Index using scikit-learn's metrics module. It demonstrates loading the Iris dataset, performing K-means clustering, and then computing the Davies-Bouldin score.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn import datasets\niris = datasets.load_iris()\nX = iris.data\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import davies_bouldin_score\nkmeans = KMeans(n_clusters=3, random_state=1).fit(X)\nlabels = kmeans.labels_\ndavies_bouldin_score(X, labels)\n0.666...\n```\n\n----------------------------------------\n\nTITLE: Estimating Covariance with Ledoit-Wolf Shrinkage in Python using scikit-learn\nDESCRIPTION: This example demonstrates how to estimate the covariance matrix using the Ledoit-Wolf shrinkage method, either with the ledoit_wolf function or by fitting a LedoitWolf object to the data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/covariance.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.covariance import ledoit_wolf, LedoitWolf\n\n# Using the function\nlw_cov, _ = ledoit_wolf(X)\n\n# Using the object\ncov = LedoitWolf(assume_centered=False)\ncov.fit(X)\n```\n\n----------------------------------------\n\nTITLE: Using multiclass roc_auc_score in scikit-learn metrics\nDESCRIPTION: Added multiclass support to roc_auc_score with new scoring options for One-vs-Rest and One-vs-One approaches, including weighted versions.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.roc_auc_score\n```\n\n----------------------------------------\n\nTITLE: Computing Contingency Matrix in Python using scikit-learn\nDESCRIPTION: Example showing how to compute a contingency matrix between true and predicted cluster labels using sklearn.metrics.cluster.contingency_matrix. The matrix shows intersection cardinality between cluster pairs.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics.cluster import contingency_matrix\nx = [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"]\ny = [0, 0, 1, 1, 2, 2]\ncontingency_matrix(x, y)\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Partial Dependence Plots with scikit-learn\nDESCRIPTION: Demonstrates how to create one-way and two-way partial dependence plots using GradientBoostingClassifier and the PartialDependenceDisplay utility.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/partial_dependence.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import make_hastie_10_2\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.inspection import PartialDependenceDisplay\n\nX, y = make_hastie_10_2(random_state=0)\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n    max_depth=1, random_state=0).fit(X, y)\nfeatures = [0, 1, (0, 1)]\nPartialDependenceDisplay.from_estimator(clf, X, features)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Kernel Functions in Scikit-learn - Python\nDESCRIPTION: This snippet outlines how to define a custom kernel function in scikit-learn. It requires the function to accept two matrices and return a kernel matrix. The example uses a simple dot product for the kernel function and creates an SVC instance using this kernel. Essential parameters include the 'kernel' for custom definition and 'SVC()' for model creation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/svm.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn import svm\n>>> def my_kernel(X, Y):\n...     return np.dot(X, Y.T)\n...\n>>> clf = svm.SVC(kernel=my_kernel)\n```\n\n----------------------------------------\n\nTITLE: Ball Tree Algorithm Configuration in scikit-learn\nDESCRIPTION: Sets up ball tree based neighbor searches which partition data in nested hyperspheres. Efficient for highly structured data in high dimensions.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neighbors.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nalgorithm = 'auto'\n```\n\n----------------------------------------\n\nTITLE: Using Brier Score Loss for Probabilistic Prediction Evaluation\nDESCRIPTION: Examples of computing the Brier score loss using scikit-learn. The Brier score measures the accuracy of probabilistic predictions and is equivalent to the mean squared error between predicted probabilities and actual outcomes.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn.metrics import brier_score_loss\n>>> y_true = np.array([0, 1, 1, 0])\n>>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n>>> y_prob = np.array([0.1, 0.9, 0.8, 0.4])\n>>> brier_score_loss(y_true, y_prob)\n0.055\n>>> brier_score_loss(y_true, 1 - y_prob, pos_label=0)\n0.055\n>>> brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n0.055\n>>> brier_score_loss(\n...    [\"eggs\", \"ham\", \"spam\"],\n...    [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.2, 0.2, 0.6]],\n...    labels=[\"eggs\", \"ham\", \"spam\"],\n... )\n0.146...\n```\n\n----------------------------------------\n\nTITLE: Accessing model coefficients in SGDClassifier\nDESCRIPTION: This snippet demonstrates how to access the learned model coefficients in a fitted SGDClassifier. The `coef_` attribute holds the weights assigned to each feature in the linear model. These coefficients determine the decision boundary learned by the classifier.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/sgd.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> clf.coef_\narray([[9.9..., 9.9...]])\n```\n\n----------------------------------------\n\nTITLE: K-D Tree Algorithm Configuration in scikit-learn\nDESCRIPTION: Specifies K-D tree based neighbor searches which partition space along data axes. Performs better for low-dimensional data (D < 20) with O[D N log(N)] complexity.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neighbors.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nalgorithm = 'kd_tree'\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using a VotingRegressor in Python\nDESCRIPTION: This snippet demonstrates how to create and use a VotingRegressor in scikit-learn, combining Gradient Boosting, Random Forest, and Linear Regression models.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import VotingRegressor\n\n# Loading some example data\nX, y = load_diabetes(return_X_y=True)\n\n# Training classifiers\nreg1 = GradientBoostingRegressor(random_state=1)\nreg2 = RandomForestRegressor(random_state=1)\nreg3 = LinearRegression()\nereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\nereg = ereg.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Using GaussianMixture Class - Python scikit-learn\nDESCRIPTION: The GaussianMixture class implements the expectation-maximization algorithm for fitting mixture-of-Gaussian models. It provides methods for fitting models (fit), making predictions (predict), and handling different covariance constraints.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/mixture.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.mixture import BayesianGaussianMixture\n```\n\n----------------------------------------\n\nTITLE: Classifier API Requirements\nDESCRIPTION: Required method implementations for scikit-learn classifier estimators including fit, predict, and score methods. May also implement decision_function, predict_proba and predict_log_proba.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/glossary.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef fit(X, y)\ndef predict(X)\ndef score(X, y)\n```\n\n----------------------------------------\n\nTITLE: Converting scikit-learn model to ONNX format\nDESCRIPTION: This snippet shows how to convert a trained scikit-learn model to ONNX format using skl2onnx. It demonstrates creating the ONNX representation and saving it to a file. This conversion requires information about the input data type.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/model_persistence.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom skl2onnx import to_onnx\nonx = to_onnx(clf, X[:1].astype(numpy.float32), target_opset=12)\nwith open(\"filename.onnx\", \"wb\") as f:\n    f.write(onx.SerializeToString())\n```\n\n----------------------------------------\n\nTITLE: Probability estimation with SGDClassifier\nDESCRIPTION: This snippet demonstrates how to use the `predict_proba` method of a trained SGDClassifier with `loss=\"log_loss\"` to estimate probabilities for each class. It first initializes and fits the SGDClassifier with logistic loss, and then predicts the probability of a sample belonging to each class. The output is a NumPy array with probability estimates for each class.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/sgd.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> clf = SGDClassifier(loss=\"log_loss\", max_iter=5).fit(X, y)\n>>> clf.predict_proba([[1., 1.]]) # doctest: +SKIP\narray([[0.00..., 0.99...]])\n```\n\n----------------------------------------\n\nTITLE: Normalization: Using the `normalize` Function\nDESCRIPTION: This snippet demonstrates how to normalize data using the `normalize` function from the `preprocessing` module. It normalizes a sample dataset using the L2 norm and prints the normalized array.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> X = [[ 1., -1.,  2.],\n...      [ 2.,  0.,  0.],\n...      [ 0.,  1., -1.]]\n>>> X_normalized = preprocessing.normalize(X, norm='l2')\n\n>>> X_normalized\narray([[ 0.40..., -0.40...,  0.81...],\n       [ 1.  ...,  0.  ...,  0.  ...],\n       [ 0.  ...,  0.70..., -0.70...]])\n```\n\n----------------------------------------\n\nTITLE: Demonstrating OneHotEncoder with Infrequent Categories in Python\nDESCRIPTION: This snippet shows how to use OneHotEncoder to group infrequent categories together. It sets min_frequency=6 to consider 'dog' and 'snake' as infrequent categories.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n>>> X = np.array([['dog'] * 5 + ['cat'] * 20 + ['rabbit'] * 10 +\n...               ['snake'] * 3], dtype=object).T\n>>> enc = preprocessing.OneHotEncoder(min_frequency=6, sparse_output=False).fit(X)\n>>> enc.infrequent_categories_\n[array(['dog', 'snake'], dtype=object)]\n>>> enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))\narray([[0., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n```\n\n----------------------------------------\n\nTITLE: Loading Built-in Dataset Example\nDESCRIPTION: Demonstrates loading a built-in dataset (breast cancer) from scikit-learn's dataset collection.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/minimal_reproducer.rst#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import load_breast_cancer\n\nX, y = load_breast_cancer(return_X_y=True)\n```\n\n----------------------------------------\n\nTITLE: Iterative MLP Training with Warm Start\nDESCRIPTION: Example showing how to implement custom iterative training of MLPClassifier using warm_start parameter. This allows for more control over the training process and enables additional monitoring between iterations.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neural_networks_supervised.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nX = [[0., 0.], [1., 1.]]\ny = [0, 1]\nclf = MLPClassifier(hidden_layer_sizes=(15,), random_state=1, max_iter=1, warm_start=True)\nfor i in range(10):\n    clf.fit(X, y)\n    # additional monitoring / inspection\n```\n\n----------------------------------------\n\nTITLE: Monotonic Constraints in HistGradientBoostingRegressor\nDESCRIPTION: Example of implementing monotonic constraints in gradient boosting to enforce positive or negative relationships between features and predictions.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\n# monotonic increase, monotonic decrease, and no constraint on the 3 features\ngbdt = HistGradientBoostingRegressor(monotonic_cst=[1, -1, 0])\n```\n\n----------------------------------------\n\nTITLE: Implementing StratifiedKFold for Cross-Validation with Imbalanced Classes in Python\nDESCRIPTION: This code demonstrates how to use StratifiedKFold for cross-validation with imbalanced classes, showing how it preserves class ratios in training and test sets. The example compares StratifiedKFold with regular KFold on a dataset with 45 samples of class 0 and 5 samples of class 1.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/cross_validation.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport numpy as np\nX, y = np.ones((50, 1)), np.hstack(([0] * 45, [1] * 5))\nskf = StratifiedKFold(n_splits=3)\nfor train, test in skf.split(X, y):\n    print('train -  {}   |   test -  {}'.format(\n        np.bincount(y[train]), np.bincount(y[test])))\ntrain -  [30  3]   |   test -  [15  2]\ntrain -  [30  3]   |   test -  [15  2]\ntrain -  [30  4]   |   test -  [15  1]\nkf = KFold(n_splits=3)\nfor train, test in kf.split(X, y):\n    print('train -  {}   |   test -  {}'.format(\n        np.bincount(y[train]), np.bincount(y[test])))\ntrain -  [28  5]   |   test -  [17]\ntrain -  [28  5]   |   test -  [17]\ntrain -  [34]   |   test -  [11  5]\n```\n\n----------------------------------------\n\nTITLE: Random State Generation in Python using check_random_state\nDESCRIPTION: Demonstrates the usage of check_random_state utility to create a numpy RandomState object from a seed value for consistent random number generation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/utilities.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.utils import check_random_state\n>>> random_state = 0\n>>> random_state = check_random_state(random_state)\n>>> random_state.rand(4)\narray([0.5488135 , 0.71518937, 0.60276338, 0.54488318])\n```\n\n----------------------------------------\n\nTITLE: Using NMF with Frobenius Norm in Python\nDESCRIPTION: Example demonstrating how to use Non-negative Matrix Factorization (NMF) with scikit-learn to decompose a matrix into two non-negative matrices W and H. The example shows initialization, fitting, and transformation of new data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/decomposition.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nX = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\nfrom sklearn.decomposition import NMF\nmodel = NMF(n_components=2, init='random', random_state=0)\nW = model.fit_transform(X)\nH = model.components_\nX_new = np.array([[1, 0], [1, 6.1], [1, 0], [1, 4], [3.2, 1], [0, 4]])\nW_new = model.transform(X_new)\n```\n\n----------------------------------------\n\nTITLE: Setting Interaction Constraints for Gradient Boosting in Python\nDESCRIPTION: Demonstrates how to set interaction constraints for HistGradientBoostingClassifier and HistGradientBoostingRegressor using the new interaction_cst parameter.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nensemble.HistGradientBoostingClassifier(interaction_cst=...)\n```\n\nLANGUAGE: Python\nCODE:\n```\nensemble.HistGradientBoostingRegressor(interaction_cst=...)\n```\n\n----------------------------------------\n\nTITLE: Imputing Categorical Data with SimpleImputer in Python\nDESCRIPTION: This snippet demonstrates how to use SimpleImputer with categorical data represented as string values or pandas categoricals, using the 'most_frequent' strategy.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/impute.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\ndf = pd.DataFrame([[\"a\", \"x\"],\n                   [np.nan, \"y\"],\n                   [\"a\", np.nan],\n                   [\"b\", \"y\"]], dtype=\"category\")\nimp = SimpleImputer(strategy=\"most_frequent\")\nprint(imp.fit_transform(df))\n```\n\n----------------------------------------\n\nTITLE: Creating a FeatureUnion with Multiple PCA Transformations\nDESCRIPTION: Shows how to combine multiple feature extraction techniques using FeatureUnion, applying linear and kernel PCA in parallel\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import KernelPCA\nestimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]\ncombined = FeatureUnion(estimators)\ncombined\n```\n\n----------------------------------------\n\nTITLE: Computing Homogeneity and Completeness Scores in Python with scikit-learn\nDESCRIPTION: This snippet demonstrates how to calculate homogeneity and completeness scores for clustering results using scikit-learn's metrics module. It uses example ground truth labels and predicted cluster labels.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import metrics\nlabels_true = [0, 0, 0, 1, 1, 1]\nlabels_pred = [0, 0, 1, 1, 2, 2]\n\nmetrics.homogeneity_score(labels_true, labels_pred)\nmetrics.completeness_score(labels_true, labels_pred)\n```\n\n----------------------------------------\n\nTITLE: Calculating D2 Log Loss Score in Python using scikit-learn\nDESCRIPTION: This code snippet demonstrates how to use the d2_log_loss_score function from scikit-learn to calculate the D2 score with log loss for classification tasks. It includes examples with different input scenarios.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics import d2_log_loss_score\ny_true = [1, 1, 2, 3]\ny_pred = [\n   [0.5, 0.25, 0.25],\n   [0.5, 0.25, 0.25],\n   [0.5, 0.25, 0.25],\n   [0.5, 0.25, 0.25],\n]\nd2_log_loss_score(y_true, y_pred)\n\ny_true = [1, 2, 3]\ny_pred = [\n    [0.98, 0.01, 0.01],\n    [0.01, 0.98, 0.01],\n    [0.01, 0.01, 0.98],\n]\nd2_log_loss_score(y_true, y_pred)\n\ny_true = [1, 2, 3]\ny_pred = [\n    [0.1, 0.6, 0.3],\n    [0.1, 0.6, 0.3],\n    [0.4, 0.5, 0.1],\n]\nd2_log_loss_score(y_true, y_pred)\n```\n\n----------------------------------------\n\nTITLE: Estimating Sparse Inverse Covariance in Python with scikit-learn\nDESCRIPTION: This code demonstrates how to estimate a sparse inverse covariance matrix using the Graphical Lasso algorithm, either with the GraphicalLasso estimator or the GraphicalLassoCV for automatic alpha parameter selection.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/covariance.rst#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.covariance import GraphicalLasso, GraphicalLassoCV\n\n# Using GraphicalLasso with a fixed alpha\ncov = GraphicalLasso(alpha=0.1)\ncov.fit(X)\n\n# Using GraphicalLassoCV to select alpha automatically\ncov = GraphicalLassoCV()\ncov.fit(X)\n```\n\n----------------------------------------\n\nTITLE: Using HalvingGridSearchCV with Standard Elimination in scikit-learn\nDESCRIPTION: Example demonstrating HalvingGridSearchCV with default aggressive_elimination=False, showing how candidate models are evaluated with limited resources.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/grid_search.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.svm import SVC\n>>> from sklearn.experimental import enable_halving_search_cv  # noqa\n>>> from sklearn.model_selection import HalvingGridSearchCV\n>>> import pandas as pd\n>>> param_grid = {'kernel': ('linear', 'rbf'),\n...               'C': [1, 10, 100]}\n>>> base_estimator = SVC(gamma='scale')\n>>> X, y = make_classification(n_samples=1000)\n>>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\n...                          factor=2, max_resources=40,\n...                          aggressive_elimination=False).fit(X, y)\n>>> sh.n_resources_\n[20, 40]\n>>> sh.n_candidates_\n[6, 3]\n```\n\n----------------------------------------\n\nTITLE: Using MissingIndicator for Missing Value Detection in Python\nDESCRIPTION: Shows how to use MissingIndicator to create binary matrices indicating missing values, with custom missing value placeholders and feature selection options.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/impute.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.impute import MissingIndicator\n>>> X = np.array([[-1, -1, 1, 3],\n...               [4, -1, 0, -1],\n...               [8, -1, 1, 0]])\n>>> indicator = MissingIndicator(missing_values=-1)\n>>> mask_missing_values_only = indicator.fit_transform(X)\n>>> mask_missing_values_only\narray([[ True,  True, False],\n       [False,  True,  True],\n       [False,  True, False]])\n\n>>> indicator = MissingIndicator(missing_values=-1, features=\"all\")\n>>> mask_all = indicator.fit_transform(X)\n>>> mask_all\narray([[ True,  True, False, False],\n       [False,  True, False,  True],\n       [False,  True, False, False]])\n```\n\n----------------------------------------\n\nTITLE: Calculating Hinge Loss for Binary SVM Classification in Python\nDESCRIPTION: This snippet demonstrates how to use the hinge_loss function with a LinearSVC classifier for a binary classification problem. It shows model fitting, decision function calculation, and hinge loss computation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn import svm\n>>> from sklearn.metrics import hinge_loss\n>>> X = [[0], [1]]\n>>> y = [-1, 1]\n>>> est = svm.LinearSVC(random_state=0)\n>>> est.fit(X, y)\nLinearSVC(random_state=0)\n>>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n>>> pred_decision\narray([-2.18...,  2.36...,  0.09...])\n>>> hinge_loss([-1, 1, 1], pred_decision)\n0.3...\n```\n\n----------------------------------------\n\nTITLE: Calculating Pairwise Distances and Kernels in Python using scikit-learn\nDESCRIPTION: Demonstrates how to use pairwise_distances and pairwise_kernels functions from scikit-learn to calculate distances and kernels between sets of samples. The example shows Manhattan distance and linear kernel calculations.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/metrics.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.metrics.pairwise import pairwise_kernels\nX = np.array([[2, 3], [3, 5], [5, 8]])\nY = np.array([[1, 0], [2, 1]])\npairwise_distances(X, Y, metric='manhattan')\npairwise_distances(X, metric='manhattan')\npairwise_kernels(X, Y, metric='linear')\n```\n\n----------------------------------------\n\nTITLE: Calculating Rand Index with scikit-learn in Python\nDESCRIPTION: This snippet demonstrates how to calculate the Rand Index using `sklearn.metrics.rand_score`. It compares true labels (`labels_true`) with predicted labels (`labels_pred`) to quantify the similarity between two clusterings. The Rand Index measures the agreement between the two assignments, ignoring permutations.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn import metrics\n>>> labels_true = [0, 0, 0, 1, 1, 1]\n>>> labels_pred = [0, 0, 1, 1, 2, 2]\n>>> metrics.rand_score(labels_true, labels_pred)\n0.66...\n```\n\n----------------------------------------\n\nTITLE: Initializing CountVectorizer with default parameters\nDESCRIPTION: This snippet initializes a `CountVectorizer` object with its default parameters. The default settings are reasonable for many text analysis tasks and will tokenize text and count word occurrences.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> vectorizer = CountVectorizer()\n>>> vectorizer\nCountVectorizer()\n```\n\n----------------------------------------\n\nTITLE: Working with Different Versions of OpenML Datasets\nDESCRIPTION: Shows how to fetch specific versions of datasets from OpenML by using either data_id or by specifying both name and version. The example demonstrates fetching different versions of the 'iris' dataset.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/datasets/loading_other_datasets.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\niris = fetch_openml(name=\"iris\")\niris.details['version']  \niris.details['id']  \n\niris_61 = fetch_openml(data_id=61)\niris_61.details['version']\niris_61.details['id']\n\niris_969 = fetch_openml(data_id=969)\niris_969.details['version']\niris_969.details['id']\n\nnp.unique(iris_969.target)\n\niris_version_3 = fetch_openml(name=\"iris\", version=3)\niris_version_3.details['version']\niris_version_3.details['id']\n```\n\n----------------------------------------\n\nTITLE: Computing Decision Function for Outlier Detection\nDESCRIPTION: Shows how to use the decision_function method to get raw scoring values where negative values indicate outliers and non-negative values indicate inliers.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/outlier_detection.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nestimator.decision_function(X_test)\n```\n\n----------------------------------------\n\nTITLE: Persisting models with skops.io for enhanced security\nDESCRIPTION: This snippet shows how to save a scikit-learn model using skops.io, which provides a more secure alternative to pickle-based formats. It demonstrates using the dump function to save the model to a file.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/model_persistence.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport skops.io as sio\nobj = sio.dump(clf, \"filename.skops\")\n```\n\n----------------------------------------\n\nTITLE: Unweighted Feature Selection with Metadata Routing (Python)\nDESCRIPTION: Demonstrates using a pipeline with SelectKBest and LogisticRegressionCV, where sample weights are only routed to LogisticRegressionCV and not to SelectKBest.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/metadata_routing.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> weighted_acc = make_scorer(accuracy_score).set_score_request(sample_weight=True)\n>>> lr = LogisticRegressionCV(\n...     cv=GroupKFold(), scoring=weighted_acc,\n... ).set_fit_request(sample_weight=True)\n>>> sel = SelectKBest(k=2)\n>>> pipe = make_pipeline(sel, lr)\n>>> cv_results = cross_validate(\n...     pipe,\n...     X,\n...     y,\n...     cv=GroupKFold(),\n...     params={\"sample_weight\": my_weights, \"groups\": my_groups},\n...     scoring=weighted_acc,\n... )\n```\n\n----------------------------------------\n\nTITLE: Using Mean Absolute Error in Python with scikit-learn\nDESCRIPTION: This example demonstrates how to use the mean_absolute_error function to evaluate the performance of regression models in scikit-learn. It showcases both single-output and multi-output scenarios with different ways to handle multiple outputs.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics import mean_absolute_error\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nmean_absolute_error(y_true, y_pred)\n0.5\ny_true = [[0.5, 1], [-1, 1], [7, -6]]\ny_pred = [[0, 2], [-1, 2], [8, -5]]\nmean_absolute_error(y_true, y_pred)\n0.75\nmean_absolute_error(y_true, y_pred, multioutput='raw_values')\narray([0.5, 1. ])\nmean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n0.85...\n```\n\n----------------------------------------\n\nTITLE: Sample Weight Usage in HistGradientBoostingClassifier\nDESCRIPTION: Demonstrates how to use sample weights to ignore specific training samples in gradient boosting classification. The example shows setting weights to 0 to completely ignore samples during training.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# ignore the first 2 training samples by setting their weight to 0\nsample_weight = [0, 0, 1, 1]\ngb = HistGradientBoostingClassifier(min_samples_leaf=1)\ngb.fit(X, y, sample_weight=sample_weight)\nHistGradientBoostingClassifier(...)\ngb.predict([[1, 0]])\narray([1])\ngb.predict_proba([[1, 0]])[0, 1]\nnp.float64(0.999...)\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with StandardScaler in scikit-learn\nDESCRIPTION: Example demonstrating how to use StandardScaler transformer to standardize features by removing the mean and scaling to unit variance. Shows the fit and transform pipeline for preprocessing.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/getting_started.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.preprocessing import StandardScaler\n>>> X = [[0, 15],\n...      [1, -10]]\n>>> # scale data according to computed scaling values\n>>> StandardScaler().fit(X).transform(X)\narray([[-1.,  1.],\n         [ 1., -1.]])\n```\n\n----------------------------------------\n\nTITLE: Accessing Pipeline Steps in scikit-learn\nDESCRIPTION: Demonstrates different ways to access individual steps within a scikit-learn Pipeline, including using slicing notation, indexing, and the named_steps attribute.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> pipe[:1]\nPipeline(steps=[('reduce_dim', PCA())])\n>>> pipe[-1:]\nPipeline(steps=[('clf', SVC())])\n>>> pipe.steps[0]\n('reduce_dim', PCA())\n>>> pipe[0]\nPCA()\n>>> pipe['reduce_dim']\nPCA()\n>>> pipe.named_steps.reduce_dim is pipe['reduce_dim']\nTrue\n```\n\n----------------------------------------\n\nTITLE: Fetching OpenML Dataset by ID\nDESCRIPTION: Demonstrates how to fetch a dataset from OpenML using its unique data_id instead of its name. This ensures you always get exactly the same dataset regardless of potential changes to dataset versions.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/datasets/loading_other_datasets.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmice = fetch_openml(data_id=40966)\nmice.details\n```\n\n----------------------------------------\n\nTITLE: Adding custom out-of-bag scoring to RandomForest and ExtraTrees\nDESCRIPTION: A custom out-of-bag score can now be computed by passing a callable to the oob_score parameter of RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier and ExtraTreesRegressor.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.3.rst#2025-04-14_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nRandomForestClassifier(oob_score=custom_oob_score_func)\n```\n\n----------------------------------------\n\nTITLE: Creating Circles and Moons Datasets in Python\nDESCRIPTION: Demonstrates the use of make_circles and make_moons functions to generate 2D binary classification datasets. These datasets are challenging for certain algorithms and useful for visualization.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/datasets/sample_generators.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_circles, make_moons\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n\nX, Y = make_circles(noise=0.1, factor=0.3, random_state=0)\nax1.scatter(X[:, 0], X[:, 1], c=Y)\nax1.set_title(\"make_circles\")\n\nX, Y = make_moons(noise=0.1, random_state=0)\nax2.scatter(X[:, 0], X[:, 1], c=Y)\nax2.set_title(\"make_moons\")\n\nplt.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Exporting Decision Tree as Text\nDESCRIPTION: Shows how to export a trained decision tree in text format using export_text function with the Iris dataset.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/tree.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> from sklearn.tree import export_text\n>>> iris = load_iris()\n>>> decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n>>> decision_tree = decision_tree.fit(iris.data, iris.target)\n>>> r = export_text(decision_tree, feature_names=iris['feature_names'])\n>>> print(r)\n```\n\n----------------------------------------\n\nTITLE: Importing GaussianProcessRegressor in Python\nDESCRIPTION: This snippet shows how to import the GaussianProcessRegressor class from scikit-learn's gaussian_process module. This class is used for implementing Gaussian Process Regression.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/gaussian_process.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.gaussian_process import GaussianProcessRegressor\n```\n\n----------------------------------------\n\nTITLE: Improving SVM Performance with RBF Kernel in Python\nDESCRIPTION: This code snippet demonstrates how changing the kernel of an SVM classifier from linear to RBF (Radial Basis Function) can significantly improve model performance on the same dataset, showing the importance of selecting appropriate model parameters.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nclf = SVC(kernel='rbf', C=1).fit(X_train, y_train)\nclf.score(X_test, y_test)\n0.94...\n```\n\n----------------------------------------\n\nTITLE: Estimating Covariance with Oracle Approximating Shrinkage in Python using scikit-learn\nDESCRIPTION: This snippet shows how to estimate the covariance matrix using the Oracle Approximating Shrinkage method, either with the oas function or by fitting an OAS object to the data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/covariance.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.covariance import oas, OAS\n\n# Using the function\noas_cov, _ = oas(X)\n\n# Using the object\ncov = OAS(assume_centered=False)\ncov.fit(X)\n```\n\n----------------------------------------\n\nTITLE: Computing Zero-One Loss in Python with scikit-learn\nDESCRIPTION: Examples of using the zero_one_loss function to compute classification error rates. The function can be used for both single-label and multilabel classification, with options to normalize the results.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.metrics import zero_one_loss\n>>> y_pred = [1, 2, 3, 4]\n>>> y_true = [2, 2, 3, 4]\n>>> zero_one_loss(y_true, y_pred)\n0.25\n>>> zero_one_loss(y_true, y_pred, normalize=False)\n1.0\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n0.5\n\n>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)),  normalize=False)\n1.0\n```\n\n----------------------------------------\n\nTITLE: Inspecting Data Shape and Target Classes from OpenML Dataset\nDESCRIPTION: Demonstrates how to examine the shape of the data and target arrays and view the unique classes in a dataset downloaded from OpenML. This helps understand the dataset's structure.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/datasets/loading_other_datasets.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmice.data.shape\nmice.target.shape\nnp.unique(mice.target)\n```\n\n----------------------------------------\n\nTITLE: Customizing CountVectorizer with Custom Tokenizer in Python\nDESCRIPTION: Demonstrates how to customize the CountVectorizer by providing a custom tokenizer function. This allows for flexible text preprocessing and tokenization based on specific requirements.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n>>> def my_tokenizer(s):\n...     return s.split()\n...\n>>> vectorizer = CountVectorizer(tokenizer=my_tokenizer)\n>>> vectorizer.build_analyzer()(u\"Some... punctuation!\") == (\n...     ['some...', 'punctuation!'])\nTrue\n```\n\n----------------------------------------\n\nTITLE: Using DictVectorizer for NLP Feature Windows in Python\nDESCRIPTION: This snippet demonstrates how to use DictVectorizer for Natural Language Processing tasks, particularly for extracting feature windows around words of interest. It shows vectorization of Part of Speech (PoS) tags and contextual words.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npos_window = [\n    {\n        'word-2': 'the',\n        'pos-2': 'DT',\n        'word-1': 'cat',\n        'pos-1': 'NN',\n        'word+1': 'on',\n        'pos+1': 'PP',\n    },\n    # in a real application one would extract many such dictionaries\n]\n\nvec = DictVectorizer()\npos_vectorized = vec.fit_transform(pos_window)\npos_vectorized\n<Compressed Sparse...dtype 'float64'\n  with 6 stored elements and shape (1, 6)>\npos_vectorized.toarray()\narray([[1., 1., 1., 1., 1., 1.]])\nvec.get_feature_names_out()\narray(['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat',\n       'word-2=the'], ...)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Scorers with n_jobs > 1\nDESCRIPTION: This snippet showcases how to use custom scoring functions with parallel processing (n_jobs > 1) in scikit-learn. It recommends importing the custom scoring function from a separate module (`custom_scorer_module.py`) for robustness across different joblib backends. This is necessary to avoid issues when defining custom scoring functions alongside the calling function, especially with non-default joblib backends.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from custom_scorer_module import custom_scoring_function # doctest: +SKIP\n>>> cross_val_score(model,\n...  X_train,\n...  y_train,\n...  scoring=make_scorer(custom_scoring_function, greater_is_better=False),\n...  cv=5,\n...  n_jobs=-1) # doctest: +SKIP\n```\n\n----------------------------------------\n\nTITLE: Using KernelDensity Estimator in scikit-learn\nDESCRIPTION: This snippet demonstrates how to use the KernelDensity estimator from scikit-learn to fit a Gaussian kernel density model to a 2D dataset and compute the log-likelihood of samples.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/density.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.neighbors import KernelDensity\nimport numpy as np\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\nkde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X)\nkde.score_samples(X)\n```\n\n----------------------------------------\n\nTITLE: Generating Blobs Dataset with make_blobs in Python\nDESCRIPTION: Creates a multiclass dataset using make_blobs function from sklearn.datasets. It generates three normally-distributed clusters and plots them using matplotlib.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/datasets/sample_generators.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(centers=3, cluster_std=0.5, random_state=0)\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.title(\"Three normally-distributed clusters\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Accessing Model Coefficients in Bayesian Ridge Regression\nDESCRIPTION: Demonstrates how to access the learned coefficients (weights) from a fitted Bayesian Ridge Regression model.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> reg.coef_\narray([0.49999993, 0.49999993])\n```\n\n----------------------------------------\n\nTITLE: Using top_k_accuracy_score in Python with scikit-learn\nDESCRIPTION: Example showing how to compute top-k accuracy score using scikit-learn. The function measures the fraction of samples where the true label is among the k labels most likely according to the model's predicted scores.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom sklearn.metrics import top_k_accuracy_score\ny_true = np.array([0, 1, 2, 2])\ny_score = np.array([[0.5, 0.2, 0.2],\n                   [0.3, 0.4, 0.2],\n                   [0.2, 0.4, 0.3],\n                   [0.7, 0.2, 0.1]])\ntop_k_accuracy_score(y_true, y_score, k=2)\n0.75\n# Not normalizing gives the number of \"correctly\" classified samples\ntop_k_accuracy_score(y_true, y_score, k=2, normalize=False)\n3.0\n```\n\n----------------------------------------\n\nTITLE: Output Code Multiclass Classification with Linear SVC\nDESCRIPTION: Demonstrates multiclass learning using OutputCodeClassifier with LinearSVC on the Iris dataset, using a randomly generated code book for classification\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/multiclass.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import datasets\nfrom sklearn.multiclass import OutputCodeClassifier\nfrom sklearn.svm import LinearSVC\nX, y = datasets.load_iris(return_X_y=True)\nclf = OutputCodeClassifier(LinearSVC(random_state=0), code_size=2, random_state=0)\nclf.fit(X, y).predict(X)\n```\n\n----------------------------------------\n\nTITLE: Using SplineTransformer in scikit-learn for B-spline Transformation\nDESCRIPTION: This snippet demonstrates how to use the SplineTransformer class to transform a simple array of values into B-spline basis features. It shows the banded matrix output characteristic of B-splines with degree=2 and 3 knots.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn.preprocessing import SplineTransformer\n>>> X = np.arange(5).reshape(5, 1)\n>>> X\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])\n>>> spline = SplineTransformer(degree=2, n_knots=3)\n>>> spline.fit_transform(X)\narray([[0.5  , 0.5  , 0.   , 0.   ],\n       [0.125, 0.75 , 0.125, 0.   ],\n       [0.   , 0.5  , 0.5  , 0.   ],\n       [0.   , 0.125, 0.75 , 0.125],\n       [0.   , 0.   , 0.5  , 0.5  ]])\n```\n\n----------------------------------------\n\nTITLE: Accessing Named Estimators in VotingClassifier (Python)\nDESCRIPTION: Adds a named_estimators_ parameter to VotingClassifier to access fitted estimators. This allows retrieving individual fitted estimators after training.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nnamed_estimators_\n```\n\n----------------------------------------\n\nTITLE: Using the OPTICS Clustering Algorithm in Scikit-learn\nDESCRIPTION: Demonstrates the functionalities of the OPTICS algorithm in Scikit-learn, which based its clustering on reachability distances and orders for each sample. The algorithm doesn't rely on a single eps value, instead, it utilizes a range and builds a reachability graph. This snippet covers concepts like reachability plot, and cluster extraction with DBSCAN-style methods. It requires the Scikit-learn package. Inputs include a dataset to fit the model and outputs consist of reachability distance and ordering attributes. Key parameters include max_eps, with limitations on runtime based on its value.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.cluster import OPTICS\noptics_instance = OPTICS()\noptics_instance.fit(data)\nreachability = optics_instance.reachability_\nordering = optics_instance.ordering_\nclusters = optics_instance.cluster_optics_dbscan(eps=0.5)\n```\n\n----------------------------------------\n\nTITLE: Multiple Metric Evaluation with cross_validate in Python\nDESCRIPTION: This code demonstrates how to evaluate a model using multiple performance metrics simultaneously with scikit-learn's cross_validate function. It returns test scores, fit times, and score times for each metric specified.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/cross_validation.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.model_selection import cross_validate\n>>> from sklearn.metrics import recall_score\n>>> scoring = ['precision_macro', 'recall_macro']\n>>> clf = svm.SVC(kernel='linear', C=1, random_state=0)\n>>> scores = cross_validate(clf, X, y, scoring=scoring)\n>>> sorted(scores.keys())\n['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']\n>>> scores['test_recall_macro']\narray([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])\n\n>>> from sklearn.metrics import make_scorer\n>>> scoring = {'prec_macro': 'precision_macro',\n```\n\n----------------------------------------\n\nTITLE: Applying GroupShuffleSplit for Random Partitions in Python\nDESCRIPTION: Illustrates the use of GroupShuffleSplit, which generates a sequence of randomized partitions where a subset of groups are held out for each split. This is useful when the number of groups is large.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/cross_validation.rst#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import GroupShuffleSplit\n\nX = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]\ny = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"a\"]\ngroups = [1, 1, 2, 2, 3, 3, 4, 4]\ngss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)\nfor train, test in gss.split(X, y, groups=groups):\n    print(\"%s %s\" % (train, test))\n```\n\n----------------------------------------\n\nTITLE: Training a Gradient Boosting Classifier with scikit-learn\nDESCRIPTION: This snippet demonstrates how to train a HistGradientBoostingClassifier on the iris dataset. It shows the basic workflow of initializing the model, loading the dataset, and fitting the model to the data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/model_persistence.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn import ensemble\n>>> from sklearn import datasets\n>>> clf = ensemble.HistGradientBoostingClassifier()\n>>> X, y = datasets.load_iris(return_X_y=True)\n>>> clf.fit(X, y)\nHistGradientBoostingClassifier()\n```\n\n----------------------------------------\n\nTITLE: Accessing Nested Parameters in scikit-learn Pipeline\nDESCRIPTION: Demonstrates how to access and set nested parameters of estimators within a Pipeline using the <estimator>__<parameter> syntax. This is useful for grid searches and parameter tuning.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> pipe = Pipeline(steps=[(\"reduce_dim\", PCA()), (\"clf\", SVC())])\n>>> pipe.set_params(clf__C=10)\nPipeline(steps=[('reduce_dim', PCA()), ('clf', SVC(C=10))])\n>>> from sklearn.model_selection import GridSearchCV\n>>> param_grid = dict(reduce_dim__n_components=[2, 5, 10],\n...                   clf__C=[0.1, 10, 100])\n>>> grid_search = GridSearchCV(pipe, param_grid=param_grid)\n>>> param_grid = dict(reduce_dim=['passthrough', PCA(5), PCA(10)],\n...                   clf=[SVC(), LogisticRegression()],\n...                   clf__C=[0.1, 10, 100])\n>>> grid_search = GridSearchCV(pipe, param_grid=param_grid)\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Values with HistGradientBoostingClassifier\nDESCRIPTION: Shows how HistGradientBoostingClassifier handles missing values (NaNs) during training and prediction, demonstrating its built-in support for missing data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.ensemble import HistGradientBoostingClassifier\n>>> import numpy as np\n\n>>> X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\n>>> y = [0, 0, 1, 1]\n\n>>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\n>>> gbdt.predict(X)\narray([0, 0, 1, 1])\n```\n\n----------------------------------------\n\nTITLE: Using ColumnTransformer with DataFrame Selection\nDESCRIPTION: Added make_column_selector function to select DataFrame columns based on name and dtype when using ColumnTransformer.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.compose import make_column_selector, ColumnTransformer\n\n# Select columns by type\nnumeric_selector = make_column_selector(dtype_include=np.number)\ntransformer = ColumnTransformer([(\"num\", scaler, numeric_selector)])\n```\n\n----------------------------------------\n\nTITLE: Using OneHotEncoder with Conditional Dropping in Python\nDESCRIPTION: Shows how to use OneHotEncoder with drop='if_binary' parameter to drop columns only for binary features while keeping all columns for features with more than two categories.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n>>> X = [['male', 'US', 'Safari'],\n...      ['female', 'Europe', 'Firefox'],\n...      ['female', 'Asia', 'Chrome']]\n>>> drop_enc = preprocessing.OneHotEncoder(drop='if_binary').fit(X)\n>>> drop_enc.categories_\n[array(['female', 'male'], dtype=object), array(['Asia', 'Europe', 'US'], dtype=object),\n array(['Chrome', 'Firefox', 'Safari'], dtype=object)]\n>>> drop_enc.transform(X).toarray()\narray([[1., 0., 0., 1., 0., 0., 1.],\n       [0., 0., 1., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 1., 0., 0.]])\n```\n\n----------------------------------------\n\nTITLE: Using Max Error in Python with scikit-learn\nDESCRIPTION: This example shows how to use the max_error function to compute the maximum residual error between predicted and true values, which captures the worst-case prediction error of a regression model.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics import max_error\ny_true = [3, 2, 7, 1]\ny_pred = [9, 2, 7, 1]\nmax_error(y_true, y_pred)\n6.0\n```\n\n----------------------------------------\n\nTITLE: Importing Kernel Classes for Gaussian Processes in Python\nDESCRIPTION: This snippet shows how to import specific kernel classes (RBF and DotProduct) from scikit-learn's gaussian_process.kernels module. These kernels are used to define the covariance function in Gaussian Processes.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/gaussian_process.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.gaussian_process.kernels import RBF, DotProduct\n```\n\n----------------------------------------\n\nTITLE: Using Label Ranking Average Precision Score in Python\nDESCRIPTION: Example demonstrating how to use the label_ranking_average_precision_score function to evaluate the quality of ranked labels predictions. The function compares binary ground truth labels with corresponding prediction scores.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_41\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn.metrics import label_ranking_average_precision_score\n>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n>>> label_ranking_average_precision_score(y_true, y_score)\n0.416...\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Metrics with Grid Search\nDESCRIPTION: Example showing how to use multiple evaluation metrics with GridSearchCV, including specifying which metric to use for selecting the best parameters.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import GridSearchCV\n\n# Define multiple metrics\nscoring = {\n    'accuracy': 'accuracy',\n    'precision': 'precision',\n    'recall': 'recall'\n}\n\n# Use 'accuracy' for selecting the best parameters\ngrid_search = GridSearchCV(\n    estimator, param_grid, \n    scoring=scoring,\n    refit='accuracy'\n)\n\ngrid_search.fit(X, y)\n\n# Access results for all metrics\nresults = grid_search.cv_results_\nprint(results['mean_test_accuracy'])\nprint(results['mean_test_precision'])\nprint(results['mean_test_recall'])\n```\n\n----------------------------------------\n\nTITLE: Weight Configuration for Nearest Neighbors\nDESCRIPTION: Demonstrates different weighting schemes for nearest neighbor calculations. Uniform weights treat all points equally while distance weights are inversely proportional to distance.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neighbors.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nweights = 'uniform'\nweights = 'distance'\n```\n\n----------------------------------------\n\nTITLE: Computing Homogeneity, Completeness, and V-measure Simultaneously in Python\nDESCRIPTION: This snippet shows how to compute homogeneity, completeness, and V-measure scores all at once using scikit-learn's combined function homogeneity_completeness_v_measure.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import metrics\nlabels_true = [0, 0, 0, 1, 1, 1]\nlabels_pred = [0, 0, 0, 1, 2, 2]\n\nmetrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\n```\n\n----------------------------------------\n\nTITLE: Predicting with a trained SGDClassifier\nDESCRIPTION: This snippet shows how to use a trained SGDClassifier to predict the class label for new data. It assumes that the SGDClassifier has already been fitted to training data. The `predict` method is called with a new data point, and the predicted class label is returned as a NumPy array.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/sgd.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> clf.predict([[2., 2.]])\narray([1])\n```\n\n----------------------------------------\n\nTITLE: Setting Monotonic Constraints for Gradient Boosting in Python\nDESCRIPTION: Demonstrates how to set monotonic constraints for HistGradientBoostingClassifier and HistGradientBoostingRegressor using a dictionary with feature names as keys.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nensemble.HistGradientBoostingClassifier(monotonic_cst={\"feature1\": 1, \"feature2\": -1})\n```\n\nLANGUAGE: Python\nCODE:\n```\nensemble.HistGradientBoostingRegressor(monotonic_cst={\"feature1\": 1, \"feature2\": -1})\n```\n\n----------------------------------------\n\nTITLE: Implementing Bayesian Ridge Regression in Python with scikit-learn\nDESCRIPTION: Demonstrates how to initialize, fit and predict using Bayesian Ridge Regression model. The example shows basic usage with a simple dataset of 2D points.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn import linear_model\n>>> X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]\n>>> Y = [0., 1., 2., 3.]\n>>> reg = linear_model.BayesianRidge()\n>>> reg.fit(X, Y)\nBayesianRidge()\n```\n\n----------------------------------------\n\nTITLE: Weighted Scoring and Unweighted Fitting with Metadata Routing (Python)\nDESCRIPTION: Shows how to configure LogisticRegressionCV to not use sample weights for fitting while still using them for scoring in cross_validate.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/metadata_routing.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> weighted_acc = make_scorer(accuracy_score).set_score_request(sample_weight=True)\n>>> lr = LogisticRegressionCV(\n...     cv=GroupKFold(), scoring=weighted_acc,\n... ).set_fit_request(sample_weight=False)\n>>> cv_results = cross_validate(\n...     lr,\n...     X,\n...     y,\n...     cv=GroupKFold(),\n...     params={\"sample_weight\": my_weights, \"groups\": my_groups},\n...     scoring=weighted_acc,\n... )\n```\n\n----------------------------------------\n\nTITLE: Adding Trees to a RandomForest with warm_start in scikit-learn\nDESCRIPTION: Demonstrates how to use the warm_start parameter to add more trees to an already fitted RandomForestClassifier model. The example shows initializing a model with 10 trees, then adding 10 more trees by setting warm_start=True and increasing n_estimators.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.ensemble import RandomForestClassifier\n\n>>> X, y = make_classification(n_samples=100, random_state=1)\n>>> clf = RandomForestClassifier(n_estimators=10)\n>>> clf = clf.fit(X, y)  # fit with 10 trees\n>>> len(clf.estimators_)\n10\n>>> # set warm_start and increase num of estimators\n>>> _ = clf.set_params(n_estimators=20, warm_start=True)\n>>> _ = clf.fit(X, y) # fit additional 10 trees\n>>> len(clf.estimators_)\n20\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with PolynomialFeatures in Python\nDESCRIPTION: This snippet demonstrates how to use the PolynomialFeatures transformer to convert an input matrix into polynomial features of degree 2. It shows the transformation of a 3x2 input matrix.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\nX = np.arange(6).reshape(3, 2)\nX\npoly = PolynomialFeatures(degree=2)\npoly.fit_transform(X)\n```\n\n----------------------------------------\n\nTITLE: Normalization: Using the `Normalizer` Class\nDESCRIPTION: This snippet demonstrates how to normalize data using the `Normalizer` class from the `preprocessing` module. It creates a Normalizer instance, fits it to a sample dataset (though the fit method is stateless), and then prints the instance.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing\n>>> normalizer\nNormalizer()\n```\n\n----------------------------------------\n\nTITLE: Setting max_samples in RandomForestClassifier in Python\nDESCRIPTION: Fix to RandomForestClassifier and RandomForestRegressor where max_samples=1.0 is now correctly interpreted as using all n_samples for bootstrapping.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nensemble.RandomForestClassifier(max_samples=1.0)\n```\n\n----------------------------------------\n\nTITLE: Using max_categories Parameter in OneHotEncoder in Python\nDESCRIPTION: This snippet illustrates how to use the max_categories parameter in OneHotEncoder to limit the number of output features. It sets max_categories=2, resulting in two features: one for 'cat' and one for all other categories.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n>>> enc = preprocessing.OneHotEncoder(max_categories=2, sparse_output=False)\n>>> enc = enc.fit(X)\n>>> enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])\narray([[0., 1.],\n       [1., 0.],\n       [0., 1.],\n       [0., 1.]])\n```\n\n----------------------------------------\n\nTITLE: Implementing StratifiedGroupKFold for Balanced Cross-Validation with Grouped Data in Python\nDESCRIPTION: This code demonstrates StratifiedGroupKFold, which combines both stratification and group-based splitting. It attempts to preserve both the class distribution and group integrity across folds, which is useful for imbalanced datasets where samples belong to distinct groups.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/cross_validation.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import StratifiedGroupKFold\nX = list(range(18))\ny = [1] * 6 + [0] * 12\ngroups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6]\nsgkf = StratifiedGroupKFold(n_splits=3)\nfor train, test in sgkf.split(X, y, groups=groups):\n    print(\"%s %s\" % (train, test))\n[ 0  2  3  4  5  6  7 10 11 15 16 17] [ 1  8  9 12 13 14]\n[ 0  1  4  5  6  7  8  9 11 12 13 14] [ 2  3 10 15 16 17]\n[ 1  2  3  8  9 10 12 13 14 15 16 17] [ 0  4  5  6  7 11]\n```\n\n----------------------------------------\n\nTITLE: Multi-class Classification PDPs\nDESCRIPTION: Shows how to create partial dependence plots for multi-class classification problems using the iris dataset and specifying target class.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/partial_dependence.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import load_iris\niris = load_iris()\nmc_clf = GradientBoostingClassifier(n_estimators=10,\n    max_depth=1).fit(iris.data, iris.target)\nfeatures = [3, 2, (3, 2)]\nPartialDependenceDisplay.from_estimator(mc_clf, X, features, target=0)\n```\n\n----------------------------------------\n\nTITLE: Using Normalizer to Transform Vectors to Unit Norm in Python\nDESCRIPTION: Demonstrates using a previously defined normalizer object to transform feature arrays to unit norm. The example shows applying normalization to different input matrices, with the result being vectors with unit norm.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> normalizer.transform(X)\narray([[ 0.40..., -0.40...,  0.81...],\n       [ 1.  ...,  0.  ...,  0.  ...],\n       [ 0.  ...,  0.70..., -0.70...]])\n\n>>> normalizer.transform([[-1.,  1., 0.]])\narray([[-0.70...,  0.70...,  0.  ...]])\n```\n\n----------------------------------------\n\nTITLE: Adjusted Mutual Information with Permuted Labels in scikit-learn in Python\nDESCRIPTION: This snippet shows that the Adjusted Mutual Information (AMI) score is invariant to permutations of the predicted labels. Changing the values of `labels_pred` while maintaining the same cluster structure does not affect the calculated AMI score.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> labels_pred = [1, 1, 0, 0, 3, 3]\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP\n0.22504...\n```\n\n----------------------------------------\n\nTITLE: Predicting Outliers with Scikit-learn\nDESCRIPTION: Demonstrates how to predict whether new observations are inliers (1) or outliers (-1) using the predict method after training.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/outlier_detection.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nestimator.predict(X_test)\n```\n\n----------------------------------------\n\nTITLE: MaxAbsScaler Usage Example\nDESCRIPTION: Shows implementation of MaxAbsScaler which scales features by their maximum absolute value. Particularly useful for sparse data or data already centered at zero.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nX_train = np.array([[ 1., -1.,  2.],\n                   [ 2.,  0.,  0.],\n                   [ 0.,  1., -1.]])\nmax_abs_scaler = preprocessing.MaxAbsScaler()\nX_train_maxabs = max_abs_scaler.fit_transform(X_train)\nX_test = np.array([[ -3., -1.,  4.]])\nX_test_maxabs = max_abs_scaler.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Handling Unknown Categories with OneHotEncoder in Python\nDESCRIPTION: Shows how to configure OneHotEncoder to handle unknown categories during transformation by using handle_unknown='infrequent_if_exist', which returns all zeros for unknown categories.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n>>> enc = preprocessing.OneHotEncoder(handle_unknown='infrequent_if_exist')\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n>>> enc.fit(X)\nOneHotEncoder(handle_unknown='infrequent_if_exist')\n>>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\narray([[1., 0., 0., 0., 0., 0.]])\n```\n\n----------------------------------------\n\nTITLE: Implementing Array API Support in Scikit-learn Estimators\nDESCRIPTION: Several scikit-learn estimators and functions now support the Array API, enabling compatibility with libraries like JAX, CuPy, and PyTorch. This allows for potential GPU-accelerated computations in certain scenarios.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.4.rst#2025-04-14_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy.array_api as xp\nfrom sklearn.metrics import accuracy_score\n\ny_true = xp.array([0, 1, 2, 3])\ny_pred = xp.array([0, 2, 1, 3])\nscore = accuracy_score(y_true, y_pred)\n```\n\n----------------------------------------\n\nTITLE: Importing RANSAC regressor in Python\nDESCRIPTION: Example of importing and using the RANSAC regressor from scikit-learn. RANSAC fits a model from random subsets of inliers from the complete data set.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.linear_model import RANSACRegressor\nransac = RANSACRegressor()\nransac.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Using TweedieRegressor for Poisson Regression in Python\nDESCRIPTION: This snippet demonstrates how to use TweedieRegressor to perform Poisson regression. It initializes the model with power=1 (Poisson distribution), alpha=0.5 for L2 regularization, and a log link function. The model is then fit on sample data and the coefficients are displayed.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.linear_model import TweedieRegressor\nreg = TweedieRegressor(power=1, alpha=0.5, link='log')\nreg.fit([[0, 0], [0, 1], [2, 2]], [0, 1, 2])\nTweedieRegressor(alpha=0.5, link='log', power=1)\nreg.coef_\narray([0.2463..., 0.4337...])\nreg.intercept_\nnp.float64(-0.7638...)\n```\n\n----------------------------------------\n\nTITLE: Plotting ROC Curve with RandomForestClassifier and Overlay with SVC\nDESCRIPTION: This Python snippet illustrates the generation of a ROC curve using a Random Forest Classifier, demonstrated through Scikit-learn's `RocCurveDisplay.from_estimator` method. An additional plot of a pre-computed SVC ROC curve is overlaid for comparison. The code requires `matplotlib.pyplot`, `sklearn.ensemble.RandomForestClassifier`, and prior definition of training and testing datasets. The random forest is fitted on the training set, and ROC curves from both classifiers are plotted on the same axes. The `alpha=0.8` parameter is used to set transparency levels for better visual clarity. Outputs comprise the `rfc_disp` display object for random forest ROC values and an overlay of the `svc_disp` ROC curve.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/visualizations.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators=10, random_state=42)\nrfc.fit(X_train, y_train)\n\nax = plt.gca()\nrfc_disp = RocCurveDisplay.from_estimator(rfc, X_test, y_test, ax=ax, alpha=0.8)\nsvc_disp.plot(ax=ax, alpha=0.8)\n```\n\n----------------------------------------\n\nTITLE: Using CategoricalNB in scikit-learn naive_bayes\nDESCRIPTION: New Categorical Naive Bayes classifier added to the naive_bayes module for handling categorical features.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nnaive_bayes.CategoricalNB\n```\n\n----------------------------------------\n\nTITLE: Adding Weak Learners with warm_start in Python\nDESCRIPTION: This snippet illustrates the `warm_start` capability in GradientBoostingRegressor that allows further training of an existing model by adding more trees. It retains the model's previous state, demonstrating how `n_estimators` can be increased for model improvement.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn.metrics import mean_squared_error\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.ensemble import GradientBoostingRegressor\n\n>>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n>>> X_train, X_test = X[:200], X[200:]\n>>> y_train, y_test = y[:200], y[200:]\n>>> est = GradientBoostingRegressor(\n...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n...     loss='squared_error'\n... )\n>>> est = est.fit(X_train, y_train)  # fit with 100 trees\n>>> mean_squared_error(y_test, est.predict(X_test))\n>>> _ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and increase num of trees\n>>> _ = est.fit(X_train, y_train) # fit additional 100 trees to est\n>>> mean_squared_error(y_test, est.predict(X_test))\n```\n\n----------------------------------------\n\nTITLE: TF-IDF Transformation Example in Python\nDESCRIPTION: Demonstrates the usage of TfidfTransformer to convert count matrix to normalized TF-IDF representation. Shows the transformation of raw counts and retrieval of feature weights.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n>>> transformer = TfidfTransformer()\n>>> transformer.fit_transform(counts).toarray()\narray([[0.85151335, 0.        , 0.52433293],\n      [1.        , 0.        , 0.        ],\n      [1.        , 0.        , 0.        ],\n      [1.        , 0.        , 0.        ],\n      [0.55422893, 0.83236428, 0.        ],\n      [0.63035731, 0.        , 0.77630514]])\n\n>>> transformer.idf_\narray([1. ..., 2.25..., 1.84...])\n```\n\n----------------------------------------\n\nTITLE: Basic Outlier Detection Training in Scikit-learn\nDESCRIPTION: Shows the basic pattern for training an outlier detection estimator on training data using the fit method.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/outlier_detection.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nestimator.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Values with OrdinalEncoder in Python\nDESCRIPTION: Shows how OrdinalEncoder handles missing values indicated by np.nan. By default, it passes through missing values, but can be configured to encode them with a specific value using the encoded_missing_value parameter.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> enc = preprocessing.OrdinalEncoder()\n>>> X = [['male'], ['female'], [np.nan], ['female']]\n>>> enc.fit_transform(X)\narray([[ 1.],\n       [ 0.],\n       [nan],\n       [ 0.]])\n```\n\n----------------------------------------\n\nTITLE: Custom Estimator Tags example\nDESCRIPTION: This snippet demonstrates how to define a custom estimator with modified tags using `__sklearn_tags__()`. It overrides the default values for `single_output` and `non_deterministic` tags.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/develop.rst#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass MyMultiOutputEstimator(BaseEstimator):\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.target_tags.single_output = False\n        tags.non_deterministic = True\n        return tags\n```\n\n----------------------------------------\n\nTITLE: Accessing model intercept in SGDClassifier\nDESCRIPTION: This snippet shows how to access the intercept (bias) term of a trained SGDClassifier. The `intercept_` attribute holds the intercept value, which represents the offset of the decision boundary from the origin. This value is a key parameter in the linear model.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/sgd.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> clf.intercept_\narray([-9.9...])\n```\n\n----------------------------------------\n\nTITLE: Gaussian Noise Estimator with Random State Handling\nDESCRIPTION: This code snippet illustrates how to properly handle random number generation in a scikit-learn estimator. The estimator takes a `random_state` argument in its `__init__` method, stores it, and then uses `check_random_state` in the `fit` method to initialize the random number generator, ensuring reproducibility.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/develop.rst#2025-04-14_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n\"class GaussianNoise(BaseEstimator, TransformerMixin):\n        \\\"\\\"\\\"This estimator ignores its input and returns random Gaussian noise.\n\n        It also does not adhere to all scikit-learn conventions,\n        but showcases how to handle randomness.\n        \\\"\\\"\\\"\n\n        def __init__(self, n_components=100, random_state=None):\n            self.random_state = random_state\n            self.n_components = n_components\n\n        # the arguments are ignored anyway, so we make them optional\n        def fit(self, X=None, y=None):\n            self.random_state_ = check_random_state(self.random_state)\n\n        def transform(self, X):\n            n_samples = X.shape[0]\n            return self.random_state_.randn(n_samples, self.n_components)\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Gaussian Random Projection\nDESCRIPTION: Shows how to use GaussianRandomProjection transformer to reduce dimensionality by projecting input space onto a random matrix with normally distributed components.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/random_projection.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn import random_projection\n>>> X = np.random.rand(100, 10000)\n>>> transformer = random_projection.GaussianRandomProjection()\n>>> X_new = transformer.fit_transform(X)\n>>> X_new.shape\n(100, 3947)\n```\n\n----------------------------------------\n\nTITLE: RBF Kernel Approximation with SGD Classifier in Python\nDESCRIPTION: Example showing how to use RBFSampler to approximate RBF kernel features and combine with SGDClassifier for binary classification. Demonstrates the transformation of input data using random kitchen sinks method.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/kernel_approximation.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.kernel_approximation import RBFSampler\n>>> from sklearn.linear_model import SGDClassifier\n>>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n>>> y = [0, 0, 1, 1]\n>>> rbf_feature = RBFSampler(gamma=1, random_state=1)\n>>> X_features = rbf_feature.fit_transform(X)\n>>> clf = SGDClassifier(max_iter=5)\n>>> clf.fit(X_features, y)\nSGDClassifier(max_iter=5)\n>>> clf.score(X_features, y)\n1.0\n```\n\n----------------------------------------\n\nTITLE: Iris Dataset Decision Tree Classification\nDESCRIPTION: Example of training a decision tree classifier on the Iris dataset and plotting the resulting tree.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/tree.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import load_iris\n>>> from sklearn import tree\n>>> iris = load_iris()\n>>> X, y = iris.data, iris.target\n>>> clf = tree.DecisionTreeClassifier()\n>>> clf = clf.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Calculating Fowlkes-Mallows Index in Python using scikit-learn\nDESCRIPTION: This snippet demonstrates how to compute the Fowlkes-Mallows Index using scikit-learn's metrics module. It shows examples with different label assignments and their corresponding scores.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn import metrics\nlabels_true = [0, 0, 0, 1, 1, 1]\nlabels_pred = [0, 0, 1, 1, 2, 2]\n\nmetrics.fowlkes_mallows_score(labels_true, labels_pred)\n0.47140...\n\nlabels_pred = [1, 1, 0, 0, 3, 3]\n\nmetrics.fowlkes_mallows_score(labels_true, labels_pred)\n0.47140...\n\nlabels_pred = labels_true[:]\nmetrics.fowlkes_mallows_score(labels_true, labels_pred)\n1.0\n\nlabels_true = [0, 1, 2, 0, 3, 4, 5, 1]\nlabels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\nmetrics.fowlkes_mallows_score(labels_true, labels_pred)\n0.0\n```\n\n----------------------------------------\n\nTITLE: Implementing Reproducible Results with RandomState in Scikit-learn\nDESCRIPTION: This snippet demonstrates how to achieve reproducible results across multiple executions by using a consistent RandomState object. It shows the proper way to initialize and use random states for dataset creation, model initialization, and train-test splitting.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/common_pitfalls.rst#2025-04-14_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.model_selection import train_test_split\n>>> import numpy as np\n\n>>> rng = np.random.RandomState(0)\n>>> X, y = make_classification(random_state=rng)\n>>> rf = RandomForestClassifier(random_state=rng)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n...                                                     random_state=rng)\n>>> rf.fit(X_train, y_train).score(X_test, y_test)\n0.84\n```\n\n----------------------------------------\n\nTITLE: Using mean_pinball_loss in Python with scikit-learn\nDESCRIPTION: Demonstrates the usage of mean_pinball_loss function from scikit-learn to evaluate quantile regression models. It shows examples with different alpha values and how to create a custom scorer.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_52\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.metrics import mean_pinball_loss\ny_true = [1, 2, 3]\nmean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)\n0.03...\nmean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)\n0.3...\nmean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)\n0.3...\nmean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)\n0.03...\nmean_pinball_loss(y_true, y_true, alpha=0.1)\n0.0\nmean_pinball_loss(y_true, y_true, alpha=0.9)\n0.0\n```\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.metrics import make_scorer\nmean_pinball_loss_95p = make_scorer(mean_pinball_loss, alpha=0.95)\n```\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nX, y = make_regression(n_samples=100, random_state=0)\nestimator = GradientBoostingRegressor(\n    loss=\"quantile\",\n    alpha=0.95,\n    random_state=0,\n)\ncross_val_score(estimator, X, y, cv=5, scoring=mean_pinball_loss_95p)\narray([13.6..., 9.7..., 23.3..., 9.5..., 10.4...])\n```\n\n----------------------------------------\n\nTITLE: Using predict_joint_log_proba with Naive Bayes classifiers in Python\nDESCRIPTION: Shows how to use the new predict_joint_log_proba method added to all Naive Bayes classifiers to obtain joint log probabilities.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(X, y)\njoint_log_probs = gnb.predict_joint_log_proba(X_test)\n```\n\n----------------------------------------\n\nTITLE: Calculating ROC AUC Score for Multi-label Classification in Python using scikit-learn\nDESCRIPTION: This snippet demonstrates how to compute the ROC AUC score for a multi-label classification problem using scikit-learn. It uses a synthetic dataset and multi-output classifier.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\n>>> from sklearn.datasets import make_multilabel_classification\n>>> from sklearn.multioutput import MultiOutputClassifier\n>>> X, y = make_multilabel_classification(random_state=0)\n>>> inner_clf = LogisticRegression(solver=\"liblinear\", random_state=0)\n>>> clf = MultiOutputClassifier(inner_clf).fit(X, y)\n>>> y_score = np.transpose([y_pred[:, 1] for y_pred in clf.predict_proba(X)])\n>>> roc_auc_score(y, y_score, average=None)\narray([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])\n\n>>> from sklearn.linear_model import RidgeClassifierCV\n>>> clf = RidgeClassifierCV().fit(X, y)\n>>> y_score = clf.decision_function(X)\n>>> roc_auc_score(y, y_score, average=None)\narray([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\n```\n\n----------------------------------------\n\nTITLE: Using Mean Absolute Percentage Error in Python with scikit-learn\nDESCRIPTION: This example shows how to use the mean_absolute_percentage_error function to evaluate regression models based on relative errors rather than absolute errors, making it useful for comparing errors across different scales.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics import mean_absolute_percentage_error\ny_true = [1, 10, 1e6]\ny_pred = [0.9, 15, 1.2e6]\nmean_absolute_percentage_error(y_true, y_pred)\n0.2666...\n```\n\n----------------------------------------\n\nTITLE: Converting Sparse Matrix to Array\nDESCRIPTION: This snippet shows how to convert the sparse matrix `X` (obtained from `fit_transform`) to a dense array using `toarray()`.  The dense array represents the term frequencies for each document in the corpus.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> X.toarray()\narray([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n       [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)\n```\n\n----------------------------------------\n\nTITLE: Initializing LassoLars Model in Python\nDESCRIPTION: Example showing how to create and fit a LassoLars model from scikit-learn with an alpha regularization parameter of 0.1. The model is fit on a simple 2D dataset.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn import linear_model\n>>> reg = linear_model.LassoLars(alpha=.1)\n>>> reg.fit([[0, 0], [1, 1]], [0, 1])\nLassoLars(alpha=0.1)\n>>> reg.coef_\narray([0.6..., 0.        ])\n```\n\n----------------------------------------\n\nTITLE: Configuring HalvingGridSearchCV for RandomForestClassifier\nDESCRIPTION: This snippet showcases how to set up HalvingGridSearchCV for a RandomForestClassifier. Key parameters include max_depth and min_samples_split, along with resource settings like max_resources.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/grid_search.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.experimental import enable_halving_search_cv  # noqa\n>>> from sklearn.model_selection import HalvingGridSearchCV\n>>> import pandas as pd\n>>> param_grid = {'max_depth': [3, 5, 10],\n...               'min_samples_split': [2, 5, 10]}\n>>> base_estimator = RandomForestClassifier(random_state=0)\n>>> X, y = make_classification(n_samples=1000, random_state=0)\n>>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\n...                          factor=2, resource='n_estimators',\n...                          max_resources=30).fit(X, y)\n>>> sh.best_estimator_\nRandomForestClassifier(max_depth=5, n_estimators=24, random_state=0)\n```\n\n----------------------------------------\n\nTITLE: Generating Gaussian Quantiles Dataset in Python\nDESCRIPTION: Uses make_gaussian_quantiles function to create a dataset with three classes separated by concentric hyperspheres. The resulting 2D dataset is visualized using matplotlib.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/datasets/sample_generators.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_gaussian_quantiles\n\nX, Y = make_gaussian_quantiles(n_features=2, n_classes=3, random_state=0)\nplt.scatter(X[:, 0], X[:, 1], c=Y)\nplt.title(\"Gaussian divided into three quantiles\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Processing Multiple Categories per Feature with DictVectorizer in Python\nDESCRIPTION: This example shows how DictVectorizer handles multiple string values for one feature, such as multiple categories for a movie. It demonstrates how lists of values are processed for a single feature key.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmovie_entry = [{'category': ['thriller', 'drama'], 'year': 2003},\n               {'category': ['animation', 'family'], 'year': 2011},\n               {'year': 1974}]\nvec.fit_transform(movie_entry).toarray()\narray([[0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 2.003e+03],\n       [1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 2.011e+03],\n       [0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.974e+03]])\nvec.get_feature_names_out()\narray(['category=animation', 'category=drama', 'category=family',\n       'category=thriller', 'year'], ...)\nvec.transform({'category': ['thriller'],\n               'unseen_feature': '3'}).toarray()\narray([[0., 0., 0., 1., 0.]])\n```\n\n----------------------------------------\n\nTITLE: Computing Multi-label Confusion Matrix for Multilabel Classification in Python\nDESCRIPTION: This snippet demonstrates how to use the multilabel_confusion_matrix function to compute a class-wise multilabel confusion matrix. It shows usage with a multilabel indicator matrix input.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn.metrics import multilabel_confusion_matrix\n>>> y_true = np.array([[1, 0, 1],\n...                    [0, 1, 0]])\n>>> y_pred = np.array([[1, 0, 0],\n...                    [0, 1, 1]])\n>>> multilabel_confusion_matrix(y_true, y_pred)\narray([[[1, 0],\n        [0, 1]],\n\n       [[1, 0],\n        [0, 1]],\n\n       [[0, 1],\n        [1, 0]]])\n```\n\n----------------------------------------\n\nTITLE: Extracting Patches from RGB Image using scikit-learn\nDESCRIPTION: This snippet demonstrates how to use extract_patches_2d function to extract 2x2 patches from a 4x4x3 RGB image. It shows both random and full patch extraction.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom sklearn.feature_extraction import image\n\none_image = np.arange(4 * 4 * 3).reshape((4, 4, 3))\none_image[:, :, 0]  # R channel of a fake RGB picture\n\npatches = image.extract_patches_2d(one_image, (2, 2), max_patches=2,\n    random_state=0)\npatches.shape\npatches[:, :, :, 0]\npatches = image.extract_patches_2d(one_image, (2, 2))\npatches.shape\npatches[4, :, :, 0]\n```\n\n----------------------------------------\n\nTITLE: Spectral Clustering with Graph Adjacency Matrix in Python\nDESCRIPTION: Example of using SpectralClustering for graph partitioning by passing an adjacency matrix with precomputed affinity. The clustering is configured to find 3 clusters using the discretize label assignment strategy.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.cluster import SpectralClustering\nsc = SpectralClustering(3, affinity='precomputed', n_init=100,\n                        assign_labels='discretize')\nsc.fit_predict(adjacency_matrix)\n```\n\n----------------------------------------\n\nTITLE: Adding Gamma deviance loss to HistGradientBoostingRegressor\nDESCRIPTION: The ensemble.HistGradientBoostingRegressor now supports the Gamma deviance loss via loss=\"gamma\". This is useful for modeling skewed distributed, strictly positive valued targets.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.3.rst#2025-04-14_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nensemble.HistGradientBoostingRegressor(loss=\"gamma\")\n```\n\n----------------------------------------\n\nTITLE: Introducing Generalized Linear Models in scikit-learn\nDESCRIPTION: Adds new classes for generalized linear models with non-normal error distributions: PoissonRegressor, GammaRegressor, and TweedieRegressor.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nlinear_model.PoissonRegressor()\nlinear_model.GammaRegressor()\nlinear_model.TweedieRegressor()\n```\n\n----------------------------------------\n\nTITLE: Using sample_weight in StandardScaler in Python\nDESCRIPTION: Added sample_weight parameter to StandardScaler to allow setting individual weights for each sample during fitting.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X, sample_weight=weights)\n```\n\n----------------------------------------\n\nTITLE: Executing Parallel Function Calls in Python using Joblib\nDESCRIPTION: This code snippet demonstrates how to use Joblib to execute a function in parallel across multiple parameters. It utilizes the Parallel and delayed functions from Joblib to distribute the workload.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nout = Parallel(...)(delayed(some_function)(param) for param in some_iterable)\n```\n\n----------------------------------------\n\nTITLE: Using KDTree for Nearest Neighbor Search\nDESCRIPTION: Demonstrates direct usage of KDTree class for finding nearest neighbors with custom leaf size and Euclidean metric.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neighbors.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.neighbors import KDTree\nimport numpy as np\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\nkdt = KDTree(X, leaf_size=30, metric='euclidean')\nkdt.query(X, k=2, return_distance=False)\n```\n\n----------------------------------------\n\nTITLE: Adding fit_params Support in MultiOutputRegressor and MultiOutputClassifier in scikit-learn\nDESCRIPTION: Enables passing fit_params to the estimator.fit method of each step in MultiOutputRegressor and MultiOutputClassifier.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\nmultioutput.MultiOutputRegressor.fit()\nmultioutput.MultiOutputClassifier.fit()\n```\n\n----------------------------------------\n\nTITLE: RandomForestClassifier with different random_state types: Integer vs RandomState\nDESCRIPTION: This snippet demonstrates the difference in cross-validation procedures when using an integer versus a `RandomState` instance for the `random_state` parameter of a `RandomForestClassifier`. Using an integer ensures the same RNG across all folds, while a `RandomState` instance results in a different RNG for each fold.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/common_pitfalls.rst#2025-04-14_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.model_selection import cross_val_score\n>>> import numpy as np\n\n>>> X, y = make_classification(random_state=0)\n\n>>> rf_123 = RandomForestClassifier(random_state=123)\n>>> cross_val_score(rf_123, X, y)\narray([0.85, 0.95, 0.95, 0.9 , 0.9 ])\n\n>>> rf_inst = RandomForestClassifier(random_state=np.random.RandomState(0))\n>>> cross_val_score(rf_inst, X, y)\narray([0.9 , 0.95, 0.95, 0.9 , 0.9 ])\n```\n\n----------------------------------------\n\nTITLE: Combining min_frequency and max_categories in OneHotEncoder in Python\nDESCRIPTION: This example shows how to use both min_frequency and max_categories parameters in OneHotEncoder. It sets min_frequency=4 and max_categories=3, resulting in 'dog' and 'snake' being considered infrequent.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n>>> enc = preprocessing.OneHotEncoder(min_frequency=4, max_categories=3, sparse_output=False)\n>>> enc = enc.fit(X)\n>>> enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])\narray([[0., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n```\n\n----------------------------------------\n\nTITLE: Calculating Recall for Multilabel Classification using Confusion Matrix in Python\nDESCRIPTION: This example demonstrates how to calculate recall (sensitivity) for each class in a multilabel classification problem using the multilabel_confusion_matrix function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n>>> y_true = np.array([[0, 0, 1],\n...                    [0, 1, 0],\n...                    [1, 1, 0]])\n>>> y_pred = np.array([[0, 1, 0],\n...                    [0, 0, 1],\n...                    [1, 1, 0]])\n>>> mcm = multilabel_confusion_matrix(y_true, y_pred)\n>>> tn = mcm[:, 0, 0]\n>>> tp = mcm[:, 1, 1]\n>>> fn = mcm[:, 1, 0]\n>>> fp = mcm[:, 0, 1]\n>>> tp / (tp + fn)\narray([1. , 0.5, 0. ])\n```\n\n----------------------------------------\n\nTITLE: Linear Regression with scikit-learn\nDESCRIPTION: This code snippet demonstrates how to fit a linear regression model using scikit-learn's LinearRegression class. It shows how to instantiate the model, fit it to training data, and access the learned coefficients and intercept.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn import linear_model\n>>> reg = linear_model.LinearRegression()\n>>> reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\nLinearRegression()\n>>> reg.coef_\narray([0.5, 0.5])\n>>> reg.intercept_\n0.0\n```\n\n----------------------------------------\n\nTITLE: Using Hamming Loss in scikit-learn for Binary and Multilabel Classification\nDESCRIPTION: Demonstrates how to calculate Hamming loss for both single-label classification and multilabel classification with binary indicators using scikit-learn's hamming_loss function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics import hamming_loss\ny_pred = [1, 2, 3, 4]\ny_true = [2, 2, 3, 4]\nhamming_loss(y_true, y_pred)\n0.25\n```\n\nLANGUAGE: python\nCODE:\n```\nhamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n0.75\n```\n\n----------------------------------------\n\nTITLE: Computing Raw Partial Dependence Values\nDESCRIPTION: Shows how to obtain raw partial dependence values using the partial_dependence function instead of plots.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/partial_dependence.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.inspection import partial_dependence\n\nresults = partial_dependence(clf, X, [0])\nresults[\"average\"]\nresults[\"grid_values\"]\n```\n\n----------------------------------------\n\nTITLE: Calculating Adjusted Rand Index with scikit-learn in Python\nDESCRIPTION: This snippet demonstrates how to calculate the Adjusted Rand Index using `sklearn.metrics.adjusted_rand_score`. It corrects the Rand Index for chance, providing a baseline value for random labelings. The Adjusted Rand Index measures the similarity between two clusterings, accounting for the expected similarity due to chance.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\n0.24...\n```\n\n----------------------------------------\n\nTITLE: Using explained_variance_score in Python with scikit-learn\nDESCRIPTION: Demonstrates how to use the explained_variance_score function from scikit-learn to evaluate regression models. It shows examples with different input types and parameters.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_50\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.metrics import explained_variance_score\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nexplained_variance_score(y_true, y_pred)\n0.957...\ny_true = [[0.5, 1], [-1, 1], [7, -6]]\ny_pred = [[0, 2], [-1, 2], [8, -5]]\nexplained_variance_score(y_true, y_pred, multioutput='raw_values')\narray([0.967..., 1.        ])\nexplained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])\n0.990...\ny_true = [-2, -2, -2]\ny_pred = [-2, -2, -2]\nexplained_variance_score(y_true, y_pred)\n1.0\nexplained_variance_score(y_true, y_pred, force_finite=False)\nnan\ny_true = [-2, -2, -2]\ny_pred = [-2, -2, -2 + 1e-8]\nexplained_variance_score(y_true, y_pred)\n0.0\nexplained_variance_score(y_true, y_pred, force_finite=False)\n-inf\n```\n\n----------------------------------------\n\nTITLE: Adding Trees to Isolation Forest with Warm Start\nDESCRIPTION: Example showing how to use warm_start parameter in IsolationForest to incrementally add more trees to an already fitted model. The code demonstrates initialization, first fit with 10 trees, and adding 10 more trees.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/outlier_detection.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.ensemble import IsolationForest\nimport numpy as np\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]])\nclf = IsolationForest(n_estimators=10, warm_start=True)\nclf.fit(X)  # fit 10 trees\nclf.set_params(n_estimators=20)  # add 10 more trees\nclf.fit(X)  # fit the added trees\n```\n\n----------------------------------------\n\nTITLE: Creating Calibration Curve Display in Scikit-Learn\nDESCRIPTION: This code demonstrates how to create a calibration curve display using the CalibrationDisplay class from scikit-learn. It requires a fitted classifier with a predict_proba method.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/calibration.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.calibration import CalibrationDisplay\n\nCalibrationDisplay.from_estimator(fitted_classifier, X, y)\n```\n\n----------------------------------------\n\nTITLE: Accessing Estimator Parameters in Python\nDESCRIPTION: Shows how to get the names and current values of all parameters for a given scikit-learn estimator using the get_params() method.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/grid_search.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nestimator.get_params()\n```\n\n----------------------------------------\n\nTITLE: Implementing Sparse Random Projection\nDESCRIPTION: Demonstrates the use of SparseRandomProjection transformer for dimensionality reduction using memory-efficient sparse random matrices.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/random_projection.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn import random_projection\n>>> X = np.random.rand(100, 10000)\n>>> transformer = random_projection.SparseRandomProjection()\n>>> X_new = transformer.fit_transform(X)\n>>> X_new.shape\n(100, 3947)\n```\n\n----------------------------------------\n\nTITLE: Preserving Random State with warm_start in scikit-learn\nDESCRIPTION: Shows how the random_state parameter is preserved between fit calls when using warm_start=True. This ensures that building a model with n estimators at once is the same as building it iteratively with multiple fit calls.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> clf = RandomForestClassifier(n_estimators=20)  # set `n_estimators` to 10 + 10\n>>> _ = clf.fit(X, y)  # fit `estimators_` will be the same as `clf` above\n```\n\n----------------------------------------\n\nTITLE: Adding Sample Weight Support for ElasticNet and Lasso in scikit-learn\nDESCRIPTION: Adds support for sample_weight parameter in ElasticNet and Lasso for dense feature matrix X.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nlinear_model.ElasticNet()\nlinear_model.Lasso()\n```\n\n----------------------------------------\n\nTITLE: Adding Early Stopping to SGD Models (Python)\nDESCRIPTION: Adds early_stopping, validation_fraction, and n_iter_no_change parameters to SGD-based models to enable early stopping based on validation set performance. Also adds an 'adaptive' learning rate strategy.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nearly_stopping, validation_fraction, n_iter_no_change, \"adaptive\"\n```\n\n----------------------------------------\n\nTITLE: Using auto mode in SequentialFeatureSelector\nDESCRIPTION: Adds an auto mode to SequentialFeatureSelector where features are selected until the score improvement does not exceed a tolerance value. The default will change to 'auto' in version 1.3.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.1.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.feature_selection import SequentialFeatureSelector\n\nsfs = SequentialFeatureSelector(n_features_to_select='auto', tol=1e-3)\n```\n\n----------------------------------------\n\nTITLE: Transforming New Text\nDESCRIPTION: This snippet demonstrates how to use the `transform` method to transform new text into a feature vector based on the vocabulary learned during the fitting process. Terms not present in the training corpus will be ignored.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> vectorizer.transform(['Something completely new.']).toarray()\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)\n```\n\n----------------------------------------\n\nTITLE: Generating Polynomial Features with PolynomialFeatures in Python\nDESCRIPTION: This example demonstrates the use of PolynomialFeatures to generate higher-order and interaction terms from input features. It shows transformations with and without interaction-only terms.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn.preprocessing import PolynomialFeatures\n>>> X = np.arange(6).reshape(3, 2)\n>>> X\narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n>>> poly = PolynomialFeatures(2)\n>>> poly.fit_transform(X)\narray([[ 1.,  0.,  1.,  0.,  0.,  1.],\n       [ 1.,  2.,  3.,  4.,  6.,  9.],\n       [ 1.,  4.,  5., 16., 20., 25.]])\n\n>>> X = np.arange(9).reshape(3, 3)\n>>> X\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n>>> poly = PolynomialFeatures(degree=3, interaction_only=True)\n>>> poly.fit_transform(X)\narray([[  1.,   0.,   1.,   2.,   0.,   0.,   2.,   0.],\n       [  1.,   3.,   4.,   5.,  12.,  15.,  20.,  60.],\n       [  1.,   6.,   7.,   8.,  42.,  48.,  56., 336.]])\n```\n\n----------------------------------------\n\nTITLE: Implementing Latent Dirichlet Allocation in scikit-learn\nDESCRIPTION: This snippet details the LatentDirichletAllocation class, which uses an online variational Bayes algorithm for topic modeling. It supports online and batch updates, with methods for decomposing document-term matrices into topic-term and document-topic matrices.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/decomposition.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass LatentDirichletAllocation:\n    # Implements LDA using online variational Bayes\n    def __init__(self, n_components=10, learning_method='online'):\n        self.n_components = n_components\n        self.learning_method = learning_method\n        # Additional initialization code...\n```\n\n----------------------------------------\n\nTITLE: Computing Multi-label Confusion Matrix for Multiclass Classification in Python\nDESCRIPTION: This snippet demonstrates how to use the multilabel_confusion_matrix function for multiclass classification. It shows usage with string labels and specified label order.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n>>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n>>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n>>> multilabel_confusion_matrix(y_true, y_pred,\n...                             labels=[\"ant\", \"bird\", \"cat\"])\narray([[[3, 1],\n        [0, 2]],\n\n       [[5, 0],\n        [1, 0]],\n\n       [[2, 1],\n        [1, 2]]])\n```\n\n----------------------------------------\n\nTITLE: Symmetry of Adjusted Mutual Information in scikit-learn in Python\nDESCRIPTION: This snippet demonstrates that the Adjusted Mutual Information (AMI) score is symmetric, meaning that swapping the `labels_true` and `labels_pred` arguments does not change the calculated score. This property makes it suitable for use as a consensus measure.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)  # doctest: +SKIP\n0.22504...\n```\n\n----------------------------------------\n\nTITLE: Adding partial_fit support to MultiOutputRegressor and MultiOutputClassifier in Python\nDESCRIPTION: MultiOutputRegressor and MultiOutputClassifier now support online learning using partial_fit method. This allows incremental fitting on batches of data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nmultioutput.MultiOutputRegressor\nmultioutput.MultiOutputClassifier\n```\n\n----------------------------------------\n\nTITLE: Changing Default Parameter Value in Python with FutureWarning\nDESCRIPTION: This example shows how to change the default value of a parameter by using a specific value ('warn') and raising a FutureWarning when users are using the default value.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_44\n\nLANGUAGE: Python\nCODE:\n```\nimport warnings\n\ndef example_function(n_clusters=\"warn\"):\n    if n_clusters == \"warn\":\n        warnings.warn(\n            \"The default value of `n_clusters` will change from 5 to 10 in 0.22.\",\n            FutureWarning,\n        )\n        n_clusters = 5\n\nclass ExampleEstimator:\n    def __init__(self, n_clusters=\"warn\"):\n        self.n_clusters = n_clusters\n\n    def fit(self, X, y):\n        if self.n_clusters == \"warn\":\n            warnings.warn(\n                \"The default value of `n_clusters` will change from 5 to 10 in 0.22.\",\n                FutureWarning,\n            )\n            self._n_clusters = 5\n```\n\n----------------------------------------\n\nTITLE: Importing TfidfTransformer from scikit-learn\nDESCRIPTION: This snippet imports the `TfidfTransformer` class from the `sklearn.feature_extraction.text` module. TfidfTransformer is used to transform a count matrix to a normalized tf or tf-idf representation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.feature_extraction.text import TfidfTransformer\n```\n\n----------------------------------------\n\nTITLE: Fixing shuffle option in MLPClassifier and MLPRegressor in Python\nDESCRIPTION: The neural_network.MLPClassifier and neural_network.MLPRegressor classes now correctly handle the shuffle=False option.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\nneural_network.MLPClassifier\n```\n\nLANGUAGE: Python\nCODE:\n```\nneural_network.MLPRegressor\n```\n\n----------------------------------------\n\nTITLE: Classifier fit method example\nDESCRIPTION: This snippet demonstrates how to handle class labels in the `fit` method of a classifier. It uses `np.unique` to ensure that class labels are properly stored and indexed, even if they are not a contiguous range of integers.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/develop.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nself.classes_, y = np.unique(y, return_inverse=True)\n```\n\n----------------------------------------\n\nTITLE: Transforming DataFrame with ColumnTransformer and Column Names - Python\nDESCRIPTION: This snippet shows how to use a ColumnTransformer fitted to a dataframe to transform a new dataframe while using column names for selection. It illustrates how ColumnTransformer selects columns dynamically based on their names.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nct = ColumnTransformer(\\n          [(\"scale\", StandardScaler(), [\"expert_rating\"])])\\.fit(X)\\nX_new = pd.DataFrame({\"expert_rating\": [5, 6, 1],\\n                       \"ignored_new_col\": [1.2, 0.3, -0.1]})\\nct.transform(X_new)\\narray([[ 0.9...],\\n         [ 2.1...],\\n         [-3.9...]])\n```\n\n----------------------------------------\n\nTITLE: Lasso Regression Example\nDESCRIPTION: This example demonstrates how to use the Lasso regression model from scikit-learn. It initializes a Lasso model with a specified alpha value, fits it to sample data, and then predicts the output for a new input.  The alpha parameter controls the strength of L1 regularization.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn import linear_model\n>>> reg = linear_model.Lasso(alpha=0.1)\n>>> reg.fit([[0, 0], [1, 1]], [0, 1])\nLasso(alpha=0.1)\n>>> reg.predict([[1, 1]])\narray([0.8])\n```\n\n----------------------------------------\n\nTITLE: Generating Binary Classification Data with NumPy\nDESCRIPTION: Creates synthetic data for binary classification with numeric features and integer targets (0 or 1).\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/minimal_reproducer.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nrng = np.random.RandomState(0)\nn_samples, n_features = 5, 5\nX = rng.randn(n_samples, n_features)\ny = rng.randint(0, 2, n_samples)  # binary target with values in {0, 1}\n```\n\n----------------------------------------\n\nTITLE: Basic Random Forest Classification Example\nDESCRIPTION: Demonstrates how to initialize and fit a RandomForestClassifier with a simple 2D dataset. Shows basic usage with a binary classification problem.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> X = [[0, 0], [1, 1]]\n>>> Y = [0, 1]\n>>> clf = RandomForestClassifier(n_estimators=10)\n>>> clf = clf.fit(X, Y)\n```\n\n----------------------------------------\n\nTITLE: Custom Loss Function Implementation\nDESCRIPTION: Demonstrates implementing a custom loss function that calculates the log of the maximum absolute difference between true and predicted values.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\ndef my_custom_loss_func(y_true, y_pred):\n    diff = np.abs(y_true - y_pred).max()\n    return float(np.log1p(diff))\n```\n\n----------------------------------------\n\nTITLE: Computing Pair Confusion Matrix with Merged Clusters\nDESCRIPTION: Example demonstrating pair_confusion_matrix for a case where some distinct clusters are merged in the predicted labels.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics.cluster import pair_confusion_matrix\npair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\n```\n\n----------------------------------------\n\nTITLE: Accessing Trained Model Parameters in MLPClassifier\nDESCRIPTION: This code snippet shows how to access the trained model parameters (weight matrices) of an MLPClassifier. It demonstrates the shape of the coefficient matrices for each layer in the neural network.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neural_networks_supervised.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> [coef.shape for coef in clf.coefs_]\n[(2, 5), (5, 2), (2, 1)]\n```\n\n----------------------------------------\n\nTITLE: Creating a Bagging Ensemble with BaggingClassifier in scikit-learn\nDESCRIPTION: Demonstrates how to instantiate a bagging ensemble using the BaggingClassifier meta-estimator with KNeighborsClassifier as the base estimator. The example configures random subsets of 50% of the samples and 50% of the features.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.ensemble import BaggingClassifier\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> bagging = BaggingClassifier(KNeighborsClassifier(),\n...                             max_samples=0.5, max_features=0.5)\n```\n\n----------------------------------------\n\nTITLE: Adding Partial Dependence Support for Tree-based Models in scikit-learn\nDESCRIPTION: Adds support for the fast 'recursion' method in partial_dependence and plot_partial_dependence for RandomForestRegressor and DecisionTreeRegressor.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ninspection.partial_dependence()\ninspection.plot_partial_dependence()\n```\n\n----------------------------------------\n\nTITLE: Using CuPy with LinearDiscriminantAnalysis on GPU\nDESCRIPTION: Demonstrates how to use CuPy to run LinearDiscriminantAnalysis on a GPU using Array API dispatch.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/array_api.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import make_classification\n>>> from sklearn import config_context\n>>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n>>> import cupy\n\n>>> X_np, y_np = make_classification(random_state=0)\n>>> X_cu = cupy.asarray(X_np)\n>>> y_cu = cupy.asarray(y_np)\n>>> X_cu.device\n<CUDA Device 0>\n\n>>> with config_context(array_api_dispatch=True):\n...     lda = LinearDiscriminantAnalysis()\n...     X_trans = lda.fit_transform(X_cu, y_cu)\n>>> X_trans.device\n<CUDA Device 0>\n```\n\n----------------------------------------\n\nTITLE: Computing Empirical Covariance in Python with scikit-learn\nDESCRIPTION: This snippet demonstrates how to compute the empirical covariance matrix of a sample using scikit-learn's empirical_covariance function or by fitting an EmpiricalCovariance object to the data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/covariance.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.covariance import empirical_covariance, EmpiricalCovariance\n\n# Using the function\ncov_matrix = empirical_covariance(X, assume_centered=False)\n\n# Using the object\ncov = EmpiricalCovariance(assume_centered=False)\ncov.fit(X)\n```\n\n----------------------------------------\n\nTITLE: Calculating Mutual Information in Python\nDESCRIPTION: This snippet shows the function calls for mutual_info_classif and mutual_info_regression from the feature_selection module. It mentions an efficiency improvement using KDTree for nearest neighbor counting.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.feature_selection import mutual_info_classif, mutual_info_regression\nfrom sklearn.neighbors import KDTree\n\n# Usage not shown, but these functions now use KDTree internally for better memory efficiency\n```\n\n----------------------------------------\n\nTITLE: Using Mean Squared Logarithmic Error in Python with scikit-learn\nDESCRIPTION: This example demonstrates the mean_squared_log_error function for evaluating regression models, particularly useful for targets with exponential growth. The function penalizes under-predictions more than over-predictions.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics import mean_squared_log_error\ny_true = [3, 5, 2.5, 7]\ny_pred = [2.5, 5, 4, 8]\nmean_squared_log_error(y_true, y_pred)\n0.039...\ny_true = [[0.5, 1], [1, 2], [7, 6]]\ny_pred = [[0.5, 2], [1, 2.5], [8, 8]]\nmean_squared_log_error(y_true, y_pred)\n0.044...\n```\n\n----------------------------------------\n\nTITLE: Documenting Parameters and Attributes in Docstrings\nDESCRIPTION: This snippet provides examples of well-formatted docstrings for documenting parameters and attributes, including type hints, shape descriptions, and default values. It covers various data types like integers, booleans, strings, arrays, and lists.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n\"n_clusters : int, default=3\n    The number of clusters detected by the algorithm.\n\nsome_param : {\\\"hello\\\", \\\"goodbye\\\"}, bool or int, default=True\n    The parameter description goes here, which can be either a string\n    literal (either `hello` or `goodbye`), a bool, or an int. The default\n    value is True.\n\narray_parameter : {array-like, sparse matrix} of shape (n_samples, n_features) \\\n    or (n_samples,)\n    This parameter accepts data in either of the mentioned forms, with one\n    of the mentioned shapes. The default value is `np.ones(shape=(n_samples,))`.\n\nlist_param : list of int\n\ntyped_ndarray : ndarray of shape (n_samples,), dtype=np.int32\n\nsample_weight : array-like of shape (n_samples,), default=None\n\nmultioutput_array : ndarray of shape (n_samples, n_classes) or list of such arrays\"\n```\n\n----------------------------------------\n\nTITLE: Enhancing Numerical Stability in Binomial Log Loss Gradient Computation\nDESCRIPTION: The gradient computation of the binomial log loss has been improved for numerical stability with very large input values. This affects models like GradientBoostingClassifier, HistGradientBoostingClassifier, and LogisticRegression.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.4.rst#2025-04-14_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\nclf = GradientBoostingClassifier(random_state=42)\nclf.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Using nan_euclidean_distances in scikit-learn metrics\nDESCRIPTION: Added a new metric to calculate Euclidean distances in the presence of missing values. This function is part of the pairwise metrics in scikit-learn.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.pairwise.nan_euclidean_distances\n```\n\n----------------------------------------\n\nTITLE: Deprecating a Function in Python using scikit-learn's utils\nDESCRIPTION: This snippet demonstrates how to deprecate a function using the @deprecated decorator from scikit-learn's utils. It renames the 'zero_one' function to 'zero_one_loss' and changes the default behavior.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_41\n\nLANGUAGE: Python\nCODE:\n```\nfrom ..utils import deprecated\n\ndef zero_one_loss(y_true, y_pred, normalize=True):\n    # actual implementation\n    pass\n\n@deprecated(\n    \"Function `zero_one` was renamed to `zero_one_loss` in 0.13 and will be \"\n    \"removed in 0.15. Default behavior is changed from `normalize=False` to \"\n    \"`normalize=True`\"\n)\ndef zero_one(y_true, y_pred, normalize=False):\n    return zero_one_loss(y_true, y_pred, normalize)\n```\n\n----------------------------------------\n\nTITLE: Quantile Transformation: Independent Testing Set Verification\nDESCRIPTION: This snippet extends the verification of quantile transformation to an independent testing set. It calculates the percentiles of the original and transformed testing data to ensure consistent behavior of the transformation across different datasets.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> np.percentile(X_test[:, 0], [0, 25, 50, 75, 100])\n... # doctest: +SKIP\narray([ 4.4  ,  5.125,  5.75 ,  6.175,  7.3  ])\n>>> np.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100])\n... # doctest: +SKIP\narray([ 0.01...,  0.25...,  0.46...,  0.60... ,  0.94...])\n```\n\n----------------------------------------\n\nTITLE: Configuring OrdinalEncoder to handle unknown categories in Python\nDESCRIPTION: New handle_unknown and unknown_value parameters added to OrdinalEncoder to allow handling of unknown categories during transform.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.preprocessing import OrdinalEncoder\n\nenc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n```\n\n----------------------------------------\n\nTITLE: MLPRegressor Parameter Update - Loss Function Options\nDESCRIPTION: Code update introducing the 'loss' parameter to MLPRegressor with support for 'squared_error' (default) and 'poisson' loss functions. This enhancement provides more flexibility in neural network regression tasks.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/sklearn.neural_network/30712.feature.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nneural_network.MLPRegressor(loss=\"squared_error\")  # default\nneural_network.MLPRegressor(loss=\"poisson\")      # new option\n```\n\n----------------------------------------\n\nTITLE: Accessing feature by name with n-grams\nDESCRIPTION: This snippet retrieves the index of the feature \"is this\" from the bigram vectorizer's vocabulary and then accesses the corresponding column in the transformed data `X_2`. This allows focusing on the presence or absence of that specific bigram in each document.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n>>> feature_index = bigram_vectorizer.vocabulary_.get('is this')\n>>> X_2[:, feature_index]\narray([0, 0, 0, 1]...)\n```\n\n----------------------------------------\n\nTITLE: Computing Permutation Importance with Single Scorer\nDESCRIPTION: Demonstrates how to calculate permutation importance using a single scoring metric on a trained model with the diabetes dataset.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/permutation_importance.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.inspection import permutation_importance\nr = permutation_importance(model, X_val, y_val,\n                           n_repeats=30,\n                           random_state=0)\nfor i in r.importances_mean.argsort()[::-1]:\n    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n        print(f\"{diabetes.feature_names[i]:<8}\"\n              f\"{r.importances_mean[i]:.3f}\"\n              f\" +/- {r.importances_std[i]:.3f}\")\n```\n\n----------------------------------------\n\nTITLE: Specifying Explicit Categories with OneHotEncoder in Python\nDESCRIPTION: Demonstrates how to specify explicit categories for OneHotEncoder using the categories parameter, allowing for transformation of data with missing categories.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n>>> genders = ['female', 'male']\n>>> locations = ['from Africa', 'from Asia', 'from Europe', 'from US']\n>>> browsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']\n>>> enc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])\n>>> # Note that for there are missing categorical values for the 2nd and 3rd\n>>> # feature\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n>>> enc.fit(X)\nOneHotEncoder(categories=[['female', 'male'],\n                          ['from Africa', 'from Asia', 'from Europe',\n                           'from US'],\n                          ['uses Chrome', 'uses Firefox', 'uses IE',\n                           'uses Safari']])\n>>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\narray([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])\n```\n\n----------------------------------------\n\nTITLE: Retaining Additional Columns in ColumnTransformer - Python\nDESCRIPTION: This snippet shows how to implement a ColumnTransformer that retains columns not explicitly transformed by specifying remainder='passthrough'. This allows additional columns to be included in the transformed output.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ncolumn_trans = ColumnTransformer(\\n     [('city_category', OneHotEncoder(dtype='int'),['city']),\\n      ('title_bow', CountVectorizer(), 'title')],\\n     remainder='passthrough')\\ncolumn_trans.fit_transform(X)\\narray([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 4],\\n         [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3, 5],\\n         [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 4],\\n         [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 5, 3]]...)\n```\n\n----------------------------------------\n\nTITLE: Configuring TimeSeriesSplit with test_size and gap in Python\nDESCRIPTION: New parameters test_size and gap added to TimeSeriesSplit for controlling out-of-sample length and gap between train and test sets.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.model_selection import TimeSeriesSplit\n\ntss = TimeSeriesSplit(test_size=10, gap=2)\n```\n\n----------------------------------------\n\nTITLE: Adding Constant Imputation Strategy (Python)\nDESCRIPTION: Adds a new 'constant' strategy to SimpleImputer for imputing missing values with a fixed value specified by fill_value. This strategy supports both numeric and non-numeric data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n'constant', fill_value\n```\n\n----------------------------------------\n\nTITLE: Specifying Categorical Features for Gradient Boosting in Python\nDESCRIPTION: Shows how to specify categorical features for HistGradientBoostingClassifier and HistGradientBoostingRegressor using feature names in the categorical_features parameter.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nensemble.HistGradientBoostingClassifier(categorical_features=[\"feature1\", \"feature2\"])\n```\n\nLANGUAGE: Python\nCODE:\n```\nensemble.HistGradientBoostingRegressor(categorical_features=[\"feature1\", \"feature2\"])\n```\n\n----------------------------------------\n\nTITLE: Computing Pair Confusion Matrix with Matching Labels\nDESCRIPTION: Example demonstrating pair_confusion_matrix computation with perfectly matching cluster labels, showing all non-zero entries on the diagonal.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics.cluster import pair_confusion_matrix\npair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 1])\n```\n\n----------------------------------------\n\nTITLE: KFold with RandomState instance: Demonstrating non-reproducible splits\nDESCRIPTION: This snippet shows that calling `split` multiple times on a `KFold` splitter with a `RandomState` instance passed to `random_state` yields different data splits. This behavior is important to consider when comparing different estimators, as it can lead to overestimating the variance of the difference in performance.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/common_pitfalls.rst#2025-04-14_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n>>> from sklearn.model_selection import KFold\n>>> import numpy as np\n\n>>> X = y = np.arange(10)\n>>> rng = np.random.RandomState(0)\n>>> cv = KFold(n_splits=2, shuffle=True, random_state=rng)\n\n>>> for train, test in cv.split(X, y):\n...     print(train, test)\n[0 3 5 6 7] [1 2 4 8 9]\n[1 2 4 8 9] [0 3 5 6 7]\n\n>>> for train, test in cv.split(X, y):\n...     print(train, test)\n[0 4 6 7 8] [1 2 3 5 9]\n[1 2 3 5 9] [0 4 6 7 8]\n```\n\n----------------------------------------\n\nTITLE: Multi-label Classification with MLPClassifier in Python\nDESCRIPTION: This snippet demonstrates how to use MLPClassifier for multi-label classification. It shows the initialization of the classifier, training on multi-label data, and making predictions for new samples.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neural_networks_supervised.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> X = [[0., 0.], [1., 1.]]\n>>> y = [[0, 1], [1, 1]]\n>>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n...                     hidden_layer_sizes=(15,), random_state=1)\n...\n>>> clf.fit(X, y)\nMLPClassifier(alpha=1e-05, hidden_layer_sizes=(15,), random_state=1,\n              solver='lbfgs')\n>>> clf.predict([[1., 2.]])\narray([[1, 1]])\n>>> clf.predict([[0., 0.]])\narray([[0, 1]])\n```\n\n----------------------------------------\n\nTITLE: Using ColumnTransformer with DataFrame Input in Python\nDESCRIPTION: Enhancement to ColumnTransformer allowing DataFrame input to have columns in a changed order in transform. Dropped columns are not required in transform, and additional columns are ignored if remainder='drop'.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ncompose.ColumnTransformer(remainder='drop')\n```\n\n----------------------------------------\n\nTITLE: Configuring Sample Weight in LogisticRegression\nDESCRIPTION: Example showing the correct way to configure sample_weight metadata routing for both fit and score methods in LogisticRegression.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/metadata_routing.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlr = LogisticRegression().set_fit_request(\n    sample_weight=True\n).set_score_request(sample_weight=False)\n```\n\n----------------------------------------\n\nTITLE: Using OrdinalEncoder with Custom Missing Value Encoding in Python\nDESCRIPTION: Demonstrates configuring OrdinalEncoder to handle missing values by encoding them with a specific value (-1) rather than passing them through as NaN values.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> enc = preprocessing.OrdinalEncoder(encoded_missing_value=-1)\n>>> X = [['male'], ['female'], [np.nan], ['female']]\n>>> enc.fit_transform(X)\narray([[ 1.],\n       [ 0.],\n       [-1.],\n       [ 0.]])\n```\n\n----------------------------------------\n\nTITLE: Applying Shrinkage to Covariance Matrix in Python with scikit-learn\nDESCRIPTION: This code shows how to apply shrinkage to a pre-computed covariance matrix using scikit-learn's shrunk_covariance function, or by fitting a ShrunkCovariance object to the data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/covariance.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.covariance import shrunk_covariance, ShrunkCovariance\n\n# Using the function\nshrunk_cov = shrunk_covariance(emp_cov, shrinkage=0.1)\n\n# Using the object\ncov = ShrunkCovariance(shrinkage=0.1, assume_centered=False)\ncov.fit(X)\n```\n\n----------------------------------------\n\nTITLE: Using r_regression for Feature Selection in Python\nDESCRIPTION: New function r_regression added to compute Pearson's R correlation coefficients between features and target for feature selection.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nfeature_selection.r_regression(X, y)\n```\n\n----------------------------------------\n\nTITLE: Computing V-measure Score with Custom Beta Parameter in Python\nDESCRIPTION: This example demonstrates how to calculate the V-measure score with a custom beta parameter, which adjusts the weight given to homogeneity versus completeness in the calculation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import metrics\nlabels_true = [0, 0, 0, 1, 1, 1]\nlabels_pred = [0, 0, 1, 1, 2, 2]\n\nmetrics.v_measure_score(labels_true, labels_pred, beta=0.6)\nmetrics.v_measure_score(labels_true, labels_pred, beta=1.8)\n```\n\n----------------------------------------\n\nTITLE: Instantiating a scikit-learn Estimator in Python\nDESCRIPTION: Demonstrates the correct way to instantiate a scikit-learn estimator, using keyword arguments with default values. The example shows both correct and incorrect instantiation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/develop.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclf2 = SGDClassifier(alpha=2.3)\nclf3 = SGDClassifier([[1, 2], [2, 3]], [-1, 1]) # WRONG!\n```\n\n----------------------------------------\n\nTITLE: Fixing Sample Weight Handling in RANSACRegressor in scikit-learn\nDESCRIPTION: Fixes a bug where sample_weight was not passed to the base_estimator during final model fitting in RANSACRegressor.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nlinear_model.RANSACRegressor()\n```\n\n----------------------------------------\n\nTITLE: Using utils.estimator_checks.check_estimator for Testing Custom Estimators\nDESCRIPTION: Demonstrates how to use the check_estimator function to test custom estimators for compliance with scikit-learn's conventions and requirements.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\nutils.estimator_checks.check_estimator\n```\n\n----------------------------------------\n\nTITLE: Brute Force Algorithm Configuration in scikit-learn\nDESCRIPTION: Configures the nearest neighbor search to use brute force computation using the algorithm parameter. This approach calculates distances between all pairs of points and scales as O[D N^2].\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neighbors.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nalgorithm = 'brute'\n```\n\n----------------------------------------\n\nTITLE: Initializing CountVectorizer with n-grams\nDESCRIPTION: This snippet shows how to initialize `CountVectorizer` to extract both 1-grams and 2-grams (bigrams) from text. It also specifies a token pattern to match whole words and sets a minimum document frequency (min_df) of 1.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n...                                     token_pattern=r'\\b\\w+\\b', min_df=1)\n```\n\n----------------------------------------\n\nTITLE: Adding Sample Weights to KMeans and MiniBatchKMeans in Python\nDESCRIPTION: KMeans and MiniBatchKMeans now support sample weights via a new sample_weight parameter in the fit function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nKMeans().fit(X, sample_weight=weights)\nMiniBatchKMeans().fit(X, sample_weight=weights)\n```\n\n----------------------------------------\n\nTITLE: Training Ridge Regression Model with Diabetes Dataset\nDESCRIPTION: Example showing how to train a Ridge regression model on the diabetes dataset and evaluate its performance using train-test split.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/permutation_importance.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\ndiabetes = load_diabetes()\nX_train, X_val, y_train, y_val = train_test_split(\n    diabetes.data, diabetes.target, random_state=0)\nmodel = Ridge(alpha=1e-2).fit(X_train, y_train)\nmodel.score(X_val, y_val)\n```\n\n----------------------------------------\n\nTITLE: Predicting with a Fitted Random Forest Classifier in scikit-learn\nDESCRIPTION: Example showing how to use a fitted RandomForestClassifier to predict class labels for both training data and new unseen data without retraining the model.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/getting_started.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> clf.predict(X)  # predict classes of the training data\narray([0, 1])\n>>> clf.predict([[4, 5, 6], [14, 15, 16]])  # predict classes of new data\narray([0, 1])\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiclass Multioutput Classification with Scikit-Learn Python\nDESCRIPTION: This code snippet demonstrates the use of Scikit-Learn's `MultiOutputClassifier` with `RandomForestClassifier` to tackle a multiclass multioutput classification task. It shows how to create a dataset with multiple output classes and fit the classifier. Required dependencies include Scikit-Learn for classification and dataset creation, as well as NumPy for array manipulation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/multiclass.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.multioutput import MultiOutputClassifier\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.utils import shuffle\n>>> import numpy as np\n>>> X, y1 = make_classification(n_samples=10, n_features=100,\n...                             n_informative=30, n_classes=3,\n...                             random_state=1)\n>>> y2 = shuffle(y1, random_state=1)\n>>> y3 = shuffle(y1, random_state=2)\n>>> Y = np.vstack((y1, y2, y3)).T\n>>> n_samples, n_features = X.shape # 10,100\n>>> n_outputs = Y.shape[1] # 3\n>>> n_classes = 3\n>>> forest = RandomForestClassifier(random_state=1)\n>>> multi_target_forest = MultiOutputClassifier(forest, n_jobs=2)\n>>> multi_target_forest.fit(X, Y).predict(X)\narray([[2, 2, 0],\n           [1, 2, 1],\n           [2, 1, 0],\n           [0, 0, 2],\n           [0, 2, 1],\n           [0, 0, 2],\n           [1, 1, 0],\n           [1, 1, 1],\n           [0, 0, 2],\n           [2, 0, 0]])\n```\n\n----------------------------------------\n\nTITLE: Implementing get_params Method in Python\nDESCRIPTION: Demonstrates how the get_params method works for a custom estimator, showing the difference between deep=True and deep=False.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/develop.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.base import BaseEstimator\n>>> from sklearn.linear_model import LogisticRegression\n>>> class MyEstimator(BaseEstimator):\n...     def __init__(self, subestimator=None, my_extra_param=\"random\"):\n...         self.subestimator = subestimator\n...         self.my_extra_param = my_extra_param\n\n>>> my_estimator = MyEstimator(subestimator=LogisticRegression())\n>>> for param, value in my_estimator.get_params(deep=True).items():\n...     print(f\"{param} -> {value}\")\nmy_extra_param -> random\nsubestimator__C -> 1.0\nsubestimator__class_weight -> None\nsubestimator__dual -> False\nsubestimator__fit_intercept -> True\nsubestimator__intercept_scaling -> 1\nsubestimator__l1_ratio -> None\nsubestimator__max_iter -> 100\nsubestimator__multi_class -> deprecated\nsubestimator__n_jobs -> None\nsubestimator__penalty -> l2\nsubestimator__random_state -> None\nsubestimator__solver -> lbfgs\nsubestimator__tol -> 0.0001\nsubestimator__verbose -> 0\nsubestimator__warm_start -> False\nsubestimator -> LogisticRegression()\n\n>>> for param, value in my_estimator.get_params(deep=False).items():\n...     print(f\"{param} -> {value}\")\nmy_extra_param -> random\nsubestimator -> LogisticRegression()\n```\n\n----------------------------------------\n\nTITLE: Using KNeighborsTransformer in scikit-learn neighbors\nDESCRIPTION: New transformer added to convert input dataset into a sparse neighbors graph, providing more control over nearest neighbors computations.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nneighbors.KNeighborsTransformer\n```\n\n----------------------------------------\n\nTITLE: Specifying Interaction Constraints for Gradient Boosting in Python\nDESCRIPTION: Shows how to specify interaction constraints as strings for HistGradientBoostingClassifier and HistGradientBoostingRegressor, using 'no_interactions' or 'pairwise' options.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nensemble.HistGradientBoostingClassifier(interaction_cst=\"no_interactions\")\n```\n\nLANGUAGE: Python\nCODE:\n```\nensemble.HistGradientBoostingRegressor(interaction_cst=\"pairwise\")\n```\n\n----------------------------------------\n\nTITLE: Adding max_error metric for regression in Python\nDESCRIPTION: The metrics.max_error function has been added as a new metric for single output regression, along with a corresponding 'max_error' scorer.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.max_error\n```\n\n----------------------------------------\n\nTITLE: Perfect Labeling Rand Index in scikit-learn in Python\nDESCRIPTION: This snippet shows that when the predicted labels perfectly match the true labels, the Rand Index is 1.0, indicating a perfect match between the two clusterings. It uses `labels_true[:]` to create a copy of the `labels_true` list and assign it to `labels_pred`.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> labels_pred = labels_true[:]\n>>> metrics.rand_score(labels_true, labels_pred)\n1.0\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\n1.0\n```\n\n----------------------------------------\n\nTITLE: Nearest Centroid Classifier Usage\nDESCRIPTION: This code snippet illustrates how to use the NearestCentroid classifier in scikit-learn. It imports the necessary classes, creates sample data, fits the classifier, and predicts the class of a new sample.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neighbors.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.neighbors import NearestCentroid\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> clf = NearestCentroid()\n>>> clf.fit(X, y)\nNearestCentroid()\n>>> print(clf.predict([[-0.8, -1]]))\n[1]\n```\n\n----------------------------------------\n\nTITLE: Creating ConfusionMatrixDisplay with custom text settings in Python\nDESCRIPTION: Demonstrates creating a ConfusionMatrixDisplay with custom text settings using the new text_kw parameter. This allows customization of the text in the confusion matrix plot.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_estimator(estimator, X, y, text_kw={'fontsize': 12})\n```\n\n----------------------------------------\n\nTITLE: Converting Distance Matrix to Similarity Matrix in Python\nDESCRIPTION: Example of transforming a distance matrix into a similarity matrix using a heat kernel transformation. This is useful when working with spectral clustering to ensure the matrix has well-distributed values.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsimilarity = np.exp(-beta * distance / distance.std())\n```\n\n----------------------------------------\n\nTITLE: Retrieving Feature Names\nDESCRIPTION: This snippet demonstrates how to retrieve the feature names (tokens) found by the analyzer during the fitting process. These names correspond to the columns in the resulting matrix, providing a mapping between column indices and tokens.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> vectorizer.get_feature_names_out()\narray(['and', 'document', 'first', 'is', 'one', 'second', 'the',\n       'third', 'this'], ...)\n```\n\n----------------------------------------\n\nTITLE: Accessing Categories from a Fitted OneHotEncoder in Python\nDESCRIPTION: Shows how to access the categories that OneHotEncoder automatically determines from the dataset using the categories_ attribute after fitting.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n>>> enc.categories_\n[array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object), array(['uses Firefox', 'uses Safari'], dtype=object)]\n```\n\n----------------------------------------\n\nTITLE: Regularization Options for Logistic Regression\nDESCRIPTION: Table showing the mathematical formulas for different regularization options (None, L1, L2, ElasticNet) available in scikit-learn's logistic regression implementation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_9\n\nLANGUAGE: latex\nCODE:\n```\n+----------------+----------------------------------------------------------------------------------+\n  | penalty        | :math:`r(W)`                                                                     |\n  +================+==================================================================================+\n  | `None`         | :math:`0`                                                                        |\n  +----------------+----------------------------------------------------------------------------------+\n  | :math:`\\ell_1` | :math:`|W|_{1,1} = \\sum_{i=1}^m\\sum_{j=1}^{K}|W_{i,j}|`                        |\n  +----------------+----------------------------------------------------------------------------------+\n  | :math:`\\ell_2` | :math:`\\frac{1}{2}|W|_F^2 = \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^{K} W_{i,j}^2`   |\n  +----------------+----------------------------------------------------------------------------------+\n  | `ElasticNet`   | :math:`\\frac{1 - \\rho}{2}|W|_F^2 + \\rho |W|_{1,1}`                           |\n  +----------------+----------------------------------------------------------------------------------+\n```\n\n----------------------------------------\n\nTITLE: Displaying System and Library Version Information with sklearn.show_versions\nDESCRIPTION: Demonstrates how to use the show_versions utility method to display debugging information including system details, Python executable, library versions, and BLAS binding information.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\nsklearn.show_versions()\n```\n\n----------------------------------------\n\nTITLE: Using PolynomialCountSketch for Kernel Approximation in Python\nDESCRIPTION: This snippet introduces the new PolynomialCountSketch class for polynomial kernel feature map approximation using the Tensor Sketch algorithm.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.kernel_approximation import PolynomialCountSketch\n\n# Usage example (parameters not shown)\nsketch = PolynomialCountSketch()\nsketch.fit_transform(X)\n```\n\n----------------------------------------\n\nTITLE: Adding ComplementNB naive Bayes classifier in Python\nDESCRIPTION: Added ComplementNB class implementing the Complement Naive Bayes classifier.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.naive_bayes import ComplementNB\ncnb = ComplementNB()\ncnb.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Metrics in Cross-Validation\nDESCRIPTION: Example of using the new cross_validate function to return multiple evaluation metrics from cross-validation, including training scores and timing information.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import recall_score\n\nscoring = ['precision_macro', 'recall_macro']\n\n# Get multiple metrics\ncv_results = cross_validate(\n    estimator, X, y, \n    scoring=scoring,\n    cv=5, \n    return_train_score=True\n)\n\n# Results contain multiple metrics\nprint(cv_results['test_precision_macro'])\nprint(cv_results['test_recall_macro'])\nprint(cv_results['train_precision_macro'])\nprint(cv_results['fit_time'])\nprint(cv_results['score_time'])\n```\n\n----------------------------------------\n\nTITLE: Skipping Parameter Validation in scikit-learn\nDESCRIPTION: Shows how to use the new skip_parameter_validation option in sklearn.set_config and sklearn.config_context to bypass parameter validation for performance, with a warning about potential risks.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.3.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn import set_config, config_context\n\n# Skip parameter validation globally\nset_config(skip_parameter_validation=True)\n\n# Or use as a context manager\nwith config_context(skip_parameter_validation=True):\n    # Code with skipped parameter validation\n```\n\n----------------------------------------\n\nTITLE: Accessing Feature Names Attribute in Scikit-learn\nDESCRIPTION: Change in behavior for the feature_names_in_ attribute when fitting an estimator on datasets with and without feature names.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nestimator.feature_names_in_\n```\n\n----------------------------------------\n\nTITLE: Adding KBinsDiscretizer for binning continuous features in Python\nDESCRIPTION: Added KBinsDiscretizer for turning continuous features into categorical or one-hot encoded features.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.preprocessing import KBinsDiscretizer\nkbd = KBinsDiscretizer(n_bins=5, encode='ordinal')\nkbd.fit(X)\n```\n\n----------------------------------------\n\nTITLE: Using Memory with Pipeline for Caching Transformations\nDESCRIPTION: Example showing how to use the memory parameter in Pipeline to cache transformations, which improves efficiency when performing grid search over pipelines that include slow transformations.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.pipeline import Pipeline\nfrom joblib import Memory\n\n# Using a string path for caching\npipe = Pipeline(steps=[...], memory=\"/path/to/cache\")\n\n# Using a joblib.Memory instance directly\nmemory = Memory(location=\"/path/to/cache\", verbose=0)\npipe = Pipeline(steps=[...], memory=memory)\n```\n\n----------------------------------------\n\nTITLE: Transforming Target Variable in Regression with QuantileTransformer\nDESCRIPTION: Shows how to use TransformedTargetRegressor to apply a QuantileTransformer to target variables before fitting a linear regression model, improving regression performance\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntransformer = QuantileTransformer(output_distribution='normal')\nregressor = LinearRegression()\nregr = TransformedTargetRegressor(regressor=regressor,\n                                   transformer=transformer)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nregr.fit(X_train, y_train)\nprint('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Kernel Configuration in scikit-learn\nDESCRIPTION: Example showing how to create and configure a compound kernel using ConstantKernel and RBF kernels, demonstrating parameter access and manipulation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/gaussian_process.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.gaussian_process.kernels import ConstantKernel, RBF\nkernel = ConstantKernel(constant_value=1.0, constant_value_bounds=(0.0, 10.0)) * RBF(length_scale=0.5, length_scale_bounds=(0.0, 10.0)) + RBF(length_scale=2.0, length_scale_bounds=(0.0, 10.0))\nfor hyperparameter in kernel.hyperparameters: print(hyperparameter)\nparams = kernel.get_params()\nfor key in sorted(params): print(\"%s : %s\" % (key, params[key]))\nprint(kernel.theta)  # Note: log-transformed\nprint(kernel.bounds)  # Note: log-transformed\n```\n\n----------------------------------------\n\nTITLE: Custom Feature Extraction Function for FeatureHasher in Python\nDESCRIPTION: This code defines a function that extracts features from token and part-of-speech pairs for NLP tasks. It demonstrates how to create custom features to be used with FeatureHasher, including syntactic and morphological features.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef token_features(token, part_of_speech):\n    if token.isdigit():\n        yield \"numeric\"\n    else:\n        yield \"token={}\".format(token.lower())\n        yield \"token,pos={},{}\".format(token, part_of_speech)\n    if token[0].isupper():\n        yield \"uppercase_initial\"\n    if token.isupper():\n        yield \"all_uppercase\"\n    yield \"pos={}\".format(part_of_speech)\n```\n\n----------------------------------------\n\nTITLE: Ensuring estimator type consistency in Stacking and Voting estimators\nDESCRIPTION: Updates StackingClassifier, StackingRegressor, VotingClassifier, and VotingRegressor to ensure their underlying estimators are either all classifiers or all regressors, raising consistent error messages if not.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nensemble.StackingClassifier\n```\n\nLANGUAGE: Python\nCODE:\n```\nensemble.StackingRegressor\n```\n\nLANGUAGE: Python\nCODE:\n```\nensemble.VotingClassifier\n```\n\nLANGUAGE: Python\nCODE:\n```\nensemble.VotingRegressor\n```\n\n----------------------------------------\n\nTITLE: Adding Sample Weights to BayesianRidge (Python)\nDESCRIPTION: Adds a sample_weight parameter to the fit method of BayesianRidge to support weighted linear regression. This allows assigning different weights to training samples.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nsample_weight\n```\n\n----------------------------------------\n\nTITLE: Using PatchExtractor for Multiple Images in scikit-learn\nDESCRIPTION: This snippet illustrates the use of PatchExtractor class to extract patches from multiple images simultaneously. It shows how PatchExtractor can be used as a scikit-learn transformer in pipelines.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfive_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)\npatches = image.PatchExtractor(patch_size=(2, 2)).transform(five_images)\npatches.shape\n```\n\n----------------------------------------\n\nTITLE: Calculating Fall Out for Multilabel Classification using Confusion Matrix in Python\nDESCRIPTION: This example demonstrates how to calculate fall out (false positive rate) for each class in a multilabel classification problem using the multilabel_confusion_matrix function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n>>> fp / (fp + tn)\narray([0. , 1. , 0.5])\n```\n\n----------------------------------------\n\nTITLE: Referencing permutation_test_score Function in Python\nDESCRIPTION: This snippet shows how to reference the permutation_test_score function from scikit-learn's model_selection module. The function is used to perform permutation tests for evaluating estimator performance.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/cross_validation.rst#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import permutation_test_score\n```\n\n----------------------------------------\n\nTITLE: Adding novelty detection to LocalOutlierFactor in Python\nDESCRIPTION: Added novelty parameter to LocalOutlierFactor to enable novelty detection on new unseen data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.neighbors import LocalOutlierFactor\nlof = LocalOutlierFactor(novelty=True)\nlof.fit(X)\ny_pred = lof.predict(X_new)\n```\n\n----------------------------------------\n\nTITLE: Adding refit_time_ attribute to GridSearchCV in Python\nDESCRIPTION: Added refit_time_ attribute to GridSearchCV and RandomizedSearchCV to measure time for hyperparameter optimization and refitting on whole dataset.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nsearch = GridSearchCV(estimator, param_grid, refit=True)\nsearch.fit(X, y)\nprint(search.refit_time_)\n```\n\n----------------------------------------\n\nTITLE: Implementing LeavePGroupsOut Cross-Validation in Python\nDESCRIPTION: Shows how to use LeavePGroupsOut, which removes samples related to P groups for each training/test set. This example demonstrates Leave-2-Group Out cross-validation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/cross_validation.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import LeavePGroupsOut\n\nX = np.arange(6)\ny = [1, 1, 1, 2, 2, 2]\ngroups = [1, 1, 2, 2, 3, 3]\nlpgo = LeavePGroupsOut(n_groups=2)\nfor train, test in lpgo.split(X, y, groups=groups):\n    print(\"%s %s\" % (train, test))\n```\n\n----------------------------------------\n\nTITLE: Loading Forest Covertypes Dataset in Python using scikit-learn\nDESCRIPTION: This snippet demonstrates how to load the Forest Covertypes dataset using the fetch_covtype function from scikit-learn. It returns a Bunch object containing the feature matrix and target values, with an option to return data as pandas DataFrames.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/covtype.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsklearn.datasets.fetch_covtype()\n```\n\n----------------------------------------\n\nTITLE: Using clip parameter in MinMaxScaler in Python\nDESCRIPTION: New clip parameter added to MinMaxScaler to clip transformed values of test data to the specified feature_range.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, 1), clip=True)\n```\n\n----------------------------------------\n\nTITLE: Setting Multiprocessing Start Method in Python for Scikit-Learn\nDESCRIPTION: This code snippet explains how to set the multiprocessing start method to 'forkserver' in Python to avoid issues with advanced libraries while using scikit-learn. It emphasizes the importance of applying this change in the '__main__' section of a script.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/faq.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport multiprocessing\n\n# other imports, custom code, load data, define model...\n\nif __name__ == \"__main__\":\n    multiprocessing.set_start_method(\"forkserver\")\n\n    # call scikit-learn utils with n_jobs > 1 here\n```\n\n----------------------------------------\n\nTITLE: Using Poisson Criterion in RandomForestRegressor in Python\nDESCRIPTION: Enhancement to RandomForestRegressor adding Poisson criterion as an option.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nensemble.RandomForestRegressor(criterion='poisson')\n```\n\n----------------------------------------\n\nTITLE: Adding error_score parameter to cross-validation functions in Python\nDESCRIPTION: Added error_score parameter to cross_validate, cross_val_score, learning_curve and validation_curve to control behavior when an error occurs in fit_and_score.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\ncross_validate(estimator, X, y, error_score='raise')\n```\n\n----------------------------------------\n\nTITLE: Using decision_function_shape in SVM Models in Python\nDESCRIPTION: Example of using the new decision_function_shape parameter in SVC and NuSVC to get a decision function of shape (n_samples, n_classes) by setting it to 'ovr' (one-vs-rest).\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.17.rst#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.svm import SVC\n\nsvm = SVC(decision_function_shape='ovr')\nsvm.fit(X, y)\ndecisions = svm.decision_function(X_test)\n```\n\n----------------------------------------\n\nTITLE: Downloading a Dataset from OpenML Repository by Name\nDESCRIPTION: Shows how to download a dataset from the OpenML repository using fetch_openml function with a name parameter. The example retrieves the 'miceprotein' dataset (version 4) which contains gene expressions in mice brains.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/datasets/loading_other_datasets.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import fetch_openml\nmice = fetch_openml(name='miceprotein', version=4)\n```\n\n----------------------------------------\n\nTITLE: Custom Target Transformation with Log Functions\nDESCRIPTION: Demonstrates applying custom transformation functions to target variables using TransformedTargetRegressor with logarithmic transformation\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef func(x):\n    return np.log(x)\ndef inverse_func(x):\n    return np.exp(x)\n\nregr = TransformedTargetRegressor(regressor=regressor,\n                                   func=func,\n                                   inverse_func=inverse_func)\nregr.fit(X_train, y_train)\nprint('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\n```\n\n----------------------------------------\n\nTITLE: Using config_context to Disable Input Validation for Faster Prediction\nDESCRIPTION: Example demonstrating the use of config_context to turn off validation of finite input values, which can speed up prediction time when you're certain the input is valid.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.utils import config_context\n\n# Regular prediction with validation\ny_pred = estimator.predict(X)\n\n# Faster prediction without finite validation\nwith config_context(assume_finite=True):\n    y_pred_fast = estimator.predict(X)\n```\n\n----------------------------------------\n\nTITLE: Implementing Gaussian Naive Bayes Classification in Python with scikit-learn\nDESCRIPTION: This snippet demonstrates how to use the GaussianNB class from scikit-learn to perform Gaussian Naive Bayes classification on the Iris dataset. It includes loading the data, splitting it into training and test sets, fitting the model, and evaluating its performance.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/naive_bayes.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train, y_train).predict(X_test)\nprint(\"Number of mislabeled points out of a total %d points : %d\"\n      % (X_test.shape[0], (y_test != y_pred).sum()))\n```\n\n----------------------------------------\n\nTITLE: Comparing Tree-Based Classifiers Performance\nDESCRIPTION: Compares the performance of DecisionTree, RandomForest, and ExtraTrees classifiers using cross-validation on a synthetic dataset with multiple centers.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.model_selection import cross_val_score\n>>> from sklearn.datasets import make_blobs\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.ensemble import ExtraTreesClassifier\n>>> from sklearn.tree import DecisionTreeClassifier\n\n>>> X, y = make_blobs(n_samples=10000, n_features=10, centers=100,\n...     random_state=0)\n\n>>> clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n...     random_state=0)\n>>> scores = cross_val_score(clf, X, y, cv=5)\n>>> scores.mean()\nnp.float64(0.98...)\n\n>>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n...     min_samples_split=2, random_state=0)\n>>> scores = cross_val_score(clf, X, y, cv=5)\n>>> scores.mean()\nnp.float64(0.999...)\n\n>>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n...     min_samples_split=2, random_state=0)\n>>> scores = cross_val_score(clf, X, y, cv=5)\n>>> scores.mean() > 0.999\nnp.True_\n```\n\n----------------------------------------\n\nTITLE: Decision function of SGDClassifier\nDESCRIPTION: This snippet demonstrates how to use the `decision_function` method of a trained SGDClassifier. This method calculates the signed distance of a sample to the hyperplane, indicating the confidence of the prediction. A positive value indicates the sample is on one side of the hyperplane, while a negative value indicates the other side.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/sgd.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> clf.decision_function([[2., 2.]])\narray([29.6...])\n```\n\n----------------------------------------\n\nTITLE: Computing Sample-wise Multi-label Confusion Matrix in Python\nDESCRIPTION: This example shows how to use the multilabel_confusion_matrix function to compute a sample-wise multilabel confusion matrix. It demonstrates usage with the samplewise parameter set to True.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n>>> multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\narray([[[1, 0],\n        [1, 1]],\n\n       [[1, 1],\n        [0, 1]]])\n```\n\n----------------------------------------\n\nTITLE: Predicting with DecisionTreeClassifier handling missing values in Python\nDESCRIPTION: This code snippet demonstrates how DecisionTreeClassifier handles missing values during prediction. By default, samples with missing values are classified with the class used in the split found during training.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/tree.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\nX = np.array([0, 1, 6, np.nan]).reshape(-1, 1)\ny = [0, 0, 1, 1]\n\ntree = DecisionTreeClassifier(random_state=0).fit(X, y)\ntree.predict(X)\n```\n\n----------------------------------------\n\nTITLE: KDTree Valid Metrics\nDESCRIPTION: This code snippet demonstrates how to retrieve a list of valid metrics for the KDTree algorithm using the `valid_metrics` attribute. It imports the KDTree class from `sklearn.neighbors` and prints a sorted list of valid metrics.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neighbors.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.neighbors import KDTree\n>>> print(sorted(KDTree.valid_metrics))\n['chebyshev', 'cityblock', 'euclidean', 'infinity', 'l1', 'l2', 'manhattan', 'minkowski', 'p']\n```\n\n----------------------------------------\n\nTITLE: Using PyTorch Tensors with LinearDiscriminantAnalysis\nDESCRIPTION: Demonstrates how to use PyTorch tensors with LinearDiscriminantAnalysis using Array API dispatch.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/array_api.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> X_torch = torch.asarray(X_np, device=\"cuda\", dtype=torch.float32)\n>>> y_torch = torch.asarray(y_np, device=\"cuda\", dtype=torch.float32)\n\n>>> with config_context(array_api_dispatch=True):\n...     lda = LinearDiscriminantAnalysis()\n...     X_trans = lda.fit_transform(X_torch, y_torch)\n>>> type(X_trans)\n<class 'torch.Tensor'>\n>>> X_trans.device.type\n'cuda'\n```\n\n----------------------------------------\n\nTITLE: Installing scikit-learn using pip\nDESCRIPTION: Command to install scikit-learn using pip package manager. This requires a working installation of NumPy and SciPy as prerequisites.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/README.rst#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U scikit-learn\n```\n\n----------------------------------------\n\nTITLE: Predictive Missing Pattern Classification\nDESCRIPTION: Demonstrates how HistGradientBoostingClassifier can learn from missing value patterns when they are predictive of the target variable.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/ensemble.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> X = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1)\n>>> y = [0, 1, 0, 0, 1]\n>>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1,\n...                                       max_depth=2,\n...                                       learning_rate=1,\n...                                       max_iter=1).fit(X, y)\n>>> gbdt.predict(X)\narray([0, 1, 0, 0, 1])\n```\n\n----------------------------------------\n\nTITLE: Using auto n_init in KMeans\nDESCRIPTION: Demonstrates the new 'auto' option for n_init in KMeans, which sets n_init=1 when using k-means++ initialization for efficiency.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.cluster import KMeans\n\nKMeans(n_init='auto', init='k-means++')\n```\n\n----------------------------------------\n\nTITLE: Adding PowerTransformer for Yeo-Johnson and Box-Cox transforms in Python\nDESCRIPTION: Added PowerTransformer to implement Yeo-Johnson and Box-Cox power transformations for feature-wise parametric transformations.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.preprocessing import PowerTransformer\npt = PowerTransformer(method='yeo-johnson')\nX_transformed = pt.fit_transform(X)\n```\n\n----------------------------------------\n\nTITLE: Computing Pair Confusion Matrix with Inverted Labels\nDESCRIPTION: Example showing pair_confusion_matrix computation with inverted but equivalent cluster assignments.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics.cluster import pair_confusion_matrix\npair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\n```\n\n----------------------------------------\n\nTITLE: Adding n_jobs parameter to feature_selection.RFECV in Python\nDESCRIPTION: Adds an n_jobs parameter to RFECV in the feature_selection module to compute scores on test folds in parallel.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.18.rst#2025-04-14_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfeature_selection.RFECV(estimator, n_jobs=n_jobs)\n```\n\n----------------------------------------\n\nTITLE: Configuring CategoricalNB with min_categories in Python\nDESCRIPTION: New parameter min_categories added to CategoricalNB to specify minimum number of categories per feature, allowing for unseen categories during training.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.naive_bayes import CategoricalNB\n\ncnb = CategoricalNB(min_categories=3)\n```\n\n----------------------------------------\n\nTITLE: Adding callable refit option to GridSearchCV and RandomizedSearchCV in Python\nDESCRIPTION: The model_selection.GridSearchCV and model_selection.RandomizedSearchCV classes now allow refit to be a callable, providing more flexibility in identifying the best estimator.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.GridSearchCV\n```\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.RandomizedSearchCV\n```\n\n----------------------------------------\n\nTITLE: Float32 dtype preservation in BayesianRidge and ARDRegression (Python)\nDESCRIPTION: Updates BayesianRidge and ARDRegression to preserve float32 dtype when input data is in float32 format, improving memory efficiency for large datasets.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.1.rst#2025-04-14_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nfrom sklearn.linear_model import BayesianRidge, ARDRegression\n\nX = np.array(..., dtype=np.float32)\ny = np.array(..., dtype=np.float32)\n\nmodel = BayesianRidge()\nmodel.fit(X, y)\n# model.coef_ will now be in float32 dtype\n```\n\n----------------------------------------\n\nTITLE: Controlling Joblib Backend in Python\nDESCRIPTION: Example of using a context manager to control the backend that joblib uses for parallelization in scikit-learn code.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/computing/parallelism.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom joblib import parallel_backend\n\nwith parallel_backend('threading', n_jobs=2):\n    # Your scikit-learn code here\n```\n\n----------------------------------------\n\nTITLE: Handling missing values in OneHotEncoder in Python\nDESCRIPTION: Added support for handling missing values in OneHotEncoder by treating them as a separate category.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.preprocessing import OneHotEncoder\n\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit_transform(X_with_missing)\n```\n\n----------------------------------------\n\nTITLE: Using HalvingGridSearchCV with Support Vector Classifier\nDESCRIPTION: This snippet illustrates the configuration of HalvingGridSearchCV for an SVC, focusing on how resources are allocated during the training process, particularly when resources may be underutilized.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/grid_search.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.svm import SVC\n>>> from sklearn.experimental import enable_halving_search_cv  # noqa\n>>> from sklearn.model_selection import HalvingGridSearchCV\n>>> import pandas as pd\n>>> param_grid= {'kernel': ('linear', 'rbf'),\n...              'C': [1, 10, 100]}\n>>> base_estimator = SVC(gamma='scale')\n>>> X, y = make_classification(n_samples=1000)\n>>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\n...                          factor=2, min_resources=20).fit(X, y)\n>>> sh.n_resources_\n[20, 40, 80]\n```\n\n----------------------------------------\n\nTITLE: Creating Sparse Neighbor Graph\nDESCRIPTION: Shows how to generate a sparse graph representing connections between neighboring points using the kneighbors_graph method.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neighbors.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nnbrs.kneighbors_graph(X).toarray()\n```\n\n----------------------------------------\n\nTITLE: Using normalized stress in MDS manifold learning in Python\nDESCRIPTION: Shows how to enable normalized stress in Multidimensional Scaling (MDS) by setting the new normalize parameter to True. This can be useful for certain types of data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.manifold import MDS\n\nmds = MDS(normalize=True)\n```\n\n----------------------------------------\n\nTITLE: Utilizing the BIRCH Clustering Algorithm in Scikit-learn\nDESCRIPTION: Illustrates how the BIRCH algorithm in Scikit-learn compresses data into Clustering Feature nodes for efficient clustering. It reduces data by forming subclusters, which hold statistical data without requiring the entire input in memory. The algorithm utilizes parameters like threshold and branching factor for node management. Suitable for datasets that require significant reduction or preprocessing before global clustering. Dependencies include Scikit-learn, and outputs result in organized subclusters suitable for further processing.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.cluster import Birch\nbirch_instance = Birch(threshold=0.5, branching_factor=50)\nbirch_instance.fit(data)\nsubclusters = birch_instance.subcluster_centers_\nlabels = birch_instance.labels_\n```\n\n----------------------------------------\n\nTITLE: Classifier predict method example\nDESCRIPTION: This snippet shows how to implement the `predict` method in a classifier that also implements `decision_function`. It uses `np.argmax` to determine the class label based on the decision values.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/develop.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef predict(self, X):\n    D = self.decision_function(X)\n    return self.classes_[np.argmax(D, axis=1)]\n```\n\n----------------------------------------\n\nTITLE: Calculating Recall Score with Label Exclusion in Python using scikit-learn\nDESCRIPTION: This example shows how to calculate the recall score for a multi-class classification problem while excluding certain labels. It demonstrates the use of the 'labels' parameter in the recall_score function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nmetrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')\n```\n\n----------------------------------------\n\nTITLE: CategoricalNB Feature Encoding\nDESCRIPTION: Categorical Naive Bayes implementation that requires features to be encoded as integers from 0 to n_i-1, where n_i is the number of categories for feature i. Typically used with OrdinalEncoder for preprocessing.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/naive_bayes.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nCategoricalNB\n```\n\n----------------------------------------\n\nTITLE: SGDClassifier with RandomState instance: Demonstrating non-reproducible fit\nDESCRIPTION: This snippet demonstrates that calling `fit` multiple times on an `SGDClassifier` with a `RandomState` instance passed to `random_state` will produce different models. This is because the Random Number Generator (RNG) is consumed and mutated with each call to `fit`.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/common_pitfalls.rst#2025-04-14_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n>>> from sklearn.linear_model import SGDClassifier\n>>> from sklearn.datasets import make_classification\n>>> import numpy as np\n\n>>> rng = np.random.RandomState(0)\n>>> X, y = make_classification(n_features=5, random_state=rng)\n>>> sgd = SGDClassifier(random_state=rng)\n\n>>> sgd.fit(X, y).coef_\narray([[ 8.85418642,  4.79084103, -3.13077794,  8.11915045, -0.56479934]])\n\n>>> sgd.fit(X, y).coef_\narray([[ 6.70814003,  5.25291366, -7.55212743,  5.18197458,  1.37845099]])\n```\n\n----------------------------------------\n\nTITLE: Supporting SciPy Sparse Arrays in Scikit-learn Functions\nDESCRIPTION: Multiple scikit-learn functions now support SciPy sparse arrays as input, improving compatibility and efficiency for sparse data structures. This includes updates to clustering, feature selection, and manifold learning functions.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.4.rst#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom scipy.sparse import csr_matrix\nfrom sklearn.cluster import compute_optics_graph\n\nX_sparse = csr_matrix([[1, 2], [3, 4]])\nresult = compute_optics_graph(X_sparse)\n```\n\n----------------------------------------\n\nTITLE: Setting eigen_tol in spectral clustering and embedding\nDESCRIPTION: Demonstrates setting the eigen_tol parameter to None for improved numerical stability when using 'amg' or 'lobpcg' solvers in spectral clustering and embedding.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.cluster import SpectralClustering\nfrom sklearn.manifold import SpectralEmbedding\n\nSpectralClustering(eigen_tol=None, eigen_solver='amg')\nSpectralEmbedding(eigen_tol=None, eigen_solver='lobpcg')\n```\n\n----------------------------------------\n\nTITLE: Getting Feature Names in Scikit-learn Transformers\nDESCRIPTION: Addition of get_feature_names_out method to transformer API and deprecation of get_feature_names.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ntransformer.get_feature_names_out()\ntransformer.get_feature_names()\n```\n\n----------------------------------------\n\nTITLE: Perfect Labeling Adjusted Mutual Information in scikit-learn in Python\nDESCRIPTION: This snippet shows that when the predicted labels perfectly match the true labels, the Adjusted Mutual Information (AMI) score is 1.0, indicating a perfect match between the two clusterings. It also shows that the Normalized Mutual Information (NMI) score is also 1.0 for perfect labelings.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n>>> labels_pred = labels_true[:]\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP\n1.0\n\n>>> metrics.normalized_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP\n1.0\n```\n\n----------------------------------------\n\nTITLE: Plotting Detection Error Tradeoff Curve in Python\nDESCRIPTION: This snippet shows how to use the new det_curve function and plot_det_curve method to compute and visualize the Detection Error Tradeoff curve.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.metrics import det_curve, plot_det_curve\n\n# Compute DET curve\nfpr, fnr, thresholds = det_curve(y_true, y_score)\n\n# Plot DET curve\nplot_det_curve(classifier, X_test, y_test)\n```\n\n----------------------------------------\n\nTITLE: Handling ties for missing values in DecisionTreeClassifier in Python\nDESCRIPTION: This code snippet shows how DecisionTreeClassifier breaks ties for missing values during prediction. If the criterion evaluation is the same for both nodes, the tie is broken by going to the right node.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/tree.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\nX = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)\ny = [0, 0, 1, 1]\n\ntree = DecisionTreeClassifier(random_state=0).fit(X, y)\n\nX_test = np.array([np.nan]).reshape(-1, 1)\ntree.predict(X_test)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Vocabulary Mapping\nDESCRIPTION: This snippet demonstrates how to access the vocabulary mapping stored in the `vocabulary_` attribute of the `CountVectorizer`.  This mapping provides a dictionary-like structure that maps each term to its corresponding column index in the feature matrix.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> vectorizer.vocabulary_.get('document')\n1\n```\n\n----------------------------------------\n\nTITLE: Installing scikit-learn on Windows with pip\nDESCRIPTION: Commands to create a virtual environment and install scikit-learn using pip on Windows. Includes verification steps to check the installation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/install.rst#2025-04-14_snippet_0\n\nLANGUAGE: powershell\nCODE:\n```\npython -m venv sklearn-env\nsklearn-env\\Scripts\\activate  # activate\npip install -U scikit-learn\n\npython -m pip show scikit-learn  # show scikit-learn version and location\npython -m pip freeze             # show all installed packages in the environment\npython -c \"import sklearn; sklearn.show_versions()\"\n```\n\n----------------------------------------\n\nTITLE: Adding normalize_components Parameter to SparsePCA in Python\nDESCRIPTION: SparsePCA now has a normalize_components parameter to control centering of data during fit and transform. Default is False for backwards compatibility.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nSparsePCA(normalize_components=True)\n```\n\n----------------------------------------\n\nTITLE: Using plot_roc_curve in scikit-learn metrics\nDESCRIPTION: New function added to plot ROC curves as part of the visualization API in scikit-learn. This allows easy visualization of model performance.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.plot_roc_curve\n```\n\n----------------------------------------\n\nTITLE: Changing Default n_estimators in Random Forest Models (Python)\nDESCRIPTION: Changes the default value of n_estimators from 10 to 100 for RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier, ExtraTreesRegressor, and RandomTreesEmbedding. Raises a FutureWarning when using the default value.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nn_estimators\n```\n\n----------------------------------------\n\nTITLE: Initializing CalibratedClassifierCV with Sample Weights\nDESCRIPTION: Fixed bug in CalibratedClassifierCV to properly handle sample_weight parameter when passed as a list for estimators that don't support sample weights.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Now correctly handles sample_weight as list\nclf = CalibratedClassifierCV()\nclf.fit(X, y, sample_weight=[1, 1, ...])\n```\n\n----------------------------------------\n\nTITLE: Improved GradientBoostingRegressor Example with Warning\nDESCRIPTION: This improved version of the GradientBoostingRegressor example includes all necessary import statements and fetches the dataset from a URL. It demonstrates the warning occurrence with n_iter_no_change parameter.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/minimal_reproducer.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf = pd.read_csv(\"https://example.com/my_data.csv\")\nX = df[[\"feature_name\"]]\ny = df[\"target\"]\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42\n)\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler(with_mean=False)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngbdt = GradientBoostingRegressor(random_state=0)\ngbdt.fit(X_train, y_train)  # no warning\ndefault_score = gbdt.score(X_test, y_test)\n\ngbdt = GradientBoostingRegressor(random_state=0, n_iter_no_change=5)\ngbdt.fit(X_train, y_train)  # raises warning\nother_score = gbdt.score(X_test, y_test)\nother_score = gbdt.score(X_test, y_test)\n```\n\n----------------------------------------\n\nTITLE: Calculating Partial Dependence and ICE Curves in Python\nDESCRIPTION: This snippet shows the updated partial_dependence function and plot_partial_dependence method, which now support calculating and plotting Individual Conditional Expectation (ICE) curves.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.inspection import partial_dependence, plot_partial_dependence\n\n# New 'kind' parameter for ICE curves\npartial_dependence(..., kind='individual')  # for ICE\nplot_partial_dependence(..., kind='individual')  # for plotting ICE\n```\n\n----------------------------------------\n\nTITLE: Cloning RandomForestClassifier with RandomState instance: Demonstrating statistical clones\nDESCRIPTION: This snippet shows how cloning a `RandomForestClassifier` with a `RandomState` instance results in statistical clones, not exact clones. The original and cloned estimators will still be different models, even when fitted on the same data, because they share the same internal RNG, and fitting one consumes the RNG of the other.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/common_pitfalls.rst#2025-04-14_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n>>> from sklearn import clone\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> import numpy as np\n\n>>> rng = np.random.RandomState(0)\n>>> a = RandomForestClassifier(random_state=rng)\n>>> b = clone(a)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Feature Names in OneHotEncoder\nDESCRIPTION: Shows how to use a custom callable to create feature names in OneHotEncoder by combining input feature and category.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.3.rst#2025-04-14_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nfeature_name_combiner(input_feature, category)\n```\n\n----------------------------------------\n\nTITLE: Computing V-measure Score in Python with scikit-learn\nDESCRIPTION: This code snippet shows how to calculate the V-measure score, which is the harmonic mean of homogeneity and completeness, using scikit-learn's metrics module.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import metrics\nlabels_true = [0, 0, 0, 1, 1, 1]\nlabels_pred = [0, 0, 1, 1, 2, 2]\n\nmetrics.v_measure_score(labels_true, labels_pred)\n```\n\n----------------------------------------\n\nTITLE: Calculating mean Tweedie deviance in Python with scikit-learn\nDESCRIPTION: Shows how to use the mean_tweedie_deviance function from scikit-learn to compute the mean Tweedie deviance error. It demonstrates the effect of different power parameters on the metric.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_51\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.metrics import mean_tweedie_deviance\nmean_tweedie_deviance([1.0], [1.5], power=0)\n0.25\nmean_tweedie_deviance([100.], [150.], power=0)\n2500.0\nmean_tweedie_deviance([1.0], [1.5], power=1)\n0.18...\nmean_tweedie_deviance([100.], [150.], power=1)\n18.9...\nmean_tweedie_deviance([1.0], [1.5], power=2)\n0.14...\nmean_tweedie_deviance([100.], [150.], power=2)\n0.14...\n```\n\n----------------------------------------\n\nTITLE: Adding Positivity Constraints to Dictionary Learning in Python\nDESCRIPTION: Dictionary learning functions and models now support optional positivity constraints on the dictionary and sparse code.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndict_learning(positive_dict=True, positive_code=True)\n```\n\n----------------------------------------\n\nTITLE: Custom Binning with FunctionTransformer and pandas.cut in Python\nDESCRIPTION: This example shows how to use FunctionTransformer with pandas.cut to create custom bins for discretization. It defines custom bin edges and labels for age categories.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n>>> import pandas as pd\n>>> import numpy as np\n>>> from sklearn import preprocessing\n>>>\n>>> bins = [0, 1, 13, 20, 60, np.inf]\n>>> labels = ['infant', 'kid', 'teen', 'adult', 'senior citizen']\n>>> transformer = preprocessing.FunctionTransformer(\n...     pd.cut, kw_args={'bins': bins, 'labels': labels, 'retbins': False}\n... )\n>>> X = np.array([0.2, 2, 15, 25, 97])\n>>> transformer.fit_transform(X)\n['infant', 'kid', 'teen', 'adult', 'senior citizen']\nCategories (5, object): ['infant' < 'kid' < 'teen' < 'adult' < 'senior citizen']\n```\n\n----------------------------------------\n\nTITLE: Setting Parameters in VotingClassifier\nDESCRIPTION: Demonstrates how to change estimators in a VotingClassifier using set_params. An estimator can be removed by setting it to None.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.ensemble import VotingClassifier\n\nvoting_clf = VotingClassifier(estimators=[...])\nvoting_clf.set_params(estimator_name=None)  # Remove an estimator\nvoting_clf.set_params(estimator_name=new_estimator)  # Change an estimator\n```\n\n----------------------------------------\n\nTITLE: Mathematical Expression - Truncated SVD Approximation\nDESCRIPTION: Shows the mathematical formulation of truncated SVD approximation where X is approximated by a low-rank decomposition using k components.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/decomposition.rst#2025-04-14_snippet_1\n\nLANGUAGE: math\nCODE:\n```\nX \\approx X_k = U_k \\Sigma_k V_k^\\top\n```\n\n----------------------------------------\n\nTITLE: Updating Base Mixin Classes in Scikit-learn\nDESCRIPTION: Updates to base mixin classes in scikit-learn, including ClusterMixin, OutlierMixin, and TransformerMixin, to improve metadata handling and provide warnings for potential issues in custom implementations.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.4.rst#2025-04-14_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.base import ClusterMixin, BaseEstimator\n\nclass MyClusterer(BaseEstimator, ClusterMixin):\n    def fit(self, X, y=None, sample_weight=None):\n        # Implementation\n        return self\n    \n    def fit_predict(self, X, y=None, **kwargs):\n        self.fit(X, y, **kwargs)\n        return self.predict(X)\n```\n\n----------------------------------------\n\nTITLE: Generating Categorical Classification Data with NumPy\nDESCRIPTION: Creates synthetic data with non-numeric class labels using numpy's random choice function with specified probabilities.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/minimal_reproducer.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nrng = np.random.RandomState(0)\nn_samples, n_features = 50, 5\nX = rng.randn(n_samples, n_features)\ny = np.random.choice([\"male\", \"female\", \"other\"], size=n_samples, p=[0.49, 0.49, 0.02])\n```\n\n----------------------------------------\n\nTITLE: Running Estimator Checks in Python\nDESCRIPTION: Demonstrates how to use the check_estimator function to verify if an estimator adheres to scikit-learn's interface and standards.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/develop.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.utils.estimator_checks import check_estimator\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> check_estimator(DecisionTreeClassifier())  # passes\n[...]\n```\n\n----------------------------------------\n\nTITLE: Importing HuberRegressor in Python\nDESCRIPTION: Example of importing and using the HuberRegressor from scikit-learn. HuberRegressor is generally faster than RANSAC and Theil-Sen for large datasets.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.linear_model import HuberRegressor\nhuber = HuberRegressor()\nhuber.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Calculating Precision Score with Additional Labels in Python using scikit-learn\nDESCRIPTION: This snippet illustrates how to calculate the precision score for a multi-class classification problem while including labels not present in the data sample. It shows the use of the 'labels' parameter in the precision_score function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nmetrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')\n```\n\n----------------------------------------\n\nTITLE: Type Checking Binary Classification Example\nDESCRIPTION: Code example showing binary class labels used in scikit-learn classification, demonstrating that numerically or lexicographically greater values are considered positive classes\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/glossary.rst#2025-04-14_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n[0, 1]  # 1 is positive class\n[1, 2]  # 2 is positive class\n['no', 'yes']  # 'yes' is positive class\n['no', 'YES']  # 'no' is positive class\n```\n\n----------------------------------------\n\nTITLE: Fixing validation set sampling in MLPClassifier in Python\nDESCRIPTION: The neural_network.MLPClassifier class now correctly samples validation sets with stratification for early stopping. Multilabel cases are still not stratified.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\nneural_network.MLPClassifier\n```\n\n----------------------------------------\n\nTITLE: Clusterer API Requirements\nDESCRIPTION: Required method implementations for scikit-learn clustering estimators. Must implement fit and either fit_predict (if transductive) or predict (if inductive).\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/glossary.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef fit(X)\ndef fit_predict(X)  # if transductive\ndef predict(X)      # if inductive\n```\n\n----------------------------------------\n\nTITLE: Adding n_iter_no_change parameter to MLPClassifier in Python\nDESCRIPTION: Added n_iter_no_change parameter to MLPClassifier and MLPRegressor to control maximum epochs without improvement.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(n_iter_no_change=10)\nmlp.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Using Squared Error Loss in Scikit-learn Estimators\nDESCRIPTION: Updates to various estimators to use 'squared_error' instead of deprecated options for squared error loss.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nExtraTreesRegressor(criterion=\"squared_error\")\nGradientBoostingRegressor(loss=\"squared_error\")\nRandomForestRegressor(criterion=\"squared_error\")\nHistGradientBoostingRegressor(loss=\"squared_error\")\nRANSACRegressor(loss=\"squared_error\")\nSGDRegressor(loss=\"squared_error\")\nDecisionTreeRegressor(criterion=\"squared_error\")\nExtraTreeRegressor(criterion=\"squared_error\")\n```\n\n----------------------------------------\n\nTITLE: Calculating Minimum Dimensions with Johnson-Lindenstrauss\nDESCRIPTION: Demonstrates how to use johnson_lindenstrauss_min_dim function to estimate the minimal size of random subspace while maintaining bounded distortion.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/random_projection.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n>>> from sklearn.random_projection import johnson_lindenstrauss_min_dim\n>>> johnson_lindenstrauss_min_dim(n_samples=1e6, eps=0.5)\nnp.int64(663)\n>>> johnson_lindenstrauss_min_dim(n_samples=1e6, eps=[0.5, 0.1, 0.01])\narray([    663,   11841, 1112658])\n>>> johnson_lindenstrauss_min_dim(n_samples=[1e4, 1e5, 1e6], eps=0.1)\narray([ 7894,  9868, 11841])\n```\n\n----------------------------------------\n\nTITLE: Configuring Positive Coefficients in LinearRegression in Python\nDESCRIPTION: This snippet shows how to use the new 'positive' parameter in LinearRegression to force all coefficients to be positive.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.linear_model import LinearRegression\n\n# Force positive coefficients\nreg = LinearRegression(positive=True)\nreg.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Incorrectly Applying Preprocessing in scikit-learn\nDESCRIPTION: Demonstrates the incorrect approach of scaling only the training data but not the test data, resulting in poor model performance when predicting on unscaled test data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/common_pitfalls.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.metrics import mean_squared_error\n>>> from sklearn.linear_model import LinearRegression\n>>> from sklearn.preprocessing import StandardScaler\n\n>>> scaler = StandardScaler()\n>>> X_train_transformed = scaler.fit_transform(X_train)\n>>> model = LinearRegression().fit(X_train_transformed, y_train)\n>>> mean_squared_error(y_test, model.predict(X_test))\n62.80...\n```\n\n----------------------------------------\n\nTITLE: Referencing Class Weight Parameter in Python\nDESCRIPTION: This snippet shows how to reference the 'class_weight' parameter, which is a common estimator parameter in scikit-learn used for specifying sample weights as a function of class in classification tasks.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/glossary.rst#2025-04-14_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n:term:`class_weight`\n```\n\n----------------------------------------\n\nTITLE: Correct Feature Selection to Prevent Data Leakage in scikit-learn\nDESCRIPTION: Demonstrates the correct approach of splitting data first, then performing feature selection only on the training data to prevent data leakage.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/common_pitfalls.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=42)\n>>> select = SelectKBest(k=25)\n>>> X_train_selected = select.fit_transform(X_train, y_train)\n\n>>> gbc = HistGradientBoostingClassifier(random_state=1)\n>>> gbc.fit(X_train_selected, y_train)\nHistGradientBoostingClassifier(random_state=1)\n\n>>> X_test_selected = select.transform(X_test)\n>>> y_pred = gbc.predict(X_test_selected)\n>>> accuracy_score(y_test, y_pred)\n0.5\n```\n\n----------------------------------------\n\nTITLE: Using make_classification for Synthetic Data\nDESCRIPTION: Creates synthetic classification data with controlled cluster properties using scikit-learn's make_classification function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/minimal_reproducer.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1\n)\n```\n\n----------------------------------------\n\nTITLE: Importing IterativeImputer in scikit-learn\nDESCRIPTION: Shows how to import the experimental IterativeImputer class, which is used for imputing missing values by modeling each feature as a function of other features.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.impute import IterativeImputer\n```\n\n----------------------------------------\n\nTITLE: Using Label Ranking Loss in Python\nDESCRIPTION: Example showing how to use the label_ranking_loss function which measures incorrectly ordered label pairs. The function calculates the ratio of label pairs where a false label receives a higher score than a true label.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_42\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn.metrics import label_ranking_loss\n>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n>>> label_ranking_loss(y_true, y_score)\n0.75...\n>>> # With the following prediction, we have perfect and minimal loss\n>>> y_score = np.array([[1.0, 0.1, 0.2], [0.1, 0.2, 0.9]])\n>>> label_ranking_loss(y_true, y_score)\n0.0\n```\n\n----------------------------------------\n\nTITLE: Evaluating XOR Perceptron Model Predictions\nDESCRIPTION: Demonstrates the prediction accuracy of the trained Perceptron model on the XOR problem. Shows both the predicted values and the perfect classification score of 1.0.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n>>> clf.predict(X)\narray([0, 1, 1, 0])\n>>> clf.score(X, y)\n1.0\n```\n\n----------------------------------------\n\nTITLE: Adding out-of-bag scores to GradientBoostingClassifier\nDESCRIPTION: The ensemble.GradientBoostingClassifier now exposes out-of-bag scores via the oob_scores_ or oob_score_ attributes.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.3.rst#2025-04-14_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ngbc = GradientBoostingClassifier()\ngbc.fit(X, y)\nprint(gbc.oob_score_)\n```\n\n----------------------------------------\n\nTITLE: Setting Polars Output for Transformers in Python\nDESCRIPTION: New feature allowing transformers to output Polars dataframes by using set_output with transform=\"polars\".\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.4.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ntransformer.set_output(transform=\"polars\")\n```\n\n----------------------------------------\n\nTITLE: Dot-Product Kernel Formula\nDESCRIPTION: Mathematical expression for the Dot-Product kernel, a non-stationary kernel parameterized by σ₀². It can be homogeneous (σ₀²=0) or inhomogeneous.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/gaussian_process.rst#2025-04-14_snippet_7\n\nLANGUAGE: latex\nCODE:\n```\nk(x_i, x_j) = \\sigma_0 ^ 2 + x_i \\cdot x_j\n```\n\n----------------------------------------\n\nTITLE: Multilabel Classification Matrix Example\nDESCRIPTION: Shows how to create a dense binary matrix representing multilabel classification, where each row represents a sample and columns represent classes\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/multiclass.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ny = np.array([[1, 0, 0, 1], [0, 0, 1, 1], [0, 0, 0, 0]])\nprint(y)\n```\n\n----------------------------------------\n\nTITLE: Adding jaccard_score function in Python\nDESCRIPTION: The metrics.jaccard_score function has been added to calculate the Jaccard coefficient as an evaluation metric for binary, multilabel and multiclass tasks, with an interface similar to metrics.f1_score.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.jaccard_score\n```\n\n----------------------------------------\n\nTITLE: Checking Custom Estimator in Python\nDESCRIPTION: Shows how to use the check_estimator function to verify if a custom estimator passes all common checks.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/develop.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.utils.estimator_checks import check_estimator\n>>> check_estimator(TemplateClassifier())  # passes            # doctest: +SKIP\n```\n\n----------------------------------------\n\nTITLE: Loading Test Dataset with Fixed Number of Features\nDESCRIPTION: Demonstrates loading a test dataset in SVMLight format while ensuring it has the same number of features as a previously loaded training dataset by explicitly specifying the n_features parameter.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/datasets/loading_other_datasets.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nX_test, y_test = load_svmlight_file(\n    \"/path/to/test_dataset.txt\", n_features=X_train.shape[1])\n```\n\n----------------------------------------\n\nTITLE: Configuring Scikit-learn for Reduced Validation Overhead in Python\nDESCRIPTION: This snippet demonstrates how to use sklearn.config_context to set the assume_finite configuration, which reduces validation overhead by assuming input data is finite.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/computing/computational_performance.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport sklearn\nwith sklearn.config_context(assume_finite=True):\n    pass  # do learning/prediction here with reduced validation\n```\n\n----------------------------------------\n\nTITLE: One-line Prediction using scikit-learn Estimator in Python\nDESCRIPTION: Demonstrates a quick one-liner in an IPython session to fit a model and make predictions using method chaining.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/develop.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ny_predicted = SGDClassifier(alpha=10).fit(X_train, y_train).predict(X_test)\n```\n\n----------------------------------------\n\nTITLE: FunctionTransformer Lambda Example for Row-wise Imputation\nDESCRIPTION: Shows how to perform row-wise imputation using FunctionTransformer as a replacement for the deprecated axis parameter in SimpleImputer.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\nFunctionTransformer(lambda X: SimpleImputer().fit_transform(X.T).T)\n```\n\n----------------------------------------\n\nTITLE: Updating classification_report labels in Python\nDESCRIPTION: The metrics.classification_report function now uses the label 'accuracy' instead of 'micro-average' to avoid confusion. 'micro-average' is only shown for multi-label or multi-class with a subset of classes.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.classification_report\n```\n\n----------------------------------------\n\nTITLE: Handling scoring failures in cross-validation functions\nDESCRIPTION: Allow estimators to fail scoring and replace score with error_score in cross_val_score, cross_validate, GridSearchCV, and RandomizedSearchCV.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV\n\ncross_val_score(estimator, X, y, error_score='raise')\n```\n\n----------------------------------------\n\nTITLE: Importing Calibration Module in Scikit-Learn\nDESCRIPTION: This code snippet shows how to import the calibration module from scikit-learn, which is necessary for working with probability calibration functions and classes.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/calibration.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\n```\n\n----------------------------------------\n\nTITLE: Adding sample_weight to KernelDensity in Python\nDESCRIPTION: Added sample_weight parameter to the fit method of KernelDensity to enable weighting in kernel density estimation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.neighbors import KernelDensity\nkde = KernelDensity()\nkde.fit(X, sample_weight=weights)\n```\n\n----------------------------------------\n\nTITLE: Example of passthrough in FeatureUnion\nDESCRIPTION: Demonstrates the new passthrough feature in Pipeline FeatureUnion that passes features unchanged.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.1.rst#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"passthrough\"\n```\n\n----------------------------------------\n\nTITLE: Configuring FastICA Whitening in Python\nDESCRIPTION: Shows how to set the whitening solver for FastICA using the new whiten_solver parameter. Options are 'svd' (default) or 'eigh', with 'eigh' potentially being faster for num_features > num_samples.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ndecomposition.FastICA(whiten_solver=\"svd\")\n```\n\nLANGUAGE: Python\nCODE:\n```\ndecomposition.FastICA(whiten_solver=\"eigh\")\n```\n\n----------------------------------------\n\nTITLE: Loading LFW Dataset for Face Identification in Python\nDESCRIPTION: This snippet demonstrates how to load the Labeled Faces in the Wild dataset for face identification using scikit-learn's fetch_lfw_people function. It loads images with at least 70 faces per person, resized to 40% of original size.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/lfw.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import fetch_lfw_people\nlfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\nfor name in lfw_people.target_names:\n    print(name)\n```\n\n----------------------------------------\n\nTITLE: Building and Using the Analyzer\nDESCRIPTION: This snippet shows how to build and use the analyzer of the `CountVectorizer` to tokenize text. The analyzer is responsible for extracting words from the input text based on a predefined pattern, typically words of at least 2 letters in the default configuration.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> analyze = vectorizer.build_analyzer()\n>>> analyze(\"This is a text document to analyze.\") == (\n...     ['this', 'is', 'text', 'document', 'to', 'analyze'])\nTrue\n```\n\n----------------------------------------\n\nTITLE: Using Randomized SVD Solver in KernelPCA in Python\nDESCRIPTION: New approximate solver (randomized SVD) added to KernelPCA, significantly accelerating computation when the number of samples is much larger than the desired number of components.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ndecomposition.KernelPCA(eigen_solver='randomized')\n```\n\n----------------------------------------\n\nTITLE: Updating AdaBoostClassifier probability computation\nDESCRIPTION: Modifies AdaBoostClassifier to compute probabilities based on the decision function, as described in literature. This ensures consistency between predict and predict_proba methods.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nensemble.AdaBoostClassifier\n```\n\n----------------------------------------\n\nTITLE: Setting Memory Parameter in Pipeline\nDESCRIPTION: Shows how to enable caching of transformers in a scikit-learn Pipeline by using the memory parameter in the constructor.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.pipeline import Pipeline\nfrom tempfile import mkdtemp\nfrom shutil import rmtree\n\ncachedir = mkdtemp()\npipeline = Pipeline([...], memory=cachedir)\n\n# Clean up the cache directory when done\nrmtree(cachedir)\n```\n\n----------------------------------------\n\nTITLE: Checking Valid Metrics for Tree Structures\nDESCRIPTION: Shows how to retrieve valid distance metrics supported by KDTree and BallTree implementations.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neighbors.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.neighbors import KDTree, BallTree\nKDTree.valid_metrics\nBallTree.valid_metrics\n```\n\n----------------------------------------\n\nTITLE: Adding return_X_y Parameter to Dataset Loading Functions in Python\nDESCRIPTION: Several dataset loading functions now have a return_X_y parameter to optionally return data and target as separate arrays.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nX, y = load_dataset(return_X_y=True)\n```\n\n----------------------------------------\n\nTITLE: Using set_output API with transformers\nDESCRIPTION: Shows how to use the new set_output API to configure output of transformers and meta-estimators containing transformers.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\npipe = Pipeline([('scaler', StandardScaler())])\npipe.set_output(transform='pandas')\n```\n\n----------------------------------------\n\nTITLE: Multilabel Classification Sparse Matrix Example\nDESCRIPTION: Demonstrates creating a sparse matrix representation of multilabel classification, which can be more memory-efficient for large, sparse datasets\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/multiclass.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ny_sparse = sparse.csr_matrix(y)\nprint(y_sparse)\n```\n\n----------------------------------------\n\nTITLE: Creating LearningCurveDisplay in Python\nDESCRIPTION: Demonstrates using the new LearningCurveDisplay class to easily plot learning curves obtained from the learning_curve function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.model_selection import LearningCurveDisplay\n\nLearningCurveDisplay.from_estimator(estimator, X, y)\n```\n\n----------------------------------------\n\nTITLE: Accessing Bunch Object Values in Scikit-learn\nDESCRIPTION: Bunch objects in scikit-learn are used for outputting function results.  They extend dictionaries by allowing values to be accessed either by key using bracket notation or by attribute using dot notation. This snippet demonstrates both methods of accessing values within a Bunch object.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/faq.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n\"bunch[\\\"value_key\\\"]\", `bunch.value_key`\n```\n\n----------------------------------------\n\nTITLE: Calculating Coverage Error in Python using scikit-learn\nDESCRIPTION: This code snippet shows how to use the coverage_error function from scikit-learn to compute the average number of labels that need to be included in the final prediction to ensure all true labels are predicted in multilabel classification.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom sklearn.metrics import coverage_error\ny_true = np.array([[1, 0, 0], [0, 0, 1]])\ny_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\ncoverage_error(y_true, y_score)\n```\n\n----------------------------------------\n\nTITLE: Importing Theil-Sen estimator in Python\nDESCRIPTION: Example of importing and using the Theil-Sen estimator from scikit-learn. Theil-Sen uses a generalization of the median in multiple dimensions and is robust to multivariate outliers.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.linear_model import TheilSenRegressor\ntheil_sen = TheilSenRegressor()\ntheil_sen.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Improving efficiency of pairwise_distances in Python\nDESCRIPTION: The metrics.pairwise_distances function is now faster with n_jobs > 1 by using a thread-based backend instead of process-based backends.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.pairwise_distances\n```\n\n----------------------------------------\n\nTITLE: Adding verbose parameter to Pipeline and related classes in Python\nDESCRIPTION: The pipeline.Pipeline, compose.ColumnTransformer, and pipeline.FeatureUnion classes now have an optional verbose parameter for showing progress and timing of each step.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\npipeline.Pipeline\n```\n\nLANGUAGE: Python\nCODE:\n```\ncompose.ColumnTransformer\n```\n\nLANGUAGE: Python\nCODE:\n```\npipeline.FeatureUnion\n```\n\n----------------------------------------\n\nTITLE: Adding beta parameter to v-measure functions in Python\nDESCRIPTION: The metrics.homogeneity_completeness_v_measure and metrics.v_measure_score functions now have a beta parameter to configure the tradeoff between homogeneity and completeness.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.homogeneity_completeness_v_measure\n```\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.v_measure_score\n```\n\n----------------------------------------\n\nTITLE: Transforming corpus to n-grams\nDESCRIPTION: This snippet transforms the corpus using the bigram vectorizer and converts the resulting sparse matrix to a dense array. The resulting array reflects the frequencies of both unigrams and bigrams in each document.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n>>> X_2\narray([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)\n```\n\n----------------------------------------\n\nTITLE: Using HalvingGridSearchCV with exhaust Resource Strategy in scikit-learn\nDESCRIPTION: Example of using HalvingGridSearchCV with the 'exhaust' resource strategy, which automatically sets min_resources to maximize resource usage in the final iteration.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/grid_search.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\n...                          factor=2, min_resources='exhaust').fit(X, y)\n>>> sh.n_resources_\n[250, 500, 1000]\n```\n\n----------------------------------------\n\nTITLE: Adding sample_weight support to partial_dependence\nDESCRIPTION: The inspection.partial_dependence function and PartialDependenceDisplay.from_estimator method now support a sample_weight parameter for weighted averaging when aggregating values.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.3.rst#2025-04-14_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ninspection.partial_dependence(estimator, X, features, sample_weight=weights)\n```\n\n----------------------------------------\n\nTITLE: Deprecating mixture.GMM in favor of mixture.GaussianMixture in Python\nDESCRIPTION: The old mixture.GMM class is being deprecated and replaced by the new mixture.GaussianMixture class. The new implementation is faster and resolves some computational issues.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.18.rst#2025-04-14_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.mixture import GaussianMixture\n```\n\n----------------------------------------\n\nTITLE: Ridge Regression with LBFGS Solver Example\nDESCRIPTION: Example of using the new LBFGS solver in Ridge regression with positive coefficient constraints. This feature was added to allow forcing coefficients to be positive using the LBFGS optimizer.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.linear_model import Ridge\n\nridge = Ridge(solver=\"lbfgs\", positive=True)\nridge.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: SVR Prediction Formula\nDESCRIPTION: The mathematical formula for making predictions with a trained SVR model. It uses the difference of dual coefficients, the kernel function applied to support vectors and test points, and the bias term.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/svm.rst#2025-04-14_snippet_10\n\nLANGUAGE: math\nCODE:\n```\n\\sum_{i \\in SV}(\\alpha_i - \\alpha_i^*) K(x_i, x) + b\n```\n\n----------------------------------------\n\nTITLE: Adding 'passthrough' option as transformer in Pipeline in Python\nDESCRIPTION: The pipeline.Pipeline class now supports using 'passthrough' as a transformer, which has the same effect as None.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\npipeline.Pipeline\n```\n\n----------------------------------------\n\nTITLE: Using LabelEncoder for Numeric Label Normalization in Python\nDESCRIPTION: Demonstrates LabelEncoder usage for normalizing numeric labels to range [0, n_classes-1]. Shows both encoding and decoding of numeric labels.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing_targets.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn import preprocessing\n>>> le = preprocessing.LabelEncoder()\n>>> le.fit([1, 2, 2, 6])\nLabelEncoder()\n>>> le.classes_\narray([1, 2, 6])\n>>> le.transform([1, 1, 2, 6])\narray([0, 0, 1, 2])\n>>> le.inverse_transform([0, 0, 1, 2])\narray([1, 1, 2, 6])\n```\n\n----------------------------------------\n\nTITLE: Passing callable to max_features in SelectFromModel\nDESCRIPTION: Allows passing a callable to the max_features parameter of SelectFromModel. The max_features_ attribute is introduced to store the inferred value.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.1.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.feature_selection import SelectFromModel\n\ndef max_features_func(X):\n    return int(X.shape[1] / 2)\n\nsfm = SelectFromModel(max_features=max_features_func)\nsfm.fit(X)\nprint(sfm.max_features_)  # Prints the inferred value\n```\n\n----------------------------------------\n\nTITLE: Importing parallel utilities in scikit-learn\nDESCRIPTION: Shows how to import the new parallel utilities in scikit-learn 1.2.1 to replace the deprecated utils.fixes.delayed.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.utils.parallel import delayed, Parallel\n```\n\n----------------------------------------\n\nTITLE: Measuring memory usage with %memit\nDESCRIPTION: This example shows how to use the ``%memit`` magic command to measure the memory usage of a single line of code. In this case, it measures the memory used to create a NumPy array of zeros.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/performance.rst#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n\"In [1]: import numpy as np\\n\\n    In [2]: %memit np.zeros(1e7)\\n    maximum of 3: 76.402344 MB per loop\"\n```\n\n----------------------------------------\n\nTITLE: Profiling with yep\nDESCRIPTION: These commands demonstrate how to use yep to profile a Python script and then visualize the report using kcachegrind. First, yep is used to generate a profiling report for `my_file.py`, and then kcachegrind is used to open the generated report file.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/performance.rst#2025-04-14_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n\"# Run yep to profile some python script\\n  python -m yep -c my_file.py\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n\"# open my_file.py.callgrin with kcachegrind\\n  kcachegrind my_file.py.prof\"\n```\n\n----------------------------------------\n\nTITLE: Loading Multiple Datasets in SVMLight/LibSVM Format at Once\nDESCRIPTION: Shows how to load multiple SVMLight/LibSVM datasets simultaneously using the load_svmlight_files function. This approach ensures that the training and test datasets have the same number of features.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/datasets/loading_other_datasets.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nX_train, y_train, X_test, y_test = load_svmlight_files(\n    (\"/path/to/train_dataset.txt\", \"/path/to/test_dataset.txt\"))\n```\n\n----------------------------------------\n\nTITLE: Creating Classification Data for Feature Selection Example\nDESCRIPTION: Creates a synthetic binary classification dataset with 10,000 randomly generated features to demonstrate feature selection and data leakage issues.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/common_pitfalls.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> n_samples, n_features, n_classes = 200, 10000, 2\n>>> rng = np.random.RandomState(42)\n>>> X = rng.standard_normal((n_samples, n_features))\n>>> y = rng.choice(n_classes, n_samples)\n```\n\n----------------------------------------\n\nTITLE: Multilabel classification support in RidgeClassifier (Python)\nDESCRIPTION: Adds support for multilabel classification to the RidgeClassifier, allowing it to handle multiple target labels simultaneously.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.1.rst#2025-04-14_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.linear_model import RidgeClassifier\n\n# Now supports multilabel classification\nclassifier = RidgeClassifier()\nclassifier.fit(X, y_multilabel)\n```\n\n----------------------------------------\n\nTITLE: Mathematical Formula for QDA Covariance Matrix\nDESCRIPTION: Mathematical representation of how the SVD solver computes the covariance matrix for Quadratic Discriminant Analysis using matrix decomposition.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/lda_qda.rst#2025-04-14_snippet_5\n\nLANGUAGE: latex\nCODE:\n```\n\\Sigma_k = \\frac{1}{n - 1} X_k^tX_k = \\frac{1}{n - 1} V S^2 V^t\n```\n\n----------------------------------------\n\nTITLE: Optimizing Memory Usage in MLPClassifier and MLPRegressor in scikit-learn Neural Networks\nDESCRIPTION: Reduces memory footprint in MLPClassifier and MLPRegressor when using stochastic solvers ('sgd' or 'adam') with shuffle=True.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\nneural_network.MLPClassifier()\nneural_network.MLPRegressor()\n```\n\n----------------------------------------\n\nTITLE: Handling Sample Weight Error in GridSearchCV\nDESCRIPTION: Example demonstrating how an error is raised when sample_weight metadata routing is not properly configured in GridSearchCV with LogisticRegression.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/metadata_routing.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nparam_grid = {\"C\": [0.1, 1]}\nlr = LogisticRegression().set_fit_request(sample_weight=True)\ntry:\n    GridSearchCV(\n        estimator=lr, param_grid=param_grid\n    ).fit(X, y, sample_weight=my_weights)\nexcept ValueError as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Enhancing Error Reporting in GridSearchCV and RandomizedSearchCV in scikit-learn\nDESCRIPTION: Improves fit failed warning messages in GridSearchCV and RandomizedSearchCV by including stack trace information.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.GridSearchCV()\nmodel_selection.RandomizedSearchCV()\n```\n\n----------------------------------------\n\nTITLE: Adding sample_weight parameter to metrics.matthews_corrcoef in Python\nDESCRIPTION: Adds a sample_weight parameter to the matthews_corrcoef function in the metrics module.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.18.rst#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.matthews_corrcoef(y_true, y_pred, sample_weight=sample_weight)\n```\n\n----------------------------------------\n\nTITLE: Importing Halving Search CV Tools from Scikit-learn\nDESCRIPTION: This snippet demonstrates how to import the necessary classes for using halving search cross-validation in Scikit-learn. Users must explicitly enable these experimental features before use.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/grid_search.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.experimental import enable_halving_search_cv  # noqa\n>>> from sklearn.model_selection import HalvingGridSearchCV\n>>> from sklearn.model_selection import HalvingRandomSearchCV\n```\n\n----------------------------------------\n\nTITLE: KFold and cross_val_score: Impact of RandomState on estimator comparison\nDESCRIPTION: This snippet demonstrates how using a `RandomState` instance in a `KFold` splitter can affect the comparison of different estimators. When `split` is called multiple times with a `RandomState` instance, it can lead to overestimating the variance of the difference in performance between the estimators.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/common_pitfalls.rst#2025-04-14_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.model_selection import KFold\n>>> from sklearn.model_selection import cross_val_score\n>>> import numpy as np\n\n>>> rng = np.random.RandomState(0)\n>>> X, y = make_classification(random_state=rng)\n>>> cv = KFold(shuffle=True, random_state=rng)\n>>> lda = LinearDiscriminantAnalysis()\n>>> nb = GaussianNB()\n\n>>> for est in (lda, nb):\n...     print(cross_val_score(est, X, y, cv=cv))\n[0.8  0.75 0.75 0.7  0.85]\n[0.85 0.95 0.95 0.85 0.95]\n```\n\n----------------------------------------\n\nTITLE: Using VotingClassifier as Base Estimator in CalibratedClassifierCV\nDESCRIPTION: Fix to allow using VotingClassifier as base_estimator in CalibratedClassifierCV.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import VotingClassifier\n\nclf = CalibratedClassifierCV(base_estimator=VotingClassifier(...))\n```\n\n----------------------------------------\n\nTITLE: Deprecating data_transposed argument in datasets.make_sparse_coded_signal\nDESCRIPTION: The data_transposed argument of datasets.make_sparse_coded_signal is being deprecated and will be removed in v1.5.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.3.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndatasets.make_sparse_coded_signal(data_transposed=True)  # deprecated\n```\n\n----------------------------------------\n\nTITLE: Loading a Dataset in SVMLight/LibSVM Format in Python\nDESCRIPTION: Demonstrates how to load a dataset in SVMLight/LibSVM format using scikit-learn's load_svmlight_file function. This format is especially suitable for sparse datasets, where scikit-learn uses scipy sparse CSR matrices for feature data and numpy arrays for labels.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/datasets/loading_other_datasets.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import load_svmlight_file\nX_train, y_train = load_svmlight_file(\"/path/to/train_dataset.txt\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Cross-Validation Results in Python\nDESCRIPTION: Shows how to access training scores and timing information from cross-validation results in scikit-learn. The cv_results_ dictionary now includes training scores and timing stats for each CV split.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.18.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ncv_results_['split0_train_score'] # Training score for first CV split\ncv_results_['mean_train_score'] # Mean training score across CV splits\ncv_results_['std_train_score'] # Standard deviation of training scores\ncv_results_['mean_time'] # Mean time taken for splitting, training and scoring\ncv_results_['std_time'] # Standard deviation of timing\n```\n\n----------------------------------------\n\nTITLE: Implementing Metadata Routing in Scikit-learn Estimators\nDESCRIPTION: Various scikit-learn estimators and functions now support metadata routing, allowing better control over parameter passing and data handling. This includes updates to cross-validation functions, pipelines, and model selection tools.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.4.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.model_selection import cross_validate, cross_val_score, cross_val_predict\n\n# New usage with metadata routing\nresults = cross_validate(estimator, X, y, params={'fit': {'sample_weight': sample_weight}})\n\n# Deprecated usage\n# results = cross_validate(estimator, X, y, fit_params={'sample_weight': sample_weight})\n```\n\n----------------------------------------\n\nTITLE: Example code usage of Matthews correlation coefficient scoring\nDESCRIPTION: Shows how to use the new Matthews correlation coefficient scoring metric in model selection tools.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.1.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nscoring=\"matthews_corrcoef\"\n```\n\n----------------------------------------\n\nTITLE: Setting tolerance in Ridge regression\nDESCRIPTION: Demonstrates the new default tolerance value of 1e-4 for Ridge regression and related estimators.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.linear_model import Ridge, RidgeClassifier\n\nRidge(tol=1e-4)\nRidgeClassifier(tol=1e-4)\n```\n\n----------------------------------------\n\nTITLE: Adding multilabel_confusion_matrix function in Python\nDESCRIPTION: The metrics.multilabel_confusion_matrix function calculates a confusion matrix with true positive, false positive, false negative and true negative counts for each class, facilitating set-wise metric calculations.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.multilabel_confusion_matrix\n```\n\n----------------------------------------\n\nTITLE: Using force_finite parameter in f_regression\nDESCRIPTION: Adds a force_finite parameter to f_regression to ensure finite output when features or target are constant or perfectly correlated.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.1.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.feature_selection import f_regression\n\nF, _ = f_regression(X, y, force_finite=True)\n```\n\n----------------------------------------\n\nTITLE: Optimizing OneHotEncoder Transformation in scikit-learn Preprocessing\nDESCRIPTION: Improves the speed of transformation in OneHotEncoder.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\npreprocessing.OneHotEncoder()\n```\n\n----------------------------------------\n\nTITLE: Adding haversine_distances function in Python\nDESCRIPTION: The metrics.pairwise.haversine_distances function has been added and can be accessed with metric='pairwise' through metrics.pairwise_distances and estimators.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.pairwise.haversine_distances\n```\n\n----------------------------------------\n\nTITLE: Enhancing OneHotEncoder with New Drop Option in scikit-learn Preprocessing\nDESCRIPTION: Adds 'if_binary' option to the drop argument in OneHotEncoder to drop the first category of each feature with two categories.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\npreprocessing.OneHotEncoder(drop='if_binary')\n```\n\n----------------------------------------\n\nTITLE: Mutual Information Score Example in scikit-learn in Python\nDESCRIPTION: This snippet demonstrates how to calculate the Mutual Information (MI) score using `sklearn.metrics.mutual_info_score`. The text notes that interpreting the raw MI score is more difficult than the adjusted or normalized versions.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n>>> metrics.mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP\n0.69...\n```\n\n----------------------------------------\n\nTITLE: Using CalibratedClassifierCV with Prefitted Pipelines\nDESCRIPTION: Fix to allow predict and predict_proba methods of CalibratedClassifierCV to work with prefitted pipelines.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.calibration import CalibratedClassifierCV\n\nclf = CalibratedClassifierCV(base_estimator=prefitted_pipeline)\nclf.predict(X)\nclf.predict_proba(X)\n```\n\n----------------------------------------\n\nTITLE: Calculating Elastic Net Regularization in scikit-learn\nDESCRIPTION: Mathematical formula for Elastic Net regularization which combines L1 and L2 penalties, where ρ is determined by 1 - l1_ratio parameter.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/sgd.rst#2025-04-14_snippet_8\n\nLANGUAGE: math\nCODE:\n```\nR(w) := \\frac{\\rho}{2} \\sum_{j=1}^{n} w_j^2 + (1-\\rho) \\sum_{j=1}^{m} |w_j|\n```\n\n----------------------------------------\n\nTITLE: Generating Regression Data with NumPy\nDESCRIPTION: Creates synthetic numeric data for regression testing using NumPy's random number generators. Generates both feature matrix X and target vector y with continuous values.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/minimal_reproducer.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nrng = np.random.RandomState(0)\nn_samples, n_features = 5, 5\nX = rng.randn(n_samples, n_features)\ny = rng.randn(n_samples)\n```\n\n----------------------------------------\n\nTITLE: Computing Pair Confusion Matrix with Split Clusters\nDESCRIPTION: Example showing pair_confusion_matrix for completely split clusters where each point is assigned to a different cluster.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics.cluster import pair_confusion_matrix\npair_confusion_matrix([0, 0, 0, 0], [0, 1, 2, 3])\n```\n\n----------------------------------------\n\nTITLE: Enhancing verbose output for cross-validation functions in Python\nDESCRIPTION: Various cross-validation functions and classes now print train scores when return_train_scores is True and verbose > 2.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.GridSearchCV\n```\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.RandomizedSearchCV\n```\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.cross_val_score\n```\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.cross_val_predict\n```\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.cross_validate\n```\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.learning_curve\n```\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.validation_curve\n```\n\n----------------------------------------\n\nTITLE: Configuring Decimal Places in tree.export_graphviz\nDESCRIPTION: Shows how to set the number of decimal places displayed when exporting a decision tree to GraphViz format.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn import tree\n\ntree.export_graphviz(decision_tree, out_file=\"tree.dot\", precision=3)\n```\n\n----------------------------------------\n\nTITLE: Validating Ensemble Voting Estimators in Scikit-learn\nDESCRIPTION: Ensures that the estimators parameter is a properly structured list of tuples for VotingClassifier and VotingRegressor. Validates input configuration to maintain consistency and prevent runtime errors.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/sklearn.ensemble/30649.fix.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# Ensemble estimator validation by Thomas Fan\n# Ensures estimators are list of tuples\n```\n\n----------------------------------------\n\nTITLE: Multiple Metric Evaluation with Callable Returning Dictionary in Scikit-learn\nDESCRIPTION: This example demonstrates a third way to specify multiple scoring metrics for the scoring parameter in Scikit-learn, by providing a callable function that returns a dictionary of scores. This allows calculation of multiple metrics within a single function, such as extracting values from a confusion matrix.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.model_selection import cross_validate\n>>> from sklearn.metrics import confusion_matrix\n>>> # A sample toy binary classification dataset\n>>> X, y = datasets.make_classification(n_classes=2, random_state=0)\n>>> svm = LinearSVC(random_state=0)\n>>> def confusion_matrix_scorer(clf, X, y):\n...      y_pred = clf.predict(X)\n...      cm = confusion_matrix(y, y_pred)\n...      return {'tn': cm[0, 0], 'fp': cm[0, 1],\n...              'fn': cm[1, 0], 'tp': cm[1, 1]}\n>>> cv_results = cross_validate(svm, X, y, cv=5,\n...                             scoring=confusion_matrix_scorer)\n>>> # Getting the test set true positive scores\n>>> print(cv_results['test_tp'])\n[10  9  8  7  8]\n>>> # Getting the test set false negative scores\n>>> print(cv_results['test_fn'])\n[0 1 2 3 2]\n```\n\n----------------------------------------\n\nTITLE: Fixing label_ranking_average_precision_score for sample weights in Python\nDESCRIPTION: The metrics.label_ranking_average_precision_score function now correctly accounts for sample_weight for samples with degenerate labels.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.label_ranking_average_precision_score\n```\n\n----------------------------------------\n\nTITLE: Loading RCV1 Dataset with scikit-learn in Python\nDESCRIPTION: This snippet demonstrates how to load the RCV1 dataset using scikit-learn's fetch_rcv1 function. It returns a dictionary-like object containing the dataset.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/rcv1.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import fetch_rcv1\n>>> rcv1 = fetch_rcv1()\n```\n\n----------------------------------------\n\nTITLE: Configuring NaN/Inf Validation in Scikit-learn\nDESCRIPTION: Using config_context to suppress validation that input data contains no NaN or inf values, which can improve runtime performance, especially for prediction. This should be used with caution.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn import config_context\n\nwith config_context(assume_finite=True):\n    # Code here will skip NaN/inf checks\n```\n\n----------------------------------------\n\nTITLE: Log-posterior formula for QDA\nDESCRIPTION: This formula shows the log of the posterior probability for Quadratic Discriminant Analysis.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/lda_qda.rst#2025-04-14_snippet_2\n\nLANGUAGE: math\nCODE:\n```\n\\log P(y=k | x) = -\\frac{1}{2} \\log |\\Sigma_k| -\\frac{1}{2} (x-\\mu_k)^t \\Sigma_k^{-1} (x-\\mu_k) + \\log P(y = k) + Cst\n```\n\n----------------------------------------\n\nTITLE: Using make_blobs for Clustering Data\nDESCRIPTION: Generates synthetic clustering data with specified number of samples, centers, and features using make_blobs.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/minimal_reproducer.rst#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=10, centers=3, n_features=2)\n```\n\n----------------------------------------\n\nTITLE: Setting max_features parameter in RandomForestClassifier\nDESCRIPTION: Changes the default value of max_features to 'sqrt' for RandomForestClassifier. The old default 'auto' is deprecated and will be removed in version 1.3.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.1.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_features='sqrt')  # New default\n```\n\n----------------------------------------\n\nTITLE: Calculating Average Weights in Averaged SGD\nDESCRIPTION: Formula for computing the average weights across all updates when using Averaged SGD, where T is the total number of updates stored in t_ attribute.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/sgd.rst#2025-04-14_snippet_12\n\nLANGUAGE: math\nCODE:\n```\ncoef_` = \\frac{1}{T} \\sum_{t=0}^{T-1} w^{(t)}\n```\n\n----------------------------------------\n\nTITLE: LinearSVR Primal Problem Formulation\nDESCRIPTION: Alternative formulation of the SVR primal problem specifically for LinearSVR, which directly optimizes the epsilon-insensitive loss. This formulation ignores errors smaller than epsilon and is more computationally efficient for linear kernels.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/svm.rst#2025-04-14_snippet_11\n\nLANGUAGE: math\nCODE:\n```\n\\min_ {w, b} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n}\\max(0, |y_i - (w^T \\phi(x_i) + b)| - \\varepsilon),\n```\n\n----------------------------------------\n\nTITLE: Passing Predict Parameters in TransformedTargetRegressor in Python\nDESCRIPTION: Enhancement to TransformedTargetRegressor.predict method to accept **predict_params keyword argument that passes keyword arguments to the regressor.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ncompose.TransformedTargetRegressor.predict(**predict_params)\n```\n\n----------------------------------------\n\nTITLE: Installing scikit-learn nightly build using pip\nDESCRIPTION: Command to install the pre-release nightly build of scikit-learn using the scientific-python-nightly-wheels index from PyPI on anaconda.org.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --pre --extra-index https://pypi.anaconda.org/scientific-python-nightly-wheels/simple scikit-learn\n```\n\n----------------------------------------\n\nTITLE: Deprecating n_iter in favor of max_iter for BayesianRidge and ARDRegression\nDESCRIPTION: The n_iter parameter is deprecated in favor of max_iter in linear_model.BayesianRidge and linear_model.ARDRegression. n_iter will be removed in scikit-learn 1.5.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.3.rst#2025-04-14_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nlinear_model.BayesianRidge(max_iter=300)  # new\nlinear_model.BayesianRidge(n_iter=300)  # old, deprecated\n```\n\n----------------------------------------\n\nTITLE: Setting idf_ in TfidfTransformer (Python)\nDESCRIPTION: Allows setting the idf_ attribute on an already initialized TfidfTransformer. This provides more flexibility in customizing the IDF values used for transformation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nidf_\n```\n\n----------------------------------------\n\nTITLE: Custom Tags subclass example\nDESCRIPTION: This snippet demonstrates how to create a new subclass of `Tags` to add custom tags to the existing set. The example showcases how to inherit from the `Tags` class and implement the `__sklearn_tags__()` method within a custom estimator class.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/develop.rst#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass, asdict\n\n@dataclass\nclass MyTags(Tags):\n    my_tag: bool = True\n\nclass MyEstimator(BaseEstimator):\n    def __sklearn_tags__(self):\n        tags_orig = super().__sklearn_tags__()\n        as_dict = {\n            field.name: getattr(tags_orig, field.name)\n            for field in fields(tags_orig)\n        }\n        tags = MyTags(**as_dict)\n        tags.my_tag = True\n        return tags\n```\n\n----------------------------------------\n\nTITLE: Enhancing cross_val_predict for multi-label targets in Python\nDESCRIPTION: The model_selection.cross_val_predict function can now handle multi-label and multioutput-multiclass targets with predict_proba-type methods.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.cross_val_predict\n```\n\n----------------------------------------\n\nTITLE: Matrix Decomposition Formula\nDESCRIPTION: Matrix form of the Factor Analysis model showing how the complete data matrix X is decomposed into components including the latent variables, weights, and noise terms.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/decomposition.rst#2025-04-14_snippet_5\n\nLANGUAGE: math\nCODE:\n```\n\\mathbf{X} = W \\mathbf{H} + \\mathbf{M} + \\mathbf{E}\n```\n\n----------------------------------------\n\nTITLE: Fix KNNImputer Memory Usage Example\nDESCRIPTION: Code snippet showing memory efficiency improvement in KNNImputer through chunked pairwise distance computation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n|Efficiency| Reduce :func:`impute.KNNImputer` asymptotic memory usage by\nchunking pairwise distance computation.\n```\n\n----------------------------------------\n\nTITLE: Fixing RidgeCV with leave-one-out cross-validation in Python\nDESCRIPTION: The linear_model.RidgeCV class now correctly fits an intercept when fit_intercept=True and the design matrix is sparse when using leave-one-out cross-validation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nlinear_model.RidgeCV\n```\n\n----------------------------------------\n\nTITLE: Using Updated LinearDiscriminantAnalysis in Python\nDESCRIPTION: Example of using the updated LinearDiscriminantAnalysis class from the discriminant_analysis module, with parameters moved from fit method to constructor.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.17.rst#2025-04-14_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis(store_covariance=True, tol=1e-4)\nlda.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Adding labels flag to metrics.log_loss in Python\nDESCRIPTION: Adds a 'labels' flag to metrics.log_loss to explicitly provide labels when the number of classes in y_true and y_pred differ.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.18.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.log_loss(y_true, y_pred, labels=labels)\n```\n\n----------------------------------------\n\nTITLE: Importing Experimental Histogram Gradient Boosting Implementation in scikit-learn\nDESCRIPTION: Code demonstrating how to import the experimental HistGradientBoostingClassifier estimator from scikit-learn. This was required prior to version 1.0 when these estimators were still experimental.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> # explicitly require this experimental feature\n>>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n>>> # now you can import normally from sklearn.ensemble\n>>> from sklearn.ensemble import HistGradientBoostingClassifier\n```\n\n----------------------------------------\n\nTITLE: Using max_iter and tol in SGD-based Estimators\nDESCRIPTION: Demonstrates the use of max_iter and tol parameters in SGD-based estimators to control convergence more precisely. The n_iter_ attribute shows the actual number of iterations.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier(max_iter=1000, tol=1e-3)\nsgd.fit(X, y)\nprint(f\"Actual iterations: {sgd.n_iter_}\")\n```\n\n----------------------------------------\n\nTITLE: Renaming classes parameter to labels in metrics.hamming_loss in Python\nDESCRIPTION: The classes parameter in the metrics.hamming_loss function has been renamed to labels for consistency.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.18.rst#2025-04-14_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.metrics import hamming_loss\n\nhamming_loss(y_true, y_pred, labels=None)\n```\n\n----------------------------------------\n\nTITLE: Updating error handling in NearestNeighbors methods in Python\nDESCRIPTION: Various methods in the neighbors.NearestNeighbors class now raise NotFittedError instead of AttributeError when called before fit.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\nneighbors.NearestNeighbors.kneighbors\n```\n\nLANGUAGE: Python\nCODE:\n```\nneighbors.NearestNeighbors.radius_neighbors\n```\n\nLANGUAGE: Python\nCODE:\n```\nneighbors.NearestNeighbors.kneighbors_graph\n```\n\nLANGUAGE: Python\nCODE:\n```\nneighbors.NearestNeighbors.radius_neighbors_graph\n```\n\n----------------------------------------\n\nTITLE: Filtering text data in the 20 newsgroups dataset for more realistic training\nDESCRIPTION: This snippet demonstrates how to filter out metadata from the 20 newsgroups dataset using the 'remove' parameter in the fetch_20newsgroups function. It shows the impact on classifier performance when removing headers, footers, and quotes.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/twenty_newsgroups.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      remove=('headers', 'footers', 'quotes'),\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n0.77310...\n\n>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       remove=('headers', 'footers', 'quotes'),\n...                                       categories=categories)\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n0.76995...\n```\n\n----------------------------------------\n\nTITLE: Setting Warning to Error for FunctionTransformer Inverse Check\nDESCRIPTION: This snippet demonstrates how to configure Python warnings to raise an error when the check_inverse functionality of FunctionTransformer detects issues with the inverse function relationship.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n>>> import warnings\n>>> warnings.filterwarnings(\"error\", message=\".*check_inverse*.\",\n...                         category=UserWarning, append=False)\n```\n\n----------------------------------------\n\nTITLE: Symmetry of Rand Index in scikit-learn in Python\nDESCRIPTION: This snippet demonstrates that the Rand Index is symmetric, meaning that swapping the `labels_true` and `labels_pred` arguments does not change the calculated score. This property makes it suitable for use as a consensus measure.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> metrics.rand_score(labels_pred, labels_true)\n0.66...\n>>> metrics.adjusted_rand_score(labels_pred, labels_true)\n0.24...\n```\n\n----------------------------------------\n\nTITLE: Importing GaussianProcessClassifier in Python\nDESCRIPTION: This snippet demonstrates how to import the GaussianProcessClassifier class from scikit-learn's gaussian_process module. This class is used for implementing Gaussian Process Classification.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/gaussian_process.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.gaussian_process import GaussianProcessClassifier\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating a Conda Environment for scikit-learn\nDESCRIPTION: Commands to create a dedicated conda environment for scikit-learn from the conda-forge channel and activate it. This isolates the scikit-learn installation from other Python environments.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/install_instructions_conda.rst#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n sklearn-env -c conda-forge scikit-learn\nconda activate sklearn-env\n```\n\n----------------------------------------\n\nTITLE: Random Sample Selection in Scikit-learn\nDESCRIPTION: This code snippet demonstrates how to choose a random sample from an array using `sklearn.utils.check_array` for input validation and `sklearn.utils.check_random_state` for managing random number generation, ensuring reproducibility by accepting a `random_state` parameter.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/develop.rst#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n\"from sklearn.utils import check_array, check_random_state\n\n    def choose_random_sample(X, random_state=0):\n        \\\"\\\"\\\"Choose a random point from X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            An array representing the data.\n        random_state : int or RandomState instance, default=0\n            The seed of the pseudo random number generator that selects a\n            random sample. Pass an int for reproducible output across multiple\n            function calls.\n            See :term:`Glossary <random_state>`.\n\n        Returns\n        -------\n        x : ndarray of shape (n_features,)\n            A random point selected from X.\n        \\\"\\\"\\\"\n        X = check_array(X)\n        random_state = check_random_state(random_state)\n        i = random_state.randint(X.shape[0])\n        return X[i]\"\n```\n\n----------------------------------------\n\nTITLE: Passing Additional Parameters to RFE Estimator in Python\nDESCRIPTION: Enhancement to RFE.fit method to accept additional estimator parameters that are passed directly to the estimator's fit method.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nfeature_selection.RFE.fit(X, y, **estimator_params)\n```\n\n----------------------------------------\n\nTITLE: Creating a feature branch for scikit-learn development in Bash\nDESCRIPTION: Git command to create and switch to a new feature branch for making changes to scikit-learn.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b my_feature\n```\n\n----------------------------------------\n\nTITLE: Enhancing confusion_matrix Handling in scikit-learn Metrics\nDESCRIPTION: Improves error handling in confusion_matrix for edge cases with empty inputs and labels.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.confusion_matrix()\n```\n\n----------------------------------------\n\nTITLE: Configuring Metadata Routing in scikit-learn\nDESCRIPTION: Demonstrates how to enable the experimental metadata routing feature using sklearn.set_config and sklearn.config_context. This feature is hidden behind a feature flag for developers to prepare their code.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.3.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn import set_config, config_context\n\n# Enable metadata routing\nset_config(enable_metadata_routing=True)\n\n# Or use as a context manager\nwith config_context(enable_metadata_routing=True):\n    # Code using metadata routing\n```\n\n----------------------------------------\n\nTITLE: Sorting sparse graph by row values in Python\nDESCRIPTION: Demonstrates using the new sort_graph_by_row_values function to sort a CSR sparse graph for improved efficiency when using precomputed sparse distance matrices.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.neighbors import sort_graph_by_row_values\n\nsorted_graph = sort_graph_by_row_values(sparse_graph)\n```\n\n----------------------------------------\n\nTITLE: Minimal GradientBoostingRegressor Example with Synthetic Data\nDESCRIPTION: This minimal example uses synthetic data to demonstrate the GradientBoostingRegressor warning. It creates a small pandas DataFrame with labeled features and shows the warning occurrence with n_iter_no_change parameter.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/minimal_reproducer.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ndf = pd.DataFrame(\n    {\n        \"feature_name\": [-12.32, 1.43, 30.01, 22.17],\n        \"target\": [72, 55, 32, 43],\n    }\n)\nX = df[[\"feature_name\"]]\ny = df[\"target\"]\n\ngbdt = GradientBoostingRegressor()\ngbdt.fit(X, y) # no warning\ngbdt = GradientBoostingRegressor(n_iter_no_change=5)\ngbdt.fit(X, y) # raises warning\n```\n\n----------------------------------------\n\nTITLE: Fixing Negative Score Issue in mutual_info_score in scikit-learn Metrics\nDESCRIPTION: Resolves an issue where mutual_info_score could return negative scores.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.mutual_info_score()\n```\n\n----------------------------------------\n\nTITLE: Using RadiusNeighborsTransformer in scikit-learn neighbors\nDESCRIPTION: New transformer added to convert input dataset into a sparse neighbors graph based on a fixed radius, complementing KNeighborsTransformer.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nneighbors.RadiusNeighborsTransformer\n```\n\n----------------------------------------\n\nTITLE: Adding var_smoothing parameter to GaussianNB in Python\nDESCRIPTION: Added var_smoothing parameter to GaussianNB to give precise control over variances calculation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB(var_smoothing=1e-9)\ngnb.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Fixing Squared Argument Handling in mean_squared_error in scikit-learn Metrics\nDESCRIPTION: Corrects the handling of the squared argument in mean_squared_error when multioutput='raw_values'.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.mean_squared_error()\n```\n\n----------------------------------------\n\nTITLE: Plotting a Sample Image in Python using scikit-learn and matplotlib\nDESCRIPTION: Loads and displays a sample image ('china.jpg') using scikit-learn's load_sample_image function and matplotlib for visualization. The code demonstrates how to load an embedded sample image and display it with axis turned off.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/datasets/loading_other_datasets.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_sample_image\n\nchina = load_sample_image(\"china.jpg\")\nplt.imshow(china)\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Model Selection Usage Example\nDESCRIPTION: Example showing usage of the new model_selection module for cross-validation splitting with groups parameter instead of the older labels parameter.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.18.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.GroupKFold().split(X, y, groups=groups)\nmodel_selection.LeaveOneGroupOut().split(X, y, groups=groups)\nmodel_selection.LeavePGroupsOut().split(X, y, groups=groups)\nmodel_selection.GroupShuffleSplit().split(X, y, groups=groups)\n```\n\n----------------------------------------\n\nTITLE: Using warm_start Parameter in scikit-learn\nDESCRIPTION: Documentation for the warm_start parameter which enables reusing the fitted model from previous calls to fit. This is particularly useful for ensemble models where you can incrementally add estimators or for iterative estimators to continue from previous state.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/glossary.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Example usage of warm_start parameter:\n\n# Ensemble models (adding more estimators)\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initial fit with 10 estimators\nrf = RandomForestClassifier(n_estimators=10, warm_start=True)\nrf.fit(X_train, y_train)\nprint(len(rf.estimators_))  # 10\n\n# Incremental fit with 5 more estimators (total becomes 15)\nrf.n_estimators = 15\nrf.fit(X_train, y_train)\nprint(len(rf.estimators_))  # 15\n\n# Gradient-based models behavior is different\nfrom sklearn.linear_model import SGDClassifier\n\n# Training for max_iter=5 iterations\nsgd = SGDClassifier(max_iter=5, warm_start=True)\nsgd.fit(X_train, y_train)\nprint(sgd.n_iter_)  # ≤ 5 (depending on convergence)\n\n# Continue training for max_iter=10 more iterations\nsgd.max_iter = 10\nsgd.fit(X_train, y_train)\nprint(sgd.n_iter_)  # ≤ 10 (not cumulative)\n```\n\n----------------------------------------\n\nTITLE: Adding fit_predict method to GaussianMixture in Python\nDESCRIPTION: Added function fit_predict to mixture.GaussianMixture and mixture.GaussianMixture classes, which is equivalent to calling fit and predict sequentially.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n# New fit_predict method added to GaussianMixture\ngmm.fit_predict(X)\n```\n\n----------------------------------------\n\nTITLE: Loading models securely with skops.io\nDESCRIPTION: This code demonstrates how to load a model saved with skops.io, checking for untrusted types before loading. It shows the security workflow of inspecting unknown types and explicitly trusting them before loading the model.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/model_persistence.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nunknown_types = sio.get_untrusted_types(file=\"filename.skops\")\n# investigate the contents of unknown_types, and only load if you trust\n# everything you see.\nclf = sio.load(\"filename.skops\", trusted=unknown_types)\n```\n\n----------------------------------------\n\nTITLE: Plotting Calibration Curves in Scikit-learn\nDESCRIPTION: Addition of CalibrationDisplay function to plot calibration curves in sklearn.calibration module.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.calibration import CalibrationDisplay\n```\n\n----------------------------------------\n\nTITLE: Adding return_estimator parameter to cross_validate in Python\nDESCRIPTION: Added return_estimator parameter to model_selection.cross_validate to return estimators fitted on each split.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nresults = cross_validate(estimator, X, y, return_estimator=True)\n```\n\n----------------------------------------\n\nTITLE: Creating Synthetic Data for Preprocessing Example in Python\nDESCRIPTION: Creates a synthetic regression dataset with a single feature and splits it into training and test sets using make_regression and train_test_split.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/common_pitfalls.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.model_selection import train_test_split\n\n>>> random_state = 42\n>>> X, y = make_regression(random_state=random_state, n_features=1, noise=1)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.4, random_state=random_state)\n```\n\n----------------------------------------\n\nTITLE: Improving Stability and Efficiency of ARDRegression in scikit-learn\nDESCRIPTION: Enhances the stability and speed of ARDRegression, especially for cases where n_samples > n_features, allowing it to scale to hundreds of thousands of samples.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nlinear_model.ARDRegression()\n```\n\n----------------------------------------\n\nTITLE: Setting Random State in IterativeImputer in Python\nDESCRIPTION: This snippet describes a fix for IterativeImputer that allows it to work with more external classes by not attempting to set the estimator's random_state attribute.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.impute import IterativeImputer\n\n# IterativeImputer will no longer attempt to set estimator's random_state\nimputer = IterativeImputer()\n# Usage with external estimators is now more flexible\n```\n\n----------------------------------------\n\nTITLE: Creating conda environment from yaml file\nDESCRIPTION: Alternative command to create a conda environment when lock file compatibility is not available, using an environment yaml specification file.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/tips.rst#2025-04-14_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nconda env create -n scikit-learn-doc -f build_tools/circle/doc_environment.yml -y\n```\n\n----------------------------------------\n\nTITLE: Referencing DBSCAN Cluster Parameter in Python\nDESCRIPTION: This snippet demonstrates how to reference the 'min_samples' parameter in the DBSCAN clustering algorithm from scikit-learn. It shows the proper way to refer to a specific class and parameter in documentation or code comments.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/glossary.rst#2025-04-14_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n:class:`cluster.DBSCAN`\n```\n\n----------------------------------------\n\nTITLE: Adding RegressorChain for multi-target regression in Python\nDESCRIPTION: Added RegressorChain class to multioutput module for multi-target regression.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.multioutput import RegressorChain\nchain = RegressorChain(base_estimator=RandomForestRegressor())\nchain.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Adding NeighborhoodComponentsAnalysis for metric learning in Python\nDESCRIPTION: The neighbors.NeighborhoodComponentsAnalysis class has been added to implement the Neighborhood Components Analysis algorithm for metric learning.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\nneighbors.NeighborhoodComponentsAnalysis\n```\n\n----------------------------------------\n\nTITLE: Adding return_X_y parameter to datasets loading functions in Python\nDESCRIPTION: Adds a return_X_y parameter and return type option to several dataset loading functions to return data and target as a tuple.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.18.rst#2025-04-14_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nX, y = datasets.load_iris(return_X_y=True)\n```\n\n----------------------------------------\n\nTITLE: Calculating Input Data Sparsity Ratio in Python\nDESCRIPTION: This function calculates the sparsity ratio of input data X, which can be used to determine if sparse matrix formats would be beneficial for performance.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/computing/computational_performance.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef sparsity_ratio(X):\n    return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])\nprint(\"input sparsity ratio:\", sparsity_ratio(X))\n```\n\n----------------------------------------\n\nTITLE: Fixing FeatureHasher error handling in Python\nDESCRIPTION: FeatureHasher now raises an informative error when the input is a list of strings.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfeature_extraction.FeatureHasher\n```\n\n----------------------------------------\n\nTITLE: Adding predict_proba Support with None Target in cross_val_predict in scikit-learn\nDESCRIPTION: Enables support for method=\"predict_proba\" in cross_val_predict when y=None.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.cross_val_predict()\n```\n\n----------------------------------------\n\nTITLE: Importing RandomizedSearchCV and ParameterSampler in Python\nDESCRIPTION: Demonstrates importing RandomizedSearchCV and ParameterSampler from sklearn.grid_search for randomized hyperparameter optimization.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.14.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.grid_search import RandomizedSearchCV, ParameterSampler\n```\n\n----------------------------------------\n\nTITLE: Accessing FeatureUnion Transformers in Python\nDESCRIPTION: Demonstrates how to use indexing notation to access transformers by name in a FeatureUnion object.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.3.rst#2025-04-14_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nfeature_union[\"scalar\"]\n```\n\n----------------------------------------\n\nTITLE: Loading Categorical Data using Parser Options\nDESCRIPTION: Example showing the difference between 'liac-arff' and 'pandas' parsers when handling categorical data with as_frame=False. The liac-arff parser returns ordinally encoded data with categories in the Bunch instance, while pandas returns a NumPy array.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/datasets/loading_other_datasets.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nas_frame=False  # When using this option\n# liac-arff parser example:\nBunch.categories  # Contains category information\n\n# For pandas parser, use explicit encoding:\nColumnTransformer([\n    ('categorical', OneHotEncoder(), categorical_columns),\n    # or\n    ('categorical', OrdinalEncoder(), categorical_columns)\n])\n```\n\n----------------------------------------\n\nTITLE: Using return_train_score Parameter in Cross-Validation Functions\nDESCRIPTION: Example showing the return_train_score parameter in cross-validation functions which is changing its default from True to False in version 0.21. Users should explicitly set this parameter based on their needs.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Explicitly setting return_train_score when using cross-validation\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n\n# For slower prediction/scoring functions, set to False\nresults = cross_validate(estimator, X, y, return_train_score=False)\n\n# When training scores are desired, set to True\ngrid_search = GridSearchCV(estimator, param_grid, return_train_score=True)\n```\n\n----------------------------------------\n\nTITLE: Adding indexing support to Pipeline in Python\nDESCRIPTION: The pipeline.Pipeline class now supports indexing notation to extract subsequences of steps as new Pipeline instances or to directly access individual steps.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\npipeline.Pipeline\n```\n\n----------------------------------------\n\nTITLE: Accessing Pipeline Steps as Attributes\nDESCRIPTION: Demonstrates how to access individual steps of a scikit-learn Pipeline as attributes of its named_steps property.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([(\"svm\", SVC()), (\"pca\", PCA())])\n\n# Access steps as attributes\nsvm_step = pipeline.named_steps.svm\npca_step = pipeline.named_steps.pca\n```\n\n----------------------------------------\n\nTITLE: Loading KDD Cup '99 Dataset in Python using scikit-learn\nDESCRIPTION: This snippet demonstrates how to load the KDD Cup '99 dataset using the sklearn.datasets.fetch_kddcup99 function. It returns a dictionary-like object containing the feature matrix and target values. The 'as_frame' option can be used to convert the data into pandas DataFrame and Series.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/kddcup99.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import fetch_kddcup99\n\n# Load the dataset\nkddcup99 = fetch_kddcup99()\n\n# Access the feature matrix and target values\nX = kddcup99.data\ny = kddcup99.target\n\n# Optionally, load as pandas DataFrame and Series\nkddcup99_df = fetch_kddcup99(as_frame=True)\nX_df = kddcup99_df.data\ny_series = kddcup99_df.target\n```\n\n----------------------------------------\n\nTITLE: Setting up Multiple Axes with GridSpec in Python\nDESCRIPTION: Example showing how to create multiple plotting axes using matplotlib's GridSpecFromSubplotSpec for complex visualizations in scikit-learn.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/plotting.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpecFromSubplotSpec\n\nfig, ax = plt.subplots()\ngs = GridSpecFromSubplotSpec(2, 2, subplot_spec=ax.get_subplotspec())\n\nax_top_left = fig.add_subplot(gs[0, 0])\nax_top_right = fig.add_subplot(gs[0, 1])\nax_bottom = fig.add_subplot(gs[1, :])\n```\n\n----------------------------------------\n\nTITLE: Adding Pandas Nullable Integer Support in scikit-learn Preprocessing Scalers\nDESCRIPTION: Enables support for pandas' nullable integer dtype with missing values in various preprocessing scalers and transformers.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\npreprocessing.MaxAbsScaler()\npreprocessing.MinMaxScaler()\npreprocessing.StandardScaler()\npreprocessing.PowerTransformer()\npreprocessing.QuantileTransformer()\npreprocessing.RobustScaler()\n```\n\n----------------------------------------\n\nTITLE: Isotonic Calibration Mathematical Formula\nDESCRIPTION: Mathematical formula for the isotonic regression calibration method, showing the minimization objective for non-parametric probability calibration.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/calibration.rst#2025-04-14_snippet_4\n\nLANGUAGE: math\nCODE:\n```\n\\sum_{i=1}^{n} (y_i - \\hat{f}_i)^2\n```\n\n----------------------------------------\n\nTITLE: Improving Numerical Stability in MLPClassifier in scikit-learn Neural Networks\nDESCRIPTION: Enhances the numerical stability of the logistic loss function in MLPClassifier by clipping the probabilities.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\nneural_network.MLPClassifier()\n```\n\n----------------------------------------\n\nTITLE: Running scikit-learn test suite\nDESCRIPTION: Command to launch the scikit-learn test suite using pytest. This requires pytest version 7.1.2 or higher to be installed.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/README.rst#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npytest sklearn\n```\n\n----------------------------------------\n\nTITLE: Using score_samples method in RandomizedSearchCV and GridSearchCV\nDESCRIPTION: Added score_samples method to RandomizedSearchCV and GridSearchCV classes for additional scoring functionality.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\nsearch.score_samples(X)\n```\n\n----------------------------------------\n\nTITLE: Using make_regression for Synthetic Data\nDESCRIPTION: Generates synthetic regression data using scikit-learn's make_regression function with specified samples and features.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/minimal_reproducer.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import make_regression\n\nX, y = make_regression(n_samples=1000, n_features=20)\n```\n\n----------------------------------------\n\nTITLE: Simplified GradientBoostingRegressor Example\nDESCRIPTION: This simplified version of the GradientBoostingRegressor example removes unnecessary steps and focuses on reproducing the warning with n_iter_no_change parameter. It uses the full dataset without splitting or scaling.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/minimal_reproducer.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\ndf = pd.read_csv(\"https://example.com/my_data.csv\")\nX = df[[\"feature_name\"]]\ny = df[\"target\"]\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngbdt = GradientBoostingRegressor()\ngbdt.fit(X, y)  # no warning\n\ngbdt = GradientBoostingRegressor(n_iter_no_change=5)\ngbdt.fit(X, y)  # raises warning\n```\n\n----------------------------------------\n\nTITLE: Consistent error messages for invalid parameters in linear models (Python)\nDESCRIPTION: Implements consistent error messages across ElasticNet, Lasso, Ridge, and RidgeClassifier when invalid values are passed for parameters like l1_ratio, alpha, max_iter, and tol.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.1.rst#2025-04-14_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge, RidgeClassifier\n\n# These will now raise consistent error messages for invalid parameters\nElasticNet(l1_ratio=-1)\nLasso(alpha=-1)\nRidge(max_iter=0)\nRidgeClassifier(tol=-1)\n```\n\n----------------------------------------\n\nTITLE: Poor Agreement Rand Index in scikit-learn in Python\nDESCRIPTION: This snippet demonstrates that poorly agreeing labels result in lower Rand Index scores. The Adjusted Rand Index is negative or close to zero for independent labelings, while the unadjusted Rand Index remains higher. This shows how the metrics quantify the agreement between clusterings.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> labels_true = [0, 0, 0, 0, 0, 0, 1, 1]\n>>> labels_pred = [0, 1, 2, 3, 4, 5, 5, 6]\n>>> metrics.rand_score(labels_true, labels_pred)\n0.39...\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\n-0.07...\n```\n\n----------------------------------------\n\nTITLE: Adding sample_weight parameter to cohen_kappa_score in Python\nDESCRIPTION: The cohen_kappa_score metric now accepts a sample_weight parameter to assign weights to individual samples.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.cohen_kappa_score\n```\n\n----------------------------------------\n\nTITLE: Using Absolute Error Loss in Scikit-learn Estimators\nDESCRIPTION: Updates to various estimators to use 'absolute_error' instead of deprecated options for absolute error loss.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nExtraTreesRegressor(criterion=\"absolute_error\")\nGradientBoostingRegressor(loss=\"absolute_error\")\nRandomForestRegressor(criterion=\"absolute_error\")\nHistGradientBoostingRegressor(loss=\"absolute_error\")\nRANSACRegressor(loss=\"absolute_error\")\nDecisionTreeRegressor(criterion=\"absolute_error\")\nExtraTreeRegressor(criterion=\"absolute_error\")\n```\n\n----------------------------------------\n\nTITLE: Updating FunctionTransformer inverse_transform method in Python\nDESCRIPTION: FunctionTransformer.inverse_transform now correctly supports DataFrames that are all numerical when check_inverse=True.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\npreprocessing.FunctionTransformer.inverse_transform\n```\n\n----------------------------------------\n\nTITLE: StandardScaler Pipeline for Linear Models\nDESCRIPTION: Example showing how to replace the deprecated normalize parameter with a StandardScaler pipeline. This pattern should be used instead of the normalize=True parameter which is being removed.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\npipe = make_pipeline(StandardScaler(with_mean=False),\n                    LinearRegression())\n```\n\n----------------------------------------\n\nTITLE: Deprecating NumPy Matrix Usage in Scikit-learn\nDESCRIPTION: Deprecation of np.matrix usage in scikit-learn, which will raise a TypeError in version 1.2.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nnp.matrix\n```\n\n----------------------------------------\n\nTITLE: Accessing KMeans inertia attribute\nDESCRIPTION: Code example showing a bug fix in KMeans where rounding errors could prevent convergence with tol=0, and fixes to the inertia_ attribute weighting in both KMeans and MiniBatchKMeans.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(tol=0)\nkmeans.fit(X)\nprint(kmeans.inertia_)\n```\n\n----------------------------------------\n\nTITLE: Adding OrdinalEncoder for ordinal encoding in Python\nDESCRIPTION: Added OrdinalEncoder to convert categorical features to ordinal integers.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.preprocessing import OrdinalEncoder\nenc = OrdinalEncoder()\nenc.fit(X)\n```\n\n----------------------------------------\n\nTITLE: Loading LFW Pairs Dataset for Face Verification in Python\nDESCRIPTION: This snippet demonstrates how to load the LFW pairs dataset for face verification tasks using scikit-learn's fetch_lfw_pairs function. It loads the training subset of the pairs dataset.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/lfw.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import fetch_lfw_pairs\nlfw_pairs_train = fetch_lfw_pairs(subset='train')\n\nlist(lfw_pairs_train.target_names)\nlfw_pairs_train.pairs.shape\nlfw_pairs_train.data.shape\nlfw_pairs_train.target.shape\n```\n\n----------------------------------------\n\nTITLE: Loading SVM Light Format Files in Python\nDESCRIPTION: Demonstrates how to use path-like objects when loading SVM light format files using the load_svmlight_file and load_svmlight_files functions.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ndatasets.load_svmlight_file(path_like_object)\n```\n\nLANGUAGE: Python\nCODE:\n```\ndatasets.load_svmlight_files(path_like_object)\n```\n\n----------------------------------------\n\nTITLE: Implementing RocCurveDisplay Class in Python\nDESCRIPTION: Example implementation of a display class for ROC curves that follows scikit-learn's plotting API conventions. Shows initialization, class methods for creating from estimator or predictions, and plotting functionality.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/plotting.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass RocCurveDisplay:\n    def __init__(self, fpr, tpr, roc_auc, estimator_name):\n        ...\n        self.fpr = fpr\n        self.tpr = tpr\n        self.roc_auc = roc_auc\n        self.estimator_name = estimator_name\n\n    @classmethod\n    def from_estimator(cls, estimator, X, y):\n        # get the predictions\n        y_pred = estimator.predict_proba(X)[:, 1]\n        return cls.from_predictions(y, y_pred, estimator.__class__.__name__)\n\n    @classmethod\n    def from_predictions(cls, y, y_pred, estimator_name):\n        # do ROC computation from y and y_pred\n        fpr, tpr, roc_auc = ...\n        viz = RocCurveDisplay(fpr, tpr, roc_auc, estimator_name)\n        return viz.plot()\n\n    def plot(self, ax=None, name=None, **kwargs):\n        ...\n        self.line_ = ...\n        self.ax_ = ax\n        self.figure_ = ax.figure_\n```\n\n----------------------------------------\n\nTITLE: Using SelectFromModel with Random Forests in Python\nDESCRIPTION: Example of using SelectFromModel meta-transformer with Random Forest models to select important features based on feature importances. This replaces the deprecated transform methods on models inheriting from _LearntSelectorMixin.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.17.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nrf = RandomForestClassifier()\nselector = SelectFromModel(rf, threshold='mean')\nselector.fit(X, y)\nX_selected = selector.transform(X)\n```\n\n----------------------------------------\n\nTITLE: Renaming cross_val to cross_validation in Python\nDESCRIPTION: This code snippet shows how to rename the `cross_val` package to `cross_validation` in Python code. This renaming is part of the API changes introduced in scikit-learn 0.9. The snippet uses shell commands to find and replace all occurrences of `cross_val` with `cross_validation` in Python files.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/older_versions.rst#2025-04-14_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n\"find -name \\\"*.py\\\" | xargs sed -i 's/\\bcross_val\\b/cross_validation/g'\"\n```\n\n----------------------------------------\n\nTITLE: Generating Authors Display Grid with Avatars in HTML/CSS\nDESCRIPTION: This HTML code creates a responsive grid display of project contributors for the scikit-learn project. It includes styling for rounded avatar images and individual containers for each contributor with their GitHub profile link, avatar image, and name.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/contributor_experience_team.rst#2025-04-14_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!-- Generated by generate_authors_table.py -->\n<div class=\"sk-authors-container\">\n<style>\n  img.avatar {border-radius: 10px;}\n</style>\n<div>\n<a href='https://github.com/virchan'><img src='https://avatars.githubusercontent.com/u/25701849?v=4' class='avatar' /></a> <br />\n<p>Virgil Chan</p>\n</div>\n<div>\n<a href='https://github.com/alfaro96'><img src='https://avatars.githubusercontent.com/u/32649176?v=4' class='avatar' /></a> <br />\n<p>Juan Carlos Alfaro Jiménez</p>\n</div>\n<div>\n<a href='https://github.com/lucyleeow'><img src='https://avatars.githubusercontent.com/u/23182829?v=4' class='avatar' /></a> <br />\n<p>Lucy Liu</p>\n</div>\n<div>\n<a href='https://github.com/MaxwellLZH'><img src='https://avatars.githubusercontent.com/u/16646940?v=4' class='avatar' /></a> <br />\n<p>Maxwell Liu</p>\n</div>\n<div>\n<a href='https://github.com/jmloyola'><img src='https://avatars.githubusercontent.com/u/2133361?v=4' class='avatar' /></a> <br />\n<p>Juan Martin Loyola</p>\n</div>\n<div>\n<a href='https://github.com/smarie'><img src='https://avatars.githubusercontent.com/u/3236794?v=4' class='avatar' /></a> <br />\n<p>Sylvain Marié</p>\n</div>\n<div>\n<a href='https://github.com/norbusan'><img src='https://avatars.githubusercontent.com/u/1735589?v=4' class='avatar' /></a> <br />\n<p>Norbert Preining</p>\n</div>\n<div>\n<a href='https://github.com/StefanieSenger'><img src='https://avatars.githubusercontent.com/u/91849487?v=4' class='avatar' /></a> <br />\n<p>Stefanie Senger</p>\n</div>\n<div>\n<a href='https://github.com/reshamas'><img src='https://avatars.githubusercontent.com/u/2507232?v=4' class='avatar' /></a> <br />\n<p>Reshama Shaikh</p>\n</div>\n<div>\n<a href='https://github.com/albertcthomas'><img src='https://avatars.githubusercontent.com/u/15966638?v=4' class='avatar' /></a> <br />\n<p>Albert Thomas</p>\n</div>\n<div>\n<a href='https://github.com/marenwestermann'><img src='https://avatars.githubusercontent.com/u/17019042?v=4' class='avatar' /></a> <br />\n<p>Maren Westermann</p>\n</div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Using fit_params in permutation_test_score and validation_curve\nDESCRIPTION: Added support for passing additional estimator parameters via fit_params in permutation_test_score and validation_curve functions.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.model_selection import permutation_test_score, validation_curve\n\npermutation_test_score(estimator, X, y, fit_params={})\nvalidation_curve(estimator, X, y, fit_params={})\n```\n\n----------------------------------------\n\nTITLE: Benchmarking a Complete Module in scikit-learn\nDESCRIPTION: Command to run benchmarks for an entire module (linear_model) in scikit-learn, comparing performance between upstream/main and HEAD.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_34\n\nLANGUAGE: bash\nCODE:\n```\nasv continuous -b linear_model upstream/main HEAD\n```\n\n----------------------------------------\n\nTITLE: Exposing Deciles Lines in PartialDependenceDisplay in scikit-learn Inspection\nDESCRIPTION: Makes deciles lines accessible as attributes in PartialDependenceDisplay for customization or hiding.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\ninspection.PartialDependenceDisplay()\n```\n\n----------------------------------------\n\nTITLE: Fixing r2_score for single sample case in Python\nDESCRIPTION: The metrics.r2_score function now returns NaN and raises exceptions.UndefinedMetricWarning when there is only a single sample, as the metric is degenerate in this case.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.r2_score\n```\n\n----------------------------------------\n\nTITLE: Bad Agreement Adjusted Mutual Information in scikit-learn in Python\nDESCRIPTION: This snippet demonstrates that poorly agreeing labels result in non-positive Adjusted Mutual Information (AMI) scores. This shows how the AMI score quantifies the agreement between clusterings, with lower scores indicating less agreement.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\n>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP\n-0.10526...\n```\n\n----------------------------------------\n\nTITLE: MiniBatchDictionaryLearning partial fit\nDESCRIPTION: Demonstrates fix for MiniBatchDictionaryLearning where partial_fit now correctly updates dictionary by iterating once over mini-batch.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.decomposition import MiniBatchDictionaryLearning\ndict_learning = MiniBatchDictionaryLearning()\ndict_learning.partial_fit(X)\n```\n\n----------------------------------------\n\nTITLE: Setting eps in log_loss metric\nDESCRIPTION: Shows the new default value of 'auto' for the eps parameter in the log_loss metric function, which sets eps to the machine epsilon for the prediction dtype.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.metrics import log_loss\n\nlog_loss(y_true, y_pred, eps='auto')\n```\n\n----------------------------------------\n\nTITLE: Fixing cross_val_predict for excluded classes in Python\nDESCRIPTION: The model_selection.cross_val_predict function now correctly handles cases where a class is excluded in a cross-validation fold when using method=\"predict_proba\".\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.cross_val_predict\n```\n\n----------------------------------------\n\nTITLE: Cloning Scikit-learn Estimators Safely\nDESCRIPTION: Example of using sklearn.base.clone for creating independent copies of estimator objects. This function was fixed to address thread safety issues that could cause \"pop from empty list\" errors.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\nsklearn.base.clone\n```\n\n----------------------------------------\n\nTITLE: Installing scikit-learn on MacOS/Linux with pip\nDESCRIPTION: Commands to create a virtual environment and install scikit-learn using pip on Unix-based systems. Includes verification commands.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/install.rst#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv sklearn-env\nsource sklearn-env/bin/activate  # activate\npip install -U scikit-learn\n\npython -m pip show scikit-learn  # show scikit-learn version and location\npython -m pip freeze             # show all installed packages in the environment\npython -c \"import sklearn; sklearn.show_versions()\"\n```\n\n----------------------------------------\n\nTITLE: Enhancing pairwise_distances_chunked in scikit-learn Metrics\nDESCRIPTION: Allows the reduce_func in pairwise_distances_chunked to not have a return value, enabling in-place operations.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.pairwise_distances_chunked()\n```\n\n----------------------------------------\n\nTITLE: Adding OneHotEncoder for categorical string features in Python\nDESCRIPTION: Expanded OneHotEncoder to encode categorical string features as numeric arrays using one-hot encoding.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit(X)\n```\n\n----------------------------------------\n\nTITLE: Mathematical Expression - Test Set Transformation\nDESCRIPTION: Shows how a test set X is transformed using the Vk matrix obtained from truncated SVD.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/decomposition.rst#2025-04-14_snippet_2\n\nLANGUAGE: math\nCODE:\n```\nX' = X V_k\n```\n\n----------------------------------------\n\nTITLE: Fixing Scoring Strategy in RidgeClassifierCV in scikit-learn\nDESCRIPTION: Fixes a bug in RidgeClassifierCV to pass a specific scoring strategy, ensuring the internal estimator outputs predictions instead of scores.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nlinear_model.RidgeClassifierCV()\n```\n\n----------------------------------------\n\nTITLE: Mathematical Expression - Matrix Norms\nDESCRIPTION: Defines Frobenius norm (||.||_Fro) and entry-wise matrix norm (||.||_1,1) used in sparse PCA implementation. The entry-wise norm helps prevent learning from noise when training samples are limited.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/decomposition.rst#2025-04-14_snippet_0\n\nLANGUAGE: math\nCODE:\n```\n||.||_{\\text{Fro}} \\text{ (Frobenius norm)}\n||.||_{1,1} \\text{ (entry-wise matrix norm)}\n```\n\n----------------------------------------\n\nTITLE: Adding sample_weight parameter to metrics.confusion_matrix in Python\nDESCRIPTION: Adds a sample_weight parameter to the confusion_matrix function in the metrics module.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.18.rst#2025-04-14_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n```\n\n----------------------------------------\n\nTITLE: Using Brier Score Loss for Multiclass Classification in scikit-learn\nDESCRIPTION: The metrics.brier_score_loss function implements the Brier score for multiclass classification problems with a scale_by_half argument. This metric is useful for assessing both sharpness and calibration of probabilistic classifiers.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/sklearn.metrics/22046.feature.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmetrics.brier_score_loss\n```\n\n----------------------------------------\n\nTITLE: Referencing Ensemble Examples in reStructuredText\nDESCRIPTION: This snippet creates a reference label for ensemble examples in reStructuredText format. It allows other parts of the documentation to link to this section using the '_ensemble_examples' reference.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/ensemble/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _ensemble_examples:\n```\n\n----------------------------------------\n\nTITLE: Adding keyword argument passing in Pipeline predict in Python\nDESCRIPTION: The predict method of Pipeline now passes keyword arguments to the last estimator's predict method.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline([('scaler', StandardScaler()), ('reg', GaussianProcessRegressor())])\ny_pred, y_std = pipe.predict(X, return_std=True)\n```\n\n----------------------------------------\n\nTITLE: Creating D2 score functions in Python with scikit-learn\nDESCRIPTION: Shows how to create custom scorer objects for D2 Tweedie score and D2 pinball score using make_scorer function from scikit-learn.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_53\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.metrics import d2_tweedie_score, make_scorer\nd2_tweedie_score_15 = make_scorer(d2_tweedie_score, power=1.5)\n```\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.metrics import d2_pinball_score, make_scorer\nd2_pinball_score_08 = make_scorer(d2_pinball_score, alpha=0.8)\n```\n\n----------------------------------------\n\nTITLE: Renaming CV splitter parameters in scikit-learn Python\nDESCRIPTION: The parameters n_iter and n_folds in old CV splitters are replaced by the new parameter n_splits to provide a consistent interface for representing the number of train-test splits.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.18.rst#2025-04-14_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ncv_splitter = CrossValidationSplitter(n_splits=5)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Newton-CG Solver in LogisticRegression in scikit-learn\nDESCRIPTION: Improves the newton-cg solver in LogisticRegression by avoiding an unnecessary iteration when checking for convergence.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nlinear_model.LogisticRegression(solver='newton-cg')\n```\n\n----------------------------------------\n\nTITLE: Fixing MultiOutputClassifier predict_proba method in Python\nDESCRIPTION: The multioutput.MultiOutputClassifier class now correctly checks for the predict_proba attribute in the estimator object.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\nmultioutput.MultiOutputClassifier\n```\n\n----------------------------------------\n\nTITLE: Accessing metrics.pairwise_distances_chunked Function in Scikit-learn\nDESCRIPTION: Example showing how to reference the pairwise_distances_chunked function from the metrics module for memory-controlled operations.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.pairwise_distances_chunked\n```\n\n----------------------------------------\n\nTITLE: Using MeanShift with bin_seeding\nDESCRIPTION: Example showing fixed behavior of MeanShift clustering when bin_seeding=True and estimated bandwidth is 0, making it equivalent to bin_seeding=False.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.cluster import MeanShift\nms = MeanShift(bin_seeding=True)\nms.fit(X)\n```\n\n----------------------------------------\n\nTITLE: Creating ICE Plots\nDESCRIPTION: Demonstrates how to create Individual Conditional Expectation plots using PartialDependenceDisplay with different display options.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/partial_dependence.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import make_hastie_10_2\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.inspection import PartialDependenceDisplay\n\nX, y = make_hastie_10_2(random_state=0)\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n    max_depth=1, random_state=0).fit(X, y)\nfeatures = [0, 1]\nPartialDependenceDisplay.from_estimator(clf, X, features,\n    kind='individual')\n```\n\n----------------------------------------\n\nTITLE: Computing Asymmetric Pair Confusion Matrix\nDESCRIPTION: Example showing the asymmetric nature of pair_confusion_matrix when comparing different clustering assignments.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics.cluster import pair_confusion_matrix\npair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 2])\n```\n\n----------------------------------------\n\nTITLE: Using Thread-safe Configuration Context in Scikit-learn\nDESCRIPTION: Improvement to make config_context thread-safe in sklearn.base module.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.base import config_context\n```\n\n----------------------------------------\n\nTITLE: Enhancing Imputer Classes to Accept Pandas Nullable Integer Dtype in scikit-learn\nDESCRIPTION: Updates SimpleImputer, KNNImputer, and IterativeImputer to accept pandas' nullable integer dtype with missing values.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimpute.SimpleImputer()\nimpute.KNNImputer()\nimpute.IterativeImputer()\n```\n\n----------------------------------------\n\nTITLE: Installing scikit-learn on Linux distributions\nDESCRIPTION: Package manager commands for installing scikit-learn on various Linux distributions including Alpine, Arch, Debian/Ubuntu, and Fedora.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/install.rst#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apk add py3-scikit-learn          # Alpine Linux\nsudo pacman -S python-scikit-learn     # Arch Linux\nsudo apt-get install python3-sklearn python3-sklearn-lib python-sklearn-doc  # Debian/Ubuntu\nsudo dnf install python3-scikit-learn  # Fedora\nsudo port install py312-scikit-learn   # MacPorts\n```\n\n----------------------------------------\n\nTITLE: Importing sklearn.kernel_approximation Module in Python\nDESCRIPTION: This snippet demonstrates how to import the kernel_approximation module from scikit-learn. The module is used for various kernel approximation techniques in machine learning.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/kernel_approximation/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import kernel_approximation\n```\n\n----------------------------------------\n\nTITLE: Configuring SpectralClustering with Sparse Input\nDESCRIPTION: Enhanced SpectralClustering to accept precomputed sparse neighbors graph and added n_components parameter to match spectral_clustering functionality.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.cluster import SpectralClustering\n\n# Now accepts sparse matrix input\nclusterer = SpectralClustering(n_components=2)\nclusterer.fit(sparse_matrix)\n```\n\n----------------------------------------\n\nTITLE: Hash Function Usage with murmurhash3_32\nDESCRIPTION: Shows how to use the murmurhash3_32 utility function for non-cryptographic hashing, useful for feature hashing and sparse random projections.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/utilities.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.utils import murmurhash3_32\n>>> murmurhash3_32(\"some feature\", seed=0) == -384616559\nTrue\n\n>>> murmurhash3_32(\"some feature\", seed=0, positive=True) == 3910350737\nTrue\n```\n\n----------------------------------------\n\nTITLE: Fetching Olivetti Faces Dataset in Python using scikit-learn\nDESCRIPTION: This code snippet demonstrates how to fetch the Olivetti faces dataset using the sklearn.datasets.fetch_olivetti_faces function. This function downloads and caches the dataset from AT&T Laboratories.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/olivetti_faces.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import fetch_olivetti_faces\n\nfaces_data = fetch_olivetti_faces()\n```\n\n----------------------------------------\n\nTITLE: Mathematical Formulation of SVR Dual Problem\nDESCRIPTION: Presents the dual optimization problem for SVR using Lagrange multipliers alpha and alpha*. The dual formulation involves the kernel matrix Q and enables the implicit mapping of training vectors to higher-dimensional spaces.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/svm.rst#2025-04-14_snippet_9\n\nLANGUAGE: math\nCODE:\n```\n\\min_{\\alpha, \\alpha^*} \\frac{1}{2} (\\alpha - \\alpha^*)^T Q (\\alpha - \\alpha^*) + \\varepsilon e^T (\\alpha + \\alpha^*) - y^T (\\alpha - \\alpha^*)\n\n\n\\textrm {subject to } & e^T (\\alpha - \\alpha^*) = 0\\\\\n& 0 \\leq \\alpha_i, \\alpha_i^* \\leq C, i=1, ..., n\n```\n\n----------------------------------------\n\nTITLE: Fixing Joblib Compatibility in ElasticNetCV and Related Classes in scikit-learn\nDESCRIPTION: Resolves a fitting failure when using joblib loky backend with ElasticNetCV, MultiTaskElasticNetCV, LassoCV, and MultiTaskLassoCV.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nlinear_model.ElasticNetCV()\nlinear_model.MultiTaskElasticNetCV()\nlinear_model.LassoCV()\nlinear_model.MultiTaskLassoCV()\n```\n\n----------------------------------------\n\nTITLE: Initial GradientBoostingRegressor Example with Warning\nDESCRIPTION: This snippet demonstrates a GradientBoostingRegressor instance being fit to data from a CSV file, showing a UserWarning when n_iter_no_change is set. It includes data loading, preprocessing, and model fitting steps.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/minimal_reproducer.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv('my_data.csv')\nX = df[[\"feature_name\"]] # my features do have names\ny = df[\"target\"]\n\n# We set random_state=42 for the train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42\n)\n\nscaler = StandardScaler(with_mean=False)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# An instance with default n_iter_no_change raises no error nor warnings\ngbdt = GradientBoostingRegressor(random_state=0)\ngbdt.fit(X_train, y_train)\ndefault_score = gbdt.score(X_test, y_test)\n\n# the bug appears when I change the value for n_iter_no_change\ngbdt = GradientBoostingRegressor(random_state=0, n_iter_no_change=5)\ngbdt.fit(X_train, y_train)\nother_score = gbdt.score(X_test, y_test)\n\nother_score = gbdt.score(X_test, y_test)\n```\n\n----------------------------------------\n\nTITLE: Model Compression with Sparsification in scikit-learn\nDESCRIPTION: Sample code demonstrating how to use the sparsify() method with a SGDRegressor to control model sparsity, reducing latency and memory usage for linear models.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/computing/computational_performance.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclf = SGDRegressor(penalty='elasticnet', l1_ratio=0.25)\nclf.fit(X_train, y_train).sparsify()\nclf.predict(X_test)\n```\n\n----------------------------------------\n\nTITLE: Referencing Covariance Examples in RST Documentation\nDESCRIPTION: This RST code snippet creates a reference label for the covariance examples section and provides a title for the examples concerning the sklearn.covariance module.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/covariance/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _covariance_examples:\n\nCovariance estimation\n---------------------\n\nExamples concerning the :mod:`sklearn.covariance` module.\n```\n\n----------------------------------------\n\nTITLE: Including Changelog Legend in Scikit-learn Release Notes\nDESCRIPTION: Includes a file containing the legend for the changelog entries.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: changelog_legend.inc\n```\n\n----------------------------------------\n\nTITLE: Adding explained variance score to metrics scorers in Python\nDESCRIPTION: A new scorer based on the explained_variance_score metric is now available through the get_scorer function and scoring parameters.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.get_scorer\nmetrics.explained_variance_score\n```\n\n----------------------------------------\n\nTITLE: Running Test Coverage with Pytest in Scikit-learn\nDESCRIPTION: Commands to install pytest and pytest-cov, and run test coverage analysis to ensure at least 80% code coverage.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install pytest pytest-cov\npytest --cov sklearn path/to/tests\n```\n\n----------------------------------------\n\nTITLE: Predicting with Fitted Bayesian Ridge Model\nDESCRIPTION: Shows how to use a fitted Bayesian Ridge Regression model to make predictions on new data points.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> reg.predict([[1, 0.]])\narray([0.50000013])\n```\n\n----------------------------------------\n\nTITLE: Importing Pandas in Python\nDESCRIPTION: Demonstrates the conventional import statement for Pandas, which is commonly abbreviated as 'pd' in scikit-learn and data science contexts.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/glossary.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Displaying Software Versions for scikit-learn Debugging\nDESCRIPTION: This Python code snippet shows how to display version information for scikit-learn and its dependencies, which is useful for diagnosing issues and providing context in bug reports.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/tips.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport sklearn; sklearn.show_versions()\n```\n\n----------------------------------------\n\nTITLE: Predicting with DecisionTreeClassifier when no missing values in training in Python\nDESCRIPTION: This code snippet illustrates how DecisionTreeClassifier handles missing values during prediction when no missing values were seen during training. In this case, missing values are mapped to the child with the most samples.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/tree.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\nX = np.array([0, 1, 2, 3]).reshape(-1, 1)\ny = [0, 1, 1, 1]\n\ntree = DecisionTreeClassifier(random_state=0).fit(X, y)\n\nX_test = np.array([np.nan]).reshape(-1, 1)\ntree.predict(X_test)\n```\n\n----------------------------------------\n\nTITLE: Cross-validation Parameter Setting Example\nDESCRIPTION: Example from the documentation showing parameter name changes in cross-validation, where n_folds and n_iter parameters were renamed to n_splits in various CV splitter classes.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.18.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.KFold(n_splits=5)\nmodel_selection.GroupKFold(n_splits=5)\nmodel_selection.StratifiedKFold(n_splits=5)\n```\n\n----------------------------------------\n\nTITLE: Text Encoding Detection Example\nDESCRIPTION: Demonstrates how to handle text encoding using chardet library to detect and decode text in different encodings.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n>>> import chardet    # doctest: +SKIP\n>>> text1 = b\"Sei mir gegr\\xc3\\xbc\\xc3\\x9ft mein Sauerkraut\"\n>>> text2 = b\"holdselig sind deine Ger\\xfcche\"\n>>> text3 = b\"\\xff\\xfeA\\x00u\\x00f\\x00 \\x00F\\x00l\\x00\\xfc\\x00g\\x00e\\x00l\\x00n\\x00 \\x00d\\x00e\\x00s\\x00 \\x00G\\x00e\\x00s\\x00a\\x00n\\x00g\\x00e\\x00s\\x00,\\x00 \\x00H\\x00e\\x00r\\x00z\\x00l\\x00i\\x00e\\x00b\\x00c\\x00h\\x00e\\x00n\\x00,\\x00 \\x00t\\x00r\\x00a\\x00g\\x00 \\x00i\\x00c\\x00h\\x00 \\x00d\\x00i\\x00c\\x00h\\x00 \\x00f\\x00o\\x00r\\x00t\\x00\"\n>>> decoded = [x.decode(chardet.detect(x)['encoding'])\n...            for x in (text1, text2, text3)]        # doctest: +SKIP\n>>> v = CountVectorizer().fit(decoded).vocabulary_    # doctest: +SKIP\n>>> for term in v: print(v)                           # doctest: +SKIP\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenML Dataset Parser in Python\nDESCRIPTION: Shows how to configure the parser for loading OpenML datasets using the fetch_openml function. It allows specifying 'pandas' or 'liac-arff' as the parser, with 'auto' mode selecting based on dataset sparsity.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ndatasets.fetch_openml(parser=\"pandas\")\n```\n\nLANGUAGE: Python\nCODE:\n```\ndatasets.fetch_openml(parser=\"liac-arff\")\n```\n\nLANGUAGE: Python\nCODE:\n```\ndatasets.fetch_openml(parser=\"auto\")\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Name Filter using pytest in Python\nDESCRIPTION: This command shows how to run tests that match a specific name pattern using pytest's -k parameter, in this case selecting common tests for LogisticRegression.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/tips.rst#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest sklearn/tests/test_common.py -v -k LogisticRegression\n```\n\n----------------------------------------\n\nTITLE: Updating VotingClassifier predict_proba method in Python\nDESCRIPTION: Removes the predict_proba method from VotingClassifier when voting='hard' is set. This ensures consistency between the voting method and available prediction methods.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nensemble.VotingClassifier.predict_proba\n```\n\n----------------------------------------\n\nTITLE: Adding fill_value parameter to IterativeImputer\nDESCRIPTION: A new fill_value parameter has been added to the impute.IterativeImputer class to specify the initial imputation value.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.3.rst#2025-04-14_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimpute.IterativeImputer(fill_value=0)\n```\n\n----------------------------------------\n\nTITLE: Importing CountVectorizer from scikit-learn\nDESCRIPTION: This snippet imports the `CountVectorizer` class from the `sklearn.feature_extraction.text` module. CountVectorizer is used to transform a given text into a vector on the basis of the frequency of each word that occurs in the entire text.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.feature_extraction.text import CountVectorizer\n```\n\n----------------------------------------\n\nTITLE: Bernoulli RBM Conditional Probability Distributions\nDESCRIPTION: Formulas for conditional probability distributions of visible and hidden units using logistic sigmoid function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neural_networks_unsupervised.rst#2025-04-14_snippet_3\n\nLANGUAGE: math\nCODE:\n```\nP(v_i=1|\\mathbf{h}) = \\sigma(\\sum_j w_{ij}h_j + b_i) \\\\ P(h_i=1|\\mathbf{v}) = \\sigma(\\sum_i w_{ij}v_i + c_j)\n```\n\n----------------------------------------\n\nTITLE: Sigmoid Calibration Mathematical Formula\nDESCRIPTION: Mathematical formula for the sigmoid calibration method based on Platt's logistic model, showing probability calculation for binary classification.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/calibration.rst#2025-04-14_snippet_3\n\nLANGUAGE: math\nCODE:\n```\np(y_i = 1 | f_i) = \\frac{1}{1 + \\exp(A f_i + B)}\n```\n\n----------------------------------------\n\nTITLE: Using Inverse Transform in Random Projection\nDESCRIPTION: Shows how to use the inverse transform feature of random projection transformers to reconstruct original dimensions from projected data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/random_projection.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n>>> import numpy as np\n>>> from sklearn.random_projection import SparseRandomProjection\n>>> X = np.random.rand(100, 10000)\n>>> transformer = SparseRandomProjection(\n...   compute_inverse_components=True\n... )\n...\n>>> X_new = transformer.fit_transform(X)\n>>> X_new.shape\n(100, 3947)\n>>> X_new_inversed = transformer.inverse_transform(X_new)\n>>> X_new_inversed.shape\n(100, 10000)\n>>> X_new_again = transformer.transform(X_new_inversed)\n>>> np.allclose(X_new, X_new_again)\nTrue\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module in reStructuredText\nDESCRIPTION: Sets the current module context for the reStructuredText document to 'sklearn'.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.17.rst#2025-04-14_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. currentmodule:: sklearn\n```\n\n----------------------------------------\n\nTITLE: Adding max_train_size parameter to TimeSeriesSplit in Python\nDESCRIPTION: The TimeSeriesSplit class now has a max_train_size parameter to control the maximum number of samples in the training set.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.TimeSeriesSplit\n```\n\n----------------------------------------\n\nTITLE: LSQR Solver Coefficient Calculation\nDESCRIPTION: Mathematical representation of how the LSQR solver computes coefficients by solving the linear system instead of explicit matrix inversion.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/lda_qda.rst#2025-04-14_snippet_6\n\nLANGUAGE: latex\nCODE:\n```\n\\omega_k = \\Sigma^{-1}\\mu_k\n```\n\n----------------------------------------\n\nTITLE: Enhancing Metrics with Array API Support\nDESCRIPTION: Added Array API compatibility to explained_variance_score and mean_pinball_loss metrics in scikit-learn, improving input flexibility and interoperability with array-like objects.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/array-api/29978.feature.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nsklearn.metrics.explained_variance_score\n```\n\nLANGUAGE: Python\nCODE:\n```\nsklearn.metrics.mean_pinball_loss\n```\n\n----------------------------------------\n\nTITLE: Improving efficiency of manifold.trustworthiness in Python\nDESCRIPTION: The manifold.trustworthiness function now uses an inverted index instead of an np.where lookup to find the rank of neighbors in the input space, improving efficiency for large datasets and many neighbors.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nmanifold.trustworthiness\n```\n\n----------------------------------------\n\nTITLE: Deprecating an Attribute in Python using scikit-learn's utils\nDESCRIPTION: This example shows how to deprecate an attribute using the @deprecated decorator in combination with the @property decorator. It renames the 'labels_' attribute to 'classes_'.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_42\n\nLANGUAGE: Python\nCODE:\n```\n@deprecated(\n    \"Attribute `labels_` was deprecated in 0.13 and will be removed in 0.15. Use \"\n    \"`classes_` instead\"\n)\n@property\ndef labels_(self):\n    return self.classes_\n```\n\n----------------------------------------\n\nTITLE: Avoiding Divide by Zero Warnings in LabelSpreading and LabelPropagation in scikit-learn Semi-supervised\nDESCRIPTION: Improves the normalization of label_distributions_ to avoid divide by zero warnings in LabelSpreading and LabelPropagation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_38\n\nLANGUAGE: Python\nCODE:\n```\nsemi_supervised.LabelSpreading()\nsemi_supervised.LabelPropagation()\n```\n\n----------------------------------------\n\nTITLE: Log Likelihood Formula for RBM Training\nDESCRIPTION: Formula showing the log likelihood expression that is optimized during RBM training.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neural_networks_unsupervised.rst#2025-04-14_snippet_5\n\nLANGUAGE: math\nCODE:\n```\n\\log P(v) = \\log \\sum_h e^{-E(v, h)} - \\log \\sum_{x, y} e^{-E(x, y)}\n```\n\n----------------------------------------\n\nTITLE: Raising ValueError for empty training sets in CV splitters in Python\nDESCRIPTION: Some CV splitter classes and model_selection.train_test_split now raise ValueError when the resulting training set is empty.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.train_test_split\n```\n\n----------------------------------------\n\nTITLE: Implementing __len__ method in Pipeline in Python\nDESCRIPTION: The pipeline.Pipeline class now implements the __len__ method.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\npipeline.Pipeline\n```\n\n----------------------------------------\n\nTITLE: Fixing StratifiedKFold shuffling bug in Python\nDESCRIPTION: The model_selection.StratifiedKFold class now correctly shuffles each class's samples with different random states when shuffle=True.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.StratifiedKFold\n```\n\n----------------------------------------\n\nTITLE: Generating HTML Annotation Report for Cython Code\nDESCRIPTION: Command to generate an HTML annotation report that helps identify CPython interpreter interactions in Cython code.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/cython.rst#2025-04-14_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# This generates a HTML report (`source.html`) for `source.c`.\ncythonX --annotate source.pyx\n```\n\n----------------------------------------\n\nTITLE: Fixing loss normalization in AdaBoostRegressor\nDESCRIPTION: Corrects the loss normalization in AdaBoostRegressor to only consider the max of samples with non-null weights, fixing an issue with the loss calculation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nensemble.AdaBoostRegressor\n```\n\n----------------------------------------\n\nTITLE: Using fit_params in learning_curve function\nDESCRIPTION: Added support for passing additional estimator parameters via fit_params in the learning_curve function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.model_selection import learning_curve\n\nlearning_curve(estimator, X, y, fit_params={})\n```\n\n----------------------------------------\n\nTITLE: Improving numerical precision of euclidean_distances in Python\nDESCRIPTION: The metrics.pairwise.euclidean_distances function and related estimators with metric='euclidean' now have improved numerical precision for float32 features, at a small performance cost.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.pairwise.euclidean_distances\n```\n\n----------------------------------------\n\nTITLE: Updating OneHotEncoder's drop_idx_ Attribute in scikit-learn Preprocessing\nDESCRIPTION: Allows OneHotEncoder's drop_idx_ ndarray to contain None, indicating no category is dropped for that index.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\npreprocessing.OneHotEncoder()\n```\n\n----------------------------------------\n\nTITLE: Saving scikit-learn model using pickle\nDESCRIPTION: Example showing how to save a scikit-learn model to disk using pickle protocol 5 for optimal memory usage and performance.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/model_persistence.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pickle import dump\nwith open(\"filename.pkl\", \"wb\") as f:\n    dump(clf, f, protocol=5)\n```\n\n----------------------------------------\n\nTITLE: Importing Contributors List in reStructuredText\nDESCRIPTION: Includes an external file containing the list of contributors into the current reStructuredText document.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.17.rst#2025-04-14_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: _contributors.rst\n```\n\n----------------------------------------\n\nTITLE: Imputing Missing Values with Feature Preservation in Python\nDESCRIPTION: Demonstrates how to use SimpleImputer to handle missing values while maintaining feature count using the keep_empty_features parameter.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/impute.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> imputer = SimpleImputer()\n>>> X = np.array([[np.nan, 1], [np.nan, 2], [np.nan, 3]])\n>>> imputer.fit_transform(X)\narray([[1.],\n       [2.],\n       [3.]])\n\n>>> imputer.set_params(keep_empty_features=True)\nSimpleImputer(keep_empty_features=True)\n>>> imputer.fit_transform(X)\narray([[0., 1.],\n       [0., 2.],\n       [0., 3.]])\n```\n\n----------------------------------------\n\nTITLE: Loguniform Random Variable Implementation in Utils\nDESCRIPTION: Implementation of log-uniform random variable in utils.fixes for use in RandomizedSearchCV. Generates values where outcomes like 1, 10, 100 are equally likely within the specified range.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nloguniform(1, 100)\n```\n\n----------------------------------------\n\nTITLE: Adding Best Score Attribute to RidgeCV and RidgeClassifierCV in scikit-learn\nDESCRIPTION: Adds the best_score_ attribute to RidgeCV and RidgeClassifierCV classes.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nlinear_model.RidgeCV()\nlinear_model.RidgeClassifierCV()\n```\n\n----------------------------------------\n\nTITLE: Mathematical Expression for Graph Laplacian Construction\nDESCRIPTION: Mathematical formulas showing the construction of unnormalized and normalized Graph Laplacian matrices used in Spectral Embedding.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/manifold.rst#2025-04-14_snippet_4\n\nLANGUAGE: math\nCODE:\n```\nL = D - A\\nL = D^{-\\frac{1}{2}} (D - A) D^{-\\frac{1}{2}}\n```\n\n----------------------------------------\n\nTITLE: Initializing AgglomerativeClustering with Single Linkage in Python\nDESCRIPTION: AgglomerativeClustering now supports Single Linkage clustering by setting the linkage parameter to 'single'.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nAgglomerativeClustering(linkage='single')\n```\n\n----------------------------------------\n\nTITLE: Referencing Scikit-learn Terminology in Python\nDESCRIPTION: These snippets demonstrate how to reference various terms used in scikit-learn documentation, including 'rectangular', 'feature', 'precomputed', 'feature extractor', 'pairwise metric', and 'target'. These terms are crucial for understanding scikit-learn's API and documentation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/glossary.rst#2025-04-14_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n:term:`rectangular`\n```\n\nLANGUAGE: Python\nCODE:\n```\n:term:`feature`\n```\n\nLANGUAGE: Python\nCODE:\n```\n:term:`precomputed`\n```\n\nLANGUAGE: Python\nCODE:\n```\n:term:`feature extractor`\n```\n\nLANGUAGE: Python\nCODE:\n```\n:term:`pairwise metric`\n```\n\nLANGUAGE: Python\nCODE:\n```\n:term:`target`\n```\n\nLANGUAGE: Python\nCODE:\n```\n:term:`multi-output`\n```\n\n----------------------------------------\n\nTITLE: Adding return_X_y Parameter to Datasets Function - Python\nDESCRIPTION: This code snippet details the addition of the 'return_X_y' parameter to the 'datasets.make_classification' function in Scikit-Learn. The inclusion of this parameter allows the function output to optionally return the features and labels as separate variables, improving flexibility for users. The default behavior remains unchanged, ensuring backward compatibility with existing code. No additional dependencies are required beyond Scikit-Learn itself.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/sklearn.datasets/30196.enhancement.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"\nNew parameter ``return_X_y`` added to :func:`datasets.make_classification`. The\ndefault value of the parameter does not change how the function behaves.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Referencing GaussianMixture Class in Python\nDESCRIPTION: Class reference syntax for the GaussianMixture class from the mixture module, demonstrating the float32 precision usage context.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/sklearn.mixture/30415.efficiency.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n~mixture.GaussianMixture\n```\n\n----------------------------------------\n\nTITLE: Fixing TransformerMixin index preservation in Python\nDESCRIPTION: When set_output(transform=\"pandas\") is used, TransformerMixin now maintains the index if the transform output is already a DataFrame.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nset_output(transform=\"pandas\")\n```\n\n----------------------------------------\n\nTITLE: Detailed Profiling Output with IPython's %prun Command\nDESCRIPTION: Extended output of the %prun command without module filtering, showing the complete picture of execution time including NumPy operations and revealing that performance is dominated by BLAS dot product operations.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/performance.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nIn [5] %prun NMF(n_components=16, tol=1e-2).fit(X)\n         16159 function calls in 1.840 CPU seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n     2833    0.653    0.000    0.653    0.000 {numpy.core._dotblas.dot}\n       46    0.651    0.014    1.636    0.036 nmf.py:151(_nls_subproblem)\n     1397    0.171    0.000    0.171    0.000 nmf.py:18(_pos)\n     2780    0.167    0.000    0.167    0.000 {method 'sum' of 'numpy.ndarray' objects}\n        1    0.064    0.064    1.840    1.840 nmf.py:352(fit_transform)\n     1542    0.043    0.000    0.043    0.000 {method 'flatten' of 'numpy.ndarray' objects}\n      337    0.019    0.000    0.019    0.000 {method 'all' of 'numpy.ndarray' objects}\n     2734    0.011    0.000    0.181    0.000 fromnumeric.py:1185(sum)\n        2    0.010    0.005    0.010    0.005 {numpy.linalg.lapack_lite.dgesdd}\n      748    0.009    0.000    0.065    0.000 nmf.py:28(norm)\n...\n```\n\n----------------------------------------\n\nTITLE: Loading scikit-learn model using pickle\nDESCRIPTION: Example demonstrating how to load a previously saved scikit-learn model from disk using pickle.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/model_persistence.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pickle import load\nwith open(\"filename.pkl\", \"rb\") as f:\n    clf = load(f)\n```\n\n----------------------------------------\n\nTITLE: Using Cython with OpenMP in Jupyter Notebooks\nDESCRIPTION: Jupyter magic commands to enable OpenMP support when compiling Cython code in notebooks, with options for different compilers.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/cython.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# For GCC and for clang\n%%cython --compile-args=-fopenmp --link-args=-fopenmp\n# For Microsoft's compilers\n%%cython --compile-args=/openmp --link-args=/openmp\n```\n\n----------------------------------------\n\nTITLE: Setting transform_alpha in DictionaryLearning in Python\nDESCRIPTION: API change in DictionaryLearning, MiniBatchDictionaryLearning, dict_learning and dict_learning_online where transform_alpha will be equal to alpha instead of 1.0 by default starting from version 1.2.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ndecomposition.DictionaryLearning(transform_alpha=alpha)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Composite Estimators in Jupyter Notebooks - Python\nDESCRIPTION: This snippet describes how to visualize estimators in a Jupyter notebook using ColumnTransformer. It shows how to set display options and export HTML representation of the estimator.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/compose.rst#2025-04-14_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ncolumn_trans  # doctest: +SKIP\\nfrom sklearn import set_config\\nset_config(display='text')  # doctest: +SKIP\\n# displays text representation in a jupyter context\\ncolumn_trans  # doctest: +SKIP\\nfrom sklearn.utils import estimator_html_repr\\nwith open('my_estimator.html', 'w') as f:  # doctest: +SKIP\\n    f.write(estimator_html_repr(clf))\n```\n\n----------------------------------------\n\nTITLE: Importing NumPy with Convention in Python\nDESCRIPTION: The conventional import statement for NumPy in Python, creating the np shorthand which is commonly used throughout scikit-learn and the data science ecosystem.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/glossary.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Initializing TfidfTransformer with smooth_idf=False\nDESCRIPTION: This snippet initializes a `TfidfTransformer` object with the `smooth_idf` parameter set to `False`. Disabling smoothing affects how inverse document frequency (IDF) is calculated.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n>>> transformer = TfidfTransformer(smooth_idf=False)\n>>> transformer\nTfidfTransformer(smooth_idf=False)\n```\n\n----------------------------------------\n\nTITLE: Mathematical Formula for Kernel Function\nDESCRIPTION: Mathematical representation of the fundamental kernel function relationship in reproducing kernel Hilbert spaces, showing how a positive definite kernel function k relates to the inner product of mapped data points in Hilbert space.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/kernel_approximation.rst#2025-04-14_snippet_1\n\nLANGUAGE: math\nCODE:\n```\nk(x,y) = \\langle \\phi(x), \\phi(y) \\rangle\n```\n\n----------------------------------------\n\nTITLE: Opting out of set_output API\nDESCRIPTION: This snippet shows how to opt-out of the `set_output` API for a custom transformer by setting `auto_wrap_output_keys=None` when defining the class.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/develop.rst#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass MyTransformer(TransformerMixin, BaseEstimator, auto_wrap_output_keys=None):\n\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        return X\n    def get_feature_names_out(self, input_features=None):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Installing scikit-learn using conda\nDESCRIPTION: Command to install scikit-learn using conda package manager from the conda-forge channel.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/README.rst#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge scikit-learn\n```\n\n----------------------------------------\n\nTITLE: Reconstructing Image from Patches using scikit-learn\nDESCRIPTION: This example shows how to reconstruct the original image from extracted patches using the reconstruct_from_patches_2d function. It demonstrates that the reconstruction is identical to the original image.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/feature_extraction.rst#2025-04-14_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nreconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3))\nnp.testing.assert_array_equal(one_image, reconstructed)\n```\n\n----------------------------------------\n\nTITLE: Adding _run_search method to BaseSearchCV in Python\nDESCRIPTION: Added experimental _run_search method to BaseSearchCV to support custom parameter search strategies. See implementations in GridSearchCV and RandomizedSearchCV.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nclass CustomSearchCV(BaseSearchCV):\n    def _run_search(self, evaluate_candidates):\n        # Custom search logic here\n        pass\n```\n\n----------------------------------------\n\nTITLE: Enabling MPS Support for PyTorch Tests\nDESCRIPTION: Command to run PyTest with MPS (Metal Performance Shaders) support enabled for PyTorch on macOS.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/array_api.rst#2025-04-14_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nPYTORCH_ENABLE_MPS_FALLBACK=1 pytest -k \"array_api\" -v\n```\n\n----------------------------------------\n\nTITLE: Running Benchmarks with Current Python Environment in scikit-learn\nDESCRIPTION: Command to run benchmarks using the version of scikit-learn already installed in the current Python environment instead of creating a new one.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_37\n\nLANGUAGE: bash\nCODE:\n```\nasv run --python=same\n```\n\n----------------------------------------\n\nTITLE: Setting up Conda Environment for Development\nDESCRIPTION: Commands for creating and activating a conda environment with necessary development dependencies.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n sklearn-dev -c conda-forge python numpy scipy cython \\\n    joblib threadpoolctl pytest compilers meson-python ninja\n\nconda activate sklearn-dev\npip install --editable . \\\n    --verbose --no-build-isolation \\\n    --config-settings editable-verbose=true\n```\n\n----------------------------------------\n\nTITLE: Updating Value Formatting in ConfusionMatrixDisplay and plot_confusion_matrix in scikit-learn\nDESCRIPTION: Changes the formatting of values in ConfusionMatrixDisplay.plot and plot_confusion_matrix to use the shorter format between '2g' and 'd'.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.ConfusionMatrixDisplay.plot()\nmetrics.plot_confusion_matrix()\n```\n\n----------------------------------------\n\nTITLE: Importing Imputer in Python for Missing Value Imputation\nDESCRIPTION: Demonstrates how to import the Imputer class from sklearn.preprocessing to impute missing values in sparse and dense matrices.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.14.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.preprocessing import Imputer\n```\n\n----------------------------------------\n\nTITLE: Feature Extractor API Requirements\nDESCRIPTION: Required method implementations for scikit-learn feature extractors that transform non-rectangular input data into rectangular feature arrays.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/glossary.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef fit(X)\ndef transform(X)\ndef get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Adding partial loading of svmlight files in Python\nDESCRIPTION: The load_svmlight_file function now allows loading a specific byte range of an svmlight formatted file.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\ndatasets.load_svmlight_file\n```\n\n----------------------------------------\n\nTITLE: Loading and using ONNX model with onnxruntime\nDESCRIPTION: This code demonstrates how to load a persisted ONNX model using onnxruntime and use it for inference. It shows the process of creating an InferenceSession and running predictions on test data.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/model_persistence.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom onnxruntime import InferenceSession\nwith open(\"filename.onnx\", \"rb\") as f:\n    onx = f.read()\nsess = InferenceSession(onx, providers=[\"CPUExecutionProvider\"])\npred_ort = sess.run(None, {\"X\": X_test.astype(numpy.float32)})[0]\n```\n\n----------------------------------------\n\nTITLE: Enabling Array API Support in Bash Environment\nDESCRIPTION: Sets the SCIPY_ARRAY_API environment variable to enable Array API support in SciPy and scikit-learn.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/array_api.rst#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport SCIPY_ARRAY_API=1\n```\n\n----------------------------------------\n\nTITLE: Log-posterior formula for LDA\nDESCRIPTION: This formula shows the log of the posterior probability for Linear Discriminant Analysis, which is a special case of QDA with shared covariance matrices.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/lda_qda.rst#2025-04-14_snippet_3\n\nLANGUAGE: math\nCODE:\n```\n\\log P(y=k | x) = -\\frac{1}{2} (x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k) + \\log P(y = k) + Cst\n```\n\n----------------------------------------\n\nTITLE: Creating a bicluster submatrix in Python with NumPy\nDESCRIPTION: This code demonstrates how to create a bicluster by extracting a submatrix from a data matrix using NumPy indexing. It creates a 10x10 matrix and extracts a 3x2 submatrix corresponding to specific rows and columns.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/biclustering.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> data = np.arange(100).reshape(10, 10)\n>>> rows = np.array([0, 2, 3])[:, np.newaxis]\n>>> columns = np.array([1, 2])\n>>> data[rows, columns]\narray([[ 1,  2],\n       [21, 22],\n       [31, 32]])\n```\n\n----------------------------------------\n\nTITLE: Updating OneHotEncoder drop_idx_ attribute in Python\nDESCRIPTION: OneHotEncoder.drop_idx_ now properly references the dropped category in the categories_ attribute when there are infrequent categories.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npreprocessing.OneHotEncoder.drop_idx_\n```\n\n----------------------------------------\n\nTITLE: Incorrect Implementation of __init__ for scikit-learn Estimator in Python\nDESCRIPTION: Demonstrates incorrect ways to implement the __init__ method, including modifying parameters and using inconsistent attribute names.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/develop.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, param1=1, param2=2, param3=3):\n    # WRONG: parameters should not be modified\n    if param1 > 1:\n        param2 += 1\n    self.param1 = param1\n    # WRONG: the object's attributes should have exactly the name of\n    # the argument in the constructor\n    self.param3 = param2\n```\n\n----------------------------------------\n\nTITLE: Initializing ARDRegression with Pandas Input in Python\nDESCRIPTION: Fix for sklearn.linear_model.ARDRegression to handle pandas input types when calling predict with return_std=True.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.4.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nARDRegression().predict(X, return_std=True)\n```\n\n----------------------------------------\n\nTITLE: Rand Index with Permuted Labels in scikit-learn in Python\nDESCRIPTION: This snippet shows that the Rand Index is invariant to permutations of the predicted labels. It demonstrates that changing the values of `labels_pred` while maintaining the same cluster structure does not affect the calculated Rand Index.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/clustering.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> labels_pred = [1, 1, 0, 0, 3, 3]\n>>> metrics.rand_score(labels_true, labels_pred)\n0.66...\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\n0.24...\n```\n\n----------------------------------------\n\nTITLE: Multi-label Data Structure Example\nDESCRIPTION: Example showing how multilabel targets are represented as 2D arrays or sparse matrices where each column is a binary target with 1 for positive labels and -1 or 0 for negative labels\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/glossary.rst#2025-04-14_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# 2D array for multilabel classification\n[\n  [1, -1],  # First sample: positive for first label, negative for second\n  [-1, 1],  # Second sample: negative for first label, positive for second\n  [1, 1]    # Third sample: positive for both labels\n]\n```\n\n----------------------------------------\n\nTITLE: Fixing named_estimators_ attribute in VotingClassifier and VotingRegressor\nDESCRIPTION: Corrects the named_estimators_ attribute mapping in VotingClassifier and VotingRegressor to properly handle dropped estimators. This fixes an issue where the mapping was incorrect when estimators were dropped.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nensemble.VotingClassifier.named_estimators_\n```\n\nLANGUAGE: Python\nCODE:\n```\nensemble.VotingRegressor.named_estimators_\n```\n\n----------------------------------------\n\nTITLE: Running Full Benchmark Suite in scikit-learn\nDESCRIPTION: Command to run the complete benchmark suite, comparing performance between upstream/main and HEAD. This can take up to two hours to complete.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_35\n\nLANGUAGE: bash\nCODE:\n```\nasv continuous upstream/main HEAD\n```\n\n----------------------------------------\n\nTITLE: Checking Attribute Existence in Python\nDESCRIPTION: Fix for sklearn.utils.metaestimators.if_delegate_has_method to properly check for attribute existence with NumPy arrays.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.0.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nsklearn.utils.metaestimators.if_delegate_has_method\n```\n\n----------------------------------------\n\nTITLE: Installing System Dependencies on Debian/Ubuntu\nDESCRIPTION: Commands for installing system-level dependencies when precompiled wheels are not available.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install cython3 python3-numpy python3-scipy\n```\n\n----------------------------------------\n\nTITLE: Updating scoring parameter names in scikit-learn Python\nDESCRIPTION: Error and loss names for scoring parameters are now prefixed with 'neg_' (e.g., neg_mean_squared_error). The unprefixed versions are deprecated and will be removed in version 0.20.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.18.rst#2025-04-14_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(estimator, X, y, scoring='neg_mean_squared_error')\n```\n\n----------------------------------------\n\nTITLE: Optimal Learning Rate Schedule for Classification in SGD\nDESCRIPTION: Formula for the optimal learning rate schedule used in classification tasks, where t is the time step and t_0 is determined heuristically.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/sgd.rst#2025-04-14_snippet_10\n\nLANGUAGE: math\nCODE:\n```\n\\eta^{(t)} = \\frac {1}{\\alpha  (t_0 + t)}\n```\n\n----------------------------------------\n\nTITLE: Mathematical formula for class conditional probability in LDA/QDA\nDESCRIPTION: This formula represents the probability density of the multivariate Gaussian distribution used to model P(x|y) in LDA and QDA.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/lda_qda.rst#2025-04-14_snippet_1\n\nLANGUAGE: math\nCODE:\n```\nP(x | y=k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}}\\exp\\left(-\\frac{1}{2} (x-\\mu_k)^t \\Sigma_k^{-1} (x-\\mu_k)\\right)\n```\n\n----------------------------------------\n\nTITLE: Fixing fit_predict inconsistency in mixture models in Python\nDESCRIPTION: The mixture.BaseMixture class and its derived classes (GaussianMixture and BayesianGaussianMixture) now have consistent behavior between fit_predict and fit.predict methods.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nmixture.BaseMixture\n```\n\nLANGUAGE: Python\nCODE:\n```\nmixture.GaussianMixture\n```\n\nLANGUAGE: Python\nCODE:\n```\nmixture.BayesianGaussianMixture\n```\n\n----------------------------------------\n\nTITLE: Regenerating Environment Files in Scikit-learn\nDESCRIPTION: Command to regenerate environment and lock files for CI systems after resolving conflicts.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython build_tools/update_environments_and_lock_files.py\n```\n\n----------------------------------------\n\nTITLE: Renaming W argument to Xt in NMF inverse_transform methods\nDESCRIPTION: The W argument in decomposition.NMF.inverse_transform and decomposition.MiniBatchNMF.inverse_transform is renamed to Xt and the old name will be removed in v1.5.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.3.rst#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nnmf.inverse_transform(Xt=W)  # new\nnmf.inverse_transform(W=W)  # old, deprecated\n```\n\n----------------------------------------\n\nTITLE: Running Single Test with pytest in Python\nDESCRIPTION: This command demonstrates how to run a single test using pytest by specifying the full path to the test file and the test function name.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/tips.rst#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest -v sklearn/linear_model/tests/test_logistic.py::test_sparsify\n```\n\n----------------------------------------\n\nTITLE: Adding multiclass support to matthews_corrcoef in Python\nDESCRIPTION: The matthews_corrcoef metric now supports multiclass classification in addition to binary classification.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.matthews_corrcoef\n```\n\n----------------------------------------\n\nTITLE: Fixing lower_bound_ calculation in BaseMixture in Python\nDESCRIPTION: Fixed a bug in mixture.BaseMixture and subclasses where lower_bound_ was not the max lower bound across all initializations when n_init > 1, but just the lower bound of the last initialization.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n# lower_bound_ now reports max across initializations\nprint(gmm.lower_bound_)\n```\n\n----------------------------------------\n\nTITLE: Alternative form of LDA log-posterior\nDESCRIPTION: This formula presents an alternative representation of the LDA log-posterior, showing its linear nature.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/lda_qda.rst#2025-04-14_snippet_4\n\nLANGUAGE: math\nCODE:\n```\n\\log P(y=k | x) = \\omega_k^t x + \\omega_{k0} + Cst\n```\n\n----------------------------------------\n\nTITLE: Fixing n_iter_ reporting in BaseMixture in Python\nDESCRIPTION: Fixed a bug in mixture.BaseMixture where the reported n_iter_ was missing an iteration. This affected GaussianMixture and BayesianGaussianMixture classes.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n# n_iter_ now correctly reports total iterations\nprint(gmm.n_iter_)\n```\n\n----------------------------------------\n\nTITLE: Using random_state Parameter in scikit-learn\nDESCRIPTION: Documentation for the random_state parameter which controls reproducibility in scikit-learn algorithms. It explains the three possible values: None (default), an integer seed, or a numpy.random.RandomState instance.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/glossary.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Example usage patterns for random_state parameter:\n\n# Default: Use global random state (different results each call)\nmodel = RandomForestClassifier(random_state=None)\n\n# Integer seed: Reproducible results across different calls\nmodel = RandomForestClassifier(random_state=42)\n\n# RandomState instance: Only affects users of the same instance\nimport numpy as np\nrandom_instance = np.random.RandomState(0)\nmodel = RandomForestClassifier(random_state=random_instance)\n```\n\n----------------------------------------\n\nTITLE: Adding callback parameter to decomposition.DictionaryLearning\nDESCRIPTION: The decomposition.DictionaryLearning class now accepts a callback parameter for consistency with the decomposition.dict_learning function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.3.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndecomposition.DictionaryLearning(callback=some_callback_function)\n```\n\n----------------------------------------\n\nTITLE: SGD Update Rule in scikit-learn\nDESCRIPTION: Mathematical formula for the SGD update rule that combines the gradient of the regularization term and the loss function to update model parameters.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/sgd.rst#2025-04-14_snippet_9\n\nLANGUAGE: math\nCODE:\n```\nw \\leftarrow w - \\eta \\left[\\alpha \\frac{\\partial R(w)}{\\partial w} + \\frac{\\partial L(w^T x_i + b, y_i)}{\\partial w}\\right]\n```\n\n----------------------------------------\n\nTITLE: Installing pre-commit hooks for scikit-learn in Bash\nDESCRIPTION: Commands to install pre-commit and set up hooks for running code style checks before each commit in scikit-learn development.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with Example Gallery in scikit-learn\nDESCRIPTION: Command to build the complete documentation including the example gallery, which runs all example scripts and requires more time.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nmake html\n```\n\n----------------------------------------\n\nTITLE: GraphicalLasso Mathematical Formula in LaTeX\nDESCRIPTION: Mathematical formulation for the GraphicalLasso algorithm showing the optimization problem for estimating the precision matrix K given sample covariance matrix S and regularization parameter alpha.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/covariance.rst#2025-04-14_snippet_5\n\nLANGUAGE: latex\nCODE:\n```\n\\hat{K} = \\mathrm{argmin}_K \\big(\\mathrm{tr} S K - \\mathrm{log} \\mathrm{det} K + \\alpha \\|K\\|_1\\big)\n```\n\n----------------------------------------\n\nTITLE: Importing Contributors List in Scikit-learn Release Notes\nDESCRIPTION: Includes a file containing the list of contributors to scikit-learn.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.24.rst#2025-04-14_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: _contributors.rst\n```\n\n----------------------------------------\n\nTITLE: Deprecating linear_model.lars_path in Python\nDESCRIPTION: The linear_model.lars_path function is being deprecated in version 0.23. Users are advised to use linear_model.lars_path_gram instead.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nlinear_model.lars_path_gram\n```\n\n----------------------------------------\n\nTITLE: Inverse Scaling Learning Rate Schedule for Regression in SGD\nDESCRIPTION: Formula for the inverse scaling learning rate schedule used in regression tasks, controlled by eta0 and power_t hyperparameters.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/sgd.rst#2025-04-14_snippet_11\n\nLANGUAGE: math\nCODE:\n```\n\\eta^{(t)} = \\frac{eta_0}{t^{power\\_t}}\n```\n\n----------------------------------------\n\nTITLE: Setting OpenMP Threads via Environment Variable in Bash\nDESCRIPTION: Example of setting the OMP_NUM_THREADS environment variable to control the number of OpenMP threads when running a Python script.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/computing/parallelism.rst#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nOMP_NUM_THREADS=4 python my_script.py\n```\n\n----------------------------------------\n\nTITLE: Loading the 20 newsgroups dataset in Python using scikit-learn\nDESCRIPTION: This snippet demonstrates how to load the 20 newsgroups dataset using the fetch_20newsgroups function from scikit-learn. It shows how to load the training subset and print the target names (categories).\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/twenty_newsgroups.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n>>> from pprint import pprint\n>>> pprint(list(newsgroups_train.target_names))\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']\n```\n\n----------------------------------------\n\nTITLE: Creating an IPython Configuration Profile\nDESCRIPTION: Command to create an IPython configuration profile, which is required for setting up the line_profiler extension with IPython 0.13+.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/performance.rst#2025-04-14_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nipython profile create\n```\n\n----------------------------------------\n\nTITLE: Installing and Configuring OpenMP on macOS\nDESCRIPTION: Commands for installing command line tools, Homebrew package manager, and configuring environment variables for OpenMP support on macOS.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nxcode-select --install\nbrew install libomp\n\nexport CC=/usr/bin/clang\nexport CXX=/usr/bin/clang++\nexport CPPFLAGS=\"$CPPFLAGS -Xpreprocessor -fopenmp\"\nexport CFLAGS=\"$CFLAGS -I/usr/local/opt/libomp/include\"\nexport CXXFLAGS=\"$CXXFLAGS -I/usr/local/opt/libomp/include\"\nexport LDFLAGS=\"$LDFLAGS -Wl,-rpath,/usr/local/opt/libomp/lib -L/usr/local/opt/libomp/lib -lomp\"\n\nmake clean\npip install --editable . \\\n    --verbose --no-build-isolation \\\n    --config-settings editable-verbose=true\n```\n\n----------------------------------------\n\nTITLE: Listing All Saved Benchmarks in scikit-learn\nDESCRIPTION: Command to display a list of all saved benchmark results from previous runs.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_39\n\nLANGUAGE: bash\nCODE:\n```\nasv show\n```\n\n----------------------------------------\n\nTITLE: Implementing __init__ for a scikit-learn Estimator in Python\nDESCRIPTION: Shows the proper implementation of the __init__ method for a scikit-learn estimator. It demonstrates setting parameters as attributes without any logic or input validation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/develop.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, param1=1, param2=2):\n    self.param1 = param1\n    self.param2 = param2\n```\n\n----------------------------------------\n\nTITLE: Fixing StandardScaler Partial Fit for Sparse Inputs in scikit-learn Preprocessing\nDESCRIPTION: Corrects the computation of statistics when calling partial_fit on sparse inputs in StandardScaler.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\npreprocessing.StandardScaler()\n```\n\n----------------------------------------\n\nTITLE: Updating BayesianRidge Covariance Matrix Estimation\nDESCRIPTION: Modifies the sigma_ calculation in BayesianRidge to use full SVD when number of samples is less than number of features, ensuring more robust statistical inference\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/sklearn.linear_model/31094.fix.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nlinear_model.BayesianRidge()\n```\n\n----------------------------------------\n\nTITLE: Exp-Sine-Squared Kernel Formula\nDESCRIPTION: Mathematical expression for the Exp-Sine-Squared kernel used for modeling periodic functions. It is parameterized by length-scale l and periodicity parameter p.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/gaussian_process.rst#2025-04-14_snippet_6\n\nLANGUAGE: latex\nCODE:\n```\nk(x_i, x_j) = \\text{exp}\\left(- \\frac{ 2\\sin^2(\\pi d(x_i, x_j) / p) }{ l^ 2} \\right)\n```\n\n----------------------------------------\n\nTITLE: Adding Jitter Parameter to LassoLars and Lars in scikit-learn\nDESCRIPTION: Adds a jitter parameter to LassoLars and Lars that adds random noise to the target, potentially improving stability in edge cases.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nlinear_model.LassoLars()\nlinear_model.Lars()\n```\n\n----------------------------------------\n\nTITLE: Changing to Benchmarks Directory in scikit-learn\nDESCRIPTION: Command to navigate to the asv_benchmarks directory where the performance benchmark suite is located.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\ncd asv_benchmarks\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies with pip in scikit-learn\nDESCRIPTION: Command to install all the required dependencies for building scikit-learn documentation, including Sphinx and various extensions needed for the build process.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\npip install sphinx sphinx-gallery numpydoc matplotlib Pillow pandas \\\n                polars scikit-image packaging seaborn sphinx-prompt \\\n                sphinxext-opengraph sphinx-copybutton plotly pooch \\\n                pydata-sphinx-theme sphinxcontrib-sass sphinx-design \\\n                sphinx-remove-toctrees\n```\n\n----------------------------------------\n\nTITLE: Configuring Git to Fetch Pull Requests as Remote Branches\nDESCRIPTION: This Git configuration allows developers to checkout pull requests as remote-tracking branches, making it easier to review and test proposed changes.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/tips.rst#2025-04-14_snippet_0\n\nLANGUAGE: git\nCODE:\n```\nfetch = +refs/pull/*/head:refs/remotes/upstream/pr/*\n```\n\n----------------------------------------\n\nTITLE: Representing Sparse Matrix Data in Python\nDESCRIPTION: This code snippet shows three lines of sparse matrix data. Each line represents a row, with entries in the format 'column:value'. The first number in each line is likely a label or identifier for the row.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/tests/data/svmlight_invalid.txt#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npython 2:2.5 10:-5.2 15:1.5\n2.0 5:1.0 12:-3\n3.0 20:27\n```\n\n----------------------------------------\n\nTITLE: Specifying Number of Samples per Cluster in make_blobs in Python\nDESCRIPTION: The make_blobs function now allows passing a list to n_samples to specify the number of samples for each cluster.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nmake_blobs(n_samples=[10, 20, 30])\n```\n\n----------------------------------------\n\nTITLE: Deprecating Public Attributes in SGD-based Classifiers and Regressors in scikit-learn\nDESCRIPTION: Deprecates public attributes standard_coef_, standard_intercept_, average_coef_, and average_intercept_ in SGD-based classifiers and regressors.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nlinear_model.SGDClassifier()\nlinear_model.SGDRegressor()\nlinear_model.PassiveAggressiveClassifier()\nlinear_model.PassiveAggressiveRegressor()\n```\n\n----------------------------------------\n\nTITLE: API Import Example\nDESCRIPTION: Example showing the recommended public API import pattern for scikit-learn.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.cluster import Birch  # correct\nfrom sklearn.cluster.birch import Birch  # deprecated\n```\n\n----------------------------------------\n\nTITLE: Verifying scikit-learn Installation in Conda Environment\nDESCRIPTION: Commands to check the scikit-learn installation by listing package information, checking all installed packages in the environment, and using Python to display version information.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/install_instructions_conda.rst#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda list scikit-learn  # show scikit-learn version and location\nconda list               # show all installed packages in the environment\npython -c \"import sklearn; sklearn.show_versions()\"\n```\n\n----------------------------------------\n\nTITLE: Profiling memory usage with %mprun\nDESCRIPTION: This example demonstrates how to use the ``%mprun`` magic command to profile the memory usage of a function line by line.  The `-f` flag specifies the function to profile, and the function is then called.  Output shows memory usage and increment per line.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/performance.rst#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"In [1] from example import my_func\\n\\n    In [2] %mprun -f my_func my_func()\\n    Filename: example.py\\n\\n    Line #    Mem usage  Increment   Line Contents\\n    ==============================================\\n         3                           @profile\\n         4      5.97 MB    0.00 MB   def my_func():\\n         5     13.61 MB    7.64 MB       a = [1] * (10 ** 6)\\n         6    166.20 MB  152.59 MB       b = [2] * (2 * 10 ** 7)\\n         7     13.61 MB -152.59 MB       del b\\n         8     13.61 MB    0.00 MB       return a\"\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Table of Contents for Release History\nDESCRIPTION: A toctree directive that creates a table of contents for all scikit-learn release documents, organized hierarchically with a depth of 2.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new.rst#2025-04-14_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   whats_new/v1.7.rst\n   whats_new/v1.6.rst\n   whats_new/v1.5.rst\n   whats_new/v1.4.rst\n   whats_new/v1.3.rst\n   whats_new/v1.2.rst\n   whats_new/v1.1.rst\n   whats_new/v1.0.rst\n   whats_new/v0.24.rst\n   whats_new/v0.23.rst\n   whats_new/v0.22.rst\n   whats_new/v0.21.rst\n   whats_new/v0.20.rst\n   whats_new/v0.19.rst\n   whats_new/v0.18.rst\n   whats_new/v0.17.rst\n   whats_new/v0.16.rst\n   whats_new/v0.15.rst\n   whats_new/v0.14.rst\n   whats_new/v0.13.rst\n   whats_new/older_versions.rst\n```\n\n----------------------------------------\n\nTITLE: Saving Benchmark Results with Commit Hash in scikit-learn\nDESCRIPTION: Command to save benchmark results when using an existing scikit-learn installation by specifying a commit hash.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_38\n\nLANGUAGE: bash\nCODE:\n```\nasv run --python=same --set-commit-hash=<commit hash>\n```\n\n----------------------------------------\n\nTITLE: Installing scikit-learn in development mode\nDESCRIPTION: Command to build and install scikit-learn in editable mode with verbose output to monitor the installation process.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install --editable . \\\n   --verbose --no-build-isolation \\\n   --config-settings editable-verbose=true\n```\n\n----------------------------------------\n\nTITLE: Using IPython's %prun Magic Command for Python Profiling\nDESCRIPTION: Demonstration of using IPython's %prun magic command to analyze performance profiles, showing function calls, execution times, and identifying hotspots in the NMF implementation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/performance.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nIn [5]: %prun -l nmf.py NMF(n_components=16, tol=1e-2).fit(X)\n         14496 function calls in 1.682 CPU seconds\n\n   Ordered by: internal time\n   List reduced from 90 to 9 due to restriction <'nmf.py'>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n       36    0.609    0.017    1.499    0.042 nmf.py:151(_nls_subproblem)\n     1263    0.157    0.000    0.157    0.000 nmf.py:18(_pos)\n        1    0.053    0.053    1.681    1.681 nmf.py:352(fit_transform)\n      673    0.008    0.000    0.057    0.000 nmf.py:28(norm)\n        1    0.006    0.006    0.047    0.047 nmf.py:42(_initialize_nmf)\n       36    0.001    0.000    0.010    0.000 nmf.py:36(_sparseness)\n       30    0.001    0.000    0.001    0.000 nmf.py:23(_neg)\n        1    0.000    0.000    0.000    0.000 nmf.py:337(__init__)\n        1    0.000    0.000    1.681    1.681 nmf.py:461(fit)\n```\n\n----------------------------------------\n\nTITLE: Matrix Factorization Formula for Dictionary Learning\nDESCRIPTION: Mathematical formula showing the optimization problem for dictionary learning. It minimizes the Frobenius norm of the reconstruction error plus an L1 regularization term, subject to column normalization constraints.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/decomposition.rst#2025-04-14_snippet_3\n\nLANGUAGE: math\nCODE:\n```\n(U^*, V^*) = \\underset{U, V}{\\operatorname{arg\\,min\\,}} & \\frac{1}{2} ||X-UV||_{\\text{Fro}}^2+\\alpha||U||_{1,1} \\\\ \\text{subject to } & ||V_k||_2 \\leq 1 \\text{ for all } 0 \\leq k < n_{\\mathrm{atoms}}\n```\n\n----------------------------------------\n\nTITLE: Fixing Normalizer with Max Norm in scikit-learn Preprocessing\nDESCRIPTION: Corrects the behavior of Normalizer with norm='max' to take the absolute value of maximum values before normalizing vectors.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\npreprocessing.Normalizer(norm='max')\n```\n\n----------------------------------------\n\nTITLE: Disabling Metadata Routing Configuration\nDESCRIPTION: Code showing how to disable the metadata routing configuration flag in scikit-learn.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/metadata_routing.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsklearn.set_config(enable_metadata_routing=False)\n```\n\n----------------------------------------\n\nTITLE: Creating Pandas DataFrame with Mixed Data Types\nDESCRIPTION: Generates a pandas DataFrame with continuous, positive, and categorical features, along with a continuous target series.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/minimal_reproducer.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n\nrng = np.random.RandomState(0)\nn_samples, n_features = 5, 5\nX = pd.DataFrame(\n    {\n        \"continuous_feature\": rng.randn(n_samples),\n        \"positive_feature\": rng.uniform(low=0.0, high=100.0, size=n_samples),\n        \"categorical_feature\": rng.choice([\"a\", \"b\", \"c\"], size=n_samples),\n    }\n)\ny = pd.Series(rng.randn(n_samples))\n```\n\n----------------------------------------\n\nTITLE: Optimizing MultiTaskLasso and Related Classes in scikit-learn\nDESCRIPTION: Improves the speed of MultiTaskLasso, MultiTaskLassoCV, MultiTaskElasticNet, and MultiTaskElasticNetCV by avoiding slower BLAS Level 2 calls on small arrays.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nlinear_model.MultiTaskLasso()\nlinear_model.MultiTaskLassoCV()\nlinear_model.MultiTaskElasticNet()\nlinear_model.MultiTaskElasticNetCV()\n```\n\n----------------------------------------\n\nTITLE: Defining RST Table of Contents for Supervised Learning\nDESCRIPTION: A reStructuredText directive that creates a nested table of contents linking to various supervised learning algorithm documentation pages. The toctree has a maximum depth of 2 levels and includes links to modules like linear models, SVM, neural networks, and other supervised learning algorithms.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/supervised_learning.rst#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _supervised-learning:\n\nSupervised learning\n-------------------\n\n.. toctree::\n    :maxdepth: 2\n\n    modules/linear_model\n    modules/lda_qda.rst\n    modules/kernel_ridge.rst\n    modules/svm\n    modules/sgd\n    modules/neighbors\n    modules/gaussian_process\n    modules/cross_decomposition.rst\n    modules/naive_bayes\n    modules/tree\n    modules/ensemble\n    modules/multiclass\n    modules/feature_selection.rst\n    modules/semi_supervised.rst\n    modules/isotonic.rst\n    modules/calibration.rst\n    modules/neural_networks_supervised\n```\n\n----------------------------------------\n\nTITLE: Creating Python virtual environment for scikit-learn development\nDESCRIPTION: Alternative to conda: commands to create and activate a Python virtual environment and install the necessary build dependencies using pip.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv sklearn-env\nsource sklearn-env/bin/activate\npip install wheel numpy scipy cython meson-python ninja\n```\n\n----------------------------------------\n\nTITLE: Running Static Analysis with Mypy in Scikit-learn\nDESCRIPTION: Command to perform static type checking using mypy on the sklearn codebase.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nmypy sklearn\n```\n\n----------------------------------------\n\nTITLE: JavaScript Window Location Redirect\nDESCRIPTION: JavaScript-based redirect that changes the window location to compose.html\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/pipeline.rst#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nwindow.location.href = \"./compose.html\";\n```\n\n----------------------------------------\n\nTITLE: Documenting Parameter Changes in reStructuredText\nDESCRIPTION: This example shows how to document changes to parameter default values in the docstring using the .. versionchanged:: directive in reStructuredText format.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_46\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. versionchanged:: 0.22\n   The default value for `n_clusters` will change from 5 to 10 in version 0.22.\n```\n\n----------------------------------------\n\nTITLE: Enabling Metadata Routing in scikit-learn (Python)\nDESCRIPTION: Enables the experimental Metadata Routing API in scikit-learn by setting the enable_metadata_routing flag to True.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/metadata_routing.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import sklearn\n>>> sklearn.set_config(enable_metadata_routing=True)\n```\n\n----------------------------------------\n\nTITLE: Rational Quadratic Kernel Formula\nDESCRIPTION: Mathematical expression for the Rational Quadratic kernel, which can be interpreted as a scale mixture of RBF kernels. It is parameterized by length-scale l and scale mixture parameter α.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/gaussian_process.rst#2025-04-14_snippet_5\n\nLANGUAGE: latex\nCODE:\n```\nk(x_i, x_j) = \\left(1 + \\frac{d(x_i, x_j)^2}{2\\alpha l^2}\\right)^{-\\alpha}\n```\n\n----------------------------------------\n\nTITLE: scikit-learn Documentation\nDESCRIPTION: This is a changelog/documentation file detailing updates to the scikit-learn library. No executable code snippets are present, only documentation of API changes, fixes, and enhancements.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.3.rst#2025-04-14_snippet_14\n\n\n\n----------------------------------------\n\nTITLE: Navigating to Documentation Directory in scikit-learn\nDESCRIPTION: Command to change directory to the doc folder, which is required before building the documentation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ncd doc\n```\n\n----------------------------------------\n\nTITLE: Deprecating linear_model.logistic_regression_path in Python\nDESCRIPTION: The linear_model.logistic_regression_path function is deprecated in version 0.21 and will be removed in version 0.23.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nlinear_model.logistic_regression_path\n```\n\n----------------------------------------\n\nTITLE: Running Array API Tests with PyTest\nDESCRIPTION: Command to run PyTest for Array API-related tests in scikit-learn, with verbose output.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/array_api.rst#2025-04-14_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npytest -k \"array_api\" -v\n```\n\n----------------------------------------\n\nTITLE: Loading and Profiling Non-Negative Matrix Factorization in IPython\nDESCRIPTION: Example of loading the digits dataset and profiling the NMF module using IPython's timing functionality to measure execution time without profiler overhead.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/performance.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: from sklearn.decomposition import NMF\n\nIn [2]: from sklearn.datasets import load_digits\n\nIn [3]: X, _ = load_digits(return_X_y=True)\n\nIn [4]: %timeit NMF(n_components=16, tol=1e-2).fit(X)\n1 loops, best of 3: 1.7 s per loop\n```\n\n----------------------------------------\n\nTITLE: Representing Multilabel Dataset in SVMlight Format\nDESCRIPTION: This snippet shows how to represent a multilabel dataset in SVMlight format. Each line represents an instance, starting with the labels, followed by feature:value pairs separated by spaces. Multiple labels are comma-separated, and features are numbered starting from 1.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/tests/data/svmlight_multilabel.txt#2025-04-14_snippet_0\n\nLANGUAGE: SVMlight\nCODE:\n```\n1,0 2:2.5   10:-5.2 15:1.5\n2 5:1.0 12:-3\n 2:3.5 11:26\n1,2 20:27\n```\n\n----------------------------------------\n\nTITLE: Fixing brier_score_loss for single class case in Python\nDESCRIPTION: The metrics.brier_score_loss function now correctly handles cases where there's only one class in y_true.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.brier_score_loss\n```\n\n----------------------------------------\n\nTITLE: Adding type checking to accept_sparse parameter in Python\nDESCRIPTION: Type checking was added to the accept_sparse parameter in validation methods. It now only accepts boolean, string, or list/tuple of strings. None is deprecated.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nsklearn.utils.validation\n```\n\n----------------------------------------\n\nTITLE: Styling Interactive ML Flowchart in CSS\nDESCRIPTION: CSS styles for the machine learning estimator selection flowchart, including viewport height setting, border styling, and dark mode support.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/machine_learning_map.rst#2025-04-14_snippet_0\n\nLANGUAGE: css\nCODE:\n```\n#sk-ml-map {\n  height: 80vh;\n  margin: 1.5rem 0;\n}\n\n#sk-ml-map svg {\n  height: 100%;\n  width: 100%;\n  border: 2px solid var(--pst-color-border);\n  border-radius: 0.5rem;\n}\n\nhtml[data-theme=\"dark\"] #sk-ml-map svg {\n  filter: invert(90%) hue-rotate(180deg);\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Intel Extension for Scikit-learn using Conda\nDESCRIPTION: Command to install the Intel-optimized version of Scikit-learn using conda package manager.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/install.rst#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconda install scikit-learn-intelex\n```\n\n----------------------------------------\n\nTITLE: Referencing the sklearn.svm Module in Python Documentation\nDESCRIPTION: This code snippet is a reStructuredText directive that creates a reference to the sklearn.svm module in the documentation. It's used to link to the SVM module's documentation within the scikit-learn library.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/svm/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:mod:`sklearn.svm`\n```\n\n----------------------------------------\n\nTITLE: Checking remote configurations for scikit-learn in Bash\nDESCRIPTION: Git command to verify the correct configuration of origin and upstream remotes for the scikit-learn repository.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit remote -v\n```\n\n----------------------------------------\n\nTITLE: Building PDF Documentation for scikit-learn\nDESCRIPTION: Command to generate the PDF version of the scikit-learn documentation using LaTeX.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\nmake latexpdf\n```\n\n----------------------------------------\n\nTITLE: Using Matplotlib in Tests with pytest Fixtures\nDESCRIPTION: Example of how to write tests that use matplotlib through the pyplot fixture in scikit-learn, which handles skipping tests when matplotlib is not installed.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef test_requiring_mpl_fixture(pyplot):\n    # you can now safely use matplotlib\n```\n\n----------------------------------------\n\nTITLE: Checking BLAS/LAPACK Implementation in scikit-learn\nDESCRIPTION: Command to display the BLAS/LAPACK implementation used by NumPy/SciPy/scikit-learn installations, which can help identify if optimized linear algebra libraries are being used.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/computing/computational_performance.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython -c \"import sklearn; sklearn.show_versions()\"\n```\n\n----------------------------------------\n\nTITLE: Loading a subset of categories from the 20 newsgroups dataset in Python\nDESCRIPTION: This snippet demonstrates how to load only specific categories from the 20 newsgroups dataset using the categories parameter in the fetch_20newsgroups function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/twenty_newsgroups.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> cats = ['alt.atheism', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n>>> list(newsgroups_train.target_names)\n['alt.atheism', 'sci.space']\n>>> newsgroups_train.filenames.shape\n(1073,)\n>>> newsgroups_train.target.shape\n(1073,)\n>>> newsgroups_train.target[:10]\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n```\n\n----------------------------------------\n\nTITLE: Dependency Specifications for Scikit-learn 32-bit Debian Build\nDESCRIPTION: A comprehensive list of pinned dependency versions for the scikit-learn project's 32-bit Debian environment. Includes testing frameworks, build tools, and core requirements with their specific versions and dependency relationships.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/build_tools/azure/debian_32bit_lock.txt#2025-04-14_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\ncoverage[toml]==7.8.0\n    # via pytest-cov\ncython==3.0.12\n    # via -r build_tools/azure/debian_32bit_requirements.txt\niniconfig==2.1.0\n    # via pytest\njoblib==1.4.2\n    # via -r build_tools/azure/debian_32bit_requirements.txt\nmeson==1.7.2\n    # via meson-python\nmeson-python==0.17.1\n    # via -r build_tools/azure/debian_32bit_requirements.txt\nninja==1.11.1.4\n    # via -r build_tools/azure/debian_32bit_requirements.txt\npackaging==24.2\n    # via\n    #   meson-python\n    #   pyproject-metadata\n    #   pytest\npluggy==1.5.0\n    # via pytest\npyproject-metadata==0.9.1\n    # via meson-python\npytest==8.3.5\n    # via\n    #   -r build_tools/azure/debian_32bit_requirements.txt\n    #   pytest-cov\npytest-cov==6.1.1\n    # via -r build_tools/azure/debian_32bit_requirements.txt\nthreadpoolctl==3.6.0\n    # via -r build_tools/azure/debian_32bit_requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Bash Command to Run HTTP Server\nDESCRIPTION: This snippet shows how to start a simple HTTP server using Python to serve the generated documentation files. It's used for previewing the documentation locally.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n\"python -m http.server -d _build/html\"\n```\n\n----------------------------------------\n\nTITLE: Renaming parameter in RANSACRegressor (Python)\nDESCRIPTION: Renames the 'base_estimator' parameter to 'estimator' in RANSACRegressor for improved readability and consistency. The old parameter name is deprecated and will be removed in version 1.3.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.1.rst#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.linear_model import RANSACRegressor\n\n# Old usage (deprecated)\nregressor = RANSACRegressor(base_estimator=...)\n\n# New usage\nregressor = RANSACRegressor(estimator=...)\n```\n\n----------------------------------------\n\nTITLE: Defining Expected Failed Checks in Scikit-learn's Estimator Checks\nDESCRIPTION: This code snippet demonstrates how to define expected failed checks when using the check_estimator or parametrize_with_checks functions in scikit-learn. It shows the structure of the dictionary used to specify which checks are expected to fail and the reasons for failure.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.6.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n{\n    \"check_name\": \"reason to mark this check as xfail\",\n}\n```\n\n----------------------------------------\n\nTITLE: Continuous Latent Variable Model Formula\nDESCRIPTION: Mathematical representation of the continuous latent variable model used in Factor Analysis, showing how observed data points are generated from latent variables with Gaussian noise.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/decomposition.rst#2025-04-14_snippet_4\n\nLANGUAGE: math\nCODE:\n```\nx_i = W h_i + \\mu + \\epsilon\n```\n\n----------------------------------------\n\nTITLE: Checking Thread Usage with threadpoolctl in Bash\nDESCRIPTION: Command to inspect how the number of threads used by BLAS & LAPACK libraries is affected by the OMP_NUM_THREADS environment variable.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/computing/parallelism.rst#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nOMP_NUM_THREADS=2 python -m threadpoolctl -i numpy scipy\n```\n\n----------------------------------------\n\nTITLE: Configuring line_profiler Extension for IPython\nDESCRIPTION: Configuration code to register the line_profiler extension in IPython, enabling the %lprun magic command for line-by-line code profiling.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/performance.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nc.TerminalIPythonApp.extensions.append('line_profiler')\nc.InteractiveShellApp.extensions.append('line_profiler')\n```\n\n----------------------------------------\n\nTITLE: Accessing RCV1 Dataset Shape in Python\nDESCRIPTION: This code shows how to access the shape of the feature matrix in the RCV1 dataset. The feature matrix is a scipy CSR sparse matrix with 804414 samples and 47236 features.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/rcv1.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> rcv1.data.shape\n(804414, 47236)\n```\n\n----------------------------------------\n\nTITLE: Safe Sparse Dot Operation with 3D+ Arrays\nDESCRIPTION: Enhancement to utils.extmath.safe_sparse_dot to support operations between 3D+ ndarray and sparse matrix.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nutils.extmath.safe_sparse_dot(ndarray_3d, sparse_matrix)\n```\n\n----------------------------------------\n\nTITLE: reStructuredText Class Referencing\nDESCRIPTION: This snippet illustrates how to create a link to a class's documentation using the `:class:` role within reStructuredText, specifying the full import path.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_20\n\nLANGUAGE: rst\nCODE:\n```\n\":class:`~sklearn.preprocessing.StandardScaler`\"\n```\n\n----------------------------------------\n\nTITLE: Mathematical Complexity Expression for HLLE\nDESCRIPTION: Mathematical expression showing the overall computational complexity of Hessian Eigenmapping (HLLE) algorithm.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/manifold.rst#2025-04-14_snippet_3\n\nLANGUAGE: math\nCODE:\n```\nO[D \\log(k) N \\log(N)] + O[D N k^3] + O[N d^6] + O[d N^2]\n```\n\n----------------------------------------\n\nTITLE: Referencing sklearn.model_selection Module in reStructuredText\nDESCRIPTION: This RST inline markup creates a cross-reference to the sklearn.model_selection module documentation. It allows readers to easily navigate to the detailed documentation of the module.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/model_selection/README.txt#2025-04-14_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n:mod:`sklearn.model_selection`\n```\n\n----------------------------------------\n\nTITLE: Cloning scikit-learn GitHub repository\nDESCRIPTION: Command to clone the scikit-learn repository from GitHub using Git and navigate to the project directory.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:scikit-learn/scikit-learn.git  # add --depth 1 if your connection is slow\ncd scikit-learn\n```\n\n----------------------------------------\n\nTITLE: Inverse Transform with OneHotEncoder and Unknown Categories in Python\nDESCRIPTION: Demonstrates how inverse_transform works with OneHotEncoder when drop parameter is used and unknown categories are encountered, showing how zeros are mapped back to dropped categories or None.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/preprocessing.rst#2025-04-14_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n>>> drop_enc = preprocessing.OneHotEncoder(drop='if_binary', sparse_output=False,\n...                                        handle_unknown='ignore').fit(X)\n>>> X_test = [['unknown', 'America', 'IE']]\n>>> X_trans = drop_enc.transform(X_test)\n>>> X_trans\narray([[0., 0., 0., 0., 0., 0., 0.]])\n>>> drop_enc.inverse_transform(X_trans)\narray([['female', None, None]], dtype=object)\n```\n\n----------------------------------------\n\nTITLE: reStructuredText Glossary Term Referencing\nDESCRIPTION: This snippet demonstrates how to link to terms defined in the glossary within reStructuredText documents, using the `:term:` role.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_18\n\nLANGUAGE: rst\nCODE:\n```\n\":term:`cross_validation`\"\n```\n\n----------------------------------------\n\nTITLE: Creating CI-like conda environment using conda-lock\nDESCRIPTION: Command to create a conda environment named scikit-learn-doc that matches the CI environment using a lock file. This ensures exact package version matching on compatible systems.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/tips.rst#2025-04-14_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nconda-lock install -n scikit-learn-doc build_tools/circle/doc_linux-64_conda.lock\n```\n\n----------------------------------------\n\nTITLE: Error handling for non-finite weights in linear models (Python)\nDESCRIPTION: Adds error messages when non-finite parameter weights are produced in ElasticNet and other linear model classes using coordinate descent optimization.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.1.rst#2025-04-14_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.linear_model import ElasticNet\n\n# This will now show an error message if non-finite weights are produced\nmodel = ElasticNet()\nmodel.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Loading Scikit-learn Toy Datasets in RST\nDESCRIPTION: ReStructuredText documentation showing the available toy dataset loading functions in scikit-learn. These functions provide immediate access to small standard datasets without requiring external downloads.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/datasets/toy_dataset.rst#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n\n   load_iris\n   load_diabetes\n   load_digits\n   load_linnerud\n   load_wine\n   load_breast_cancer\n```\n\n----------------------------------------\n\nTITLE: Configuring IPython to load memory_profiler extension\nDESCRIPTION: These lines should be added to the IPython configuration file (``~/.ipython/profile_default/ipython_config.py``) to register the memory_profiler extension. This enables the ``%memit`` and ``%mprun`` magic commands.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/performance.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\"c.TerminalIPythonApp.extensions.append('memory_profiler')\\n    c.InteractiveShellApp.extensions.append('memory_profiler')\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies on Debian/Ubuntu\nDESCRIPTION: Commands for installing build dependencies and Python packages on Debian-based Linux systems.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install build-essential python3-dev python3-pip\n\npip3 install cython\npip3 install --editable . \\\n    --verbose --no-build-isolation \\\n    --config-settings editable-verbose=true\n```\n\n----------------------------------------\n\nTITLE: Running Tests with All Seeds in Bash\nDESCRIPTION: Command to run a specific test with all admissible seed values (0-99) to ensure deterministic passing. This is useful when writing new test functions that use the global random seed fixture.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/computing/parallelism.rst#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nSKLEARN_TESTS_GLOBAL_RANDOM_SEED=\"all\" pytest -v -k test_your_test_name\n```\n\n----------------------------------------\n\nTITLE: Building scikit-learn on macOS with conda-forge compilers\nDESCRIPTION: Commands to activate the conda environment, clean previous builds, and install scikit-learn in development mode on macOS.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nconda activate sklearn-dev\nmake clean\npip install --editable . \\\n    --verbose --no-build-isolation \\\n    --config-settings editable-verbose=true\n```\n\n----------------------------------------\n\nTITLE: Retrieving RCV1 Target Names in Python\nDESCRIPTION: This snippet demonstrates how to retrieve the target names (topics) of the RCV1 dataset. There are 103 topics, each represented by a string.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/rcv1.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> rcv1.target_names[:3].tolist()  # doctest: +SKIP\n['E11', 'ECAT', 'M11']\n```\n\n----------------------------------------\n\nTITLE: Logistic Regression Cost Function in LaTeX\nDESCRIPTION: Mathematical formula for the logistic regression cost function, including regularization term. It uses sample weights, class weights, and supports multiple regularization options.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/linear_model.rst#2025-04-14_snippet_8\n\nLANGUAGE: latex\nCODE:\n```\n\\min_W -\\frac{1}{S}\\sum_{i=1}^n \\sum_{k=0}^{K-1} s_{ik} [y_i = k] \\log(\\hat{p}_k(X_i))\n    + \\frac{r(W)}{S C}\\,,\n```\n\n----------------------------------------\n\nTITLE: Debugging Segfaults in Cython Code with GDB\nDESCRIPTION: Command for using GDB to debug C code generated from Cython, particularly useful for investigating segmentation faults.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/cython.rst#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngdb --ex r --args python ./entrypoint_to_bug_reproducer.py\n```\n\n----------------------------------------\n\nTITLE: Parsing LIBSVM Formatted Data Examples\nDESCRIPTION: This snippet shows multiple examples of LIBSVM formatted data. It includes various label values, feature:value pairs, comments, and special cases such as empty lines and explicit zeros. The format is used for sparse datasets in machine learning applications.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/tests/data/svmlight_classification.txt#2025-04-14_snippet_0\n\nLANGUAGE: LIBSVM\nCODE:\n```\n# comment\n# note: the next line contains a tab\n1.0 3:2.5 \t   11:-5.2 16:1.5 # and an inline comment\n2.0 6:1.0 13:-3\n# another comment\n3.0 21:27\n4.0 2:1.234567890123456e10 # double precision value\n1.0     # empty line, all zeros\n2.0 3:0 # explicit zeros\n```\n\n----------------------------------------\n\nTITLE: Referencing sklearn.feature_selection Module in reStructuredText\nDESCRIPTION: This snippet uses reStructuredText syntax to create a reference to the sklearn.feature_selection module. It provides a link to the module's documentation within the context of the feature selection examples.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/feature_selection/README.txt#2025-04-14_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n:mod:`sklearn.feature_selection`\n```\n\n----------------------------------------\n\nTITLE: Creating Shell Alias for Cython Compilation\nDESCRIPTION: Bash alias definition for compiling Cython modules with optimized flags and generating annotated HTML reports to analyze CPython interactions.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/cython.rst#2025-04-14_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# You might want to add this alias to your shell script config.\nalias cythonX=\"cython -X language_level=3 -X boundscheck=False -X wraparound=False -X initializedcheck=False -X nonecheck=False -X cdivision=True\"\n\n# This generates `source.c` as if you had recompiled scikit-learn entirely.\ncythonX --annotate source.pyx\n```\n\n----------------------------------------\n\nTITLE: Synchronizing feature branch with upstream main in Bash\nDESCRIPTION: Git commands to fetch the latest changes from the upstream scikit-learn repository and merge them into your feature branch.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ngit fetch upstream\ngit merge upstream/main\n```\n\n----------------------------------------\n\nTITLE: Cloning scikit-learn fork in Bash\nDESCRIPTION: Command to clone your fork of the scikit-learn repository from GitHub to your local machine and change into the project directory.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:YourLogin/scikit-learn.git  # add --depth 1 if your connection is slow\ncd scikit-learn\n```\n\n----------------------------------------\n\nTITLE: BaseEstimator instantiation\nDESCRIPTION: This snippet shows how to instantiate a `BaseEstimator`.  This example is used to display the HTML representation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/develop.rst#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.base import BaseEstimator\n\nBaseEstimator()\n```\n\n----------------------------------------\n\nTITLE: Fixing OneVsOneClassifier decision_function scaling in Python\nDESCRIPTION: The multiclass.OneVsOneClassifier.decision_function method now produces consistent output regardless of whether it's evaluated on a single sample or a batch of samples.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\nmulticlass.OneVsOneClassifier.decision_function\n```\n\n----------------------------------------\n\nTITLE: Defining Examples Section in reStructuredText\nDESCRIPTION: This snippet creates an 'Examples' section in the documentation using reStructuredText syntax. It includes a reference to an example script demonstrating species distribution modeling.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/species_distributions.rst#2025-04-14_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py`\n```\n\n----------------------------------------\n\nTITLE: Writing RestructuredText Documentation for Bug Triaging\nDESCRIPTION: RestructuredText documentation defining the bug triaging and issue curation process for scikit-learn. Contains section headers, link references, and formatting for explaining issue management procedures.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/bug_triaging.rst#2025-04-14_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. _bug_triaging:\n\nBug triaging and issue curation\n===============================\n\nThe `issue tracker <https://github.com/scikit-learn/scikit-learn/issues>`_\nis important to the communication in the project: it helps\ndevelopers identify major projects to work on, as well as to discuss\npriorities.\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Directive for Linking to Libraries.io Subscription\nDESCRIPTION: A tip section that shows users how to subscribe to scikit-learn releases on libraries.io to receive notifications when new versions are released.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new.rst#2025-04-14_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. tip::\n\n   `Subscribe to scikit-learn releases <https://libraries.io/pypi/scikit-learn>`__\n   on libraries.io to be notified when new versions are released.\n```\n\n----------------------------------------\n\nTITLE: Running check_estimator on VotingClassifier and VotingRegressor\nDESCRIPTION: Adds default checks using utils.estimator_checks.check_estimator for VotingClassifier and VotingRegressor. This resolves issues with shape consistency during predict when underlying estimators output inconsistent array dimensions.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nutils.estimator_checks.check_estimator\n```\n\nLANGUAGE: Python\nCODE:\n```\nensemble.VotingClassifier\n```\n\nLANGUAGE: Python\nCODE:\n```\nensemble.VotingRegressor\n```\n\n----------------------------------------\n\nTITLE: Importing Standard Types for Cython in scikit-learn\nDESCRIPTION: Pattern for importing common numeric types from scikit-learn's centralized type definitions to maintain consistency across the codebase.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/cython.rst#2025-04-14_snippet_8\n\nLANGUAGE: cython\nCODE:\n```\nfrom sklearn.utils._typedefs cimport float32, float64\n```\n\n----------------------------------------\n\nTITLE: RBM Joint Probability Formula\nDESCRIPTION: Formula defining the joint probability of the RBM model in terms of energy function.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neural_networks_unsupervised.rst#2025-04-14_snippet_1\n\nLANGUAGE: math\nCODE:\n```\nP(\\mathbf{v}, \\mathbf{h}) = \\frac{e^{-E(\\mathbf{v}, \\mathbf{h})}}{Z}\n```\n\n----------------------------------------\n\nTITLE: Running Selective Examples in Documentation Build for scikit-learn\nDESCRIPTION: Command to build the documentation while running only examples that match a specific pattern, useful for testing changes to specific examples.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\nEXAMPLES_PATTERN=\"plot_calibration\" make html\n```\n\n----------------------------------------\n\nTITLE: Building Documentation without Examples in scikit-learn\nDESCRIPTION: Basic command to build the documentation without generating the example gallery, which is faster and sufficient for most documentation changes.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nmake\n```\n\n----------------------------------------\n\nTITLE: Deprecating a Parameter in Python using warnings\nDESCRIPTION: This snippet demonstrates how to deprecate a parameter by raising a FutureWarning. It renames the 'k' parameter to 'n_clusters' in a function and a class.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_43\n\nLANGUAGE: Python\nCODE:\n```\nimport warnings\n\ndef example_function(n_clusters=8, k=\"deprecated\"):\n    if k != \"deprecated\":\n        warnings.warn(\n            \"`k` was renamed to `n_clusters` in 0.13 and will be removed in 0.15\",\n            FutureWarning,\n        )\n        n_clusters = k\n\nclass ExampleEstimator(BaseEstimator):\n    def __init__(self, n_clusters=8, k='deprecated'):\n        self.n_clusters = n_clusters\n        self.k = k\n\n    def fit(self, X, y):\n        if self.k != \"deprecated\":\n            warnings.warn(\n                \"`k` was renamed to `n_clusters` in 0.13 and will be removed in 0.15.\",\n                FutureWarning,\n            )\n            self._n_clusters = self.k\n        else:\n            self._n_clusters = self.n_clusters\n```\n\n----------------------------------------\n\nTITLE: Optimizing Memory Usage in RidgeCV and RidgeClassifierCV in scikit-learn\nDESCRIPTION: Improves memory efficiency by avoiding allocation of large arrays for dual coefficients and error/LOO predictions unless store_cv_values is True.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nlinear_model.RidgeCV()\nlinear_model.RidgeClassifierCV()\n```\n\n----------------------------------------\n\nTITLE: Jaccard Index Formula for Bicluster Comparison in Mathematical Notation\nDESCRIPTION: Mathematical formula for the Jaccard index used to compare individual biclusters. The index is defined as the ratio of intersection size to the sum of sizes minus intersection size, achieving a minimum of 0 for no overlap and maximum of 1 for identical biclusters.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/biclustering.rst#2025-04-14_snippet_1\n\nLANGUAGE: math\nCODE:\n```\nJ(A, B) = \\frac{|A \\cap B|}{|A| + |B| - |A \\cap B|}\n```\n\n----------------------------------------\n\nTITLE: Accessing data attributes of the 20 newsgroups dataset in Python\nDESCRIPTION: This snippet shows how to access the filenames and target attributes of the loaded 20 newsgroups dataset. It demonstrates the shape of these attributes and displays a sample of the target values.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/twenty_newsgroups.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> newsgroups_train.filenames.shape\n(11314,)\n>>> newsgroups_train.target.shape\n(11314,)\n>>> newsgroups_train.target[:10]\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n```\n\n----------------------------------------\n\nTITLE: Mathematical Complexity for Isomap Algorithm\nDESCRIPTION: Documents the time complexity analysis for the three stages of the Isomap algorithm: nearest neighbor search, shortest-path graph search, and partial eigenvalue decomposition.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/manifold.rst#2025-04-14_snippet_0\n\nLANGUAGE: math\nCODE:\n```\nO[D \\log(k) N \\log(N)] + O[N^2(k + \\log(N))] + O[d N^2]\n```\n\n----------------------------------------\n\nTITLE: reStructuredText Section Referencing\nDESCRIPTION: This snippet demonstrates how to create and reference sections within reStructuredText documents using labels. This allows linking to specific parts of the documentation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_17\n\nLANGUAGE: rst\nCODE:\n```\n\".. _my-section:\n\nMy section\n----------\n\nThis is the text of the section.\n\nTo refer to itself use :ref:`my-section`.\"\n```\n\n----------------------------------------\n\nTITLE: Creating conda environment with compilers for macOS\nDESCRIPTION: Command to create a dedicated conda environment for scikit-learn development on macOS, including compilers with OpenMP support.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n sklearn-dev -c conda-forge python numpy scipy cython \\\n    joblib threadpoolctl pytest compilers llvm-openmp meson-python ninja\n```\n\n----------------------------------------\n\nTITLE: Float Type References in Python\nDESCRIPTION: Data type references showing the precision levels used in the GaussianMixture implementation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/sklearn.mixture/30415.efficiency.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfloat32\n```\n\nLANGUAGE: python\nCODE:\n```\nfloat64\n```\n\n----------------------------------------\n\nTITLE: RST Documentation for Pipelines and Composite Estimators\nDESCRIPTION: This RST documentation snippet provides a header and brief description for a section on pipelines and composite estimators in scikit-learn. It includes a reference tag and points users to the User Guide for more detailed information.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/compose/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _compose_examples:\n\nPipelines and composite estimators\n----------------------------------\n\nExamples of how to compose transformers and pipelines from other estimators. See the :ref:`User Guide <combining_estimators>`.\n```\n\n----------------------------------------\n\nTITLE: Referencing Model Selection Examples in reStructuredText\nDESCRIPTION: This RST directive creates a reference label for the model selection examples section. It allows other parts of the documentation to link to this section using the '_model_selection_examples' reference.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/model_selection/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _model_selection_examples:\n```\n\n----------------------------------------\n\nTITLE: Creating Author Display with Avatars in HTML\nDESCRIPTION: HTML markup for displaying scikit-learn project authors with GitHub avatars and links. The template includes styling for rounded avatar images and organizes contributors in a container div.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/communication_team.rst#2025-04-14_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!-- Generated by generate_authors_table.py -->\n<div class=\"sk-authors-container\">\n<style>\n  img.avatar {border-radius: 10px;}\n</style>\n<div>\n<a href='https://github.com/laurburke'><img src='https://avatars.githubusercontent.com/u/35973528?v=4' class='avatar' /></a> <br />\n<p>Lauren Burke-McCarthy</p>\n</div>\n<div>\n<a href='https://github.com/francoisgoupil'><img src='https://avatars.githubusercontent.com/u/98105626?v=4' class='avatar' /></a> <br />\n<p>François Goupil</p>\n</div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Deprecating jaccard_similarity_score in favor of jaccard_score in Python\nDESCRIPTION: The metrics.jaccard_similarity_score function is deprecated in favor of the more consistent metrics.jaccard_score. The former had issues with binary and multiclass targets.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.jaccard_score\n```\n\n----------------------------------------\n\nTITLE: Renaming scikits.learn to sklearn in Python\nDESCRIPTION: This snippet demonstrates how to rename the `scikits.learn` package to `sklearn` in Python code. This change is necessary when upgrading to scikit-learn 0.9 or later. The code uses `find` and `sed` commands to search for and replace all instances of `scikits.learn` with `sklearn` in Python files.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/older_versions.rst#2025-04-14_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n\"find -name \\\"*.py\\\" | xargs sed -i 's/\\bscikits.learn\\b/sklearn/g'\"\n```\n\n----------------------------------------\n\nTITLE: Viewing Specific Benchmark Report in scikit-learn\nDESCRIPTION: Command to view the benchmark report for a specific commit hash.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_40\n\nLANGUAGE: bash\nCODE:\n```\nasv show <commit hash>\n```\n\n----------------------------------------\n\nTITLE: RBM Conditional Independence Formulas\nDESCRIPTION: Mathematical expressions showing conditional independence assumptions in RBM.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neural_networks_unsupervised.rst#2025-04-14_snippet_2\n\nLANGUAGE: math\nCODE:\n```\nh_i \\bot h_j | \\mathbf{v} \\\\ v_i \\bot v_j | \\mathbf{h}\n```\n\n----------------------------------------\n\nTITLE: Mathematical Complexity Expression for MLLE\nDESCRIPTION: Mathematical expression showing the overall computational complexity of Modified Locally Linear Embedding (MLLE) algorithm.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/manifold.rst#2025-04-14_snippet_2\n\nLANGUAGE: math\nCODE:\n```\nO[D \\log(k) N \\log(N)] + O[D N k^3] + O[N (k-D) k^2] + O[d N^2]\n```\n\n----------------------------------------\n\nTITLE: Referencing sklearn.ensemble Module in reStructuredText\nDESCRIPTION: This snippet uses reStructuredText syntax to create a cross-reference to the sklearn.ensemble module. It allows readers to easily navigate to the detailed documentation of the ensemble module.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/ensemble/README.txt#2025-04-14_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n:mod:`sklearn.ensemble`\n```\n\n----------------------------------------\n\nTITLE: Checking RCV1 Target Shape in Python\nDESCRIPTION: This snippet demonstrates how to check the shape of the target values in the RCV1 dataset. The target values are stored in a scipy CSR sparse matrix with 804414 samples and 103 categories.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/rcv1.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> rcv1.target.shape\n(804414, 103)\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText anchor and module reference for sklearn.cluster\nDESCRIPTION: This code snippet consists of a ReStructuredText anchor definition and a module reference to the sklearn.cluster module. It's used for documentation organization and cross-referencing.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/cluster/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _cluster_examples:\n\nClustering\n----------\n\nExamples concerning the :mod:`sklearn.cluster` module.\n```\n\n----------------------------------------\n\nTITLE: Installing macOS command line tools\nDESCRIPTION: Command to install the required macOS command line developer tools before building scikit-learn from source.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nxcode-select --install\n```\n\n----------------------------------------\n\nTITLE: Enhancing Bagging Estimators with Metadata Routing\nDESCRIPTION: Adds support for metadata routing to BaggingClassifier and BaggingRegressor methods, enabling more flexible parameter passing to underlying estimators\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/metadata-routing/30833.feature.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nensemble.BaggingClassifier()\nensemble.BaggingRegressor()\n```\n\n----------------------------------------\n\nTITLE: Importing Restricted Boltzmann Machine in Python\nDESCRIPTION: Demonstrates importing BernoulliRBM from sklearn.neural_network for working with Restricted Boltzmann Machines.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.14.rst#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.neural_network import BernoulliRBM\n```\n\n----------------------------------------\n\nTITLE: Defining the Calibration examples section using reStructuredText\nDESCRIPTION: This snippet uses reStructuredText syntax to create a reference label for the calibration examples section and provides a brief description of what users can expect to find in this section of the documentation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/calibration/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _calibration_examples:\n\nCalibration\n-----------------------\n\nExamples illustrating the calibration of predicted probabilities of classifiers.\n```\n\n----------------------------------------\n\nTITLE: Referencing Frozen Estimators Section in RST\nDESCRIPTION: This RST directive creates a reference label for the 'Frozen Estimators' section in the documentation. It allows other parts of the documentation to link to this section using the '_frozen_examples' reference.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/frozen/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _frozen_examples:\n```\n\n----------------------------------------\n\nTITLE: Allowing non-finite features in Dummy estimators in Python\nDESCRIPTION: DummyClassifier and DummyRegressor now accept non-finite feature values.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\ndummy.DummyClassifier\ndummy.DummyRegressor\n```\n\n----------------------------------------\n\nTITLE: Setting up ARM64 development environment\nDESCRIPTION: Commands to prepare an ARM64 development environment using QEMU emulation and Docker.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/tips.rst#2025-04-14_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmkdir arm64\npushd arm64\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-aarch64.sh\ngit clone https://github.com/scikit-learn/scikit-learn.git\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm --privileged multiarch/qemu-user-static --reset -p yes\ndocker run -v `pwd`:/io --rm -it arm64v8/ubuntu /bin/bash\n```\n\nLANGUAGE: bash\nCODE:\n```\nbash Miniforge3-Linux-aarch64.sh\n```\n\nLANGUAGE: bash\nCODE:\n```\n/io/miniforge3/bin/conda init\nsource /root/.bashrc\n```\n\n----------------------------------------\n\nTITLE: Example See Also Docstring\nDESCRIPTION: This snippet demonstrates how to format the \"See Also\" section in docstrings, providing related classes/functions. Each reference includes a colon and a concise explanation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_15\n\nLANGUAGE: text\nCODE:\n```\n\"See Also\n--------\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\"\n```\n\n----------------------------------------\n\nTITLE: Formatting a Changelog Entry in RST\nDESCRIPTION: This snippet demonstrates how to format a changelog entry for classes in scikit-learn using reStructuredText (RST). The entry includes a description of added functionality for specific classifiers, along with attribution to the contributor.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/README.md#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n\"\"\"\n- :class:`ensemble.ExtraTreesClassifier` and :class:`ensemble.ExtraTreesRegressor`\n  now supports missing values in the data matrix `X`. Missing-values are\n  handled by randomly moving all of the samples to the left, or right child\n  node as the tree is traversed.\n  By :user:`Adam Li <adam2392>`\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Synchronizing main branch with upstream in Bash\nDESCRIPTION: Git commands to synchronize your local main branch with the latest changes from the upstream scikit-learn repository.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout main\ngit fetch upstream\ngit merge upstream/main\n```\n\n----------------------------------------\n\nTITLE: Mathematical Formulation of SVR Primal Problem\nDESCRIPTION: Defines the primal optimization problem for epsilon-SVR with weight vector w, bias b, and slack variables zeta. The objective function minimizes the model complexity and training errors, subject to constraints on prediction accuracy.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/svm.rst#2025-04-14_snippet_8\n\nLANGUAGE: math\nCODE:\n```\n\\min_ {w, b, \\zeta, \\zeta^*} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} (\\zeta_i + \\zeta_i^*)\n\n\n\\textrm {subject to } & y_i - w^T \\phi (x_i) - b \\leq \\varepsilon + \\zeta_i,\\\\\n                      & w^T \\phi (x_i) + b - y_i \\leq \\varepsilon + \\zeta_i^*,\\\\\n                      & \\zeta_i, \\zeta_i^* \\geq 0, i=1, ..., n\n```\n\n----------------------------------------\n\nTITLE: Referencing sklearn.frozen Module in RST\nDESCRIPTION: This RST directive creates a cross-reference to the sklearn.frozen module in the documentation. It allows users to easily navigate to the module's documentation for more detailed information.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/frozen/README.txt#2025-04-14_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n:mod:`sklearn.frozen`\n```\n\n----------------------------------------\n\nTITLE: Badge Substitution Definitions for Documentation\nDESCRIPTION: Defines various badge substitutions for marking documentation sections with semantic labels\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/_contributors.rst#2025-04-14_snippet_2\n\nLANGUAGE: ReST\nCODE:\n```\n.. |MajorFeature| replace:: :raw-html:`<span class=\"badge text-bg-success\">Major Feature</span>` :raw-latex:`{\\small\\sc [Major Feature]}`\n```\n\n----------------------------------------\n\nTITLE: Mathematical Complexity for LLE Algorithm\nDESCRIPTION: Outlines the computational complexity for the three main stages of the standard Locally Linear Embedding algorithm.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/manifold.rst#2025-04-14_snippet_1\n\nLANGUAGE: math\nCODE:\n```\nO[D \\log(k) N \\log(N)] + O[D N k^3] + O[d N^2]\n```\n\n----------------------------------------\n\nTITLE: Referencing the sklearn.decomposition module in RST\nDESCRIPTION: This RST directive references the sklearn.decomposition module using the :mod: role, which creates a link to the module's documentation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/decomposition/README.txt#2025-04-14_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n:mod:`sklearn.decomposition`\n```\n\n----------------------------------------\n\nTITLE: Creating conda environment with scikit-learn build dependencies\nDESCRIPTION: Command to create a dedicated conda environment with all necessary dependencies (NumPy, SciPy, Cython, meson-python, Ninja) for building scikit-learn from source.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n sklearn-env -c conda-forge python numpy scipy cython meson-python ninja\n```\n\n----------------------------------------\n\nTITLE: Enhancing check_estimator to prevent attribute setting in Python\nDESCRIPTION: The check_estimator function now attempts to ensure that methods like transform and predict do not set attributes on the estimator.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nutils.estimator_checks.check_estimator\n```\n\n----------------------------------------\n\nTITLE: Importing AdaBoost Classifiers and Regressors in Python\nDESCRIPTION: Shows how to import AdaBoostClassifier and AdaBoostRegressor from sklearn.ensemble for ensemble learning.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.14.rst#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n```\n\n----------------------------------------\n\nTITLE: Reinstalling Scikit-learn on Windows with Existing Installation\nDESCRIPTION: PowerShell command to reinstall scikit-learn while ignoring the previous broken installation, useful after fixing the Windows long path issue.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/install.rst#2025-04-14_snippet_5\n\nLANGUAGE: powershell\nCODE:\n```\npip install --exists-action=i scikit-learn\n```\n\n----------------------------------------\n\nTITLE: Logistic Sigmoid Function Definition\nDESCRIPTION: Mathematical definition of the logistic sigmoid activation function used in Bernoulli RBM.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neural_networks_unsupervised.rst#2025-04-14_snippet_4\n\nLANGUAGE: math\nCODE:\n```\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n```\n\n----------------------------------------\n\nTITLE: Referencing sklearn.multioutput Module in ReStructuredText\nDESCRIPTION: This snippet uses reStructuredText syntax to create a reference to the sklearn.multioutput module. It provides a link to the module's documentation for users seeking more detailed information.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/multioutput/README.txt#2025-04-14_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n:mod:`sklearn.multioutput`\n```\n\n----------------------------------------\n\nTITLE: Activating Cython Debug Directives in scikit-learn\nDESCRIPTION: Command to enable debug Cython directives during compilation, which activates bound checking for easier debugging.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/cython.rst#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport SKLEARN_ENABLE_DEBUG_CYTHON_DIRECTIVES=1\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies on Red Hat/CentOS\nDESCRIPTION: Command for installing required dependencies on Red Hat-based Linux systems.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum -y install gcc gcc-c++ python3-devel numpy scipy\n```\n\n----------------------------------------\n\nTITLE: Accessing Python Print in Cython nogil Context\nDESCRIPTION: Technique to temporarily reacquire the GIL in a nogil context to print debug information.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/cython.rst#2025-04-14_snippet_3\n\nLANGUAGE: cython\nCODE:\n```\nwith gil:\n    print(state_to_print)\n```\n\n----------------------------------------\n\nTITLE: RST HTML Strike Tag Definition\nDESCRIPTION: ReStructuredText directive defining HTML strike-through tag opening and closing elements for documentation formatting\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/roadmap.rst#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. |ss| raw:: html\n\n   <strike>\n\n.. |se| raw:: html\n\n   </strike>\n```\n\n----------------------------------------\n\nTITLE: RST External Resources Documentation\nDESCRIPTION: ReStructuredText markup defining the structure and content of the external resources documentation page for scikit-learn. Includes sections for MOOC information, video resources, and external tutorials.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/presentations.rst#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _external_resources:\n\n===========================================\nExternal Resources, Videos and Talks\n===========================================\n\nThe scikit-learn MOOC\n=====================\n\nIf you are new to scikit-learn, or looking to strengthen your understanding,\nwe highly recommend the **scikit-learn MOOC (Massive Open Online Course)**.\n\nThe MOOC, created and maintained by some of the scikit-learn core-contributors,\nis **free of charge** and is designed to help learners of all levels master\nmachine learning using scikit-learn. It covers topics\nfrom the fundamental machine learning concepts to more advanced areas like\npredictive modeling pipelines and model evaluation.\n\nThe course materials are available on the\n`scikit-learn MOOC website <https://inria.github.io/scikit-learn-mooc/>`_.\n\nThis course is also hosted on the `FUN platform\n<https://www.fun-mooc.fr/en/courses/machine-learning-python-scikit-learn/>`_,\nwhich additionally makes the content interactive without the need to install\nanything, and gives access to a discussion forum.\n\nThe videos are available on the\n`Inria Learning Lab channel <https://www.youtube.com/@inrialearninglab>`_\nin a\n`playlist <https://www.youtube.com/playlist?list=PL2okA_2qDJ-m44KooOI7x8tu85wr4ez4f>`__.\n\n.. _videos:\n\nVideos\n======\n\n- The `scikit-learn YouTube channel <https://www.youtube.com/@scikit-learn>`_\n  features a\n  `playlist <https://www.youtube.com/@scikit-learn/playlists>`__\n  of videos\n  showcasing talks by maintainers\n  and community members.\n\nNew to Scientific Python?\n==========================\n\nFor those that are still new to the scientific Python ecosystem, we highly\nrecommend the `Python Scientific Lecture Notes\n<https://scipy-lectures.org>`_. This will help you find your footing a\nbit and will definitely improve your scikit-learn experience.  A basic\nunderstanding of NumPy arrays is recommended to make the most of scikit-learn.\n\nExternal Tutorials\n===================\n\nThere are several online tutorials available which are geared toward\nspecific subject areas:\n\n- `Machine Learning for NeuroImaging in Python <https://nilearn.github.io/>`_\n- `Machine Learning for Astronomical Data Analysis <https://github.com/astroML/sklearn_tutorial>`_\n```\n\n----------------------------------------\n\nTITLE: Deprecating presort parameter in GradientBoosting estimators\nDESCRIPTION: Deprecates the presort parameter in GradientBoostingClassifier and GradientBoostingRegressor. The parameter now has no effect, and users are recommended to use HistGradientBoostingClassifier and HistGradientBoostingRegressor instead.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nensemble.GradientBoostingClassifier\n```\n\nLANGUAGE: Python\nCODE:\n```\nensemble.GradientBoostingRegressor\n```\n\nLANGUAGE: Python\nCODE:\n```\nensemble.HistGradientBoostingClassifier\n```\n\nLANGUAGE: Python\nCODE:\n```\nensemble.HistGradientBoostingRegressor\n```\n\n----------------------------------------\n\nTITLE: Table of Contents Configuration using toctree in Scikit-learn\nDESCRIPTION: This code snippet uses the 'toctree' directive in reStructuredText (used by Sphinx for documentation generation) to create a table of contents for the Unsupervised Learning section of scikit-learn's documentation. The ':maxdepth: 2' option specifies that the table of contents should display headings up to two levels deep. The listed modules represent the different areas of unsupervised learning covered in this section.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/unsupervised_learning.rst#2025-04-14_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 2\n\n    modules/mixture\n    modules/manifold\n    modules/clustering\n    modules/biclustering\n    modules/decomposition\n    modules/covariance\n    modules/outlier_detection\n    modules/density\n    modules/neural_networks_unsupervised\n```\n\n----------------------------------------\n\nTITLE: Documenting Deprecation in reStructuredText\nDESCRIPTION: This snippet demonstrates how to document deprecation in the docstring using the .. deprecated:: directive in reStructuredText format.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_45\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. deprecated:: 0.13\n   ``k`` was renamed to ``n_clusters`` in version 0.13 and will be removed\n   in 0.15.\n```\n\n----------------------------------------\n\nTITLE: Using correlation_models and regression_models from Legacy Gaussian Processes\nDESCRIPTION: Deprecation notice for the correlation_models and regression_models from the legacy gaussian processes implementation. These were belatedly deprecated in version 0.19.1.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.19.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncorrelation_models  # deprecated\nregression_models  # deprecated\n```\n\n----------------------------------------\n\nTITLE: Converting Array API Estimator to NumPy\nDESCRIPTION: Shows how to convert an estimator with Array API attributes back to NumPy arrays using _estimator_with_converted_arrays utility.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/array_api.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.utils._array_api import _estimator_with_converted_arrays\n>>> cupy_to_ndarray = lambda array : array.get()\n>>> lda_np = _estimator_with_converted_arrays(lda, cupy_to_ndarray)\n>>> X_trans = lda_np.transform(X_np)\n>>> type(X_trans)\n<class 'numpy.ndarray'>\n```\n\n----------------------------------------\n\nTITLE: Checking conda-forge compiler installation\nDESCRIPTION: Command to verify that the compilers meta-package and OpenMP support are properly installed in the conda environment.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nconda list\n```\n\n----------------------------------------\n\nTITLE: RBM Energy Function Formula\nDESCRIPTION: Mathematical formula describing the energy function that measures the quality of joint assignment in RBM.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/neural_networks_unsupervised.rst#2025-04-14_snippet_0\n\nLANGUAGE: math\nCODE:\n```\nE(\\mathbf{v}, \\mathbf{h}) = -\\sum_i \\sum_j w_{ij}v_ih_j - \\sum_i b_iv_i - \\sum_j c_jh_j\n```\n\n----------------------------------------\n\nTITLE: Deprecating values key in partial_dependence return value\nDESCRIPTION: The inspection.partial_dependence function now returns a utils.Bunch with a new grid_values key. The old values key is deprecated in favor of grid_values and will be removed in 1.5.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.3.rst#2025-04-14_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nresult = inspection.partial_dependence(estimator, X, features)\nprint(result.grid_values)  # new\nprint(result.values)  # old, deprecated\n```\n\n----------------------------------------\n\nTITLE: Defining ReStructuredText Table of Contents for Developer Guide\nDESCRIPTION: ReStructuredText markup defining the structure and navigation for the Scikit-learn developer documentation, including section reference and table of contents directive with links to key developer resources.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/index.rst#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _developers_guide:\n\n=================\nDeveloper's Guide\n=================\n\n.. toctree::\n\n   contributing\n   minimal_reproducer\n   develop\n   tips\n   utilities\n   performance\n   cython\n   advanced_installation\n   bug_triaging\n   maintainer\n   plotting\n```\n\n----------------------------------------\n\nTITLE: Updating pairwise_distances Behavior for Mahalanobis and Seuclidean Distances in scikit-learn\nDESCRIPTION: Changes the behavior of pairwise_distances to require manual computation of VI and V parameters for Mahalanobis and seuclidean distances when Y is passed.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.pairwise_distances()\n```\n\n----------------------------------------\n\nTITLE: Checking Python architecture for Windows build\nDESCRIPTION: Command to determine if you're running 64-bit or 32-bit Python by displaying the size of a pointer in bits, which is needed for selecting the correct Visual Studio build tools.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -c \"import struct; print(struct.calcsize('P') * 8)\"\n```\n\n----------------------------------------\n\nTITLE: Importing OpenMP Functions in scikit-learn Cython Code\nDESCRIPTION: Pattern for safely importing OpenMP functions in a way that maintains compatibility with builds that don't include OpenMP support.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/cython.rst#2025-04-14_snippet_7\n\nLANGUAGE: cython\nCODE:\n```\nfrom sklearn.utils._openmp_helpers cimport omp_get_max_threads\nmax_threads = omp_get_max_threads()\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Model Selection\nDESCRIPTION: ReStructuredText markup defining the documentation structure for model selection and evaluation topics in scikit-learn, including a section label and toctree directive listing relevant module documentation pages.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/model_selection.rst#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _model_selection:\n\nModel selection and evaluation\n------------------------------\n\n.. toctree::\n    :maxdepth: 2\n\n    modules/cross_validation\n    modules/grid_search\n    modules/classification_threshold\n    modules/model_evaluation\n    modules/learning_curve\n```\n\n----------------------------------------\n\nTITLE: Using line_profiler with %lprun in IPython\nDESCRIPTION: Example of using the %lprun magic command to perform detailed line-by-line profiling of the _nls_subproblem function in scikit-learn's NMF implementation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/performance.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: from sklearn.datasets import load_digits\n\nIn [2]: from sklearn.decomposition import NMF\n  ... : from sklearn.decomposition._nmf import _nls_subproblem\n\nIn [3]: X, _ = load_digits(return_X_y=True)\n\nIn [4]: %lprun -f _nls_subproblem NMF(n_components=16, tol=1e-2).fit(X)\nTimer unit: 1e-06 s\n\nFile: sklearn/decomposition/nmf.py\nFunction: _nls_subproblem at line 137\nTotal time: 1.73153 s\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n   137                                           def _nls_subproblem(V, W, H_init, tol, max_iter):\n   138                                               \"\"\"Non-negative least square solver\n   ...\n   170                                               \"\"\"\n   171        48         5863    122.1      0.3      if (H_init < 0).any():\n   172                                                   raise ValueError(\"Negative values in H_init passed to NLS solver.\")\n   173\n   174        48          139      2.9      0.0      H = H_init\n   175        48       112141   2336.3      5.8      WtV = np.dot(W.T, V)\n   176        48        16144    336.3      0.8      WtW = np.dot(W.T, W)\n   177\n   178                                               # values justified in the paper\n   179        48          144      3.0      0.0      alpha = 1\n   180        48          113      2.4      0.0      beta = 0.1\n```\n\n----------------------------------------\n\nTITLE: OneHotEncoder infrequent category grouping parameters\nDESCRIPTION: Shows the new parameters for grouping infrequent categories in OneHotEncoder.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.1.rst#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmin_frequency\nmax_categories\n```\n\n----------------------------------------\n\nTITLE: Configuring Git Blame to Ignore Code Style Migration Commit\nDESCRIPTION: This bash command configures git blame to ignore the commit that migrated the code style to black. This helps maintain useful git blame output by excluding large-scale formatting changes.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_48\n\nLANGUAGE: bash\nCODE:\n```\ngit config blame.ignoreRevsFile .git-blame-ignore-revs\n```\n\n----------------------------------------\n\nTITLE: Installing line_profiler for Detailed Python Line Profiling\nDESCRIPTION: Instructions for installing the line_profiler package to enable detailed line-by-line profiling of Python code.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/performance.rst#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install line_profiler\n```\n\n----------------------------------------\n\nTITLE: RST Section Headers for Examples Documentation\nDESCRIPTION: ReStructuredText markup defining the section headers and references for the scikit-learn examples documentation page.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _general_examples:\n\nExamples\n========\n\n```\n\n----------------------------------------\n\nTITLE: Defining TOC Structure in RST for scikit-learn Computing Documentation\nDESCRIPTION: ReStructuredText markup defining the table of contents structure for scikit-learn computing documentation. Uses toctree directive with maxdepth of 2 to organize three main computing-related sections.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/computing.rst#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 2\n\n    computing/scaling_strategies\n    computing/computational_performance\n    computing/parallelism\n```\n\n----------------------------------------\n\nTITLE: Running valgrind for memory debugging\nDESCRIPTION: Command to run valgrind memory checker on Python scripts, using suppressions file to filter out Python interpreter-related issues.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/tips.rst#2025-04-14_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nvalgrind -v --suppressions=valgrind-python.supp python my_test_script.py\n```\n\n----------------------------------------\n\nTITLE: Referencing Preprocessing Examples Section in reStructuredText\nDESCRIPTION: This snippet creates a reference label for the preprocessing examples section in the documentation. It allows other parts of the documentation to link to this section easily.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/preprocessing/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _preprocessing_examples:\n```\n\n----------------------------------------\n\nTITLE: Defining ReStructuredText Label for Multioutput Examples\nDESCRIPTION: This snippet creates a reference label for the multioutput examples section using reStructuredText syntax. It allows other parts of the documentation to link to this section.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/multioutput/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _multioutput_examples:\n```\n\n----------------------------------------\n\nTITLE: Updating array_api_compat directory using maintenance script in scikit-learn\nDESCRIPTION: This shell command updates the array_api_compat directory in the scikit-learn project using a maintenance tool script. The script is located in the maint_tools directory and is named vendor_array_api_compat.sh.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/externals/array_api_compat/README.md#2025-04-14_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmaint_tools/vendor_array_api_compat.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring pytest with IPython Debugger in Bash\nDESCRIPTION: This command sets up an alias to run pytest with the IPython debugger, which provides a more feature-rich debugging experience when tests fail.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/tips.rst#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npytest --pdbcls=IPython.terminal.debugger:TerminalPdb --capture no\n```\n\n----------------------------------------\n\nTITLE: Microsoft Visual C++ Runtime Distribution Paths\nDESCRIPTION: File paths and locations for Microsoft Visual C++ Runtime components that can and cannot be distributed with programs.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/build_tools/wheels/LICENSE_windows.txt#2025-04-14_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\redist\\debug_nonredist\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\redist\\onecore\\debug_nonredist\nVC\\atlmfc\\lib\\mfcmifc80.dll\nVC\\atlmfc\\lib\\amd64\\mfcmifc80.dll\n```\n\n----------------------------------------\n\nTITLE: Installing development dependencies for scikit-learn in Bash\nDESCRIPTION: Command to install the necessary Python packages for scikit-learn development using pip.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install pytest pytest-cov ruff mypy numpydoc black==24.3.0\n```\n\n----------------------------------------\n\nTITLE: Pip Requirements Lock File for Ubuntu Atlas Build\nDESCRIPTION: A complete list of pinned Python package dependencies and their versions required for the Ubuntu Atlas build environment. Includes core packages like Cython, pytest, joblib and their dependencies with specific version constraints.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/build_tools/azure/ubuntu_atlas_lock.txt#2025-04-14_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\ncython==3.0.10\nexceptiongroup==1.2.2\nexecnet==2.1.1\niniconfig==2.1.0\njoblib==1.2.0\nmeson==1.7.2\nmeson-python==0.17.1\nninja==1.11.1.4\npackaging==24.2\nplugggy==1.5.0\npyproject-metadata==0.9.1\npytest==8.3.5\npytest-xdist==3.6.1\nthreadpoolctl==3.1.0\ntomli==2.2.1\n```\n\n----------------------------------------\n\nTITLE: Checking scikit-learn development version\nDESCRIPTION: Command to verify the installed scikit-learn has a development version number ending with .dev0 by showing version information.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython -c \"import sklearn; sklearn.show_versions()\"\n```\n\n----------------------------------------\n\nTITLE: Importing LDA and QDA classes from scikit-learn\nDESCRIPTION: This snippet shows how to import the LinearDiscriminantAnalysis and QuadraticDiscriminantAnalysis classes from scikit-learn's discriminant_analysis module.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/lda_qda.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n```\n\n----------------------------------------\n\nTITLE: Adding fit_params Support in RegressorChain in scikit-learn Multioutput\nDESCRIPTION: Adds support for fit_params for base_estimator during fit in RegressorChain.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\nmultioutput.RegressorChain()\n```\n\n----------------------------------------\n\nTITLE: Referencing Impute Examples in reStructuredText\nDESCRIPTION: This RST directive creates a reference label for the impute examples section, allowing other parts of the documentation to link to this section easily.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/impute/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _impute_examples:\n```\n\n----------------------------------------\n\nTITLE: Referencing sklearn.preprocessing Module in reStructuredText\nDESCRIPTION: This snippet creates a cross-reference to the sklearn.preprocessing module in the documentation. It allows readers to easily navigate to the module's documentation for more detailed information.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/preprocessing/README.txt#2025-04-14_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n:mod:`sklearn.preprocessing`\n```\n\n----------------------------------------\n\nTITLE: Cloning scikit-learn repository\nDESCRIPTION: Git command to clone the scikit-learn source code repository from GitHub for development purposes.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/README.rst#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/scikit-learn/scikit-learn.git\n```\n\n----------------------------------------\n\nTITLE: reStructuredText Function Referencing\nDESCRIPTION: This snippet illustrates how to create a link to a function's documentation using the `:func:` role within reStructuredText, specifying the full import path.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_19\n\nLANGUAGE: rst\nCODE:\n```\n\":func:`~sklearn.model_selection.cross_val_score`\"\n```\n\nLANGUAGE: rst\nCODE:\n```\n\":func:`cross_val_score`\"\n```\n\n----------------------------------------\n\nTITLE: Referencing sklearn.mixture Module in reStructuredText\nDESCRIPTION: This snippet references the sklearn.mixture module using reStructuredText syntax. It creates a cross-reference to the module documentation within the scikit-learn library.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/mixture/README.txt#2025-04-14_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n:mod:`sklearn.mixture`\n```\n\n----------------------------------------\n\nTITLE: Defining Inspection Examples Section in reStructuredText\nDESCRIPTION: This snippet defines a section for inspection examples in the scikit-learn documentation using reStructuredText syntax. It creates a reference label and a section header.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/inspection/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _inspection_examples:\n\nInspection\n----------\n\nExamples related to the :mod:`sklearn.inspection` module.\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for scikit-learn\nDESCRIPTION: This snippet lists the Python packages required for scikit-learn development and testing. The file is auto-generated by the 'update_environments_and_lock_files.py' script and includes core dependencies like Cython, joblib, and threadpoolctl, as well as testing dependencies such as pytest and build tools like Ninja and meson-python.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/build_tools/azure/debian_32bit_requirements.txt#2025-04-14_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ncython\njoblib\nthreadpoolctl\npytest\npytest-cov\nninja\nmeson-python\n```\n\n----------------------------------------\n\nTITLE: Importing Intel-optimized Nearest Neighbors Implementation\nDESCRIPTION: Example of importing the Intel-optimized version of NearestNeighbors algorithm from scikit-learn-intelex package.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/install.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearnex.neighbors import NearestNeighbors\n```\n\n----------------------------------------\n\nTITLE: Supporting pandas.Int64 dtype for classifiers and regressors in Python\nDESCRIPTION: Classifiers and regressors now support y with pandas.Int64 dtype.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.2.rst#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npandas.Int64\n```\n\n----------------------------------------\n\nTITLE: Array Conversion Implementation\nDESCRIPTION: Converting input data structures implementing duck array to numpy array using __array__ method to ensure consistent behavior instead of __array_function__.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.22.rst#2025-04-14_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\narray = input_data.__array__()\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation Structure for Dispatching Module\nDESCRIPTION: RST directive for configuring Sphinx documentation tree structure with maxdepth of 2, including array_api module documentation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/dispatching.rst#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 2\n\n    modules/array_api\n```\n\n----------------------------------------\n\nTITLE: Updating vendor_array_api_extra directory using maintenance script in Shell\nDESCRIPTION: This command runs a shell script located in the maint_tools directory to update the vendor_array_api_extra directory. It's likely part of the project's maintenance process to keep external dependencies up-to-date.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/externals/array_api_extra/README.md#2025-04-14_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nmaint_tools/vendor_array_api_extra.sh\n```\n\n----------------------------------------\n\nTITLE: Including Changelog Legend in RST Documentation\nDESCRIPTION: ReStructuredText directive to include the changelog legend file\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.7.rst#2025-04-14_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: changelog_legend.inc\n```\n\n----------------------------------------\n\nTITLE: Documenting Empty Binder Requirements in Markdown\nDESCRIPTION: Comment block explaining the purpose of an empty binder requirements file and its relationship to sphinx-gallery and the .binder directory configuration.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/binder/requirements.txt#2025-04-14_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# A binder requirement file is required by sphinx-gallery.\\n# We don't really need one since our binder requirement file lives in the\\n# .binder directory.\\n# This file can be removed if 'dependencies' is made an optional key for\\n# binder in sphinx-gallery.\n```\n\n----------------------------------------\n\nTITLE: Referencing Feature Selection Examples in reStructuredText\nDESCRIPTION: This snippet creates a reference label for the feature selection examples section in the documentation. It uses reStructuredText syntax to define a label that can be linked to from other parts of the documentation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/feature_selection/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _feature_selection_examples:\n```\n\n----------------------------------------\n\nTITLE: Improving Error Messaging in CategoricalNB in scikit-learn Naive Bayes\nDESCRIPTION: Enhances error messaging in CategoricalNB when the number of features in the input differs between predict and fit.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\nnaive_bayes.CategoricalNB()\n```\n\n----------------------------------------\n\nTITLE: Adding 'drop' transformer to FeatureUnion in Python\nDESCRIPTION: FeatureUnion now supports 'drop' as a transformer to drop features.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.20.rst#2025-04-14_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.pipeline import FeatureUnion\nfeat_union = FeatureUnion([\n    ('pca', PCA()),\n    ('drop', 'drop')\n])\n```\n\n----------------------------------------\n\nTITLE: Renaming CV splitter classes in scikit-learn Python\nDESCRIPTION: Several CV splitter classes have been renamed for clarity and consistency. The split method parameter labels has been renamed to groups in some cases.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.18.rst#2025-04-14_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.model_selection import GroupKFold, GroupShuffleSplit, LeaveOneGroupOut, LeavePGroupsOut\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Label Definition for Dataset Examples\nDESCRIPTION: Defines a ReStructuredText label '_dataset_examples' that can be referenced elsewhere in the documentation. The label is followed by a section title 'Dataset examples' and a brief description of examples related to the sklearn.datasets module.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/datasets/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _dataset_examples:\n\nDataset examples\n-----------------------\n\nExamples concerning the :mod:`sklearn.datasets` module.\n```\n\n----------------------------------------\n\nTITLE: Setting ReStructuredText reference for biclustering examples in scikit-learn\nDESCRIPTION: This snippet defines a ReStructuredText reference tag for biclustering examples that can be used for internal linking within the scikit-learn documentation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/bicluster/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. _bicluster_examples:\n```\n\n----------------------------------------\n\nTITLE: Defining RST Section for Semi-Supervised Classification Examples\nDESCRIPTION: This RST snippet defines a section for semi-supervised classification examples in the scikit-learn documentation. It sets up a reference label and provides a brief description of the module's purpose.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/semi_supervised/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _semi_supervised_examples:\n\nSemi Supervised Classification\n------------------------------\n\nExamples concerning the :mod:`sklearn.semi_supervised` module.\n```\n\n----------------------------------------\n\nTITLE: Matérn Kernel Mathematical Formula\nDESCRIPTION: Mathematical expression for the Matérn kernel function with its general form and special cases for ν=1/2, ν=3/2, and ν=5/2. The kernel is parameterized by length-scale l and smoothness parameter ν.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/gaussian_process.rst#2025-04-14_snippet_4\n\nLANGUAGE: latex\nCODE:\n```\nk(x_i, x_j) = \\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg)^\\nu K_\\nu\\Bigg(\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg)\n```\n\n----------------------------------------\n\nTITLE: Committing changes to scikit-learn in Bash\nDESCRIPTION: Git commands to stage modified files, commit changes, and push the new feature branch to your fork on GitHub.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ngit add modified_files\ngit commit\ngit push -u origin my_feature\n```\n\n----------------------------------------\n\nTITLE: Including Contributors List in RST Documentation\nDESCRIPTION: ReStructuredText directive to include external contributors file\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.7.rst#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: _contributors.rst\n```\n\n----------------------------------------\n\nTITLE: Mathematical Expression for MDS Stress Function\nDESCRIPTION: Mathematical formula showing the normalized Stress-1 objective function used in Non-metric Multidimensional Scaling.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/manifold.rst#2025-04-14_snippet_5\n\nLANGUAGE: math\nCODE:\n```\n\\sqrt{\\frac{\\sum_{i < j} (\\hat{d}_{ij} - d_{ij}(Z))^2}{\\sum_{i < j} d_{ij}(Z)^2}}\n```\n\n----------------------------------------\n\nTITLE: Expected output of git remote command for scikit-learn\nDESCRIPTION: The expected output when running 'git remote -v' showing the correctly configured origin and upstream remotes for scikit-learn.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_4\n\nLANGUAGE: text\nCODE:\n```\norigin    git@github.com:YourLogin/scikit-learn.git (fetch)\norigin    git@github.com:YourLogin/scikit-learn.git (push)\nupstream  git@github.com:scikit-learn/scikit-learn.git (fetch)\nupstream  git@github.com:scikit-learn/scikit-learn.git (push)\n```\n\n----------------------------------------\n\nTITLE: Creating a debug environment with conda\nDESCRIPTION: This bash script creates a new conda environment with a source-built CPython interpreter configured for debugging.  This includes debug symbols and proper optimization levels. After creating the environment, it is activated, the CPython source is downloaded, configured, built and installed in the environment.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/performance.rst#2025-04-14_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n\"git clone https://github.com/python/cpython.git\\n         conda create -n debug-scikit-dev\\n         conda activate debug-scikit-dev\\n         cd cpython\\n         mkdir debug\\n         cd debug\\n         ../configure --prefix=$CONDA_PREFIX --with-pydebug\\n         make EXTRA_CFLAGS='-DPy_DEBUG' -j<num_cores>\\n         make install\"\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module in RST Documentation\nDESCRIPTION: ReStructuredText directive to set the current module context to sklearn\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.7.rst#2025-04-14_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: sklearn\n```\n\n----------------------------------------\n\nTITLE: Referencing Mixture Examples in reStructuredText\nDESCRIPTION: This snippet creates a reference label for mixture examples in the documentation. It uses reStructuredText syntax to define a label that can be referenced elsewhere in the documentation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/mixture/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _mixture_examples:\n```\n\n----------------------------------------\n\nTITLE: Checking compiler environment variables on macOS\nDESCRIPTION: Commands to display the compiler-related environment variables set by the conda-forge compilers meta-package.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\necho $CC\necho $CXX\necho $CFLAGS\necho $CXXFLAGS\necho $LDFLAGS\n```\n\n----------------------------------------\n\nTITLE: Using nogil Context Manager in Cython Functions\nDESCRIPTION: Pattern for implementing functions that can release the Global Interpreter Lock (GIL) for performance-critical sections while maintaining compatibility with CPython.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/cython.rst#2025-04-14_snippet_6\n\nLANGUAGE: cython\nCODE:\n```\ncdef inline void my_func(self) nogil:\n\n    # Some logic interacting with CPython, e.g. allocating arrays via NumPy.\n\n    with nogil:\n        # The code here is run as if it were written in C.\n\n    return 0\n```\n\n----------------------------------------\n\nTITLE: Installing memory_profiler\nDESCRIPTION: This command installs the memory_profiler package using pip. This is a prerequisite for using the memory profiling tools discussed in the document.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/performance.rst#2025-04-14_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n\"pip install -U memory_profiler\"\n```\n\n----------------------------------------\n\nTITLE: Interactive Pan/Zoom Implementation for ML Flowchart\nDESCRIPTION: JavaScript implementation for interactive pan and zoom functionality of the ML estimator flowchart, including pan limits and zoom controls.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/machine_learning_map.rst#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\ndocument.addEventListener(\"DOMContentLoaded\", function () {\n  const beforePan = function (oldPan, newPan) {\n    const gutterWidth = 100, gutterHeight = 100;\n    const sizes = this.getSizes();\n\n    const leftLimit = -((sizes.viewBox.x + sizes.viewBox.width) * sizes.realZoom) + gutterWidth;\n    const rightLimit = sizes.width - gutterWidth - (sizes.viewBox.x * sizes.realZoom);\n    const topLimit = -((sizes.viewBox.y + sizes.viewBox.height) * sizes.realZoom) + gutterHeight;\n    const bottomLimit = sizes.height - gutterHeight - (sizes.viewBox.y * sizes.realZoom);\n\n    return {\n      x: Math.max(leftLimit, Math.min(rightLimit, newPan.x)),\n      y: Math.max(topLimit, Math.min(bottomLimit, newPan.y))\n    };\n  };\n\n  svgPanZoom(\"#sk-ml-map svg\", {\n    zoomEnabled: true,\n    controlIconsEnabled: true,\n    fit: 1,\n    center: 1,\n    beforePan: beforePan,\n  });\n});\n```\n\n----------------------------------------\n\nTITLE: Using KBinsDiscretizer with sample_weight and quantile_method in Python\nDESCRIPTION: This snippet demonstrates the updated usage of KBinsDiscretizer in scikit-learn. It now supports sample_weight for uniform strategy and allows specifying quantile_method for quantile strategy. The averaged_inverted_cdf method will become the default in future versions.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/sklearn.preprocessing/29907.enhancement.rst#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.preprocessing import KBinsDiscretizer\n\n# Using uniform strategy with sample_weight\nkbd_uniform = KBinsDiscretizer(strategy=\"uniform\")\nkbd_uniform.fit(X, sample_weight=sample_weights)\n\n# Using quantile strategy with specified quantile_method\nkbd_quantile = KBinsDiscretizer(strategy=\"quantile\", quantile_method=\"averaged_inverted_cdf\")\nkbd_quantile.fit(X)\n```\n\n----------------------------------------\n\nTITLE: Displaying top features for each category in the 20 newsgroups dataset\nDESCRIPTION: This snippet defines a function to display the top 10 most informative features for each category in the 20 newsgroups dataset, based on the coefficients of a trained classifier.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/twenty_newsgroups.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> def show_top10(classifier, vectorizer, categories):\n...     feature_names = vectorizer.get_feature_names_out()\n...     for i, category in enumerate(categories):\n...         top10 = np.argsort(classifier.coef_[i])[-10:]\n...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n...\n>>> show_top10(clf, vectorizer, newsgroups_train.target_names)\nalt.atheism: edu it and in you that is of to the\ncomp.graphics: edu in graphics it is for and of to the\nsci.space: edu it that is in and space to of the\ntalk.religion.misc: not it you in is that and to of the\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for Scikit-learn Core Paper\nDESCRIPTION: BibTeX entry for citing the main scikit-learn paper published in the Journal of Machine Learning Research (JMLR) in 2011, authored by Pedregosa et al.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/about.rst#2025-04-14_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{scikit-learn,\n  title={Scikit-learn: Machine Learning in {P}ython},\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n  journal={Journal of Machine Learning Research},\n  volume={12},\n  pages={2825--2830},\n  year={2011}\n}\n```\n\n----------------------------------------\n\nTITLE: Adding upstream remote for scikit-learn in Bash\nDESCRIPTION: Git command to add the main scikit-learn repository as an upstream remote to keep your fork synchronized.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit remote add upstream git@github.com:scikit-learn/scikit-learn.git\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for Scikit-learn API Design Paper\nDESCRIPTION: BibTeX entry for citing the paper on API design for machine learning software, focusing on experiences from the scikit-learn project, authored by Buitinck et al. in 2013.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/about.rst#2025-04-14_snippet_1\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{sklearn_api,\n  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and\n                Fabian Pedregosa and Andreas Mueller and Olivier Grisel and\n                Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort\n                and Jaques Grobler and Robert Layton and Jake VanderPlas and\n                Arnaud Joly and Brian Holt and Ga{\\\"{e}}l Varoquaux},\n  title     = {{API} design for machine learning software: experiences from the scikit-learn\n                project},\n  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},\n  year      = {2013},\n  pages = {108--122},\n}\n```\n\n----------------------------------------\n\nTITLE: reStructuredText Dropdown Syntax\nDESCRIPTION: This snippet illustrates the syntax for creating dropdown elements in reStructuredText documents. Dropdowns are used to hide less important information, improving the document's readability.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_16\n\nLANGUAGE: rst\nCODE:\n```\n\".. dropdown:: Dropdown title\n\n    Dropdown content.\"\n```\n\n----------------------------------------\n\nTITLE: Creating HTML Structure for scikit-learn Authors Display\nDESCRIPTION: HTML structure that sets up a container for displaying scikit-learn project contributors. The HTML includes a comment indicating it was generated by a script, styling, and a series of div elements containing author information with GitHub avatar images and links.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/maintainers.rst#2025-04-14_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!-- Generated by generate_authors_table.py -->\n    <div class=\"sk-authors-container\">\n    <style>\n      img.avatar {border-radius: 10px;}\n    </style>\n    <div>\n    <a href='https://github.com/jeremiedbb'><img src='https://avatars.githubusercontent.com/u/34657725?v=4' class='avatar' /></a> <br />\n    <p>Jérémie du Boisberranger</p>\n    </div>\n    <div>\n    <a href='https://github.com/lesteve'><img src='https://avatars.githubusercontent.com/u/1680079?v=4' class='avatar' /></a> <br />\n    <p>Loïc Estève</p>\n    </div>\n    <div>\n    <a href='https://github.com/thomasjpfan'><img src='https://avatars.githubusercontent.com/u/5402633?v=4' class='avatar' /></a> <br />\n    <p>Thomas J. Fan</p>\n    </div>\n    <div>\n    <a href='https://github.com/agramfort'><img src='https://avatars.githubusercontent.com/u/161052?v=4' class='avatar' /></a> <br />\n    <p>Alexandre Gramfort</p>\n    </div>\n    <div>\n    <a href='https://github.com/ogrisel'><img src='https://avatars.githubusercontent.com/u/89061?v=4' class='avatar' /></a> <br />\n    <p>Olivier Grisel</p>\n    </div>\n    <div>\n    <a href='https://github.com/betatim'><img src='https://avatars.githubusercontent.com/u/1448859?v=4' class='avatar' /></a> <br />\n    <p>Tim Head</p>\n    </div>\n    <div>\n    <a href='https://github.com/NicolasHug'><img src='https://avatars.githubusercontent.com/u/1190450?v=4' class='avatar' /></a> <br />\n    <p>Nicolas Hug</p>\n    </div>\n    <div>\n    <a href='https://github.com/adrinjalali'><img src='https://avatars.githubusercontent.com/u/1663864?v=4' class='avatar' /></a> <br />\n    <p>Adrin Jalali</p>\n    </div>\n    <div>\n    <a href='https://github.com/jjerphan'><img src='https://avatars.githubusercontent.com/u/13029839?v=4' class='avatar' /></a> <br />\n    <p>Julien Jerphanion</p>\n    </div>\n    <div>\n    <a href='https://github.com/glemaitre'><img src='https://avatars.githubusercontent.com/u/7454015?v=4' class='avatar' /></a> <br />\n    <p>Guillaume Lemaitre</p>\n    </div>\n    <div>\n    <a href='https://github.com/adam2392'><img src='https://avatars.githubusercontent.com/u/3460267?v=4' class='avatar' /></a> <br />\n    <p>Adam Li</p>\n    </div>\n    <div>\n    <a href='https://github.com/lucyleeow'><img src='https://avatars.githubusercontent.com/u/23182829?v=4' class='avatar' /></a> <br />\n    <p>Lucy Liu</p>\n    </div>\n    <div>\n    <a href='https://github.com/lorentzenchr'><img src='https://avatars.githubusercontent.com/u/15324633?v=4' class='avatar' /></a> <br />\n    <p>Christian Lorentzen</p>\n    </div>\n    <div>\n    <a href='https://github.com/amueller'><img src='https://avatars.githubusercontent.com/u/449558?v=4' class='avatar' /></a> <br />\n    <p>Andreas Mueller</p>\n    </div>\n    <div>\n    <a href='https://github.com/jnothman'><img src='https://avatars.githubusercontent.com/u/78827?v=4' class='avatar' /></a> <br />\n    <p>Joel Nothman</p>\n    </div>\n    <div>\n    <a href='https://github.com/OmarManzoor'><img src='https://avatars.githubusercontent.com/u/17495884?v=4' class='avatar' /></a> <br />\n    <p>Omar Salman</p>\n    </div>\n    <div>\n    <a href='https://github.com/GaelVaroquaux'><img src='https://avatars.githubusercontent.com/u/208217?v=4' class='avatar' /></a> <br />\n    <p>Gael Varoquaux</p>\n    </div>\n    <div>\n    <a href='https://github.com/Charlie-XIAO'><img src='https://avatars.githubusercontent.com/u/108576690?v=4' class='avatar' /></a> <br />\n    <p>Yao Xiao</p>\n    </div>\n    <div>\n    <a href='https://github.com/Micky774'><img src='https://avatars.githubusercontent.com/u/34613774?v=4' class='avatar' /></a> <br />\n    <p>Meekail Zain</p>\n    </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Converting text to TF-IDF vectors using scikit-learn's TfidfVectorizer\nDESCRIPTION: This snippet shows how to convert the text data from the 20 newsgroups dataset into TF-IDF vectors using scikit-learn's TfidfVectorizer. It demonstrates the process of extracting features from a subset of categories.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/twenty_newsgroups.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> categories = ['alt.atheism', 'talk.religion.misc',\n...               'comp.graphics', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       categories=categories)\n>>> vectorizer = TfidfVectorizer()\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> vectors.shape\n(2034, 34118)\n\n>>> vectors.nnz / float(vectors.shape[0])\n159.01327...\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for scikit-learn CI\nDESCRIPTION: Specifies minimum required versions of core dependencies like Cython, joblib, and threadpoolctl along with testing dependencies pytest, pytest-xdist, ninja, and meson-python. This configuration is auto-generated and used to ensure consistent CI builds.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/build_tools/azure/ubuntu_atlas_requirements.txt#2025-04-14_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncython==3.0.10  # min\njoblib==1.2.0  # min\nthreadpoolctl==3.1.0  # min\npytest\npytest-xdist\nninja\nmeson-python\n```\n\n----------------------------------------\n\nTITLE: Running Benchmarks for Current Branch in scikit-learn\nDESCRIPTION: Command to run benchmarks for just the current HEAD commit without comparing to another branch.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_36\n\nLANGUAGE: bash\nCODE:\n```\nasv run -b linear_model HEAD^!\n```\n\n----------------------------------------\n\nTITLE: Computing Log Loss for Binary Classification in Python\nDESCRIPTION: This snippet demonstrates how to use the log_loss function to compute the logarithmic loss for binary classification. It uses ground truth labels and probability estimates to calculate the log loss.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/model_evaluation.rst#2025-04-14_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n>>> from sklearn.metrics import log_loss\n>>> y_true = [0, 0, 1, 1]\n>>> y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]\n>>> log_loss(y_true, y_pred)\n0.1738...\n```\n\n----------------------------------------\n\nTITLE: Fetching Upstream Repository for Benchmarking in scikit-learn\nDESCRIPTION: Command to ensure the local clone is up to date with the upstream scikit-learn repository before running benchmarks.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\ngit fetch upstream\n```\n\n----------------------------------------\n\nTITLE: Activating conda environment for scikit-learn development\nDESCRIPTION: Command to activate the previously created conda environment for scikit-learn development.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconda activate sklearn-env\n```\n\n----------------------------------------\n\nTITLE: Running Benchmarks for a Specific Estimator in scikit-learn\nDESCRIPTION: Command to compare performance of a specific estimator (LogisticRegression) between the upstream main branch and your current HEAD using ASV.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\nasv continuous -b LogisticRegression upstream/main HEAD\n```\n\n----------------------------------------\n\nTITLE: Creating IPython profile\nDESCRIPTION: This command creates a new IPython profile. This is required to configure IPython to load the memory_profiler extension.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/performance.rst#2025-04-14_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n\"ipython profile create\"\n```\n\n----------------------------------------\n\nTITLE: Referencing Species Distribution Dataset in reStructuredText\nDESCRIPTION: This snippet defines a reference label for the species distribution dataset section in the documentation. It's used for internal linking within the documentation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/species_distributions.rst#2025-04-14_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. _species_distribution_dataset:\n```\n\n----------------------------------------\n\nTITLE: Examining LFW Dataset Properties in Python\nDESCRIPTION: This code snippet shows how to examine the properties of the loaded LFW dataset, including data type, shape of the data and images arrays, and the structure of the target array.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/lfw.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlfw_people.data.dtype\nlfw_people.data.shape\nlfw_people.images.shape\nlfw_people.target.shape\nlist(lfw_people.target[:10])\n```\n\n----------------------------------------\n\nTITLE: Referencing Manifold Learning Examples in reStructuredText\nDESCRIPTION: This snippet creates a reference label for manifold learning examples and provides a section title for the content. It also includes a brief description indicating that the examples are related to the sklearn.manifold module.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/manifold/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _manifold_examples:\n\nManifold learning\n-----------------------\n\nExamples concerning the :mod:`sklearn.manifold` module.\n```\n\n----------------------------------------\n\nTITLE: Referencing Neural Network Examples in reStructuredText\nDESCRIPTION: This snippet creates a reference label for the neural network examples section and sets up the header for the documentation page. It also includes a brief description of the examples' focus on the sklearn.neural_network module.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/neural_networks/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _neural_network_examples:\n\nNeural Networks\n-----------------------\n\nExamples concerning the :mod:`sklearn.neural_network` module.\n```\n\n----------------------------------------\n\nTITLE: Setting Text Removal Options in Scikit-learn\nDESCRIPTION: Code snippet showing how to configure text preprocessing by removing headers, footers, and quotes from documents. This affects F-score results by making the evaluation more realistic.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/twenty_newsgroups.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nremove=('headers', 'footers', 'quotes')\n```\n\n----------------------------------------\n\nTITLE: Defining References in reStructuredText\nDESCRIPTION: This snippet defines a 'References' section in the documentation using reStructuredText syntax. It includes a citation for the paper that provided the dataset.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/species_distributions.rst#2025-04-14_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. rubric:: References\n\n* `\"Maximum entropy modeling of species geographic distributions\"\n  <http://rob.schapire.net/papers/ecolmod.pdf>`_ S. J. Phillips,\n  R. P. Anderson, R. E. Schapire - Ecological Modelling, 190:231-259, 2006.\n```\n\n----------------------------------------\n\nTITLE: Handling Version Inconsistency Warnings\nDESCRIPTION: Code showing how to catch InconsistentVersionWarning when loading a model saved with a different scikit-learn version to obtain the original version information.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/model_persistence.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.exceptions import InconsistentVersionWarning\nwarnings.simplefilter(\"error\", InconsistentVersionWarning)\n\ntry:\n    with open(\"model_from_previous_version.pickle\", \"rb\") as f:\n        est = pickle.load(f)\nexcept InconsistentVersionWarning as w:\n    print(w.original_sklearn_version)\n```\n\n----------------------------------------\n\nTITLE: Resolving Lock File Conflicts in Scikit-learn\nDESCRIPTION: Bash commands to resolve conflicts in environment and lock files when merging from upstream, keeping the upstream version for specific files.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ngit pull upstream main --no-rebase\ngit checkout --theirs  build_tools/*/*.lock build_tools/*/*environment.yml \\\n    build_tools/*/*lock.txt build_tools/*/*requirements.txt\ngit add build_tools/*/*.lock build_tools/*/*environment.yml \\\n    build_tools/*/*lock.txt build_tools/*/*requirements.txt\ngit merge --continue\n```\n\n----------------------------------------\n\nTITLE: Importing sklearn.feature_extraction.text Module in Python\nDESCRIPTION: This snippet demonstrates how to import the text feature extraction module from scikit-learn. It's a prerequisite for working with text documents in machine learning tasks using scikit-learn.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/text/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.feature_extraction import text\n```\n\n----------------------------------------\n\nTITLE: Accessing RCV1 Sample IDs in Python\nDESCRIPTION: This code shows how to access the sample IDs of the RCV1 dataset. Each sample can be identified by its ID, ranging (with gaps) from 2286 to 810596.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/descr/rcv1.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> rcv1.sample_id[:3]\narray([2286, 2287, 2288], dtype=uint32)\n```\n\n----------------------------------------\n\nTITLE: Specifying Module Reference in reStructuredText\nDESCRIPTION: This RST syntax creates a cross-reference to the sklearn.impute module documentation, providing a link for users to access more detailed information about the module.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/impute/README.txt#2025-04-14_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n:mod:`sklearn.impute`\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Refresh Redirect Implementation\nDESCRIPTION: Sets up a meta refresh redirect to compose.html with a 1 second delay\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/modules/pipeline.rst#2025-04-14_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"refresh\" content=\"1; url=./compose.html\" />\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Image and Link Grid Documentation\nDESCRIPTION: RST code that creates a grid layout for displaying sponsor logos with hyperlinks including Microsoft, BCG, Fujitsu, APHP and HuggingFace. Includes custom CSS styling for image size control.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/about.rst#2025-04-14_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. |msn| image:: images/microsoft.png\n  :target: https://www.microsoft.com/\n\n.. |bcg| image:: images/bcg.png\n  :target: https://www.bcg.com/beyond-consulting/bcg-gamma/default.aspx\n\n.. |fujitsu| image:: images/fujitsu.png\n  :target: https://www.fujitsu.com/global/\n\n.. |aphp| image:: images/logo_APHP_text.png\n  :target: https://aphp.fr/\n\n.. |hf| image:: images/huggingface_logo-noborder.png\n  :target: https://huggingface.co\n\n.. raw:: html\n\n  <style>\n    div.image-subgrid img {\n      max-height: 50px;\n      max-width: 90%;\n    }\n  </style>\n\n.. grid:: 2 2 4 4\n  :class-row: image-subgrid\n  :gutter: 1\n```\n\n----------------------------------------\n\nTITLE: Installing ASV for Performance Benchmarking in scikit-learn\nDESCRIPTION: Command to install the development version of the Airspeed Velocity (ASV) benchmarking tool used for monitoring performance in scikit-learn.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/airspeed-velocity/asv\n```\n\n----------------------------------------\n\nTITLE: Setting up Visual Studio build environment for Windows\nDESCRIPTION: Commands to configure the build environment for 64-bit Python on Windows using Visual Studio Build Tools.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_8\n\nLANGUAGE: batch\nCODE:\n```\nSET DISTUTILS_USE_SDK=1\n\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64\n```\n\n----------------------------------------\n\nTITLE: Running Benchmarks with Virtualenv in scikit-learn\nDESCRIPTION: Command to run benchmarks using virtualenv instead of conda for creating the benchmark environments.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/contributing.rst#2025-04-14_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\nasv continuous -E virtualenv -b LogisticRegression upstream/main HEAD\n```\n\n----------------------------------------\n\nTITLE: Importing Biclustering Algorithms in Python\nDESCRIPTION: Shows how to import SpectralCoclustering and SpectralBiclustering from sklearn.cluster.bicluster for biclustering analysis.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.14.rst#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.cluster.bicluster import SpectralCoclustering, SpectralBiclustering\n```\n\n----------------------------------------\n\nTITLE: Defining Section Headers in reStructuredText\nDESCRIPTION: Defines section headers for version numbers and changelog sections using reStructuredText syntax.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.17.rst#2025-04-14_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n============\nVersion 0.17\n============\n\n.. _changes_0_17_1:\n\nVersion 0.17.1\n==============\n\n**February 18, 2016**\n\nChangelog\n---------\n\nBug fixes\n.........\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenMP on FreeBSD\nDESCRIPTION: Commands for installing and configuring OpenMP support on FreeBSD systems.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/developers/advanced_installation.rst#2025-04-14_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nsudo pkg install openmp\n\nexport CFLAGS=\"$CFLAGS -I/usr/local/include\"\nexport CXXFLAGS=\"$CXXFLAGS -I/usr/local/include\"\nexport LDFLAGS=\"$LDFLAGS -Wl,-rpath,/usr/local/lib -L/usr/local/lib -lomp\"\n\npip install --editable . \\\n    --verbose --no-build-isolation \\\n    --config-settings editable-verbose=true\n```\n\n----------------------------------------\n\nTITLE: Deprecating fit_grid_point in scikit-learn Model Selection\nDESCRIPTION: Deprecates the fit_grid_point function, which will be removed in version 0.25.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.23.rst#2025-04-14_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\nmodel_selection.fit_grid_point()\n```\n\n----------------------------------------\n\nTITLE: Deprecating labels parameter in hamming_loss in Python\nDESCRIPTION: The labels parameter in metrics.hamming_loss is deprecated in version 0.21 and will be removed in version 0.23.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v0.21.rst#2025-04-14_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nmetrics.hamming_loss\n```\n\n----------------------------------------\n\nTITLE: Starting Towncrier Release Notes in RST Documentation\nDESCRIPTION: ReStructuredText directive to mark the beginning of towncrier release notes\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.7.rst#2025-04-14_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. towncrier release notes start\n```\n\n----------------------------------------\n\nTITLE: Defining HTML Raw Role in ReST\nDESCRIPTION: Creates a raw HTML role that allows direct HTML insertion in ReST documentation\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/_contributors.rst#2025-04-14_snippet_0\n\nLANGUAGE: ReST\nCODE:\n```\n.. role:: raw-html(raw)\n   :format: html\n```\n\n----------------------------------------\n\nTITLE: Referencing the decomposition examples section in RST\nDESCRIPTION: This RST directive creates a reference label for the decomposition examples section that can be linked to from other parts of the documentation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/examples/decomposition/README.txt#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _decomposition_examples:\n```\n\n----------------------------------------\n\nTITLE: HTML Authors Grid with CSS Styling\nDESCRIPTION: Generates a responsive grid layout of author profiles with GitHub avatars and links. The CSS styling adds rounded corners to the avatar images. Each author div contains a linked avatar image and name paragraph.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/documentation_team.rst#2025-04-14_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!-- Generated by generate_authors_table.py -->\n<div class=\"sk-authors-container\">\n<style>\n  img.avatar {border-radius: 10px;}\n</style>\n<div>\n<a href='https://github.com/ArturoAmorQ'><img src='https://avatars.githubusercontent.com/u/86408019?v=4' class='avatar' /></a> <br />\n<p>Arturo Amor</p>\n</div>\n<div>\n<a href='https://github.com/lucyleeow'><img src='https://avatars.githubusercontent.com/u/23182829?v=4' class='avatar' /></a> <br />\n<p>Lucy Liu</p>\n</div>\n<div>\n<a href='https://github.com/marenwestermann'><img src='https://avatars.githubusercontent.com/u/17019042?v=4' class='avatar' /></a> <br />\n<p>Maren Westermann</p>\n</div>\n<div>\n<a href='https://github.com/Charlie-XIAO'><img src='https://avatars.githubusercontent.com/u/108576690?v=4' class='avatar' /></a> <br />\n<p>Yao Xiao</p>\n</div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Defining LaTeX Raw Role in ReST\nDESCRIPTION: Creates a raw LaTeX role that allows direct LaTeX insertion in ReST documentation\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/_contributors.rst#2025-04-14_snippet_1\n\nLANGUAGE: ReST\nCODE:\n```\n.. role:: raw-latex(raw)\n   :format: latex\n```\n\n----------------------------------------\n\nTITLE: Cleaning draw.io URL Prefixes in SVG Using Perl\nDESCRIPTION: Perl command to remove 'https://app.diagrams.net/' prefixes from links in the SVG file after exporting from draw.io. This maintains proper relative paths in the documentation.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/images/ml_map.README.rst#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nperl -pi -e 's@https://app.diagrams.net/\\./@./@g' doc/images/ml_map.svg\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Avatar Styling with CSS\nDESCRIPTION: CSS styling that applies a border-radius of 10px to avatar images, giving them rounded corners. This style is embedded within the HTML to format contributor profile images.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/maintainers.rst#2025-04-14_snippet_0\n\nLANGUAGE: css\nCODE:\n```\nimg.avatar {border-radius: 10px;}\n```\n\n----------------------------------------\n\nTITLE: Styling FAQ Headers and Navigation - CSS/HTML\nDESCRIPTION: Custom CSS styling for FAQ page headers, including formatting for h3 headings, section margins, header links, and table of contents backlinks.\nSOURCE: https://github.com/scikit-learn/scikit-learn/blob/main/doc/faq.rst#2025-04-14_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n.. raw:: html\n\n  <style>\n    /* h3 headings on this page are the questions; make them rubric-like */\n    h3 {\n      font-size: 1rem;\n      font-weight: bold;\n      padding-bottom: 0.2rem;\n      margin: 2rem 0 1.15rem 0;\n      border-bottom: 1px solid var(--pst-color-border);\n    }\n\n    /* Increase top margin for first question in each section */\n    h2 + section > h3 {\n      margin-top: 2.5rem;\n    }\n\n    /* Make the headerlinks a bit more visible */\n    h3 > a.headerlink {\n      font-size: 0.9rem;\n    }\n\n    /* Remove the backlink decoration on the titles */\n    h2 > a.toc-backref,\n    h3 > a.toc-backref {\n      text-decoration: none;\n    }\n  </style>\n```"
  }
]
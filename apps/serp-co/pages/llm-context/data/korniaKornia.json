[
  {
    "owner": "kornia",
    "repo": "kornia",
    "content": "TITLE: Advanced Image Augmentation with AugmentationSequential\nDESCRIPTION: An example showing how to use AugmentationSequential container for complex image augmentation pipelines that can handle masks, bounding boxes, and keypoints with support for inverse transformations.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/image_augmentations.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport kornia.augmentation as K\n\naug = K.AugmentationSequential(\n   K.ColorJiggle(0.1, 0.1, 0.1, 0.1, p=1.0),\n   K.RandomAffine(360, [0.1, 0.1], [0.7, 1.2], [30., 50.], p=1.0),\n   K.RandomPerspective(0.5, p=1.0),\n   data_keys=[\"input\", \"bbox\", \"keypoints\", \"mask\"],  # Just to define the future input here.\n   return_transform=False,\n   same_on_batch=False,\n)\n# forward the operation\nout_tensors = aug(img_tensor, bbox, keypoints, mask)\n# Inverse the operation\nout_tensor_inv = aug.inverse(*out_tensor)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using RT-DETR Object Detector with Kornia\nDESCRIPTION: Demonstrates how to load an image, initialize the RT-DETR detector, perform object detection, visualize results, and export to ONNX format. Supports multiple model variants (r18vd, r34vd, r50vd_m, r50vd, r101vd) and various image scales from 480 to 800 pixels.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/models/rt_detr.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kornia.io import load_image\nfrom kornia.models.detector.rtdetr import RTDETRDetectorBuilder\n\ninput_img = load_image(img_path)[None]  # Load image to BCHW\n\n# NOTE: available models: 'rtdetr_r18vd', 'rtdetr_r34vd', 'rtdetr_r50vd_m', 'rtdetr_r50vd', 'rtdetr_r101vd'.\n# NOTE: recommended image scales: [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800]\ndetector = RTDETRDetectorBuilder.build(\"rtdetr_r18vd\", image_size=640)\n\n# get the output boxes\nboxes = detector(input_img)\n\n# draw the bounding boxes on the images directly.\noutput = detector.draw(input_img, output_type=\"pil\")\noutput[0].save(\"Kornia-RTDETR-output.png\")\n\n# convert the whole model to ONNX directly\nRTDETRDetectorBuilder.to_onnx(\"RTDETR-640.onnx\", model_name=\"rtdetr_r18vd\", image_size=640)\n```\n\n----------------------------------------\n\nTITLE: Creating Dataloaders and Transforms for Semantic Segmentation in Python\nDESCRIPTION: Sets up PyTorch dataloaders and transformations for training a semantic segmentation model. Includes configuration for batch size, image size, and split ratios for training and validation sets.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/semantic_segmentation.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Create the dataloaders and transforms:\n\nBATCH_SIZE = 8\nIMAGE_SIZE = (224, 224)\nNUM_CLASSES = 21\n\n# replace with your own dataset\ndataset = torchvision.datasets.VOCSegmentation(\n    root=\"./data\",\n    image_set=\"train\",\n    download=True,\n)\n\ndataset_test = torchvision.datasets.VOCSegmentation(\n    root=\"./data\",\n    image_set=\"val\",\n    download=True,\n)\n\n# split the dataset into train and val\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ndataset_train, dataset_val = torch.utils.data.random_split(dataset, [train_size, val_size])\n\n# create the dataloaders\ndataloader_train = torch.utils.data.DataLoader(\n    dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True\n)\n\ndataloader_val = torch.utils.data.DataLoader(\n    dataset_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=True\n)\n```\n\n----------------------------------------\n\nTITLE: Using VisualPrompter with SAM for Image Segmentation in Kornia\nDESCRIPTION: This code demonstrates how to initialize the VisualPrompter API, load the SAM model weights, set an image for processing, and generate segmentation predictions using various prompts like keypoints and boxes. It shows multiple ways to query the model, including using only keypoints, only boxes, no prompts, or previous logits.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/models/segment_anything.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom kornia.contrib.models.sam import SamConfig\nfrom kornia.contrib.visual_prompter import VisualPrompter\nfrom kornia.io import load_image, ImageLoadType\nfrom kornia.geometry.keypoints import Keypoints\nfrom kornia.geometry.boxes import Boxes\nfrom kornia.utils import get_cuda_or_mps_device_if_available\n\nmodel_type = 'vit_h'\ncheckpoint = './https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth'\ndevice = get_cuda_or_mps_device_if_available\n\n# Load image\nimage = load_image('./example.jpg', ImageLoadType.RGB32, device)\n\n# Define the model config\nconfig = SamConfig(model_type, checkpoint)\n\n# Load the prompter\nprompter = VisualPrompter(config, device=device)\n\n# You can use torch dynamo/compile API with:\n# prompter.compile()\n\n# set the image: This will preprocess the image and already generate the embeddings of it\nprompter.set_image(image)\n\n# Generate the prompts\nkeypoints = Keypoints(torch.tensor([[[500, 375]]], device=device, dtype=torch.float32)) # BxNx2\n# For the keypoints label: 1 indicates a foreground point; 0 indicates a background point\nkeypoints_labels = torch.tensor([[1]], device=device) # BxN\nboxes = Boxes(\n    torch.tensor([[[[425, 600], [425, 875], [700, 600], [700, 875]]]], device=device, dtype=torch.float32), mode='xyxy'\n)\n\n# Runs the prediction with all prompts\nprediction = prompter.predict(\n    keypoints=keypoints,\n    keypoints_labels=keypoints_labels,\n    boxes=boxes,\n    multimask_output=True,\n)\n\n#----------------------------------------------\n# or run the prediction with just the keypoints\nprediction = prompter.predict(\n    keypoints=keypoints,\n    keypoints_labels=keypoints_labels,\n    multimask_output=True,\n)\n\n#----------------------------------------------\n# or run the prediction with just the box\nprediction = prompter.predict(\n    boxes=boxes,\n    multimask_output=True,\n)\n\n#----------------------------------------------\n# or run the prediction without prompts\nprediction = prompter.predict(\n    multimask_output=True,\n)\n\n#------------------------------------------------\n# or run the prediction using the previous logits\nprediction = prompter.predict(\n    masks=prediction.logits\n    multimask_output=True,\n)\n\n# The `prediction` is a SegmentationResults dataclass with the masks, scores and logits\nprint(prediction.masks.shape)\nprint(prediction.scores)\nprint(prediction.logits.shape)\n```\n\n----------------------------------------\n\nTITLE: Creating Preprocessing and Augmentation Pipeline for Object Detection in Python\nDESCRIPTION: Implements a preprocessing and data augmentation pipeline for object detection using Kornia transforms. Handles both training and validation data transformations.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/object_detection.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# create your preprocessing and augmentations pipeline\nclass PreProcessing(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.normalize = K.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    def forward(self, data):\n        # Unpack data which is a tuple with (image, bounding box, label)\n        images, targets = data\n\n        # Apply normalization to images\n        images = self.normalize(images)\n\n        return images, targets\n\n\nclass TrainAugmentations(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.transform = K.AugmentationSequential(\n            K.RandomHorizontalFlip(p=0.5),\n            K.RandomBrightness(0.2, p=0.8),\n            K.ColorJitter(brightness=0.2, contrast=0.3, saturation=0.3, hue=0.3, p=0.8),\n            data_keys=[\"input\", \"bbox\"],\n        )\n\n    def forward(self, data):\n        # Unpack data which is a tuple with (image, bounding box, label)\n        images, targets = data\n\n        # Apply transformations to images and boxes\n        # Process each image in the batch individually\n        for i, (image, target) in enumerate(zip(images, targets)):\n            image_tensor = image.unsqueeze(0)  # Add batch dimension\n            boxes = target[\"boxes\"]  # Get boxes for current image\n\n            # Apply transformations\n            image_out, boxes_out = self.transform(image_tensor, boxes.unsqueeze(0))\n\n            # Update data\n            images[i] = image_out.squeeze(0)\n            target[\"boxes\"] = boxes_out.squeeze(0)\n\n        return images, targets\n```\n\n----------------------------------------\n\nTITLE: Creating Preprocessing and Augmentation Pipeline for Semantic Segmentation in Python\nDESCRIPTION: Defines data preprocessing and augmentation pipelines using Kornia transforms. Includes normalization using ImageNet statistics, resizing, and custom segmentation preprocessing functions for input images and target masks.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/semantic_segmentation.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Create your preprocessing and augmentations pipeline:\n\n# define the augmentations and normalizations\nmean = torch.tensor([0.485, 0.456, 0.406])\nstd = torch.tensor([0.229, 0.224, 0.225])\n\n# define the preprocessing pipeline\npreprocess = transforms.AugmentationSequential(\n    transforms.Resize(IMAGE_SIZE),\n    transforms.Normalize(mean, std),\n)\n\n# define your own preprocessing functions for your data\ndef preprocess_sample(sample):\n    img, target = sample\n    img = preprocess(img)\n    target = K.geometry.resize(target.unsqueeze(0).unsqueeze(0), IMAGE_SIZE, interpolation=\"nearest\")\n    target = target.squeeze(0).squeeze(0).long()\n    return img, target\n\ndef preprocess_batch(batch):\n    imgs, targets = batch\n    imgs = imgs.float() / 255.0\n    targets = targets.long()\n    return imgs, targets\n```\n\n----------------------------------------\n\nTITLE: Using LoFTR for Image Matching in Kornia\nDESCRIPTION: This code demonstrates how to use the LoFTR (Local Feature Transformer) model from Kornia to find correspondences between two images. The snippet initializes the LoFTR model with pre-trained weights for outdoor scenes and applies it to a pair of input images to obtain correspondence dictionaries.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/image_matching.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kornia.feature import LoFTR\n\nmatcher = LoFTR(pretrained=\"outdoor\")\ninput = {\"image0\": img1, \"image1\": img2}\ncorrespondences_dict = matcher(input)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Processing SAM Model in Python with Kornia\nDESCRIPTION: Complete example of loading and using the SAM model for image segmentation. Includes model initialization, image loading and preprocessing, prompt encoding, and two options for generating segmentation masks. Demonstrates both manual API calls and direct model usage.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/models/segment_anything.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom kornia.contrib.models.sam import Sam\nfrom kornia.contrib.models import SegmentationResults\nfrom kornia.io import load_image, ImageLoadType\nfrom kornia.utils import get_cuda_or_mps_device_if_available\nfrom kornia.geometry import resize\nfrom kornia.enhance import normalize\n\nmodel_type = 'vit_b' # or can be a number `2` or the enum sam.SamModelType.vit_b\ncheckpoint_path = './path_for_the_model_checkpoint.pth'\ndevice = get_cuda_or_mps_device_if_available()\n\n# Load the model\nsam_model = Sam.from_pretrained(model_type, checkpoint_path, device)\n\n# Load image\nimage = load_image('./example.jpg', ImageLoadType.RGB32, device)\n\n# Transform the image (CxHxW) into a batched input (BxCxHxW)\nimage = image[None, ...]\n\n# Resize the image to have the maximum size 1024 on its largest side\ndata = resize(image, 1024, side='long')\n\n# Embed prompts -- ATTENTION: should match the coordinates after the resize of the image\nsparse_embeddings, dense_embeddings = sam_model.prompt_encoder(points=None, boxes=None, masks=None)\n\n# define the info for normalize the input\npixel_mean = torch.tensor(...)\npixel_std = torch.tensor(...)\n\n# Preprocess input\ndata = normalize(data, pixel_mean, pixel_std)\npadh = model_sam.image_encoder.img_size - h\npadw = model_sam.image_encoder.img_size - w\ndata = pad(data, (0, padw, 0, padh))\n\n#--------------------------------------------------------------------\n# Option A: Manually calling each API\n#--------------------------------------------------------------------\nlow_res_logits, iou_predictions = sam_model.mask_decoder(\n    image_embeddings=sam_model.image_encoder(data),\n    image_pe=sam_model.prompt_encoder.get_dense_pe(),\n    sparse_prompt_embeddings=sparse_embeddings,\n    dense_prompt_embeddings=dense_embeddings,\n    multimask_output=True,\n)\n\nprediction = SegmentationResults(low_res_logits, iou_predictions)\n\n#--------------------------------------------------------------------\n# Option B: Calling the model itself\n#--------------------------------------------------------------------\nprediction = sam_model(data[None, ...], [{}], multimask_output=True)\n\n#--------------------------------------------------------------------\n# Post processing\n#--------------------------------------------------------------------\n# Upscale the masks to the original image resolution\ninput_size = (data.shape[-2], data.shape[-1])\noriginal_size = (image.shape[-2], image.shape[-1])\nimage_size_encoder = (model_sam.image_encoder.img_size, model_sam.image_encoder.img_size)\nprediction.original_res_logits(input_size, original_size, image_size_encoder)\n\n# If wants to check the binary masks\nmasks = prediction.binary_masks\n```\n\n----------------------------------------\n\nTITLE: Creating Data Loaders for Image Classification in Python\nDESCRIPTION: This code snippet shows how to create data loaders for training and validation datasets using torchvision's CIFAR10 dataset. It sets up batch sizes, transforms, and workers for efficient data loading.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/image_classification.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision import transforms as T\nfrom torchvision.datasets import CIFAR10\nfrom torch.utils.data import DataLoader\n\nbatch_size: int = 32\nnum_workers: int = 4\n\ntransform = T.Compose([T.ToTensor(), T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrain_set = CIFAR10('./data', train=True, download=True, transform=transform)\ntest_set = CIFAR10('./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(\n    train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n)\n\ntest_loader = DataLoader(\n    test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n)\n```\n\n----------------------------------------\n\nTITLE: Using MobileSAM with Kornia's VisualPrompter\nDESCRIPTION: This snippet demonstrates how to load and use MobileSAM through Kornia's implementation. It shows the initialization of the VisualPrompter with MobileSAM configuration, setting an input image, defining keypoints for prompting, and generating segmentation predictions.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/models/mobile_sam.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom kornia.contrib.models.sam import SamConfig\nfrom kornia.contrib.visual_prompter import VisualPrompter\n\nimage = torch.randn(3, 512, 512)\nprompter = VisualPrompter(SamConfig(\"mobile_sam\", pretrained=True))\nprompter.set_image(image)\n\nkeypoints = Keypoints(torch.tensor([[[500.0, 375.0]]])) # BxNx2\nlabels = torch.tensor([[1]], device=device) # BxN\n\nprediction = prompter.predict(\n    keypoints=keypoints,\n    keypoints_labels=labels,\n    multimask_output=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Using Kornia's FaceDetector for Face Detection in Python\nDESCRIPTION: This code demonstrates how to use Kornia's FaceDetector class to detect faces in an image. It shows the complete workflow including device selection, image loading, preprocessing with color conversion, and running the detector to identify faces.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/face_detection.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# select the device\ndevice = torch.device('cpu')\nif args.cuda and torch.cuda.is_available():\n    device = torch.device('cuda:0')\n\n# load the image and scale\nimg_raw = cv2.imread(args.image_file, cv2.IMREAD_COLOR)\nimg_raw = scale_image(img_raw, args.image_size)\n\n# preprocess\nimg = K.image_to_tensor(img_raw, keepdim=False).to(device)\nimg = K.color.bgr_to_rgb(img.float())\n\n# create the detector and find the faces !\nface_detection = FaceDetector().to(device)\n\nwith torch.no_grad():\n    dets = face_detection(img)\ndets = [FaceDetectorResult(o) for o in dets[0]]\n```\n\n----------------------------------------\n\nTITLE: Basic Image Registration Using Kornia's ImageRegistrator API\nDESCRIPTION: A simple example demonstrating how to use Kornia's ImageRegistrator class to align two images using a similarity transformation. This creates a registrator, takes source and destination images, and computes the homography matrix between them.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/image_registration.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kornia.geometry import ImageRegistrator\nimg_src = torch.rand(1, 1, 32, 32)\nimg_dst = torch.rand(1, 1, 32, 32)\nregistrator = ImageRegistrator('similarity')\nhomo = registrator.register(img_src, img_dst)\n```\n\n----------------------------------------\n\nTITLE: Using ImageStitcher in Kornia with LoFTR Matcher\nDESCRIPTION: This snippet demonstrates how to use Kornia's ImageStitcher with a Local Feature Transformer (LoFTR) matcher to stitch multiple images together into a panorama. It initializes the LoFTR matcher with a pretrained outdoor model, creates an ImageStitcher with RANSAC estimation, and processes a batch of images to produce a stitched panorama.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/image_stitching.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kornia.contrib import ImageStitcher\n\nmatcher = KF.LoFTR(pretrained='outdoor')\nIS = ImageStitcher(matcher, estimator='ransac').cuda()\n# NOTE: it would require a large CPU memory if many images.\nwith torch.no_grad():\n    out = IS(*imgs)\n```\n\n----------------------------------------\n\nTITLE: Using VisualPrompter for Image Segmentation in Python\nDESCRIPTION: This code snippet demonstrates how to use the VisualPrompter API in Kornia for image segmentation tasks. It includes loading an image, initializing the VisualPrompter, setting the image, generating prompts, and making predictions.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/visual_prompting.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kornia.io import load_image, ImageLoadType\nfrom kornia.contrib.visual_prompter import VisualPrompter\n\n# load an image\nimage = load_image('./example.jpg', ImageLoadType.RGB32, device)\n\n# Load the prompter\nprompter = VisualPrompter()\n\n# set the image: This will preprocess the image and already generate the embeddings of it\nprompter.set_image(image)\n\n# Generate the prompts\nkeypoints = Keypoints(torch.tensor([[[500, 375]]], device=device, dtype=torch.float32)) # BxNx2\n\n# For the keypoints label: 1 indicates a foreground point; 0 indicates a background point\nkeypoints_labels = torch.tensor([[1]], device=device) # BxN\n\n# Runs the prediction with the kypoints prompts\nprediction = prompter.predict(\n   keypoints=keypoints,\n   keypoints_labels=keypoints_labels,\n   multimask_output=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Image Classification Pipeline in Python\nDESCRIPTION: This code shows how to build a custom image classification pipeline using Kornia's VisionTransformer and ClassificationHead. It combines these components using nn.Sequential to create a classifier that processes an input image and outputs class scores.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/models/vit.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nimport kornia.contrib as K\n\nclassifier = nn.Sequential(\n    K.VisionTransformer(image_size=224, patch_size=16),\n    K.ClassificationHead(num_classes=1000)\n)\n\nimg = torch.rand(1, 3, 224, 224)\nout = classifier(img)     # BxN\nscores = out.argmax(-1)   # B\n```\n\n----------------------------------------\n\nTITLE: Creating Image Classifier with Vision Transformer in Python\nDESCRIPTION: This snippet demonstrates how to create an image classifier using Kornia's VisionTransformer and ClassificationHead. It sets up a sequential model for processing 224x224 images with 16x16 patches and 1000 output classes.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/image_classification.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nimport kornia.contrib as K\n\nclassifier = nn.Sequential(\n   K.VisionTransformer(image_size=224, patch_size=16),\n   K.ClassificationHead(num_classes=1000)\n)\n\nimg = torch.rand(1, 3, 224, 224)\nout = classifier(img)     # BxN\nscores = out.argmax(-1)   # B\n```\n\n----------------------------------------\n\nTITLE: Instantiating and Running a Semantic Segmentation Trainer in Python\nDESCRIPTION: Creates a SemanticSegmentationTrainer instance and executes the training pipeline. Configures device settings, provides the necessary components (model, dataloaders, loss function, etc.), and starts the training process.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/semantic_segmentation.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Finally, instantiate the SemanticSegmentationTrainer and execute your training pipeline.\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrainer = K.x.SemanticSegmentationTrainer(\n    model, dataloader_train, loss_fn, optimizer, scheduler, device=device,\n    batch_preprocess_fn=preprocess_batch, sample_preprocess_fn=preprocess_sample\n)\n\ntrainer.fit(10)\n```\n\n----------------------------------------\n\nTITLE: Loading SAM Model Checkpoints in Kornia\nDESCRIPTION: This code demonstrates how to load SAM model checkpoints either from a local file or directly from an official URL. The model supports three variants (vit_h, vit_l, vit_b) and will automatically download and cache the weights when a URL is provided.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/models/segment_anything.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom kornia.contrib.models.sam import Sam, SamConfig\nfrom kornia.utils import get_cuda_or_mps_device_if_available\n\nmodel_type = 'vit_b'\n\n# The checkpoint can be a filepath or a url\ncheckpoint = './path_for_the_model_checkpoint.pth'\ndevice = get_cuda_or_mps_device_if_available()\n\n# Load/build the model\nsam_model = Sam.from_config(SamConfig(model_type))\n\n# Load the checkpoint\n```\n\n----------------------------------------\n\nTITLE: Defining Model, Losses, Optimizer and Scheduler for Object Detection in Python\nDESCRIPTION: Initializes the object detection model, defines the loss functions, and configures the optimizer and learning rate scheduler for training.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/object_detection.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# define your model, losses, optimizers and schedulers\nmodel = fasterrcnn_resnet50_fpn(pretrained=True, progress=True)\nloss_fn = nn.CrossEntropyLoss()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n```\n\n----------------------------------------\n\nTITLE: Loading SAM Model from Configuration in Kornia\nDESCRIPTION: This code shows how to create a SAM model by specifying the encoder parameters through SamConfig. It supports three model variants (vit_h, vit_l, vit_b) and allows loading checkpoints from either a local file or a URL.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/models/segment_anything.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kornia.contrib.models.sam import Sam, SamConfig\nfrom kornia.utils import get_cuda_or_mps_device_if_available\n\n# model_type can be:\n#   0, 'vit_h' or `kornia.contrib.models.sam.SamModelType.vit_h`\n#   1, 'vit_l' or `kornia.contrib.models.sam.SamModelType.vit_l`\n#   2, 'vit_b' or `kornia.contrib.models.sam.SamModelType.vit_b`\nmodel_type = 'vit_b'\n\n# The checkpoint can be a filepath or a url\ncheckpoint = './path_for_the_model_checkpoint.pth'\ndevice = get_cuda_or_mps_device_if_available()\n\n# Load config\nconfig = SamConfig(model_type, checkpoint)\n\n# Load the model with checkpoint\nsam_model = Sam.from_config(config)\n\n# Move to desired device\nsam_model = sam_model.to(device)\n```\n\n----------------------------------------\n\nTITLE: Creating Dataloaders and Transforms for Object Detection in Python\nDESCRIPTION: Sets up the dataloaders and transforms for training object detection models. Includes dataset initialization for COCO format data and configuration of batch sizes and workers.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/object_detection.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# create the dataloaders and transforms\ndata_path = \"/tmp\"\n\n# create the train dataset\ntrain_dataset = CocoDetection(\n    os.path.join(data_path, \"train\"), os.path.join(data_path, \"annotations/train.json\")\n)\n\n# create the validation dataset\nvalid_dataset = CocoDetection(\n    os.path.join(data_path, \"val\"), os.path.join(data_path, \"annotations/val.json\")\n)\n\n# create the data loaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=4,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=collate_fn,\n)\nvalid_loader = DataLoader(\n    valid_dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Datasets and DataLoaders for CIFAR10 in Python\nDESCRIPTION: Sets up training and validation datasets using CIFAR10, and creates corresponding DataLoader objects for batch processing.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# datasets\ntrain_dataset = torchvision.datasets.CIFAR10(\n  root=config.data_path, train=True, download=True, transform=T.ToTensor())\n\nvalid_dataset = torchvision.datasets.CIFAR10(\n  root=config.data_path, train=False, download=True, transform=T.ToTensor())\n\n# dataloaders\ntrain_dataloader = torch.utils.data.DataLoader(\n  train_dataset, batch_size=config.batch_size, shuffle=True)\n\nvalid_daloader = torch.utils.data.DataLoader(\n  valid_dataset, batch_size=config.batch_size, shuffle=True)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using VisionTransformer in Python\nDESCRIPTION: This snippet demonstrates how to create a VisionTransformer instance and process an input image. It initializes a ViT model with specific image and patch sizes, then applies it to a random input tensor.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/models/vit.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimg = torch.rand(1, 3, 224, 224)\nvit = VisionTransformer(image_size=224, patch_size=16)\nout = vit(img)\n```\n\n----------------------------------------\n\nTITLE: Loading and Using TinyViT Model in Python with Kornia\nDESCRIPTION: This snippet demonstrates how to import, initialize, and use a TinyViT model from Kornia. It loads a pre-trained '5m' configuration model with ImageNet-1k weights and performs inference on a random input image.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/models/tiny_vit.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom kornia.contrib.models.tiny_vit import TinyViT\n\nmodel = TinyViT.from_config(\"5m\", pretrained=True)  # ImageNet-1k pre-trained\n\nimg = torch.rand(1, 3, 224, 224)\nout = classifier(img)     # 1x1000\n```\n\n----------------------------------------\n\nTITLE: Setting Up Loss Function, Optimizer, and Scheduler in Python\nDESCRIPTION: Defines a cross-entropy loss function, AdamW optimizer, and cosine annealing learning rate scheduler for training.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# loss function\ncriterion = nn.CrossEntropyLoss()\n\n# optimizer and scheduler\noptimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n  optimizer, config.num_epochs * len(train_dataloader)\n)\n```\n\n----------------------------------------\n\nTITLE: Image Transformation and Augmentation Example\nDESCRIPTION: Example showing how to use Kornia for image loading, resizing, augmentation using RandomAffine and RandomBrightness, and applying StableDiffusion dissolving effects.\nSOURCE: https://github.com/kornia/kornia/blob/main/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport kornia_rs as kr\n\nfrom kornia.augmentation import AugmentationSequential, RandomAffine, RandomBrightness\nfrom kornia.filters import StableDiffusionDissolving\n\n# Load and prepare your image\nimg: np.ndarray = kr.read_image_any(\"img.jpeg\")\nimg = kr.resize(img, (256, 256), interpolation=\"bilinear\")\n\n# alternatively, load image with PIL\n# img = Image.open(\"img.jpeg\").resize((256, 256))\n# img = np.array(img)\n\nimg = np.stack([img] * 2)  # batch images\n\n# Define an augmentation pipeline\naugmentation_pipeline = AugmentationSequential(\n    RandomAffine((-45., 45.), p=1.),\n    RandomBrightness((0.,1.), p=1.)\n)\n\n# Leveraging StableDiffusion models\ndslv_op = StableDiffusionDissolving()\n\nimg = augmentation_pipeline(img)\ndslv_op(img, step_number=500)\n\ndslv_op.save(\"Kornia-enhanced.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Instantiating and Running ObjectDetectionTrainer in Python\nDESCRIPTION: Sets up and executes the training pipeline using Kornia's ObjectDetectionTrainer. Configures the device, model, preprocessors, augmentations, and executes the training process with validation.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/object_detection.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# instantiate the ObjectDetectionTrainer and execute your training pipeline\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrainer = ObjectDetectionTrainer(\n    model=model,\n    train_dataloader=train_loader,\n    valid_dataloader=valid_loader,\n    criterion=None,  # Object detection models handle their loss internally\n    optimizer=optimizer,\n    scheduler=scheduler,\n    device=device,\n    train_preprocess=PreProcessing(),\n    valid_preprocess=PreProcessing(),\n    train_transform=TrainAugmentations(),\n    valid_transform=None,  # No augmentations for validation\n    metrics={\n        \"map\": MeanAveragePrecision(),\n    },\n)\n\n# Train the model for 10 epochs\ntrainer.fit(10)\n```\n\n----------------------------------------\n\nTITLE: Semantic Segmentation with Low-Level Randomness Control\nDESCRIPTION: An example pipeline for semantic segmentation that demonstrates low-level control of randomness by applying identical transformations to both input images and masks to maintain alignment.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/image_augmentations.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport kornia.augmentation as K\n\nclass MyAugmentationPipeline(nn.Module):\n   def __init__(self) -> None:\n      super(MyAugmentationPipeline, self).__init__()\n      self.aff = K.RandomAffine(360)\n      self.jit = K.ColorJiggle(0.2, 0.3, 0.2, 0.3)\n\n   def forward(self, input, mask):\n      assert input.shape == mask.shape,\\\n         f\"Input shape should be consistent with mask shape, \"\\\n         f\"while got {input.shape}, {mask.shape}\"\n\n      aff_params = self.aff.forward_parameters(input.shape)\n      input = self.aff(input, aff_params)\n      mask = self.aff(mask, aff_params)\n\n      jit_params = self.jit.forward_parameters(input.shape)\n      input = self.jit(input, jit_params)\n      mask = self.jit(mask, jit_params)\n      return input, mask\n```\n\n----------------------------------------\n\nTITLE: Instantiating and Executing Image Classifier Trainer in Python\nDESCRIPTION: This snippet shows how to create an ImageClassifierTrainer instance with the defined model, loss function, optimizer, scheduler, and augmentations. It then executes the training process for a specified number of epochs.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/image_classification.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom kornia.x import ImageClassifierTrainer\n\ntrainer = ImageClassifierTrainer(\n    model, train_loader, test_loader,\n    optimizer, scheduler, loss_fn,\n    aug_config=aug_config,\n    device='cuda'\n)\n\nnum_epochs = 10\nfor i in range(num_epochs):\n    trainer.step()\n    # val_metrics = trainer.evaluate()\n    trainer.save_checkpoint(f'checkpoint_{i}.ckpt')\n```\n\n----------------------------------------\n\nTITLE: Object Detection with RTDETRDetectorBuilder in Python\nDESCRIPTION: Demonstrates how to use the RTDETRDetectorBuilder class to detect objects in an image using Kornia's implementation of RT-DETR architecture.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/models.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport kornia\nimage = kornia.utils.sample.get_sample_images()[0][None]\nmodel = kornia.models.detection.rtdetr.RTDETRDetectorBuilder.build()\nmodel.save(image)\n```\n\n----------------------------------------\n\nTITLE: Defining Model, Loss, Optimizer, and Scheduler for Image Classification in Python\nDESCRIPTION: This snippet demonstrates how to set up a ResNet18 model, cross-entropy loss, Adam optimizer, and a step learning rate scheduler for image classification training.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/image_classification.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import resnet18\n\nmodel = resnet18(pretrained=True)\nmodel.fc = nn.Linear(512, 10)  # 10 classes in CIFAR10\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n```\n\n----------------------------------------\n\nTITLE: Defining Model, Loss Functions, and Optimizers for Semantic Segmentation in Python\nDESCRIPTION: Initializes a semantic segmentation model from torchvision, along with a Cross Entropy loss function, Adam optimizer, and a learning rate scheduler for training.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/semantic_segmentation.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Define your model, losses, optimizers and schedulers:\n\nmodel = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\nmodel.classifier[-1] = torch.nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n```\n\n----------------------------------------\n\nTITLE: Defining Augmentations for Image Classification Training in Python\nDESCRIPTION: This code sets up image augmentations using Kornia's augmentation module. It includes random affine transformations, color jitter, and random erasing for data augmentation during training.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/image_classification.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport kornia.augmentation as K\n\ntrain_augs = nn.Sequential(\n    K.RandomAffine(degrees=(-15., 20.), scale=(0.9, 1.1)),\n    K.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n    K.RandomErasing()\n)\n\nval_augs = nn.Sequential()\n\naug_config = {\n    'train': train_augs,\n    'val': val_augs\n}\n```\n\n----------------------------------------\n\nTITLE: Image Segmentation with SegmentationModelsBuilder in Python\nDESCRIPTION: Demonstrates binary segmentation using the SegmentationModelsBuilder class which supports multiple architectures like UNet and DeepLabV3.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/models.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport kornia\ninput_tensor = kornia.utils.sample.get_sample_images()[0][None]\nmodel = kornia.models.segmentation.segmentation_models.SegmentationModelsBuilder.build()\nsegmented_output = model(input_tensor)\nprint(segmented_output.shape)\n```\n\n----------------------------------------\n\nTITLE: Chaining ONNX Models with Kornia's ONNXSequential\nDESCRIPTION: Creates a sequential pipeline of ONNX models by combining a horizontal flip transform and an object detection model from HuggingFace. Demonstrates model chaining, inference with random input data, and exporting the combined model to a new ONNX file.\nSOURCE: https://github.com/kornia/kornia/blob/main/README.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Chain ONNX models from HuggingFace repo and your own local model together\nonnx_seq = ONNXSequential(\n    \"hf://operators/kornia.geometry.transform.flips.Hflip\",\n    \"hf://models/kornia.models.detection.rtdetr_r18vd_640x640\",  # Or you may use \"YOUR_OWN_MODEL.onnx\"\n)\n# Prepare some input data\ninput_data = np.random.randn(1, 3, 384, 512).astype(np.float32)\n# Perform inference\noutputs = onnx_seq(input_data)\n# Print the model outputs\nprint(outputs)\n\n# Export a new ONNX model that chains up all three models together!\nonnx_seq.export(\"chained_model.onnx\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Task Learning with VisionTransformer in Python\nDESCRIPTION: This snippet defines a MultiTaskTransformer class that uses a VisionTransformer as the base model and two separate ClassificationHead instances for different tasks. It demonstrates how to create a flexible multi-task learning setup using Kornia's ViT implementation.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/models/vit.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MultiTaskTransfornmer(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.transformer = K.VisionTransformer(\n            image_size=224, patch_size=16)\n        self.head1 = K.ClassificationHead(num_classes=10)\n        self.head2 = K.ClassificationHead(num_classes=50)\n\n    def forward(self, x: torch.Tensor):\n        out = self.transformer(x)\n        return {\n            \"head1\": self.head1(out),\n            \"head2\": self.head2(out),\n        }\n```\n\n----------------------------------------\n\nTITLE: Defining a Vision Transformer Model in Python\nDESCRIPTION: Creates a sequential model combining a Vision Transformer and a Classification Head for image classification tasks.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmodel = nn.Sequential(\n  kornia.contrib.VisionTransformer(image_size=32, patch_size=16),\n  kornia.contrib.ClassificationHead(num_classes=10),\n)\n```\n\n----------------------------------------\n\nTITLE: Video Augmentation with VideoSequential in Kornia\nDESCRIPTION: Demonstrates how to use VideoSequential for video data augmentation, maintaining consistent transformations across frames with support for different data formats like (B, C, T, H, W) and (B, T, C, H, W).\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/image_augmentations.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport kornia.augmentation as K\n\ntransform = K.VideoSequential(\n   K.RandomAffine(360),\n   K.RandomGrayscale(p=0.5),\n   K.RandomAffine(p=0.5),\n   data_format=\"BCTHW\",\n   same_on_frame=True\n)\n```\n\n----------------------------------------\n\nTITLE: Customizing Augmentation Pipeline with Transform Control\nDESCRIPTION: A custom augmentation pipeline that demonstrates controlling transformation parameters like return_transform and same_on_batch for advanced augmentation behaviors such as enabling reverse transformations.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/image_augmentations.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport kornia.augmentation as K\n\nclass MyAugmentationPipeline(nn.Module):\n   def __init__(self) -> None:\n      super(MyAugmentationPipeline, self).__init__()\n      self.aff = K.RandomAffine(\n         360, return_transform=True, same_on_batch=True\n      )\n      self.jit = K.ColorJiggle(0.2, 0.3, 0.2, 0.3, same_on_batch=True)\n\n   def forward(self, input):\n      input, transform = self.aff(input)\n      input, transform = self.jit((input, transform))\n      return input, transform\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a Kornia Trainer in Python\nDESCRIPTION: Instantiates a Kornia Trainer object with the model, dataloaders, loss function, optimizer, and scheduler, then starts the training process.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrainer = kornia.train.Trainer(\n  model, train_dataloader, valid_daloader, criterion, optimizer, scheduler, config,\n)\ntrainer.fit()  # execute your training !\n```\n\n----------------------------------------\n\nTITLE: Creating an Image Classifier with MobileViT\nDESCRIPTION: Example of building a complete image classifier using MobileViT as the backbone. This code creates a sequential model combining MobileViT with pooling, flattening, and a linear classifier layer.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/models/vit_mobile.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nimport kornia.contrib as K\n\nclassifier = nn.Sequential(\n    K.MobileViT(mode='xxs'),\n    nn.AvgPool2d(256 // 32, 1),\n    nn.Flatten(),\n    nn.Linear(320, 1000)\n)\n\nimg = torch.rand(1, 3, 256, 256)\nout = classifier(img)     # 1x1000\n```\n\n----------------------------------------\n\nTITLE: Edge Detection with DexiNedBuilder in Python\nDESCRIPTION: Shows how to perform edge detection on an image using the DexiNedBuilder class which implements the DexiNed architecture for fine-grained edge detection.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/models.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport kornia\nimage = kornia.utils.sample.get_sample_images()[0][None]\nmodel = kornia.models.edge_detection.dexined.DexiNedBuilder.build()\nmodel.save(image)\n```\n\n----------------------------------------\n\nTITLE: Basic Image Augmentation with Sequential in Kornia\nDESCRIPTION: A simple example demonstrating how to create an image augmentation pipeline using torch.nn.Sequential with Kornia's RandomAffine and ColorJiggle transforms.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/image_augmentations.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport kornia.augmentation as K\nimport torch.nn as nn\n\ntransform = nn.Sequential(\n   K.RandomAffine(360),\n   K.ColorJiggle(0.2, 0.3, 0.2, 0.3)\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Preprocess and Augmentations in Kornia Trainer\nDESCRIPTION: Demonstrates how to define custom preprocessing and augmentation functions and pass them to the Trainer as callbacks.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess(x):\n  return x.float() / 255.\n\naugmentations = nn.Sequential(\n  K.augmentation.RandomHorizontalFlip(p=0.75),\n  K.augmentation.RandomVerticalFlip(p=0.75),\n  K.augmentation.RandomAffine(degrees=10.),\n  K.augmentation.PatchSequential(\n    K.augmentation.ColorJiggle(0.1, 0.1, 0.1, 0.1, p=0.8),\n    grid_size=(2, 2),  # cifar-10 is 32x32 and vit is patch 16\n    patchwise_apply=False,\n  ),\n)\n\n# create the trainer and pass the augmentation or preprocess\ntrainer = K.train.ImageClassifierTrainer(...,\n  callbacks={\"preprocess\", preprocess, \"augmentations\": augmentations})\n```\n\n----------------------------------------\n\nTITLE: Using CUDA Execution Provider for Optimized Inference\nDESCRIPTION: Example showing how to initialize ONNXSequential with the CUDA execution provider to leverage GPU acceleration for optimized model inference.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/onnx.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Initialize with CUDA execution provider\nonnx_seq = ONNXSequential(\n   \"hf://operators/kornia.geometry.transform.flips.Hflip\",\n   # Or you may use a local model with either a filepath \"YOUR_OWN_MODEL.onnx\" or a loaded ONNX model.\n   \"hf://models/kornia.models.detection.rtdetr_r18vd_640x640\",\n   providers=['CUDAExecutionProvider']\n)\n\n# Run inference\noutputs = onnx_seq(input_data)\n```\n\n----------------------------------------\n\nTITLE: RTDETR Object Detection Implementation\nDESCRIPTION: Demonstrates object detection using the RTDETRDetector model with sample images.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/index.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimage = kornia.utils.sample.get_sample_images()[0][None]\nmodel = RTDETRDetectorBuilder.build()\nmodel.save(image)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Geometric Augmentation in Kornia\nDESCRIPTION: This code demonstrates how to create a custom rigid geometric augmentation by extending GeometricAugmentationBase2D. The example implements a simple transformation that applies a random factor to the input image, with customizable factor range and batch behavior.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/augmentation.base.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport kornia as K\n\nfrom kornia.augmentation import GeometricAugmentationBase2D\nfrom kornia.augmentation import random_generator as rg\n\n\nclass MyRandomTransform(GeometricAugmentationBase2D):\n\n   def __init__(\n      self,\n      factor=(0., 1.),\n      same_on_batch: bool = False,\n      p: float = 1.0,\n      keepdim: bool = False,\n   ) -> None:\n      super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)\n      self._param_generator = rg.PlainUniformGenerator((factor, \"factor\", None, None))\n\n   def compute_transformation(self, input, params):\n      # a simple identity transformation example\n      factor = params[\"factor\"].to(input) * 0. + 1\n      return K.eyelike(input, 3) * factor\n\n   def apply_transform(\n      self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None\n   ) -> Tensor:\n      factor = params[\"factor\"].to(input)\n      return input * factor\n```\n\n----------------------------------------\n\nTITLE: Customizing Fit Method in Kornia Trainer\nDESCRIPTION: Shows how to override the entire fit method of the Trainer with a custom PyTorch training loop.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef my_fit(self, ):  # this is a custom pytorch training loop\n  self.model.train()\n  for epoch in range(self.num_epochs):\n    for source, targets in self.train_dataloader:\n      self.optimizer.zero_grad()\n\n      output = self.model(source)\n      loss = self.criterion(output, targets)\n\n      self.backward(loss)\n      self.optimizer.step()\n\n      stats = self.evaluate()  # do whatever you want with validation\n\n# create the trainer and pass the evaluate method as follows\ntrainer = K.train.Trainer(..., callbacks={\"fit\", my_fit})\n```\n\n----------------------------------------\n\nTITLE: Matrix Transformation for Pinhole Camera Model\nDESCRIPTION: Mathematical representation of the perspective transformation that maps 3D points to 2D image coordinates using the pinhole camera model. Includes the camera calibration matrix K and the extrinsic matrix [R|t].\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/geometry.camera.pinhole.rst#2025-04-21_snippet_0\n\nLANGUAGE: math\nCODE:\n```\ns \\begin{bmatrix} u \\\\ v \\\\ 1\\end{bmatrix} = \\begin{bmatrix} f_x & 0 & u_0 \\\\ 0 & f_y & v_0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_1  \\\\ r_{21} & r_{22} & r_{23} & t_2  \\\\ r_{31} & r_{32} & r_{33} & t_3 \\end{bmatrix} \\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}\n```\n\n----------------------------------------\n\nTITLE: Multi-Object Tracking with BoxMotTracker in Python\nDESCRIPTION: Shows how to use the BoxMotTracker class for tracking multiple objects across video frames, including updating tracker state and saving results.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/models.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport kornia\nimage = kornia.utils.sample.get_sample_images()[0][None]\nmodel = kornia.models.tracking.boxmot_tracker.BoxMotTracker()\nfor i in range(4):\n    model.update(image)  # Update the tracker with new frames\nmodel.save(image)       # Save the tracking result\n```\n\n----------------------------------------\n\nTITLE: Combining ONNX Models with ONNXSequential\nDESCRIPTION: Example demonstrating how to initialize an ONNXSequential instance with two ONNX models from a Hugging Face repository, prepare random input data, and perform inference.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/onnx.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom kornia.onnx import ONNXSequential\n\n# Initialize ONNXSequential with two models, loading from our only repo\nonnx_seq = ONNXSequential(\n   \"hf://operators/kornia.color.gray.RgbToGrayscale\",\n   \"hf://operators/kornia.geometry.transform.affwarp.Resize_512x512\"\n)\n\n# Prepare some input data\ninput_data = np.random.randn(1, 3, 256, 512).astype(np.float32)\n\n# Perform inference\noutputs = onnx_seq(input_data)\n\n# Print the model outputs\nprint(outputs)\n```\n\n----------------------------------------\n\nTITLE: Using Gaussian Blur in Python with Kornia\nDESCRIPTION: This snippet demonstrates how to apply Gaussian blur to an image using Kornia's gaussian_blur2d function. The function takes an input tensor and blur parameters.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/filters.rst#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nblurred_image = filters.gaussian_blur2d(input_image, kernel_size=(5, 5), sigma=(1.0, 1.0))\n```\n\n----------------------------------------\n\nTITLE: Applying Canny Edge Detection in Python with Kornia\nDESCRIPTION: This code shows how to use Kornia's Canny edge detection on an image. The Canny function detects edges in the input image based on intensity gradients.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/filters.rst#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nedges = filters.canny(input_image, low_threshold=0.1, high_threshold=0.2)\n```\n\n----------------------------------------\n\nTITLE: Running image classifier with FP16 using Accelerate\nDESCRIPTION: Command to run the image classifier with mixed-precision (FP16) using Accelerate launcher. The --fp16 flag enables mixed-precision training.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --fp16 ./train/image_classifier/main.py --data_path path_to_data\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Camera Model in Python\nDESCRIPTION: Demonstrates how to create a custom camera model using CameraModelBase, AffineTransform for distortion, and Z1Projection for projection. This approach allows for flexible camera model definition with custom distortion and projection characteristics.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/sensors.camera.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kornia.image import ImageSize\nfrom kornia.sensors.camera import CameraModelBase\nfrom kornia.sensors.camera.distortion_model import AffineTransform\nfrom kornia.sensors.camera.projection_model import Z1Projection\n\nparams = torch.tensor([328., 328., 320., 240.])\ncam = CameraModelBase(AffineTransform(), Z1Projection(), ImageSize(480, 640), params)\n```\n\n----------------------------------------\n\nTITLE: Comparing CPU and GPU Performance with ONNXSequential\nDESCRIPTION: Example demonstrating how to benchmark ONNXSequential performance by switching between CPU and CUDA execution providers, showing the speed difference.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/onnx.rst#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport kornia\nonnx_seq = ONNXSequential(\n   \"hf://operators/kornia.geometry.transform.flips.Hflip\",\n   \"hf://models/kornia.models.detection.rtdetr_r18vd_640x640\",  # Or you may use \"YOUR_OWN_MODEL.onnx\"\n)\ninp = kornia.utils.sample.get_sample_images()[0].numpy()[None]\nimport time\nonnx_seq.as_cuda()\nonnx_seq(inp)  # GPU warm up\nstart_time = time.time()\nonnx_seq(inp)\nprint(\"--- GPU %s seconds ---\" % (time.time() - start_time))\nonnx_seq.as_cpu()\nstart_time = time.time()\nonnx_seq(inp)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n```\n\n----------------------------------------\n\nTITLE: DexiNed Edge Detection Model Usage\nDESCRIPTION: Shows how to use the DexiNed model for edge detection with a sample image.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/index.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimage = kornia.utils.sample.get_sample_images()[0][None]\nmodel = DexiNedBuilder.build()\nmodel.save(image)\n```\n\n----------------------------------------\n\nTITLE: Initializing MobileViT in Kornia\nDESCRIPTION: Basic usage example of MobileViT in Kornia to process an image. The example initializes a MobileViT model with 'xxs' mode and passes an image tensor through it.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/models/vit_mobile.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimg = torch.rand(1, 3, 256, 256)\nmvit = MobileViT(mode='xxs')\nout = mvit(img)\n```\n\n----------------------------------------\n\nTITLE: Loading ONNX Models with ONNXLoader Utility\nDESCRIPTION: Example demonstrating how to use the ONNXLoader utility to load ONNX models either from Hugging Face repositories or from local files.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/onnx.rst#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Load a HuggingFace operator\nONNXLoader.load_model(\"hf://operators/kornia.color.gray.GrayscaleToRgb\")  # doctest: +SKIP\n# Load a local converted/downloaded operator\nONNXLoader.load_model(\"operators/kornia.color.gray.GrayscaleToRgb\")  # doctest: +SKIP\n```\n\n----------------------------------------\n\nTITLE: Implementing Test Class Structure for Kornia Functions in Python\nDESCRIPTION: This snippet demonstrates the recommended structure for writing test classes in the Kornia project. It includes methods for smoke tests, exception handling, cardinality checks, feature tests, gradient checks, and Dynamo optimization tests.\nSOURCE: https://github.com/kornia/kornia/blob/main/CONTRIBUTING.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom testing.base import BaseTester\n\nclass TestMyFunction(BaseTester):\n    # To compare the actual and expected tensors use `self.assert_close(...)`\n\n\n    def test_smoke(self, device, dtype):\n        # test the function with different parameters arguments, to check if the function at least runs with all the\n        # arguments allowed.\n        pass\n\n    def test_exception(self, device, dtype):\n        # tests the exceptions which can occur on your function\n\n        # example of how to properly test your exceptions\n        # with pytest.raises(<raised Error>) as errinfo:\n        #     your_function(<set of parameters that raise the error>)\n        # assert '<msg of error>' in str(errinfo)\n\n        pass\n\n    def test_cardinality(self, device, dtype):\n        # test if with different parameters the shape of the output is the expected\n        pass\n\n    def test_feature_foo(self, device, dtype):\n        # test basic functionality\n        pass\n\n    def test_feature_bar(self, device, dtype):\n        # test another functionality\n        pass\n\n    def test_gradcheck(self, device):\n        # test the functionality gradients\n        # Uses `self.gradcheck(...)`\n        pass\n\n    def test_dynamo(self, device, dtype, torch_optimizer):\n        #  test the functionality using dynamo optimizer\n\n        # Example of how to properly test your function for dynamo\n        # inputs = (...)\n        # op = your_function\n        # op_optimized = torch_optimizer(op)\n        # self.assert_close(op(inputs), op_optimized(inputs))\n\n        pass\n```\n\n----------------------------------------\n\nTITLE: Using Kornia with NumPy\nDESCRIPTION: This code shows how to use Kornia with NumPy arrays. Similar to the TensorFlow example, it creates a NumPy-compatible version of Kornia through Ivy's transpilation.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/multi-framework-support.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport kornia\nimport numpy as np\n\nnp_kornia = kornia.to_numpy()\n\nrgb_image = np.random.normal(size=(1, 3, 224, 224))\ngray_image = np_kornia.color.rgb_to_grayscale(rgb_image)\n```\n\n----------------------------------------\n\nTITLE: Using Callback Utilities in Kornia Trainer\nDESCRIPTION: Shows how to use ModelCheckpoint and EarlyStopping callbacks with the Trainer for saving checkpoints and early stopping.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef my_evaluate(self):\n  # stats = StatsTracker()\n  # loss = nn.CrossEntropyLoss()\n  # rest of your evaluation loop\n  prediction = self.on_model(self.model, sample)\n  val_loss = self.compute_loss(out, sample[\"mask\"])\n  stats.update(\"loss\", val_loss.item(), batch_size)\n  return stats.as_dict()\n\nmodel_checkpoint = ModelCheckpoint(\n    filepath=\"./outputs\", monitor=\"loss\",\n)\n\nearly_stop = EarlyStopping(\n    monitor=\"loss\", patience=2\n)\n\ntrainer = SemanticSegmentationTrainer(...,\n    callbacks={\"on_epoch_end\": early_stop, \"on_checkpoint\", model_checkpoint}\n)\n```\n\n----------------------------------------\n\nTITLE: Using Kornia with JAX\nDESCRIPTION: This code demonstrates using Kornia with JAX arrays. It shows how to create a JAX-compatible version of Kornia and apply image processing functions to JAX arrays.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/multi-framework-support.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport kornia\nimport jax\n\njax_kornia = kornia.to_jax()\n\nrgb_image = jax.random.normal(jax.random.key(42), shape=(1, 3, 224, 224))\ngray_image = jax_kornia.color.rgb_to_grayscale(rgb_image)\n```\n\n----------------------------------------\n\nTITLE: Customizing Evaluation Function in Kornia Trainer\nDESCRIPTION: Demonstrates how to create a custom evaluation function and pass it to the Trainer as a callback.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@torch.no_grad()\ndef my_evaluate(self) -> dict:\n  self.model.eval()\n  for sample_id, sample in enumerate(self.valid_dataloader):\n    source, target = sample  # this might change with new pytorch ataset structure\n\n    # perform the preprocess and augmentations in batch\n    img = self.preprocess(source)\n    # Forward\n    out = self.model(img)\n    # Loss computation\n    val_loss = self.criterion(out, target)\n\n    # measure accuracy and record loss\n    acc1, acc5 = accuracy(out.detach(), target, topk=(1, 5))\n\n# create the trainer and pass the evaluate method as follows\ntrainer = K.train.Trainer(..., callbacks={\"evaluate\", my_evaluate})\n```\n\n----------------------------------------\n\nTITLE: Box Motion Tracking Implementation\nDESCRIPTION: Shows how to implement object tracking using BoxMotTracker with multiple frame updates.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/index.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport kornia\nimage = kornia.utils.sample.get_sample_images()[0][None]\nmodel = BoxMotTracker()\nfor i in range(4):\n   model.update(image)\nmodel.save(image)\n```\n\n----------------------------------------\n\nTITLE: Parameterized Testing for Different Batch Sizes in Python\nDESCRIPTION: This snippet shows how to use pytest's parametrize decorator to test a function with different batch sizes. It demonstrates testing across various devices and dtypes using fixtures.\nSOURCE: https://github.com/kornia/kornia/blob/main/CONTRIBUTING.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\n@pytest.mark.parametrize(\"batch_size\", [1, 2, 5])\ndef test_smoke(batch_size, device, dtype):\n    x = torch.rand(batch_size, 2, 3, device=device, dtype=dtype)\n    assert x.shape == (batch_size, 2, 3)\n```\n\n----------------------------------------\n\nTITLE: Depth Processing Functions in Kornia\nDESCRIPTION: Core functions exposed by the kornia.geometry.depth module including depth_from_disparity, depth_to_3d, depth_to_3d_v2, unproject_meshgrid, depth_to_normals, depth_from_plane_equation, and warp_frame_depth. These functions handle various depth-related transformations and computations for 3D vision applications.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/geometry.depth.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndepth_from_disparity\ndepth_to_3d\ndepth_to_3d_v2\nunproject_meshgrid\ndepth_to_normals\ndepth_from_plane_equation\nwarp_frame_depth\n```\n\n----------------------------------------\n\nTITLE: Defining Q Matrix for Stereo Reprojection in LaTeX\nDESCRIPTION: This LaTeX code defines the Q matrix used for reprojecting 2D pixels with disparity to 3D space in stereo vision. It includes focal lengths (fx, fy), principal point coordinates (cx, cy), and the horizontal baseline (tx).\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/geometry.camera.stereo.rst#2025-04-21_snippet_3\n\nLANGUAGE: LaTeX\nCODE:\n```\nQ = \\begin{bmatrix}\nfy * tx & 0       & 0   & -fy * cx * tx \\\\\n0       & fx * tx & 0   & -fx * cy * tx \\\\\n0       & 0       & 0   & fx * fy * tx  \\\\\n0       & 0       & -fy & fy * (cx_{left} -cx_{right})\n\\end{bmatrix}\n```\n\n----------------------------------------\n\nTITLE: Mapping Inputs and Outputs Between ONNX Models\nDESCRIPTION: Example showing how to specify custom input/output mapping between models when chaining them with ONNXSequential, allowing for custom connection patterns.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/onnx.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nio_map = [(\"model1_output_0\", \"model2_input_0\"), (\"model1_output_1\", \"model2_input_1\")]\nonnx_seq = ONNXSequential(\"model1.onnx\", \"model2.onnx\", io_map=io_map)\n```\n\n----------------------------------------\n\nTITLE: Extrinsic Matrix Format\nDESCRIPTION: Format of the extrinsic matrix used in the PinholeCamera class, representing camera pose through rotation and translation in a 4x4 matrix format.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/geometry.camera.pinhole.rst#2025-04-21_snippet_2\n\nLANGUAGE: math\nCODE:\n```\n\\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_1  \\\\ r_{21} & r_{22} & r_{23} & t_2  \\\\ r_{31} & r_{32} & r_{33} & t_3  \\\\ 0      & 0      & 0      & 1 \\end{bmatrix}\n```\n\n----------------------------------------\n\nTITLE: Type Hinting for Kornia Functions in Python\nDESCRIPTION: This example demonstrates the recommended way to add type hints to functions in the Kornia project. It shows proper annotation for function inputs and outputs, including the use of Tensor types from kornia.core.\nSOURCE: https://github.com/kornia/kornia/blob/main/CONTRIBUTING.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nfrom kornia.core import Tensor\n\ndef homography_warp(\n  patch_src: Tensor,\n  dst_homo_src: Tensor,\n  dsize: tuple[int, int],\n  mode: str = 'bilinear',\n  padding_mode: str = 'zeros'\n) -> Tensor:\n```\n\n----------------------------------------\n\nTITLE: Exporting Combined ONNX Models\nDESCRIPTION: Code snippet demonstrating how to export multiple combined ONNX models into a single ONNX file for easier deployment and sharing.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/onnx.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Export the combined model to a file\nonnx_seq.export(\"combined_model.onnx\")\n```\n\n----------------------------------------\n\nTITLE: Importing Quaternion Class from Kornia Geometry Module in Python\nDESCRIPTION: This snippet shows how to import the Quaternion class from the kornia.geometry.quaternion module. The Quaternion class provides tools for working with quaternions, which are used in 3D geometry and computer vision for representing rotations and transformations.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/geometry.quaternion.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom kornia.geometry.quaternion import Quaternion\n```\n\n----------------------------------------\n\nTITLE: Running image classifier with FP16 using Python\nDESCRIPTION: Command to run the image classifier with mixed-precision (FP16) directly using Python. Passes the fp16 flag to enable mixed-precision training.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython ./train/image_classifier/main.py --data_path path_to_data --fp16\n```\n\n----------------------------------------\n\nTITLE: First Call to Transpiled TensorFlow Function\nDESCRIPTION: This code shows the initial (slow) call to a transpiled Kornia function with TensorFlow tensors. The function is being transpiled during this step, which causes the slowdown.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/multi-framework-support.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nrgb_image = tf.random.normal((1, 3, 224, 224))\ngray_image = tf_kornia.color.rgb_to_grayscale(rgb_image)  # slow\n```\n\n----------------------------------------\n\nTITLE: Defining a Pinhole Camera Model in Python\nDESCRIPTION: Creates a Pinhole camera model using CameraModel and CameraModelType. It initializes the camera with parameters including focal lengths (fx, fy) and principal point coordinates (cx, cy) along with the image dimensions.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/sensors.camera.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kornia.image import ImageSize\nfrom kornia.sensors.camera import CameraModel, CameraModelType\n\nparams = torch.tensor([328., 328., 320., 240.]) # fx, fy, cx, cy\ncam = CameraModel(ImageSize(480, 640), CameraModelType.PINHOLE, params)\n```\n\n----------------------------------------\n\nTITLE: Subsequent Calls to Transpiled TensorFlow Function\nDESCRIPTION: This demonstrates that subsequent calls to already transpiled functions are much faster, as the transpilation only happens once. Performance should match the original Kornia function.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/multi-framework-support.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ngray_image = tf_kornia.color.rgb_to_grayscale(rgb_image)  # fast\n```\n\n----------------------------------------\n\nTITLE: Running image classifier training on CPU with Accelerate\nDESCRIPTION: Command to launch the image classifier training script on CPU using Accelerate. Specifies the data path as a parameter.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --cpu ./train/image_classifier/main.py --data_path path_to_data\n```\n\n----------------------------------------\n\nTITLE: Vision Transformer Classification Setup\nDESCRIPTION: Demonstrates setting up a Vision Transformer for image classification with custom patch size and class count.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/index.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch.nn as nn\n>>> import kornia.contrib as K\n>>> classifier = nn.Sequential(\n...   K.VisionTransformer(image_size=224, patch_size=16),\n...   K.ClassificationHead(num_classes=1000),\n... )\n>>> logits = classifier(img)    # BxN\n>>> scores = logits.argmax(-1)  # B\n```\n\n----------------------------------------\n\nTITLE: Importing Kornia Geometry Boxes Module in Python\nDESCRIPTION: This snippet demonstrates how to import the kornia.geometry.boxes module in Python. The module provides functions for manipulating 2D and 3D bounding boxes, which are crucial for spatial data processing in computer vision applications.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/geometry.boxes.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom kornia.geometry import boxes\n```\n\n----------------------------------------\n\nTITLE: Running image classifier training on single GPU with Python\nDESCRIPTION: Direct Python command to run the image classifier training script on a server with a GPU.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython ./train/image_classifier/main.py  # from a server with a GPU\n```\n\n----------------------------------------\n\nTITLE: Importing Camera Geometry Module in Kornia\nDESCRIPTION: This snippet shows how to import the camera geometry module in Kornia. It sets up the current module context for the documentation.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/geometry.camera.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. currentmodule:: kornia.geometry.camera\n```\n\n----------------------------------------\n\nTITLE: Implementing Video Augmentation with VideoSequential in Kornia\nDESCRIPTION: Demonstrates how to create a video augmentation pipeline using Kornia's VideoSequential class. This example shows how to apply random affine transformations and color jiggle consistently across all frames in a video sequence with BCTHW data format.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/augmentation.container.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport kornia.augmentation as K\n\ntransform = K.VideoSequential(\n   K.RandomAffine(360),\n   K.ColorJiggle(0.2, 0.3, 0.2, 0.3),\n   data_format=\"BCTHW\",\n   same_on_frame=True\n)\n```\n\n----------------------------------------\n\nTITLE: Defining kornia.tracking Module Documentation in reStructuredText\nDESCRIPTION: This snippet defines the documentation structure for the kornia.tracking module using reStructuredText. It includes a meta description, sets the current module, and declares the HomographyTracker class for detailed documentation.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/tracking.rst#2025-04-21_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\nkornia.tracking\n===============\n\n.. meta::\n   :name: description\n   :content: \"The `kornia.tracking` module provides tools for tracking objects across frames in computer vision tasks. It includes classes like `HomographyTracker` to estimate homography transformations and track objects over time.\"\n\n.. currentmodule:: kornia.tracking\n\n.. autoclass:: HomographyTracker\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Running Kornia benchmarks in Bash\nDESCRIPTION: Commands to run various benchmarks for Kornia, including options for specific files, backends, and devices.\nSOURCE: https://github.com/kornia/kornia/blob/main/CONTRIBUTING.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# To run all suite\n$ make benchmark\n\n# To run a specific file you can pass `BENCHMARK_SOURCE`\n$ make benchmark BENCHMARK_SOURCE=benchmarks/augmentation/2d_geometric_test.py\n\n# To run a specific benchmark you use `BENCHMARK_SOURCE` as the pytest standard behaviour\n$ make benchmark BENCHMARK_SOURCE=benchmarks/augmentation/2d_geometric_test.py::test_aug_2d_elastic_transform\n\n# To update the optimizer backends desired to execute you can pass `BENCHMARK_BACKENDS=`\n$ make benchmark BENCHMARK_BACKENDS='inductor,eager'\n\n# To pass other options to the runner, you can use `BENCHMARK_OPTS`\n# Example, setup to run the benchmark on cuda on verbose mode\n$ make benchmark BENCHMARK_OPTS='--device=cuda -vv'\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ make benchmark-docker\n```\n\n----------------------------------------\n\nTITLE: Loading Images with Kornia IO in Different Formats\nDESCRIPTION: This snippet demonstrates various ways to load images using Kornia's IO module with different ImageLoadType options and device targets. It shows how to load images in their original format, as RGB or grayscale, with uint8 or float32 data types.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/io.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport kornia as K\nfrom kornia.io import ImageLoadType\nfrom kornia.core import Tensor\n\nimg: Tensor = K.io.load_image(file_path, ImageLoadType.UNCHANGED, device=\"cuda\")\n# will load CxHxW / in the original format in \"cuda\"\n\nimg: Tensor = K.io.load_image(file_path, ImageLoadType.RGB8, device=\"cpu\")\n# will load 3xHxW / in torch.uint in range [0,255] in \"cpu\"\n\nimg: Tensor = K.io.load_image(file_path, ImageLoadType.GRAY8, device=\"cuda\")\n# will load 1xHxW / in torch.uint8 in range [0,255] in \"cuda\"\n\nimg: Tensor = K.io.load_image(file_path, ImageLoadType.GRAY32, device=\"cpu\")\n# will load 1xHxW / in torch.float32 in range [0,1] in \"cpu\"\n\nimg: Tensor = K.io.load_image(file_path, ImageLoadType.RGB32, device=\"cuda\")\n# will load 3xHxW / in torch.float32 in range [0,1] in \"cuda\"\n```\n\n----------------------------------------\n\nTITLE: RestructuredText Module Documentation\nDESCRIPTION: Documentation structure for the kornia.morphology module, including meta description, function references, and interactive demo integration.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/morphology.rst#2025-04-21_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\nkornia.morphology\n======================\n\n.. meta::\n   :name: description\n   :content: \"The Kornia.morphology module offers a set of morphological image processing operations such as dilation, erosion, opening, closing, gradient, top hat, and bottom hat. It enables users to apply these transformations to images for advanced computer vision tasks. Try the interactive demo on Hugging Face Spaces.\"\n\n.. currentmodule::  kornia.morphology\n\n.. autofunction:: dilation\n.. autofunction:: erosion\n.. autofunction:: opening\n.. autofunction:: closing\n.. autofunction:: gradient\n.. autofunction:: top_hat\n.. autofunction:: bottom_hat\n\nInteractive Demo\n~~~~~~~~~~~~~~~~\n.. raw:: html\n\n    <gradio-app src=\"kornia/morphological_operators\"></gradio-app>\n\nVisit the demo on `Hugging Face Spaces <https://huggingface.co/spaces/kornia/morphological_operators>`_.\n```\n\n----------------------------------------\n\nTITLE: Running Kornia tests in Bash\nDESCRIPTION: Command to run Kornia tests with specified data types and devices.\nSOURCE: https://github.com/kornia/kornia/blob/main/CONTRIBUTING.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ pytest tests/<TEST_TO_RUN>.py --dtype=float32,float64 --device=all\n```\n\n----------------------------------------\n\nTITLE: Documenting Image Class in Kornia\nDESCRIPTION: This snippet shows the documentation for the Image class using Sphinx autodoc. It includes all members and undocumented members.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/image.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: Image\n    :members:\n    :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Image Registration with Kornia Geometry\nDESCRIPTION: Demonstrates how to perform image registration using Kornia's geometry module with similarity transformation.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/index.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import kornia.geometry as K\n>>> registrator = K.ImageRegistrator('similarity')\n>>> model = registrator.register(img1, img2)\n```\n\n----------------------------------------\n\nTITLE: Documenting PixelFormat Class in Kornia\nDESCRIPTION: This snippet shows the documentation for the PixelFormat class using Sphinx autodoc. It includes all members and undocumented members.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/image.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: PixelFormat\n    :members:\n    :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Installing Kornia from source\nDESCRIPTION: Commands to install Kornia from source code in editable mode.\nSOURCE: https://github.com/kornia/kornia/blob/main/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Using Kornia with TensorFlow\nDESCRIPTION: This snippet demonstrates how to convert Kornia functions to work with TensorFlow tensors. It shows the lazy transpilation process that creates a TensorFlow-compatible version of the Kornia library.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/multi-framework-support.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport kornia\nimport tensorflow as tf\n\ntf_kornia = kornia.to_tensorflow()\n\nrgb_image = tf.random.normal((1, 3, 224, 224))\ngray_image = tf_kornia.color.rgb_to_grayscale(rgb_image)\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX Runtime with CUDA Support\nDESCRIPTION: Command to install ONNX Runtime with CUDA 11.X support, which is necessary for leveraging GPU acceleration with ONNXSequential.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/onnx.rst#2025-04-21_snippet_5\n\nLANGUAGE: console\nCODE:\n```\npip install onnxruntime-gpu==1.19.2 --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-11/pypi/simple/\n```\n\n----------------------------------------\n\nTITLE: Installing Kornia via pip\nDESCRIPTION: Commands to install the Kornia library using pip package manager.\nSOURCE: https://github.com/kornia/kornia/blob/main/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install kornia\n```\n\n----------------------------------------\n\nTITLE: Importing Kornia Sensors Module in Python\nDESCRIPTION: This code snippet shows how to import the kornia.sensors module in Python. It sets the current module context for the documentation.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/sensors.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: kornia.sensors\n```\n\n----------------------------------------\n\nTITLE: TensorFlow Framework Integration\nDESCRIPTION: Shows how to convert Kornia functionality to work with TensorFlow.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/index.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> import kornia\n>>> tf_kornia = kornia.to_tensorflow()\n```\n\n----------------------------------------\n\nTITLE: Importing Kornia Filters Module in Python\nDESCRIPTION: This snippet shows how to import the kornia.filters module in Python. It's a prerequisite for using any of the filtering functions described in the documentation.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/filters.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom kornia import filters\n```\n\n----------------------------------------\n\nTITLE: Installing Kornia from source\nDESCRIPTION: Command to install Kornia directly from source code using setup.py.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/installation.rst#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython setup.py install\n```\n\n----------------------------------------\n\nTITLE: Transpiling Kornia to TensorFlow\nDESCRIPTION: This code snippet shows the first step in the transpilation process, which lazily converts Kornia functions to their TensorFlow equivalents. No function is transpiled until it's actually called.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/multi-framework-support.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntf_kornia = kornia.to_tensorflow()\n```\n\n----------------------------------------\n\nTITLE: ONNX Model Sequential Example\nDESCRIPTION: Partial example showing initialization of ONNXSequential for handling ONNX models in Kornia.\nSOURCE: https://github.com/kornia/kornia/blob/main/README.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom kornia.onnx import ONNXSequential\n```\n\n----------------------------------------\n\nTITLE: Manually setting up Kornia development environment in Bash\nDESCRIPTION: Commands to create a virtual environment, install PyTorch and Kornia for development manually.\nSOURCE: https://github.com/kornia/kornia/blob/main/CONTRIBUTING.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Using virtualenv\n$ virtualenv venv -p <your python version / alias> # e.g python3.10\n$ ./venv/bin/activate\n\n# Using conda\n$ conda create -p venv python=<language version> # e.g 3.10\n$ conda activate venv/\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Installing pytorch: https://pytorch.org/get-started/locally/\n# With pip\n$ pip install torch\n# With conda\n$ conda install pytorch cudatoolkit -c pytorch -c nvidia # For GPU env\n# or\n$ conda install pytorch cpuonly -c pytorch # For CPU env\n\n# Installing Kornia for development\n$ pip install -e .[dev]\n\n# If you want to contribute to the documentation\n$ pip install -e .[docs]\n```\n\n----------------------------------------\n\nTITLE: Installing Kornia via pip\nDESCRIPTION: Commands to install Kornia using pip package manager. The second command with [x] includes the training API functionality.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/installation.rst#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install kornia\npip install kornia[x]  # to get the training API !\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU training with Accelerate configuration\nDESCRIPTION: Two-step process for multi-GPU training using Accelerate. First creates a configuration file, then launches the training script with that configuration.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config  # This will create a config file on your server\naccelerate launch ./train/image_classifier/main.py --data_path path_to_data  # This will run the script on your server\n```\n\n----------------------------------------\n\nTITLE: Verifying Kornia installation\nDESCRIPTION: Command to verify Kornia was successfully installed by importing the library and printing its version.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/installation.rst#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -c \"import kornia; print(kornia.__version__)\"\n```\n\n----------------------------------------\n\nTITLE: Setting up Kornia development environment using script in Bash\nDESCRIPTION: Commands to set up a local conda environment for Kornia development using the provided script.\nSOURCE: https://github.com/kornia/kornia/blob/main/CONTRIBUTING.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ source ./path.bash.inc\n$ python setup.py develop\n$ python -c \"import kornia; print(kornia.__version__)\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ ./setup_dev_env.sh\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU training with PyTorch distributed launcher\nDESCRIPTION: Command to run the image classifier on multiple GPUs using PyTorch's distributed launcher. Uses 2 processes per node and passes environment variables.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython -m torch.distributed.launch --nproc_per_node 2 --use_env ./train/image_classifier/main.py --data_path path_to_data\n```\n\n----------------------------------------\n\nTITLE: Installing Kornia from GitHub\nDESCRIPTION: Commands to install the latest version of Kornia directly from GitHub repository.\nSOURCE: https://github.com/kornia/kornia/blob/main/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/kornia/kornia\n```\n\n----------------------------------------\n\nTITLE: Multi-node distributed training with PyTorch launcher (first server)\nDESCRIPTION: Command for multi-node training using PyTorch's distributed launcher on the first server. Specifies node rank 0 and the master node IP address.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npython -m torch.distributed.launch --nproc_per_node 2 \\\n  --use_env \\\n  --node_rank 0 \\\n  --master_addr master_node_ip_address \\\n  ./train/image_classifier/main.py --data_path path_to_data  # On the first server\n```\n\n----------------------------------------\n\nTITLE: Cloning and configuring Kornia repository in Bash\nDESCRIPTION: Commands to clone the Kornia repository, add the upstream remote, and create a new branch for development.\nSOURCE: https://github.com/kornia/kornia/blob/main/CONTRIBUTING.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone git@github.com:<your Github username>/kornia.git\n$ cd kornia\n$ git remote add upstream https://github.com/kornia/kornia.git\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ git checkout upstream/main -b feat/foo_feature\n# or\n$ git checkout upstream/main -b fix/bar_bug\n```\n\n----------------------------------------\n\nTITLE: Multi-node distributed training with Accelerate\nDESCRIPTION: Commands to run on each machine for multi-node training with Accelerate. Creates a config file on each server before launching the distributed training.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config  # This will create a config file on each server\naccelerate launch ./train/image_classifier/main.py --data_path path_to_data  # This will run the script on each server\n```\n\n----------------------------------------\n\nTITLE: Building Kornia documentation in Bash\nDESCRIPTION: Commands to build and view the Kornia documentation locally.\nSOURCE: https://github.com/kornia/kornia/blob/main/CONTRIBUTING.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ make build-docs\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ open docs/build/html/index.html\n```\n\n----------------------------------------\n\nTITLE: Defining Simplified Q Matrix for Stereo Reprojection in LaTeX\nDESCRIPTION: This LaTeX code shows a simplified version of the Q matrix used for stereo reprojection, assuming fx = fy. It reduces the complexity of the matrix while maintaining the essential components for 3D reconstruction.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/geometry.camera.stereo.rst#2025-04-21_snippet_4\n\nLANGUAGE: LaTeX\nCODE:\n```\nQ = \\begin{bmatrix}\n1 & 0       & 0   & -cx \\\\\n0       & 1 & 0   & -cy \\\\\n0       & 0       & 0   & fx  \\\\\n0       & 0       & -1/tx & (cx_{left} -cx_{right} / tx)\n\\end{bmatrix}\n```\n\n----------------------------------------\n\nTITLE: Multi-node distributed training with PyTorch launcher (second server)\nDESCRIPTION: Command for multi-node training using PyTorch's distributed launcher on the second server. Specifies node rank 1 and the master node IP address.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npython -m torch.distributed.launch --nproc_per_node 2 \\\n  --use_env \\\n  --node_rank 1 \\\n  --master_addr master_node_ip_address \\\n  ./train/image_classifier/main.py --data_path path_to_data  # On the second server\n```\n\n----------------------------------------\n\nTITLE: Embedding Gradio App in HTML\nDESCRIPTION: HTML code to embed a Gradio application that demonstrates total variation denoising functionality.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/image_denoising.rst#2025-04-21_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<gradio-app src=\"kornia/total_variation_denoising\"></gradio-app>\n```\n\n----------------------------------------\n\nTITLE: Defining Left Rectified Camera Matrix in LaTeX\nDESCRIPTION: This LaTeX code defines the structure of the left rectified camera matrix P_0, which is used in stereo camera setups. It includes focal lengths (fx, fy), principal point coordinates (cx, cy), and zeros for other elements.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/geometry.camera.stereo.rst#2025-04-21_snippet_0\n\nLANGUAGE: LaTeX\nCODE:\n```\nP_0 = \\begin{bmatrix}\nfx & 0  & cx & 0 \\\\\n0  & fy & cy & 0 \\\\\n0  & 0  & 1  & 0\n\\end{bmatrix}\n```\n\n----------------------------------------\n\nTITLE: Citing Differentiable Data Augmentation with Kornia\nDESCRIPTION: BibTeX citation for a paper discussing differentiable data augmentation techniques implemented in Kornia, showcasing its advanced capabilities in computer vision tasks.\nSOURCE: https://github.com/kornia/kornia/blob/main/CITATION.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n@misc{shi2020differentiable,\n    title={Differentiable Data Augmentation with Kornia},\n    author={Jian Shi and Edgar Riba and Dmytro Mishkin and Francesc Moreno and Anguelos Nicolaou},\n    year={2020},\n    eprint={2011.09832},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n\n----------------------------------------\n\nTITLE: TPU training with Accelerate\nDESCRIPTION: Commands for running the image classifier on TPUs using Accelerate. Creates a configuration file on the TPU server before launching the training script.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/training.rst#2025-04-21_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config  # This will create a config file on your TPU server\naccelerate launch ./train/image_classifier/main.py --data_path path_to_data  # This will run the script on each server\n```\n\n----------------------------------------\n\nTITLE: Citing torchgeometry: Predecessor to Kornia\nDESCRIPTION: BibTeX citation for the torchgeometry project, presented at the PyTorch Developer Conference in 2018. This project was a precursor to Kornia, focusing on geometry operations in PyTorch.\nSOURCE: https://github.com/kornia/kornia/blob/main/CITATION.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n@misc{Arraiy2018,\n  author    = {E. Riba, M. Fathollahi, W. Chaney, E. Rublee and G. Bradski},\n  title     = {torchgeometry: when PyTorch meets geometry},\n  booktitle = {PyTorch Developer Conference},\n  year      = {2018},\n  url       = {https://drive.google.com/file/d/1xiao1Xj9WzjJ08YY_nYwsthE-wxfyfhG/view?usp=sharing}\n}\n```\n\n----------------------------------------\n\nTITLE: Documenting ImageSize Class in Kornia\nDESCRIPTION: This snippet demonstrates how to document the ImageSize class using Sphinx autodoc. It includes all members and undocumented members.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/image.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: ImageSize\n    :members:\n    :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX and ONNXRuntime Dependencies\nDESCRIPTION: Command to install the required ONNX and ONNXRuntime packages using pip, which are prerequisites for using the ONNXSequential class.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/onnx.rst#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install onnx onnxruntime\n```\n\n----------------------------------------\n\nTITLE: Citing Kornia Survey Paper (2020)\nDESCRIPTION: BibTeX citation for the 2020 survey paper on Kornia by Riba et al. This citation references the comprehensive survey of the Kornia library's capabilities.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/about.rst#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@inproceedings{eriba2020kornia,\n  author    = {E. Riba, D. Mishkin, J. Shi, D. Ponsa, F. Moreno-Noguer and G. Bradski},\n  title     = {A survey on Kornia: an Open Source Differentiable Computer Vision Library for PyTorch},\n  year      = {2020},\n}\n```\n\n----------------------------------------\n\nTITLE: Documenting ImageLayout Class in Kornia\nDESCRIPTION: This snippet presents the documentation for the ImageLayout class using Sphinx autodoc. It includes all members and undocumented members.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/image.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: ImageLayout\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Installing Kornia from GitHub repository\nDESCRIPTION: Command to install Kornia directly from the GitHub repository using pip.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/installation.rst#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/kornia/kornia\n```\n\n----------------------------------------\n\nTITLE: Documenting Linear Algebra Functions in reST\nDESCRIPTION: reStructuredText directives to generate documentation for the kornia.geometry.linalg module. The code uses Sphinx autofunction directives to automatically document various geometric computation functions.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/geometry.linalg.rst#2025-04-21_snippet_0\n\nLANGUAGE: reST\nCODE:\n```\n.. autofunction:: relative_transformation\n.. autofunction:: compose_transformations\n.. autofunction:: inverse_transformation\n.. autofunction:: transform_points\n.. autofunction:: point_line_distance\n.. autofunction:: squared_norm\n.. autofunction:: batched_dot_product\n.. autofunction:: euclidean_distance\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Kornia\nDESCRIPTION: This requirements file lists the Python packages needed for Kornia. It includes accelerate (with no version constraint) and onnxruntime-gpu (version 1.16 or higher) with a condition to exclude macOS installations.\nSOURCE: https://github.com/kornia/kornia/blob/main/requirements/requirements-x.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate\nonnxruntime-gpu>=1.16; sys_platform != 'darwin'\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Kornia Core Module\nDESCRIPTION: ReStructuredText documentation configuration for the kornia.core module, including metadata definition and module/class documentation directives.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/core.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. meta::\n   :name: description\n   :content: \"The kornia.core module in Kornia provides foundational classes and utilities for tensor manipulation. Key classes like TensorWrapper allow for enhanced handling of image tensors with support for various operations and transformations in computer vision tasks.\"\n\n.. currentmodule:: kornia.core\n\n.. autoclass:: TensorWrapper\n    :members:\n    :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Performance Benchmark Results for CPU vs GPU\nDESCRIPTION: Example output showing the performance difference between GPU and CPU execution when using ONNXSequential, highlighting the speed advantage of GPU acceleration.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/onnx.rst#2025-04-21_snippet_7\n\nLANGUAGE: console\nCODE:\n```\n--- GPU 0.014804363250732422 seconds ---\n--- CPU 0.17681646347045898 seconds ---\n```\n\n----------------------------------------\n\nTITLE: Citing Kornia: An Open Source Differentiable Computer Vision Library for PyTorch\nDESCRIPTION: BibTeX citation for the paper introducing Kornia, presented at the Winter Conference on Applications of Computer Vision in 2020.\nSOURCE: https://github.com/kornia/kornia/blob/main/CITATION.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@inproceedings{eriba2019kornia,\n  author    = {E. Riba, D. Mishkin, D. Ponsa, E. Rublee and G. Bradski},\n  title     = {Kornia: an Open Source Differentiable Computer Vision Library for PyTorch},\n  booktitle = {Winter Conference on Applications of Computer Vision},\n  year      = {2020},\n  url       = {https://arxiv.org/pdf/1910.02190.pdf}\n}\n```\n\n----------------------------------------\n\nTITLE: Advanced Image Registration with External File Reference\nDESCRIPTION: A reference to a more sophisticated image registration process that is included from an external file. This indicates that a more complex implementation is available in the _static/image_registration.py file.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/applications/image_registration.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# This is a reference to an external file\n# ../_static/image_registration.py\n```\n\n----------------------------------------\n\nTITLE: Citing TorchGeometry PyTorch Conference Paper (2018)\nDESCRIPTION: BibTeX citation for the 2018 PyTorch Developer Conference paper about torchgeometry (Kornia's predecessor) by Riba et al. This citation references the initial presentation of the library's concept.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/about.rst#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n@misc{Arraiy2018,\n  author    = {E. Riba, M. Fathollahi, W. Chaney, E. Rublee and G. Bradski},\n  title     = {torchgeometry: when PyTorch meets geometry},\n  booktitle = {PyTorch Developer Conference},\n  year      = {2018},\n  url       = {https://drive.google.com/file/d/1xiao1Xj9WzjJ08YY_nYwsthE-wxfyfhG/view?usp=sharing}\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Kornia Geometry Homography Module in Python\nDESCRIPTION: This snippet shows how to import the kornia.geometry.homography module. The module provides functions for computing, applying, and manipulating homographies, useful for tasks like image stitching and perspective warping.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/geometry.homography.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom kornia.geometry import homography\n```\n\n----------------------------------------\n\nTITLE: Citing Kornia WACV Paper (2020)\nDESCRIPTION: BibTeX citation for the 2020 Winter Conference on Applications of Computer Vision paper introducing Kornia by Riba et al. This citation references the official paper presenting the Kornia library.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/get-started/about.rst#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n@inproceedings{eriba2019kornia,\n  author    = {E. Riba, D. Mishkin, D. Ponsa, E. Rublee and G. Bradski},\n  title     = {Kornia: an Open Source Differentiable Computer Vision Library for PyTorch},\n  booktitle = {Winter Conference on Applications of Computer Vision},\n  year      = {2020},\n  url       = {https://arxiv.org/pdf/1910.02190.pdf}\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Kornia Image Module in Python\nDESCRIPTION: This snippet shows how to import the kornia.image module in Python. It sets up the current module context for the subsequent class documentation.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/image.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: kornia.image\n```\n\n----------------------------------------\n\nTITLE: Documenting ChannelsOrder Class in Kornia\nDESCRIPTION: This snippet illustrates the documentation for the ChannelsOrder class using Sphinx autodoc. It includes all members and undocumented members.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/image.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: ChannelsOrder\n    :members:\n    :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Embedding Interactive Augmentation Demo in HTML\nDESCRIPTION: HTML iframe code for embedding an interactive demo of Kornia's augmentation capabilities, hosted on Hugging Face Spaces.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/augmentation.rst#2025-04-21_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<iframe\n   id=\"augmentation-tester\"\n   src=\"https://kornia-kornia-augmentations-tester.hf.space\"\n   frameborder=\"0\"\n   width=\"850\"\n   height=\"450\"\n></iframe>\n```\n\n----------------------------------------\n\nTITLE: Citing Survey on Kornia\nDESCRIPTION: BibTeX citation for a survey paper on Kornia, providing an overview of the library's capabilities and applications in computer vision.\nSOURCE: https://github.com/kornia/kornia/blob/main/CITATION.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n@misc{riba2020survey,\n    title={A survey on Kornia: an Open Source Differentiable Computer Vision Library for PyTorch},\n    author={E. Riba and D. Mishkin and J. Shi and D. Ponsa and F. Moreno-Noguer and G. Bradski},\n    year={2020},\n    eprint={2009.10521},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Format\nDESCRIPTION: Structured markdown document following Keep a Changelog format to track version changes, new features, bug fixes and improvements in the Kornia project.\nSOURCE: https://github.com/kornia/kornia/blob/main/CHANGELOG.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Changelog\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n```\n\n----------------------------------------\n\nTITLE: Intrinsic Matrix Format\nDESCRIPTION: Format of the intrinsic matrix used in the PinholeCamera class, representing camera calibration parameters in a 4x4 matrix format.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/geometry.camera.pinhole.rst#2025-04-21_snippet_1\n\nLANGUAGE: math\nCODE:\n```\n\\begin{bmatrix} f_x & 0 & u_0 & 0\\\\ 0 & f_y & v_0 & 0\\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}\n```\n\n----------------------------------------\n\nTITLE: Defining Right Rectified Camera Matrix in LaTeX\nDESCRIPTION: This LaTeX code defines the structure of the right rectified camera matrix P_1 for stereo camera setups. It includes focal lengths (fx, fy), principal point coordinates (cx, cy), and the horizontal baseline (tx) multiplied by fx.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/geometry.camera.stereo.rst#2025-04-21_snippet_1\n\nLANGUAGE: LaTeX\nCODE:\n```\nP_1 = \\begin{bmatrix}\nfx & 0  & cx & tx * fx \\\\\n0  & fy & cy & 0       \\\\\n0  & 0  & 1  & 0\n\\end{bmatrix}\n```\n\n----------------------------------------\n\nTITLE: Defining Combined Stereo Projection Matrix in LaTeX\nDESCRIPTION: This LaTeX code shows the combined stereo projection matrix that relates 3D world coordinates to pixel coordinates in both left and right camera frames. It includes disparity (d) and combines elements from both P_0 and P_1 matrices.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/geometry.camera.stereo.rst#2025-04-21_snippet_2\n\nLANGUAGE: LaTeX\nCODE:\n```\n\\begin{bmatrix}\nu \\\\\nv \\\\\nu-d \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\nfx & 0 & cx_{left} & 0 \\\\\n0  & fy & cy & 0 \\\\\nfx & 0 & cx_{right} & fx * tx \\\\\n0  & 0 & 1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nX \\\\\nY \\\\\nZ \\\\\n1\n\\end{bmatrix}\n```\n\n----------------------------------------\n\nTITLE: Defining 3D Point Reprojection Equation in LaTeX\nDESCRIPTION: This LaTeX code demonstrates how to use the Q matrix to reproject 2D pixel coordinates and disparity values into 3D points. It shows the matrix multiplication between Q and the input vector containing u, v, disparity, and z coordinates.\nSOURCE: https://github.com/kornia/kornia/blob/main/docs/source/geometry.camera.stereo.rst#2025-04-21_snippet_5\n\nLANGUAGE: LaTeX\nCODE:\n```\n\\begin{bmatrix}\nX \\\\\nY \\\\\nZ \\\\\nW\n\\end{bmatrix} = Q *\n\\begin{bmatrix}\nu \\\\\nv \\\\\ndisparity(y, v) \\\\\nz\n\\end{bmatrix}\n```"
  }
]
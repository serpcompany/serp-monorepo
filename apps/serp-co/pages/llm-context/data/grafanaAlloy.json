[
  {
    "owner": "grafana",
    "repo": "alloy",
    "content": "TITLE: Configuring Alloy Pipeline Components in Alloy\nDESCRIPTION: This snippet demonstrates the basic concepts of an Alloy configuration, forming a pipeline that collects, transforms, and delivers telemetry data. It includes components for file matching, log sourcing, log processing, and writing to a Loki endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/_index.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\n// Collection: mount a local directory with a certain path spec\nlocal.file_match \"applogs\" {\n    path_targets = [{\"__path__\" = \"/tmp/app-logs/app.log\"}]\n}\n\n// Collection: Take the file match as input, and scrape those mounted log files\nloki.source.file \"local_files\" {\n    targets    = local.file_match.applogs.targets\n\n    // This specifies which component should process the logs next, the \"link in the chain\"\n    forward_to = [loki.process.add_new_label.receiver]\n}\n\n// Transformation: pull some data out of the log message, and turn it into a label\nloki.process \"add_new_label\" {\n    stage.logfmt {\n        mapping = {\n            \"extracted_level\" = \"level\",\n        }\n    }\n\n    // Add the value of \"extracted_level\" from the extracted map as a \"level\" label\n    stage.labels {\n        values = {\n            \"level\" = \"extracted_level\",\n        }\n    }\n\n    // The next link in the chain is the local_loki \"receiver\" (receives the telemetry)\n    forward_to = [loki.write.local_loki.receiver]\n}\n\n// Anything that comes into this component gets written to the loki remote API\nloki.write \"local_loki\" {\n    endpoint {\n        url = \"http://loki:3100/loki/api/v1/push\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a local.file Component in Alloy\nDESCRIPTION: This snippet demonstrates how to define a local.file component labeled 'targets' with a required filename argument. The component exposes the file content as a string in its exports, which can be referenced by other components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/components.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file \"targets\" {\n  // Required argument\n  filename = \"/etc/alloy/targets\"\n\n  // Optional arguments: Components may have some optional arguments that\n  // do not need to be defined.\n  //\n  // The optional arguments for local.file are is_secret, detector, and\n  // poll_frequency.\n\n  // Exports: a single field named `content`\n  // It can be referred to as `local.file.targets.content`\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Attribute in Alloy\nDESCRIPTION: Demonstrates setting a basic attribute with a string value. Shows how to set the log_level attribute to debug.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/syntax.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nlog_level = \"debug\"\n```\n\n----------------------------------------\n\nTITLE: Importing and Using a Custom Module in Alloy\nDESCRIPTION: This snippet shows how to import a custom module (helpers.alloy) and use its components in the main configuration. It demonstrates setting up a log processing pipeline that collects logs, filters them using the imported 'log_filter' component, and forwards the filtered logs to a Loki endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/modules.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\n// Import our helpers.alloy module, exposing its custom components as\n// helpers.COMPONENT_NAME.\nimport.file \"helpers\" {\n  filename = \"helpers.alloy\"\n}\n\nloki.source.file \"self\" {\n  targets = LOG_TARGETS\n\n  // Forward collected logs to the input of our filter.\n  forward_to = [helpers.log_filter.default.filter_input]\n}\n\nhelpers.log_filter \"default\" {\n  // Configure the filter to forward filtered logs to loki.write below.\n  write_to = [loki.write.default.receiver]\n}\n\nloki.write \"default\" {\n  endpoint {\n    url = LOKI_URL\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Complete Alloy Pipeline with Kubernetes Discovery and Prometheus Remote Write\nDESCRIPTION: This example demonstrates a complete Alloy configuration that discovers Kubernetes pods, scrapes metrics from them using Prometheus, reads an API key from disk, and forwards the collected metrics to a Prometheus remote_write endpoint with basic authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/syntax/README.md#2025-04-22_snippet_0\n\nLANGUAGE: grafana-alloy\nCODE:\n```\n// Discover Kubernetes pods to collect metrics from.\ndiscovery.kubernetes \"pods\" {\n  role = \"pod\"\n}\n\n// Collect metrics from Kubernetes pods.\nprometheus.scrape \"default\" {\n  targets    = discovery.kubernetes.pods.targets\n  forward_to = [prometheus.remote_write.default.receiver]\n}\n\n// Get an API key from disk.\nlocal.file \"apikey\" {\n  filename  = \"/var/data/my-api-key.txt\"\n  is_secret = true\n}\n\n// Send metrics to a Prometheus remote_write endpoint.\nprometheus.remote_write \"default\" {\n  endpoint {\n    url = \"http://localhost:9009/api/prom/push\"\n\n    basic_auth {\n      username = \"MY_USERNAME\"\n      password = local.file.apikey.content\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic import.file Block Usage in Alloy\nDESCRIPTION: The basic syntax for the import.file block in Alloy, which requires a namespace label and a filename argument specifying the path to the file or directory to import.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/import.file.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nimport.file \"NAMESPACE\" {\n  filename = PATH_NAME\n}\n```\n\n----------------------------------------\n\nTITLE: Basic K8s Attributes Processor Configuration in Alloy\nDESCRIPTION: This example shows a basic configuration of the Kubernetes Attributes Processor. It sets up an OTLP receiver, processes the data through the K8s attributes processor, and then exports it via OTLP. This configuration adds standard Kubernetes metadata to all logs, metrics, and traces.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.k8sattributes.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  http {}\n  grpc {}\n\n  output {\n    metrics = [otelcol.processor.k8sattributes.default.input]\n    logs    = [otelcol.processor.k8sattributes.default.input]\n    traces  = [otelcol.processor.k8sattributes.default.input]\n  }\n}\n\notelcol.processor.k8sattributes \"default\" {\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = sys.env(\"OTLP_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Complete OpenTelemetry Pipeline in Alloy\nDESCRIPTION: Comprehensive configuration that sets up an OpenTelemetry pipeline with OTLP receiver, batch processor, and three different exporters for metrics, logs, and traces. The configuration includes authentication and endpoints for Grafana Cloud services.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-to-lgtm-stack.md#2025-04-22_snippet_10\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"example\" {\n  grpc {\n    endpoint = \"127.0.0.1:4317\"\n  }\n\n  http {\n    endpoint = \"127.0.0.1:4318\"\n  }\n\n  output {\n    metrics = [otelcol.processor.batch.example.input]\n    logs    = [otelcol.processor.batch.example.input]\n    traces  = [otelcol.processor.batch.example.input]\n  }\n}\n\notelcol.processor.batch \"example\" {\n  output {\n    metrics = [otelcol.exporter.prometheus.grafana_cloud_metrics.input]\n    logs    = [otelcol.exporter.loki.grafana_cloud_logs.input]\n    traces  = [otelcol.exporter.otlp.grafana_cloud_traces.input]\n  }\n}\n\notelcol.exporter.otlp \"grafana_cloud_traces\" {\n  client {\n    endpoint = \"tempo-us-central1.grafana.net:443\"\n    auth     = otelcol.auth.basic.grafana_cloud_traces.handler\n  }\n}\n\notelcol.auth.basic \"grafana_cloud_traces\" {\n  username = \"4094\"\n  password = sys.env(\"GRAFANA_CLOUD_API_KEY\")\n}\n\notelcol.exporter.prometheus \"grafana_cloud_metrics\" {\n        forward_to = [prometheus.remote_write.grafana_cloud_metrics.receiver]\n    }\n\nprometheus.remote_write \"grafana_cloud_metrics\" {\n    endpoint {\n        url = \"https://prometheus-us-central1.grafana.net/api/prom/push\"\n\n        basic_auth {\n            username = \"12690\"\n            password = sys.env(\"GRAFANA_CLOUD_API_KEY\")\n        }\n    }\n}\n\notelcol.exporter.loki \"grafana_cloud_logs\" {\n  forward_to = [loki.write.grafana_cloud_logs.receiver]\n}\n\nloki.write \"grafana_cloud_logs\" {\n  endpoint {\n    url = \"https://logs-prod-us-central1.grafana.net/loki/api/v1/push\"\n\n    basic_auth {\n      username = \"5252\"\n      password = sys.env(\"GRAFANA_CLOUD_API_KEY\")\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Discovering Kubernetes Pods in Alloy\nDESCRIPTION: This snippet shows how to configure a discovery.kubernetes component to discover Kubernetes Pods. It includes options for limiting Namespaces and using field and label selectors.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/prometheus-metrics.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubernetes \"<DISCOVERY_LABEL>\" {\n  role = \"pod\"\n}\n```\n\nLANGUAGE: alloy\nCODE:\n```\nnamespaces {\n  own_namespace = true\n  names         = [<NAMESPACE_NAMES>]\n}\n```\n\nLANGUAGE: alloy\nCODE:\n```\nselectors {\n  role  = \"pod\"\n  field = \"<FIELD_SELECTOR>\"\n}\n```\n\nLANGUAGE: alloy\nCODE:\n```\nselectors {\n  role  = \"pod\"\n  label = \"LABEL_SELECTOR\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Processing in Grafana Alloy\nDESCRIPTION: Alloy configuration for the loki.process component that processes log entries by parsing JSON, extracting timestamps, setting labels, and adding structured metadata.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-structured-logs.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\nloki.process \"labels\" {\n    stage.json {\n      expressions = { \n                      \"timestamp\" = \"\",\n                      \"state\" = \"\", \n                      \"package_size\" = \"\", \n                      \"package_status\" = \"\", \n                      \"package_id\" = \"\",\n                    }\n    }\n\n  stage.timestamp {\n    source = \"timestamp\"\n    format = \"RFC3339\"\n  }\n\n  stage.labels {\n    values = {\n      \"state\" = \"\",\n      \"package_size\" = \"\",\n    }\n  }\n\n  stage.structured_metadata {\n    values = {\n      \"package_status\" = \"\",\n      \"package_id\" = \"\",\n    }\n  }\n\n  stage.static_labels {\n    values = {\n      \"service_name\" = \"Delivery World\",\n    }\n  }\n\n  stage.output {\n    source = \"message\"\n  }\n\n  forward_to = [loki.write.local.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Basic OpenTelemetry Collection Pipeline Configuration in Alloy\nDESCRIPTION: A basic Alloy configuration that sets up an OpenTelemetry receiver that accepts data via gRPC and HTTP, processes it through a batch processor, and forwards it to an OTLP exporter. This serves as the foundation for collecting OpenTelemetry data.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-to-lgtm-stack.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"example\" {\n  grpc {\n    endpoint = \"127.0.0.1:4317\"\n  }\n\n  http {\n    endpoint = \"127.0.0.1:4318\"\n  }\n\n  output {\n    metrics = [otelcol.processor.batch.example.input]\n    logs    = [otelcol.processor.batch.example.input]\n    traces  = [otelcol.processor.batch.example.input]\n  }\n}\n\notelcol.processor.batch \"example\" {\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = \"my-otlp-grpc-server:4317\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete OTLP Export Configuration with Authentication\nDESCRIPTION: Full example showing OTLP exporter configuration with basic authentication and receiver setup.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-data.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = \"my-otlp-grpc-server:4317\"\n    auth     = otelcol.auth.basic.credentials.handler\n  }\n}\n\notelcol.auth.basic \"credentials\" {\n  // Retrieve credentials using environment variables.\n\n  username = sys.env(\"BASIC_AUTH_USER\")\n  password = sys.env(\"API_KEY\")\n}\n\notelcol.receiver.otlp \"example\" {\n  grpc {\n    endpoint = \"127.0.0.1:4317\"\n  }\n\n  http {\n    endpoint = \"127.0.0.1:4318\"\n  }\n\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Importing a Local Module in Alloy\nDESCRIPTION: Example of importing a module from a local file and instantiating a custom component from the import. The main file imports a math module that contains an addition component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/import.file.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nimport.file \"math\" {\n  filename = \"module.alloy\"\n}\n\nmath.add \"default\" {\n  a = 15\n  b = 45\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating Prometheus and OpenTelemetry with otelcol.receiver.prometheus\nDESCRIPTION: This example showcases how to use otelcol.receiver.prometheus as a bridge between Prometheus and OpenTelemetry ecosystems. It configures Prometheus scraping, metric conversion, and forwarding to an OTLP endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.prometheus.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"default\" {\n    // Collect metrics from the default HTTP listen address.\n    targets = [{\"__address__\"   = \"127.0.0.1:12345\"}]\n\n    forward_to = [otelcol.receiver.prometheus.default.receiver]\n}\n\notelcol.receiver.prometheus \"default\" {\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = sys.env(\"OTLP_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Metrics Collection Pipeline in Alloy\nDESCRIPTION: A complete pipeline that collects Unix metrics using the prometheus.exporter.unix component, scrapes them with prometheus.scrape, and sends them to Prometheus using prometheus.remote_write.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/first-components-and-stdlib.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.unix \"localhost\" {\n    // This component exposes a lot of metrics by default, so we will keep all of the default arguments.\n}\n\nprometheus.scrape \"default\" {\n    // Setting the scrape interval lower to make it faster to be able to see the metrics\n    scrape_interval = \"10s\"\n\n    targets    = prometheus.exporter.unix.localhost.targets\n    forward_to = [\n        prometheus.remote_write.local_prom.receiver,\n    ]\n}\n\nprometheus.remote_write \"local_prom\" {\n    endpoint {\n        url = \"http://localhost:9090/api/v1/write\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Converted Alloy Configuration File\nDESCRIPTION: The resulting Alloy configuration file after conversion from Grafana Agent Static format. This configuration includes components for metrics scraping, remote writing, log processing, and trace collection.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-static.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"metrics_test_local_agent\" {\n    targets = [{\n        __address__ = \"127.0.0.1:12345\",\n        cluster     = \"localhost\",\n    }]\n    forward_to      = [prometheus.remote_write.metrics_test.receiver]\n    job_name        = \"local-agent\"\n    scrape_interval = \"15s\"\n}\n\nprometheus.remote_write \"metrics_test\" {\n    endpoint {\n        name = \"test-4dec64\"\n        url  = \"https://prometheus-us-central1.grafana.net/api/prom/push\"\n\n        basic_auth {\n            username = \"<USERNAME>\"\n            password = \"<PASSWORD>\"\n        }\n\n        queue_config { }\n\n        metadata_config { }\n    }\n}\n\nlocal.file_match \"logs_varlogs_varlogs\" {\n    path_targets = [{\n        __address__ = \"localhost\",\n        __path__    = \"/var/log/*.log\",\n        host        = \"mylocalhost\",\n        job         = \"varlogs\",\n    }]\n}\n\nloki.process \"logs_varlogs_varlogs\" {\n    forward_to = [loki.write.logs_varlogs.receiver]\n\n    stage.match {\n        selector = \"{filename=\\\"/var/log/*.log\\\"}\"\n\n        stage.drop {\n            expression = \"^[^0-9]{4}\"\n        }\n\n        stage.regex {\n            expression = \"^(?P<timestamp>\\\\d{4}/\\\\d{2}/\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2}) \\\\[(?P<level>[[:alpha:]]+)\\\\] (?:\\\\d+)\\\\#(?:\\\\d+): \\\\*(?:\\\\d+) (?P<message>.+)$\"\n        }\n\n        stage.pack {\n            labels           = [\"level\"]\n            ingest_timestamp = false\n        }\n    }\n}\n\nloki.source.file \"logs_varlogs_varlogs\" {\n    targets    = local.file_match.logs_varlogs_varlogs.targets\n    forward_to = [loki.process.logs_varlogs_varlogs.receiver]\n\n    file_watch {\n        min_poll_frequency = \"1s\"\n        max_poll_frequency = \"5s\"\n    }\n    legacy_positions_file = \"/var/lib/agent/data-agent/varlogs.yml\"\n}\n\nloki.write \"logs_varlogs\" {\n    endpoint {\n        url = \"https://USER_ID:API_KEY@logs-prod3.grafana.net/loki/api/v1/push\"\n    }\n    external_labels = {}\n}\n\notelcol.receiver.otlp \"default\" {\n    grpc {\n        include_metadata = true\n    }\n\n    http {\n        include_metadata = true\n    }\n\n    output {\n        metrics = []\n        logs    = []\n        traces  = [otelcol.processor.batch.default.input]\n    }\n}\n\notelcol.processor.batch \"default\" {\n    timeout         = \"20s\"\n    send_batch_size = 10000\n\n    output {\n        metrics = []\n        logs    = []\n        traces  = [otelcol.exporter.otlp.default_0.input]\n    }\n}\n\notelcol.exporter.otlp \"default_0\" {\n    retry_on_failure {\n        max_elapsed_time = \"1m0s\"\n    }\n\n    client {\n        endpoint = \"tempo-us-central1.grafana.net:443\"\n        headers  = {\n            authorization = \"Basic VVNFUk5BTUU6UEFTU1dPUkQ=\",\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Graph Connector in Alloy\nDESCRIPTION: Basic configuration example showing how to set up the otelcol.connector.servicegraph component with required output configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.servicegraph.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.connector.servicegraph \"LABEL\" {\n  output {\n    metrics = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Exporter and Scraper in Alloy\nDESCRIPTION: Sets up a local system metrics exporter and configures a scraper to collect metrics every 10 seconds. The scraper will forward the collected metrics to a filtering component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/send-metrics-to-prometheus.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.unix \"local_system\" { }\n\nprometheus.scrape \"scrape_metrics\" {\n  targets         = prometheus.exporter.unix.local_system.targets\n  forward_to      = [prometheus.relabel.filter_metrics.receiver]\n  scrape_interval = \"10s\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Docker Source in Alloy\nDESCRIPTION: Alloy configuration for the loki.source.docker component that collects logs from Docker containers, applies labels and relabeling rules, and forwards to a Loki writer.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-docker-containers.md#2025-04-22_snippet_9\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.docker \"default\" {\n  host       = \"unix:///var/run/docker.sock\"\n  targets    = discovery.docker.linux.targets\n  labels     = {\"platform\" = \"docker\"}\n  relabel_rules = discovery.relabel.logs_integrations_docker.rules\n  forward_to = [loki.write.local.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Unix Metrics Collection with Prometheus Remote Write in Alloy\nDESCRIPTION: This snippet demonstrates how to set up a Unix metrics exporter, configure Prometheus scraping, and send the collected metrics to a remote write endpoint. It includes the necessary components for exporting Unix metrics, scraping them, and forwarding to a Prometheus-compatible remote write API.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.unix.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.unix \"demo\" { }\n\n// Configure a prometheus.scrape component to collect unix metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.unix.demo.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Navigating to the Kubernetes Logs Example Directory\nDESCRIPTION: Command to navigate to the k8s-logs directory in the cloned repository.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd alloy-scenarios/k8s-logs\n```\n\n----------------------------------------\n\nTITLE: Complete OTLP Pipeline Configuration Example\nDESCRIPTION: Comprehensive example showing a complete OpenTelemetry pipeline configuration including receiver, processor, and exporter components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-data.md#2025-04-22_snippet_8\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"example\" {\n  grpc {\n    endpoint = \"127.0.0.1:4317\"\n  }\n\n  http {\n    endpoint = \"127.0.0.1:4318\"\n  }\n\n  output {\n    metrics = [otelcol.processor.batch.example.input]\n    logs    = [otelcol.processor.batch.example.input]\n    traces  = [otelcol.processor.batch.example.input]\n  }\n}\n\notelcol.processor.batch \"example\" {\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = \"my-otlp-grpc-server:4317\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Converted Grafana Alloy Configuration\nDESCRIPTION: The resulting Grafana Alloy configuration after converting from Prometheus. It shows how the Prometheus scrape job and remote write configurations are represented in Alloy's configuration format.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-prometheus.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"prometheus\" {\n  targets = [{\n    __address__ = \"localhost:12345\",\n  }]\n  forward_to     = [prometheus.remote_write.default.receiver]\n  job_name       = \"prometheus\"\n  scrape_timeout = \"45s\"\n}\n\nprometheus.remote_write \"default\" {\n  endpoint {\n    name = \"grafana-cloud\"\n    url  = \"https://prometheus-us-central1.grafana.net/api/prom/push\"\n\n    basic_auth {\n      username = \"USERNAME\"\n      password = \"PASSWORD\"\n    }\n\n    queue_config {\n      capacity             = 2500\n      max_shards           = 200\n      max_samples_per_send = 500\n    }\n\n    metadata_config {\n      max_samples_per_send = 500\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Alloy Configuration with Kuma Discovery and Prometheus Integration\nDESCRIPTION: A comprehensive example showing how to configure Kuma service discovery, Prometheus scraping, and remote write in Grafana Alloy. It demonstrates the integration between discovery.kuma, prometheus.scrape, and prometheus.remote_write components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.kuma.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kuma \"example\" {\n    server     = \"http://kuma-control-plane.kuma-system.svc:5676\"\n}\nprometheus.scrape \"demo\" {\n    targets    = discovery.kuma.example.targets\n    forward_to = [prometheus.remote_write.demo.receiver]\n}\nprometheus.remote_write \"demo\" {\n    endpoint {\n        url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n        basic_auth {\n            username = \"<USERNAME>\"\n            password = \"<PASSWORD>\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Metrics to Grafana Cloud with Environment Variables in Alloy\nDESCRIPTION: This example demonstrates how to configure a prometheus.remote_write component to send metrics to a managed service like Grafana Cloud, with authentication credentials injected through environment variables for security.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.remote_write.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.remote_write \"default\" {\n  endpoint {\n    url = \"https://prometheus-xxx.grafana.net/api/prom/push\"\n      basic_auth {\n        username = sys.env(\"PROMETHEUS_USERNAME\")\n        password = sys.env(\"GRAFANA_CLOUD_API_KEY\")\n      }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring a Prometheus Scraper Component with References in Alloy\nDESCRIPTION: This snippet shows how to configure a prometheus.scrape component that references the exports of a local.file component. It demonstrates how components can be wired together, using the content exported by one component as an argument to another.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/components.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"default\" {\n  targets = [\n    { \"__address__\" = local.file.targets.content }, // tada!\n    { \"__address__\" = \"localhost:9001\" },\n  ]\n\n  forward_to = [prometheus.remote_write.default.receiver]\n  scrape_config {\n    job_name = \"default\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Relabeling Rules with prometheus.relabel in Alloy\nDESCRIPTION: Shows a complete example of using prometheus.relabel to apply relabeling rules to incoming metrics and forward the results to a specific receiver.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.relabel.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.relabel \"keep_backend_only\" {\n  forward_to = [prometheus.remote_write.onprem.receiver]\n\n  rule {\n    action        = \"replace\"\n    source_labels = [\"__address__\", \"instance\"]\n    separator     = \"/\"\n    target_label  = \"host\"\n  }\n  rule {\n    action        = \"keep\"\n    source_labels = [\"app\"]\n    regex         = \"backend\"\n  }\n  rule {\n    action = \"labeldrop\"\n    regex  = \"instance\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Static Configuration with Error Bypassing\nDESCRIPTION: Command to convert a Grafana Agent Static configuration to Grafana Alloy while bypassing non-critical issues. This allows for best-effort conversion when the configuration has elements that cannot be directly converted.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-static.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=static --bypass-errors --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Converted Grafana Alloy Configuration\nDESCRIPTION: The result of converting the sample Promtail configuration to Grafana Alloy format, showing the component-based approach with file_match, source.file, and write components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-promtail.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file_match \"example\" {\n  path_targets = [{\n    __address__ = \"localhost\",\n    __path__    = \"/var/log/*.log\",\n  }]\n}\n\nloki.source.file \"example\" {\n  targets    = local.file_match.example.targets\n  forward_to = [loki.write.default.receiver]\n}\n\nloki.write \"default\" {\n  endpoint {\n    url = \"http://localhost/loki/api/v1/push\"\n  }\n  external_labels = {}\n}\n```\n\n----------------------------------------\n\nTITLE: Complex Discovery Relabel Example in Alloy\nDESCRIPTION: Comprehensive example showing how to apply multiple relabel rules including label combination, filtering based on values, and adding static labels.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.relabel.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.relabel \"keep_backend_only\" {\n  targets = [\n    { \"__meta_foo\" = \"foo\", \"__address__\" = \"localhost\", \"instance\" = \"one\",   \"app\" = \"backend\"  },\n    { \"__meta_bar\" = \"bar\", \"__address__\" = \"localhost\", \"instance\" = \"two\",   \"app\" = \"database\" },\n    { \"__meta_baz\" = \"baz\", \"__address__\" = \"localhost\", \"instance\" = \"three\", \"app\" = \"frontend\" },\n  ]\n\n  # Combine the \"__address__\" and \"instance\" labels into a new \"destination\" label.\n  rule {\n    source_labels = [\"__address__\", \"instance\"]\n    separator     = \"/\"\n    target_label  = \"destination\"\n    action        = \"replace\"\n  }\n\n  # Drop any targets that do not have the value \"backend\" in their \"app\" label.\n  rule {\n    source_labels = [\"app\"]\n    action        = \"keep\"\n    regex         = \"backend\"\n  }\n\n  # Add a static label to all remaining targets.\n  rule {\n    target_label = \"custom_static_label\"\n    replacement = \"static_value\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Full OpenTelemetry Pipeline with Datadog Integration\nDESCRIPTION: Comprehensive example demonstrating a complete OpenTelemetry pipeline that receives metrics and traces in Datadog format, converts them to OTel format, and exports them to both OTLP and Datadog endpoints. Includes detailed configuration for traces and metrics handling.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.datadog.md#2025-04-22_snippet_10\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.datadog \"default\" {\n\toutput {\n\t\tmetrics = [otelcol.exporter.otlp.default.input, otelcol.exporter.datadog.default input]\n\t\ttraces  = [otelcol.exporter.otlp.default.input, otelcol.exporter.datadog.default.input]\n\t}\n}\n\notelcol.exporter.otlp \"default\" {\n\tclient {\n\t\tendpoint = \"database:4317\"\n\t}\n}\n\notelcol.exporter.datadog \"default\" {\n\tclient {\n\t\ttimeout = \"10s\"\n\t}\n\n\tapi {\n\t\tapi_key             = \"abc\"\n\t\tfail_on_invalid_key = true\n\t}\n\n\ttraces {\n\t\tendpoint             = \"https://trace.agent.datadoghq.com\"\n\t\tignore_resources     = [\"(GET|POST) /healthcheck\"]\n\t\tspan_name_remappings = {\n\t\t\t\"instrumentation:express.server\" = \"express\",\n\t\t}\n\t}\n\n\tmetrics {\n\t\tdelta_ttl = 1200\n\t\tendpoint  = \"https://api.datadoghq.com\"\n\n\t\texporter {\n\t\t\tresource_attributes_as_tags = true\n\t\t}\n\n\t\thistograms {\n\t\t\tmode = \"counters\"\n\t\t}\n\n\t\tsums {\n\t\t\tinitial_cumulative_monotonic_value = \"keep\"\n\t\t}\n\n\t\tsummaries {\n\t\t\tmode = \"noquantiles\"\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Complete DigitalOcean Discovery and Prometheus Scrape Configuration\nDESCRIPTION: Full example of configuring DigitalOcean Droplet discovery, Prometheus scraping, and remote write. This setup discovers Droplets, scrapes metrics from them, and sends the data to a Prometheus-compatible remote write endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.digitalocean.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.digitalocean \"example\" {\n  port             = 8080\n  refresh_interval = \"5m\"\n  bearer_token     = \"my-secret-bearer-token\"\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.digitalocean.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Label-based Tenant ID Assignment\nDESCRIPTION: Configures tenant ID extraction from a Kubernetes namespace label using labels and tenant stages.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_47\n\nLANGUAGE: alloy\nCODE:\n```\nstage.labels {\n  values = {\n    \"namespace\" = \"k8s_namespace\",\n  }\n}\n\nstage.tenant {\n    label = \"namespace\"\n}\n```\n\n----------------------------------------\n\nTITLE: Collecting and Sending Logs to Loki in Alloy\nDESCRIPTION: This snippet shows how to use local.file_match to discover log files, loki.source.file to collect logs, and loki.write to send them to a Loki instance. It sets up a pipeline to collect logs from /tmp/alloy-logs/*.log and forward them to a local Loki endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/logs-and-relabeling-basics.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file_match \"tmplogs\" {\n    path_targets = [{\"__path__\" = \"/tmp/alloy-logs/*.log\"}]\n}\n\nloki.source.file \"local_files\" {\n    targets    = local.file_match.tmplogs.targets\n    forward_to = [loki.write.local_loki.receiver]\n}\n\nloki.write \"local_loki\" {\n    endpoint {\n        url = \"http://localhost:3100/loki/api/v1/push\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete OTLP Pipeline Configuration Example\nDESCRIPTION: Demonstrates a complete telemetry pipeline configuration using OTLP receiver, batch processor, and OTLP exporter to collect and forward telemetry data.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.otlp.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  http {}\n  grpc {}\n\n  output {\n    metrics = [otelcol.processor.batch.default.input]\n    logs    = [otelcol.processor.batch.default.input]\n    traces  = [otelcol.processor.batch.default.input]\n  }\n}\n\notelcol.processor.batch \"default\" {\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = sys.env(\"OTLP_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Converted Grafana Alloy Configuration\nDESCRIPTION: This is the resulting Grafana Alloy configuration after converting the example OpenTelemetry Collector configuration. It maintains the same components and pipeline structure in Alloy's format.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-otelcol.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n    grpc { }\n\n    http { }\n\n    output {\n        metrics = [otelcol.processor.memory_limiter.default.input]\n        logs    = [otelcol.processor.memory_limiter.default.input]\n        traces  = [otelcol.processor.memory_limiter.default.input]\n    }\n}\n\notelcol.processor.memory_limiter \"default\" {\n    check_interval   = \"1s\"\n    limit_percentage = 90\n\n    output {\n        metrics = [otelcol.exporter.otlp.default.input]\n        logs    = [otelcol.exporter.otlp.default.input]\n        traces  = [otelcol.exporter.otlp.default.input]\n    }\n}\n\notelcol.exporter.otlp \"default\" {\n    client {\n        endpoint = \"database:4317\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Remote Write with Multiple Endpoints in Alloy\nDESCRIPTION: This example shows how to configure prometheus.remote_write with multiple endpoints, mixed usage of basic authentication, and a prometheus.scrape component forwarding metrics to it.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/prometheus-metrics.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.remote_write \"default\" {\n  endpoint {\n    url = \"http://localhost:9090/api/prom/push\"\n  }\n\n  endpoint {\n    url = \"https://prometheus-us-central1.grafana.net/api/prom/push\"\n\n    // Get basic authentication based on environment variables.\n    basic_auth {\n      username = sys.env(\"<REMOTE_WRITE_USERNAME>\")\n      password = sys.env(\"<REMOTE_WRITE_PASSWORD>\")\n    }\n  }\n}\n\nprometheus.scrape \"example\" {\n  // Collect metrics from the default listen address.\n  targets = [{\n    __address__ = \"127.0.0.1:12345\",\n  }]\n\n  forward_to = [prometheus.remote_write.default.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Receiver and Splunk HEC Exporter\nDESCRIPTION: This example demonstrates how to configure an OpenTelemetry receiver to forward metrics, logs, and traces to the Splunk HEC exporter. It includes detailed client configuration for the Splunk HEC exporter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.splunkhec.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n\tgrpc {\n\t\tendpoint = \"localhost:4317\"\n\t}\n\n\thttp {\n\t\tendpoint               = \"localhost:4318\"\n\t\tcompression_algorithms = [\"zlib\"]\n\t}\n\n\toutput {\n\t\tmetrics = [otelcol.exporter.splunkhec.default.input]\n\t\tlogs    = [otelcol.exporter.splunkhec.default.input]\n\t\ttraces  = [otelcol.exporter.splunkhec.default.input]\n\t}\n}\n\notelcol.exporter.splunkhec \"default\" {\n\tclient {\n\t\tendpoint                = \"https://splunkhec.domain.com:8088/services/collector\"\n\t\ttimeout                 = \"10s\"\n\t\tmax_idle_conns          = 200\n\t\tmax_idle_conns_per_host = 200\n\t\tidle_conn_timeout       = \"10s\"\n\t}\n\n\tsplunk {\n\t\ttoken              = \"SPLUNK_TOKEN\"\n\t\tsource             = \"otel\"\n\t\tsourcetype         = \"otel\"\n\t\tindex              = \"metrics\"\n\t\tsplunk_app_name    = \"OpenTelemetry-Collector Splunk Exporter\"\n\t\tsplunk_app_version = \"v0.0.1\"\nz\n\t\totel_to_hec_fields {\n\t\t\tseverity_text   = \"otel.log.severity.text\"\n\t\t\tseverity_number = \"otel.log.severity.number\"\n\t\t}\n\n\t\theartbeat {\n\t\t\tinterval = \"30s\"\n\t\t}\n\n\t\ttelemetry {\n\t\t\tenabled                = true\n\t\t\toverride_metrics_names = {\n\t\t\t\totelcol_exporter_splunkhec_heartbeats_failed = \"app_heartbeats_failed_total\",\n\t\t\t\totelcol_exporter_splunkhec_heartbeats_sent   = \"app_heartbeats_success_total\",\n\t\t\t}\n\t\t\textra_attributes = {\n\t\t\t\tcustom_key   = \"custom_value\",\n\t\t\t\tdataset_name = \"SplunkCloudBeaverStack\",\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Example Using ConfigMap and Secret in Alloy\nDESCRIPTION: A complete example that reads both a Secret and a ConfigMap from Kubernetes and uses them to supply remote-write credentials for Prometheus.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/remote/remote.kubernetes.configmap.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nremote.kubernetes.secret \"credentials\" {\n  namespace = \"monitoring\"\n  name = \"metrics-secret\"\n}\n\nremote.kubernetes.configmap \"endpoint\" {\n  namespace = \"monitoring\"\n  name = \"metrics-endpoint\"\n}\n\nprometheus.remote_write \"default\" {\n  endpoint {\n    url = remote.kubernetes.configmap.endpoint.data[\"url\"]\n    basic_auth {\n      username = remote.kubernetes.configmap.endpoint.data[\"username\"]\n      password = remote.kubernetes.secret.credentials.data[\"password\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating Prometheus Squid Exporter with Remote Write in Alloy\nDESCRIPTION: Complete example demonstrating how to configure prometheus.exporter.squid, use prometheus.scrape to collect metrics, and send them to a remote write endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.squid.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.squid \"example\" {\n  address = \"localhost:3128\"\n}\n\n// Configure a prometheus.scrape component to collect squid metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.squid.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Metrics Exporter with Prometheus Remote Write\nDESCRIPTION: Example configuration showing how to set up Azure metrics exporter to collect metrics from Azure Storage accounts and forward them to a Prometheus remote write endpoint. Demonstrates setting up subscription access, resource targeting, metric selection, and authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.azure.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.azure \"example\" {\n    subscriptions    = \"<SUBSCRIPTIONS>\"\n    resource_type    = \"Microsoft.Storage/storageAccounts\"\n    regions          = [\n        \"westeurope\",\n    ]\n    metric_namespace = \"Microsoft.Storage/storageAccounts/blobServices\"\n    metrics          = [\n        \"Availability\",\n        \"BlobCapacity\",\n        \"BlobCount\",\n        \"ContainerCount\",\n        \"Egress\",\n        \"IndexCapacity\",\n        \"Ingress\",\n        \"SuccessE2ELatency\",\n        \"SuccessServerLatency\",\n        \"Transactions\",\n    ]\n    included_dimensions = [\n        \"ApiName\",\n        \"TransactionType\",\n    ]\n    timespan         = \"PT1H\"\n}\n\n// Configure a prometheus.scrape component to send metrics to.\nprometheus.scrape \"demo\" {\n    targets    = prometheus.exporter.azure.example.targets\n    forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n    endpoint {\n        url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n        basic_auth {\n            username = \"<USERNAME>\"\n            password = \"<PASSWORD>\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Promtail Configuration with Diagnostic Report\nDESCRIPTION: Command for converting a Promtail configuration to Grafana Alloy with the --report flag to generate a diagnostic report about the conversion process.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-promtail.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=promtail --report=<OUTPUT_REPORT_PATH> --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.processor.memory_limiter in Alloy\nDESCRIPTION: This snippet demonstrates how to configure the otelcol.processor.memory_limiter component in Alloy. It sets the check interval, memory limit, and output configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.memory_limiter.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.memory_limiter \"LABEL\" {\n  check_interval = \"1s\"\n\n  limit = \"50MiB\" // alternatively, set `limit_percentage` and `spike_limit_percentage`\n\n  output {\n    metrics = [...]\n    logs    = [...]\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows Event Logs Processing with Loki in Alloy\nDESCRIPTION: This example sets up a Windows security event log source and processing pipeline. It demonstrates how to collect Windows event logs, process them through JSON parsing, Windows event parsing, and label extraction stages before forwarding to a write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_50\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.windowsevent \"security\"  {\n    eventlog_name = \"Security\"\n    forward_to = [loki.process.default.receiver]\n}\nloki.process \"default\" {\n  forward_to = [loki.write.default.receiver]\n  stage.json {\n      expressions = {\n          message = \"\",\n          Overwritten = \"\",\n      }\n  }\n  stage.windowsevent {\n      source = \"message\"\n      overwrite_existing = true\n  }\n  stage.labels {\n    values = {\n      Description = \"\",\n      Subject_SecurityID  = \"\",        \n      ReadOP = \"Subject_ReadOperation\",\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Configuration Example for MySQL Database Observability\nDESCRIPTION: This example demonstrates a full configuration for MySQL database observability, including Prometheus scraping and forwarding to Grafana Cloud. It sets up database observability, configures Prometheus scraping, and defines remote write endpoints for metrics and logs.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/database_observability/database_observability.mysql.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndatabase_observability.mysql \"orders_db\" {\n  data_source_name = \"user:pass@tcp(mysql:3306)/\"\n  forward_to = [loki.write.logs_service.receiver]\n}\n\nprometheus.scrape \"orders_db\" {\n  targets = database_observability.mysql.orders_db.targets\n  honor_labels = true // required to keep job and instance labels\n  forward_to = [prometheus.remote_write.metrics_service.receiver]\n}\n\nprometheus.remote_write \"metrics_service\" {\n  endpoint {\n    url = sys.env(\"<GRAFANA_CLOUD_HOSTED_METRICS_URL>\")\n    basic_auth {\n      username = sys.env(\"<GRAFANA_CLOUD_HOSTED_METRICS_ID>\")\n      password = sys.env(\"<GRAFANA_CLOUD_RW_API_KEY>\")\n    }\n  }\n}\n\nloki.write \"logs_service\" {\n  endpoint {\n    url = sys.env(\"<GRAFANA_CLOUD_HOSTED_LOGS_URL>\")\n    basic_auth {\n      username = sys.env(\"<GRAFANA_CLOUD_HOSTED_LOGS_ID>\")\n      password = sys.env(\"<GRAFANA_CLOUD_RW_API_KEY>\")\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete cAdvisor Exporter Implementation\nDESCRIPTION: Full example showing how to configure prometheus.exporter.cadvisor with prometheus.scrape and remote_write setup.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.cadvisor.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.cadvisor \"example\" {\n  docker_host = \"unix:///var/run/docker.sock\"\n\n  storage_duration = \"5m\"\n}\n\n// Configure a prometheus.scrape component to collect cadvisor metrics.\nprometheus.scrape \"scraper\" {\n  targets    = prometheus.exporter.cadvisor.example.targets\n  forward_to = [ prometheus.remote_write.demo.receiver ]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Filter Processor for HTTP Spans and Sensitive Logs\nDESCRIPTION: Configuration example that shows how to filter out non-HTTP spans and sensitive log records using the OpenTelemetry Collector filter processor. The configuration drops spans without HTTP attributes and filters logs containing password strings or with severity below warning level. All processed data is then forwarded to the default OTLP exporter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.filter.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.filter \"default\" {\n  error_mode = \"ignore\"\n\n  traces {\n    span = [\n      `attributes[\"http.request.method\"] == nil`,\n    ]\n  }\n\n  logs {\n    log_record = [\n      `IsMatch(body, \".*password.*\")`,\n      `severity_number < SEVERITY_NUMBER_WARN`,\n    ]\n  }\n\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Configuration for Azure Event Hubs Source with OAuth\nDESCRIPTION: This example shows how to configure the loki.source.azure_event_hubs component to consume messages from Azure Event Hub using OAuth 2.0 authentication and forward them to a Loki write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.azure_event_hubs.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.azure_event_hubs \"example\" {\n    fully_qualified_namespace = \"my-ns.servicebus.windows.net:9093\"\n    event_hubs                = [\"gw-logs\"]\n    forward_to                = [loki.write.example.receiver]\n\n    authentication {\n        mechanism = \"oauth\"\n    }\n}\n\nloki.write \"example\" {\n    endpoint {\n        url = \"loki:3100/api/v1/push\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Example of Complete Logs Collection Pipeline in Alloy\nDESCRIPTION: Demonstrates a full configuration for collecting and forwarding Alloy logs to a Loki instance, specifying warning level logs in JSON format.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/metamonitoring.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\nlogging {\n  level    = \"warn\"\n  format   = \"json\"\n  write_to = [loki.write.default.receiver]\n}\n\nloki.write \"default\" {\n  endpoint {\n    url = \"http://loki:3100/loki/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Tail Sampling Processor with Multiple Policies in Alloy\nDESCRIPTION: This example demonstrates how to configure the tail sampling processor with various sampling policies including always_sample, latency, numeric_attribute, probabilistic, status_code, string_attribute, rate_limiting, span_count, trace_state, ottl_condition, and composite policies.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.tail_sampling.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\ntracing {\n  sampling_fraction = 1\n  write_to          = [otelcol.processor.tail_sampling.default.input]\n}\n\notelcol.processor.tail_sampling \"default\" {\n  decision_cache = {\n    sampled_cache_size     = 100000,\n    non_sampled_cache_size = 100000,\n    }\n  decision_wait               = \"10s\"\n  num_traces                  = 100\n  expected_new_traces_per_sec = 10\n\n  policy {\n    name = \"test-policy-1\"\n    type = \"always_sample\"\n  }\n\n  policy {\n    name = \"test-policy-2\"\n    type = \"latency\"\n\n    latency {\n      threshold_ms = 5000\n    }\n  }\n\n  policy {\n    name = \"test-policy-3\"\n    type = \"numeric_attribute\"\n\n    numeric_attribute {\n      key       = \"key1\"\n      min_value = 50\n      max_value = 100\n    }\n  }\n\n  policy {\n    name = \"test-policy-4\"\n    type = \"probabilistic\"\n\n    probabilistic {\n      sampling_percentage = 10\n    }\n  }\n\n  policy {\n    name = \"test-policy-5\"\n    type = \"status_code\"\n\n    status_code {\n      status_codes = [\"ERROR\", \"UNSET\"]\n    }\n  }\n\n  policy {\n    name = \"test-policy-6\"\n    type = \"string_attribute\"\n\n    string_attribute {\n      key    = \"key2\"\n      values = [\"value1\", \"value2\"]\n    }\n  }\n\n  policy {\n    name = \"test-policy-7\"\n    type = \"string_attribute\"\n\n    string_attribute {\n      key                    = \"key2\"\n      values                 = [\"value1\", \"val*\"]\n      enabled_regex_matching = true\n      cache_max_size         = 10\n    }\n  }\n\n  policy {\n    name = \"test-policy-8\"\n    type = \"rate_limiting\"\n\n    rate_limiting {\n      spans_per_second = 35\n    }\n  }\n\n  policy {\n    name = \"test-policy-9\"\n    type = \"string_attribute\"\n\n    string_attribute {\n      key                    = \"http.url\"\n      values                 = [\"/health\", \"/metrics\"]\n      enabled_regex_matching = true\n      invert_match           = true\n    }\n  }\n\n  policy {\n    name = \"test-policy-10\"\n    type = \"span_count\"\n\n    span_count {\n      min_spans = 2\n    }\n  }\n\n  policy {\n    name = \"test-policy-11\"\n    type = \"trace_state\"\n\n    trace_state {\n      key    = \"key3\"\n      values = [\"value1\", \"value2\"]\n    }\n  }\n\n  policy {\n    name = \"test-policy-12\"\n    type = \"ottl_condition\"\n    ottl_condition {\n      error_mode = \"ignore\"\n      span = [\n        \"attributes[\\\"test_attr_key_1\\\"] == \\\"test_attr_val_1\\\"\",\n        \"attributes[\\\"test_attr_key_2\\\"] != \\\"test_attr_val_1\\\"\",\n      ]\n      spanevent = [\n        \"name != \\\"test_span_event_name\\\"\",\n        \"attributes[\\\"test_event_attr_key_2\\\"] != \\\"test_event_attr_val_1\\\"\",\n      ]\n    }\n  }\n\n  policy {\n    name = \"and-policy-1\"\n    type = \"and\"\n\n    and {\n      and_sub_policy {\n        name = \"test-and-policy-1\"\n        type = \"numeric_attribute\"\n\n        numeric_attribute {\n          key       = \"key1\"\n          min_value = 50\n          max_value = 100\n        }\n      }\n\n      and_sub_policy {\n        name = \"test-and-policy-2\"\n        type = \"string_attribute\"\n\n        string_attribute {\n          key    = \"key1\"\n          values = [\"value1\", \"value2\"]\n        }\n      }\n    }\n  }\n\n  policy {\n    name = \"composite-policy-1\"\n    type = \"composite\"\n\n    composite {\n      max_total_spans_per_second = 1000\n      policy_order               = [\"test-composite-policy-1\", \"test-composite-policy-2\", \"test-composite-policy-3\"]\n\n      composite_sub_policy {\n        name = \"test-composite-policy-1\"\n        type = \"numeric_attribute\"\n\n        numeric_attribute {\n          key       = \"key1\"\n          min_value = 50\n          max_value = 100\n        }\n      }\n\n      composite_sub_policy {\n        name = \"test-composite-policy-2\"\n        type = \"string_attribute\"\n\n        string_attribute {\n          key    = \"key1\"\n          values = [\"value1\", \"value2\"]\n        }\n      }\n\n      composite_sub_policy {\n        name = \"test-composite-policy-3\"\n        type = \"always_sample\"\n      }\n\n      rate_allocation {\n        policy  = \"test-composite-policy-1\"\n        percent = 50\n      }\n\n      rate_allocation {\n        policy  = \"test-composite-policy-2\"\n        percent = 50\n      }\n    }\n  }\n\n  output {\n    traces = [otelcol.exporter.otlp.production.input]\n  }\n}\n\notelcol.exporter.otlp \"production\" {\n  client {\n    endpoint = sys.env(\"OTLP_SERVER_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Pipeline in Alloy\nDESCRIPTION: Example configuration showing a complete OpenTelemetry pipeline setup with OTLP receiver, batch processor, and OTLP exporter. Demonstrates the configuration of input/output relationships between components and endpoint specifications.\nSOURCE: https://github.com/grafana/alloy/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"example\" {\n  grpc {\n    endpoint = \"127.0.0.1:4317\"\n  }\n\n  output {\n    metrics = [otelcol.processor.batch.example.input]\n    logs    = [otelcol.processor.batch.example.input]\n    traces  = [otelcol.processor.batch.example.input]\n  }\n}\n\notelcol.processor.batch \"example\" {\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = \"my-otlp-grpc-server:4317\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Service Graph Connector in Alloy\nDESCRIPTION: This snippet demonstrates the configuration of various OpenTelemetry components including OTLP receiver, service graph connector, Prometheus exporter, and OTLP exporter. It shows how to set up trace processing, metric generation, and data export to Mimir and Tempo.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.servicegraph.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  grpc {\n    endpoint = \"0.0.0.0:4320\"\n  }\n\n  output {\n    traces  = [otelcol.connector.servicegraph.default.input,otelcol.exporter.otlp.grafana_cloud_traces.input]\n  }\n}\n\notelcol.connector.servicegraph \"default\" {\n  dimensions = [\"http.method\"]\n  output {\n    metrics = [otelcol.exporter.prometheus.default.input]\n  }\n}\n\notelcol.exporter.prometheus \"default\" {\n  forward_to = [prometheus.remote_write.mimir.receiver]\n}\n\nprometheus.remote_write \"mimir\" {\n  endpoint {\n    url = \"https://prometheus-xxx.grafana.net/api/prom/push\"\n\n    basic_auth {\n      username = sys.env(\"PROMETHEUS_USERNAME\")\n      password = sys.env(\"GRAFANA_CLOUD_API_KEY\")\n    }\n  }\n}\n\notelcol.exporter.otlp \"grafana_cloud_traces\" {\n  client {\n    endpoint = \"https://tempo-xxx.grafana.net/tempo\"\n    auth     = otelcol.auth.basic.grafana_cloud_traces.handler\n  }\n}\n\notelcol.auth.basic \"grafana_cloud_traces\" {\n  username = sys.env(\"TEMPO_USERNAME\")\n  password = sys.env(\"GRAFANA_CLOUD_API_KEY\")\n}\n```\n\n----------------------------------------\n\nTITLE: Comprehensive DNS Discovery and Prometheus Scraping Configuration\nDESCRIPTION: This example demonstrates how to configure DNS discovery for A records, set up Prometheus scraping, and forward the data to a remote write endpoint. It includes authentication for the remote write API.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.dns.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.dns \"dns_lookup\" {\n  names = [\"myservice.example.com\", \"myotherservice.example.com\"]\n  type = \"A\"\n  port = 8080\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.dns.dns_lookup.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Vault Integration Example with Token Authentication in Alloy\nDESCRIPTION: Example configuration showing how to read a Vault token from a local file, authenticate to a Vault server, and use retrieved credentials in a metrics.remote_write component for basic authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/remote/remote.vault.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file \"vault_token\" {\n  filename  = \"/var/data/vault_token\"\n  is_secret = true\n}\n\nremote.vault \"remote_write\" {\n  server = \"https://prod-vault.corporate.internal\"\n  path   = \"secret\"\n  key    = \"prometheus/remote_write\n\n  auth.token {\n    token = local.file.vault_token.content\n  }\n}\n\nmetrics.remote_write \"prod\" {\n  remote_write {\n    url = \"https://onprem-mimir:9009/api/v1/push\"\n    basic_auth {\n      username = remote.vault.remote_write.data.username\n      password = remote.vault.remote_write.data.password\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Tempo Integration Configuration in Alloy\nDESCRIPTION: Configuration for sending traces from the OpenTelemetry collector to Grafana Tempo. This sets up the OTLP exporter to send trace data directly to a Tempo server endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-to-lgtm-stack.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = \"tempo-server:4317\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana Alloy for Redis and Unix Metrics Scraping\nDESCRIPTION: This Alloy configuration sets up components for Redis and Unix metric collection, scraping, and remote writing to Prometheus. It includes exporters for Redis and Unix, a scraper, and a remote write endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/first-components-and-stdlib.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\n// Configure your first components, learn about the standard library, and learn how to run Grafana Alloy\n\n// prometheus.exporter.redis collects information about Redis and exposes\n// targets for other components to use\nprometheus.exporter.redis \"local_redis\" {\n    redis_addr = \"localhost:6379\"\n}\n\nprometheus.exporter.unix \"localhost\" { }\n\n// prometheus.scrape scrapes the targets that it is configured with and forwards\n// the metrics to other components (typically prometheus.relabel or prometheus.remote_write)\nprometheus.scrape \"default\" {\n    // This is scraping too often for typical use-cases, but is easier for testing and demo-ing!\n    scrape_interval = \"10s\"\n\n    // Here, prometheus.exporter.redis.local_redis.targets refers to the 'targets' export\n    // of the prometheus.exporter.redis component with the label \"local_redis\".\n    //\n    // If you have more than one set of targets that you would like to scrape, you can use\n    // the 'array.concat' function from the standard library to combine them.\n    targets    = array.concat(prometheus.exporter.redis.local_redis.targets, prometheus.exporter.unix.localhost.targets)\n    forward_to = [prometheus.remote_write.local_prom.receiver]\n}\n\n// prometheus.remote_write exports a 'receiver', which other components can forward\n// metrics to and it will remote_write them to the configured endpoint(s)\nprometheus.remote_write \"local_prom\" {\n    endpoint {\n        url = \"http://localhost:9090/api/v1/write\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Discovery with Namespace and Label Filtering in Alloy\nDESCRIPTION: This example demonstrates how to limit Kubernetes discovery to specific namespaces and filter Pods by label values. It includes configuration for Kubernetes discovery, Prometheus scraping, and remote write.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.kubernetes.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubernetes \"k8s_pods\" {\n  role = \"pod\"\n\n  selectors {\n    role = \"pod\"\n    label = \"app.kubernetes.io/name=prometheus-node-exporter\"\n  }\n\n  namespaces {\n    names = [\"myapp\"]\n  }\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.kubernetes.k8s_pods.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Formatting Strings with string.format in Alloy\nDESCRIPTION: The string.format function produces a formatted string based on a specification and provided values. It supports various formatting verbs for different data types.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/string.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nstring.format(spec, values...)\n```\n\nLANGUAGE: alloy\nCODE:\n```\n> string.format(\"Hello, %s!\", \"Ander\")\n\"Hello, Ander!\"\n> string.format(\"There are %d lights\", 4)\n\"There are 4 lights\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Exporter for Grafana Cloud Traces in Alloy\nDESCRIPTION: This snippet demonstrates how to configure an OTLP exporter in Alloy to send trace data to Grafana Cloud. It sets up the client endpoint and uses basic authentication with credentials stored in environment variables.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.otlp.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlp \"grafana_cloud_traces\" {\n    client {\n        endpoint = \"tempo-xxx.grafana.net/tempo:443\"\n        auth     = otelcol.auth.basic.grafana_cloud_traces.handler\n    }\n}\notelcol.auth.basic \"grafana_cloud_traces\" {\n    username = sys.env(\"TEMPO_USERNAME\")\n    password = sys.env(\"GRAFANA_CLOUD_API_KEY\")\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SpanMetrics with Explicit Histogram and Extra Dimensions\nDESCRIPTION: Demonstrates configuring the spanmetrics connector with custom dimensions (http.status_code, http.method), explicit histogram buckets, and OTLP export. Shows how to set dimension defaults, cache size, aggregation temporality and metrics flush interval.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.spanmetrics.md#2025-04-22_snippet_8\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  http {}\n  grpc {}\n\n  output {\n    traces  = [otelcol.connector.spanmetrics.default.input]\n  }\n}\n\notelcol.connector.spanmetrics \"default\" {\n  // Since a default is not provided, the http.status_code dimension will be omitted\n  // if the span does not contain http.status_code.\n  dimension {\n    name = \"http.status_code\"\n  }\n\n  // If the span is missing http.method, the connector will insert\n  // the http.method dimension with value 'GET'.\n  dimension {\n    name = \"http.method\"\n    default = \"GET\"\n  }\n\n  dimensions_cache_size = 333\n\n  aggregation_temporality = \"DELTA\"\n\n  histogram {\n    unit = \"s\"\n    explicit {\n      buckets = [\"333ms\", \"777s\", \"999h\"]\n    }\n  }\n\n  // The period on which all metrics (whose dimension keys remain in cache) will be emitted.\n  metrics_flush_interval = \"33s\"\n\n  namespace = \"test.namespace\"\n\n  output {\n    metrics = [otelcol.exporter.otlp.production.input]\n  }\n}\n\notelcol.exporter.otlp \"production\" {\n  client {\n    endpoint = sys.env(\"OTLP_SERVER_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Kubernetes Services Metrics Collection Example\nDESCRIPTION: Full example configuration for collecting metrics from production Kubernetes services in the default namespace.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/prometheus-metrics.md#2025-04-22_snippet_10\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubernetes \"services\" {\n  role = \"service\"\n\n  namespaces {\n    own_namespace = false\n\n    names = [\"default\"]\n  }\n\n  selectors {\n    role  = \"service\"\n    label = \"environment in (production)\"\n  }\n}\n\nprometheus.scrape \"services\" {\n  targets    = discovery.kubernetes.services.targets\n  forward_to = [prometheus.remote_write.default.receiver]\n}\n\nprometheus.remote_write \"default\" {\n  endpoint {\n    url = \"http://localhost:9090/api/prom/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud OpenTelemetry Exporter in Grafana Alloy\nDESCRIPTION: This snippet demonstrates a complete configuration for scraping logs from local files, converting them to OpenTelemetry format, and sending them to Cloud Logging. It includes the recommended memory_limiter and batch processors for stability and performance.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.googlecloud.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file_match \"logs\" {\n  path_targets = [{\n    __address__ = \"localhost\",\n    __path__    = \"/var/log/{syslog,messages,*.log}\",\n    instance    = constants.hostname,\n    job         = \"integrations/node_exporter\",\n  }]\n}\n\nloki.source.file \"logs\" {\n  targets    = local.file_match.logs.targets\n  forward_to = [otelcol.receiver.loki.gcp.receiver]\n}\n\notelcol.receiver.loki \"gcp\" {\n  output {\n    logs = [otelcol.processor.memory_limiter.gcp.input]\n  }\n}\n\notelcol.processor.memory_limiter \"gcp\" {\n  check_interval = \"1s\"\n  limit = \"200MiB\"\n\n  output {\n    metrics = [otelcol.processor.batch.gcp.input]\n    logs = [otelcol.processor.batch.gcp.input]\n    traces = [otelcol.processor.batch.gcp.input]\n  }\n}\n\notelcol.processor.batch \"gcp\" {\n  output {\n    metrics = [otelcol.exporter.googlecloud.default.input]\n    logs = [otelcol.exporter.googlecloud.default.input]\n    traces = [otelcol.exporter.googlecloud.default.input]\n  }\n}\n\notelcol.exporter.googlecloud \"default\" {\n  project = \"my-gcp-project\"\n  log {\n    default_log_name = \"opentelemetry.io/collector-exported-log\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Log Processing Pipeline Configuration\nDESCRIPTION: Full configuration showing log reception, multi-stage processing including JSON parsing, timestamp handling, filtering, and forwarding to Loki.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/processing-logs.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.api \"listener\" {\n    http {\n        listen_address = \"127.0.0.1\"\n        listen_port    = 9999\n    }\n\n    labels = { \"source\" = \"api\" }\n\n    forward_to = [loki.process.process_logs.receiver]\n}\n\nloki.process \"process_logs\" {\n    stage.json {\n        expressions = {\n            log = \"\",\n            ts  = \"timestamp\",\n        }\n    }\n\n    stage.timestamp {\n        source = \"ts\"\n        format = \"RFC3339\"\n    }\n\n    stage.json {\n        source = \"log\"\n\n        expressions = {\n            is_secret = \"\",\n            level     = \"\",\n            log_line  = \"message\",\n        }\n    }\n\n    stage.drop {\n        source = \"is_secret\"\n        value  = \"true\"\n    }\n\n    stage.labels {\n        values = {\n            level = \"\",\n        }\n    }\n\n    stage.output {\n        source = \"log_line\"\n    }\n\n    stage.static_labels {\n        values = {\n            source = \"demo-api\",\n        }\n    }\n\n    forward_to = [loki.write.local_loki.receiver]\n}\n\nloki.write \"local_loki\" {\n    endpoint {\n        url = \"http://localhost:3100/loki/api/v1/push\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Collector Configuration Example\nDESCRIPTION: This is an example OpenTelemetry Collector configuration that defines OTLP receivers, memory limiter processors, and OTLP exporters for metrics, logs, and traces pipelines.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-otelcol.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n      http:\n\nexporters:\n  otlp:\n    endpoint: database:4317\n\nprocessors:\n  memory_limiter:\n    limit_percentage: 90\n    check_interval: 1s\n\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [otlp]\n      processors: [memory_limiter]\n      exporters: [otlp]\n    logs:\n      receivers: [otlp]\n      processors: [memory_limiter]\n      exporters: [otlp]\n    traces:\n      receivers: [otlp]\n      processors: [memory_limiter]\n      exporters: [otlp]\n```\n\n----------------------------------------\n\nTITLE: Complete Alloy Configuration for Collecting Kubernetes Pod Metrics\nDESCRIPTION: This example shows a complete configuration for collecting metrics from production Kubernetes Pods in the default Namespace. It includes Pod discovery, metric scraping, and remote writing.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/prometheus-metrics.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubernetes \"pods\" {\n  role = \"pod\"\n\n  namespaces {\n    own_namespace = false\n\n    names = [\"default\"]\n  }\n\n  selectors {\n    role  = \"pod\"\n    label = \"environment in (production)\"\n  }\n}\n\nprometheus.scrape \"pods\" {\n  targets    = discovery.kubernetes.pods.targets\n  forward_to = [prometheus.remote_write.default.receiver]\n}\n\nprometheus.remote_write \"default\" {\n  endpoint {\n    url = \"http://localhost:9090/api/prom/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Loki Source File Usage\nDESCRIPTION: Basic syntax for configuring a loki.source.file component with targets and forward_to parameters.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.file.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.file \"<LABEL>\" {\n  targets    = <TARGET_LIST>\n  forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Grafana Cloud Metrics Configuration with Basic Authentication in Alloy\nDESCRIPTION: Configuration for sending metrics to Grafana Cloud Metrics using basic authentication. This example shows how to set up the Prometheus exporter with the prometheus.remote_write component configured with authentication details for Grafana Cloud.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-to-lgtm-stack.md#2025-04-22_snippet_9\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.prometheus \"grafana_cloud_metrics\" {\n        forward_to = [prometheus.remote_write.grafana_cloud_metrics.receiver]\n    }\n\nprometheus.remote_write \"grafana_cloud_metrics\" {\n    endpoint {\n        url = \"https://prometheus-us-central1.grafana.net/api/prom/push\"\n\n        basic_auth {\n            username = \"12690\"\n            password = sys.env(\"GRAFANA_CLOUD_API_KEY\")\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Prometheus Configuration to Alloy Using CLI\nDESCRIPTION: Command to convert a Prometheus configuration file to a Grafana Alloy configuration using the convert CLI command. This allows you to migrate your configuration while taking advantage of Alloy's additional features.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-prometheus.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=prometheus --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Basic PodMonitors Configuration with Remote Write\nDESCRIPTION: Discovers all PodMonitors in the Kubernetes cluster and forwards collected metrics to a prometheus.remote_write component that sends data to a Mimir instance.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.operator.podmonitors.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.remote_write \"staging\" {\n  // Send metrics to a locally running Mimir.\n  endpoint {\n    url = \"http://mimir:9009/api/v1/push\"\n\n    basic_auth {\n      username = \"example-user\"\n      password = \"example-password\"\n    }\n  }\n}\n\nprometheus.operator.podmonitors \"pods\" {\n    forward_to = [prometheus.remote_write.staging.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.processor.interval with OTLP Receiver and Exporter\nDESCRIPTION: This example demonstrates how to use otelcol.processor.interval in a complete pipeline with an OTLP receiver and exporter. It includes configuration for authentication and setting the interval to 30 seconds.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.interval.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  grpc { ... }\n  http { ... }\n\n  output {\n    metrics = [otelcol.processor.interval.default.input]\n  }\n}\n\notelcol.processor.interval \"default\" {\n  interval = \"30s\"\n  output {\n    metrics = [otelcol.exporter.otlphttp.grafana_cloud.input]\n  }\n}\n\notelcol.exporter.otlphttp \"grafana_cloud\" {\n  client {\n    endpoint = \"https://otlp-gateway-prod-gb-south-0.grafana.net/otlp\"\n    auth     = otelcol.auth.basic.grafana_cloud.handler\n  }\n}\n\notelcol.auth.basic \"grafana_cloud\" {\n  username = env(\"GRAFANA_CLOUD_USERNAME\")\n  password = env(\"GRAFANA_CLOUD_API_KEY\")\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Example of remote.http with Prometheus Integration in Alloy\nDESCRIPTION: A complete example showing how to read a JSON array from a remote endpoint using remote.http and use it as scrape targets for Prometheus with remote write configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/remote/remote.http.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nremote.http \"targets\" {\n  url = sys.env(\"MY_TARGETS_URL\")\n}\n\nprometheus.scrape \"default\" {\n  targets    = encoding.from_json(remote.http.targets.content)\n  forward_to = [prometheus.remote_write.default.receiver]\n}\n\nprometheus.remote_write \"default\" {\n  client {\n    url = sys.env(\"PROMETHEUS_URL\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Pipeline with Jaeger Receiver\nDESCRIPTION: Example configuration showing how to set up a pipeline that accepts Jaeger-formatted traces and forwards them to an OTLP server through a batch processor.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.jaeger.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.jaeger \"default\" {\n  protocols {\n    grpc {}\n    thrift_http {}\n    thrift_binary {}\n    thrift_compact {}\n  }\n\n  output {\n    traces = [otelcol.processor.batch.default.input]\n  }\n}\n\notelcol.processor.batch \"default\" {\n  output {\n    traces = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = \"my-otlp-server:4317\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring In-cluster Kubernetes Discovery in Alloy\nDESCRIPTION: This example demonstrates how to use in-cluster authentication to discover all Pods in a Kubernetes cluster. It sets up Kubernetes discovery, Prometheus scraping, and remote write configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.kubernetes.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubernetes \"k8s_pods\" {\n  role = \"pod\"\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.kubernetes.k8s_pods.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Discovery with Kubeconfig Authentication in Alloy\nDESCRIPTION: This example shows how to use a Kubeconfig file for authentication when discovering Kubernetes Pods. It includes Kubernetes discovery setup, Prometheus scraping, and remote write configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.kubernetes.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubernetes \"k8s_pods\" {\n  role = \"pod\"\n  kubeconfig_file = \"/etc/k8s/kubeconfig.yaml\"\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.kubernetes.k8s_pods.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Pyroscope Write Configuration\nDESCRIPTION: Basic usage example showing how to configure a pyroscope.write component with an endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.write.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\npyroscope.write \"<LABEL>\" {\n  endpoint {\n    url = \"<PYROSCOPE_URL>\"\n\n    ...\n  }\n\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Kubernetes API Server for Prometheus Scraping\nDESCRIPTION: Configuration for securely authenticating with the Kubernetes API server when scraping metrics. This example shows TLS configuration with certificate verification and bearer token authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.scrape.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"kubelet\" {\n        scheme = \"https\"\n        tls_config {\n            server_name = \"kubernetes\"\n            ca_file = \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"\n            insecure_skip_verify = false\n        }\n        bearer_token_file = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Prometheus Elasticsearch Exporter Configuration Example\nDESCRIPTION: A comprehensive example showing how to configure prometheus.exporter.elasticsearch with basic authentication, and use it with prometheus.scrape and prometheus.remote_write components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.elasticsearch.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.elasticsearch \"example\" {\n  address = \"http://localhost:9200\"\n  basic_auth {\n    username = \"<USERNAME>\"\n    password = \"<PASSWORD>\"\n  }\n}\n\n// Configure a prometheus.scrape component to collect Elasticsearch metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.elasticsearch.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Alloy OTEL Collection Pipeline\nDESCRIPTION: Complete configuration for Alloy's OpenTelemetry pipeline including OTLP receiver, batch processor, tail sampling processor, and OTLP exporter. Sets up trace collection on port 4320 with sampling and forwarding capabilities.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/updating-otel/README.md#2025-04-22_snippet_3\n\nLANGUAGE: grafana-alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n    grpc {\n        endpoint = \"0.0.0.0:4320\"\n    }\n\n    output {\n        traces  = [otelcol.processor.batch.default.input]\n    }\n}\n\notelcol.processor.batch \"default\" {\n    timeout = \"5s\"\n    send_batch_size = 100\n\n    output {\n        traces  = [otelcol.processor.tail_sampling.default.input]\n    }\n}\n\notelcol.processor.tail_sampling \"default\" {\n  decision_wait               = \"5s\"\n  num_traces                  = 50000\n  expected_new_traces_per_sec = 0\n\n  policy {\n    name = \"test-policy-1\"\n    type = \"probabilistic\"\n\n    probabilistic {\n      sampling_percentage = 10\n    }\n  }\n\n  policy {\n    name = \"test-policy-2\"\n    type = \"status_code\"\n\n    status_code {\n      status_codes = [\"ERROR\"]\n    }\n  }\n\n  output {\n    traces = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n    client {\n        endpoint = \"localhost:4317\"\n        tls {\n            insecure = true\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Pod Log Collection and Loki Writing in Alloy\nDESCRIPTION: This snippet demonstrates how to use Alloy components to discover Kubernetes Pods, collect their logs, and forward them to Loki. It includes three main components: Kubernetes discovery, log source configuration, and Loki write endpoint setup.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.kubernetes.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubernetes \"pods\" {\n  role = \"pod\"\n}\n\nloki.source.kubernetes \"pods\" {\n  targets    = discovery.kubernetes.pods.targets\n  forward_to = [loki.write.local.receiver]\n}\n\nloki.write \"local\" {\n  endpoint {\n    url = sys.env(\"<LOKI_URL>\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring remote.vault Component in Alloy\nDESCRIPTION: Basic configuration example for connecting to a HashiCorp Vault server to retrieve secrets using token authentication. This snippet demonstrates how to set up server connection, path to the secret, key to retrieve, and token-based authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/remote/remote.vault.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nremote.vault \"<LABEL>\" {\n  server = \"<VAULT_SERVER>\"\n  path   = \"<VAULT_PATH>\"\n  key    = \"<VAULT_KEY>\"\n\n  // Alternatively, use one of the other auth.* mechanisms.\n  auth.token {\n    token = \"<AUTH_TOKEN>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Order Status Tracking with Regex and Metrics in Alloy\nDESCRIPTION: Pipeline that uses regex to extract order status and creates counters for successful and failed orders. Demonstrates pattern matching and conditional metric incrementing.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_22\n\nLANGUAGE: alloy\nCODE:\n```\nstage.regex {\n    expression = \"^.* order_status=(?P<order_status>.*?) .*$\"\n}\nstage.metrics {\n    metric.counter {\n        name        = \"successful_orders_total\"\n        description = \"successful orders\"\n        source      = \"order_status\"\n        value       = \"success\"\n        action      = \"inc\"\n    }\n}\nstage.metrics {\n    metric.counter {\n        name        = \"failed_orders_total\"\n        description = \"failed orders\"\n        source      = \"order_status\"\n        value       = \"fail\"\n        action      = \"inc\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Advanced Loki AWS Firehose Configuration with Relabeling\nDESCRIPTION: Extended configuration that includes relabeling rules for CloudWatch logs, preserving log stream and group information. This setup enhances the basic configuration by adding label management for better log organization and traceability.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.awsfirehose.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nloki.write \"local\" {\n    endpoint {\n        url = \"http://loki:3100/api/v1/push\"\n        basic_auth {\n            username = \"<USERNAME>\"\n            password_file = \"<PASSWORD_FILE>\"\n        }\n    }\n}\n\nloki.source.awsfirehose \"loki_fh_receiver\" {\n    http {\n        listen_address = \"0.0.0.0\"\n        listen_port = 9999\n    }\n    forward_to = [\n        loki.write.local.receiver,\n    ]\n    relabel_rules = loki.relabel.logging_origin.rules\n}\n\nloki.relabel \"logging_origin\" {\n  rule {\n    action = \"replace\"\n    source_labels = [\"__aws_cw_log_group\"]\n    target_label = \"log_group\"\n  }\n  rule {\n    action = \"replace\"\n    source_labels = [\"__aws_cw_log_stream\"]\n    target_label = \"log_stream\"\n  }\n  forward_to = []\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring a Pipeline with Multiple Components in Alloy\nDESCRIPTION: This complex snippet demonstrates a complete pipeline configuration in Alloy. It includes components for reading an API key from a file, setting up Prometheus remote write, discovering Kubernetes pods, and scraping metrics from those pods.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/components.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\n// Get our API key from disk.\n//\n// This component has an exported field called \"content\", holding the content\n// of the file.\n//\n// local.file will watch the file and update its exports any time the\n// file changes.\nlocal.file \"api_key\" {\n  filename  = \"/var/data/secrets/api-key\"\n\n  // Mark this file as sensitive to prevent its value from being shown in the\n  // UI.\n  is_secret = true\n}\n\n// Create a prometheus.remote_write component, which other components can send\n// metrics to.\n//\n// This component exports a \"receiver\" value, which can be used by other\n// components to send metrics.\nprometheus.remote_write \"prod\" {\n  endpoint {\n    url = \"https://prod:9090/api/v1/write\"\n\n    basic_auth {\n      username = \"admin\"\n\n      // Use the password file to authenticate with the production database.\n      password = local.file.api_key.content\n    }\n  }\n}\n\n// Find Kubernetes pods where we can collect metrics.\n//\n// This component exports a \"targets\" value, which contains the list of\n// discovered pods.\ndiscovery.kubernetes \"pods\" {\n  role = \"pod\"\n}\n\n// Collect metrics from Kubernetes pods and send them to prod.\nprometheus.scrape \"default\" {\n  targets    = discovery.kubernetes.pods.targets\n  forward_to = [prometheus.remote_write.prod.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Grafana Cloud Logs Configuration with Basic Authentication in Alloy\nDESCRIPTION: Configuration for sending logs to Grafana Cloud Logs using basic authentication. This example shows how to set up the Loki exporter with the loki.write component configured with authentication details for Grafana Cloud.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-to-lgtm-stack.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.loki \"grafana_cloud_logs\" {\n  forward_to = [loki.write.grafana_cloud_logs.receiver]\n}\n\nloki.write \"grafana_cloud_logs\" {\n  endpoint {\n    url = \"https://logs-prod-us-central1.grafana.net/loki/api/v1/push\"\n\n    basic_auth {\n      username = \"5252\"\n      password = sys.env(\"GRAFANA_CLOUD_API_KEY\")\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Operator ScrapeConfigs with Remote Write in Alloy\nDESCRIPTION: This example demonstrates how to configure the prometheus.operator.scrapeconfigs component to discover all scrapeconfigs in a cluster and forward collected metrics to a prometheus.remote_write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.operator.scrapeconfigs.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.remote_write \"staging\" {\n  // Send metrics to a locally running Mimir.\n  endpoint {\n    url = \"http://mimir:9009/api/v1/push\"\n\n    basic_auth {\n      username = \"example-user\"\n      password = \"example-password\"\n    }\n  }\n}\n\nprometheus.operator.scrapeconfigs \"scrapeconfigs\" {\n    forward_to = [prometheus.remote_write.staging.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring prometheus.relabel Component in Alloy\nDESCRIPTION: Demonstrates the basic structure for configuring a prometheus.relabel component in Alloy, including the forward_to argument and rule block.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.relabel.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.relabel \"<LABEL>\" {\n  forward_to = <RECEIVER_LIST>\n\n  rule {\n    ...\n  }\n\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Grafana Alloy Configuration with GCE Discovery\nDESCRIPTION: Full example of configuring GCE discovery, Prometheus scraping, and remote write in Grafana Alloy. Includes discovery.gce, prometheus.scrape, and prometheus.remote_write components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.gce.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.gce \"gce\" {\n  project = \"alloy\"\n  zone    = \"us-east1-a\"\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.gce.gce.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Tracing with OTLP Export in Alloy\nDESCRIPTION: Example showing how to configure tracing with sampling fraction and OTLP export to Tempo. Demonstrates setting up sampling and connecting to a local Tempo instance without TLS.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/tracing.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ntracing {\n  sampling_fraction = 0.1\n\n  write_to = [otelcol.exporter.otlp.tempo.input]\n}\n\notelcol.exporter.otlp \"tempo\" {\n  // Send traces to a locally running Tempo without TLS enabled.\n  client {\n    endpoint = sys.env(\"TEMPO_OTLP_ENDPOINT\")\n\n    tls {\n      insecure = true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Remote Write in Alloy\nDESCRIPTION: This snippet demonstrates how to configure a Prometheus remote write component in Alloy. It sets up an endpoint for sending metrics to a specified URL.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/_index.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.remote_write \"default\" {\n  endpoint {\n    url = \"http://localhost:9009/api/prom/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring prometheus.write.queue for a local Mimir instance in Grafana Alloy\nDESCRIPTION: Example configuration that sets up a prometheus.write.queue component named 'staging' to send metrics to a local Mimir instance, with basic authentication. It also shows how to connect a prometheus.scrape component to send collected metrics through the queue.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.write.queue.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.write.queue \"staging\" {\n  // Send metrics to a locally running Mimir.\n  endpoint \"mimir\" {\n    url = \"http://mimir:9009/api/v1/push\"\n\n    basic_auth {\n      username = \"example-user\"\n      password = \"example-password\"\n    }\n  }\n}\n\n// Configure a prometheus.scrape component to send metrics to\n// prometheus.write.queue component.\nprometheus.scrape \"demo\" {\n  targets = [\n    // Collect metrics from the default HTTP listen address.\n    {\"__address__\" = \"127.0.0.1:12345\"},\n  ]\n  forward_to = [prometheus.write.queue.staging.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Label Selector Configuration\nDESCRIPTION: Configuration block for filtering Kubernetes services using label selectors.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/prometheus-metrics.md#2025-04-22_snippet_8\n\nLANGUAGE: alloy\nCODE:\n```\nselectors {\n  role  = \"service\"\n  label = \"<LABEL_SELECTOR>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Delta to Cumulative with OTLP Export Configuration\nDESCRIPTION: Example showing delta to cumulative conversion before sending to OTLP exporter with environment-based endpoint configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.deltatocumulative.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.deltatocumulative \"default\" {\n  output {\n    metrics = [otelcol.exporter.otlp.production.input]\n  }\n}\n\notelcol.exporter.otlp \"production\" {\n  client {\n    endpoint = sys.env(\"OTLP_SERVER_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Processor Chain Configuration\nDESCRIPTION: Example showing a full processor chain configuration with memory limiter, batch processor, and OTLP exporter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-data.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.memory_limiter \"default\" {\n  check_interval = \"1s\"\n  limit          = \"1GiB\"\n\n  output {\n    metrics = [otelcol.processor.batch.default.input]\n    logs    = [otelcol.processor.batch.default.input]\n    traces  = [otelcol.processor.batch.default.input]\n  }\n}\n\notelcol.processor.batch \"default\" {\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = \"my-otlp-grpc-server:4317\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete OpenCensus Receiver Configuration Example\nDESCRIPTION: Comprehensive example showing OpenCensus receiver setup with batch processing and OTLP export, including TLS, keepalive settings, and CORS configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.opencensus.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.opencensus \"default\" {\n    cors_allowed_origins = [\"https://*.test.com\", \"https://test.com\"]\n\n    endpoint  = \"0.0.0.0:9090\"\n    transport = \"tcp\"\n\n    max_recv_msg_size      = \"32KB\"\n    max_concurrent_streams = \"16\"\n    read_buffer_size       = \"1024KB\"\n    write_buffer_size      = \"1024KB\"\n    include_metadata       = true\n\n    tls {\n        cert_file = \"test.crt\"\n        key_file  = \"test.key\"\n    }\n\n    keepalive {\n        server_parameters {\n            max_connection_idle      = \"11s\"\n            max_connection_age       = \"12s\"\n            max_connection_age_grace = \"13s\"\n            time                     = \"30s\"\n            timeout                  = \"5s\"\n        }\n\n        enforcement_policy {\n            min_time              = \"10s\"\n            permit_without_stream = true\n        }\n    }\n\n    output {\n        metrics = [otelcol.processor.batch.default.input]\n        logs    = [otelcol.processor.batch.default.input]\n        traces  = [otelcol.processor.batch.default.input]\n    }\n}\n\notelcol.processor.batch \"default\" {\n    output {\n        metrics = [otelcol.exporter.otlp.default.input]\n        logs    = [otelcol.exporter.otlp.default.input]\n        traces  = [otelcol.exporter.otlp.default.input]\n    }\n}\n\notelcol.exporter.otlp \"default\" {\n    client {\n        endpoint = sys.env(\"OTLP_ENDPOINT\")\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Processing Logs by Body Content in Alloy\nDESCRIPTION: Configures an attribute processor that filters logs based on log body content matching a regex pattern. It obfuscates password attributes and removes token attributes for logs with bodies matching \"AUTH.*\", targeting only log signals.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.attributes.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.attributes \"default\" {\n    include {\n        match_type = \"regexp\"\n        log_bodies = [\"AUTH.*\"]\n    }\n    action {\n        key = \"password\"\n        action = \"update\"\n        value = \"obfuscated\"\n    }\n    action {\n        key = \"token\"\n        action = \"delete\"\n    }\n\n    output {\n        logs    = [otelcol.exporter.otlp.default.input]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana Alloy for ECS Container Metrics Collection\nDESCRIPTION: A Grafana Alloy configuration for scraping Prometheus metrics from an ECS exporter and forwarding them via remote write. This setup collects container metrics from the ECS metadata endpoint and sends them to a Prometheus-compatible endpoint configured via environment variables.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/ecs-opentelemetry-data.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprometheus.scrape \"stats\" {\n  targets    = [\n    { \"__address__\" = \"localhost:9779\" },\n  ]\n  metrics_path = \"/metrics\"\n  scheme       = \"http\"\n  forward_to   = [prometheus.remote_write.default.receiver]\n}\n\n// additional OTEL config as in [ecs-default-config]\n// OTLP receiver\n// statsd\n// Use the alloy convert command to use one of the AWS ADOT files\n// https://grafana.com/docs/alloy/latest/reference/cli/convert/\n...\n\nprometheus.remote_write \"default\" {\n  endpoint {\n    url = sys.env(\"PROMETHEUS_REMOTE_WRITE_URL\")\n      basic_auth {\n        username = sys.env(\"PROMETHEUS_USERNAME\")\n        password = sys.env(\"PROMETHEUS_PASSWORD\")\n      }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Container Discovery and Metric Scraping on Windows\nDESCRIPTION: This snippet shows how to configure Docker container discovery on a Windows host, set up Prometheus scraping, and forward metrics to a remote_write endpoint. It requires the Docker daemon to be exposed on tcp://localhost:2375 without TLS.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.docker.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.docker \"containers\" {\n  host = \"tcp://localhost:2375\"\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.docker.containers.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Multi-line Log Processing Configuration\nDESCRIPTION: Example of multiline log processing configuration with Flask web service logs. Shows how to merge related log lines into single entries based on timestamp patterns.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_25\n\nLANGUAGE: text\nCODE:\n```\nstage.multiline {\n    firstline     = \"^\\\\[\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{1,2}:\\\\d{2}:\\\\d{2}\\\\]\"\n    max_wait_time = \"10s\"\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Alloy Configuration with OracleDB Exporter and Remote Write\nDESCRIPTION: Provides a full example of configuring prometheus.exporter.oracledb along with prometheus.scrape and prometheus.remote_write components. This setup collects metrics from an Oracle database and forwards them to a Prometheus-compatible remote write endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.oracledb.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.oracledb \"example\" {\n  connection_string = \"oracle://user:password@localhost:1521/orcl.localnet\"\n}\n\n// Configure a prometheus.scrape component to collect oracledb metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.oracledb.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Metrics with Multiple Criteria\nDESCRIPTION: Example showing how to filter metrics based on multiple criteria including metric name, resource attributes, and metric type.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.filter.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.filter \"default\" {\n  error_mode = \"ignore\"\n\n  metrics {\n    metric = [\n       `name == \"my.metric\" and resource.attributes[\"my_label\"] == \"abc123\"`,\n       `type == METRIC_DATA_TYPE_HISTOGRAM`,\n    ]\n  }\n\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Example of discovery.lightsail with Prometheus Scraping\nDESCRIPTION: Provides a full example of using discovery.lightsail to discover Lightsail targets, scrape them with Prometheus, and send the metrics to a remote write endpoint. Includes configuration for region, authentication, and target selection.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.lightsail.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.lightsail \"lightsail\" {\n  region = \"us-east-1\"\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.lightsail.lightsail.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using local.file to Read SNMP Targets from YAML in Alloy\nDESCRIPTION: This example demonstrates reading SNMP targets from a YAML file using the local.file component and parsing them with encoding.from_yaml before providing them to the prometheus.exporter.snmp component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.snmp.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file \"targets\" {\n  filename = \"targets.yml\"\n}\n\nprometheus.exporter.snmp \"example\" {\n    config_file = \"snmp_modules.yml\"\n\n    targets = encoding.from_yaml(local.file.targets.content)\n\n    walk_param \"private\" {\n        retries = \"2\"\n    }\n\n    walk_param \"public\" {\n        retries = \"2\"\n    }\n}\n\n// Configure a prometheus.scrape component to collect SNMP metrics.\nprometheus.scrape \"demo\" {\n    targets    = prometheus.exporter.snmp.example.targets\n    forward_to = [ /* ... */ ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Detection Processor in Alloy\nDESCRIPTION: Basic configuration example for the otelcol.processor.resourcedetection component that shows how to set up output streams for logs, metrics and traces\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.resourcedetection.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.resourcedetection \"LABEL\" {\n  output {\n    logs    = [...]\n    metrics = [...]\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Loki Echo Pipeline Example\nDESCRIPTION: Example showing how to create a pipeline that reads log files from /var/log and prints them using loki.echo. Demonstrates integration with local.file_match and loki.source.file components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.echo.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file_match \"varlog\" {\n  path_targets = [{\n    __path__ = \"/var/log/*log\",\n    job      = \"varlog\",\n  }]\n}\n\nloki.source.file \"logs\" {\n  targets    = local.file_match.varlog.targets\n  forward_to = [loki.echo.example.receiver]\n}\n\nloki.echo \"example\" { }\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Discovery Configuration for Pyroscope eBPF Profiling\nDESCRIPTION: Alloy configuration that discovers Kubernetes pods on the same node and collects performance profiles from them using Pyroscope's eBPF collector. The configuration includes relabeling rules to properly identify services and attach metadata.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.ebpf.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubernetes \"all_pods\" {\n  role = \"pod\"\n  selectors {\n    field = \"spec.nodeName=\" + sys.env(\"HOSTNAME\")\n    role = \"pod\"\n  }\n}\n\ndiscovery.relabel \"local_pods\" {\n  targets = discovery.kubernetes.all_pods.targets\n  rule {\n    action = \"drop\"\n    regex = \"Succeeded|Failed\"\n    source_labels = [\"__meta_kubernetes_pod_phase\"]\n  }\n  rule {\n    action = \"replace\"\n    regex = \"(.*)@(.*)\"\n    replacement = \"ebpf/${1}/${2}\"\n    separator = \"@\"\n    source_labels = [\"__meta_kubernetes_namespace\", \"__meta_kubernetes_pod_container_name\"]\n    target_label = \"service_name\"\n  }\n  rule {\n    action = \"labelmap\"\n    regex = \"__meta_kubernetes_pod_label_(.+)\"\n  }\n  rule {\n    action = \"replace\"\n    source_labels = [\"__meta_kubernetes_namespace\"]\n    target_label = \"namespace\"\n  }\n  rule {\n    action = \"replace\"\n    source_labels = [\"__meta_kubernetes_pod_name\"]\n    target_label = \"pod\"\n  }\n  rule {\n    action = \"replace\"\n    source_labels = [\"__meta_kubernetes_pod_node_name\"]\n    target_label = \"node\"\n  }\n  rule {\n    action = \"replace\"\n    source_labels = [\"__meta_kubernetes_pod_container_name\"]\n    target_label = \"container\"\n  }\n}\npyroscope.ebpf \"local_pods\" {\n  forward_to = [ pyroscope.write.endpoint.receiver ]\n  targets = discovery.relabel.local_pods.output\n}\n\npyroscope.write \"endpoint\" {\n  endpoint {\n    url = \"http://pyroscope:4040\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Loki AWS Firehose Configuration\nDESCRIPTION: Basic configuration for setting up an HTTP server that receives AWS Firehose logs and forwards them to Loki using basic authentication. The server listens on 0.0.0.0:9999 and requires username and password credentials for Loki access.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.awsfirehose.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nloki.write \"local\" {\n    endpoint {\n        url = \"http://loki:3100/api/v1/push\"\n        basic_auth {\n            username = \"<USERNAME>\"\n            password_file = \"<PASSWORD_FILE>\"\n        }\n    }\n}\n\nloki.source.awsfirehose \"loki_fh_receiver\" {\n    http {\n        listen_address = \"0.0.0.0\"\n        listen_port = 9999\n    }\n    forward_to = [\n        loki.write.local.receiver,\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Scaleway Discovery and Prometheus Integration\nDESCRIPTION: Complete example showing how to set up Scaleway service discovery for machine monitoring, configure Prometheus scraping of discovered targets, and send metrics to a remote Prometheus server. Requires Scaleway API credentials and Prometheus remote write endpoint details.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.scaleway.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.scaleway \"example\" {\n    project_id = \"<SCALEWAY_PROJECT_ID>\"\n    role       = \"<SCALEWAY_PROJECT_ROLE>\"\n    access_key = \"<SCALEWAY_ACCESS_KEY>\"\n    secret_key = \"<SCALEWAY_SECRET_KEY>\"\n}\n\nprometheus.scrape \"demo\" {\n    targets    = discovery.scaleway.example.targets\n    forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n    endpoint {\n        url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n        basic_auth {\n            username = \"<USERNAME>\"\n            password = \"<PASSWORD>\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows Exporter with Remote Write in Alloy\nDESCRIPTION: This example demonstrates how to configure the Windows Exporter component to collect metrics and send them to a Prometheus remote_write endpoint. The configuration includes setting up the exporter, a scraper to collect the metrics, and a remote write component to forward the data.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.windows.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.windows \"default\" { }\n\n// Configure a prometheus.scrape component to collect windows metrics.\nprometheus.scrape \"example\" {\n  targets    = prometheus.exporter.windows.default.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Unix Metrics Exporter in Alloy\nDESCRIPTION: Configuration for the prometheus.exporter.unix component that exposes hardware and Linux kernel metrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-linux.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.unix \"integrations_node_exporter\" {\n  disable_collectors = [\"ipvs\", \"btrfs\", \"infiniband\", \"xfs\", \"zfs\"]\n  enable_collectors = [\"meminfo\"]\n\n  filesystem {\n    fs_types_exclude     = \"^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|tmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\"\n    mount_points_exclude = \"^/(dev|proc|run/credentials/.+|sys|var/lib/docker/.+)($|/)\"\n    mount_timeout        = \"5s\"\n  }\n\n  netclass {\n    ignored_devices = \"^(veth.*|cali.*|[a-f0-9]{15})$\"\n  }\n\n  netdev {\n    device_exclude = \"^(veth.*|cali.*|[a-f0-9]{15})$\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Hashing Sensitive Data in Log Entries\nDESCRIPTION: Implements data obfuscation by applying secure hashing algorithms to sensitive information with salt values to protect PII in logs.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_44\n\nLANGUAGE: alloy\nCODE:\n```\nstage.template {\n    source   = \"output\"\n    template = `{{ Hash .Value \"salt\" }}`\n}\nstage.template {\n    source   = \"output\"\n    template = `{{ Sha2Hash .Value \"salt\" }}`\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry OTLP Exporter in Alloy\nDESCRIPTION: Basic configuration for the otelcol.exporter.otlp component to export OpenTelemetry data to a specified endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-data.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlp \"<EXPORTER_LABEL>\" {\n  client {\n    endpoint = \"<HOST>:<PORT>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using discovery.file for Dynamic SNMP Target Discovery in Alloy\nDESCRIPTION: This example shows how to use the discovery.file component to dynamically discover SNMP targets from a YAML file and provide them to the prometheus.exporter.snmp component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.snmp.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.file \"example\" {\n  files = [\"targets.yml\"]\n}\n\nprometheus.exporter.snmp \"example\" {\n  config_file = \"snmp_modules.yml\"\n  targets = discovery.file.example.targets\n}\n\n// Configure a prometheus.scrape component to collect SNMP metrics.\nprometheus.scrape \"demo\" {\n    targets    = prometheus.exporter.snmp.example.targets\n    forward_to = [ /* ... */ ]\n}\n```\n\n----------------------------------------\n\nTITLE: Prometheus Remote Write Configuration in Alloy\nDESCRIPTION: Configuration for sending metrics from the OpenTelemetry collector to a Prometheus-compatible remote write endpoint. This sets up the Prometheus exporter to forward metrics to the Prometheus remote write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-to-lgtm-stack.md#2025-04-22_snippet_8\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.prometheus \"default\" {\n  forward_to = [prometheus.remote_write.default.receiver]\n}\n\nprometheus.remote_write \"default\" {\n  endpoint {\n    url = \"http://prometheus:9090/api/v1/write\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Discovering Docker Containers on Linux/macOS with Prometheus Scraping\nDESCRIPTION: Complete example demonstrating how to discover Docker containers on Linux or macOS hosts, scrape metrics using Prometheus, and send them to a remote write endpoint. It includes configuration for discovery.docker, prometheus.scrape, and prometheus.remote_write components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.docker.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.docker \"containers\" {\n  host = \"unix:///var/run/docker.sock\"\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.docker.containers.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete OpenTelemetry Pipeline Configuration\nDESCRIPTION: Full example showing integration of host_info connector in an OpenTelemetry pipeline with OTLP receiver, resource detection, and Prometheus export to Mimir.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.host_info.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"otlp\" {\n  http {}\n  grpc {}\n\n  output {\n    traces = [otelcol.processor.resourcedetection.otlp_resources.input]\n  }\n}\n\notelcol.processor.resourcedetection \"otlp_resources\" {\n  detectors = [\"system\"]\n  system {\n    hostname_sources = [ \"os\" ]\n    resource_attributes {\n      host.id {\n        enabled = true\n      }\n    }\n  }\n  output {\n    traces = [otelcol.connector.host_info.default.input]\n  }\n}\n\notelcol.connector.host_info \"default\" {\n  output {\n    metrics = [otelcol.exporter.prometheus.otlp_metrics.input]\n  }\n}\n\notelcol.exporter.prometheus \"otlp_metrics\" {\n  forward_to = [prometheus.remote_write.default.receiver]\n}\n\nprometheus.remote_write \"default\" {\n  endpoint {\n    url = \"https://prometheus-xxx.grafana.net/api/prom/push\"\n    basic_auth {\n      username = sys.env(\"PROMETHEUS_USERNAME\")\n      password = sys.env(\"GRAFANA_CLOUD_API_KEY\")\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic File Discovery Configuration in Alloy\nDESCRIPTION: Complete example of configuring discovery.file with Prometheus scraping and remote write. It demonstrates how to discover targets from a file and use them in a scraping configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.file.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.file \"example\" {\n  files = [\"/tmp/example.json\"]\n}\n\nprometheus.scrape \"default\" {\n  targets    = discovery.file.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Metric Statements in Alloy\nDESCRIPTION: Example of configuring multiple metric statements using the 'statements' block in Alloy. This snippet shows how to set resource attributes and metric descriptions.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.transform.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nstatements {\n    metric = [\n        `resource.attributes[\"test\"], \"passed\"`,\n        `set(metric.description, \"test passed\"`,\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Comprehensive otelcol.receiver.awscloudwatch Configuration\nDESCRIPTION: A more detailed example showing how to collect logs from specific EKS cluster log groups and forward them through a batch processor to an OTLP exporter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.awscloudwatch.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.awscloudwatch \"default\" {\n  region = \"us-west-2\"\n\n  logs {\n    poll_interval = \"3m\"\n    max_events_per_request = 5000\n\n    groups {\n      named {\n        group_name = \"/aws/eks/dev-cluster/cluster\"\n        names = [\"api-gateway\"]\n      }\n      named {\n        group_name = \"/aws/eks/prod-cluster/cluster\"\n        prefixes = [\"app-\", \"service-\"]\n      }\n    }\n  }\n\n  output {\n    logs = [otelcol.processor.batch.default.input]\n  }\n}\n\notelcol.processor.batch \"default\" {\n  output {\n    logs = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = env(\"<OTLP_ENDPOINT>\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Forwarding Loki Logs to Splunk HEC\nDESCRIPTION: This example demonstrates how to watch for log files, tail them with Loki, and forward the logs to a configured Splunk HEC endpoint. It includes configuration for file matching, Loki source, OpenTelemetry receiver, processor, and the Splunk HEC exporter with heartbeat settings.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.splunkhec.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file_match \"local_files\" {\n\tpath_targets = [{\"__path__\" = \"/var/log/*.log\"}]\n\tsync_period  = \"5s\"\n}\n\notelcol.receiver.loki \"default\" {\n\toutput {\n\t\tlogs = [otelcol.processor.attributes.default.input]\n\t}\n}\n\notelcol.processor.attributes \"default\" {\n\taction {\n\t\tkey    = \"host\"\n\t\taction = \"upsert\"\n\t\tvalue  = \"myhost\"\n\t}\n\n\taction {\n\t\tkey    = \"host.name\"\n\t\taction = \"upsert\"\n\t\tvalue  = \"myhost\"\n\t}\n\n\toutput {\n\t\tlogs = [otelcol.exporter.splunkhec.default.input]\n\t}\n}\n\nloki.source.file \"log_scrape\" {\n\ttargets       = local.file_match.local_files.targets\n\tforward_to    = [otelcol.receiver.loki.default.receiver]\n\ttail_from_end = false\n}\n\notelcol.exporter.splunkhec \"default\" {\n\tretry_on_failure {\n\t\tenabled = false\n\t}\n\n\tclient {\n\t\tendpoint                = \"http://splunkhec.domain.com:8088\"\n\t\ttimeout                 = \"5s\"\n\t\tmax_idle_conns          = 200\n\t\tmax_idle_conns_per_host = 200\n\t\tidle_conn_timeout       = \"10s\"\n\t\twrite_buffer_size       = 8000\n\t}\n\n\tsending_queue {\n\t\tenabled = false\n\t}\n\n\tsplunk {\n\t\ttoken            = \"SPLUNK_TOKEN\"\n\t\tsource           = \"otel\"\n\t\tsourcetype       = \"otel\"\n\t\tindex            = \"devnull\"\n\t\tlog_data_enabled = true\n\n\t\theartbeat {\n\t\t\tinterval = \"5s\"\n\t\t}\n\n\t\tbatcher {\n\t\t\tflush_timeout = \"200ms\"\n\t\t}\n\n\t\ttelemetry {\n\t\t\tenabled                = true\n\t\t\toverride_metrics_names = {\n\t\t\t\totelcol_exporter_splunkhec_heartbeats_failed = \"app_heartbeats_failed_total\",\n\t\t\t\totelcol_exporter_splunkhec_heartbeats_sent   = \"app_heartbeats_success_total\",\n\t\t\t}\n\t\t\textra_attributes = {\n\t\t\t\thost   = \"myhost\",\n\t\t\t\tdataset_name = \"SplunkCloudBeaverStack\",\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Excluding Spans Based on Attributes in OpenTelemetry Collector\nDESCRIPTION: This configuration demonstrates how to exclude spans from processing based on specific attributes. It uses a strict match type and defines criteria for services and attributes. The example includes explanations of which spans would match or not match the exclusion criteria.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.attributes.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.attributes \"default\" {\n    exclude {\n        match_type = \"strict\"\n        services = [\"svcA\", \"svcB\"]\n        attribute {\n            key = \"env\"\n            value = \"dev\"\n        }\n        attribute {\n            key = \"test_request\"\n        }\n    }\n    action {\n        key = \"credit_card\"\n        action = \"delete\"\n    }\n    action {\n        key = \"duplicate_key\"\n        action = \"delete\"\n    }\n    output {\n        traces  = [otelcol.exporter.otlp.default.input]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Postgres Metrics Collection with Database Denylist\nDESCRIPTION: Sets up a Prometheus postgres exporter to collect metrics from databases excluding the 'secrets' database. Includes configuration for auto-discovery, scraping, and remote write endpoint with authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.postgres.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.postgres \"example\" {\n  data_source_names = [\"postgresql://username:password@localhost:5432/database_name?sslmode=disable\"]\n\n  // The database_denylist field will filter out those databases from the list of databases to scrape,\n  // meaning that all databases *except* these will be scraped.\n  //\n  // In this example it will scrape all databases except for the one named 'secrets'.\n  autodiscovery {\n    enabled           = true\n    database_denylist = [\"secrets\"]\n  }\n}\n\nprometheus.scrape \"default\" {\n  targets    = prometheus.exporter.postgres.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Resource Export Example in Alloy\nDESCRIPTION: Shows how to create a custom component that discovers Kubernetes pods and nodes, and exports their combined targets. Uses array concatenation to combine discovery results.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/export.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndeclare \"pods_and_nodes\" {\n  discovery.kubernetes \"pods\" {\n    role = \"pod\"\n  }\n\n  discovery.kubernetes \"nodes\" {\n    role = \"nodes\"\n  }\n\n  export \"kubernetes_resources\" {\n    value = array.concat(\n      discovery.kubernetes.pods.targets,\n      discovery.kubernetes.nodes.targets,\n    )\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Hetzner Discovery with Prometheus Integration\nDESCRIPTION: Full example showing Hetzner discovery configuration integrated with Prometheus scraping and remote write setup.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.hetzner.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.hetzner \"example\" {\n  role = \"<HETZNER_ROLE>\"\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.hetzner.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Collecting Metrics from Discovered Kubernetes Pods in Alloy\nDESCRIPTION: This snippet demonstrates how to configure a prometheus.scrape component to collect metrics from discovered Kubernetes Pods and forward them to a prometheus.remote_write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/prometheus-metrics.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"<SCRAPE_LABEL>\" {\n  targets    = discovery.kubernetes.<DISCOVERY_LABEL>.targets\n  forward_to = [prometheus.remote_write.<REMOTE_WRITE_LABEL>.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Cumulative to Delta Processor Example with OTLP Export\nDESCRIPTION: Extended example demonstrating how to configure the cumulative to delta processor with OTLP exporter integration, including environment variable configuration for the endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.cumulativetodelta.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.cumulativetodelta \"default\" {\n  output {\n    metrics = [otelcol.exporter.otlp.production.input]\n  }\n}\n\notelcol.exporter.otlp \"production\" {\n  client {\n    endpoint = sys.env(\"OTLP_SERVER_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynamic Targets for Pyroscope Scraping\nDESCRIPTION: Demonstrates setting up dynamic target discovery for Pyroscope scraping using HTTP discovery. The configuration fetches scrape targets from an external URL every 15 seconds and forwards collected profiles to a Pyroscope instance.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.scrape.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.http \"dynamic_targets\" {\n  url = \"https://example.com/scrape_targets\"\n  refresh_interval = \"15s\"\n}\n\npyroscope.scrape \"local\" {\n  targets = [discovery.http.dynamic_targets.targets]\n\n  forward_to = [pyroscope.write.local.receiver]\n}\n\npyroscope.write \"local\" {\n  endpoint {\n    url = \"http://pyroscope:4040\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Faro Receiver with Loki and OpenTelemetry Integration\nDESCRIPTION: Example configuration showing how to set up faro.receiver with sourcemap support, Loki logging, and OpenTelemetry trace export. Includes server configuration, sourcemap settings, and output routing to Loki and OTLP endpoints.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/faro/faro.receiver.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nfaro.receiver \"default\" {\n    server {\n        listen_address = \"<NETWORK_ADDRESS>\"\n    }\n\n    sourcemaps {\n        location {\n            path                 = \"<PATH_TO_SOURCEMAPS>\"\n            minified_path_prefix = \"<WEB_APP_PREFIX>\"\n        }\n    }\n\n    output {\n        logs   = [loki.write.default.receiver]\n        traces = [otelcol.exporter.otlp.traces.input]\n    }\n}\n\nloki.write \"default\" {\n    endpoint {\n        url = \"https://LOKI_ADDRESS/loki/api/v1/push\"\n    }\n}\n\notelcol.exporter.otlp \"traces\" {\n    client {\n        endpoint = \"<OTLP_ADDRESS>\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Discovery in Alloy\nDESCRIPTION: Alloy configuration for the discovery.docker component that discovers Docker containers and extracts metadata.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-docker-containers.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.docker \"linux\" {\n  host = \"unix:///var/run/docker.sock\"\n}\n```\n\n----------------------------------------\n\nTITLE: Azure VM Resource Attributes Configuration\nDESCRIPTION: Configuration block for Azure VM resource attributes including VM details, cloud platform information, and host information. Includes support for custom tag matching through regex patterns.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.resourcedetection.md#2025-04-22_snippet_2\n\nLANGUAGE: hcl\nCODE:\n```\nazure {\n  tags = []\n  resource_attributes {\n    azure.resourcegroup.name { enabled = true }\n    azure.vm.name { enabled = true }\n    azure.vm.scaleset.name { enabled = true }\n    azure.vm.size { enabled = true }\n    cloud.account.id { enabled = true }\n    cloud.platform { enabled = true }\n    cloud.provider { enabled = true }\n    cloud.region { enabled = true }\n    host.id { enabled = true }\n    host.name { enabled = true }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Prometheus Self-Exporter Setup with Remote Write\nDESCRIPTION: Example showing how to configure prometheus.exporter.self with prometheus.scrape and prometheus.remote_write for collecting and forwarding metrics to a remote endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.self.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.self \"example\" {}\n\n// Configure a prometheus.scrape component to collect Alloy metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.self.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana Alloy for Pod Log Collection and Processing\nDESCRIPTION: Alloy configuration for collecting and processing pod logs. It discovers pods, relabels the targets, matches log files, and processes logs for both containerd and docker runtimes before sending to Loki.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-operator.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nremote.kubernetes.secret \"credentials\" {\n  namespace = \"monitoring\"\n  name      = \"primary-credentials-logs\"\n}\n\ndiscovery.kubernetes \"pods\" {\n  role = \"pod\"\n  selectors {\n    role  = \"pod\"\n    field = \"spec.nodeName=\" + sys.env(\"<HOSTNAME>\")\n  }\n}\n\ndiscovery.relabel \"pod_logs\" {\n  targets = discovery.kubernetes.pods.targets\n  rule {\n    source_labels = [\"__meta_kubernetes_namespace\"]\n    target_label  = \"namespace\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_name\"]\n    target_label  = \"pod\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_container_name\"]\n    target_label  = \"container\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_namespace\", \"__meta_kubernetes_pod_name\"]\n    separator     = \"/\"\n    target_label  = \"job\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_uid\", \"__meta_kubernetes_pod_container_name\"]\n    separator     = \"/\"\n    action        = \"replace\"\n    replacement   = \"/var/log/pods/*$1/*.log\"\n    target_label  = \"__path__\"\n  }\n  rule {\n    action = \"replace\"\n    source_labels = [\"__meta_kubernetes_pod_container_id\"]\n    regex = \"^(\\\\w+):\\\\/\\\\/.+$\"\n    replacement = \"$1\"\n    target_label = \"tmp_container_runtime\"\n  }\n}\n\nlocal.file_match \"pod_logs\" {\n  path_targets = discovery.relabel.pod_logs.output\n}\n\nloki.source.file \"pod_logs\" {\n  targets    = local.file_match.pod_logs.targets\n  forward_to = [loki.process.pod_logs.receiver]\n}\n\nloki.process \"pod_logs\" {\n  stage.match {\n    selector = \"{tmp_container_runtime=\\\"containerd\\\"}\"\n    stage.cri {}\n    stage.labels {\n      values = {\n        flags   = \"\",\n        stream  = \"\",\n      }\n    }\n  }\n\n  stage.match {\n    selector = \"{tmp_container_runtime=\\\"docker\\\"}\"\n    stage.docker {}\n    stage.labels {\n      values = {\n        stream  = \"\",\n      }\n    }\n  }\n\n  stage.label_drop {\n    values = [\"tmp_container_runtime\"]\n  }\n\n  forward_to = [loki.write.loki.receiver]\n}\n\nloki.write \"loki\" {\n  endpoint {\n    url = \"https://<LOKI_URL>/loki/api/v1/push\"\n    basic_auth {\n      username = convert.nonsensitive(remote.kubernetes.secret.credentials.data[\"username\"])\n      password = remote.kubernetes.secret.credentials.data[\"password\"]\n    }\n}\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Metrics to Local Mimir Instance with Prometheus Remote Write in Alloy\nDESCRIPTION: This example demonstrates how to configure a prometheus.remote_write component to send metrics to a local Mimir instance with basic authentication, along with a prometheus.scrape component that collects metrics and forwards them to the remote_write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.remote_write.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.remote_write \"staging\" {\n  // Send metrics to a locally running Mimir.\n  endpoint {\n    url = \"http://mimir:9009/api/v1/push\"\n\n    basic_auth {\n      username = \"example-user\"\n      password = \"example-password\"\n    }\n  }\n}\n\n// Configure a prometheus.scrape component to send metrics to\n// prometheus.remote_write component.\nprometheus.scrape \"demo\" {\n  targets = [\n    // Collect metrics from the default HTTP listen address.\n    {\"__address__\" = \"127.0.0.1:12345\"},\n  ]\n  forward_to = [prometheus.remote_write.staging.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Prometheus Scraping Configuration with Snowflake Exporter\nDESCRIPTION: Comprehensive example showing how to configure prometheus.exporter.snowflake with prometheus.scrape and remote_write components for collecting and forwarding Snowflake metrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.snowflake.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.snowflake \"example\" {\n  account_name = \"XXXXXXX-YYYYYYY\"\n  username     = \"grafana\"\n  password     = \"snowflake\"\n  warehouse    = \"examples\"\n}\n\n// Configure a prometheus.scrape component to collect snowflake metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.snowflake.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Nomad Discovery with Prometheus Integration\nDESCRIPTION: Complete example showing Nomad service discovery integrated with Prometheus scraping and remote write functionality. Includes authentication configuration and target forwarding.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.nomad.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.nomad \"example\" {\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.nomad.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete StatsD Exporter Setup with Prometheus Scraping\nDESCRIPTION: Comprehensive example demonstrating how to configure the StatsD exporter with TCP listening, custom mapping, and integration with prometheus.scrape and remote_write components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.statsd.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.statsd \"example\" {\n  listen_udp            = \"\"\n  listen_tcp            = \":9125\"\n  listen_unixgram       = \"\"\n  unix_socket_mode      = \"755\"\n  mapping_config_path   = \"mapTest.yaml\"\n  read_buffer           = 1\n  cache_size            = 1000\n  cache_type            = \"lru\"\n  event_queue_size      = 10000\n  event_flush_threshold = 1000\n  event_flush_interval  = \"200ms\"\n  parse_dogstatsd_tags  = true\n  parse_influxdb_tags   = true\n  parse_librato_tags    = true\n  parse_signalfx_tags   = true\n}\n\n// Configure a prometheus.scrape component to collect statsd metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.statsd.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring PodLogs Collection and Loki Write in Alloy\nDESCRIPTION: This snippet demonstrates how to configure the 'loki.source.podlogs' component to discover all PodLogs resources and forward collected logs to a 'loki.write' component for writing to Loki.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.podlogs.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.podlogs \"default\" {\n  forward_to = [loki.write.local.receiver]\n}\n\nloki.write \"local\" {\n  endpoint {\n    url = sys.env(\"LOKI_URL\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Mimir Rules Kubernetes Component in Alloy\nDESCRIPTION: This snippet demonstrates how to create a mimir.rules.kubernetes component that loads discovered rules to a local Mimir instance under the 'team-a' tenant. It includes selectors for namespaces and rules with specific labels.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/mimir/mimir.rules.kubernetes.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nmimir.rules.kubernetes \"local\" {\n    address = \"mimir:8080\"\n    tenant_id = \"team-a\"\n\n    rule_namespace_selector {\n        match_labels = {\n            alloy = \"yes\",\n        }\n    }\n\n    rule_selector {\n        match_labels = {\n            alloy = \"yes\",\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Grafana Package Repositories with Chef for Multiple Linux Distributions\nDESCRIPTION: This Chef recipe adds the Grafana package repositories to different Linux distribution families. For Debian-based systems, it creates APT sources, while for RHEL/Amazon/Fedora, it creates YUM repositories. The recipe fails if an unsupported platform is detected.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/chef.md#2025-04-22_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\nif platform_family?('debian', 'rhel', 'amazon', 'fedora')\n  if platform_family?('debian')\n    remote_file '/etc/apt/keyrings/grafana.gpg' do\n      source 'https://apt.grafana.com/gpg.key'\n      mode '0644'\n      action :create\n      end\n\n    file '/etc/apt/sources.list.d/grafana.list' do\n      content \"deb [signed-by=/etc/apt/keyrings/grafana.gpg] https://apt.grafana.com/ stable main\"\n      mode '0644'\n      notifies :update, 'apt_update[update apt cache]', :immediately\n    end\n\n    apt_update 'update apt cache' do\n      action :nothing\n    end\n  elsif platform_family?('rhel', 'amazon', 'fedora')\n    yum_repository 'grafana' do\n      description 'grafana'\n      baseurl 'https://rpm.grafana.com/oss/rpm'\n      gpgcheck true\n      gpgkey 'https://rpm.grafana.com/gpg.key'\n      enabled true\n      action :create\n      notifies :run, 'execute[add-rhel-key]', :immediately\n    end\n\n    execute 'add-rhel-key' do\n      command \"rpm --import https://rpm.grafana.com/gpg.key\"\n      action :nothing\n    end\n  end\nelse\n    fail \"The #{node['platform_family']} platform is not supported.\"\nend\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Cluster Events Logging with Loki in Alloy\nDESCRIPTION: This snippet demonstrates how to configure Loki components to capture Kubernetes cluster events and process them before sending to a Loki endpoint. It uses loki.source.kubernetes_events to tail events and loki.process to add labels and forward the logs.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/logs-in-kubernetes.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.kubernetes_events \"cluster_events\" {\n  job_name   = \"integrations/kubernetes/eventhandler\"\n  log_format = \"logfmt\"\n  forward_to = [\n    loki.process.cluster_events.receiver,\n  ]\n}\n\nloki.process \"cluster_events\" {\n  forward_to = [loki.write.<WRITE_COMPONENT_NAME>.receiver]\n\n  stage.static_labels {\n    values = {\n      cluster = \"<CLUSTER_NAME>\",\n    }\n  }\n\n  stage.labels {\n    values = {\n      kubernetes_cluster_events = \"job\",\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Multi-stage JSON and Regex Processing\nDESCRIPTION: Demonstrates a two-stage pipeline combining JSON parsing and regex matching to extract and process timestamp information.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_29\n\nLANGUAGE: alloy\nCODE:\n```\nstage.json {\n    expressions = { time = \"timestamp\" }\n}\nstage.regex {\n    expression = \"^(?P<year>\\\\d+)\"\n    source     = \"time\"\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Loki Journal Source Implementation Example\nDESCRIPTION: Comprehensive example showing loki.source.journal configuration with relabeling rules and endpoint writing.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.journal.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.relabel \"journal\" {\n  forward_to = []\n\n  rule {\n    source_labels = [\"__journal__systemd_unit\"]\n    target_label  = \"unit\"\n  }\n}\n\nloki.source.journal \"read\"  {\n  forward_to    = [loki.write.endpoint.receiver]\n  relabel_rules = loki.relabel.journal.rules\n  labels        = {component = \"loki.source.journal\"}\n}\n\nloki.write \"endpoint\" {\n  endpoint {\n    url =\"loki:3100/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP to Prometheus Conversion Pipeline in Alloy\nDESCRIPTION: This example sets up a pipeline to receive OTLP metrics, convert them to Prometheus format, and forward them using prometheus.remote_write.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.prometheus.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  grpc {}\n\n  output {\n    metrics = [otelcol.exporter.prometheus.default.input]\n  }\n}\n\notelcol.exporter.prometheus \"default\" {\n  forward_to = [prometheus.remote_write.mimir.receiver]\n}\n\nprometheus.remote_write \"mimir\" {\n  endpoint {\n    url = \"http://mimir:9009/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Remote Write in Alloy\nDESCRIPTION: Alloy configuration for the prometheus.remote_write component that sends metrics to a Prometheus server endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-docker-containers.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"http://prometheus:9090/api/v1/write\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Example of import.string with S3 Integration in Alloy\nDESCRIPTION: Shows a complete workflow that retrieves a module from an S3 bucket, imports it using import.string, and then instantiates a custom component from the imported module. The example demonstrates a math.add component that adds two numbers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/import.string.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nremote.s3 \"module\" {\n  path = \"s3://test-bucket/module.alloy\"\n}\n\nimport.string \"math\" {\n  content = remote.s3.module.content\n}\n\nmath.add \"default\" {\n  a = 15\n  b = 45\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring File Log Receiver with Timestamp Parsing in Alloy\nDESCRIPTION: This example configures the OpenTelemetry File Log Receiver to read log entries from specific files, parse timestamps using a regex operator, and send the processed logs to a debug exporter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.filelog.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.filelog \"default\" {\n  include = [\"/var/log/*.log\"]\n  operators = [{\n    type = \"regex_parser\",\n    regex = \"^(?P<timestamp>\\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\.\\\\d{3,6}Z)\",\n    timestamp = {\n      parse_from = \"attributes.timestamp\",\n      layout = \"%Y-%m-%dT%H:%M:%S.%fZ\",\n      location = \"UTC\",\n    },\n  }]\n  output {\n      logs = [otelcol.exporter.debug.default.input]\n  }\n}\n\notelcol.exporter.debug \"default\" {}\n```\n\n----------------------------------------\n\nTITLE: Collecting and Processing Docker Logs with Loki in Grafana Alloy\nDESCRIPTION: This code configures a complete pipeline for collecting Docker logs and sending them to Loki. It discovers Docker containers, relabels them to add container names as labels, collects logs using the Docker source, processes them with the Docker stage, and finally sends them to a local Loki instance.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/processing-logs.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\n// Discover docker containers to collect logs from\ndiscovery.docker \"docker_containers\" {\n    // Note that if you are using Docker Desktop Engine this may need to be changed to\n    // something like \"unix:///${HOME}/.docker/desktop/docker.sock\"\n    host = \"unix:///var/run/docker.sock\"\n}\n\n// Extract container name from __meta_docker_container_name label and add as label\ndiscovery.relabel \"docker_containers\" {\n    targets = discovery.docker.docker_containers.targets\n\n    rule {\n        source_labels = [\"__meta_docker_container_name\"]\n        target_label  = \"container\"\n    }\n}\n\n// Scrape logs from docker containers and send to be processed\nloki.source.docker \"docker_logs\" {\n    host    = \"unix:///var/run/docker.sock\"\n    targets = discovery.relabel.docker_containers.output\n    forward_to = [loki.process.process_logs.receiver]\n}\n\n// Process logs and send to Loki\nloki.process \"process_logs\" {\n    stage.docker { }\n\n    forward_to = [loki.write.local_loki.receiver]\n}\n\nloki.write \"local_loki\" {\n    endpoint {\n        url = \"http://localhost:3100/loki/api/v1/push\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Kafka Exporter in Alloy\nDESCRIPTION: This snippet demonstrates how to configure the OpenTelemetry Kafka exporter along with OTLP receiver and batch processor. It shows the setup for forwarding telemetry data through a batch processor before sending it to Kafka.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.kafka.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  http {}\n  grpc {}\n\n  output {\n    metrics = [otelcol.processor.batch.default.input]\n    logs    = [otelcol.processor.batch.default.input]\n    traces  = [otelcol.processor.batch.default.input]\n  }\n}\n\notelcol.processor.batch \"default\" {\n  output {\n    metrics = [otelcol.exporter.kafka.default.input]\n    logs    = [otelcol.exporter.kafka.default.input]\n    traces  = [otelcol.exporter.kafka.default.input]\n  }\n}\n\notelcol.exporter.kafka \"default\" {\n  brokers          = [\"localhost:9092\"]\n  protocol_version = \"2.0.0\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Datadog Receiver in Grafana Alloy\nDESCRIPTION: Sets up the otelcol.receiver.datadog component which receives Datadog metrics and traces. This component allows Alloy to ingest telemetry data from Datadog-instrumented applications.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/datadog-traces-metrics.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.datadog \"default\" {\n  endpoint = \"<HOST>:<PORT>\"\n  output {\n    metrics = [otelcol.processor.deltatocumulative.default.input]\n    traces  = [otelcol.processor.batch.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Structure\nDESCRIPTION: The main documentation structure written in Markdown, outlining the organization and purpose of different documentation sections in Grafana Alloy.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/writing-docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Writing documentation\n\nThis page is a collection of guidelines and best practices for writing\ndocumentation for Grafana Alloy.\n\n## Documentation organisation\n\nThe documentation is organized into the following sections:\n\n### Introduction\n\nThe best place to start for new users who are onboarding.\n\nWe showcase the features of Alloy and help users decide when to use it and\nwhether it's a good fit for them.\n\n### Get started\n\nThis section includes how to quickly install Alloy and get hands-on experience\nwith a simple \"hello world\" configuration.\n\n### Concepts\n\nAs defined in the [writer's toolkit][]:\n\n> Provides an overview and background information. Answers the question \"What is\n> it?\".\n\nIt helps users to learn the concepts of Alloy used throughout the\ndocumentation.\n\n### Tutorials\n\nAs defined in the [writer's toolkit][]:\n\n> Provides procedures that users can safely reproduce and learn from. Answers\n> the question: \"Can you teach me to …?\"\n\nThese are pages dedicated to learning. These are more broad,\nwhile [Tasks](#tasks) are focused on one objective. Tutorials may use\nnon-production-ready examples to facilitate learning, while tasks are expected\nto provide production-ready solutions.\n\n### Tasks\n\nAs defined in the [writer's toolkit][]:\n\n> Provides numbered steps that describe how to achieve an outcome. Answers the\n> question \"How do I?\".\n\nHowever, in Alloy documentation we don't mandate the use of numbered steps.\nWe do expect that tasks allow users to achieve a specific outcome by following\nthe page step by step, but we don't require numbered steps because some tasks\nbranch out into multiple paths, and numbering the steps would look more\nconfusing.\n\nTasks are production-ready and contain best practices and recommendations. They\nare quite detailed, with Reference pages being the only type of documentation\nthat has more detail.\n\nTasks should not be paraphrasing things which are already mentioned in the\nReference pages, such as default values and the meaning of the arguments.\nInstead, they should link to relevant Reference pages.\n\n### Reference\n\nThe Reference section is a collection of pages that describe Alloy components\nand their configuration options exhaustively. This is a more narrow definition\nthan the one found in the [writer's toolkit][].\n\nWe have a dedicated page with the best practices for writing Reference\ndocs: [writing components documentation][writing-docs].\n\nThis is our most detailed documentation, and it should be used as a source of\ntruth. The contents of the Reference pages should not be repeated in other parts\nof the documentation.\n\n### Release notes\n\nRelease notes notify users of changes in Alloy that require user action when\nupgrading. They are updated as part of the release process.\n\n[writer's toolkit]: https://grafana.com/docs/writers-toolkit/structure/topic-types/\n[writing-docs]: writing-component-documentation.md\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Remote Write in Alloy\nDESCRIPTION: This snippet demonstrates how to set up a prometheus.remote_write component for metrics delivery. It includes options for basic authentication and multiple endpoints.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/prometheus-metrics.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.remote_write \"<LABEL>\" {\n  endpoint {\n    url = \"<PROMETHEUS_URL>\"\n  }\n}\n```\n\nLANGUAGE: alloy\nCODE:\n```\nbasic_auth {\n  username = \"<USERNAME>\"\n  password = \"<PASSWORD>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Pipeline with Component References in Alloy\nDESCRIPTION: This code snippet demonstrates how to reference component exports in Alloy. It creates a file component that provides content to a Prometheus scrape component, which then forwards metrics to a remote_write component. The references show how components can be connected to form a pipeline.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/expressions/referencing_exports.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file \"target\" {\n  filename = \"/etc/alloy/target\"\n}\n\nprometheus.scrape \"default\" {\n  targets    = [{ \"__address__\" = local.file.target.content }]\n  forward_to = [prometheus.remote_write.onprem.receiver]\n}\n\nprometheus.remote_write \"onprem\" {\n  endpoint {\n    url = \"http://prometheus:9009/api/prom/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Kafka Receiver in Grafana Alloy\nDESCRIPTION: This example demonstrates how to configure the Kafka receiver to forward telemetry data through a batch processor before sending it to an OTLP-capable endpoint. It includes settings for brokers, protocol version, and output configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.kafka.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.kafka \"default\" {\n  brokers          = [\"localhost:9092\"]\n  protocol_version = \"2.0.0\"\n\n  output {\n    metrics = [otelcol.processor.batch.default.input]\n    logs    = [otelcol.processor.batch.default.input]\n    traces  = [otelcol.processor.batch.default.input]\n  }\n}\n\notelcol.processor.batch \"default\" {\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = sys.env(\"OTLP_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete loki.source.docker Configuration Example\nDESCRIPTION: A full example of configuring loki.source.docker with discovery.docker for target discovery and forwarding to loki.write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.docker.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.docker \"linux\" {\n  host = \"unix:///var/run/docker.sock\"\n}\n\nloki.source.docker \"default\" {\n  host       = \"unix:///var/run/docker.sock\"\n  targets    = discovery.docker.linux.targets\n  labels     = {\"app\" = \"docker\"}\n  forward_to = [loki.write.local.receiver]\n}\n\nloki.write \"local\" {\n  endpoint {\n    url = \"http://loki:3100/loki/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Grafana Alloy Configuration with IONOS Discovery and Prometheus Scraping\nDESCRIPTION: A full example demonstrating the use of discovery.ionos with Prometheus scraping and remote write. It includes setting up the discovery, configuring the scrape job, and defining the remote write endpoint with basic authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.ionos.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.ionos \"example\" {\n    datacenter_id = \"15f67991-0f51-4efc-a8ad-ef1fb31a480c\"\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.ionos.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Syslog Components in Alloy\nDESCRIPTION: Example configuration demonstrating how to set up syslog message proxying between OpenTelemetry receiver, exporter, and Loki components. Shows TCP server setup, TLS configuration, and message forwarding chain.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.syslog.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.syslog \"default\" {\n    protocol = \"rfc5424\"\n    tcp {\n        listen_address = \"localhost:1515\"\n    }\n    output {\n        logs = [otelcol.exporter.syslog.default.input]\n    }\n}\n\notelcol.exporter.syslog \"default\" {\n    endpoint = \"localhost\"\n    network = \"tcp\"\n    port = 1514\n    protocol = \"rfc5424\"\n    enable_octet_counting = false\n    tls {\n        insecure = true\n    }\n}\n\nloki.source.syslog \"default\" {\n  listener {\n    address = \"localhost:1514\"\n    protocol = \"tcp\"\n    syslog_format = \"rfc5424\"\n    label_structured_data = true\n    use_rfc5424_message = true\n  }\n  forward_to = [loki.echo.default.receiver]\n}\n\nloki.echo \"default\" {}\n```\n\n----------------------------------------\n\nTITLE: Configuring Local File Matching for Log Collection in Alloy\nDESCRIPTION: This snippet configures the local.file_match component to discover system log files on the local filesystem using glob patterns. It targets localhost, specifies log file paths, and adds instance and job labels.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-linux.md#2025-04-22_snippet_8\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file_match \"logs_integrations_integrations_node_exporter_direct_scrape\" {\n  path_targets = [{\n    __address__ = \"localhost\",\n    __path__    = \"/var/log/{syslog,messages,*.log}\",\n    instance    = constants.hostname,\n    job         = \"integrations/node_exporter\",\n  }]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Output Block Structure for Traces in YAML\nDESCRIPTION: This YAML snippet outlines the structure of the output block for configuring trace data forwarding in Grafana Alloy. It shows the available arguments, their types, descriptions, default values, and whether they are required.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/shared/reference/components/output-block-traces.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nName     | Type                     | Description                          | Default | Required\n---------|--------------------------|--------------------------------------|---------|---------\n`traces` | `list(otelcol.Consumer)` | List of consumers to send traces to. | `[]`    | no\n```\n\n----------------------------------------\n\nTITLE: Configuring Consul Discovery with Prometheus Scraping in Alloy\nDESCRIPTION: Sets up Consul service discovery for target discovery, configures Prometheus scraping of those targets, and forwards metrics to a remote write endpoint. Requires a running Consul server and Prometheus-compatible remote write endpoint with authentication credentials.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.consul.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.consul \"example\" {\n  server = \"localhost:8500\"\n  services = [\n    \"service1\",\n    \"service2\",\n  ]\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.consul.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring prometheus.receive_http in Alloy\nDESCRIPTION: Basic configuration for the prometheus.receive_http component, specifying the HTTP server settings and forwarding destinations.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.receive_http.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.receive_http \"<LABEL?\" {\n  http {\n    listen_address = \"<LISTEN_ADDRESS>\"\n    listen_port = <PORT>\n  }\n  forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Grafana Cloud Traces Configuration with Basic Authentication in Alloy\nDESCRIPTION: Configuration for sending traces to Grafana Cloud Traces using basic authentication. This example demonstrates how to set up the OTLP exporter with authentication via the otelcol.auth.basic component for Grafana Cloud.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-to-lgtm-stack.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlp \"grafana_cloud_traces\" {\n  client {\n    endpoint = \"tempo-us-central1.grafana.net:443\"\n    auth     = otelcol.auth.basic.grafana_cloud_traces.handler\n  }\n}\n\notelcol.auth.basic \"grafana_cloud_traces\" {\n  username = \"4094\"\n  password = sys.env(\"GRAFANA_CLOUD_API_KEY\")\n}\n```\n\n----------------------------------------\n\nTITLE: Loki Integration Configuration in Alloy\nDESCRIPTION: Configuration for sending logs from the OpenTelemetry collector to Grafana Loki. This sets up the Loki exporter to forward logs to the Loki write component, which then sends them to the specified Loki endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-to-lgtm-stack.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.loki \"default\" {\n  forward_to = [loki.write.default.receiver]\n}\n\nloki.write \"default\" {\n  endpoint {\n    url = \"http://loki-endpoint:8080/loki/api/v1/push\"\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Receiver with Debug Exporter in River\nDESCRIPTION: This example demonstrates how to configure an OTLP receiver to send metrics, logs, and traces to the debug exporter for console output.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.debug.md#2025-04-22_snippet_3\n\nLANGUAGE: river\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n    grpc {}\n    http {}\n\n    output {\n        metrics = [otelcol.exporter.debug.default.input]\n        logs    = [otelcol.exporter.debug.default.input]\n        traces  = [otelcol.exporter.debug.default.input]\n    }\n}\n\notelcol.exporter.debug \"default\" {}\n```\n\n----------------------------------------\n\nTITLE: Processing Different Resource Attributes - Input Spans\nDESCRIPTION: Example showing two incoming spans with different k8s.pod.name resource attributes but the same service.name. Each span belongs to a different pod.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.spanmetrics.md#2025-04-22_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resourceSpans\": [\n    {\n      \"resource\": {\n        \"attributes\": [\n          {\n            \"key\": \"service.name\",\n            \"value\": { \"stringValue\": \"TestSvcName\" }\n          },\n          {\n            \"key\": \"k8s.pod.name\",\n            \"value\": { \"stringValue\": \"first\" }\n          }\n        ]\n      },\n      \"scopeSpans\": [\n        {\n          \"spans\": [\n            {\n              \"trace_id\": \"7bba9f33312b3dbb8b2c2c62bb7abe2d\",\n              \"span_id\": \"086e83747d0e381e\",\n              \"name\": \"TestSpan\",\n              \"attributes\": [\n                {\n                  \"key\": \"attribute1\",\n                  \"value\": { \"intValue\": \"78\" }\n                }\n              ]\n            }\n          ]\n        }\n      ]\n    },\n    {\n      \"resource\": {\n        \"attributes\": [\n          {\n            \"key\": \"service.name\",\n            \"value\": { \"stringValue\": \"TestSvcName\" }\n          },\n          {\n            \"key\": \"k8s.pod.name\",\n            \"value\": { \"stringValue\": \"second\" }\n          }\n        ]\n      },\n      \"scopeSpans\": [\n        {\n          \"spans\": [\n            {\n              \"trace_id\": \"7bba9f33312b3dbb8b2c2c62bb7abe2d\",\n              \"span_id\": \"086e83747d0e381b\",\n              \"name\": \"TestSpan\",\n              \"attributes\": [\n                {\n                  \"key\": \"attribute1\",\n                  \"value\": { \"intValue\": \"78\" }\n                }\n              ]\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Example of Profile Filtering with pyroscope.relabel\nDESCRIPTION: A complete example showing how to implement profile filtering using pyroscope.relabel. This configuration samples approximately 50% of profile series by creating a consistent hash based on the 'env' label.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.relabel.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\npyroscope.receive_http \"default\" {\n    forward_to = [pyroscope.relabel.filter_profiles.receiver]\n\n    http {\n        listen_address = \"0.0.0.0\"\n        listen_port = 9999\n    }\n}\n\npyroscope.relabel \"filter_profiles\" {\n    forward_to = [pyroscope.write.staging.receiver]\n\n    // This creates a consistent hash value (0 or 1) for each unique combination of labels\n    // Using multiple source labels provides better sampling distribution across your profiles\n    rule {\n        source_labels = [\"env\"]\n        target_label = \"__tmp_hash\"\n        action = \"hashmod\"\n        modulus = 2\n    }\n\n    // This effectively samples ~50% of profile series\n    // The same combination of source label values will always hash to the same number,\n    // ensuring consistent sampling\n    rule {\n        source_labels = [\"__tmp_hash\"]\n        action       = \"drop\"\n        regex        = \"^1$\"\n    }\n}\n\npyroscope.write \"staging\" {\n  endpoint {\n    url = \"http://pyroscope-staging:4040\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Log Transformation to Syslog Format in Alloy\nDESCRIPTION: Demonstrates configuration for transforming OpenTelemetry logs to syslog format using the otelcol.processor.transform component. The configuration includes JSON parsing, attribute mapping, structured data handling, and priority setting based on severity levels. Assumes input from OpenTelemetry receiver in JSON format.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.syslog.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.transform \"syslog\" {\n  error_mode = \"ignore\"\n\n  log_statements {\n    context = \"log\"\n\n    statements = [\n      // Parse body as JSON and merge the resulting map with the cache map, ignoring non-json bodies.\n      // cache is a field exposed by OTTL that is a temporary storage place for complex operations.\n      `merge_maps(cache, ParseJSON(body), \"upsert\") where IsMatch(body, \"^\\\\{\")`\n\n      // Set some example syslog attributes using the values from a JSON message body\n      // If the attribute doesn't exist in cache then nothing happens.\n      `set(attributes[\"message\"], cache[\"log\"])`\n      `set(attributes[\"appname\"], cache[\"application\"])`\n      `set(attributes[\"hostname\"], cache[\"source\"])`\n\n      // To set structured data you can chain index ([]) operations.\n      `set(attributes[\"structured_data\"][\"auth@32473\"][\"user\"], attributes[\"user\"])`\n      `set(attributes[\"structured_data\"][\"auth@32473\"][\"user_host\"], cache[\"source\"])`\n      `set(attributes[\"structured_data\"][\"auth@32473\"][\"valid\"], cache[\"authenticated\"])`\n\n      // Example priority setting, using facility 1 (user messages) and default to Info\n      `set(attributes[\"priority\"], 14)`\n      `set(attributes[\"priority\"], 12) where severity_number == SEVERITY_NUMBER_WARN`\n      `set(attributes[\"priority\"], 11) where severity_number == SEVERITY_NUMBER_ERROR`\n      `set(attributes[\"priority\"], 10) where severity_number == SEVERITY_NUMBER_FATAL`\n    ]\n  }\n\n  output {\n    metrics = []\n    logs    = [otelcol.exporter.syslog.default.input]\n    traces  = []\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Grafana Cloud Authentication and Endpoint Configuration in Alloy\nDESCRIPTION: Configuration for connecting to Grafana Cloud's OTLP endpoints with authentication. This uses basic authentication with an account ID and API token to securely send data to Grafana Cloud.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-to-lgtm-stack.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.auth.basic \"default\" {\n  username = \"<ACCOUNT ID>\"\n  password = \"<API TOKEN>\"\n}\n\notelcol.exporter.otlphttp \"default\" {\n  client {\n    endpoint = \"<OTLP_ENDPOINT>\"\n    auth     = otelcol.auth.basic.default.handler\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Basic Authentication to Datadog Receiver\nDESCRIPTION: Adds basic authentication to the Datadog receiver configuration. This block should be placed inside the endpoint block when authentication is required for security.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/datadog-traces-metrics.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\nbasic_auth {\n  username = \"<USERNAME>\"\n  password = \"<PASSWORD>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Delta to Cumulative Processor in Grafana Alloy\nDESCRIPTION: Sets up the otelcol.processor.deltatocumulative component which converts delta metrics to cumulative metrics. This is necessary for compatibility with some monitoring systems that expect cumulative metrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/datadog-traces-metrics.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.deltatocumulative \"default\" {\n  max_stale = \"<MAX_STALE>\"\n  max_streams = <MAX_STREAMS>\n  output {\n    metrics = [otelcol.processor.batch.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Discovering Kubernetes Services Configuration\nDESCRIPTION: Basic configuration for discovering Kubernetes Services across all namespaces using the discovery.kubernetes component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/prometheus-metrics.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubernetes \"<DISCOVERY_LABEL>\" {\n  role = \"service\"\n}\n```\n\n----------------------------------------\n\nTITLE: Advanced K8s Attributes Processor with Additional Metadata in Alloy\nDESCRIPTION: This configuration extends the basic setup by adding additional metadata and labels. It extracts more Kubernetes-specific information and includes a custom label extraction rule using regex.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.k8sattributes.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  http {}\n  grpc {}\n\n  output {\n    metrics = [otelcol.processor.k8sattributes.default.input]\n    logs    = [otelcol.processor.k8sattributes.default.input]\n    traces  = [otelcol.processor.k8sattributes.default.input]\n  }\n}\n\notelcol.processor.k8sattributes \"default\" {\n  extract {\n    label {\n      from      = \"pod\"\n      key_regex = \"(.*)/(.*)\"\n      tag_name  = \"$1.$2\"\n    }\n\n    metadata = [\n      \"k8s.namespace.name\",\n      \"k8s.deployment.name\",\n      \"k8s.statefulset.name\",\n      \"k8s.daemonset.name\",\n      \"k8s.cronjob.name\",\n      \"k8s.job.name\",\n      \"k8s.node.name\",\n      \"k8s.pod.name\",\n      \"k8s.pod.uid\",\n      \"k8s.pod.start_time\",\n    ]\n  }\n\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = sys.env(\"OTLP_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Loki Endpoints with File Source\nDESCRIPTION: Example showing a loki.write component with multiple endpoints and mixed authentication methods, along with a loki.source.file component that collects logs from the filesystem.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/logs-in-kubernetes.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nloki.write \"default\" {\n  endpoint {\n    url = \"http://localhost:3100/loki/api/v1/push\"\n  }\n\n  endpoint {\n    url = \"https://logs-us-central1.grafana.net/loki/api/v1/push\"\n\n    // Get basic authentication based on environment variables.\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n\nloki.source.file \"example\" {\n  // Collect logs from the default listen address.\n  targets = [\n    {__path__ = \"/tmp/foo.txt\", \"color\" = \"pink\"},\n    {__path__ = \"/tmp/bar.txt\", \"color\" = \"blue\"},\n    {__path__ = \"/tmp/baz.txt\", \"color\" = \"grey\"},\n  ]\n\n  forward_to = [loki.write.default.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Example of Complete Metrics Collection Pipeline in Alloy\nDESCRIPTION: Demonstrates a full configuration for collecting and forwarding Alloy metrics to a Mimir instance using the self exporter, scrape, and remote_write components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/metamonitoring.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.self \"default\" {\n}\n\nprometheus.scrape \"metamonitoring\" {\n  targets    = prometheus.exporter.self.default.targets\n  forward_to = [prometheus.remote_write.default.receiver]\n}\n\nprometheus.remote_write \"default\" {\n  endpoint {\n    url = \"http://mimir:9009/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Using a Self-Collecting Metrics Component in Alloy\nDESCRIPTION: This example creates a custom component 'self_collect' that collects process metrics and forwards them to a user-specified output. It demonstrates the use of argument blocks, component definitions, and how to use the custom component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/declare.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndeclare \"self_collect\" {\n  argument \"metrics_output\" {\n    optional = false\n    comment  = \"Where to send collected metrics.\"\n  }\n\n  prometheus.scrape \"selfmonitor\" {\n    targets = [{\n      __address__ = \"127.0.0.1:12345\",\n    }]\n\n    forward_to = [argument.metrics_output.value]\n  }\n}\n\nself_collect \"example\" {\n  metrics_output = prometheus.remote_write.example.receiver\n}\n\nprometheus.remote_write \"example\" {\n  endpoint {\n    url = REMOTE_WRITE_URL\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Sending /tmp/logs/*.log Files to Loki\nDESCRIPTION: This example demonstrates how to use local.file_match to discover log files in /tmp/logs and send them to Loki using loki.source.file and loki.write components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/local/local.file_match.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file_match \"tmp\" {\n  path_targets = [{\"__path__\" = \"/tmp/logs/**/*.log\"}]\n}\n\nloki.source.file \"files\" {\n  targets    = local.file_match.tmp.targets\n  forward_to = [loki.write.endpoint.receiver]\n}\n\nloki.write \"endpoint\" {\n  endpoint {\n      url = <LOKI_URL>\n      basic_auth {\n          username = <USERNAME>\n          password = <PASSWORD>\n      }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring loki.source.syslog with TCP and UDP Listeners\nDESCRIPTION: Example configuration of loki.source.syslog with both TCP and UDP listeners, forwarding to a loki.write component. It demonstrates setting up multiple listeners with different protocols and labels.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.syslog.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.syslog \"local\" {\n  listener {\n    address  = \"127.0.0.1:51893\"\n    labels   = { component = \"loki.source.syslog\", protocol = \"tcp\" }\n  }\n\n  listener {\n    address  = \"127.0.0.1:51898\"\n    protocol = \"udp\"\n    labels   = { component = \"loki.source.syslog\", protocol = \"udp\"}\n  }\n\n  forward_to = [loki.write.local.receiver]\n}\n\nloki.write \"local\" {\n  endpoint {\n    url = \"loki:3100/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Azure VM Discovery Implementation\nDESCRIPTION: Complete example showing discovery.azure configuration with OAuth authentication, Prometheus scraping, and remote write setup.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.azure.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.azure \"example\" {\n  port = 80\n  subscription_id = \"<AZURE_SUBSCRIPTION_ID>\"\n  oauth {\n      client_id = \"<AZURE_CLIENT_ID>\"\n      client_secret = \"<AZURE_CLIENT_SECRET>\"\n      tenant_id = \"<AZURE_TENANT_ID>\"\n  }\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.azure.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting MetricsInstance to Grafana Alloy Components\nDESCRIPTION: Alloy configuration equivalent to a MetricsInstance resource. It sets up remote write, and discovers PodMonitors and ServiceMonitors with a specific label selector.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-operator.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nremote.kubernetes.secret \"credentials\" {\n  namespace = \"monitoring\"\n  name = \"primary-credentials-metrics\"\n}\n\nprometheus.remote_write \"primary\" {\n    endpoint {\n        url = \"https://<PROMETHEUS_URL>/api/v1/push\"\n        basic_auth {\n            username = convert.nonsensitive(remote.kubernetes.secret.credentials.data[\"username\"])\n            password = remote.kubernetes.secret.credentials.data[\"password\"]\n        }\n    }\n}\n\nprometheus.operator.podmonitors \"primary\" {\n    forward_to = [prometheus.remote_write.primary.receiver]\n    selector {\n        match_labels = {instance = \"primary\"}\n    }\n}\n\nprometheus.operator.servicemonitors \"primary\" {\n    forward_to = [prometheus.remote_write.primary.receiver]\n    selector {\n        match_labels = {instance = \"primary\"}\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Alloy Config with Jsonnet Example\nDESCRIPTION: Example showing how to use the alloy-syntax-jsonnet library to create an Alloy configuration file. The example demonstrates defining attributes, creating a labeled block, and using the expr helper function to inject literal Alloy expressions.\nSOURCE: https://github.com/grafana/alloy/blob/main/operations/alloy-syntax-jsonnet/README.md#2025-04-22_snippet_0\n\nLANGUAGE: jsonnet\nCODE:\n```\nlocal alloy = import 'github.com/grafana/alloy/operations/alloy-syntax-jsonnet/main.libsonnet';\n\nalloy.manifestAlloy({\n  attr_1: \"Hello, world!\",\n\n  [alloy.block(\"some_block\", \"foobar\")]: {\n    expr: alloy.expr('sys.env(\"HOME\")'),\n    inner_attr_1: [0, 1, 2, 3],\n    inner_attr_2: {\n      first_name: \"John\",\n      last_name: \"Smith\",\n    },\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Complete Example of loki.enrich with HTTP Discovery\nDESCRIPTION: This example shows a full configuration using loki.enrich with HTTP discovery, relabeling, and syslog input. It demonstrates how to set up discovery, configure relabeling rules, and use the enriched logs.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.enrich.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\n// Configure HTTP discovery\ndiscovery.http \"default\" {\n    url = \"http://network-inventory.example.com/prometheus_sd\"\n}\n\ndiscovery.relabel \"default\" {\n    targets = discovery.http.default.targets\n    rule {\n        action        = \"replace\"\n        source_labels = [\"__inventory_rack\"]\n        target_label  = \"rack\"\n    }\n    rule {\n        action        = \"replace\"\n        source_labels = [\"__inventory_datacenter\"]\n        target_label  = \"datacenter\"\n    }\n    rule {\n        action        = \"replace\"\n        source_labels = [\"__inventory_environment\"]\n        target_label  = \"environment\"\n    }\n    rule {\n        action        = \"replace\"\n        source_labels = [\"__inventory_tenant\"]\n        target_label  = \"tenant\"\n    }\n    rule {\n        action        = \"replace\"\n        source_labels = [\"__inventory_primary_ip\"]\n        target_label  = \"primary_ip\"\n    }\n}\n\n// Receive syslog messages\nloki.source.syslog \"incoming\" {\n    listener {\n        address = \":514\"\n        protocol = \"tcp\"\n        labels = {\n            job = \"syslog\"\n        }\n    }\n    forward_to = [loki.enrich.default.receiver]\n}\n\n// Enrich logs using HTTP discovery\nloki.enrich \"default\" {\n    // Use targets from HTTP discovery (after relabeling)\n    targets = discovery.relabel.default.output\n\n    // Match hostname from logs to DNS name\n    target_match_label = \"primary_ip\"\n\n    forward_to = [loki.write.default.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Pyroscope HTTP Receiver Setup with Multiple Forwarding\nDESCRIPTION: Advanced example showing how to configure a pyroscope.receive_http component that forwards profiles to multiple pyroscope.write components for both staging and production environments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.receive_http.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\n// Receives profiles over HTTP\npyroscope.receive_http \"default\" {\n  http {\n    listen_address = \"0.0.0.0\"\n    listen_port = 9999\n  }\n  forward_to = [pyroscope.write.staging.receiver, pyroscope.write.production.receiver]\n}\n\n// Send profiles to a staging Pyroscope instance\npyroscope.write \"staging\" {\n  endpoint {\n    url = \"http://pyroscope-staging:4040\"\n  }\n}\n\n// Send profiles to a production Pyroscope instance\npyroscope.write \"production\" {\n  endpoint {\n    url = \"http://pyroscope-production:4040\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP HTTP Exporter in Alloy\nDESCRIPTION: This snippet demonstrates how to create an exporter to send data to a locally running Grafana Tempo without TLS. It configures the client endpoint and TLS settings for insecure connections.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.otlphttp.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlphttp \"tempo\" {\n    client {\n        endpoint = \"http://tempo:4317\"\n        tls {\n            insecure             = true\n            insecure_skip_verify = true\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: ServiceMonitors with Node Filtering in Alloy\nDESCRIPTION: This example shows how to apply additional relabel rules to filter discovered targets by hostname, which can be useful when running Grafana Alloy as a DaemonSet.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.operator.servicemonitors.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.operator.servicemonitors \"services\" {\n    forward_to = [prometheus.remote_write.staging.receiver]\n    rule {\n      action = \"keep\"\n      regex = sys.env(\"HOSTNAME\")\n      source_labels = [\"__meta_kubernetes_pod_node_name\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CloudWatch Custom Namespace Scraping in Alloy\nDESCRIPTION: This example demonstrates how to configure the CloudWatch exporter to scrape metrics from a custom namespace. It shows setting up the exporter with a specific STS region and defining custom metrics with their statistics and period.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.cloudwatch.md#2025-04-22_snippet_8\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.cloudwatch \"discover_instances\" {\n    sts_region = \"eu-west-1\"\n\n    custom_namespace \"customEC2Metrics\" {\n        namespace = \"CustomEC2Metrics\"\n        regions   = [\"us-east-1\"]\n\n        metric {\n            name       = \"cpu_usage_idle\"\n            statistics = [\"Average\"]\n            period     = \"5m\"\n        }\n\n        metric {\n            name       = \"disk_free\"\n            statistics = [\"Average\"]\n            period     = \"5m\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubelet Discovery with Namespace Filtering\nDESCRIPTION: Demonstrates how to limit Kubelet Pod discovery to specific namespaces while using bearer token authentication. Includes configuration for Prometheus scraping and remote write with basic auth.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.kubelet.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubelet \"k8s_pods\" {\n  bearer_token_file = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n  namespaces = [\"default\", \"kube-system\"]\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.kubelet.k8s_pods.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"PROMETHEUS_REMOTE_WRITE_URL\"\n\n    basic_auth {\n      username = \"USERNAME\"\n      password = \"PASSWORD\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Dynamic Target Discovery Configuration\nDESCRIPTION: Example showing how to configure blackbox exporter with dynamic target discovery using discovery.file component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.blackbox.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.file \"example\" {\n  files = [\"targets.yml\"]\n}\n\nprometheus.exporter.blackbox \"example\" {\n  config = \"{ modules: { http_2xx: { prober: http, timeout: 5s } } }\"\n  targets = discovery.file.example.targets\n}\n\nprometheus.scrape \"example\" {\n  targets    = prometheus.exporter.blackbox.example.targets\n  forward_to = [prometheus.remote_write.example.receiver]\n}\n\nprometheus.remote_write \"example\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Python Equivalent of JSON Processing Stage\nDESCRIPTION: Illustrative Python code showing the equivalent operation of the JSON processing stage in the pipeline.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/processing-logs.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nextracted_map = {}\nlog_line      = {\"log\": {\"is_secret\": \"true\", \"level\": \"info\", \"message\": \"This is a secret message!\"}, \"timestamp\": \"2023-11-16T06:01:50Z\"}\n\nextracted_map[\"log\"] = log_line[\"log\"]\nextracted_map[\"ts\"]  = log_line[\"timestamp\"]\n```\n\n----------------------------------------\n\nTITLE: GeoIP Country Database Configuration in Alloy\nDESCRIPTION: Configures GeoIP processing with a country database to extract geographical information from IP addresses. Uses JSON parsing to extract client IP and adds geographical data as labels.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_8\n\nLANGUAGE: alloy\nCODE:\n```\n{\"log\":\"log message\",\"client_ip\":\"34.120.177.193\"}\n\nloki.process \"example\" {\n    stage.json {\n        expressions = {ip = \"client_ip\"}\n    }\n\n    stage.geoip {\n        source  = \"ip\"\n        db      = \"/path/to/db/GeoLite2-Country.mmdb\"\n        db_type = \"country\"\n    }\n\n    stage.labels {\n        values = {\n            geoip_country_name       = \"\",\n            geoip_country_code       = \"\",\n            geoip_continent_name     = \"\",\n            geoip_continent_code     = \"\",\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector Components for Loki Export in Alloy\nDESCRIPTION: This snippet demonstrates the configuration of various OpenTelemetry Collector components to receive OTLP data, convert spans to logs, process attributes, and export to Loki. It includes settings for OTLP receiver, span-to-log connector, attribute processor, and Loki exporter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.spanlogs.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  grpc {}\n\n  output {\n    traces = [otelcol.connector.spanlogs.default.input]\n  }\n}\n\notelcol.connector.spanlogs \"default\" {\n  spans              = true\n  roots              = true\n  processes          = true\n  events             = true\n  labels             = [\"attribute1\", \"res_attribute1\"]\n  span_attributes    = [\"attribute1\"]\n  process_attributes = [\"res_attribute1\"]\n  event_attributes   = [\"log.severity\", \"log.message\"]\n\n  output {\n    logs = [otelcol.processor.attributes.default.input]\n  }\n}\n\notelcol.processor.attributes \"default\" {\n  action {\n    key = \"loki.attribute.labels\"\n    action = \"insert\"\n    value = \"attribute1\"\n  }\n\n  output {\n    logs = [otelcol.exporter.loki.default.input]\n  }\n}\n\notelcol.exporter.loki \"default\" {\n  forward_to = [loki.write.local.receiver]\n}\n\nloki.write \"local\" {\n  endpoint {\n    url = \"loki:3100\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kuma Service Discovery in Alloy\nDESCRIPTION: Basic configuration for the discovery.kuma component, specifying the server address for the Kuma Control Plane's MADS xDS server.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.kuma.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kuma \"LABEL\" {\n    server = \"SERVER\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Pod Logs Collection in Kubernetes Monitoring Helm Chart\nDESCRIPTION: YAML configuration for enabling and configuring the collection of Pod logs from the 'meta' and 'prod' namespaces in the Kubernetes Monitoring Helm chart.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\npodLogs:\n  enabled: true\n  gatherMethod: kubernetesApi\n  collector: alloy-logs\n  labelsToKeep: [\"app_kubernetes_io_name\",\"container\",\"instance\",\"job\",\"level\",\"namespace\",\"service_name\",\"service_namespace\",\"deployment_environment\",\"deployment_environment_name\"]\n  structuredMetadata:\n    pod: pod  # Set structured metadata \"pod\" from label \"pod\"\n  namespaces:\n    - meta\n    - prod\n```\n\n----------------------------------------\n\nTITLE: Configuring Beyla eBPF with Prometheus Metrics Collection\nDESCRIPTION: Example configuration showing how to collect metrics from beyla.ebpf using prometheus.scrape and forward them to a remote write endpoint. The configuration includes authentication and required port settings.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/beyla/beyla.ebpf.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nbeyla.ebpf \"default\" {\n    open_port = <OPEN_PORT>\n}\n\nprometheus.scrape \"beyla\" {\n  targets = beyla.ebpf.default.targets\n  honor_labels = true // required to keep job and instance labels\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = <PROMETHEUS_REMOTE_WRITE_URL>\n\n    basic_auth {\n      username = <USERNAME>\n      password = <PASSWORD>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Beyla eBPF with OpenTelemetry Trace Collection\nDESCRIPTION: Example configuration demonstrating how to collect traces from beyla.ebpf and forward them to an OTLP endpoint using OpenTelemetry collectors. Includes batch processing configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/beyla/beyla.ebpf.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\nbeyla.ebpf \"default\" {\n    open_port = <OPEN_PORT>\n    output {\n        traces = [otelcol.processor.batch.default.input]\n    }\n}\notelcol.processor.batch \"default\" {\n    output {\n        traces  = [otelcol.exporter.otlp.default.input]\n    }\n}\notelcol.exporter.otlp \"default\" {\n    client {\n        endpoint = sys.env(\"<OTLP_ENDPOINT>\")\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Swarm Discovery in Alloy\nDESCRIPTION: Basic configuration for Docker Swarm service discovery that specifies the Docker daemon host and role of targets to retrieve. The role must be one of 'services', 'tasks', or 'nodes'.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.dockerswarm.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.dockerswarm \"<LABEL>\" {\n  host = \"<DOCKER_DAEMON_HOST>\"\n  role = \"<SWARM_ROLE>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring prometheus.exporter.apache with prometheus.scrape in Alloy\nDESCRIPTION: Provides an example of how to configure prometheus.exporter.apache component along with prometheus.scrape and prometheus.remote_write components to collect and forward Apache metrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.apache.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.apache \"example\" {\n  scrape_uri = \"http://web.example.com/server-status?auto\"\n}\n\n// Configure a prometheus.scrape component to collect apache metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.apache.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Joining discovery.process with discovery.kubernetes in Alloy\nDESCRIPTION: Demonstrates how to join discovered processes with Kubernetes discovery. This example combines process discovery with Kubernetes pod discovery based on the node name.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.process.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubernetes \"pyroscope_kubernetes\" {\n  selectors {\n    field = \"spec.nodeName=\" + sys.env(\"HOSTNAME\")\n    role = \"pod\"\n  }\n  role = \"pod\"\n}\n\ndiscovery.process \"all\" {\n  join = discovery.kubernetes.pyroscope_kubernetes.targets\n  refresh_interval = \"60s\"\n  discover_config {\n    cwd = true\n    exe = true\n    commandline = true\n    username = true\n    uid = true\n    container_id = true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying IAM Permissions for Database Migration Service Discovery\nDESCRIPTION: This text block lists the IAM permissions required to discover tagged Database Migration Service (DMS) replication instances and tasks using the CloudWatch exporter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.cloudwatch.md#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n\"dms:DescribeReplicationInstances\",\n\"dms:DescribeReplicationTasks\"\n```\n\n----------------------------------------\n\nTITLE: Collecting Kubernetes System Logs\nDESCRIPTION: Configuration to collect system logs from Kubernetes nodes using local.file_match to discover files and loki.source.file to read log entries, then forward them to a Loki endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/logs-in-kubernetes.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\n// local.file_match discovers files on the local filesystem using glob patterns and the doublestar library. It returns an array of file paths.\nlocal.file_match \"node_logs\" {\n  path_targets = [{\n      // Monitor syslog to scrape node-logs\n      __path__  = \"/var/log/syslog\",\n      job       = \"node/syslog\",\n      node_name = sys.env(\"HOSTNAME\"),\n      cluster   = <CLUSTER_NAME>,\n  }]\n}\n\n// loki.source.file reads log entries from files and forwards them to other loki.* components.\n// You can specify multiple loki.source.file components by giving them different labels.\nloki.source.file \"node_logs\" {\n  targets    = local.file_match.node_logs.targets\n  forward_to = [loki.write.<WRITE_COMPONENT_NAME>.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Metrics Exporter in Alloy\nDESCRIPTION: This snippet demonstrates how to configure the prometheus.exporter.azure component in Alloy. It specifies subscriptions, resource type, and metrics to be collected from Azure Monitor.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.azure.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.azure \"<LABEL>\" {\n        subscriptions = [\n                <SUB_ID_1>,\n                <SUB_ID_2>,\n                ...\n        ]\n\n        resource_type = \"<RESOURCE_TYPE>\"\n\n        metrics = [\n                \"<METRIC_1>\",\n                \"<METRIC_2>\",\n                ...\n        ]\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Compose for the Grafana Stack\nDESCRIPTION: Commands to navigate to the docker-monitoring directory and start the Docker Compose environment for deploying the Grafana stack.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-docker-containers.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd alloy-scenarios/docker-monitoring\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Source File for Log Reading in Alloy\nDESCRIPTION: This snippet sets up the loki.source.file component to read log entries from files specified by the local.file_match component and forward them to the Loki write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-linux.md#2025-04-22_snippet_9\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.file \"logs_integrations_integrations_node_exporter_direct_scrape\" {\n  targets    = local.file_match.logs_integrations_integrations_node_exporter_direct_scrape.targets\n  forward_to = [loki.write.local.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Conditional Processing with stage.match in Alloy\nDESCRIPTION: This example demonstrates using stage.match to conditionally process log lines based on LogQL selectors. It shows how to apply different processing to different log lines, and how to drop noisy errors based on content matching.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_17\n\nLANGUAGE: alloy\nCODE:\n```\n{ \"time\":\"2023-01-18T17:08:41+00:00\", \"app\":\"example1\", \"component\": [\"parser\",\"type\"], \"level\" : \"WARN\", \"message\" : \"app1 log line\" }\n{ \"time\":\"2023-01-18T17:08:42+00:00\", \"app\":\"example2\", \"component\": [\"parser\",\"type\"], \"level\" : \"ERROR\", \"message\" : \"example noisy error\" }\n\nstage.json {\n    expressions = { \"appname\" = \"app\" }\n}\n\nstage.labels {\n    values = { \"applbl\" = \"appname\" }\n}\n\nstage.match {\n    selector = \"{applbl=\\\"examplelabel\\\"}\"\n\n    stage.json {\n        expressions = { \"msg\" = \"message\" }\n    }\n}\n\nstage.match {\n    selector = \"{applbl=\\\"qux\\\"}\"\n    stage.json {\n        expressions = { \"msg\" = \"msg\" }\n    }\n}\n\nstage.match {\n    selector = \"{applbl=\\\"bar\\\"} |~ \\\".*noisy error.*\\\"\"\n    action   = \"drop\"\n\n    drop_counter_reason = \"discard_noisy_errors\"\n}\n\nstage.output {\n    source = \"msg\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing otelcol.receiver.loki with Loki and OTLP in Alloy\nDESCRIPTION: Example showing how to use otelcol.receiver.loki as a bridge between Loki and OpenTelemetry ecosystems. It configures a Loki file source, converts logs to OTLP format, and sends them to an OTLP-capable endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.loki.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.file \"default\" {\n  targets = [\n    {__path__ = \"/tmp/foo.txt\", \"loki.format\" = \"logfmt\"},\n    {__path__ = \"/tmp/bar.txt\", \"loki.format\" = \"json\"},\n  ]\n  forward_to = [otelcol.receiver.loki.default.receiver]\n}\n\notelcol.receiver.loki \"default\" {\n  output {\n    logs = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = sys.env(\"OTLP_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example loki.source.windowsevent and loki.write Configuration\nDESCRIPTION: This example demonstrates how to configure loki.source.windowsevent to collect log entries from the Application Event Log and forward them to a loki.write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.windowsevent.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.windowsevent \"application\"  {\n    eventlog_name = \"Application\"\n    forward_to = [loki.write.endpoint.receiver]\n}\n\nloki.write \"endpoint\" {\n    endpoint {\n        url =\"loki:3100/api/v1/push\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic otelcol.exporter.loki Configuration\nDESCRIPTION: Demonstrates the basic syntax for configuring a Loki exporter component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.loki.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.loki \"LABEL\" {\n  forward_to = [...]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector Components in Alloy\nDESCRIPTION: This snippet demonstrates how to configure various OpenTelemetry Collector components including a receiver, attributes processor, and exporter. It showcases different uses of the 'action' block within the attributes processor for inserting, updating, copying, hashing, extracting, converting, and deleting attributes.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.attributes.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  http {}\n  grpc {}\n\n  output {\n    metrics = [otelcol.processor.attributes.default.input]\n    logs    = [otelcol.processor.attributes.default.input]\n    traces  = [otelcol.processor.attributes.default.input]\n  }\n}\n\notelcol.processor.attributes \"default\" {\n    // Inserts a new attribute \"attribute1\" to spans where\n    // the key \"attribute1\" doesn't exist.\n    // The type of `attribute1` is inferred by the configuration.\n    // `123` is an integer and is stored as an integer in the attributes.\n    action {\n        key = \"attribute1\"\n        value = 123\n        action = \"insert\"\n    }\n\n    // Inserts a new attribute with a key of \"string key\" and\n    // a string value of \"anotherkey\".\n    action {\n        key = \"string key\"\n        value = \"anotherkey\"\n        action = \"insert\"\n    }\n\n    // Setting an attribute on all spans.\n    // Any spans that already had `region` now have value `planet-earth`.\n    // This can be done to set properties for all traces without\n    // requiring an instrumentation change.\n    action {\n        key = \"region\"\n        value = \"planet-earth\"\n        action = \"upsert\"\n    }\n\n    // The following demonstrates copying a value to a new key.\n    // If a span doesn't contain `user_key`, no new attribute `new_user_key` is created.\n    action {\n        key = \"new_user_key\"\n        from_attribute = \"user_key\"\n        action = \"upsert\"\n    }\n\n    // Hashing existing attribute values.\n    action {\n        key = \"user.email\"\n        action = \"hash\"\n    }\n\n    // Uses the value from key `example_user_key` to upsert attributes\n    // to the target keys specified in the `pattern`.\n    // (Insert attributes for target keys that do not exist and update keys that exist.)\n    // Given example_user_key = /api/v1/document/12345678/update/v1\n    // then the following attributes will be inserted:\n    // new_example_user_key: 12345678\n    // version: v1\n    //\n    // Note: Similar to the Span Processor, if a target key already exists,\n    // it will be updated.\n    //\n    // Note: The regex pattern is enclosed in backticks instead of quotation marks.\n    // This constitutes a raw {{< param \"PRODUCT_NAME\" >}} syntax string, and lets us avoid the need to escape backslash characters.\n    action {\n        key = \"example_user_key\"\n        pattern = `\\/api\\/v1\\/document\\/(?P<new_user_key>.*)\\/update\\/(?P<version>.*)$`\n        action = \"extract\"\n    }\n\n    // Converting the type of an existing attribute value.\n    action {\n        key = \"http.status_code\"\n        converted_type = \"int\"\n        action = \"convert\"\n    }\n\n    // Deleting keys from an attribute.\n    action {\n        key = \"credit_card\"\n        action = \"delete\"\n    }\n\n    output {\n        metrics = [otelcol.exporter.otlp.default.input]\n        logs    = [otelcol.exporter.otlp.default.input]\n        traces  = [otelcol.exporter.otlp.default.input]\n    }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = sys.env(\"OTLP_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Processing Spans by Attribute Pattern in Alloy\nDESCRIPTION: Configures an attribute processor that targets spans with db.statement attributes matching a specific regex pattern. It replaces the matched statements with an obfuscated version, targeting all OpenTelemetry signal types.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.attributes.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.attributes \"default\" {\n    include {\n        // \"match_type\" of \"regexp\" defines that the \"value\" attributes \n        // in the \"attribute\" blocks are regexp-es.\n        match_type = \"regexp\"\n\n        // This attribute ('db.statement') must exist in the span and match \n        // the regex ('SELECT \\* FROM USERS.*') for a match.\n        attribute {\n            key = \"db.statement\"\n            value = \"SELECT \\* FROM USERS.*\"\n        }\n    }\n\n    action {\n        key = \"db.statement\"\n        action = \"update\"\n        value = \"SELECT * FROM USERS [obfuscated]\"\n    }\n\n    output {\n        metrics = [otelcol.exporter.otlp.default.input]\n        logs    = [otelcol.exporter.otlp.default.input]\n        traces  = [otelcol.exporter.otlp.default.input]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Integration Example with OTLP Exporter\nDESCRIPTION: Example showing how to configure the OTLP exporter with basic authentication using environment variables for credentials.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.auth.basic.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlp \"example\" {\n  client {\n    endpoint = \"my-otlp-grpc-server:4317\"\n    auth     = otelcol.auth.basic.creds.handler\n  }\n}\n\notelcol.auth.basic \"creds\" {\n  username = \"demo\"\n  password = sys.env(\"API_KEY\")\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Processing and Output Configuration\nDESCRIPTION: Three-stage pipeline demonstrating JSON parsing, label extraction, and output modification. Shows how to process JSON logs and modify their final output format.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_26\n\nLANGUAGE: alloy\nCODE:\n```\n{\"user\": \"John Doe\", \"message\": \"hello, world!\"}\n\nstage.json {\n    expressions = { \"user\" = \"user\", \"message\" = \"message\" }\n}\n\nstage.labels {\n    values = { \"user\" = \"user\" }\n}\n\nstage.output {\n    source = \"message\"\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Output Log from otelcol.connector.spanlogs in JSON\nDESCRIPTION: This JSON snippet shows the expected output log structure after processing by the otelcol.connector.spanlogs component. It includes resource logs, scope logs, and log records with various attributes derived from the input trace.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.spanlogs.md#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resourceLogs\": [\n    {\n      \"scopeLogs\": [\n        {\n          \"log_records\": [\n            {\n              \"body\": {\n                \"stringValue\": \"span=TestSpan dur=0ns attribute1=78 svc=TestSvcName res_attribute1=78 tid=7bba9f33312b3dbb8b2c2c62bb7abe2d\"\n              },\n              \"attributes\": [\n                {\n                  \"key\": \"traces\",\n                  \"value\": { \"stringValue\": \"span\" }\n                },\n                {\n                  \"key\": \"attribute1\",\n                  \"value\": { \"intValue\": \"78\" }\n                },\n                {\n                  \"key\": \"res_attribute1\",\n                  \"value\": { \"intValue\": \"78\" }\n                }\n              ]\n            },\n            {\n              \"body\": {\n                \"stringValue\": \"span=TestSpan dur=0ns attribute1=78 svc=TestSvcName res_attribute1=78 tid=7bba9f33312b3dbb8b2c2c62bb7abe2d\"\n              },\n              \"attributes\": [\n                {\n                  \"key\": \"traces\",\n                  \"value\": { \"stringValue\": \"root\" }\n                },\n                {\n                  \"key\": \"attribute1\",\n                  \"value\": { \"intValue\": \"78\" }\n                },\n                {\n                  \"key\": \"res_attribute1\",\n                  \"value\": { \"intValue\": \"78\" }\n                }\n              ]\n            },\n            {\n              \"body\": {\n                \"stringValue\": \"svc=TestSvcName res_attribute1=78 tid=7bba9f33312b3dbb8b2c2c62bb7abe2d\"\n              },\n              \"attributes\": [\n                {\n                  \"key\": \"traces\",\n                  \"value\": { \"stringValue\": \"process\" }\n                },\n                {\n                  \"key\": \"res_attribute1\",\n                  \"value\": { \"intValue\": \"78\" }\n                }\n              ]\n            },\n            {\n              \"body\": { \"stringValue\": \"span=TestSpan dur=0ns attribute1=78 svc=TestSvcName res_attribute1=78 tid=7bba9f33312b3dbb8b2c2c62bb7abe2d log.severity=INFO log.message=TestLogMessage\" },\n              \"attributes\": [\n                {\n                  \"key\": \"traces\",\n                  \"value\": { \"stringValue\": \"event\" }\n                },\n                {\n                  \"key\": \"attribute1\",\n                  \"value\": { \"intValue\": \"78\" }\n                },\n                {\n                  \"key\": \"res_attribute1\",\n                  \"value\": { \"intValue\": \"78\" }\n                },\n                {\n                  \"key\": \"log.severity\",\n                  \"value\": { \"stringValue\": \"INFO\" }\n                },\n                {\n                  \"key\": \"log.message\",\n                  \"value\": { \"stringValue\": \"TestLogMessage\" }\n                }\n              ]\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows Exporter in Alloy\nDESCRIPTION: Configure the prometheus.exporter.windows component to expose Windows metrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-windows.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.windows \"default\" {\n  enabled_collectors = [\"cpu\",\"cs\",\"logical_disk\",\"net\",\"os\",\"service\",\"system\", \"memory\", \"scheduled_task\", \"tcp\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Label Drop Stage Configuration in Alloy\nDESCRIPTION: Configures a stage to remove specific labels from log entries. Useful for filtering out unnecessary label information.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_11\n\nLANGUAGE: alloy\nCODE:\n```\nstage.label_drop {\n    values = [ \"kubernetes_node_name\", \"kubernetes_namespace\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Complete PuppetDB Discovery Example with Prometheus Integration\nDESCRIPTION: Example showing how to discover targets from PuppetDB for servers with a specific package, configure scraping, and forward metrics to a Prometheus remote write endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.puppetdb.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.puppetdb \"example\" {\n    url   = \"http://puppetdb.local:8080\"\n    query = \"resources { type = \\\"Package\\\" and title = \\\"node_exporter\\\" }\"\n    port  = 9100\n}\n\nprometheus.scrape \"demo\" {\n    targets    = discovery.puppetdb.example.targets\n    forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n    endpoint {\n        url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n        basic_auth {\n            username = \"<USERNAME>\"\n            password = \"<PASSWORD>\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Write for Managed Service in Alloy\nDESCRIPTION: Example configuration for sending log entries to a managed Loki service, such as Grafana Cloud. This setup includes basic authentication using environment variables for the Loki username and Grafana Cloud API key.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.write.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nloki.write \"default\" {\n    endpoint {\n        url = \"https://logs-xxx.grafana.net/loki/api/v1/push\"\n        basic_auth {\n            username = sys.env(\"LOKI_USERNAME\")\n            password = sys.env(\"GRAFANA_CLOUD_API_KEY\")\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing HTTP-based Service Discovery with Custom Refresh Interval\nDESCRIPTION: Example of using discovery.http in Alloy with a custom refresh interval, demonstrating how to query a URL every 15 seconds for dynamic targets.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.http.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.http \"dynamic_targets\" {\n  url = \"https://example.com/scrape_targets\"\n  refresh_interval = \"15s\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring vCenter OpenTelemetry Receiver with Batch Processing\nDESCRIPTION: Example configuration showing how to set up a vCenter receiver that forwards telemetry data through a batch processor to an OTLP endpoint. Demonstrates receiver setup with authentication, batch processing configuration, and OTLP export configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.vcenter.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.vcenter \"default\" {\n  endpoint = \"http://localhost:15672\"\n  username = \"otelu\"\n  password = \"password\"\n\n  output {\n    metrics = [otelcol.processor.batch.default.input]\n  }\n}\n\notelcol.processor.batch \"default\" {\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = sys.env(\"OTLP_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Example of prometheus.exporter.catchpoint with Scraping and Remote Write\nDESCRIPTION: This example demonstrates how to configure prometheus.exporter.catchpoint along with prometheus.scrape for collecting metrics and prometheus.remote_write for sending metrics to a remote endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.catchpoint.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.catchpoint \"example\" {\n  port             = \"9090\"\n  verbose_logging  = false\n  webhook_path     = \"/catchpoint-webhook\"\n}\n\n// Configure a prometheus.scrape component to collect catchpoint metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.catchpoint.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = <PROMETHEUS_REMOTE_WRITE_URL>\n\n    basic_auth {\n      username = <USERNAME>\n      password = <PASSWORD>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Kubernetes Pod Logs to Loki\nDESCRIPTION: This example shows how to use local.file_match in combination with discovery.kubernetes and discovery.relabel to find and monitor all logs on Kubernetes Pods, then send them to Loki.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/local/local.file_match.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubernetes \"k8s\" {\n  role = \"pod\"\n}\n\ndiscovery.relabel \"k8s\" {\n  targets = discovery.kubernetes.k8s.targets\n\n  rule {\n    source_labels = [\"__meta_kubernetes_namespace\", \"__meta_kubernetes_pod_label_name\"]\n    target_label  = \"job\"\n    separator     = \"/\"\n  }\n\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_uid\", \"__meta_kubernetes_pod_container_name\"]\n    target_label  = \"__path__\"\n    separator     = \"/\"\n    replacement   = \"/var/log/pods/*$1/*.log\"\n  }\n}\n\nlocal.file_match \"pods\" {\n  path_targets = discovery.relabel.k8s.output\n}\n\nloki.source.file \"pods\" {\n  targets = local.file_match.pods.targets\n  forward_to = [loki.write.endpoint.receiver]\n}\n\nloki.write \"endpoint\" {\n  endpoint {\n      url = <LOKI_URL>\n      basic_auth {\n          username = <USERNAME>\n          password = <PASSWORD>\n      }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Pyroscope Write Implementation\nDESCRIPTION: Extended example showing pyroscope.write configuration with multiple endpoints and integration with pyroscope.scrape.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.write.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\npyroscope.write \"staging\" {\n  // Send metrics to a locally running Pyroscope instance.\n  endpoint {\n    url = \"http://pyroscope:4040\"\n    headers = {\n      \"X-Scope-OrgID\" = \"squad-1\",\n    }\n  }\n  external_labels = {\n    \"env\" = \"staging\",\n  }\n}\n\npyroscope.scrape \"default\" {\n  targets = [\n    {\"__address__\" = \"pyroscope:4040\", \"service_name\"=\"pyroscope\"},\n    {\"__address__\" = \"alloy:12345\", \"service_name\"=\"alloy\"},\n  ]\n  forward_to = [pyroscope.write.staging.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Replace Stage Configuration\nDESCRIPTION: Shows how to use the replace stage to mask sensitive information in log lines using regex pattern matching.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_30\n\nLANGUAGE: alloy\nCODE:\n```\nstage.replace {\n    expression = \"password (\\\\S+)\"\n    replace    = \"*****\"\n}\n```\n\n----------------------------------------\n\nTITLE: Complete remote.s3 Configuration Example\nDESCRIPTION: Complete example demonstrating how to configure remote.s3 to access a specific file from an S3 bucket. This example shows the minimal required configuration with a specific bucket and file path.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/remote/remote.s3.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nremote.s3 \"data\" {\n  path = \"s3://test-bucket/file.txt\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring prometheus.exporter.consul with prometheus.scrape in Alloy\nDESCRIPTION: Demonstrates a complete example of configuring prometheus.exporter.consul along with prometheus.scrape and prometheus.remote_write components. This setup collects Consul metrics and forwards them to a Prometheus-compatible remote write endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.consul.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.consul \"example\" {\n  server = \"https://consul.example.com:8500\"\n}\n\n// Configure a prometheus.scrape component to collect consul metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.consul.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring loki.source.gelf with Relabeling and Forwarding in Alloy\nDESCRIPTION: Example configuration for loki.source.gelf component with relabeling rules and forwarding to a Loki write endpoint. It demonstrates how to use loki.relabel for custom labeling and how to set up the GELF source with relabeling and forwarding.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.gelf.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.relabel \"gelf\" {\n  rule {\n    source_labels = [\"__gelf_message_host\"]\n    target_label  = \"host\"\n  }\n}\n\nloki.source.gelf \"listen\"  {\n  forward_to    = [loki.write.endpoint.receiver]\n  relabel_rules = loki.relabel.gelf.rules\n}\n\nloki.write \"endpoint\" {\n  endpoint {\n    url =\"loki:3100/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring loki.secretfilter with Custom Settings\nDESCRIPTION: An example configuration for loki.secretfilter that demonstrates how to use it to redact secrets from log lines before forwarding them to a Loki receiver. It includes custom settings for redaction string and file matching.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.secretfilter.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file_match \"local_logs\" {\n    path_targets = \"<PATH_TARGETS>\"\n}\n\nloki.source.file \"local_logs\" {\n    targets    = local.file_match.local_logs.targets\n    forward_to = [loki.secretfilter.secret_filter.receiver]\n}\n\nloki.secretfilter \"secret_filter\" {\n    forward_to  = [loki.write.local_loki.receiver]\n    redact_with = \"<ALLOY-REDACTED-SECRET:$SECRET_NAME:$SECRET_HASH>\"\n}\n\nloki.write \"local_loki\" {\n    endpoint {\n        url = \"<LOKI_ENDPOINT>\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Consul Agent Discovery and Prometheus Scraping Configuration\nDESCRIPTION: This example demonstrates how to configure Consul Agent discovery for specific services, set up Prometheus scraping for the discovered targets, and send the scraped metrics to a Prometheus remote write endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.consulagent.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.consulagent \"example\" {\n  server = \"localhost:8500\"\n  services = [\n    \"service1\",\n    \"service2\",\n  ]\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.consul.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Elasticsearch Exporter in Alloy\nDESCRIPTION: Basic configuration for the prometheus.exporter.elasticsearch component, specifying the Elasticsearch server address.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.elasticsearch.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.elasticsearch \"<LABEL>\" {\n    address = \"<ELASTICSEARCH_ADDRESS>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Server with TLS and Basic Authentication in Alloy\nDESCRIPTION: Example of an http block configuration that sets up TLS certificates from environment variables and configures basic authentication with path filtering. This demonstrates securing the HTTP server with encryption and access control.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/http.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nhttp {\n  tls {\n    cert_file = sys.env(\"TLS_CERT_FILE_PATH\")\n    key_file  = sys.env(\"TLS_KEY_FILE_PATH\")\n  }\n\n  auth {\n    basic {\n      username = sys.env(\"BASIC_AUTH_USERNAME\")\n      password = sys.env(\"BASIC_AUTH_PASSWORD\")\n    }\n\n    filter {\n      paths                       = [\"/\"]\n      authenticate_matching_paths = true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DigitalOcean Droplet Discovery in Alloy\nDESCRIPTION: Basic configuration for discovering DigitalOcean Droplets using either a bearer token or a file containing the bearer token. This component discovers Droplets and exposes them as targets.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.digitalocean.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.digitalocean \"<LABEL>\" {\n    // Use one of:\n    // bearer_token      = \"<BEARER_TOKEN>\"\n    // bearer_token_file = \"<PATH_TO_BEARER_TOKEN_FILE>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Event Log Message Processing with JSON Stage\nDESCRIPTION: Example of combining JSON and eventlogmessage stages to process Windows Event Log messages with custom field extraction and overwriting.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\nstage.json {\n    expressions = {\n        message = \"\",\n        Overwritten = \"\",\n    }\n}\n\nstage.eventlogmessage {\n    source = \"message\"\n    overwrite_existing = true\n}\n```\n\nLANGUAGE: text\nCODE:\n```\n{\"event_id\": 1, \"Overwritten\": \"old\", \"message\": \"Message type:\\r\\nOverwritten: new\\r\\nImage: C:\\\\Users\\\\User\\\\alloy.exe\"}\n```\n\n----------------------------------------\n\nTITLE: Advanced JSON and Replace Processing\nDESCRIPTION: Demonstrates combining JSON parsing with replace operations to redact URLs from log messages.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_31\n\nLANGUAGE: alloy\nCODE:\n```\nstage.json {\n    expressions = { \"level\" = \"\", \"msg\" = \"\" }\n}\n\nstage.replace {\n    expression = \"\\\\S+ - \\\"POST (\\\\S+) .*\"\n    source     = \"msg\"\n    replace    = \"redacted_url\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Scrape Component in Alloy\nDESCRIPTION: Sets up a prometheus.scrape component to collect metrics from the self exporter and forward them to specified receivers. Requires targets from the exporter and a list of components to receive the metrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/metamonitoring.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"<SCRAPE_LABEL>\" {\n  targets    = prometheus.exporter.self.<SELF_LABEL>.targets\n  forward_to = [<METRICS_RECEIVER_LIST>]\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Grafana Helm Chart Repository for Kubernetes\nDESCRIPTION: This command adds the Grafana Helm chart repository to your local Helm configuration, which is required to install Grafana Alloy.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/kubernetes.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nhelm repo add grafana https://grafana.github.io/helm-charts\n```\n\n----------------------------------------\n\nTITLE: Basic OTLP to Loki Export Pipeline\nDESCRIPTION: Shows how to set up a complete pipeline that receives OTLP logs over gRPC, converts them to Loki format, and forwards them to a Loki write endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.loki.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  grpc {}\n\n  output {\n    logs = [otelcol.exporter.loki.default.input]\n  }\n}\n\notelcol.exporter.loki \"default\" {\n  forward_to = [loki.write.local.receiver]\n}\n\nloki.write \"local\" {\n  endpoint {\n    url = \"loki:3100\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Components in Alloy\nDESCRIPTION: This snippet demonstrates how to configure an example OpenTelemetry processor in Alloy, along with a file log receiver and a debug exporter. It showcases the Alloy configuration syntax for setting up a telemetry pipeline.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/add-otel-component.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.filelog \"default\" {\n    include = [\"/var/log/syslog\"]\n    output {\n        logs = [otelcol.processor.example.default.input]\n    }\n}\n\notelcol.processor.example \"default\" {\n    attribute = \"test\"\n    value = \"example.com\"\n    affect_traces = false\n\n    output {\n        logs = [otelcol.exporter.debug.default.input]\n        metrics = [otelcol.exporter.debug.default.input]\n        traces = [otelcol.exporter.debug.default.input]\n    }\n}\n\notelcol.exporter.debug \"default\" {}\n```\n\n----------------------------------------\n\nTITLE: Complete Example of loki.source.kafka Configuration\nDESCRIPTION: This example demonstrates a full configuration of loki.source.kafka, including relabeling rules and forwarding to a loki.write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.kafka.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.kafka \"local\" {\n  brokers                = [\"localhost:9092\"]\n  topics                 = [\"quickstart-events\"]\n  labels                 = {component = \"loki.source.kafka\"}\n  forward_to             = [loki.write.local.receiver]\n  use_incoming_timestamp = true\n  relabel_rules          = loki.relabel.kafka.rules\n}\n\nloki.relabel \"kafka\" {\n  forward_to      = [loki.write.local.receiver]\n\n  rule {\n    source_labels = [\"__meta_kafka_topic\"]\n    target_label  = \"topic\"\n  }\n}\n\nloki.write \"local\" {\n  endpoint {\n    url = \"loki:3100/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Serving Jaeger Remote Sampling from File in Alloy\nDESCRIPTION: This example configures the Jaeger remote sampling extension to load a local JSON document and serve it over the default HTTP port of 5778. It uses a file source with a specified reload interval.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.extension.jaeger_remote_sampling.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.extension.jaeger_remote_sampling \"example\" {\n  http {\n  }\n  source {\n    file             = \"/path/to/jaeger-sampling.json\"\n    reload_interval  = \"10s\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: RBAC Configuration for Kubernetes Authorization\nDESCRIPTION: This YAML configuration sets up RBAC (Role-Based Access Control) for Kubernetes. It creates a ServiceAccount, ClusterRole, and ClusterRoleBinding to authorize the product to query the Kubernetes REST API for namespaces and PrometheusRules resources.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.rules.kubernetes.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: alloy\n  namespace: default\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: alloy\nrules:\n- apiGroups: [\"\"]\n  resources: [\"namespaces\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"monitoring.coreos.com\"]\n  resources: [\"prometheusrules\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: alloy\nsubjects:\n- kind: ServiceAccount\n  name: alloy\n  namespace: default\nroleRef:\n  kind: ClusterRole\n  name: alloy\n  apiGroup: rbac.authorization.k8s.io\n```\n\n----------------------------------------\n\nTITLE: Enabling Clustering for Prometheus Scrape Components in Alloy\nDESCRIPTION: This configuration enables clustering for Prometheus scrape components, allowing multiple Alloy instances to dynamically distribute their scrape load for high-availability. Add this block to all prometheus.scrape components that should use auto-distribution.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/clustering/distribute-prometheus-scrape-load.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nclustering {\n  enabled = true\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Metrics to Mimir with Tenant Specification in Alloy\nDESCRIPTION: This example shows how to configure a prometheus.remote_write component to send metrics to a specific tenant within a Mimir instance by using custom headers. This is useful in multi-tenant Mimir deployments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.remote_write.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.remote_write \"staging\" {\n  // Send metrics to a Mimir instance\n  endpoint {\n    url = \"http://mimir:9009/api/v1/push\"\n\n    headers = {\n      \"X-Scope-OrgID\" = \"staging\",\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CloudWatch Metric Discovery for EC2 in Alloy\nDESCRIPTION: This snippet demonstrates how to configure the CloudWatch exporter to automatically discover and scrape CPU utilization and network traffic metrics from all EC2 instances in a specified region. It uses the 'discovery' block to define the AWS service, region, and specific metrics to collect.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.cloudwatch.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.cloudwatch \"discover_instances\" {\n    sts_region = \"us-east-2\"\n\n    discovery {\n        type    = \"AWS/EC2\"\n        regions = [\"us-east-2\"]\n\n        metric {\n            name       = \"CPUUtilization\"\n            statistics = [\"Average\"]\n            period     = \"5m\"\n        }\n\n        metric {\n            name       = \"NetworkPacketsIn\"\n            statistics = [\"Average\"]\n            period     = \"5m\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Eureka Discovery and Prometheus Scraping Example\nDESCRIPTION: Example configuration showing Eureka discovery, Prometheus scraping, and remote write setup. Demonstrates how to use discovered targets for scraping and sending metrics to a remote Prometheus-compatible endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.eureka.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.eureka \"example\" {\n    server = \"https://eureka.example.com/eureka/v1\"\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.eureka.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Processing Logs by Severity Level in Alloy\nDESCRIPTION: Configures an attribute processor that filters logs based on severity level. It targets logs with severity equal to or higher than INFO, including those with undefined severity. It obfuscates password attributes and removes token attributes for matching logs.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.attributes.md#2025-04-22_snippet_8\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.attributes \"default\" {\n    include {\n        match_type = \"regexp\"\n\t\tlog_severity {\n\t\t\tmin = \"INFO\"\n\t\t\tmatch_undefined = true\n\t\t}\n    }\n    action {\n        key = \"password\"\n        action = \"update\"\n        value = \"obfuscated\"\n    }\n    action {\n        key = \"token\"\n        action = \"delete\"\n    }\n\n    output {\n        logs    = [otelcol.exporter.otlp.default.input]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring prometheus.exporter.process with prometheus.scrape in Alloy\nDESCRIPTION: Shows a complete example of configuring prometheus.exporter.process along with prometheus.scrape and prometheus.remote_write components in Alloy.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.process.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.process \"example\" {\n  track_children = false\n\n  matcher {\n    comm = [\"alloy\"]\n  }\n}\n\n// Configure a prometheus.scrape component to collect process_exporter metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.process.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Forwarding Prometheus Metrics to Splunk HEC\nDESCRIPTION: This example shows how to forward Prometheus metrics from the product through a receiver for conversion to OpenTelemetry format before sending them to Splunk HEC. It includes configuration for Prometheus exporter, scraper, and the Splunk HEC exporter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.splunkhec.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.self \"default\" {\n}\n\nprometheus.scrape \"metamonitoring\" {\n  targets    = prometheus.exporter.self.default.targets\n  forward_to = [otelcol.receiver.prometheus.default.receiver]\n}\n\notelcol.receiver.prometheus \"default\" {\n  output {\n    metrics = [otelcol.exporter.splunkhec.default.input]\n  }\n}\n\n\notelcol.exporter.splunkhec \"default\" {\n    splunk {\n        token = \"SPLUNK_TOKEN\"\n    }\n    client {\n        endpoint = \"http://splunkhec.domain.com:8088\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring prometheus.exporter.memcached with Prometheus Scrape and Remote Write\nDESCRIPTION: Provides a comprehensive example of configuring a prometheus.exporter.memcached component to collect metrics from a local Memcached server, scrape those metrics using prometheus.scrape, and send them to a remote Prometheus server using prometheus.remote_write.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.memcached.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.memcached \"example\" {\n  address = \"localhost:13321\"\n  timeout = \"5s\"\n}\n\nprometheus.scrape \"example\" {\n  targets    = [prometheus.exporter.memcached.example.targets]\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example of Complete Traces Collection Pipeline in Alloy\nDESCRIPTION: Demonstrates a full configuration for collecting and forwarding Alloy traces to a Tempo instance via OpenTelemetry, with a sampling fraction of 0.1 (10% of traces).\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/metamonitoring.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\ntracing {\n  sampling_fraction = 0.1\n  write_to          = [otelcol.exporter.otlp.default.input]\n}\n\notelcol.exporter.otlp \"default\" {\n    client {\n        endpoint = \"tempo:4317\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Labels to Logs and Filtering in Alloy\nDESCRIPTION: This solution demonstrates how to add an 'os' label to logs using loki.relabel component. It extends the previous log collection example by adding a relabeling step to include the OS information before sending logs to Loki.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/logs-and-relabeling-basics.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file_match \"tmplogs\" {\n    path_targets = [{\"__path__\" = \"/tmp/alloy-logs/*.log\"}]\n}\n\nloki.source.file \"local_files\" {\n    targets    = local.file_match.tmplogs.targets\n    forward_to = [loki.relabel.add_static_label.receiver]\n}\n\nloki.relabel \"add_static_label\" {\n    forward_to = [loki.write.local_loki.receiver]\n\n    rule {\n        target_label = \"os\"\n        replacement  = constants.os\n    }\n}\n\nloki.write \"local_loki\" {\n    endpoint {\n        url = \"http://localhost:3100/loki/api/v1/push\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Google Cloud Exporter in Alloy\nDESCRIPTION: Basic configuration template for setting up a Google Cloud exporter component. The exporter can be customized with different labels to create multiple instances.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.googlecloud.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.googlecloud \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Regex Pattern Matching Basic Example\nDESCRIPTION: Shows how to parse log lines using regular expressions with named capture groups to extract timestamp, stream, flags and content.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_28\n\nLANGUAGE: alloy\nCODE:\n```\nstage.regex {\n    expression = \"^(?s)(?P<time>\\\\S+?) (?P<stream>stdout|stderr) (?P<flags>\\\\S+?) (?P<content>.*)$\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SNMP Exporter with Target in Alloy\nDESCRIPTION: This snippet demonstrates how to configure the prometheus.exporter.snmp component with a specific target. It includes setting the config file and defining a target with its address.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.snmp.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.snmp \"<LABEL>\" {\n  config_file = \"<SNMP_CONFIG_FILE_PATH>\"\n\n  target \"<TARGET_NAME>\" {\n    address = \"<TARGET_ADDRESS>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Configuration Block in Alloy\nDESCRIPTION: Example configuration showing how to set up remote configuration fetching with basic authentication, custom ID, attributes, and polling frequency. This demonstrates the core functionality of the remotecfg block.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/remotecfg.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nremotecfg {\n    url = \"SERVICE_URL\"\n    basic_auth {\n        username      = \"USERNAME\"\n        password_file = \"PASSWORD_FILE\"\n    }\n\n    id             = constants.hostname\n    attributes     = {\"cluster\" = \"dev\", \"namespace\" = \"otlp-dev\"}\n    poll_frequency = \"5m\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic OpenTelemetry Processor Discovery Configuration\nDESCRIPTION: Basic configuration structure for the otelcol.processor.discovery component showing required blocks and arguments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.discovery.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.discovery \"LABEL\" {\n  targets = [...]\n  output {\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Discovery Usage Example\nDESCRIPTION: Example showing how to use otelcol.processor.discovery with an HTTP discovery component to dynamically add attributes to traces.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.discovery.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.http \"dynamic_targets\" {\n    url              = \"https://example.com/scrape_targets\"\n    refresh_interval = \"15s\"\n}\n\notelcol.processor.discovery \"default\" {\n    targets = discovery.http.dynamic_targets.targets\n\n    output {\n        traces = [otelcol.exporter.otlp.default.input]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Receiver with Basic Authentication in Alloy\nDESCRIPTION: This snippet demonstrates how to set up an OTLP receiver with basic authentication for both HTTP and gRPC protocols. It also shows the configuration for the basic authentication credentials using environment variables.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.otlp.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  http {\n    auth = otelcol.auth.basic.creds.handler\n  }\n  grpc {\n     auth = otelcol.auth.basic.creds.handler\n  }\n\n  output {\n   ...\n  }\n}\n\notelcol.auth.basic \"creds\" {\n    username = sys.env(\"USERNAME\")\n    password = sys.env(\"PASSWORD\")\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Prometheus Operator Probes Setup in Alloy\nDESCRIPTION: A basic example that discovers all Probes in the Kubernetes cluster and forwards collected metrics to a prometheus.remote_write component. The remote_write component is configured to send metrics to a locally running Mimir instance with basic authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.operator.probes.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.remote_write \"staging\" {\n  // Send metrics to a locally running Mimir.\n  endpoint {\n    url = \"http://mimir:9009/api/v1/push\"\n\n    basic_auth {\n      username = \"example-user\"\n      password = \"example-password\"\n    }\n  }\n}\n\nprometheus.operator.probes \"pods\" {\n    forward_to = [prometheus.remote_write.staging.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana Cloud Mimir Rules Kubernetes Component in Alloy\nDESCRIPTION: This example shows how to create a mimir.rules.kubernetes component that loads discovered rules to Grafana Cloud. It includes basic authentication and adds a label to each rule.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/mimir/mimir.rules.kubernetes.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nmimir.rules.kubernetes \"default\" {\n    address = \">GRAFANA_CLOUD_METRICS_URL>\"\n    basic_auth {\n        username = \"<GRAFANA_CLOUD_USER>\"\n        password = \"<GRAFANA_CLOUD_API_KEY>\"\n        // Alternatively, load the password from a file:\n        // password_file = \"<GRAFANA_CLOUD_API_KEY_PATH>\"\n    }\n    external_labels = {\"label1\" = \"value1\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Profiling All Java Processes on Host with pyroscope.java\nDESCRIPTION: Complete Alloy configuration example demonstrating how to profile every Java process on the current host using pyroscope.java, including discovery and relabeling.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.java.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\npyroscope.write \"staging\" {\n  endpoint {\n    url = \"http://localhost:4040\"\n  }\n}\n\ndiscovery.process \"all\" {\n  refresh_interval = \"60s\"\n  discover_config {\n    cwd = true\n    exe = true\n    commandline = true\n    username = true\n    uid = true\n    container_id = true\n  }\n}\n\ndiscovery.relabel \"java\" {\n  targets = discovery.process.all.targets\n  rule {\n    action = \"keep\"\n    regex = \".*/java$\"\n    source_labels = [\"__meta_process_exe\"]\n  }\n}\n\npyroscope.java \"java\" {\n  targets = discovery.relabel.java.output\n  forward_to = [pyroscope.write.staging.receiver]\n  profiling_config {\n    interval = \"60s\"\n    alloc = \"512k\"\n    cpu = true\n    sample_rate = 100\n    lock = \"1ms\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Docker Discovery Configuration for Pyroscope eBPF Profiling\nDESCRIPTION: Alloy configuration that discovers Docker containers and collects performance profiles from them using Pyroscope's eBPF collector. The service_name label is set to the container name for easy identification.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.ebpf.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.docker \"linux\" {\n  host = \"unix:///var/run/docker.sock\"\n}\n\ndiscovery.relabel \"local_containers\" {\n  targets = discovery.docker.linux.targets\n  rule {\n    action = \"replace\"\n    source_labels = [\"__meta_docker_container_name\"]\n    target_label = \"service_name\"\n  }\n}\n\npyroscope.write \"staging\" {\n  endpoint {\n    url = \"http://pyroscope:4040\"\n  }\n}\n\npyroscope.ebpf \"default\" {\n  forward_to   = [ pyroscope.write.staging.receiver ]\n  targets      = discovery.relabel.local_containers.output\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Scrape Job in Alloy\nDESCRIPTION: Basic configuration template for setting up a Prometheus scraping job. Requires specifying a unique label, list of targets to scrape metrics from, and list of receivers to forward the metrics to.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.scrape.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"<LABEL>\" {\n  targets    = <TARGET_LIST>\n  forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Static Targets Configuration\nDESCRIPTION: Example showing how to collect log entries from specific files and forward them to a loki.write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.file.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.file \"tmpfiles\" {\n  targets    = [\n    {__path__ = \"/tmp/foo.txt\", \"color\" = \"pink\"},\n    {__path__ = \"/tmp/bar.txt\", \"color\" = \"blue\"},\n    {__path__ = \"/tmp/baz.txt\", \"color\" = \"grey\"},\n  ]\n  forward_to = [loki.write.local.receiver]\n}\n\nloki.write \"local\" {\n  endpoint {\n    url = \"loki:3100/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Continuous Profiling in Grafana Alloy\nDESCRIPTION: Configuration for setting up continuous profiling in Grafana Alloy. This example shows an instance profiling itself and sending the data to Grafana Pyroscope, with authentication credentials retrieved from environment variables.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/troubleshoot/profile.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\npyroscope.scrape \"default\" {\n  targets    = [{\"__address__\" = \"localhost:12345\", \"service_name\"=\"alloy\"}]\n  forward_to = [pyroscope.write.default.receiver]\n}\n\npyroscope.write \"default\" {\n  endpoint {\n    url = \"https://profiles-prod-014.grafana.net\"\n    basic_auth {\n      username = sys.env(\"PYROSCOPE_USERNAME\")\n      password = sys.env(\"PYROSCOPE_PASSWORD\")\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Kubernetes Discovery Components in Alloy\nDESCRIPTION: This snippet demonstrates how to define two Kubernetes discovery components in Alloy, one for pods and another for nodes. It shows the basic structure of component definition with arguments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/components.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubernetes \"pods\" {\n  role = \"pod\"\n}\n\ndiscovery.kubernetes \"nodes\" {\n  role = \"node\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Security Context Constraints (SCC) for Grafana Alloy on OpenShift\nDESCRIPTION: This YAML configuration defines a Security Context Constraints (SCC) object for Grafana Alloy deployment on OpenShift. It specifies that the application must run as user 473 with fsGroup 1000, and includes SELinux context settings. The SCC controls permissions for pods within the OpenShift environment.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/openshift.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nkind: SecurityContextConstraints\napiVersion: security.openshift.io/v1\nmetadata:\n  name: scc-alloy\nrunAsUser:\n  type: MustRunAs\n  uid: 473\nfsGroup:\n  type: MustRunAs\n  uid: 1000\nvolumes: \n- '*'\nusers:\n- my-admin-user\ngroups:\n- my-admin-group\nseLinuxContext:\n  type: MustRunAs\n  user: <SYSTEM_USER>\n  role: <SYSTEM_ROLE>\n  type: <CONTAINER_TYPE>\n  level: <LEVEL>\n```\n\n----------------------------------------\n\nTITLE: Processing Logs by Severity Text in Alloy\nDESCRIPTION: Configures an attribute processor that filters logs based on severity text matching a regex pattern. It obfuscates password attributes and removes token attributes for logs with severity text matching \"info.*\", targeting only log signals.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.attributes.md#2025-04-22_snippet_9\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.attributes \"default\" {\n    include {\n        match_type = \"regexp\"\n        log_severity_texts = [\"info.*\"]\n    }\n    action {\n        key = \"password\"\n        action = \"update\"\n        value = \"obfuscated\"\n    }\n    action {\n        key = \"token\"\n        action = \"delete\"\n    }\n\n    output {\n        logs    = [otelcol.exporter.otlp.default.input]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring EC2 Discovery and Prometheus Scraping in Grafana Alloy\nDESCRIPTION: This snippet demonstrates how to set up EC2 discovery, configure Prometheus scraping, and send metrics to a remote write endpoint. It includes the EC2 discovery configuration, Prometheus scrape job setup, and remote write configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.ec2.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.ec2 \"ec2\" {\n  region = \"us-east-1\"\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.ec2.ec2.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Linode Private IP Discovery and Remote Write in Alloy\nDESCRIPTION: Sets up Linode service discovery using private IP addresses and configures Prometheus remote write. The configuration includes service discovery with bearer token authentication, relabeling rules to use private IPv4 addresses, scrape configuration, and remote write setup with basic authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.linode.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.linode \"example\" {\n    bearer_token = sys.env(\"LINODE_TOKEN\")\n    port = 8876\n}\ndiscovery.relabel \"private_ips\" {\n    targets = discovery.linode.example.targets\n    rule {\n        source_labels = [\"__meta_linode_private_ipv4\"]\n        replacement     = \"[$1]:8876\"\n        target_label  = \"__address__\"\n    }\n}\nprometheus.scrape \"demo\" {\n    targets    = discovery.relabel.private_ips.targets\n    forward_to = [prometheus.remote_write.demo.receiver]\n}\nprometheus.remote_write \"demo\" {\n    endpoint {\n        url = <PROMETHEUS_REMOTE_WRITE_URL>\n        basic_auth {\n            username = <USERNAME>\n            password = <PASSWORD>\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Git Import with Math Component Example - Alloy\nDESCRIPTION: Example showing how to import and use custom math components from a Git repository. Demonstrates importing from a specific file and using the imported component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/import.git.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nimport.git \"math\" {\n  repository = \"https://github.com/wildum/module.git\"\n  revision   = \"master\"\n  path       = \"math.alloy\"\n}\n\nmath.add \"default\" {\n  a = 15\n  b = 45\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SNMP Exporter with Inline Target Definitions in Alloy\nDESCRIPTION: This example shows how to set up prometheus.exporter.snmp with inline target definitions for network devices and connect it to a prometheus.scrape component to collect metrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.snmp.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.snmp \"example\" {\n    config_file = \"snmp_modules.yml\"\n\n    target \"network_switch_1\" {\n        address     = \"192.168.1.2\"\n        module      = \"system,if_mib\"\n        walk_params = \"public\"\n        labels = {\n            \"env\" = \"dev\",\n        }\n    }\n\n    target \"network_router_2\" {\n        address     = \"192.168.1.3\"\n        module      = \"system,if_mib,mikrotik\"\n        walk_params = \"private\"\n    }\n\n    walk_param \"private\" {\n        retries = \"2\"\n    }\n\n    walk_param \"public\" {\n        retries = \"2\"\n    }\n}\n\n// Configure a prometheus.scrape component to collect SNMP metrics.\nprometheus.scrape \"demo\" {\n    targets    = prometheus.exporter.snmp.example.targets\n    forward_to = [ /* ... */ ]\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Dnsmasq Exporter Implementation\nDESCRIPTION: Complete example showing how to configure prometheus.exporter.dnsmasq with prometheus.scrape and remote_write setup\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.dnsmasq.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.dnsmasq \"example\" {\n  address = \"localhost:53\"\n}\n\n// Configure a prometheus.scrape component to collect github metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.dnsmasq.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic ServiceMonitors Configuration in Alloy\nDESCRIPTION: This example demonstrates a basic configuration for discovering all ServiceMonitors in a Kubernetes cluster and forwarding the collected metrics to a prometheus.remote_write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.operator.servicemonitors.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.remote_write \"staging\" {\n  // Send metrics to a locally running Mimir.\n  endpoint {\n    url = \"http://mimir:9009/api/v1/push\"\n\n    basic_auth {\n      username = \"example-user\"\n      password = \"example-password\"\n    }\n  }\n}\n\nprometheus.operator.servicemonitors \"services\" {\n    forward_to = [prometheus.remote_write.staging.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Receiving and Forwarding Prometheus Metrics\nDESCRIPTION: Example configuration that sets up a prometheus.receive_http component to listen for metrics and forward them to a prometheus.remote_write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.receive_http.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\n// Receives metrics over HTTP\nprometheus.receive_http \"api\" {\n  http {\n    listen_address = \"0.0.0.0\"\n    listen_port = 9999\n  }\n  forward_to = [prometheus.remote_write.local.receiver]\n}\n\n// Send metrics to a locally running Mimir.\nprometheus.remote_write \"local\" {\n  endpoint {\n    url = \"http://mimir:9009/api/v1/push\"\n\n    basic_auth {\n      username = \"example-user\"\n      password = \"example-password\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Exporter in Grafana Alloy\nDESCRIPTION: Configures the otelcol.exporter.otlp component in Alloy to deliver OpenTelemetry data to compatible endpoints. This component is responsible for writing collected telemetry to a specified destination.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/datadog-traces-metrics.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = \"<OTLP_ENDPOINT_URL>\"\n    auth     = otelcol.auth.basic.auth.handler\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring GCP Load Balancer Metrics Export with Filtering\nDESCRIPTION: Shows configuration for exporting all Load Balancer metrics with a specific backend target filter. Demonstrates basic filtering for load balancer monitoring.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.gcp.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.gcp \"lb_with_filter\" {\n        project_ids = [\n                \"foo\",\n                \"bar\",\n        ]\n        metrics_prefixes = [\n                \"loadbalancing.googleapis.com\",\n        ]\n        extra_filters = [\n                \"loadbalancing.googleapis.com:resource.labels.backend_target_name=\\\"sample-value\\\"\",\n        ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring loki.source.cloudflare in Alloy\nDESCRIPTION: Basic configuration for the loki.source.cloudflare component, specifying zone ID, API token, and forward destination.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.cloudflare.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.cloudflare \"<LABEL>\" {\n  zone_id   = \"<ZONE_ID>\"\n  api_token = \"<API_TOKEN>\"\n\n  forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Authentication with Basic Auth and Path Filtering in Alloy\nDESCRIPTION: This snippet demonstrates how to set up HTTP authentication using basic auth and configure path filtering. It shows how to protect specific API paths (/metrics and /v1) with authentication using environment variables for credentials.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/http.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nhttp {\n  auth {\n    basic {\n      username = sys.env(\"BASIC_AUTH_USERNAME\")\n      password = sys.env(\"BASIC_AUTH_PASSWORD\")\n    }\n\n    filter {\n      paths                       = [\"/metrics\", \"/v1\"]\n      authenticate_matching_paths = true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Cloudflare Log Configuration with Local Loki Write\nDESCRIPTION: Demonstrates how to configure loki.source.cloudflare to pull logs from Cloudflare and forward them to a local Loki write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.cloudflare.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.cloudflare \"dev\" {\n  zone_id   = sys.env(\"CF_ZONE_ID\")\n  api_token = local.file.api.content\n\n  forward_to = [loki.write.local.receiver]\n}\n\nloki.write \"local\" {\n  endpoint {\n    url = \"loki:3100/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring GCP Load Balancer Specific Metrics Export\nDESCRIPTION: Configures export of specific Load Balancer metrics (request bytes and latencies) with backend target filtering. Shows how to select specific metrics while applying filters.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.gcp.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.gcp \"lb_subset_with_filter\" {\n        project_ids = [\n                \"foo\",\n                \"bar\",\n        ]\n        metrics_prefixes = [\n                \"loadbalancing.googleapis.com/https/request_bytes_count\",\n                \"loadbalancing.googleapis.com/https/total_latencies\",\n        ]\n        extra_filters = [\n                \"loadbalancing.googleapis.com:resource.labels.backend_target_name=\\\"sample-value\\\"\",\n        ]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Scrape Jobs for Blackbox Exporter Targets\nDESCRIPTION: Example configuration of a scrape job for collecting metrics from blackbox exporter instances. This setup includes custom scrape intervals, query parameters, and forwards metrics to multiple remote write receivers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.scrape.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"blackbox_scraper\" {\n  targets = [\n    {\"__address__\" = \"blackbox-exporter:9115\", \"instance\" = \"one\"},\n    {\"__address__\" = \"blackbox-exporter:9116\", \"instance\" = \"two\"},\n  ]\n\n  forward_to = [prometheus.remote_write.grafanacloud.receiver, prometheus.remote_write.onprem.receiver]\n\n  scrape_interval = \"10s\"\n  params          = { \"target\" = [\"grafana.com\"], \"module\" = [\"http_2xx\"] }\n  metrics_path    = \"/probe\"\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Linode Discovery with Prometheus Integration\nDESCRIPTION: Comprehensive example showing Linode discovery configuration integrated with Prometheus scraping and remote write setup. Includes bearer token authentication, custom port configuration, and basic auth for remote write endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.linode.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.linode \"example\" {\n    bearer_token = sys.env(\"LINODE_TOKEN\")\n    port = 8876\n}\nprometheus.scrape \"demo\" {\n    targets    = discovery.linode.example.targets\n    forward_to = [prometheus.remote_write.demo.receiver]\n}\nprometheus.remote_write \"demo\" {\n    endpoint {\n        url = <PROMETHEUS_REMOTE_WRITE_URL>\n        basic_auth {\n            username = <USERNAME>\n            password = <PASSWORD>\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.exporter.awss3 in Alloy\nDESCRIPTION: This snippet demonstrates how to configure the otelcol.exporter.awss3 component in Alloy. It specifies the AWS region, S3 bucket, and prefix for storing telemetry data.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.awss3.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.awss3 \"<LABEL>\" {\n  s3_uploader {\n    region = \"<REGION>\"\n    s3_bucket = \"<BUCKET_NAME>\"\n    s3_prefix = \"<PREFIX>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Local File Component Example with Secret Integration\nDESCRIPTION: Demonstrates a practical example of using local.file to read a password file and use its contents as a secret token in a Grafana Cloud stack configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/local/local.file.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file \"secret_key\" {\n  filename  = \"/var/secrets/password.txt\"\n  is_secret = true\n}\ngrafana_cloud.stack \"receivers\" {\n  stack_name = \"mystack\"\n  token = local.file.secret_key.content\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Loki Source API Configuration\nDESCRIPTION: Basic configuration structure for setting up a loki.source.api component with HTTP listener settings and forward_to destinations.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.api.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.api \"<LABEL>\" {\n    http {\n        listen_address = \"<LISTEN_ADDRESS>\"\n        listen_port = \"<PORT>\"\n    }\n    forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Basic StatsD Exporter Configuration in Alloy\nDESCRIPTION: Basic usage example showing the minimal configuration syntax for the prometheus.exporter.statsd component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.statsd.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.statsd \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Removing Unnecessary Resource Attributes Configuration\nDESCRIPTION: Shows how to configure the transform processor to remove unnecessary resource attributes before sending spans to the spanmetrics connector, preventing duplicate timestamp errors when exporting to Prometheus.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.spanmetrics.md#2025-04-22_snippet_9\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  http {}\n  grpc {}\n\n  output {\n    traces  = [otelcol.processor.transform.default.input]\n  }\n}\n\n// Remove all resource attributes except the ones which\n// the otelcol.connector.spanmetrics needs.\n// If this is not done, otelcol.exporter.prometheus may fail to\n// write some samples due to an \"err-mimir-sample-duplicate-timestamp\" error.\n// This is because the spanmetricsconnector will create a new\n// metrics resource scope for each traces resource scope.\notelcol.processor.transform \"default\" {\n  error_mode = \"ignore\"\n\n  trace_statements {\n    context = \"resource\"\n    statements = [\n      // We keep only the \"service.name\" and \"special.attr\" resource attributes,\n      // because they are the only ones which otelcol.connector.spanmetrics needs.\n      //\n      // There is no need to list \"span.name\", \"span.kind\", and \"status.code\"\n      // here because they are properties of the span (and not resource attributes):\n      // https://github.com/open-telemetry/opentelemetry-proto/blob/v1.0.0/opentelemetry/proto/trace/v1/trace.proto\n      `keep_keys(attributes, [\"service.name\", \"special.attr\"])`,\n    ]\n  }\n\n  output {\n    traces  = [otelcol.connector.spanmetrics.default.input]\n  }\n}\n\notelcol.connector.spanmetrics \"default\" {\n  histogram {\n    explicit {}\n  }\n\n  dimension {\n    name = \"special.attr\"\n  }\n  output {\n    metrics = [otelcol.exporter.prometheus.default.input]\n  }\n}\n\notelcol.exporter.prometheus \"default\" {\n  forward_to = [prometheus.remote_write.mimir.receiver]\n}\n\nprometheus.remote_write \"mimir\" {\n  endpoint {\n    url = \"http://mimir:9009/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting a span status in Alloy\nDESCRIPTION: This example shows how to set a specific status code and description for all processed spans.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.span.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.span \"default\" {\n  status {\n    code        = \"Error\"\n    description = \"some additional error description\"\n  }\n\n  output {\n      traces = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Processing Stage Configuration in Alloy\nDESCRIPTION: Shows multi-stage JSON processing configuration with nested JSON parsing. Extracts values using JMESPath expressions and handles multiple JSON parsing stages.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_10\n\nLANGUAGE: alloy\nCODE:\n```\n{\"log\":\"log message\\n\",\"extra\":\"{\\\"user\\\":\\\"alloy\\\"}\"}\n\nloki.process \"username\" {\n  stage.json {\n      expressions = {output = \"log\", extra = \"\"}\n  }\n\n  stage.json {\n      source      = \"extra\"\n      expressions = {username = \"user\"}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Triton Discovery and Prometheus Integration\nDESCRIPTION: Comprehensive example showing Triton discovery configuration integrated with Prometheus scraping and remote write setup.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.triton.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.triton \"example\" {\n    account    = \"<TRITON_ACCOUNT>\"\n    dns_suffix = \"<TRITON_DNS_SUFFIX>\"\n    endpoint   = \"<TRITON_ENDPOINT>\"\n}\n\nprometheus.scrape \"demo\" {\n    targets    = discovery.triton.example.targets\n    forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n    endpoint {\n        url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n        basic_auth {\n            username = \"<USERNAME>\"\n            password = \"<PASSWORD>\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Blackbox Exporter with Configuration File\nDESCRIPTION: Complete example showing prometheus.exporter.blackbox setup with a configuration file and multiple targets, including prometheus.scrape and remote_write integration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.blackbox.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.blackbox \"example\" {\n  config_file = \"blackbox_modules.yml\"\n\n  target {\n    name    = \"example\"\n    address = \"https://example.com\"\n    module  = \"http_2xx\"\n  }\n\n  target {\n    name    = \"grafana\"\n    address = \"https://grafana.com\"\n    module  = \"http_2xx\"\n    labels = {\n      \"env\" = \"dev\",\n    }\n  }\n}\n\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.blackbox.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Prometheus Configuration for Conversion\nDESCRIPTION: A sample Prometheus configuration file that includes global settings, scrape configuration for a job named 'prometheus', and remote write configuration for Grafana Cloud.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-prometheus.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  scrape_timeout:    45s\n\nscrape_configs:\n  - job_name: \"prometheus\"\n    static_configs:\n      - targets: [\"localhost:12345\"]\n\nremote_write:\n  - name: \"grafana-cloud\"\n    url: \"https://prometheus-us-central1.grafana.net/api/prom/push\"\n    basic_auth:\n      username: <USERNAME>\n      password: <PASSWORD>\n```\n\n----------------------------------------\n\nTITLE: Prometheus Scrape Configuration\nDESCRIPTION: Configuration for scraping metrics from discovered Kubernetes services and forwarding them to a remote write endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/prometheus-metrics.md#2025-04-22_snippet_9\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"<SCRAPE_LABEL>\" {\n  targets    = discovery.kubernetes.<DISCOVERY_LABEL>.targets\n  forward_to = [prometheus.remote_write.<REMOTE_WRITE_LABEL>.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Using Embedded Configuration with Secrets for SNMP Exporter in Alloy\nDESCRIPTION: This example demonstrates configuring prometheus.exporter.snmp with an embedded configuration using a local.file component marked as a secret, and connecting it to prometheus.remote_write for sending metrics to a remote server.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.snmp.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file \"snmp_config\" {\n    filename  = \"snmp_modules.yml\"\n    is_secret = true\n}\n\nprometheus.exporter.snmp \"example\" {\n    config = local.file.snmp_config.content\n\n    target \"network_switch_1\" {\n        address     = \"192.168.1.2\"\n        module      = \"system,if_mib\"\n        walk_params = \"public\"\n    }\n\n    target \"network_router_2\" {\n        address     = \"192.168.1.3\"\n        module      = \"system,if_mib,mikrotik\"\n        walk_params = \"private\"\n    }\n\n    walk_param \"private\" {\n        retries = \"2\"\n    }\n\n    walk_param \"public\" {\n        retries = \"2\"\n    }\n}\n\n// Configure a prometheus.scrape component to collect SNMP metrics.\nprometheus.scrape \"demo\" {\n    targets    = prometheus.exporter.snmp.example.targets\n    forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n    endpoint {\n        url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n        basic_auth {\n            username = \"<USERNAME>\"\n            password = \"<PASSWORD>\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Native Histogram Scraping over gRPC in Prometheus\nDESCRIPTION: Configuration for enabling native histogram scraping over gRPC in Prometheus by specifying PrometheusProto as the first protocol to negotiate and enabling native histograms.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.scrape.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"prometheus\" {\n  ...\n  scrape_native_histograms = true\n  scrape_protocols = [\"PrometheusProto\", \"OpenMetricsText1.0.0\", \"OpenMetricsText0.0.1\", \"PrometheusText0.0.4\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring Custom Component in Alloy\nDESCRIPTION: This example defines a custom 'add' component in Alloy. It takes two arguments 'a' and 'b', and exports their sum. This component would be stored in a file named module.alloy.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/import.http.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndeclare \"add\" {\n  argument \"a\" {}\n  argument \"b\" {}\n\n  export \"sum\" {\n    value = argument.a.value + argument.b.value\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring K8s Attributes Processor in Alloy\nDESCRIPTION: Basic configuration structure for the k8sattributes processor showing the required output block configuration for metrics, logs and traces.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.k8sattributes.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.k8sattributes \"LABEL\" {\n  output {\n    metrics = [...]\n    logs    = [...]\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Push-based GCP Log Source with Default HTTP Configuration\nDESCRIPTION: Configuration example demonstrating how to set up a GCP log source using push strategy with default HTTP settings, forwarding logs to a local Loki write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.gcplog.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.gcplog \"local\" {\n  push {}\n\n  forward_to = [loki.write.local.receiver]\n}\n\nloki.write \"local\" {\n  endpoint {\n    url = \"loki:3100/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Event Hubs Source for Loki in Alloy\nDESCRIPTION: This snippet demonstrates the basic usage of the loki.source.azure_event_hubs component, including required arguments and the authentication block.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.azure_event_hubs.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.azure_event_hubs \"<LABEL>\" {\n    fully_qualified_namespace = \"<HOST:PORT>\"\n    event_hubs                = \"<EVENT_HUB_LIST>\"\n    forward_to                = <RECEIVER_LIST>\n\n    authentication {\n        mechanism = \"AUTHENTICATION_MECHANISM\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Local File Component Usage in Alloy\nDESCRIPTION: Shows the basic syntax for declaring a local.file component with a label and filename.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/local/local.file.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file \"<LABEL>\" {\n  filename = \"<FILE_NAME>\"\n}\n```\n\n----------------------------------------\n\nTITLE: PodMonitors Configuration with Node Filtering for DaemonSet Deployments\nDESCRIPTION: Applies additional relabel rules to filter discovered targets by hostname, which is useful when running Grafana Alloy as a DaemonSet to ensure each node monitors only its local pods.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.operator.podmonitors.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.operator.podmonitors \"pods\" {\n    forward_to = [prometheus.remote_write.staging.receiver]\n    rule {\n      action = \"keep\"\n      regex = sys.env(\"<HOSTNAME>\")\n      source_labels = [\"__meta_kubernetes_pod_node_name\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Database Observability for MySQL in Alloy\nDESCRIPTION: This snippet demonstrates the basic usage of the database_observability.mysql component. It defines the data source name and specifies where to forward log entries after processing.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/database_observability/database_observability.mysql.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndatabase_observability.mysql \"<LABEL>\" {\n  data_source_name = <DATA_SOURCE_NAME>\n  forward_to       = [<LOKI_RECEIVERS>]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Systemd Journal Log Relabeling\nDESCRIPTION: Configuration for relabeling rules specific to systemd journal logs.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-linux.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.relabel \"logs_integrations_integrations_node_exporter_journal_scrape\" {\n  targets = []\n\n  rule {\n    source_labels = [\"__journal__systemd_unit\"]\n    target_label  = \"unit\"\n  }\n\n  rule {\n    source_labels = [\"__journal__boot_id\"]\n    target_label  = \"boot_id\"\n  }\n\n  rule {\n    source_labels = [\"__journal__transport\"]\n    target_label  = \"transport\"\n  }\n\n  rule {\n    source_labels = [\"__journal_priority_keyword\"]\n    target_label  = \"level\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Client Block for Datadog Exporter in YAML\nDESCRIPTION: The client block sets up HTTP client options for the Datadog exporter, including buffer sizes, timeouts, and connection settings.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.datadog.md#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nclient:\n  read_buffer_size: \"4096\"\n  write_buffer_size: \"4096\"\n  timeout: \"15s\"\n  max_idle_conns: 100\n  max_idle_conns_per_host: 5\n  idle_conn_timeout: \"45s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Consul Service Discovery in Alloy\nDESCRIPTION: Basic configuration for the discovery.consul component in Alloy. It specifies a label and the Consul server address.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.consul.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.consul \"<LABEL>\" {\n  server = \"<CONSUL_SERVER>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Datadog Exporter in Grafana Alloy\nDESCRIPTION: Basic usage example for configuring the Datadog exporter in Grafana Alloy. This snippet shows how to set up the exporter with an API key for authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.datadog.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.datadog \"LABEL\" {\n    api {\n        api_key = \"YOUR_API_KEY_HERE\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Capsule Types with Prometheus Components in Alloy\nDESCRIPTION: Example demonstrating how to use capsule types in Alloy, specifically showing how a capsule exported by prometheus.remote_write can be used in the forward_to attribute of prometheus.scrape.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/expressions/types_and_values.md#2025-04-22_snippet_9\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.remote_write \"default\" {\n  endpoint {\n    url = \"http://localhost:9090/api/v1/write\"\n  }\n}\n\nprometheus.scrape \"default\" {\n  targets    = [/* ... */]\n  forward_to = [prometheus.remote_write.default.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Usage Example with Hash Seed\nDESCRIPTION: Example showing probabilistic sampler configuration with hash seed and sampling percentage settings.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.probabilistic_sampler.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.probabilistic_sampler \"default\" {\n  hash_seed           = 123\n  sampling_percentage = 15.3\n\n  output {\n    logs = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Redacting Credit Card Numbers with stage.luhn in Alloy\nDESCRIPTION: This example shows using stage.luhn to identify and redact credit card numbers in log lines using the Luhn algorithm. The stage searches for credit card numbers and replaces them with a custom redaction string.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_16\n\nLANGUAGE: alloy\nCODE:\n```\ntime=2012-11-01T22:08:41+00:00 app=loki level=WARN duration=125 message=\"credit card approved 4032032513548443\" extra=\"user=example_name\"\n\nstage.luhn {\n    replacement = \"**DELETED**\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring GCP PubSub Metrics Export with Full Configuration\nDESCRIPTION: Demonstrates a complete configuration for exporting PubSub metrics from GCP with custom metric prefixes, filters for subscriptions, and timing settings. Includes configuration for snapshot and subscription metrics with targeted filtering.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.gcp.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.gcp \"pubsub_full_config\" {\n        project_ids = [\n                \"foo\",\n                \"bar\",\n        ]\n\n        // Using pubsub metrics (https://cloud.google.com/monitoring/api/metrics_gcp/gcp-pubsub) as an example\n        // all metrics.\n        //   [\n        //     \"pubsub.googleapis.com/\"\n        //   ]\n        // all snapshot specific metrics\n        //   [\n        //     \"pubsub.googleapis.com/snapshot\"\n        //   ]\n        // all snapshot specific metrics and a few subscription metrics\n        metrics_prefixes = [\n                \"pubsub.googleapis.com/snapshot\",\n                \"pubsub.googleapis.com/subscription/num_undelivered_messages\",\n                \"pubsub.googleapis.com/subscription/oldest_unacked_message_age\",\n        ]\n\n        // Given the above metrics_prefixes list, some examples of\n        // targeted_metric_prefix option behavior with respect to the filter string\n        // format <targeted_metric_prefix>:<filter_query> would be:\n        //   pubsub.googleapis.com (apply to all defined prefixes)\n        //   pubsub.googleapis.com/snapshot (apply to only snapshot metrics)\n        //   pubsub.googleapis.com/subscription (apply to only subscription metrics)\n        //   pubsub.googleapis.com/subscription/num_undelivered_messages (apply to only the specific subscription metric)\n        extra_filters = [\n                \"pubsub.googleapis.com/subscription:resource.labels.subscription_id=monitoring.regex.full_match(\\\"my-subs-prefix.*\\\")\",\n        ]\n\n        request_interval        = \"5m\"\n        request_offset          = \"0s\"\n        ingest_delay            = false\n        drop_delegated_projects = false\n        gcp_client_timeout      = \"15s\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Jaeger Receiver in OpenTelemetry Collector\nDESCRIPTION: Example configuration for setting up a Jaeger receiver in OpenTelemetry Collector. The receiver can accept data over multiple protocols including gRPC, Thrift HTTP, Thrift binary, and Thrift compact. The output section specifies where to forward the received telemetry data.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.jaeger.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.jaeger \"LABEL\" {\n  protocols {\n    grpc {}\n    thrift_http {}\n    thrift_binary {}\n    thrift_compact {}\n  }\n\n  output {\n    metrics = [...]\n    logs    = [...]\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete otelcol.receiver.solace Configuration Example\nDESCRIPTION: This example demonstrates a full configuration of otelcol.receiver.solace, including authentication, TLS settings, and integration with batch processing and OTLP exporting.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.solace.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.solace \"default\" {\n  queue = \"queue://#telemetry-testprofile\"\n  broker = \"localhost:5672\"\n  auth {\n    sasl_plain {\n      username = \"alloy\"\n      password = \"password\"\n    }\n  }\n  tls {\n    insecure             = true\n    insecure_skip_verify = true\n  }\n  output {\n    traces = [otelcol.processor.batch.default.input]\n  }\n}\n\notelcol.processor.batch \"default\" {\n  output {\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = sys.env(\"OTLP_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Detection Processor with Environment and EC2 Detectors\nDESCRIPTION: This Alloy configuration snippet sets up the Resource Detection Processor with environment and EC2 detectors. It demonstrates how to configure the processor and route its output to an OTLP exporter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.resourcedetection.md#2025-04-22_snippet_8\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.resourcedetection \"default\" {\n  detectors = [\"env\", \"ec2\"]\n\n  output {\n    logs    = [otelcol.exporter.otlp.default.input]\n    metrics = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CloudWatch Exporter for SQS Metrics in Alloy\nDESCRIPTION: This Alloy configuration snippet sets up a CloudWatch exporter job for AWS SQS queues. It defines the AWS region, discovery type, and specific metrics to collect such as NumberOfMessagesSent and NumberOfMessagesReceived.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.cloudwatch.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.cloudwatch \"queues\" {\n    sts_region      = \"us-east-2\"\n    aws_sdk_version_v2 = false\n    discovery {\n        type        = \"AWS/SQS\"\n        regions     = [\"us-east-2\"]\n        search_tags = {\n            \"scrape\" = \"true\",\n        }\n\n        metric {\n            name       = \"NumberOfMessagesSent\"\n            statistics = [\"Sum\", \"Average\"]\n            period     = \"1m\"\n        }\n\n        metric {\n            name       = \"NumberOfMessagesReceived\"\n            statistics = [\"Sum\", \"Average\"]\n            period     = \"1m\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Advanced PostgreSQL Exporter Configuration with Custom Metrics\nDESCRIPTION: Advanced example demonstrating custom metrics collection from specific databases using autodiscovery and custom queries configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.postgres.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.postgres \"example\" {\n  data_source_names = [\"postgresql://username:password@localhost:5432/database_name?sslmode=disable\"]\n\n  autodiscovery {\n    enabled            = true\n    database_allowlist = [\"frontend_app\", \"backend_app\"]\n  }\n\n  disable_default_metrics    = true\n  custom_queries_config_path = \"/etc/alloy/custom-postgres-metrics.yaml\"\n}\n\nprometheus.scrape \"default\" {\n  targets    = prometheus.exporter.postgres.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Consul Resource Attributes Configuration\nDESCRIPTION: Configuration block for Consul agent resource attributes including server address, datacenter, token, namespace, and metadata settings. Supports ACL token authentication and custom metadata key filtering.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.resourcedetection.md#2025-04-22_snippet_4\n\nLANGUAGE: hcl\nCODE:\n```\nconsul {\n  address = \"\"\n  datacenter = \"\"\n  token = \"\"\n  namespace = \"\"\n  meta = []\n}\n```\n\n----------------------------------------\n\nTITLE: Serving Jaeger Remote Sampling from Component in Alloy\nDESCRIPTION: This example uses the output of a local.file component to determine what sampling rules to serve. It demonstrates how to use content from another component as the source for the Jaeger remote sampling extension.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.extension.jaeger_remote_sampling.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file \"sampling\" {\n  filename  = \"/path/to/jaeger-sampling.json\"\n}\n\notelcol.extension.jaeger_remote_sampling \"example\" {\n  http {\n  }\n  source {\n    content = local.file.sampling.content\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring loki.source.docker in Alloy\nDESCRIPTION: Basic configuration for the loki.source.docker component, specifying the label, host, targets, and forward_to arguments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.docker.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.docker \"LABEL\" {\n  host       = HOST\n  targets    = TARGET_LIST\n  forward_to = RECEIVER_LIST\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Scaleway Service Discovery in Grafana Alloy\nDESCRIPTION: This snippet demonstrates how to configure the discovery.scaleway component in Grafana Alloy. It includes the required arguments for project ID, role, access key, and secret key.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.scaleway.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.scaleway \"<LABEL>\" {\n    project_id = \"<SCALEWAY_PROJECT_ID>\"\n    role       = \"<SCALEWAY_PROJECT_ROLE>\"\n    access_key = \"<SCALEWAY_ACCESS_KEY>\"\n    secret_key = \"<SCALEWAY_SECRET_KEY>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Histogram Metrics in Grafana Alloy\nDESCRIPTION: Configuration options for 'metric.histogram' which defines a metric whose values are recorded in predefined buckets. Requires bucket definitions and supports options for metric metadata and value handling.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_20\n\nLANGUAGE: markdown\nCODE:\n```\n#### `metric.histogram`\n\nDefines a histogram metric whose values are recorded in predefined buckets.\n\nThe following arguments are supported:\n\n| Name                | Type          | Description                                                                         | Default                  | Required |\n| ------------------- | ------------- | ----------------------------------------------------------------------------------- | ------------------------ | -------- |\n| `buckets`           | `list(float)` | Predefined buckets                                                                    |                          | yes      |\n| `name`              | `string`      | The metric name.                                                                    |                          | yes      |\n| `description`       | `string`      | The metric's description and help text.                                             | `\"\"`                     | no       |\n| `max_idle_duration` | `duration`    | Maximum amount of time to wait until the metric is marked as 'stale' and removed.   | `\"5m\"`                   | no       |\n| `prefix`            | `string`      | The prefix to the metric name.                                                      | `\"loki_process_custom_\"` | no       |\n| `source`            | `string`      | Key from the extracted data map to use for the metric. Defaults to the metric name. | `\"\"`                     | no       |\n| `value`             | `string`      | If set, the metric only changes if `source` exactly matches the `value`.            | `\"\"`                     | no       |\n```\n\n----------------------------------------\n\nTITLE: Configuring DNS Discovery in Grafana Alloy\nDESCRIPTION: Basic configuration for the discovery.dns component in Grafana Alloy. It specifies the DNS names to look up for service discovery.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.dns.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.dns \"<LABEL>\" {\n  names = [\"<NAME_1>\", \"<NAME_2>\", ...]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring loki.enrich Component in Alloy\nDESCRIPTION: This snippet demonstrates the basic configuration structure for the loki.enrich component. It shows how to specify targets, match labels, and forward enriched logs.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.enrich.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.enrich \"<LABEL>\" {\n  // List of targets from a discovery component\n  targets = <DISCOVERY_COMPONENT>.targets\n  \n  // Which label from discovered targets to match against\n  match_label = \"<LABEL>\"\n  \n  // Which label from incoming logs to match against\n  source_label = \"<LABEL>\"\n  \n  // List of labels to copy from discovered targets to logs\n  labels_to_copy = [\"<LABEL>\", ...]\n  \n  // Where to send enriched logs\n  forward_to = [<RECEIVER_LIST>]\n}\n```\n\n----------------------------------------\n\nTITLE: File Storage with Filelog Receiver Example\nDESCRIPTION: Comprehensive example showing how to use file storage component with a filelog receiver to store file offsets. The example demonstrates storage configuration, log parsing, and debug output.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.storage.file.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.storage.file \"default\" {}\n\notelcol.receiver.filelog \"default\" {\n    include = [\"/var/log/*.log\"]\n    storage = otelcol.storage.file.default.handler\n    operators = [{\n      type = \"regex_parser\",\n      regex = \"^(?P<timestamp>[^ ]+)\",\n      timestamp = {\n        parse_from = \"attributes.timestamp\",\n        layout = \"%Y-%m-%dT%H:%M:%S.%fZ\",\n        location = \"UTC\",\n      },\n    }]\n    output {\n        logs = [otelcol.exporter.debug.default.input]\n    }\n}\n\n\notelcol.exporter.debug \"default\" {}\n```\n\n----------------------------------------\n\nTITLE: Complete OpenTelemetry Configuration with Header Authentication\nDESCRIPTION: Comprehensive example demonstrating integration of header authentication with OTLP receiver, batch processor, and OTLP exporter components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.auth.headers.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  http {\n    include_metadata = true\n  }\n  grpc {\n    include_metadata = true\n  }\n\n  output {\n    metrics = [otelcol.processor.batch.default.input]\n    logs    = [otelcol.processor.batch.default.input]\n    traces  = [otelcol.processor.batch.default.input]\n  }\n}\n\notelcol.processor.batch \"default\" {\n  // Preserve the tenant_id metadata.\n  metadata_keys = [\"tenant_id\"]\n\n  output {\n    metrics = [otelcol.exporter.otlp.production.input]\n    logs    = [otelcol.exporter.otlp.production.input]\n    traces  = [otelcol.exporter.otlp.production.input]\n  }\n}\n\notelcol.auth.headers \"creds\" {\n  header {\n    key          = \"X-Scope-OrgID\"\n    from_context = \"tenant_id\"\n  }\n\n  header {\n    key   = \"User-ID\"\n    value = \"user_id\"\n  }\n}\n\notelcol.exporter.otlp \"production\" {\n  client {\n    endpoint = sys.env(\"OTLP_SERVER_ENDPOINT\")\n    auth     = otelcol.auth.headers.creds.handler\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Block for Meta-monitoring in Alloy\nDESCRIPTION: Sets up the logging configuration block to define log level, format, and forwarding destination for Alloy's internal logs. This block can only appear once in a configuration file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/metamonitoring.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nlogging {\n  level    = \"<LOG_LEVEL>\"\n  format   = \"<LOG_FORMAT>\"\n  write_to = [<LOGS_RECEIVER_LIST>]\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of remote.kubernetes.secret in Alloy\nDESCRIPTION: Demonstrates the basic syntax for using the remote.kubernetes.secret component to read a Kubernetes Secret from a specific namespace.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/remote/remote.kubernetes.secret.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nremote.kubernetes.secret \"<LABEL>\" {\n  namespace = \"<NAMESPACE_OF_SECRET>\"\n  name = \"<NAME_OF_SECRET>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Marathon Service Discovery in Alloy\nDESCRIPTION: Basic usage of the discovery.marathon component to retrieve scrape targets from Marathon servers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.marathon.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.marathon \"<LABEL>\" {\n  servers = [\"<MARATHON_SERVER1>\", \"<MARATHON_SERVER2>\"...]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring GCE Discovery in Grafana Alloy\nDESCRIPTION: Basic configuration for the discovery.gce component, specifying the project and zone for discovering GCE instances.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.gce.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.gce \"<LABEL>\" {\n  project = \"<PROJECT_NAME>\"\n  zone    = \"<ZONE_NAME>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Using a Custom Addition Component in Alloy\nDESCRIPTION: This example defines a custom 'add' component that takes two arguments 'a' and 'b', and exports their sum. The component is then used with specific values, resulting in a sum of 32.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/custom_components.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndeclare \"add\" {\n    argument \"a\" { }\n    argument \"b\" { }\n\n    export \"sum\" {\n        value = argument.a.value + argument.b.value\n    }\n}\n\nadd \"example\" {\n    a = 15\n    b = 17\n}\n\n// add.example.sum == 32\n```\n\n----------------------------------------\n\nTITLE: Static Target Configuration\nDESCRIPTION: Example showing how to configure the processor with a static list of attributes without using a discovery component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.discovery.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.discovery \"default\" {\n    targets = [{\n        \"__address__\"          = \"1.2.2.2\",\n        \"__internal_label__\"   = \"test_val\",\n        \"test_label\"           = \"test_val2\",\n        \"test.label.with.dots\" = \"test.val2.with.dots\"}]\n\n    output {\n        traces = [otelcol.exporter.otlp.default.input]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Parsing Logfmt and Extracting Fields with stage.logfmt in Alloy\nDESCRIPTION: This example demonstrates using stage.logfmt to parse logfmt-formatted log lines and extract values. The first stage extracts the 'extra' field from the main log line, then a second stage parses the 'extra' field to extract a username.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_15\n\nLANGUAGE: alloy\nCODE:\n```\ntime=2012-11-01T22:08:41+00:00 app=loki level=WARN duration=125 message=\"this is a log line\" extra=\"user=example_name\"\n\nstage.logfmt {\n    mapping = { \"extra\" = \"\" }\n}\n\nstage.logfmt {\n    mapping = { \"username\" = \"user\" }\n    source  = \"extra\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Forwarding to AWS S3 in Alloy\nDESCRIPTION: Complete configuration for forwarding local log files to AWS S3. Sets up file matching patterns for system logs, configures Loki source for collection, and establishes AWS S3 export pipeline with bucket configuration. Uses OpenTelemetry collector for handling the export process.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.awss3.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file_match \"logs\" {\n  path_targets = [{\n    __address__ = \"localhost\",\n    __path__    = \"/var/log/{syslog,messages,*.log}\",\n    instance    = constants.hostname,\n    job         = \"integrations/node_exporter\",\n  }]\n}\n\nloki.source.file \"logs\" {\n  targets    = local.file_match.logs.targets\n  forward_to = [otelcol.receiver.loki.default.receiver]\n}\n\notelcol.receiver.loki \"default\" {\n  output {\n    logs = [otelcol.exporter.awss3.logs.input]\n  }\n}\n\notelcol.exporter.awss3 \"logs\" {\n  s3_uploader {\n    region = \"us-east-1\"\n    s3_bucket = \"logs_bucket\"\n    s3_prefix = \"logs\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Telemetry Pipeline Configuration\nDESCRIPTION: Example showing a complete telemetry pipeline that forwards received data through a batch processor to an OTLP endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.datadog.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.datadog \"default\" {\n  output {\n    metrics = [otelcol.processor.batch.default.input]\n    traces  = [otelcol.processor.batch.default.input]\n  }\n}\n\notelcol.processor.batch \"default\" {\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = sys.env(\"OTLP_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Loki Heroku Source Example\nDESCRIPTION: Full example showing Heroku source configuration with TCP listening and forwarding to a Loki write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.heroku.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.heroku \"local\" {\n    http {\n        listen_address = \"0.0.0.0\"\n        listen_port    = 4040\n    }\n    use_incoming_timestamp = true\n    labels                 = {component = \"loki.source.heroku\"}\n    forward_to             = [loki.write.local.receiver]\n}\n\nloki.write \"local\" {\n    endpoint {\n        url = \"loki:3100/api/v1/push\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Foreach Block Example with Redis Exporter in Alloy\nDESCRIPTION: Full example of the foreach block proposal processing Redis targets from a discovery component, showing how each target would be processed by its own Redis exporter, relabeled, and scraped.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/design/1443-dynamic-pipelines.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.file \"default\" {\n  files = [\"/Users/batman/Desktop/redis_addresses.yaml\"]\n}\n\n// Every component defined in the \"foreach\" block will be instantiated for each item in the collection. \n// The instantiated components will be scoped using the name of the foreach block and the index of the\n// item in the collection. For example: /foreach.redis/0/prometheus.exporter.redis.default\nforeach \"redis\" {\n  collection = discovery.file.default.targets\n  // Here, \"target\" is a variable whose value is the current item in the collection.\n  var = \"target\"\n\n  prometheus.exporter.redis \"default\" {\n    redis_addr = target[\"__address__\"] // we can also do the necessary rewrites before this.\n  }\n\n  discovery.relabel \"default\" {\n    targets = prometheus.exporter.redis.default.targets\n    // Add a label which comes from the discovery component.\n    rule {\n      target_label = \"filepath\"\n      // __meta_filepath comes from discovery.file\n      replacement  = target[\"__meta_filepath\"]\n    }\n  }\n\n  prometheus.scrape \"default\" {\n    targets = discovery.relabel.default.targets\n    forward_to = prometheus.remote_write.mimir.receiver\n  }\n}\n\nprometheus.remote_write \"mimir\" {\n  endpoint {\n    url = \"https://prometheus-prod-05-gb-south-0.grafana.net/api/prom/push\"\n    basic_auth {\n      username = \"\"\n      password = \"\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Operator ScrapeConfigs with Namespace and Label Selector in Alloy\nDESCRIPTION: This example shows how to configure the prometheus.operator.scrapeconfigs component to discover scrapeconfigs in a specific namespace with a label selector, and forward metrics to a prometheus.remote_write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.operator.scrapeconfigs.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.operator.scrapeconfigs \"scrapeconfigs\" {\n    forward_to = [prometheus.remote_write.staging.receiver]\n    namespaces = [\"my-app\"]\n    selector {\n        match_expression {\n            key = \"team\"\n            operator = \"In\"\n            values = [\"ops\"]\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Advanced File Discovery with Relabeling in Alloy\nDESCRIPTION: Extended example of discovery.file configuration with wildcard file paths and relabeling. It shows how to discover targets from multiple files and retain the file path as a label.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.file.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.file \"example\" {\n  files = [\"/tmp/example_*.yaml\"]\n}\n\ndiscovery.relabel \"keep_filepath\" {\n  targets = discovery.file.example.targets\n  rule {\n    source_labels = [\"__meta_filepath\"]\n    target_label = \"filepath\"\n  }\n}\n\nprometheus.scrape \"default\" {\n  targets    = discovery.relabel.keep_filepath.output\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = <PROMETHEUS_REMOTE_WRITE_URL>\n\n    basic_auth {\n      username = <USERNAME>\n      password = <PASSWORD>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Span Processor in Alloy\nDESCRIPTION: Basic configuration structure for the OpenTelemetry Collector span processor. This component allows modification of span names and attributes, and supports filtering of input data.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.span.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.span \"LABEL\" {\n  output {\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic otelcol.auth.headers Configuration Example\nDESCRIPTION: Basic example showing how to configure custom header authentication using otelcol.auth.headers component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.auth.headers.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.auth.headers \"LABEL\" {\n  header {\n    key   = \"HEADER_NAME\"\n    value = \"HEADER_VALUE\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of loki.source.kafka in Alloy\nDESCRIPTION: This snippet shows the basic usage of the loki.source.kafka component, specifying brokers, topics, and forwarding destinations.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.kafka.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.kafka \"<LABEL>\" {\n  brokers    = \"<BROKER_LIST>\"\n  topics     = \"<TOPIC_LIST>\"\n  forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Uyuni Discovery in Alloy\nDESCRIPTION: Basic configuration for the discovery.uyuni component. It specifies the server, username, and password for authenticating with the Uyuni API.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.uyuni.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.uyuni \"<LABEL>\" {\n    server   = \"<SERVER>\"\n    username = \"<USERNAME>\"\n    password = \"<PASSWORD>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Retry Counter Implementation with Gauge Metric\nDESCRIPTION: Extracts retry counts using regex and updates a gauge metric. Shows how to process numeric values from log lines into metrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_23\n\nLANGUAGE: alloy\nCODE:\n```\nstage.regex {\n    expression = \"^.* retries=(?P<retries>\\\\d+) .*$\"\n}\nstage.metrics {\n    metric.gauge {\n        name        = \"retries_total\"\n        description = \"total_retries\"\n        source      = \"retries\"\n        action      = \"add\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Grafana Agent Static Configuration in YAML\nDESCRIPTION: A sample Grafana Agent Static configuration file in YAML format that includes metrics, logs, and traces configurations which will be used as input for conversion to Alloy format.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-static.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nserver:\n  log_level: info\n\nmetrics:\n  global:\n    scrape_interval: 15s\n    remote_write:\n      - url: https://prometheus-us-central1.grafana.net/api/prom/push\n        basic_auth:\n          username: USERNAME\n          password: PASSWORD\n  configs:\n    - name: test\n      host_filter: false\n      scrape_configs:\n        - job_name: local-agent\n          static_configs:\n            - targets: ['127.0.0.1:12345']\n              labels:\n                cluster: 'localhost'\n\nlogs:\n  global:\n    file_watch_config:\n      min_poll_frequency: 1s\n      max_poll_frequency: 5s\n  positions_directory: /var/lib/agent/data-agent\n  configs:\n    - name: varlogs\n      scrape_configs:\n        - job_name: varlogs\n          static_configs:\n            - targets:\n              - localhost\n              labels:\n                job: varlogs\n                host: mylocalhost\n                __path__: /var/log/*.log\n          pipeline_stages:\n            - match:\n                selector: '{filename=\"/var/log/*.log\"}'\n                stages:\n                - drop:\n                    expression: '^[^0-9]{4}'\n                - regex:\n                    expression: '^(?P<timestamp>\\d{4}/\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}) \\[(?P<level>[[:alpha:]]+)\\] (?:\\d+)\\#(?:\\d+): \\*(?:\\d+) (?P<message>.+)$'\n                - pack:\n                    labels:\n                      - level\n      clients:\n        - url: https://USER_ID:API_KEY@logs-prod3.grafana.net/loki/api/v1/push\n\ntraces:\n  configs:\n    - name: tempo\n      receivers:\n        otlp:\n          protocols:\n            grpc:\n            http:\n      batch:\n        send_batch_size: 10000\n        timeout: 20s\n      remote_write:\n        - endpoint: tempo-us-central1.grafana.net:443\n          basic_auth:\n            username: USERNAME\n            password: PASSWORD\n```\n\n----------------------------------------\n\nTITLE: Complete Nerve Discovery Implementation with Prometheus Integration\nDESCRIPTION: Full example showing discovery.nerve configuration with Prometheus scraping and remote write setup. Includes timeout configuration and authentication settings.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.nerve.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.nerve \"example\" {\n    servers = [\"localhost\"]\n    paths   = [\"/monitoring\"]\n    timeout = \"1m\"\n}\nprometheus.scrape \"demo\" {\n    targets    = discovery.nerve.example.targets\n    forward_to = [prometheus.remote_write.demo.receiver]\n}\nprometheus.remote_write \"demo\" {\n    endpoint {\n        url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n        basic_auth {\n            username = \"<USERNAME>\"\n            password = \"<PASSWORD>\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Advanced OTLP Exporter with OAuth2 Configuration\nDESCRIPTION: Extended example demonstrating OAuth2 configuration with optional parameters including endpoint_params, scopes, and timeout settings.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.auth.oauth2.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlp \"example\" {\n  client {\n    endpoint = \"my-otlp-grpc-server:4317\"\n    auth     = otelcol.auth.oauth2.creds.handler\n  }\n}\n\notelcol.auth.oauth2 \"creds\" {\n    client_id       = \"someclientid2\"\n    client_secret   = \"someclientsecret2\"\n    token_url       = \"https://example.com/oauth2/default/v1/token\"\n    endpoint_params = {\"audience\" = [\"someaudience\"]}\n    scopes          = [\"api.metrics\"]\n    timeout         = \"3600s\"\n}\n```\n\n----------------------------------------\n\nTITLE: K8s Metadata for Prometheus Metrics in Alloy\nDESCRIPTION: This configuration demonstrates how to add Kubernetes metadata to Prometheus metrics. It uses the K8s attributes processor to add metadata, then uses a transform processor to convert resource attributes to datapoint attributes for Prometheus compatibility.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.k8sattributes.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  http {}\n  grpc {}\n\n  output {\n    metrics = [otelcol.processor.k8sattributes.default.input]\n  }\n}\n\notelcol.processor.k8sattributes \"default\" {\n  extract {\n    label {\n      from = \"pod\"\n    }\n\n    metadata = [\n      \"k8s.namespace.name\",\n      \"k8s.pod.name\",\n    ]\n  }\n\n  output {\n    metrics = [otelcol.processor.transform.add_kube_attrs.input]\n  }\n}\n\notelcol.processor.transform \"add_kube_attrs\" {\n  error_mode = \"ignore\"\n\n  metric_statements {\n    context = \"datapoint\"\n    statements = [\n      \"set(attributes[\\\"k8s.pod.name\\\"], resource.attributes[\\\"k8s.pod.name\\\"])\",\n      \"set(attributes[\\\"k8s.namespace.name\\\"], resource.attributes[\\\"k8s.namespace.name\\\"])\",\n    ]\n  }\n\n  output {\n    metrics = [otelcol.exporter.prometheus.default.input]\n  }\n}\n\notelcol.exporter.prometheus \"default\" {\n  forward_to = [prometheus.remote_write.mimir.receiver]\n}\n\nprometheus.remote_write \"mimir\" {\n  endpoint {\n    url = \"http://mimir:9009/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Static Labels to Log Entries\nDESCRIPTION: Configures a static_labels stage that adds predetermined key-value pairs to all incoming log entries, enhancing logs with consistent metadata.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_34\n\nLANGUAGE: alloy\nCODE:\n```\nstage.static_labels {\n    values = {\n      foo = \"fooval\",\n      bar = \"barval\",\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Pyroscope Scrape Block in YAML\nDESCRIPTION: This snippet demonstrates the structure of a pyroscope.scrape block in YAML configuration. It includes various sub-blocks for authorization, profiling configuration, and TLS settings.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.scrape.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npyroscope.scrape:\n  scrape_interval: \"60s\"\n  authorization:\n    # Authorization settings\n  basic_auth:\n    # Basic auth settings\n  clustering:\n    enabled: false\n  oauth2:\n    # OAuth2 settings\n    tls_config:\n      # TLS settings for OAuth2\n  profiling_config:\n    path_prefix: \"/debug/pprof\"\n    profile.block:\n      delta: false\n      enabled: true\n      path: \"/debug/pprof/block\"\n    # Other profile configurations\n  tls_config:\n    # TLS settings\n```\n\n----------------------------------------\n\nTITLE: Basic Nerve Discovery Configuration in Alloy\nDESCRIPTION: Basic configuration structure for the discovery.nerve component showing required servers and paths arguments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.nerve.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.nerve \"<LABEL>\" {\n    servers = [\"<SERVER_1>\", \"<SERVER_2>\"]\n    paths   = [\"<PATH_1>\", \"<PATH_2>\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SNMP Exporter with Target List in Alloy\nDESCRIPTION: This example shows an alternative approach to configure prometheus.exporter.snmp by providing targets as a list of maps instead of using individual target blocks, which is useful for programmatic target generation.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.snmp.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.snmp \"example\" {\n    config_file = \"snmp_modules.yml\"\n\n    targets = [\n        {\n            \"name\"        = \"network_switch_1\",\n            \"address\"     = \"192.168.1.2\",\n            \"module\"      = \"system,if_mib\",\n            \"walk_params\" = \"public\",\n            \"env\"         = \"dev\",\n        },\n        {\n            \"name\"        = \"network_router_2\",\n            \"address\"     = \"192.168.1.3\",\n            \"module\"      = \"system,if_mib,mikrotik\",\n            \"walk_params\" = \"private\",\n        },\n    ]\n\n    walk_param \"private\" {\n        retries = \"2\"\n    }\n\n    walk_param \"public\" {\n        retries = \"2\"\n    }\n}\n\n// Configure a prometheus.scrape component to collect SNMP metrics.\nprometheus.scrape \"demo\" {\n    targets    = prometheus.exporter.snmp.example.targets\n    forward_to = [ /* ... */ ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring GCP Monitoring Exporter in Alloy\nDESCRIPTION: Example configuration for the prometheus.exporter.gcp component to collect PubSub metrics from GCP projects. Demonstrates setting project IDs and specific metrics prefixes for monitoring.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.gcp.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.gcp \"pubsub\" {\n        project_ids = [\n                \"<PROJECT_ID_1>\",\n                \"<PROJECT_ID_2>\",\n        ]\n\n        metrics_prefixes = [\n                \"pubsub.googleapis.com/snapshot\",\n                \"pubsub.googleapis.com/subscription/num_undelivered_messages\",\n                \"pubsub.googleapis.com/subscription/oldest_unacked_message_age\",\n        ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Pipeline with File Read and Prometheus Remote Write\nDESCRIPTION: Example pipeline that reads a file's content and uses it as a password for Prometheus remote write authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/first-components-and-stdlib.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file \"example\" {\n    filename = sys.env(\"HOME\") + \"/file.txt\"\n}\n\nprometheus.remote_write \"local_prom\" {\n    endpoint {\n        url = \"http://localhost:9090/api/v1/write\"\n\n        basic_auth {\n            username = \"admin\"\n            password = local.file.example.content\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Rules for Kubernetes with Local Loki Instance\nDESCRIPTION: This snippet demonstrates how to create a loki.rules.kubernetes component that loads discovered rules to a local Loki instance. It specifies the address, tenant ID, and selectors for namespaces and rules based on labels.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.rules.kubernetes.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.rules.kubernetes \"local\" {\n    address = \"loki:3100\"\n    tenant_id = \"team-a\"\n\n    rule_namespace_selector {\n        match_labels = {\n            alloy = \"yes\",\n        }\n    }\n\n    rule_selector {\n        match_labels = {\n            alloy = \"yes\",\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Error Logs with Loki Relabel\nDESCRIPTION: Example showing how to configure loki.relabel to only forward log entries with 'level' set to 'error'.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.relabel.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.relabel \"keep_error_only\" {\n  forward_to = [loki.write.onprem.receiver]\n\n  rule {\n    action        = \"keep\"\n    source_labels = [\"level\"]\n    regex         = \"error\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Targets for Profile Collection\nDESCRIPTION: Example configuration for setting up scrape jobs with static targets, including Pyroscope itself and Alloy. Shows how to configure targets and forward collected profiles to a Pyroscope database.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.scrape.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\npyroscope.scrape \"local\" {\n  targets = [\n    {\"__address__\" = \"localhost:4040\", \"service_name\"=\"pyroscope\"},\n    {\"__address__\" = \"localhost:12345\", \"service_name\"=\"alloy\"},\n  ]\n\n  forward_to = [pyroscope.write.local.receiver]\n}\n\npyroscope.write \"local\" {\n  endpoint {\n    url = \"http://pyroscope:4040\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: OTLP Attributes to Loki Labels Conversion Configuration\nDESCRIPTION: Illustrates how to configure attribute processing to convert specific OTLP resource and log attributes into Loki labels.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.loki.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  grpc {}\n\n  output {\n    logs = [otelcol.processor.attributes.default.input]\n  }\n}\n\notelcol.processor.attributes \"default\" {\n  action {\n    key = \"loki.attribute.labels\"\n    action = \"insert\"\n    value = \"event.domain, event.name\"\n  }\n\n  action {\n    key = \"loki.resource.labels\"\n    action = \"insert\"\n    value = \"service.name, service.namespace\"\n  }\n\n  output {\n    logs = [otelcol.exporter.loki.default.input]\n  }\n}\n\notelcol.exporter.loki \"default\" {\n  forward_to = [loki.write.local.receiver]\n}\n\nloki.write \"local\" {\n  endpoint {\n      url = \"loki:3100\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Forwarding Prometheus Metrics to Datadog\nDESCRIPTION: Configuration example showing how to forward Prometheus metrics through an OpenTelemetry receiver to Datadog. Sets up a Prometheus self-exporter, scrape job, and Datadog exporter with API authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.datadog.md#2025-04-22_snippet_9\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.self \"default\" {\n}\n\nprometheus.scrape \"metamonitoring\" {\n  targets    = prometheus.exporter.self.default.targets\n  forward_to = [otelcol.receiver.prometheus.default.receiver]\n}\n\notelcol.receiver.prometheus \"default\" {\n  output {\n    metrics = [otelcol.exporter.datadog.default.input]\n  }\n}\n\n\notelcol.exporter.datadog \"default\" {\n    api {\n        api_key = \"API_KEY\"\n    }\n\n     metrics {\n        endpoint = \"https://api.ap1.datadoghq.com\"\n        resource_attributes_as_tags = true\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Source for File Reading\nDESCRIPTION: Configuration for the loki.source.file component which reads log entries from files and forwards them to other Loki components. It specifies the targets to read from and where to forward the logs.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-logs-from-file.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.file \"log_scrape\" {\n    targets    = local.file_match.local_files.targets\n    forward_to = [loki.write.local.receiver]\n    tail_from_end = true\n}\n```\n\n----------------------------------------\n\nTITLE: Delta to Cumulative with Prometheus Export Configuration\nDESCRIPTION: Configuration example for converting delta metrics to cumulative before exporting to Prometheus format and remote write endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.deltatocumulative.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.deltatocumulative \"default\" {\n  output {\n    metrics = [otelcol.exporter.prometheus.default.input]\n  }\n}\n\notelcol.exporter.prometheus \"default\" {\n  forward_to = [prometheus.remote_write.default.receiver]\n}\n\nprometheus.remote_write \"default\" {\n  endpoint {\n    url = sys.env(\"PROMETHEUS_SERVER_URL\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Filtered Prometheus Operator Probes with Label Selector in Alloy\nDESCRIPTION: An example that limits discovered Probes to those with the label 'team=ops' in the 'my-app' namespace. This demonstrates how to use namespace filtering and label selectors to target specific Probes.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.operator.probes.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.operator.probes \"pods\" {\n    forward_to = [prometheus.remote_write.staging.receiver]\n    namespaces = [\"my-app\"]\n    selector {\n        match_expression {\n            key = \"team\"\n            operator = \"In\"\n            values = [\"ops\"]\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of otelcol.auth.basic in Alloy\nDESCRIPTION: Simple configuration example showing how to set up basic authentication with username and password.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.auth.basic.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.auth.basic \"LABEL\" {\n  username = \"USERNAME\"\n  password = \"PASSWORD\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Grafana Alloy Using Ansible Playbook in YAML\nDESCRIPTION: This Ansible playbook installs Grafana Alloy on all hosts with a basic configuration that collects metrics and forwards them to Prometheus. It uses the grafana.grafana.alloy role with a configuration that sets up a scrape job and remote write endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/ansible.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Install Alloy\n  hosts: all\n  become: true\n\n  tasks:\n    - name: Install Alloy\n      ansible.builtin.include_role:\n        name: grafana.grafana.alloy\n      vars:\n        alloy_config: |\n          prometheus.scrape \"default\" {\n            targets = [{\"__address__\" = \"localhost:12345\"}]\n            forward_to = [prometheus.remote_write.prom.receiver]\n          }\n          prometheus.remote_write \"prom\" {\n            endpoint {\n                url = \"<YOUR_PROMETHEUS_PUSH_ENDPOINT>\"\n            }\n          }\n```\n\n----------------------------------------\n\nTITLE: Basic Windows Exporter Configuration in Alloy\nDESCRIPTION: Basic usage example of the prometheus.exporter.windows component. The component requires a label parameter and supports optional configuration blocks for various collectors.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.windows.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.windows \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Push-based GCP Log Source with Custom HTTP Port\nDESCRIPTION: Configuration example showing how to set up a GCP log source using push strategy with a custom HTTP port (4040), forwarding logs to a local Loki write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.gcplog.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.gcplog \"local\" {\n  push {\n    http {\n        listen_port = 4040\n    }\n  }\n\n  forward_to = [loki.write.local.receiver]\n}\n\nloki.write \"local\" {\n  endpoint {\n    url = \"loki:3100/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Prometheus Remote Write Integration Configuration\nDESCRIPTION: Configuration example demonstrating how to forward InfluxDB metrics to Prometheus Remote Write (Mimir) through the OpenTelemetry Collector.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.influxdb.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.influxdb \"influxdb_metrics\" {\n  output {\n    metrics = [otelcol.exporter.prometheus.influx_output.input]  // Forward metrics to Prometheus exporter\n  }\n}\n\notelcol.exporter.prometheus \"influx_output\" {\n  forward_to = [prometheus.remote_write.mimir.receiver]  // Forward metrics to Prometheus remote write (Mimir)\n}\n\nprometheus.remote_write \"mimir\" {\n  endpoint {\n    url = \"https://prometheus-xxx.grafana.net/api/prom/push\"\n\n    basic_auth {\n      username = \"xxxxx\"\n      password = \"xxxx==\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP HTTP Exporter in Alloy\nDESCRIPTION: Basic configuration example for setting up an OTLP HTTP exporter component. This snippet shows how to define the exporter with a label and configure the client endpoint where telemetry data will be sent.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.otlphttp.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlphttp \"LABEL\" {\n  client {\n    endpoint = \"HOST:PORT\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering ServiceMonitors by Label and Namespace in Alloy\nDESCRIPTION: This example demonstrates how to limit discovered ServiceMonitors to those with specific labels in a specific namespace using match expressions.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.operator.servicemonitors.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.operator.servicemonitors \"services\" {\n    forward_to = [prometheus.remote_write.staging.receiver]\n    namespaces = [\"my-app\"]\n    selector {\n        match_expression {\n            key = \"team\"\n            operator = \"In\"\n            values = [\"ops\"]\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Loki Source API Implementation\nDESCRIPTION: Full example showing loki.source.api configuration with loki.write integration, including HTTP server setup, authentication, and label forwarding.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.api.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.write \"local\" {\n    endpoint {\n        url = \"http://loki:3100/api/v1/push\"\n        basic_auth {\n            username = \"<USERNAME>\"\n            password_file = \"<PASSWORD_FILE>\"\n        }\n    }\n}\n\nloki.source.api \"loki_push_api\" {\n    http {\n        listen_address = \"0.0.0.0\"\n        listen_port = 9999\n    }\n    forward_to = [\n        loki.write.local.receiver,\n    ]\n    labels = {\n        forwarded = \"true\",\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Loki Heroku Source Configuration\nDESCRIPTION: Basic configuration structure for setting up a Heroku log source in Loki with HTTP listener settings.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.heroku.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.heroku \"<LABEL>\" {\n    http {\n        listen_address = \"<LISTEN_ADDRESS>\"\n        listen_port    = \"<LISTEN_PORT>\"\n    }\n    forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Processor in Grafana Alloy\nDESCRIPTION: Sets up the otelcol.processor.batch component which batches telemetry data before sending it to the OTLP exporter. This improves efficiency by reducing the number of outgoing requests.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/datadog-traces-metrics.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.batch \"default\" {\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of prometheus.exporter.oracledb in Alloy\nDESCRIPTION: Demonstrates the basic syntax for configuring the prometheus.exporter.oracledb component. It requires a label and a connection string to connect to an Oracle Database.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.oracledb.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.oracledb \"<LABEL>\" {\n    connection_string = \"<CONNECTION_STRING>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Prometheus Exporter Self Component in Alloy\nDESCRIPTION: Adds a prometheus.exporter.self component to expose Alloy's internal metrics. This component requires a unique label but accepts no arguments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/metamonitoring.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.self \"<SELF_LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Combining Static and Dynamic Targets for Pyroscope Scraping\nDESCRIPTION: Shows how to combine static targets with dynamically discovered targets using array.concat. This configuration scrapes both predefined services and dynamically discovered endpoints, forwarding the collected profiles to a Pyroscope instance.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.scrape.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.http \"dynamic_targets\" {\n  url = \"https://example.com/scrape_targets\"\n  refresh_interval = \"15s\"\n}\n\npyroscope.scrape \"local\" {\n  targets = array.concat([\n    {\"__address__\" = \"localhost:4040\", \"service_name\"=\"pyroscope\"},\n    {\"__address__\" = \"localhost:12345\", \"service_name\"=\"alloy\"},\n  ], discovery.http.dynamic_targets.targets)\n\n  forward_to = [pyroscope.write.local.receiver]\n}\n\npyroscope.write \"local\" {\n  endpoint {\n    url = \"http://pyroscope:4040\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Mimir Rules Kubernetes Configuration\nDESCRIPTION: Basic configuration example showing how to set up mimir.rules.kubernetes component with a Mimir ruler URL. This is the minimal required configuration to connect to a Mimir instance.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/mimir/mimir.rules.kubernetes.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nmimir.rules.kubernetes \"<LABEL>\" {\n  address = \"<MIMIR_RULER_URL>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Probabilistic Sampler Configuration in Alloy\nDESCRIPTION: Basic configuration for the OpenTelemetry probabilistic sampler processor showing usage with output configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.probabilistic_sampler.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.probabilistic_sampler \"LABEL\" {\n  output {\n    logs    = [...]\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL Exporter and Prometheus Scrape in Alloy\nDESCRIPTION: This snippet demonstrates how to set up a MySQL exporter, configure Prometheus to scrape metrics from it, and send those metrics to a remote write endpoint. It includes configuring the data source, enabling specific collectors, and setting up authentication for the remote write API.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.mysql.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.mysql \"example\" {\n  data_source_name  = \"root@(server-a:3306)/\"\n  enable_collectors = [\"heartbeat\", \"mysql.user\"]\n}\n\n// Configure a prometheus.scrape component to collect mysql metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.mysql.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Journal Source\nDESCRIPTION: Configuration for collecting logs from systemd journal and forwarding them to Loki.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-linux.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.journal \"logs_integrations_integrations_node_exporter_journal_scrape\" {\n  max_age       = \"24h0m0s\"\n  relabel_rules = discovery.relabel.logs_integrations_integrations_node_exporter_journal_scrape.rules\n  forward_to    = [loki.write.local.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Using foreach with Prometheus Exporters and Kubernetes Service Discovery\nDESCRIPTION: Example configuration demonstrating how to use foreach to loop over Redis instances discovered by Kubernetes and start separate Prometheus exporter pipelines for each.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/foreach.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubernetes \"default\" {\n    role = \"pod\"\n}\n\ndiscovery.relabel \"redis\" {\n    targets = discovery.kubernetes.default.targets\n\n    // Remove all targets except the Redis ones.\n    rule {\n        source_labels = [\"__meta_kubernetes_pod_container_name\"]\n        regex         = \"redis-cont\"\n        action        = \"keep\"\n    }\n}\n\n// Collect metrics for each Redis instance.\nforeach \"redis\" {\n    collection = discovery.relabel.redis.output\n    var        = \"each\"\n\n    template {\n        prometheus.exporter.redis \"default\" {\n            // This is the \"__address__\" label from discovery.kubernetes.\n            redis_addr = each[\"__address__\"]\n        }\n\n        prometheus.scrape \"default\" {\n            targets    = prometheus.exporter.redis.default.targets\n            forward_to = [prometheus.relabel.default.receiver]\n        }\n\n        // Add labels from discovery.kubernetes.\n        prometheus.relabel \"default\" {\n            rule {\n                replacement  = each[\"__meta_kubernetes_namespace\"]\n                target_label = \"k8s_namespace\"\n                action       = \"replace\"\n            }\n\n            rule {\n                replacement  = each[\"__meta_kubernetes_pod_container_name\"]\n                target_label = \"k8s_pod_container_name\"\n                action       = \"replace\"\n            }\n\n            forward_to = [prometheus.remote_write.mimir.receiver]\n        }\n    }\n}\n\nprometheus.remote_write \"mimir\" {\n    endpoint {\n        url = \"https://prometheus-xxx.grafana.net/api/prom/push\"\n\n        basic_auth {\n            username = sys.env(\"<PROMETHEUS_USERNAME>\")\n            password = sys.env(\"<GRAFANA_CLOUD_API_KEY>\")\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Rules for Kubernetes with Grafana Cloud\nDESCRIPTION: This snippet shows how to create a loki.rules.kubernetes component that loads discovered rules to Grafana Cloud. It includes configuration for the address and basic authentication with username and password or password file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.rules.kubernetes.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nloki.rules.kubernetes \"default\" {\n    address = \"<GRAFANA_CLOUD_URL>\"\n    basic_auth {\n        username = \"<GRAFANA_CLOUD_USER>\"\n        password = \"<GRAFANA_CLOUD_API_KEY>\"\n        // Alternatively, load the password from a file:\n        // password_file = \"<GRAFANA_CLOUD_API_KEY_PATH>\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting span status based on attribute value in Alloy\nDESCRIPTION: This configuration demonstrates how to set a span status to 'Ok' only when a specific attribute matches a given value.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.span.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.span \"default\" {\n  include {\n    match_type = \"strict\"\n    attribute {\n      key   = \"http.status_code\"\n      value = 400\n    }\n  }\n  status {\n    code = \"Ok\"\n  }\n\n  output {\n      traces = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubelet Discovery with Bearer Token Authentication\nDESCRIPTION: Sets up Kubelet API discovery using a bearer token file for authentication, with Prometheus scraping and remote write configuration. The bearer token is read from a Kubernetes service account token file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.kubelet.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubelet \"k8s_pods\" {\n  bearer_token_file = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.kubelet.k8s_pods.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"PROMETHEUS_REMOTE_WRITE_URL\"\n\n    basic_auth {\n      username = \"USERNAME\"\n      password = \"PASSWORD\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Redis Exporter Setup with Prometheus Scraping\nDESCRIPTION: Comprehensive example showing Redis exporter configuration integrated with Prometheus scraping and remote write setup. Includes authentication and target configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.redis.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.redis \"example\" {\n  redis_addr = \"localhost:6379\"\n}\n\n// Configure a prometheus.scrape component to collect Redis metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.redis.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS IAM Permissions for CloudWatch Exporter\nDESCRIPTION: This JSON snippet defines an AWS IAM policy that grants the necessary permissions for the CloudWatch exporter to function fully, including access to CloudWatch metrics, EC2 resources, API Gateway, and Database Migration Service.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.cloudwatch.md#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Stmt1674249227793\",\n      \"Action\": [\n        \"tag:GetResources\",\n        \"cloudwatch:GetMetricData\",\n        \"cloudwatch:GetMetricStatistics\",\n        \"cloudwatch:ListMetrics\",\n        \"ec2:DescribeTags\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeRegions\",\n        \"ec2:DescribeTransitGateway*\",\n        \"apigateway:GET\",\n        \"dms:DescribeReplicationInstances\",\n        \"dms:DescribeReplicationTasks\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Basic InfluxDB Receiver Configuration in Alloy\nDESCRIPTION: Basic configuration example showing how to set up an InfluxDB receiver with endpoint configuration and metric output routing.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.influxdb.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.influxdb \"influxdb_metrics\" {\n  endpoint = \"localhost:8086\"  // InfluxDB metrics ingestion endpoint\n\n  output {\n    metrics = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.receiver.solace in Alloy\nDESCRIPTION: This snippet demonstrates the basic usage of the otelcol.receiver.solace component, including queue configuration, authentication, and output specification.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.solace.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.solace \"LABEL\" {\n  queue = \"QUEUE\"\n  auth {\n    // sasl_plain or sasl_xauth2 or sasl_external block\n  }\n  output {\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complex Example: Combining remote.kubernetes.secret with Other Components\nDESCRIPTION: Illustrates how to use remote.kubernetes.secret along with remote.kubernetes.configmap to supply credentials for a Prometheus remote write configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/remote/remote.kubernetes.secret.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nremote.kubernetes.secret \"credentials\" {\n  namespace = \"monitoring\"\n  name = \"metrics-secret\"\n}\n\nremote.kubernetes.configmap \"endpoint\" {\n  namespace = \"monitoring\"\n  name = \"metrics-endpoint\"\n}\n\nprometheus.remote_write \"default\" {\n  endpoint {\n    url = remote.kubernetes.configmap.endpoint.data[\"url\"]\n    basic_auth {\n      username = convert.nonsensitive(remote.kubernetes.configmap.endpoint.data[\"username\"])\n      password = remote.kubernetes.secret.credentials.data[\"password\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring EC2 Resource Detection with Explicit Attributes in Alloy\nDESCRIPTION: This example shows EC2 resource detection with explicitly configured tag filters and resource attributes. It specifies which EC2 tags to collect and enables/disables specific cloud and host resource attributes.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.resourcedetection.md#2025-04-22_snippet_10\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.resourcedetection \"default\" {\n  detectors = [\"ec2\"]\n  ec2 {\n    tags = [\"^tag1$\", \"^tag2$\", \"^label.*$\"]\n    resource_attributes {\n      cloud.account.id  { enabled = true }\n      cloud.availability_zone  { enabled = true }\n      cloud.platform  { enabled = true }\n      cloud.provider  { enabled = true }\n      cloud.region  { enabled = true }\n      host.id  { enabled = true }\n      host.image.id  { enabled = false }\n      host.name  { enabled = false }\n      host.type  { enabled = false }\n    }\n  }\n\n  output {\n    logs    = [otelcol.exporter.otlp.default.input]\n    metrics = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Full Uyuni Discovery and Prometheus Scraping Example\nDESCRIPTION: Comprehensive example showing Uyuni discovery configuration, Prometheus scraping of discovered targets, and remote write setup. Includes placeholders for server details and authentication credentials.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.uyuni.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.uyuni \"example\" {\n  server    = \"https://127.0.0.1/rpc/api\"\n  username  = \"<UYUNI_USERNAME>\"\n  password  = \"<UYUNI_PASSWORD>\"\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.uyuni.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Controlling Profile Types in Pyroscope Scraping\nDESCRIPTION: Demonstrates how to selectively enable and disable specific profile types using the profiling_config block. This configuration enables fgprof while explicitly disabling block and mutex profiling for the target service.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.scrape.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\npyroscope.scrape \"local\" {\n  targets = [\n    {\"__address__\" = \"localhost:12345\", \"service_name\"=\"alloy\"},\n  ]\n\n  profiling_config {\n    profile.fgprof {\n      enabled = true\n    }\n    profile.block {\n      enabled = false\n    }\n    profile.mutex {\n      enabled = false\n    }\n  }\n\n  forward_to = [pyroscope.write.local.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Scrape in Alloy\nDESCRIPTION: Alloy configuration for the prometheus.scrape component that scrapes cAdvisor metrics and forwards them to a remote_write receiver at a specified interval.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-docker-containers.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"scraper\" {\n  targets    = prometheus.exporter.cadvisor.example.targets\n  forward_to = [ prometheus.remote_write.demo.receiver ]\n\n  scrape_interval = \"10s\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic remote.s3 Configuration in Alloy\nDESCRIPTION: Basic usage example showing how to configure a remote.s3 component to access a file from S3. The component requires a label and path argument specifying the S3 file location.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/remote/remote.s3.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nremote.s3 \"<LABEL>\" {\n  path = \"<S3_FILE_PATH>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Discovering Systemd Services with discovery.process and discovery.relabel in Alloy\nDESCRIPTION: Shows how to use discovery.process in combination with discovery.relabel to discover processes running under systemd services on the local host.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.process.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.process \"all\" {\n  refresh_interval = \"60s\"\n  discover_config {\n    cwd = true\n    exe = true\n    commandline = true\n    username = true\n    uid = true\n    cgroup_path = true\n    container_id = true\n  }\n}\n\ndiscovery.relabel \"systemd_services\" {\n  targets = discovery.process.all.targets\n  // Only keep the targets that correspond to systemd services\n  rule {\n    action = \"keep\"\n    regex = \"^.*/([a-zA-Z0-9-_]+).service(?:.*$)\"\n    source_labels = [\"__meta_cgroup_id\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring prometheus.exporter.catchpoint in Alloy\nDESCRIPTION: This snippet shows the basic usage of the prometheus.exporter.catchpoint component, including its available arguments for configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.catchpoint.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.catchpoint \"<LABEL>\" {\n    port              = \"<PORT>\"\n    verbose_logging   = <VERBOSE_LOGGING>\n    webhook_path      = \"<WEBHOOK_PATH>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Extracted Values into a New Field\nDESCRIPTION: Creates a new field by combining multiple existing extracted values into a formatted template string, demonstrating complex data transformation.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_38\n\nLANGUAGE: alloy\nCODE:\n```\nstage.template {\n    source   = \"output_msg\"\n    template = \"{{ .level }} for app {{ ToUpper .app }} in module {{.module}}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Target Discovery YAML Configuration\nDESCRIPTION: YAML configuration example for target discovery file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.blackbox.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n- targets:\n  - localhost:9009\n  labels:\n    name: t1\n    module: http_2xx\n    other_label: example\n- targets:\n  - localhost:9009\n  labels:\n    name: t2\n    module: http_2xx\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Scraping\nDESCRIPTION: Configuration for scraping node_exporter metrics and forwarding them to a receiver.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-linux.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"integrations_node_exporter\" {\n  scrape_interval = \"15s\"\n  targets    = discovery.relabel.integrations_node_exporter.output\n  forward_to = [prometheus.remote_write.local.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.receiver.prometheus in Alloy\nDESCRIPTION: This snippet demonstrates the basic usage of the otelcol.receiver.prometheus component. It shows how to define the component with a label and configure its output.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.prometheus.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.prometheus \"LABEL\" {\n  output {\n    metrics = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Application and Network Filters in Alloy\nDESCRIPTION: Example configuration showing how to set up application and network filters. The application filter matches URL paths starting with '/user/' while the network filter matches all Kubernetes source owner names.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/beyla/beyla.ebpf.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nfilters {\n\tapplication {\n\t  attr = \"url.path\"\n\t  match = \"/user/*\"\n\t}\n\tnetwork {\n\t  attr = \"k8s.src.owner.name\"\n\t  match = \"*\"\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Write in Alloy\nDESCRIPTION: Set up the loki.write component to send logs to a Loki destination.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-windows.md#2025-04-22_snippet_9\n\nLANGUAGE: alloy\nCODE:\n```\nloki.write \"endpoint\" {\n    endpoint {\n        url =\"http://localhost:3100/loki/api/v1/push\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Metrics Path and Scrape Settings in JSON\nDESCRIPTION: JSON example demonstrating how to specify custom metrics path, scrape interval, and timeout for a target in the discovery.http response.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.http.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n   {\n      \"labels\" : {\n         \"__metrics_path__\" : \"/api/prometheus\",\n         \"__scheme__\" : \"https\",\n         \"__scrape_interval__\" : \"60s\",\n         \"__scrape_timeout__\" : \"10s\",\n         \"service\" : \"custom-api-service\"\n      },\n      \"targets\" : [\n         \"custom-api:443\"\n      ]\n   },\n]\n```\n\n----------------------------------------\n\nTITLE: Writing Logs to Loki with loki.write in Alloy\nDESCRIPTION: This component configures a Loki writer that sends logs to a Loki instance. It specifies the endpoint URL and includes commented-out authentication options for reference in other environments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/send-logs-to-loki.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\n  loki.write \"grafana_loki\" {\n    endpoint {\n      url = \"http://localhost:3100/loki/api/v1/push\"\n\n      // basic_auth {\n      //  username = \"admin\"\n      //  password = \"admin\"\n      // }\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of otelcol.processor.filter\nDESCRIPTION: Basic configuration structure for the OpenTelemetry Collector filter processor component showing the required output block.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.filter.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.filter \"LABEL\" {\n  output {\n    metrics = [...]\n    logs    = [...]\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Log Filter Custom Component in Alloy\nDESCRIPTION: This snippet demonstrates defining a custom 'log_filter' component in Alloy that filters out debug and info level log lines. It accepts a required 'write_to' argument specifying where filtered logs should be sent, and exports a 'filter_input' value for receiving logs from the module consumer.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/modules.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndeclare \"log_filter\" {\n  // argument.write_to is a required argument that specifies where filtered\n  // log lines are sent.\n  //\n  // The value of the argument is retrieved in this file with\n  // argument.write_to.value.\n  argument \"write_to\" {\n    optional = false\n  }\n\n  // loki.process.filter is our component which executes the filtering,\n  // passing filtered logs to argument.write_to.value.\n  loki.process \"filter\" {\n    // Drop all debug- and info-level logs.\n    stage.match {\n      selector = `{job!=\"\"} |~ \"level=(debug|info)\"`\n      action   = \"drop\"\n    }\n\n    // Send processed logs to our argument.\n    forward_to = argument.write_to.value\n  }\n\n  // export.filter_input exports a value to the module consumer.\n  export \"filter_input\" {\n    // Expose the receiver of loki.process so the module importer can send\n    // logs to our loki.process component.\n    value = loki.process.filter.receiver\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Nested File Import Example in Alloy\nDESCRIPTION: Example of importing a module from a file inside another module that's imported via import.file. Shows how to establish a chain of imports for component reuse.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/import.file.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\nimport.file \"math\" {\n  filename = \"path/to/module/relative_math.alloy\"\n}\n\nmath.add \"default\" {\n  a = 15\n  b = 45\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing otelcol.receiver.zipkin in Alloy\nDESCRIPTION: Basic usage of the otelcol.receiver.zipkin component, specifying the output for traces.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.zipkin.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.zipkin \"LABEL\" {\n  output {\n    traces = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying Local Grafana Stack with Docker Compose for Loki and Prometheus\nDESCRIPTION: Docker Compose configuration that sets up a local development environment with Grafana, Loki, and Prometheus. The configuration includes automatic datasource provisioning for both Loki and Prometheus.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/send-logs-to-loki.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '3'\nservices:\n  loki:\n    image: grafana/loki:3.0.0\n    ports:\n      - \"3100:3100\"\n    command: -config.file=/etc/loki/local-config.yaml\n  prometheus:\n    image: prom/prometheus:v2.47.0\n    command:\n      - --web.enable-remote-write-receiver\n      - --config.file=/etc/prometheus/prometheus.yml\n    ports:\n      - \"9090:9090\"\n  grafana:\n    environment:\n      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning\n      - GF_AUTH_ANONYMOUS_ENABLED=true\n      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin\n    entrypoint:\n      - sh\n      - -euc\n      - |\n        mkdir -p /etc/grafana/provisioning/datasources\n        cat <<EOF > /etc/grafana/provisioning/datasources/ds.yaml\n        apiVersion: 1\n        datasources:\n        - name: Loki\n          type: loki\n          access: proxy\n          orgId: 1\n          url: http://loki:3100\n          basicAuth: false\n          isDefault: false\n          version: 1\n          editable: false\n        - name: Prometheus\n          type: prometheus\n          orgId: 1\n          url: http://prometheus:9090\n          basicAuth: false\n          isDefault: true\n          version: 1\n          editable: false\n        EOF\n        /run.sh\n    image: grafana/grafana:11.0.0\n    ports:\n      - \"3000:3000\"\n```\n\n----------------------------------------\n\nTITLE: Configuring loki.source.syslog in Alloy\nDESCRIPTION: Basic usage template for configuring a loki.source.syslog component in Alloy. It defines the component label, listener configuration, and forward_to list.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.syslog.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.syslog \"<LABEL>\" {\n  listener {\n    address = \"<LISTEN_ADDRESS>\"\n  }\n  ...\n\n  forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Complete OVHcloud Discovery with Prometheus Integration\nDESCRIPTION: Complete example showing OVHcloud discovery configuration integrated with Prometheus scraping and remote write setup.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.ovhcloud.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.ovhcloud \"example\" {\n    application_key    = \"<APPLICATION_KEY>\"\n    application_secret = \"<APPLICATION_SECRET>\"\n    consumer_key       = \"<CONSUMER_KEY>\"\n    service            = \"<SERVICE>\"\n}\n\nprometheus.scrape \"demo\" {\n    targets    = discovery.ovhcloud.example.targets\n    forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n    endpoint {\n        url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n        basic_auth {\n            username = \"<USERNAME>\"\n            password = \"<PASSWORD>\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Request Metric in otelcol.connector.spanmetrics\nDESCRIPTION: Example of a 'calls' metric with 'Ok' status code generated by otelcol.connector.spanmetrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.spanmetrics.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncalls { service.name=\"shipping\", span.name=\"get_shipping/{shippingId}\", span.kind=\"SERVER\", status.code=\"Ok\" }\n```\n\n----------------------------------------\n\nTITLE: Configuration Example for Prometheus Scrape Protocols\nDESCRIPTION: Example of scrape_protocols configuration for enabling Prometheus Protocol Buffers with native histogram support. This replaces the deprecated enable_protobuf_negotiation setting.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.scrape.md#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nscrape_protocols = [\"PrometheusProto\", \"OpenMetricsText1.0.0\", \"OpenMetricsText0.0.1\", \"PrometheusText0.0.4\"]\n```\n\n----------------------------------------\n\nTITLE: Renaming Resource Attributes in Alloy\nDESCRIPTION: Example of renaming a resource attribute using the transform processor. This snippet demonstrates two methods: setting a new attribute and deleting the old one, and using regular expressions to update the key.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.transform.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.transform \"default\" {\n  error_mode = \"ignore\"\n\n  trace_statements {\n    context = \"resource\"\n    statements = [\n      `set(attributes[\"namespace\"], attributes[\"k8s.namespace.name\"])`,\n      `delete_key(attributes, \"k8s.namespace.name\")`,\n    ]\n  }\n\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.transform \"default\" {\n  error_mode = \"ignore\"\n\n  trace_statements {\n    context = \"resource\"\n    statements = [\n     `replace_all_patterns(attributes, \"key\", \"k8s\\\\.namespace\\\\.name\", \"namespace\")`,\n    ]\n  }\n\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Write for Log Submission in Alloy\nDESCRIPTION: This snippet configures the loki.write component to send logs to a specified Loki endpoint. It defines the full URL for the Loki API push endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-linux.md#2025-04-22_snippet_10\n\nLANGUAGE: alloy\nCODE:\n```\nloki.write \"local\" {\n  endpoint {\n    url = \"http://loki:3100/loki/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Transform Processor Configuration in Alloy\nDESCRIPTION: Basic configuration example showing the structure of an otelcol.processor.transform component with output configuration. The processor requires an output block to specify where to send the transformed telemetry data.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.transform.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.transform \"LABEL\" {\n  output {\n    metrics = [...]\n    logs    = [...]\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Remote Write in Alloy\nDESCRIPTION: Basic configuration structure for prometheus.remote_write component showing endpoint setup. The component requires a label and endpoint URL, with optional additional configuration blocks for authentication and metrics handling.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.remote_write.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.remote_write \"<LABEL>\" {\n  endpoint {\n    url = \"<REMOTE_WRITE_URL>\"\n\n    ...\n  }\n\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Processing Identical Resource Attributes - Output Metrics\nDESCRIPTION: Example showing the resulting single metric resource created from two spans with identical resource attributes. The connector preserves the original resource attributes in the output metric.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.spanmetrics.md#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resourceMetrics\": [\n    {\n      \"resource\": {\n        \"attributes\": [\n          {\n            \"key\": \"service.name\",\n            \"value\": { \"stringValue\": \"TestSvcName\" }\n          },\n          {\n            \"key\": \"k8s.pod.name\",\n            \"value\": { \"stringValue\": \"first\" }\n          }\n        ]\n      },\n      \"scopeMetrics\": [\n        {\n          \"scope\": { \"name\": \"spanmetricsconnector\" },\n          \"metrics\": [\n            {\n              \"name\": \"calls\",\n              \"sum\": {\n                \"dataPoints\": [\n                  {\n                    \"attributes\": [\n                      {\n                        \"key\": \"service.name\",\n                        \"value\": { \"stringValue\": \"TestSvcName\" }\n                      },\n                      {\n                        \"key\": \"span.name\",\n                        \"value\": { \"stringValue\": \"TestSpan\" }\n                      },\n                      {\n                        \"key\": \"span.kind\",\n                        \"value\": { \"stringValue\": \"SPAN_KIND_UNSPECIFIED\" }\n                      },\n                      {\n                        \"key\": \"status.code\",\n                        \"value\": { \"stringValue\": \"STATUS_CODE_UNSET\" }\n                      }\n                    ],\n                    \"startTimeUnixNano\": \"1702582936761872000\",\n                    \"timeUnixNano\": \"1702582936761872012\",\n                    \"asInt\": \"2\"\n                  }\n                ],\n                \"aggregationTemporality\": 2,\n                \"isMonotonic\": true\n              }\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Counter Metrics in Grafana Alloy\nDESCRIPTION: Configuration options for 'metric.counter' which defines a metric whose value only increases. Supports actions like 'inc' and 'add', with options for matching log lines, counting bytes, and specifying metric metadata.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_18\n\nLANGUAGE: markdown\nCODE:\n```\n#### `metric.counter`\n\nDefines a metric whose value only goes up.\n\nThe following arguments are supported:\n\n| Name                | Type       | Description                                                                                               | Default                  | Required |\n| ------------------- | ---------- | --------------------------------------------------------------------------------------------------------- | ------------------------ | -------- |\n| `action`            | `string`   | The action to take. Valid actions are `inc` and `add`.                                                    |                          | yes      |\n| `name`              | `string`   | The metric name.                                                                                          |                          | yes      |\n| `count_entry_bytes` | `bool`     | If set to true, counts all log lines bytes.                                                               | `false`                  | no       |\n| `description`       | `string`   | The metric's description and help text.                                                                   | `\"\"`                     | no       |\n| `match_all`         | `bool`     | If set to true, all log lines are counted, without attempting to match the `source` to the extracted map. | `false`                  | no       |\n| `max_idle_duration` | `duration` | Maximum amount of time to wait until the metric is marked as 'stale' and removed.                         | `\"5m\"`                   | no       |\n| `prefix`            | `string`   | The prefix to the metric name.                                                                            | `\"loki_process_custom_\"` | no       |\n| `source`            | `string`   | Key from the extracted data map to use for the metric. Defaults to the metric name.                       | `\"\"`                     | no       |\n| `value`             | `string`   | If set, the metric only changes if `source` exactly matches the `value`.                                  | `\"\"`                     | no       |\n\nA counter can't set both `match_all` to true _and_ a `value`.\nA counter can't set `count_entry_bytes` without also setting `match_all=true` _or_ `action=add`.\nThe valid `action` values are `inc` and `add`.\nThe `inc` action increases the metric value by 1 for each log line that passed the filter.\nThe `add` action converts the extracted value to a positive float and adds it to the metric.\n```\n\n----------------------------------------\n\nTITLE: Basic OAuth2 Configuration in Alloy\nDESCRIPTION: Basic usage example showing how to configure the otelcol.auth.oauth2 component with required parameters including client_id, client_secret, and token_url.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.auth.oauth2.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.auth.oauth2 \"LABEL\" {\n    client_id     = \"CLIENT_ID\"\n    client_secret = \"CLIENT_SECRET\"\n    token_url     = \"TOKEN_URL\"\n}\n```\n\n----------------------------------------\n\nTITLE: Decompression Configuration\nDESCRIPTION: Example showing how to configure decompression for reading compressed log files with a delay to avoid partial reads.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.file.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file_match \"logs\" {\n  path_targets = [\n    {__path__ = \"/tmp/*.gz\"},\n  ]\n}\n\nloki.source.file \"tmpfiles\" {\n  targets    = local.file_match.logs.targets\n  forward_to = [loki.write.local.receiver]\n  decompression {\n    enabled       = true\n    initial_delay = \"10s\"\n    format        = \"gz\"\n  }\n}\n\nloki.write \"local\" {\n  endpoint {\n    url = \"loki:3100/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Operator ServiceMonitors in Alloy\nDESCRIPTION: This snippet shows the basic usage of the prometheus.operator.servicemonitors component. It defines a labeled component and specifies where to forward the scraped metrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.operator.servicemonitors.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.operator.servicemonitors \"<LABEL>\" {\n    forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Attribute Selection in Alloy\nDESCRIPTION: This snippet demonstrates how to include and exclude specific attributes for metrics in Grafana Alloy. It shows how to use wildcards and override settings for groups of metrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/beyla/beyla.ebpf.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nattributes {\n  select {\n      attr = \"sql_client_duration\"\n      include = [\"*\"]\n      exclude = [\"db_statement\"]\n  }\n}\n```\n\nLANGUAGE: alloy\nCODE:\n```\nattributes {\n  select {\n      attr = \"http_*\"\n      include = [\"*\"]\n      exclude = [\"http_path\", \"http_route\"]\n  }\n  select {\n      attr = \"http_client_*\"\n      // override http_* exclusion\n      include = [\"http_path\"]\n  }  \n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Authentication for Jaeger Remote Sampling in Alloy\nDESCRIPTION: This example demonstrates how to enable authentication for the Jaeger remote sampling extension using basic authentication. It configures both HTTP and gRPC endpoints to use the same authentication handler.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.extension.jaeger_remote_sampling.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.extension.jaeger_remote_sampling \"default\" {\n  http {\n    auth = otelcol.auth.basic.creds.handler\n  }\n  grpc {\n     auth = otelcol.auth.basic.creds.handler\n  }\n}\n\notelcol.auth.basic \"creds\" {\n    username = sys.env(\"USERNAME\")\n    password = sys.env(\"PASSWORD\")\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Discovery Relabeling in Alloy\nDESCRIPTION: Alloy configuration for the discovery.relabel component that defines rules to create a service name from the container name using regex pattern matching.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-docker-containers.md#2025-04-22_snippet_8\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.relabel \"logs_integrations_docker\" {\n  targets = []\n\n  rule {\n    source_labels = [\"__meta_docker_container_name\"]\n    regex = \"/(.*)\"\n    target_label = \"service_name\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Scraping in Grafana Alloy\nDESCRIPTION: Alloy configuration component that uses loki.source.file to scrape logs from files matched by the local_files component. It forwards logs to a filter component and tails files from the end.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/send-logs-to-loki.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.file \"log_scrape\" {\n  targets    = local.file_match.local_files.targets\n  forward_to = [loki.process.filter_logs.receiver]\n  tail_from_end = true\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Parsing with Tenant ID Extraction\nDESCRIPTION: Shows how to parse JSON and extract a tenant ID from a customer_id field using multiple pipeline stages.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_46\n\nLANGUAGE: alloy\nCODE:\n```\nstage.json {\n    expressions = { \"customer_id\" = \"\" }\n}\nstage.tenant {\n    source = \"customer_id\"\n}\n```\n\n----------------------------------------\n\nTITLE: Processing Spans by Service Pattern and Span Names in Alloy\nDESCRIPTION: Configures an attribute processor that filters spans based on service name regex and excludes spans matching certain patterns. It obfuscates password attributes and removes token attributes for matching spans, targeting only trace signals.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.attributes.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.attributes \"default\" {\n    // Specifies the span properties that must exist for the processor to be applied.\n    include {\n        // \"match_type\" defines that \"services\" is an array of regexp-es.\n        match_type = \"regexp\"\n        // The span service name must match \"auth.*\" pattern.\n        services = [\"auth.*\"]\n    }\n\n    exclude {\n        // \"match_type\" defines that \"span_names\" is an array of regexp-es.\n        match_type = \"regexp\"\n        // The span name must not match \"login.*\" pattern.\n        span_names = [\"login.*\"]\n    }\n\n    action {\n        key = \"password\"\n        action = \"update\"\n        value = \"obfuscated\"\n    }\n\n    action {\n        key = \"token\"\n        action = \"delete\"\n    }\n\n    output {\n        traces  = [otelcol.exporter.otlp.default.input]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a new span name without separator in Alloy\nDESCRIPTION: This example shows how to create a new span name by concatenating attribute values without using a separator.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.span.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.span \"default\" {\n  name {\n    from_attributes = [\"db.svc\", \"operation\", \"id\"]\n  }\n\n  output {\n      traces = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Kubernetes Events Source Configuration\nDESCRIPTION: Basic configuration example showing required forward_to parameter for the loki.source.kubernetes_events component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.kubernetes_events.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.kubernetes_events \"<LABEL>\" {\n  forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Basic OTLP Exporter with OAuth2 Authentication\nDESCRIPTION: Example showing how to configure an OTLP exporter with OAuth2 authentication using basic credentials.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.auth.oauth2.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlp \"example\" {\n  client {\n    endpoint = \"my-otlp-grpc-server:4317\"\n    auth     = otelcol.auth.oauth2.creds.handler\n  }\n}\n\notelcol.auth.oauth2 \"creds\" {\n    client_id     = \"someclientid\"\n    client_secret = \"someclientsecret\"\n    token_url     = \"https://example.com/oauth2/default/v1/token\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows Event Log Source in Alloy\nDESCRIPTION: Set up the loki.source.windowsevent component to read Windows Event Logs.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-windows.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.windowsevent \"application\"  {\n    eventlog_name = \"Application\"\n    use_incoming_timestamp = true\n    forward_to = [loki.process.endpoint.receiver]\n}\n\nloki.source.windowsevent \"System\"  {\n    eventlog_name = \"System\"\n    use_incoming_timestamp = true\n    forward_to = [loki.process.endpoint.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Keeping original span name while adding attributes in Alloy\nDESCRIPTION: This example shows how to add a new attribute based on a regular expression match while preserving the original span name.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.span.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.span \"keep_original_name\" {\n  name {\n    to_attributes {\n      keep_original_name = true\n      rules = [`^\\/api\\/v1\\/document\\/(?P<documentId>.*)\\/update$`]\n    }\n  }\n\n  output {\n      traces = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Static CloudWatch Metric Scraping for EC2 in Alloy\nDESCRIPTION: This example shows how to configure the CloudWatch exporter to scrape specific metrics from a particular EC2 instance. It uses the 'static' block to define the exact namespace, dimensions, and metrics to collect, allowing for precise control over the data being exported.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.cloudwatch.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.cloudwatch \"static_instances\" {\n    sts_region = \"us-east-2\"\n\n    static \"instances\" {\n        regions    = [\"us-east-2\"]\n        namespace  = \"AWS/EC2\"\n        dimensions = {\n            \"InstanceId\" = \"i01u29u12ue1u2c\",\n        }\n\n        metric {\n            name       = \"CPUUsage\"\n            statistics = [\"Sum\", \"Average\"]\n            period     = \"1m\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Target Auto-distribution in Prometheus Scrape Component with Alloy\nDESCRIPTION: This snippet demonstrates how to enable clustering for the prometheus.scrape component in Grafana Alloy. By defining a clustering block with enabled=true, the component will participate in target auto-distribution across the cluster.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/clustering.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"default\" {\n    clustering {\n        enabled = true\n    }\n\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Attribute from Log Body in Alloy\nDESCRIPTION: Example of creating an attribute from the contents of a log body using the transform processor. This snippet sets the 'body' attribute to the value of the log body.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.transform.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.transform \"default\" {\n  error_mode = \"ignore\"\n\n  log_statements {\n    context = \"log\"\n    statements = [\n      `set(attributes[\"body\"], body)`,\n    ]\n  }\n\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering, renaming spans, and adding attributes in Alloy\nDESCRIPTION: This configuration demonstrates how to filter spans based on service name and span name patterns, rename the span, and add a new attribute.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.span.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.span \"default\" {\n  include {\n    match_type = \"regexp\"\n    services   = [\"banks\"]\n    span_names = [\"^(.*?)/(.*?)$\"]\n  }\n  exclude {\n    match_type = \"strict\"\n    span_names = [\"donot/change\"]\n  }\n  name {\n    to_attributes {\n      rules = [\"(?P<operation_website>.*?)$\"]\n    }\n  }\n\n  output {\n      traces = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Metric Statements in Alloy\nDESCRIPTION: Example of configuring metric statements using the 'statements' block in Alloy. This snippet demonstrates how to set a metric description based on a datapoint attribute.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.transform.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nstatements {\n    metric = [`set(metric.description, \"test passed\") where datapoint.attributes[\"test\"] == \"pass\"`]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Log File Discovery in Grafana Alloy\nDESCRIPTION: Alloy configuration component that uses local.file_match to identify log files in the /var/log directory. It checks for new files every 5 seconds.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/send-logs-to-loki.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file_match \"local_files\" {\n    path_targets = [{\"__path__\" = \"/var/log/*.log\"}]\n    sync_period = \"5s\"\n}\n```\n\n----------------------------------------\n\nTITLE: Processing Identical Resource Attributes - Input Spans\nDESCRIPTION: Example showing two incoming spans with identical resource attributes (service.name and k8s.pod.name). Both spans belong to the same service and pod.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.spanmetrics.md#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resourceSpans\": [\n    {\n      \"resource\": {\n        \"attributes\": [\n          {\n            \"key\": \"service.name\",\n            \"value\": { \"stringValue\": \"TestSvcName\" }\n          },\n          {\n            \"key\": \"k8s.pod.name\",\n            \"value\": { \"stringValue\": \"first\" }\n          }\n        ]\n      },\n      \"scopeSpans\": [\n        {\n          \"spans\": [\n            {\n              \"trace_id\": \"7bba9f33312b3dbb8b2c2c62bb7abe2d\",\n              \"span_id\": \"086e83747d0e381e\",\n              \"name\": \"TestSpan\",\n              \"attributes\": [\n                {\n                  \"key\": \"attribute1\",\n                  \"value\": { \"intValue\": \"78\" }\n                }\n              ]\n            }\n          ]\n        }\n      ]\n    },\n    {\n      \"resource\": {\n        \"attributes\": [\n          {\n            \"key\": \"service.name\",\n            \"value\": { \"stringValue\": \"TestSvcName\" }\n          },\n          {\n            \"key\": \"k8s.pod.name\",\n            \"value\": { \"stringValue\": \"first\" }\n          }\n        ]\n      },\n      \"scopeSpans\": [\n        {\n          \"spans\": [\n            {\n              \"trace_id\": \"7bba9f33312b3dbb8b2c2c62bb7abe2d\",\n              \"span_id\": \"086e83747d0e381b\",\n              \"name\": \"TestSpan\",\n              \"attributes\": [\n                {\n                  \"key\": \"attribute1\",\n                  \"value\": { \"intValue\": \"78\" }\n                }\n              ]\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Labels Stage Configuration in Alloy\nDESCRIPTION: Configures a stage to set new labels on log entries based on extracted values.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_13\n\nLANGUAGE: alloy\nCODE:\n```\nstage.labels {\n    values = {\n      env  = \"\",         // Sets up an 'env' label, based on the 'env' extracted value.\n      user = \"username\", // Sets up a 'user' label, based on the 'username' extracted value.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Discovery in Grafana Alloy\nDESCRIPTION: Basic usage of the discovery.docker component to discover Docker Engine containers. It requires specifying a label and the Docker Engine host to connect to.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.docker.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.docker \"<LABEL>\" {\n  host = \"<DOCKER_ENGINE_HOST>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Serverset Discovery in Alloy\nDESCRIPTION: This snippet shows the basic usage of the discovery.serverset component in Alloy. It specifies the label, servers list, and Zookeeper paths to discover Serversets from.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.serverset.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.serverset \"<LABEL>\" {\n    servers = \"<SERVERS_LIST>\"\n    paths   = \"<ZOOKEEPER_PATHS_LIST>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Exporter in Alloy\nDESCRIPTION: Basic configuration for the otelcol.exporter.kafka component. It specifies a label and the Kafka protocol version to use.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.kafka.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.kafka \"LABEL\" {\n  protocol_version = \"PROTOCOL_VERSION\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Transform Processor for Default Attribute in Alloy\nDESCRIPTION: Example of configuring the transform processor to set a default attribute value if it doesn't exist. This snippet uses the 'trace_statements' block to set the 'test' attribute to 'pass' if it's not present.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.transform.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.transform \"default\" {\n  error_mode = \"ignore\"\n\n  trace_statements {\n    context = \"span\"\n    statements = [\n      // Accessing a map with a key that does not exist will return nil.\n      `set(attributes[\"test\"], \"pass\") where attributes[\"test\"] == nil`,\n    ]\n  }\n\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana Alloy Logging\nDESCRIPTION: This snippet shows a basic Grafana Alloy configuration file that sets up logging. It specifies the log level as 'info' and the format as 'logfmt'.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/docker.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nlogging {\n  level  = \"info\"\n  format = \"logfmt\"\n}\n```\n\n----------------------------------------\n\nTITLE: Filelog Receiver with Container Operator Configuration\nDESCRIPTION: Example showing how to configure the filelog receiver with a container operator for parsing container logs.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.filelog.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.filelog \"default\" {\n    ...\n    operators = [\n      {\n        type = \"container\"\n      }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Write in Alloy\nDESCRIPTION: Alloy configuration block for the loki.write component, which writes logs to a Loki destination.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-logs-over-tcp.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\nloki.write \"local\" {\n  endpoint {\n    url = \"http://loki:3100/loki/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: GeoIP Custom Fields Configuration in Alloy\nDESCRIPTION: Demonstrates GeoIP processing with custom field lookups using an enriched MMDB file. Extracts custom data like department, VNet, and subnet information from IP addresses.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_9\n\nLANGUAGE: alloy\nCODE:\n```\nloki.process \"example\" {\n    stage.json {\n        expressions = {ip = \"client_ip\"}\n    }\n\n    stage.geoip {\n        source         = \"ip\"\n        db             = \"/path/to/db/GeoIP2-Enriched.mmdb\"\n        db_type        = \"city\"\n        custom_lookups = {\n            \"department\"  = \"MyCompany.DeptName\",\n            \"parent_vnet\" = \"MyCompany.ParentVNet\",\n            \"subnet\"      = \"MyCompany.Subnet\",\n        }\n    }\n\n    stage.labels {\n        values = {\n            department  = \"\",\n            parent_vnet = \"\",\n            subnet      = \"\",\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Line Tracking Metrics in Alloy\nDESCRIPTION: Creates two counter metrics - one tracking total log lines and another tracking total bytes of log lines. Both metrics expire after 24 hours of inactivity to prevent stale metric accumulation.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_21\n\nLANGUAGE: alloy\nCODE:\n```\nstage.metrics {\n    metric.counter {\n        name        = \"log_lines_total\"\n        description = \"total number of log lines\"\n        prefix      = \"my_custom_tracking_\"\n\n        match_all         = true\n        action            = \"inc\"\n        max_idle_duration = \"24h\"\n    }\n}\nstage.metrics {\n    metric.counter {\n        name        = \"log_bytes_total\"\n        description = \"total bytes of log lines\"\n        prefix      = \"my_custom_tracking_\"\n\n        match_all         = true\n        count_entry_bytes = true\n        action            = \"add\"\n        max_idle_duration = \"24h\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DaemonSet for Grafana Alloy Deployment on OpenShift\nDESCRIPTION: This YAML configuration defines a DaemonSet for deploying Grafana Alloy as a non-root user on OpenShift. It specifies container security context settings including running as user 473, read-only root filesystem, and proper volume mounting. This configuration is designed to be compliant with OpenShift security requirements.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/openshift.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: aapps/v1\nkind: DaemonSet\nmetadata:\n  name: alloy-logs\n  namespace: monitoring\nspec:\n  selector:\n    matchLabels:\n      app: alloy-logs\n   template:\n     metadata:\n       labels:\n         app: alloy-logs\n      spec:\n        containers:\n        - name: alloy-logs\n          image: grafana/alloy:<ALLOY_VERSION>\n          ports:\n          - containerPort: 12345\n          # The security context configuration\n          securityContext:\n            readOnlyRootFilesystem: true\n            allowPrivilegeEscalation: false\n            runAsUser: 473\n            runAsGroup: 473\n            fsGroup: 1000\n         volumes:\n         - name: log-volume\n           emptyDir: {}\n```\n\n----------------------------------------\n\nTITLE: Configuring loki.source.podlogs in Alloy\nDESCRIPTION: This snippet shows how to configure the loki.source.podlogs component in Alloy. It specifies a label for the component and sets the forward_to argument to send log entries to specified receivers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.podlogs.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.podlogs \"<LABEL>\" {\n  forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.auth.sigv4 with Assume Role\nDESCRIPTION: Shows how to configure otelcol.auth.sigv4 with explicit region and service, and additional configuration to assume a role.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.auth.sigv4.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlp \"example\" {\n  client {\n    endpoint = \"my-otlp-grpc-server:4317\"\n    auth     = otelcol.auth.sigv4.creds.handler\n  }\n}\n\notelcol.auth.sigv4 \"creds\" {\n  region  = \"example_region\"\n  service = \"example_service\"\n\n  assume_role {\n    session_name = \"role_session_name\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Syslog Source in Alloy\nDESCRIPTION: Configuration block for the loki.source.syslog component that listens for and processes syslog messages over TCP and UDP.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-syslog-messages.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.syslog \"local\" {\n  listener {\n    address  = \"0.0.0.0:51893\"\n    labels   = { component = \"loki.source.syslog\", protocol = \"tcp\" }\n  }\n\n  listener {\n    address  = \"0.0.0.0:51898\"\n    protocol = \"udp\"\n    labels   = { component = \"loki.source.syslog\", protocol = \"udp\" }\n  }\n\n  forward_to = [loki.write.local.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Renaming a span and adding attributes in Alloy\nDESCRIPTION: This configuration demonstrates how to rename a span using a regular expression and add a new attribute based on the captured value.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.span.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.span \"default\" {\n  name {\n    to_attributes {\n      rules = [\"^\\\\/api\\\\/v1\\\\/document\\\\/(?P<documentId>.*)\\\\/update$\"]\n    }\n  }\n\n  output {\n      traces = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Datadog Agent Environment Variables for Dual Forwarding\nDESCRIPTION: Sets environment variables in the Datadog Agent to forward metrics and traces to both Datadog and Grafana Alloy. This allows for a gradual migration without disrupting existing Datadog workflows.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/datadog-traces-metrics.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nDD_ADDITIONAL_ENDPOINTS='{\"http://<DATADOG_RECEIVER_HOST>:<DATADOG_RECEIVER_PORT>\": [\"datadog-receiver\"]}'\nDD_APM_ADDITIONAL_ENDPOINTS='{\"http://<DATADOG_RECEIVER_HOST>:<DATADOG_RECEIVER_PORT>\": [\"datadog-receiver\"]}'\n```\n\n----------------------------------------\n\nTITLE: Basic Azure VM Discovery Configuration\nDESCRIPTION: Basic usage example showing the discovery.azure component configuration syntax.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.azure.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.azure \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Example: Receiving TCP Logs and Debugging\nDESCRIPTION: An example configuration that receives log messages from TCP and forwards them to a debug exporter for logging.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.tcplog.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.tcplog \"default\" {\n    listen_address = \"localhost:1515\"\n    output {\n        logs = [otelcol.exporter.debug.default.input]\n    }\n}\n\notelcol.exporter.debug \"default\" {}\n```\n\n----------------------------------------\n\nTITLE: Configuring Drop Stage in Alloy\nDESCRIPTION: Examples of drop stage configuration to filter log entries based on patterns, size and age. Shows both single and multiple drop block usage.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\nstage.drop {\n    expression  = \".*debug.*\"\n    longer_than = \"1KB\"\n}\n```\n\nLANGUAGE: alloy\nCODE:\n```\nstage.drop {\n    older_than          = \"24h\"\n    drop_counter_reason = \"too old\"\n}\n\nstage.drop {\n    longer_than         = \"8KB\"\n    drop_counter_reason = \"too long\"\n}\n\nstage.drop {\n    source = \"app\"\n    value  = \"example\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.receiver.tcplog in Alloy\nDESCRIPTION: Basic configuration for the otelcol.receiver.tcplog component, specifying the listen address and output for logs.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.tcplog.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.tcplog \"<LABEL>\" {\n  listen_address = \"<IP_ADDRESS:PORT>\"\n\n  output {\n    logs    = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Authentication with Exclusion Path in Alloy\nDESCRIPTION: This snippet shows how to set up HTTP authentication for all endpoints except /metrics. It uses basic auth with environment variables for credentials and configures the filter to exclude the /metrics path from authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/http.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nhttp {\n  auth {\n    basic {\n      username = sys.env(\"BASIC_AUTH_USERNAME\")\n      password = sys.env(\"BASIC_AUTH_PASSWORD\")\n    }\n\n    filter {\n      paths                       = [\"/metrics\"]\n      authenticate_matching_paths = false\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Duration Metric in otelcol.connector.spanmetrics\nDESCRIPTION: Example of a 'duration' histogram metric generated by otelcol.connector.spanmetrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.spanmetrics.md#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nduration { service.name=\"shipping\", span.name=\"get_shipping/{shippingId}\", span.kind=\"SERVER\", status.code=\"Ok\" }\n```\n\n----------------------------------------\n\nTITLE: GeoIP Processing with City Database\nDESCRIPTION: Configuration for GeoIP processing using City database to extract geographical information from IP addresses.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\nloki.process \"example\" {\n    stage.json {\n        expressions = {ip = \"client_ip\"}\n    }\n\n    stage.geoip {\n        source  = \"ip\"\n        db      = \"/path/to/db/GeoLite2-City.mmdb\"\n        db_type = \"city\"\n    }\n\n    stage.labels {\n        values = {\n            geoip_city_name          = \"\",\n            geoip_country_name       = \"\",\n            geoip_country_code       = \"\",\n            geoip_continent_name     = \"\",\n            geoip_continent_code     = \"\",\n            geoip_location_latitude  = \"\",\n            geoip_location_longitude = \"\",\n            geoip_postal_code        = \"\",\n            geoip_timezone           = \"\",\n            geoip_subdivision_name   = \"\",\n            geoip_subdivision_code   = \"\",\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SNMP Exporter with Target List in Alloy\nDESCRIPTION: This snippet shows an alternative configuration for the prometheus.exporter.snmp component using a list of targets. It sets the config file and a targets list.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.snmp.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.snmp \"<LABEL>\" {\n  config_file = \"<SNMP_CONFIG_FILE_PATH>\"\n  targets     = <TARGET_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Basic PostgreSQL Exporter Configuration in Alloy\nDESCRIPTION: Basic example showing how to configure the PostgreSQL exporter to collect metrics from a local PostgreSQL server and forward them to a remote write endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.postgres.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.postgres \"example\" {\n  data_source_names = [\"postgresql://username:password@localhost:5432/database_name?sslmode=disable\"]\n}\n\nprometheus.scrape \"default\" {\n  targets    = prometheus.exporter.postgres.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring stage.cri in loki.process\nDESCRIPTION: Shows how to configure the stage.cri block within loki.process. This stage enables a predefined pipeline for processing CRI-format log lines.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nstage.cri {}\n```\n\n----------------------------------------\n\nTITLE: HTTP Response Time Histogram Configuration\nDESCRIPTION: Creates a histogram metric for tracking HTTP response times with predefined buckets. Demonstrates histogram configuration for response time distribution analysis.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_24\n\nLANGUAGE: alloy\nCODE:\n```\nstage.metrics {\n    metric.histogram {\n        name        = \"http_response_time_seconds\"\n        description = \"recorded response times\"\n        source      = \"response_time\"\n        buckets     = [0.001,0.0025,0.005,0.010,0.025,0.050]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Processing in Alloy\nDESCRIPTION: Configure the loki.process component to process and forward log entries.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-windows.md#2025-04-22_snippet_8\n\nLANGUAGE: alloy\nCODE:\n```\nloki.process \"endpoint\" {\n  forward_to = [loki.write.endpoint.receiver]\n  stage.json {\n      expressions = {\n          message = \"\",\n          Overwritten = \"\",\n          source = \"\",\n          computer = \"\",\n          eventRecordID = \"\",\n          channel = \"\",\n          component_id = \"\",\n          execution_processId = \"\",\n          execution_processName = \"\",\n      }\n  }\n\n  stage.structured_metadata {\n      values = {\n          \"eventRecordID\" = \"\",\n          \"channel\" = \"\",\n          \"component_id\" = \"\",\n          \"execution_processId\" = \"\",\n          \"execution_processName\" = \"\",\n      }\n  }\n\n  stage.eventlogmessage {\n      source = \"message\"\n      overwrite_existing = true\n  }\n\n  stage.labels {\n      values = {\n          \"service_name\" = \"source\",\n      }\n  }\n\n  stage.output {\n    source = \"message\"\n  }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Discovery Relabeling for Node Exporter\nDESCRIPTION: Configuration for relabeling instance and job labels from node_exporter metrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-linux.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.relabel \"integrations_node_exporter\" {\n  targets = prometheus.exporter.unix.integrations_node_exporter.targets\n\n  rule {\n    target_label = \"instance\"\n    replacement  = constants.hostname\n  }\n\n  rule {\n    target_label = \"job\"\n    replacement = \"integrations/node_exporter\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Authentication for otelcol.receiver.zipkin in Alloy\nDESCRIPTION: Example of creating an otelcol.receiver.zipkin component that requires basic authentication for requests.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.zipkin.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.zipkin \"default\" {\n  auth = otelcol.auth.basic.creds.handler\n}\n\notelcol.auth.basic \"creds\" {\n    username = sys.env(\"USERNAME\")\n    password = sys.env(\"PASSWORD\")\n}\n```\n\n----------------------------------------\n\nTITLE: Batch Processor with Timeout Configuration\nDESCRIPTION: Example demonstrating batch processor configuration with custom timeout and batch size settings.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.batch.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.batch \"default\" {\n  timeout = \"10s\"\n  send_batch_size = 10000\n\n  output {\n    metrics = [otelcol.exporter.otlp.production.input]\n    logs    = [otelcol.exporter.otlp.production.input]\n    traces  = [otelcol.exporter.otlp.production.input]\n  }\n}\n\notelcol.exporter.otlp \"production\" {\n  client {\n    endpoint = sys.env(\"OTLP_SERVER_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Label Matchers to Discovered Queries in Alloy\nDESCRIPTION: This snippet demonstrates how to add a label matcher to all queries discovered by the mimir.rules.kubernetes component. It modifies queries to include a specific cluster label matcher.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/mimir/mimir.rules.kubernetes.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nmimir.rules.kubernetes \"default\" {\n    address = \"<GRAFANA_CLOUD_METRICS_URL>\"\n    extra_query_matchers {\n        matcher {\n            name = \"cluster\"\n            match_type = \"=~\"\n            value = \"prod-.*\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Live Debugging in Alloy\nDESCRIPTION: This snippet enables the livedebugging feature in Grafana Alloy, which allows streaming real-time data from components to the UI for troubleshooting purposes. An empty configuration block uses default values.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-linux.md#2025-04-22_snippet_11\n\nLANGUAGE: alloy\nCODE:\n```\nlivedebugging{}\n```\n\n----------------------------------------\n\nTITLE: Explicitly Specifying Region and Service in otelcol.auth.sigv4\nDESCRIPTION: Demonstrates how to explicitly specify the region and service when they cannot be inferred from the endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.auth.sigv4.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlp \"example\" {\n  client {\n    endpoint = \"my-otlp-grpc-server:4317\"\n    auth     = otelcol.auth.sigv4.creds.handler\n  }\n}\n\notelcol.auth.sigv4 \"creds\" {\n    region = \"example_region\"\n    service = \"example_service\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring EC2 Resource Detection with Default Attributes in Alloy\nDESCRIPTION: This snippet demonstrates how to configure the OpenTelemetry Collector to use the EC2 resource detector with default settings. EC2 defaults are applied automatically without needing an explicit 'ec2 {}' block.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.resourcedetection.md#2025-04-22_snippet_9\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.resourcedetection \"default\" {\n  detectors = [\"ec2\"]\n\n  output {\n    logs    = [otelcol.exporter.otlp.default.input]\n    metrics = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Authentication Configuration for InfluxDB Receiver\nDESCRIPTION: Example showing how to configure basic authentication for the InfluxDB receiver using environment variables for credentials.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.influxdb.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.influxdb \"influxdb_metrics\" {\n  output {\n    metrics = [...]\n  }\n  auth = otelcol.auth.basic.creds.handler\n}\n\notelcol.auth.basic \"creds\" {\n    username = sys.env(\"USERNAME\")\n    password = sys.env(\"PASSWORD\")\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Faro Receiver Configuration in Alloy\nDESCRIPTION: Basic configuration template for setting up a faro.receiver component with outputs for logs and traces.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/faro/faro.receiver.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nfaro.receiver \"<LABEL>\" {\n    output {\n        logs   = [<LOKI_RECEIVERS>]\n        traces = [<OTELCOL_COMPONENTS>]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Telemetry Pipeline with Batch Processing Configuration\nDESCRIPTION: Example showing how to configure a complete telemetry pipeline that receives InfluxDB metrics, processes them in batches, and forwards them to an OTLP endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.influxdb.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.influxdb \"influxdb_metrics\" {\n  output {\n    metrics = [otelcol.processor.batch.default.input]\n  }\n}\n\notelcol.processor.batch \"default\" {\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = sys.env(\"OTLP_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete MongoDB Monitoring Setup with Remote Write\nDESCRIPTION: Comprehensive example showing how to configure MongoDB metrics collection with prometheus.scrape and remote write setup.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.mongodb.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.mongodb \"example\" {\n  mongodb_uri = \"mongodb://127.0.0.1:27017\"\n}\n\n// Configure a prometheus.scrape component to collect MongoDB metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.mongodb.example.targets\n  forward_to = [ prometheus.remote_write.default.receiver ]\n}\n\nprometheus.remote_write \"default\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Header Parsing for File Log Receiver in Alloy\nDESCRIPTION: This snippet demonstrates how to configure the header block for parsing metadata from a log header line. It uses a regex pattern to identify the header and applies metadata operators to extract specific information.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.filelog.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.filelog \"default\" {\n    ...\n    header {\n      pattern = '^HEADER_IDENTIFIER .*$'\n      metadata_operators = [\n        {\n          type = \"regex_parser\"\n          regex = 'env=\"(?P<environment>.+)\"'\n        }\n      ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Component with Argument in Alloy\nDESCRIPTION: Example of a custom component that collects process metrics and forwards them to a user-specified output. It demonstrates the use of the 'argument' block to define a required input.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/argument.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndeclare \"self_collect\" {\n  argument \"metrics_output\" {\n    optional = false\n    comment  = \"Where to send collected metrics.\"\n  }\n\n  prometheus.scrape \"selfmonitor\" {\n    targets = [{\n      __address__ = \"127.0.0.1:12345\",\n    }]\n\n    forward_to = [argument.metrics_output.value]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Beyla eBPF Component Configuration in Alloy\nDESCRIPTION: Basic configuration structure for initializing a beyla.ebpf component. The component requires administrative privileges or specific capabilities like BPF, SYS_PTRACE, NET_RAW, etc.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/beyla/beyla.ebpf.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nbeyla.ebpf \"<LABEL>\" {\n\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Spans by Container Name\nDESCRIPTION: Example configuration that drops spans containing a specific container name attribute. Uses error_mode 'ignore' for error handling.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.filter.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.filter \"default\" {\n  error_mode = \"ignore\"\n\n  traces {\n    span = [\n      `attributes[\"container.name\"] == \"app_container_1\"`,\n    ]\n  }\n\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Pod Association Rules in Alloy\nDESCRIPTION: This snippet demonstrates how to configure pod association rules using the 'pod_association' block. It shows two examples: one associating logs/traces/metrics to pods based on the pod IP, and another using both pod UID and connection information.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.k8sattributes.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\npod_association {\n    source {\n        from = \"resource_attribute\"\n        name = \"k8s.pod.ip\"\n    }\n}\n\npod_association {\n    source {\n        from = \"resource_attribute\"\n        name = \"k8s.pod.uid\"\n    }\n    source {\n        from = \"connection\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic cAdvisor Exporter Configuration\nDESCRIPTION: Basic configuration structure for the prometheus.exporter.cadvisor component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.cadvisor.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.cadvisor \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Basic GCP Log Source Configuration in Alloy\nDESCRIPTION: Basic configuration example showing how to set up a GCP log source using the pull strategy to retrieve logs from a specific project and subscription.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.gcplog.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.gcplog \"<LABEL>\" {\n  pull {\n    project_id   = \"<PROJECT_ID>\"\n    subscription = \"<SUB_ID>\"\n  }\n\n  forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Kubelet Discovery Configuration in Alloy\nDESCRIPTION: Basic configuration block for discovering Kubernetes Pods using Kubelet. This component requires Kubelet to be reachable from the alloy Pod network and proper authentication configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.kubelet.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubelet \"LABEL\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a new span name from attribute values in Alloy\nDESCRIPTION: This example demonstrates how to create a new span name by combining values from multiple attributes using a specified separator.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.span.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.span \"default\" {\n  name {\n    separator        = \"::\"\n    from_attributes  = [\"db.svc\", \"operation\", \"id\"]\n  }\n\n  output {\n      traces = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Loki to Syslog Transform Pipeline Configuration\nDESCRIPTION: Complex example showing how to transform Loki source logs into syslog format using otelcol.processor.transform component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.syslog.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.loki \"default\" {\n  output {\n    logs = [otelcol.processor.transform.syslog.input]\n  }\n}\n\notelcol.processor.transform \"syslog\" {\n  error_mode = \"ignore\"\n\n  log_statements {\n    context = \"log\"\n\n    statements = [\n      `set(attributes[\"message\"], attributes[\"__syslog_message\"])`,\n      `set(attributes[\"appname\"], attributes[\"__syslog_appname\"])`,\n      `set(attributes[\"hostname\"], attributes[\"__syslog_hostname\"])`,\n\n      // To set structured data you can chain index ([]) operations.\n      `set(attributes[\"structured_data\"][\"auth@32473\"][\"user\"], attributes[\"__syslog_message_sd_auth_32473_user\"])`,\n      `set(attributes[\"structured_data\"][\"auth@32473\"][\"user_host\"], attributes[\"__syslog_message_sd_auth_32473_user_host\"])`,\n      `set(attributes[\"structured_data\"][\"auth@32473\"][\"valid\"], attributes[\"__syslog_message_sd_auth_32473_authenticated\"])`,\n    ]\n  }\n\n  output {\n    metrics = []\n    logs    = [otelcol.exporter.syslog.default.input]\n    traces  = []\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Authenticated Datadog Receiver Configuration\nDESCRIPTION: Example demonstrating how to enable basic authentication for the Datadog receiver using username and password credentials.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.datadog.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.datadog \"default\" {\n  output {\n    metrics = [otelcol.processor.batch.default.input]\n    traces  = [otelcol.processor.batch.default.input]\n  }\n  auth = otelcol.auth.basic.creds.handler\n}\n\notelcol.auth.basic \"creds\" {\n    username = sys.env(\"USERNAME\")\n    password = sys.env(\"PASSWORD\")\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Metrics with Relabeling in Alloy\nDESCRIPTION: Configures a metrics filter that drops metrics with the 'env' label set to 'dev'. This helps reduce storage costs and focuses on important metrics. The filtered metrics are forwarded to the remote_write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/send-metrics-to-prometheus.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.relabel \"filter_metrics\" {\n  rule {\n    action        = \"drop\"\n    source_labels = [\"env\"]\n    regex         = \"dev\"\n  }\n\n  forward_to = [prometheus.remote_write.metrics_service.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Tail Sampling Processor in Alloy\nDESCRIPTION: Basic configuration structure for setting up a tail sampling processor. The component requires policy configuration and output traces specification. Multiple instances can be created with different labels.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.tail_sampling.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.tail_sampling \"LABEL\" {\n  policy {\n    ...\n  }\n  ...\n\n  output {\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: OTLP gRPC Endpoint Configuration\nDESCRIPTION: Configuration block for enabling gRPC protocol support on port 4317 for receiving OTLP data.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-data.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\ngrpc {\n  endpoint = \"<HOST>:4317\"\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Error Metric in otelcol.connector.spanmetrics\nDESCRIPTION: Example of a 'calls' metric with 'Error' status code generated by otelcol.connector.spanmetrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.spanmetrics.md#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\ncalls { service.name=\"shipping\", span.name=\"get_shipping/{shippingId}, span.kind=\"SERVER\", status.code=\"Error\" }\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Log Listener in Alloy\nDESCRIPTION: Basic configuration for loki.source.api component to listen for HTTP log inputs on localhost:9999 with source labeling.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/processing-logs.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.api \"listener\" {\n    http {\n        listen_address = \"127.0.0.1\"\n        listen_port    = 9999\n    }\n\n    labels = { source = \"api\" }\n\n    forward_to = [loki.process.process_logs.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Alloy Configuration with Marathon Discovery and Prometheus Scraping\nDESCRIPTION: Example configuration demonstrating Marathon service discovery, Prometheus scraping, and remote write setup in Alloy.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.marathon.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.marathon \"example\" {\n  servers = [\"localhost:8500\"]\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.marathon.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.auth.sigv4 with Elasticsearch Endpoint\nDESCRIPTION: Shows how to set up otelcol.auth.sigv4 with an Elasticsearch endpoint, where region and service are inferred from the URL.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.auth.sigv4.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlp \"example\" {\n  client {\n    endpoint = \"https://search-my-domain.us-east-1.es.amazonaws.com/_search?q=house\"\n    auth     = otelcol.auth.sigv4.creds.handler\n  }\n}\n\notelcol.auth.sigv4 \"creds\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Write Component in Alloy\nDESCRIPTION: Basic configuration example for the loki.write component showing how to specify an endpoint for log transmission. The component requires a label and remote write URL to be configured.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.write.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.write \"<LABEL>\" {\n  endpoint {\n    url = \"<REMOTE_WRITE_URL>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Loki Process Configuration with JSON Parsing and Label Extraction\nDESCRIPTION: A simplified example showing how to create a Loki process component that extracts an environment value from JSON logs and converts it to a label. This demonstrates the core functionality of extracting fields and transforming them into labels.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_52\n\nLANGUAGE: alloy\nCODE:\n```\nloki.process \"local\" {\n  forward_to = [loki.write.onprem.receiver]\n\n  stage.json {\n      expressions = { \"extracted_env\" = \"environment\" }\n  }\n\n  stage.labels {\n      values = { \"env\" = \"extracted_env\" }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Log Packing Stage Example\nDESCRIPTION: Demonstrates how to pack log entries with labels into JSON objects using the stage.pack configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_27\n\nLANGUAGE: alloy\nCODE:\n```\nstage.pack {\n    labels = [\"env\", \"user_id\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Using pyroscope.java Component in Alloy\nDESCRIPTION: Basic usage syntax for the pyroscope.java component in Alloy configuration. It specifies the label, targets, and where to forward the collected profiles.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.java.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\npyroscope.java \"<LABEL>\" {\n  targets    = <TARGET_LIST>\n  forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Pyroscope Scrape Component in Alloy\nDESCRIPTION: Basic configuration structure for setting up a pyroscope.scrape component. Requires targets to scrape and forward_to receivers to be specified.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.scrape.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\npyroscope.scrape \"<LABEL>\" {\n  targets    = <TARGET_LIST>\n  forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TLS Support for Prometheus Write Queue\nDESCRIPTION: Added TLS support to the prometheus.write.queue component for secure metric transmission.\nSOURCE: https://github.com/grafana/alloy/blob/main/CHANGELOG.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nprometheus.write.queue:\n  tls:\n    enabled: true\n    cert_file: \"/path/to/cert\"\n    key_file: \"/path/to/key\"\n```\n\n----------------------------------------\n\nTITLE: Exporting to Local Tempo Instance in Alloy\nDESCRIPTION: Shows how to configure the otelcol.exporter.otlp component to send data to a local Grafana Tempo instance without TLS.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.otlp.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlp \"tempo\" {\n    client {\n        endpoint = \"tempo:4317\"\n        tls {\n            insecure             = true\n            insecure_skip_verify = true\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Host Info Connector Configuration\nDESCRIPTION: Basic configuration structure for the otelcol.connector.host_info component showing the required output block for metrics configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.host_info.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.connector.host_info \"LABEL\" {\n  output {\n    metrics = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Datapoint Statements in Alloy\nDESCRIPTION: Example of configuring datapoint statements using the 'metric_statements' block in Alloy. This snippet demonstrates how to set a metric description based on a datapoint attribute.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.transform.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nmetric_statements {\n  context = \"datapoint\"\n  statements = [\n    \"set(metric.description, \\\"test passed\\\") where attributes[\\\"test\\\"] == \\\"pass\\\"\",\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Authentication for Jaeger Receiver\nDESCRIPTION: Configuration example demonstrating how to enable basic authentication for GRPC and HTTP endpoints of the Jaeger receiver using environment variables for credentials.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.jaeger.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.jaeger \"default\" {\n  protocols {\n    grpc {\n      auth = otelcol.auth.basic.creds.handler\n    }\n    thrift_http {\n      auth = otelcol.auth.basic.creds.handler\n    }\n  }\n}\n\notelcol.auth.basic \"creds\" {\n    username = sys.env(\"USERNAME\")\n    password = sys.env(\"PASSWORD\")\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Process Discovery with discover_config in Alloy\nDESCRIPTION: Shows how to use the discover_config block to configure which process metadata to discover. This example enables all available discovery options.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.process.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.process \"all\" {\n  refresh_interval = \"60s\"\n  discover_config {\n    cwd = true\n    exe = true\n    commandline = true\n    username = true\n    uid = true\n    cgroup_path = true\n    container_id = true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Process in Alloy\nDESCRIPTION: Alloy configuration block for the loki.process component, which receives log entries, applies processing stages, and forwards the results to receivers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-logs-over-tcp.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\nloki.process \"labels\" {\n    stage.json {\n      expressions = { \"extracted_service\" = \"service_name\", \n                      \"extracted_code_line\" = \"code_line\", \n                      \"extracted_server\" = \"server_id\", \n                    }\n    }\n\n  stage.labels {\n    values = {\n      \"service_name\" = \"extracted_service\",\n    }\n  }\n\n  stage.structured_metadata {\n    values = {\n      \"code_line\" = \"extracted_code_line\",\n      \"server\" = \"extracted_server\",\n      }\n    }\n\n  forward_to = [loki.write.local.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Node Resource Detection with Custom Environment Variable in Alloy\nDESCRIPTION: This configuration demonstrates how to use a custom environment variable name instead of the default K8S_NODE_NAME for Kubernetes node detection by setting node_from_env_var to 'my_custom_var'.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.resourcedetection.md#2025-04-22_snippet_13\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.resourcedetection \"default\" {\n  detectors = [\"kubernetes_node\"]\n  kubernetes_node {\n    node_from_env_var = \"my_custom_var\"\n  }\n\n  output {\n    logs    = [otelcol.exporter.otlp.default.input]\n    metrics = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Datadog Receiver Configuration\nDESCRIPTION: Basic usage example showing how to configure the Datadog receiver component with required output configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.datadog.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.datadog \"LABEL\" {\n  output {\n    metrics = [...]\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: File Globbing Configuration\nDESCRIPTION: Example demonstrating how to use local.file_match for dynamic file discovery with pattern matching.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.file.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file_match \"logs\" {\n  path_targets = [\n    {__path__ = \"/tmp/*.log\"},\n  ]\n}\n\nloki.source.file \"tmpfiles\" {\n  targets    = local.file_match.logs.targets\n  forward_to = [loki.write.local.receiver]\n}\n\nloki.write \"local\" {\n  endpoint {\n    url = \"loki:3100/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Remote Write in Alloy\nDESCRIPTION: Sets up a remote_write component that sends the filtered metrics to a local Prometheus instance. Includes commented-out authentication configuration for reference when connecting to secured Prometheus endpoints.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/send-metrics-to-prometheus.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.remote_write \"metrics_service\" {\n    endpoint {\n        url = \"http://localhost:9090/api/v1/write\"\n\n        // basic_auth {\n        //   username = \"admin\"\n        //   password = \"admin\"\n        // }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Discovery in Alloy\nDESCRIPTION: Basic configuration structure for setting up Kubernetes service discovery. Requires specifying a label and discovery role to determine what Kubernetes resources to discover.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.kubernetes.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubernetes \"<LABEL>\" {\n  role = \"<DISCOVERY_ROLE>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Exporter with RSA Authentication\nDESCRIPTION: Example configuration for prometheus.exporter.snowflake using RSA key-pair authentication. Requires account name, username, private key path, private key password, and warehouse name.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.snowflake.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.snowflake \"LABEL\" {\n    account_name =         \"<SNOWFLAKE_ACCOUNT_NAME>\"\n    username =             \"<USERNAME>\"\n    private_key_path =     \"<RSA_PRIVATE_KEY_PATH>\"\n    private_key_password = \"<RSA_PRIVATE_KEY_PASSWORD>\"\n    warehouse =            \"<VIRTUAL_WAREHOUSE>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Write Component for Log Export\nDESCRIPTION: Configuration for the loki.write component which sends logs to a Loki destination. It defines the URL endpoint for the Loki instance.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-logs-from-file.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\nloki.write \"local\" {\n  endpoint {\n    url = \"http://loki:3100/loki/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Block in Alloy\nDESCRIPTION: Block configuration example for setting up logging in Grafana Alloy, specifying debug level and JSON format output.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/first-components-and-stdlib.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nlogging {\n    level  = \"debug\"\n    format = \"json\"\n}\n```\n\n----------------------------------------\n\nTITLE: Log Sampling by Attribute Configuration\nDESCRIPTION: Example showing how to configure log sampling based on a specific logID attribute.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.probabilistic_sampler.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.probabilistic_sampler \"default\" {\n  sampling_percentage = 15\n  attribute_source    = \"record\"\n  from_attribute      = \"logID\"\n\n  output {\n    logs = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Write for Local Instance in Alloy\nDESCRIPTION: Example configuration for sending log entries to a local Loki instance using the loki.write component. This setup specifies the endpoint URL for pushing logs to a Loki server running on the same network.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.write.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.write \"local\" {\n    endpoint {\n        url = \"http://loki:3100/loki/api/v1/push\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Default HTTP Configuration Example\nDESCRIPTION: Example showing Heroku source configuration using default HTTP settings that listen on port 8080.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.heroku.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.heroku \"local\" {\n    use_incoming_timestamp = true\n    labels                 = {component = \"loki.source.heroku\"}\n    forward_to             = [loki.write.local.receiver]\n}\n\nloki.write \"local\" {\n    endpoint {\n        url = \"loki:3100/api/v1/push\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Prometheus MSSQL Monitoring Setup\nDESCRIPTION: Full example showing prometheus.exporter.mssql configuration with scraping and remote write setup.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.mssql.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.mssql \"example\" {\n  connection_string = \"sqlserver://user:pass@localhost:1433\"\n}\n\n// Configure a prometheus.scrape component to collect mssql metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.mssql.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus cAdvisor Exporter in Alloy\nDESCRIPTION: Alloy configuration for the prometheus.exporter.cadvisor component that exposes Docker container metrics. Sets the Docker host endpoint and storage duration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-docker-containers.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.cadvisor \"example\" {\n  docker_host = \"unix:///var/run/docker.sock\"\n\n  storage_duration = \"5m\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic MSSQL Exporter Configuration in Alloy\nDESCRIPTION: Basic configuration structure for setting up the prometheus.exporter.mssql component with a connection string.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.mssql.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.mssql \"<LABEL>\" {\n    connection_string = \"<CONNECTION_STRING>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON Response for HTTP-based Service Discovery\nDESCRIPTION: Example of the expected JSON response format for the discovery.http component. It shows how to define targets and labels for service discovery.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.http.md#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"targets\": [ \"<HOST>\", ... ],\n    \"labels\": {\n      \"<labelname>\": \"<LABELVALUE>\", ...\n    }\n  },\n  ...\n]\n```\n\n----------------------------------------\n\nTITLE: Defining VMware vCenter Metric Configuration in Markdown\nDESCRIPTION: This code snippet defines the structure for configuring VMware vCenter metrics. It includes a table with columns for metric name, type, description, default value, and whether it's required. Each row represents a different vCenter metric that can be enabled or disabled.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.vcenter.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n### metrics block\n\nName | Type | Description | Default | Required\n---- | ---- | ----------- | ------- | --------\n`vcenter.cluster.cpu.effective` | [metric][] | Enables the `vcenter.cluster.cpu.effective` metric. | `true` | no\n`vcenter.cluster.cpu.limit` | [metric][] | Enables the `vcenter.cluster.cpu.limit` metric. | `true` | no\n`vcenter.cluster.host.count` | [metric][] | Enables the `vcenter.cluster.host.count` metric. | `true` | no\n# ... (additional metrics omitted for brevity)\n`vcenter.vm.vsan.throughput` | [metric][] | Enables the `vcenter.vm.vsan.throughput` metric. | `true` | no\n\n[metric]: #metric-block\n```\n\n----------------------------------------\n\nTITLE: Basic Batch Processor with OTLP Exporter\nDESCRIPTION: Example showing batch processor configuration with OTLP exporter integration for processing telemetry data.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.batch.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.batch \"default\" {\n  output {\n    metrics = [otelcol.exporter.otlp.production.input]\n    logs    = [otelcol.exporter.otlp.production.input]\n    traces  = [otelcol.exporter.otlp.production.input]\n  }\n}\n\notelcol.exporter.otlp \"production\" {\n  client {\n    endpoint = sys.env(\"OTLP_SERVER_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Summaries Block for Datadog Exporter in YAML\nDESCRIPTION: The summaries block sets up how summary metrics are reported, with options for quantile reporting.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.datadog.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nsummaries:\n  mode: \"gauges\"\n```\n\n----------------------------------------\n\nTITLE: Deploying Grafana Stack with Docker\nDESCRIPTION: Commands to start and manage the Grafana stack using Docker Compose.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-linux.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd alloy-scenarios/linux\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Basic OpenCensus Receiver Usage\nDESCRIPTION: Basic configuration example showing the minimal required setup for an OpenCensus receiver with output configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.opencensus.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.opencensus \"LABEL\" {\n  output {\n    metrics = [...]\n    logs    = [...]\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: MSSQL Connection String Format\nDESCRIPTION: Example connection string format for connecting to Azure SQL Managed Instance.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.mssql.md#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nsqlserver://<USERNAME>:<PASSWORD>@<SQLMI_ENDPOINT>.database.windows.net:1433?encrypt=true&hostNameInCertificate=%2A.<SQL_MI_DOMAIN>.database.windows.net&trustservercertificate=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Serverset Discovery with Prometheus Scraping and Remote Write\nDESCRIPTION: This example demonstrates a complete configuration using discovery.serverset to discover targets, prometheus.scrape to scrape the discovered targets, and prometheus.remote_write to send the scraped data to a remote endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.serverset.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.serverset \"zookeeper\" {\n    servers = [\"zk1\", \"zk2\", \"zk3\"]\n    paths   = [\"/path/to/znode1\", \"/path/to/znode2\"]\n    timeout = \"30s\"\n}\n\nprometheus.scrape \"default\" {\n    targets    = discovery.serverset.zookeeper.targets\n    forward_to = [prometheus.remote_write.default.receiver]\n}\n\nprometheus.remote_write \"default\" {\n    endpoint {\n        url = \"http://remote-write-url1\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Grafana Alloy Using Helm\nDESCRIPTION: This command installs Grafana Alloy using the Helm chart within the specified namespace. It requires providing both the namespace and a release name for the installation.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/kubernetes.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nhelm install --namespace <NAMESPACE> <RELEASE_NAME> grafana/alloy\n```\n\n----------------------------------------\n\nTITLE: Basic PuppetDB Discovery Configuration\nDESCRIPTION: Basic configuration structure for setting up PuppetDB service discovery in Grafana Alloy.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.puppetdb.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.puppetdb \"<LABEL>\" {\n  url = \"<PUPPET_SERVER>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Triton Discovery Configuration\nDESCRIPTION: Basic configuration example showing required arguments for setting up Triton service discovery.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.triton.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.triton \"<LABEL>\" {\n    account    = \"<ACCOUNT>\"\n    dns_suffix = \"<DNS_SUFFIX>\"\n    endpoint   = \"<ENDPOINT>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Exporter with Password Authentication\nDESCRIPTION: Example configuration for prometheus.exporter.snowflake using password-based authentication. Requires account name, username, password, and warehouse name.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.snowflake.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.snowflake \"LABEL\" {\n    account_name = \"<SNOWFLAKE_ACCOUNT_NAME>\"\n    username =     \"<USERNAME>\"\n    password =     \"<PASSWORD>\"\n    warehouse =    \"<VIRTUAL_WAREHOUSE>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector Span Logs Connector in Alloy\nDESCRIPTION: Basic configuration example for the otelcol.connector.spanlogs component. The component requires an output block to specify where to send the telemetry data, and supports optional configuration for logging spans, roots, processes, and various attributes.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.spanlogs.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.connector.spanlogs \"LABEL\" {\n  output {\n    logs    = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Basic Authentication to OTLP Exporter\nDESCRIPTION: Adds basic authentication to the OTLP endpoint configuration. This block should be placed inside the endpoint block when the destination requires authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/datadog-traces-metrics.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nbasic_auth {\n  username = \"<USERNAME>\"\n  password = \"<PASSWORD>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Loki Write Component in Alloy\nDESCRIPTION: Basic configuration for a loki.write component that delivers logs to a Loki endpoint. This component is essential for sending collected logs to Loki storage.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/logs-in-kubernetes.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.write \"<LABEL>\" {\n  endpoint {\n    url = \"<LOKI_URL>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Namespace Configuration\nDESCRIPTION: Configuration block for limiting Kubernetes service discovery to specific namespaces.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/prometheus-metrics.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\nnamespaces {\n  own_namespace = true\n  names         = [<NAMESPACE_NAMES>]\n}\n```\n\n----------------------------------------\n\nTITLE: Rate Limiting Stage Configuration in Alloy\nDESCRIPTION: Implements rate limiting for log processing using token bucket algorithm. Supports per-label rate limiting and configurable burst sizes.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_14\n\nLANGUAGE: alloy\nCODE:\n```\nstage.limit {\n    rate  = 5\n    burst = 10\n}\n```\n\nLANGUAGE: alloy\nCODE:\n```\nstage.limit {\n    rate  = 10\n    burst = 10\n    drop  = true\n\n    by_label_name = \"namespace\"\n}\n```\n\n----------------------------------------\n\nTITLE: Importing a Module from Git Repository in Alloy\nDESCRIPTION: Example of importing a module from a file inside a Git repository using import.git. The main file imports a math module from GitHub that contains an addition component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/import.file.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nimport.git \"math\" {\n  repository = \"https://github.com/wildum/module.git\"\n  path       = \"relative_math.alloy\"\n  revision   = \"master\"\n}\n\nmath.add \"default\" {\n  a = 15\n  b = 45\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Nomad Discovery Configuration\nDESCRIPTION: Basic configuration example showing how to declare a Nomad discovery component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.nomad.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.nomad \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Gauge Metrics in Grafana Alloy\nDESCRIPTION: Configuration options for 'metric.gauge' which defines a metric whose value can go up or down. Supports various actions including 'inc', 'dec', 'set', 'add', and 'sub', with options for metric metadata and value handling.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_19\n\nLANGUAGE: markdown\nCODE:\n```\n#### `metric.gauge`\n\nDefines a gauge metric whose value can go up or down.\n\nThe following arguments are supported:\n\n| Name                | Type       | Description                                                                         | Default                  | Required |\n| ------------------- | ---------- | ----------------------------------------------------------------------------------- | ------------------------ | -------- |\n| `action`            | `string`   | The action to take. Valid actions are  `inc`, `dec`, `set`, `add`, or `sub`.        |                          | yes      |\n| `name`              | `string`   | The metric name.                                                                    |                          | yes      |\n| `description`       | `string`   | The metric's description and help text.                                             | `\"\"`                     | no       |\n| `max_idle_duration` | `duration` | Maximum amount of time to wait until the metric is marked as 'stale' and removed.   | `\"5m\"`                   | no       |\n| `prefix`            | `string`   | The prefix to the metric name.                                                      | `\"loki_process_custom_\"` | no       |\n| `source`            | `string`   | Key from the extracted data map to use for the metric. Defaults to the metric name. | `\"\"`                     | no       |\n| `value`             | `string`   | If set, the metric only changes if `source` exactly matches the `value`.            | `\"\"`                     | no       |\n\nThe valid `action` values are `inc`, `dec`, `set`, `add`, or `sub`.\n`inc` and `dec` increment and decrement the metric's value by 1 respectively.\nIf `set`, `add`, or `sub` is chosen, the extracted value must be convertible to a positive float and is set, added to, or subtracted from the metric's value.\n```\n\n----------------------------------------\n\nTITLE: Blackbox Exporter with Embedded Configuration\nDESCRIPTION: Example using an embedded configuration string instead of a configuration file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.blackbox.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.blackbox \"example\" {\n  config = \"{ modules: { http_2xx: { prober: http, timeout: 5s } } }\"\n\n  target {\n    name    = \"example\"\n    address = \"https://example.com\"\n    module  = \"http_2xx\"\n  }\n\n  target {\n    name    = \"grafana\"\n    address = \"https://grafana.com\"\n    module  = \"http_2xx\"\n    labels = {\n      \"env\" = \"dev\",\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: AWS Lambda Resource Attributes Configuration\nDESCRIPTION: Configuration block for AWS Lambda resource attributes including cloud platform, provider, region, function details, and log information. Each attribute can be toggled and is enabled by default.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.resourcedetection.md#2025-04-22_snippet_1\n\nLANGUAGE: hcl\nCODE:\n```\nresource_attributes {\n  aws.log.group.names { enabled = true }\n  aws.log.stream.names { enabled = true }\n  cloud.platform { enabled = true }\n  cloud.provider { enabled = true }\n  cloud.region { enabled = true }\n  faas.instance { enabled = true }\n  faas.max_memory { enabled = true }\n  faas.name { enabled = true }\n  faas.version { enabled = true }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Host Metadata Block for Datadog Exporter in YAML\nDESCRIPTION: The host_metadata block configures settings for host metadata reporting, including enabling the feature, hostname source, and tags.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.datadog.md#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nhost_metadata:\n  enabled: true\n  hostname_source: \"config_or_system\"\n  tags:\n    - \"key1:value1\"\n    - \"key2:value2\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Swarm Service Discovery with Tasks Role\nDESCRIPTION: Example configuration for discovering targets from Docker Swarm tasks using discovery.dockerswarm. Includes filter configuration for task ID and desired state, and sets up Prometheus scraping with remote write capability.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.dockerswarm.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.dockerswarm \"example\" {\n  host = \"unix:///var/run/docker.sock\"\n  role = \"tasks\"\n\n  filter {\n    name = \"id\"\n    values = [\"0kzzo1i0y4jz6027t0k7aezc7\"]\n  }\n\n  filter {\n    name = \"desired-state\"\n    values = [\"running\", \"accepted\"]\n  }\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.dockerswarm.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Sums Block for Datadog Exporter in YAML\nDESCRIPTION: The sums block configures settings for reporting cumulative monotonic sums, including mode and initial value handling.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.datadog.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nsums:\n  cumulative_monotonic_mode: \"to_delta\"\n  initial_cumulative_monotonic_value: \"auto\"\n```\n\n----------------------------------------\n\nTITLE: Proxy Environment Variables Example\nDESCRIPTION: Examples of how to set proxy-related environment variables including HTTP_PROXY, HTTPS_PROXY, and NO_PROXY with various formats for IP addresses, CIDR blocks, and domains.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/cli/environment-variables.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nHTTP_PROXY=http://proxy.example.com\nHTTPS_PROXY=http://proxy.example.com\nNO_PROXY=1.2.3.4,1.2.3.4:80,1.2.3.4/8,example.com,.example.com,*\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.receiver.zipkin with Batch Processor and OTLP Exporter in Alloy\nDESCRIPTION: Example configuration that forwards received traces through a batch processor before sending to an OTLP-capable endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.zipkin.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.zipkin \"default\" {\n  output {\n    traces = [otelcol.processor.batch.default.input]\n  }\n}\n\notelcol.processor.batch \"default\" {\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n    logs    = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = sys.env(\"OTLP_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Cumulative to Delta Processor Configuration in Alloy\nDESCRIPTION: Basic usage example showing how to configure the cumulative to delta processor component with required output block.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.cumulativetodelta.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.cumulativetodelta \"<LABEL>\" {\n  output {\n    metrics = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Scraping and Remote Write\nDESCRIPTION: Example configuration for scraping Prometheus metrics and sending them to a prometheus.receive_http component using prometheus.remote_write.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.receive_http.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\n// Collects metrics of localhost:12345\nprometheus.scrape \"self\" {\n  targets = [\n    {\"__address__\" = \"localhost:12345\", \"job\" = \"alloy\"},\n  ]\n  forward_to = [prometheus.remote_write.local.receiver]\n}\n\n// Writes metrics to localhost:9999/api/v1/metrics/write - e.g. served by\n// the prometheus.receive_http component from the example above.\nprometheus.remote_write \"local\" {\n  endpoint {\n    url = \"http://localhost:9999/api/v1/metrics/write\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Multiple Discovery Sources Configuration\nDESCRIPTION: Example demonstrating how to combine multiple discovery sources using array.concat to supply targets to the processor.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.discovery.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.http \"dynamic_targets\" {\n    url              = \"https://example.com/scrape_targets\"\n    refresh_interval = \"15s\"\n}\n\ndiscovery.kubelet \"k8s_pods\" {\n  bearer_token_file = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n  namespaces        = [\"default\", \"kube-system\"]\n}\n\notelcol.processor.discovery \"default\" {\n    targets = array.concat(discovery.http.dynamic_targets.targets, discovery.kubelet.k8s_pods.targets)\n\n    output {\n        traces = [otelcol.exporter.otlp.default.input]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic PodMonitors Configuration in Alloy\nDESCRIPTION: Basic configuration for setting up prometheus.operator.podmonitors component with required forward_to parameter to specify metrics receivers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.operator.podmonitors.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.operator.podmonitors \"<LABEL>\" {\n    forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Linode Discovery Configuration\nDESCRIPTION: Basic configuration for Linode service discovery requiring a bearer token for authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.linode.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.linode \"<LABEL>\" {\n    bearer_token = \"<LINODE_API_TOKEN>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.auth.sigv4 with APS Workspaces Endpoint\nDESCRIPTION: Demonstrates how to configure otelcol.auth.sigv4 with an APS Workspaces endpoint, where region and service are inferred.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.auth.sigv4.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlp \"example\" {\n  client {\n    endpoint = \"https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-XXX/api/v1/remote_write\"\n    auth     = otelcol.auth.sigv4.creds.handler\n  }\n}\n\notelcol.auth.sigv4 \"creds\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Defining otelcol.processor.interval Component in Alloy\nDESCRIPTION: Basic usage example of the otelcol.processor.interval component in Alloy configuration. It shows how to define the component and specify its output.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.interval.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.interval \"LABEL\" {\n  output {\n    metrics = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Export Block Structure in Alloy\nDESCRIPTION: Demonstrates the basic syntax for creating an export block that specifies a named exported value. The block requires a label and a value argument.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/export.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nexport \"ARGUMENT_NAME\" {\n  value = ARGUMENT_VALUE\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Live Debugging in Grafana Alloy\nDESCRIPTION: This snippet demonstrates how to enable the live debugging feature using the livedebugging configuration block. When enabled, this feature streams real-time data from components to the Grafana Alloy UI, allowing for more effective troubleshooting.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/livedebugging.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nlivedebugging {\n  enabled = true\n}\n```\n\n----------------------------------------\n\nTITLE: Kubernetes RBAC Configuration for Alloy\nDESCRIPTION: This YAML configuration sets up the necessary RBAC rules to authorize Alloy to query the Kubernetes REST API. It creates a ServiceAccount, ClusterRole, and ClusterRoleBinding.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/mimir/mimir.rules.kubernetes.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: alloy\n  namespace: default\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: alloy\nrules:\n- apiGroups: [\"\"]\n  resources: [\"namespaces\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"monitoring.coreos.com\"]\n  resources: [\"prometheusrules\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: alloy\nsubjects:\n- kind: ServiceAccount\n  name: alloy\n  namespace: default\nroleRef:\n  kind: ClusterRole\n  name: alloy\n  apiGroup: rbac.authorization.k8s.io\n```\n\n----------------------------------------\n\nTITLE: Extracting Kubernetes Labels Using Regex Pattern in Alloy\nDESCRIPTION: Configuration block demonstrating how to extract Kubernetes labels with a specific prefix pattern and transform them into custom tags. This example captures labels with 'kubernetes.io/' prefix and creates new tags without the prefix using regex capturing groups.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/shared/reference/components/extract-field-block.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nextract {\n  label {\n    from = \"pod\"\n    key_regex = \"kubernetes.io/(.*)\"\n    tag_name  = \"$1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Grafana Alloy in Linux Docker Container\nDESCRIPTION: This shell command runs Grafana Alloy in a Linux Docker container. It mounts a configuration file, exposes port 12345, and specifies runtime arguments including the HTTP listen address and storage path.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/docker.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker run \\\n  -v <CONFIG_FILE_PATH>:/etc/alloy/config.alloy \\\n  -p 12345:12345 \\\n  grafana/alloy:latest \\\n    run --server.http.listen-addr=0.0.0.0:12345 --storage.path=/var/lib/alloy/data \\\n    /etc/alloy/config.alloy\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenStack Discovery in Grafana Alloy\nDESCRIPTION: Basic configuration for OpenStack discovery in Grafana Alloy, specifying the role and region.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.openstack.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.openstack \"<LABEL>\" {\n  role   = \"<OPENSTACK_ROLE>\"\n  region = \"<OPENSTACK_REGION>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.receiver.kafka in Alloy\nDESCRIPTION: This snippet demonstrates how to configure the otelcol.receiver.kafka component in Alloy. It shows the basic structure with required arguments and the output block.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.kafka.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.kafka \"LABEL\" {\n  brokers          = [\"BROKER_ADDR\"]\n  protocol_version = \"PROTOCOL_VERSION\"\n\n  output {\n    metrics = [...]\n    logs    = [...]\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring stage.docker in loki.process\nDESCRIPTION: Shows the configuration for the stage.docker block in loki.process. This stage enables a predefined pipeline for processing Docker log format entries.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nstage.docker {}\n```\n\n----------------------------------------\n\nTITLE: Enabling Live Debugging in Grafana Alloy Configuration\nDESCRIPTION: This code snippet demonstrates how to enable live debugging in Grafana Alloy. By setting 'enabled' to true within the 'livedebugging' configuration block, debugging data becomes visible in the Grafana Alloy UI.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-windows.md#2025-04-22_snippet_10\n\nLANGUAGE: alloy\nCODE:\n```\nlivedebugging {\n  enabled = true\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying IAM Permissions for Transit Gateway Metrics\nDESCRIPTION: This text block lists additional IAM permissions required for the CloudWatch exporter to collect Transit Gateway attachment metrics, including various EC2 describe actions.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.cloudwatch.md#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n\"ec2:DescribeTags\",\n\"ec2:DescribeInstances\",\n\"ec2:DescribeRegions\",\n\"ec2:DescribeTransitGateway*\"\n```\n\n----------------------------------------\n\nTITLE: Metadata-based Batch Processing Configuration\nDESCRIPTION: Advanced configuration showing batch processor setup with metadata-based batching for multi-tenant scenarios.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.batch.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.jaeger \"default\" {\n  protocols {\n    grpc {\n      include_metadata = true\n    }\n    thrift_http {}\n    thrift_binary {}\n    thrift_compact {}\n  }\n\n  output {\n    traces = [otelcol.processor.batch.default.input]\n  }\n}\n\notelcol.processor.batch \"default\" {\n  metadata_keys = [\"tenant_id\"]\n  metadata_cardinality_limit = 123\n\n  output {\n    traces  = [otelcol.exporter.otlp.production.input]\n  }\n}\n\notelcol.exporter.otlp \"production\" {\n  client {\n    endpoint = sys.env(\"OTLP_SERVER_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Target JVM for Accurate Profiling\nDESCRIPTION: Java command line flags to add to the target JVM for ensuring accurate profiling, especially for inlined methods.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.java.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n-XX:+UnlockDiagnosticVMOptions -XX:+DebugNonSafepoints\n```\n\n----------------------------------------\n\nTITLE: Basic Discovery Relabel Usage in Alloy\nDESCRIPTION: Basic syntax for configuring a discovery.relabel component with targets and rules.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.relabel.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.relabel \"<LABEL>\" {\n  targets = \"<TARGET_LIST>\"\n\n  rule {\n    ...\n  }\n\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of prometheus.exporter.apache in Alloy\nDESCRIPTION: Demonstrates the basic syntax for using the prometheus.exporter.apache component in an Alloy configuration file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.apache.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.apache \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Sampling in Alloy\nDESCRIPTION: Sets up a sampling stage that processes 25% of logs and drops the remaining 75%. Adds a custom label to the dropped logs metric for better observability.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_33\n\nLANGUAGE: alloy\nCODE:\n```\nstage.sampling {\n    rate = 0.25\n    drop_counter_reason = \"logs_sampling\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Component with Declare Block in Alloy\nDESCRIPTION: This snippet demonstrates the basic structure of a 'declare' block in Alloy. It shows how to define a new custom component named 'COMPONENT_NAME' with a placeholder for the component definition.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/declare.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndeclare \"COMPONENT_NAME\" {\n    COMPONENT_DEFINITION\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring vCenter Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration example for setting up a vCenter metrics receiver. Shows how to specify the endpoint, credentials and output configuration for collecting metrics from a vCenter or ESXi host.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.vcenter.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.vcenter \"LABEL\" {\n  endpoint = \"VCENTER_ENDPOINT\"\n  username = \"VCENTER_USERNAME\"\n  password = \"VCENTER_PASSWORD\"\n\n  output {\n    metrics = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic OVHcloud Discovery Configuration\nDESCRIPTION: Basic configuration snippet for setting up OVHcloud service discovery with required parameters including application keys and service type.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.ovhcloud.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.ovhcloud \"<LABEL>\" {\n    application_key    = \"<APPLICATION_KEY>\"\n    application_secret = \"<APPLICATION_SECRET>\"\n    consumer_key       = \"<CONSUMER_KEY>\"\n    service            = \"<SERVICE>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Excluding Spans Based on Library Version in OpenTelemetry Collector\nDESCRIPTION: This configuration illustrates how to exclude spans from processing based on a specific library version. It uses a strict match type and defines library name and version criteria for exclusion. The example notes that this configuration is not applicable to metrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.attributes.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.attributes \"default\" {\n    exclude {\n        match_type = \"strict\"\n        library {\n            name = \"mongo-java-driver\"\n            version = \"3.8.0\"\n        }\n    }\n    action {\n        key = \"credit_card\"\n        action = \"delete\"\n    }\n    action {\n        key = \"duplicate_key\"\n        action = \"delete\"\n    }\n    output {\n        logs    = [otelcol.exporter.otlp.default.input]\n        traces  = [otelcol.exporter.otlp.default.input]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki API Source in Grafana Alloy\nDESCRIPTION: Alloy configuration for the loki.source.api component that receives log entries over HTTP and forwards them to other Loki components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-structured-logs.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.api \"loki_push_api\" {\n    http {\n        listen_address = \"0.0.0.0\"\n        listen_port = 9999\n    }\n    forward_to = [\n        loki.process.labels.receiver,\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Delta to Cumulative Processor Configuration\nDESCRIPTION: Demonstrates basic usage of the delta to cumulative processor component with output to OTLP exporter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.deltatocumulative.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.deltatocumulative \"LABEL\" {\n  output {\n    metrics = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of import.string in Alloy\nDESCRIPTION: Demonstrates the basic syntax for importing custom components from a string into a specified namespace in Alloy. The block requires a namespace label and content argument.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/import.string.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nimport.string \"NAMESPACE\" {\n  content = CONTENT\n}\n```\n\n----------------------------------------\n\nTITLE: GCP Trace Configuration Parameters\nDESCRIPTION: Configuration parameters for trace export functionality, including attribute mappings, endpoint settings, and connection pool configuration\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.googlecloud.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Name                                 | Type           | Description                                                                                                                                                                      | Default                         | Required |\n|--------------------------------------|----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------|----------|\n| `attribute_mappings`                 | `list(object)` | Determines how to map from OpenTelemetry attribute keys to Google Cloud Trace keys. By default, it changes HTTP and service keys so that they appear more prominently in the UI. | `[]`                            | no       |\n```\n\n----------------------------------------\n\nTITLE: PodMonitors Configuration with Label Selector and Namespace Filtering\nDESCRIPTION: Configures PodMonitors discovery with specific namespace and label filtering. This example targets only PodMonitors in the 'my-app' namespace that have the 'team=ops' label.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.operator.podmonitors.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.operator.podmonitors \"pods\" {\n    forward_to = [prometheus.remote_write.staging.receiver]\n    namespaces = [\"my-app\"]\n    selector {\n        match_expression {\n            key = \"team\"\n            operator = \"In\"\n            values = [\"ops\"]\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana Alloy as Linux Systemd Service\nDESCRIPTION: This systemd service file defines how to run Grafana Alloy as a service. It includes service dependencies, restart behavior, and execution parameters. Replace <WORKING_DIRECTORY> and <BINARY_PATH> with appropriate values.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/binary.md#2025-04-22_snippet_2\n\nLANGUAGE: systemd\nCODE:\n```\n[Unit]\nDescription=Vendor-neutral programmable observability pipelines.\nDocumentation=https://grafana.com/docs/alloy/\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nRestart=always\nUser=alloy\nEnvironment=HOSTNAME=%H\nEnvironmentFile=/etc/default/alloy\nWorkingDirectory=<WORKING_DIRECTORY>\nExecStart=<BINARY_PATH> run $CUSTOM_ARGS --storage.path=<WORKING_DIRECTORY> $CONFIG_FILE\nExecReload=/usr/bin/env kill -HUP $MAINPID\nTimeoutStopSec=20s\nSendSIGKILL=no\n\n[Install]\nWantedBy=multi-user.target\n```\n\n----------------------------------------\n\nTITLE: String Declaration in Alloy Syntax\nDESCRIPTION: Example of a basic string declaration in Alloy syntax, enclosed in double quotes.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/expressions/types_and_values.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\n\"Hello, world!\"\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.connector.spanmetrics in Alloy\nDESCRIPTION: Basic configuration structure for otelcol.connector.spanmetrics in Alloy, including histogram and output blocks.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.spanmetrics.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.connector.spanmetrics \"LABEL\" {\n  histogram {\n    ...\n  }\n\n  output {\n    metrics = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki AWS Firehose Source in Alloy\nDESCRIPTION: Basic configuration for setting up a loki.source.awsfirehose component that listens for HTTP requests from AWS Firehose and forwards logs to specified receivers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.awsfirehose.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.awsfirehose \"<LABEL>\" {\n    http {\n        listen_address = \"<LISTEN_ADDRESS>\"\n        listen_port = \"<PORT>\"\n    }\n    forward_to = RECEIVER_LIST\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Metrics Output in Mimir\nDESCRIPTION: This snippet shows examples of metrics that may be generated and stored in Mimir as a result of the service graph connector configuration. These metrics include request totals and failed request totals with various labels.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.servicegraph.md#2025-04-22_snippet_2\n\nLANGUAGE: prometheus\nCODE:\n```\ntraces_service_graph_request_total{client=\"shop-backend\",failed=\"false\",server=\"article-service\",client_http_method=\"DELETE\",server_http_method=\"DELETE\"}\ntraces_service_graph_request_failed_total{client=\"shop-backend\",client_http_method=\"POST\",failed=\"false\",server=\"auth-service\",server_http_method=\"POST\"}\n```\n\n----------------------------------------\n\nTITLE: Setting Log Level Argument in Alloy\nDESCRIPTION: This snippet shows how to set a constant value for a component argument, specifically setting the log_level to \"debug\".\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/components.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nlog_level = \"debug\"\n```\n\n----------------------------------------\n\nTITLE: Basic prometheus.write.queue Configuration in Alloy\nDESCRIPTION: Basic configuration template for setting up a prometheus.write.queue component with an endpoint. The component requires a label and at least one endpoint with a remote write URL.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.write.queue.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.write.queue \"<LABEL>\" {\n  endpoint \"default \"{\n    url = \"<REMOTE_WRITE_URL>\"\n\n    ...\n  }\n\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Relabel Rules to Discovered Targets in Alloy\nDESCRIPTION: An example demonstrating how to apply additional relabel rules to discovered targets to filter by hostname. This is useful when running Grafana Alloy as a DaemonSet, ensuring each node only scrapes targets assigned to it.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.operator.probes.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.operator.probes \"probes\" {\n    forward_to = [prometheus.remote_write.staging.receiver]\n    rule {\n      action = \"keep\"\n      regex = sys.env(\"HOSTNAME\")\n      source_labels = [\"__meta_kubernetes_pod_node_name\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: 15% Log Sampling Configuration\nDESCRIPTION: Configuration example showing how to sample 15% of logs using the probabilistic sampler.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.probabilistic_sampler.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.probabilistic_sampler \"default\" {\n  sampling_percentage = 15\n\n  output {\n    logs = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.receiver.loki in Alloy\nDESCRIPTION: Basic configuration structure for the otelcol.receiver.loki component. It requires an output block to specify where to send converted telemetry data.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.loki.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.loki \"LABEL\" {\n  output {\n    logs = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring discovery.file in Alloy\nDESCRIPTION: Basic usage of the discovery.file component in Alloy configuration. It specifies how to declare the component and its required arguments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.file.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.file \"<LABEL>\" {\n  files = [\"<FILE_PATH_1>\", \"<FILE_PATH_2>\", ...]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring loki.source.gelf Component in Alloy\nDESCRIPTION: Basic usage example for the loki.source.gelf component. It defines the component with a label and specifies the forward_to argument.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.gelf.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.gelf \"<LABEL>\" {\n  forward_to    = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Relabeling Metrics with prometheus.relabel in Alloy\nDESCRIPTION: This snippet demonstrates how to use the prometheus.relabel component to add an 'os' label to metrics scraped from a unix exporter. It uses the constants.os value to set the label and forwards the relabeled metrics to a Prometheus remote write endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/logs-and-relabeling-basics.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.unix \"localhost\" { }\n\nprometheus.scrape \"default\" {\n    scrape_interval = \"10s\"\n\n    targets    = prometheus.exporter.unix.localhost.targets\n    forward_to = [\n        prometheus.relabel.example.receiver,\n    ]\n}\n\nprometheus.relabel \"example\" {\n    forward_to = [\n        prometheus.remote_write.local_prom.receiver,\n    ]\n\n    rule {\n        action       = \"replace\"\n        target_label = \"os\"\n        replacement  = constants.os\n    }\n}\n\nprometheus.remote_write \"local_prom\" {\n    endpoint {\n        url = \"http://localhost:9090/api/v1/write\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Capturing CPU Profiles in Grafana Alloy\nDESCRIPTION: Command to collect a CPU profile from Grafana Alloy over a specified duration (30 seconds). This is useful for diagnosing high CPU consumption issues.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/troubleshoot/profile.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:12345/debug/pprof/profile?seconds=30 -o cpu.pprof\n```\n\n----------------------------------------\n\nTITLE: Filtering Non-Essential Logs with Loki.process in Alloy\nDESCRIPTION: This component filters out specific log entries using the stage.drop functionality. It looks for log messages matching a specific expression and drops them before forwarding the remaining logs to the Loki writer component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/send-logs-to-loki.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\n  loki.process \"filter_logs\" {\n    stage.drop {\n        source = \"\"\n        expression  = \".*Connection closed by authenticating user root\"\n        drop_counter_reason = \"noisy\"\n      }\n    forward_to = [loki.write.grafana_loki.receiver]\n    }\n```\n\n----------------------------------------\n\nTITLE: Basic Blackbox Exporter Configuration\nDESCRIPTION: Basic usage example of prometheus.exporter.blackbox component with a single target configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.blackbox.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.blackbox \"<LABEL>\" {\n  target {\n    name    = \"<NAME>\"\n    address = \"<EXAMPLE_ADDRESS>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Grouping Metrics with otelcol.processor.groupbyattrs\nDESCRIPTION: This example demonstrates how to configure otelcol.processor.groupbyattrs to group metrics based on the 'host.name' attribute. It shows the component definition and how to set the output.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.groupbyattrs.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.groupbyattrs \"default\" {\n  keys = [ \"host.name\" ]\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic MongoDB Exporter Configuration in Alloy\nDESCRIPTION: Basic configuration example showing how to set up the MongoDB Prometheus exporter with a MongoDB URI.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.mongodb.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.mongodb \"<LABEL>\" {\n    mongodb_uri = \"<MONGODB_URI>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Hetzner Discovery Configuration\nDESCRIPTION: Basic configuration structure for discovering Hetzner targets using the discovery.hetzner component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.hetzner.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.hetzner \"<LABEL>\" {\n  role = \"<HETZNER_ROLE>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Instance Label Configuration with Relabeling\nDESCRIPTION: Example showing how to set the instance label to target URL using Prometheus relabeling.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.blackbox.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.blackbox \"example\" {\n  config = \"{ modules: { http_2xx: { prober: http, timeout: 5s } } }\"\n\n  target {\n    name    = \"example\"\n    address = \"example.com\"\n    module = \"http_2xx\"\n  }\n}\n\ndiscovery.relabel \"example\" {\n  targets = prometheus.exporter.blackbox.example.targets\n\n  rule {\n    source_labels = [\"__param_target\"]\n    target_label = \"instance\"\n  }\n}\n\nprometheus.scrape \"example\" {\n  targets = discovery.relabel.example.output\n  forward_to = [prometheus.remote_write.metrics_service.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Dnsmasq Exporter Configuration\nDESCRIPTION: Basic configuration structure for prometheus.exporter.dnsmasq component\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.dnsmasq.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.dnsmasq \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Prometheus Metrics Before Relabeling\nDESCRIPTION: Provides an example set of Prometheus metrics before applying the relabeling rules, demonstrating various label combinations.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.relabel.md#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nmetric_a{__address__ = \"localhost\", instance = \"development\", app = \"frontend\"} 10\nmetric_a{__address__ = \"localhost\", instance = \"development\", app = \"backend\"}  2\nmetric_a{__address__ = \"cluster_a\", instance = \"production\",  app = \"frontend\"} 7\nmetric_a{__address__ = \"cluster_a\", instance = \"production\",  app = \"backend\"}  9\nmetric_a{__address__ = \"cluster_b\", instance = \"production\",  app = \"database\"} 4\n```\n\n----------------------------------------\n\nTITLE: OTLP HTTP Exporter with Custom Bearer Scheme\nDESCRIPTION: Example showing OTLP HTTP exporter configuration with custom bearer scheme authentication using environment variable as token source.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.auth.bearer.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlphttp \"example\" {\n  client {\n    endpoint = \"my-otlp-grpc-server:4317\"\n    auth     = otelcol.auth.bearer.creds.handler\n  }\n}\n\notelcol.auth.bearer \"creds\" {\n  token = sys.env(\"API_KEY\")\n  scheme = \"MyScheme\"\n}\n```\n\n----------------------------------------\n\nTITLE: Converting String Case with Template Functions\nDESCRIPTION: Demonstrates using the ToLower and ToUpper template functions to transform string case in two different syntax styles.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_40\n\nLANGUAGE: alloy\nCODE:\n```\nstage.template {\n    source   = \"out\"\n    template = \"{{ ToLower .app }}\"\n}\nstage.template {\n    source   = \"out\"\n    template = \"{{ .app | ToUpper }}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cluster Name in Kubernetes Monitoring Helm Chart\nDESCRIPTION: YAML configuration for defining the cluster name in the Kubernetes Monitoring Helm chart. This adds a static label to all logs collected.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\ncluster:\n  name: meta-monitoring-tutorial\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.processor.groupbyattrs in Alloy\nDESCRIPTION: This snippet shows the basic usage of the otelcol.processor.groupbyattrs component in Alloy. It demonstrates how to define the component with a label and configure its output.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.groupbyattrs.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.groupbyattrs \"LABEL\" {\n  output {\n    metrics = [...]\n    logs    = [...]\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: GCP Metric Configuration Parameters\nDESCRIPTION: Configuration parameters for the metric export functionality, including compression settings, buffer sizes, endpoint configuration, and resource filters\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.googlecloud.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Name                                   | Type           | Description                                                                                                                                                                                                                 | Default                                                  | Required |\n|----------------------------------------|----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------|----------|\n| `compression`                          | `string`       | Compression format for Metrics gRPC requests. Supported values: [`gzip`].                                                                                                                                                   | `\"\"` (no compression)                                    | no       |\n```\n\n----------------------------------------\n\nTITLE: Defining OpenTelemetry Component Arguments Structure\nDESCRIPTION: Defines the configuration structure for the OpenTelemetry processor component including attributes, values, and affect flags. Includes Output and DebugMetrics blocks for telemetry handling.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/add-otel-component.md#2025-04-22_snippet_1\n\nLANGUAGE: go\nCODE:\n```\ntype Arguments struct {\n    Attribute string `alloy:\"attribute,attr\"`\n    Value string `alloy:\"value,attr\"`\n\n    AffectLogs bool `alloy:\"affect_logs,attr,optional\"`\n    AffectMetrics bool `alloy:\"affect_metrics,attr,optional\"`\n    AffectTraces bool `alloy:\"affect_traces,attr,optional\"`\n\n    // Output configures where to send processed data. Required.\n    Output *otelcol.ConsumerArguments `alloy:\"output,block\"`\n\n    // DebugMetrics configures component internal metrics. Optional.\n    DebugMetrics otelcolCfg.DebugMetricsArguments `alloy:\"debug_metrics,block,optional\"`\n}\n```\n\n----------------------------------------\n\nTITLE: Priority-based Log Sampling Configuration\nDESCRIPTION: Configuration example demonstrating log sampling based on a priority attribute.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.probabilistic_sampler.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.probabilistic_sampler \"default\" {\n  sampling_percentage = 15\n  sampling_priority   = \"priority\"\n\n  output {\n    logs = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: OpenShift ClusterRole Configuration\nDESCRIPTION: YAML configuration defining required OpenShift API permissions for resource attribute collection. Specifies get, watch, and list permissions for infrastructures resources.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.resourcedetection.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nkind: ClusterRole\nmetadata:\n  name: alloy\nrules:\n- apiGroups: [\"config.openshift.io\"]\n  resources: [\"infrastructures\", \"infrastructures/status\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n```\n\n----------------------------------------\n\nTITLE: Using import.http and Custom Component in Alloy\nDESCRIPTION: This example demonstrates how to use the import.http block to retrieve custom components from an HTTP server, and then instantiate a custom 'add' component. It would be stored in a file named main.alloy.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/import.http.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nimport.http \"math\" {\n  url = SERVER_URL\n}\n\nmath.add \"default\" {\n  a = 15\n  b = 45\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Pyroscope HTTP Receiver Configuration in Alloy\nDESCRIPTION: Basic configuration structure for setting up a pyroscope.receive_http component with HTTP server settings and forward_to configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.receive_http.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\npyroscope.receive_http \"<LABEL>\" {\n  http {\n    listen_address = \"<LISTEN_ADDRESS>\"\n    listen_port = \"<PORT>\"\n  }\n  forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Grafana Alloy Service Status\nDESCRIPTION: Command to verify the running status of Grafana Alloy service\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/linux.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemctl status alloy\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.receiver.awscloudwatch in Alloy\nDESCRIPTION: Basic configuration for the otelcol.receiver.awscloudwatch component, specifying the AWS region and output for logs.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.awscloudwatch.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.awscloudwatch \"<LABEL>\" {\n  region = \"us-west-2\"\n\n  output {\n    logs = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Syslog Exporter Configuration in Alloy\nDESCRIPTION: Basic usage example showing how to configure the syslog exporter with a required endpoint parameter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.syslog.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.syslog \"LABEL\" {\n  endpoint = \"HOST\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Destinations in Kubernetes Monitoring Helm Chart\nDESCRIPTION: YAML configuration for defining a destination named 'loki' to forward logs to Loki in the Kubernetes Monitoring Helm chart.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\ndestinations:\n  - name: loki\n    type: loki\n    url: http://loki-gateway.meta.svc.cluster.local/loki/api/v1/push\n```\n\n----------------------------------------\n\nTITLE: Configuring Eureka Discovery in Alloy\nDESCRIPTION: Basic configuration for discovering instances in a Eureka Registry and exposing them as targets.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.eureka.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.eureka \"<LABEL>\" {\n    server = \"<SERVER>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Sourcemap Location Configuration in Alloy\nDESCRIPTION: Example configuration for specifying sourcemap location in the faro.receiver component, demonstrating how to map minified paths to local sourcemap files.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/faro/faro.receiver.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nlocation {\n    path                 = \"/var/my-app/build\"\n    minified_path_prefix = \"http://example.com/\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Loki Relabel Component Structure\nDESCRIPTION: Shows the basic structure for configuring a loki.relabel component with rules and forward_to arguments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.relabel.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.relabel \"<LABEL>\" {\n  forward_to = <RECEIVER_LIST>\n\n  rule {\n    ...\n  }\n\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of remote.kubernetes.configmap in Alloy\nDESCRIPTION: Basic syntax for using the remote.kubernetes.configmap component to read a ConfigMap from a specific namespace in Kubernetes.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/remote/remote.kubernetes.configmap.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nremote.kubernetes.configmap \"<LABEL>\" {\n  namespace = \"<NAMESPACE_OF_CONFIGMAP>\"\n  name = \"<NAME_OF_CONFIGMAP>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using prometheus.remote_write wal-stats Command in Grafana Alloy CLI\nDESCRIPTION: This command reads a Prometheus Write-Ahead Log (WAL) directory and reports general information about it, including timestamps, series counts, sample counts, segment information, and per-target statistics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/cli/tools.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nalloy tools prometheus.remote_write wal-stats <WAL_DIRECTORY>\n```\n\n----------------------------------------\n\nTITLE: Creating Puppet Manifest for Grafana Alloy Installation\nDESCRIPTION: Ruby-based Puppet manifest that sets up Grafana package repositories, installs the Alloy package, and configures the service to run automatically. It includes platform-specific configuration for both Debian and RedHat family distributions.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/puppet.md#2025-04-22_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nclass grafana_alloy::grafana_alloy () {\n  case $::os['family'] {\n    'debian': {\n      apt::source { 'grafana':\n        location => 'https://apt.grafana.com/',\n        release  => '',\n        repos    => 'stable main',\n        key      => {\n          id     => 'B53AE77BADB630A683046005963FA27710458545',\n          source => 'https://apt.grafana.com/gpg.key',\n        },\n      } -> package { 'alloy':\n        require => Exec['apt_update'],\n      } -> service { 'alloy':\n        ensure    => running,\n        name      => 'alloy',\n        enable    => true,\n        subscribe => Package['alloy'],\n      }\n    }\n    'redhat': {\n      yumrepo { 'grafana':\n        ensure   => 'present',\n        name     => 'grafana',\n        descr    => 'grafana',\n        baseurl  => 'https://packages.grafana.com/oss/rpm',\n        gpgkey   => 'https://packages.grafana.com/gpg.key',\n        enabled  => '1',\n        gpgcheck => '1',\n        target   => '/etc/yum.repo.d/grafana.repo',\n      } -> package { 'alloy':\n      } -> service { 'alloy':\n        ensure    => running,\n        name      => 'alloy',\n        enable    => true,\n        subscribe => Package['alloy'],\n      }\n    }\n    default: {\n      fail(\"Unsupported OS family: (${$::os['family']})\")\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying JSON with json_path Function in Alloy\nDESCRIPTION: Examples of using the json_path function to extract values from JSON strings using JSONPath expressions. The function takes two string parameters: the JSON string and the JSONPath expression. It always returns a list of values, or an empty list if no matches are found.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/json_path.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\n> json_path(\"{\\\"key\\\": \\\"value\\\"}\", \".key\")\n[\"value\"]\n\n\n> json_path(\"[{\\\"name\\\": \\\"Department\\\",\\\"value\\\": \\\"IT\\\"},{\\\"name\\\":\\\"TestStatus\\\",\\\"value\\\":\\\"Pending\\\"}]\", \"[?(@.name == \\\"Department\\\")].value\")\n[\"IT\"]\n\n> json_path(\"{\\\"key\\\": \\\"value\\\"}\", \".nonexists\")\n[]\n\n> json_path(\"{\\\"key\\\": \\\"value\\\"}\", \".key\")[0]\nvalue\n```\n\n----------------------------------------\n\nTITLE: Basic Bearer Token Configuration in Alloy\nDESCRIPTION: Basic example showing how to configure bearer token authentication with required token parameter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.auth.bearer.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.auth.bearer \"LABEL\" {\n  token = \"TOKEN\"\n}\n```\n\n----------------------------------------\n\nTITLE: Testing TLS Configuration with cURL on Windows\nDESCRIPTION: This command uses curl with client certificates to access metrics on a local server. It demonstrates how to specify the client key, certificate, and target URL for accessing metrics in an insecure mode.\nSOURCE: https://github.com/grafana/alloy/blob/main/internal/static/server/testdata/windows/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n.\\curl.exe -v -GET --key .\\client_key_unencrypted.key --cert .\\client_cert.crt --insecure https://localhost:12345/metrics\n```\n\n----------------------------------------\n\nTITLE: Parsing YAML Strings to Alloy Values with encoding.from_yaml\nDESCRIPTION: Demonstrates how to decode a string representing YAML into an Alloy value. This is commonly used to decode the output of a local.file component. The function fails if the string argument provided cannot be parsed as YAML.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/encoding.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\n> encoding.from_yaml(\"15\")\n15\n> encoding.from_yaml(\"[1, 2, 3]\")\n[1, 2, 3]\n> encoding.from_yaml(\"null\")\nnull\n> encoding.from_yaml(\"key: value\")\n{\n  key = \"value\",\n}\n> encoding.from_yaml(local.file.some_file.content)\n\"Hello, world!\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Write Destination in Grafana Alloy\nDESCRIPTION: Alloy configuration for the loki.write component that writes processed logs to a Loki endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-structured-logs.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\nloki.write \"local\" {\n  endpoint {\n    url = \"http://loki:3100/loki/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics Block for Datadog Exporter in YAML\nDESCRIPTION: The metrics block sets up metric-specific exporter settings, including delta TTL and endpoint configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.datadog.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  delta_ttl: 3600\n  endpoint: \"https://api.datadoghq.com\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Write Component in Alloy\nDESCRIPTION: Configuration block for the loki.write component that sends logs to a Loki endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-syslog-messages.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\nloki.write \"local\" {\n  endpoint {\n    url = \"http://loki:3100/loki/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabled Profiling Endpoints for Pyroscope\nDESCRIPTION: Lists the specific endpoints that will be scraped by the Pyroscope scraper with the given configuration. These endpoints are scraped every 15 seconds to collect various types of profiling data.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.scrape.md#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nhttp://localhost:12345/debug/pprof/allocs\nhttp://localhost:12345/debug/pprof/goroutine\nhttp://localhost:12345/debug/pprof/profile?seconds=14\nhttp://localhost:12345/debug/fgprof?seconds=14\n```\n\n----------------------------------------\n\nTITLE: GCP WAL Configuration Parameters\nDESCRIPTION: Configuration options for the Write-Ahead Log (WAL) experimental feature, including directory path and retry settings\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.googlecloud.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Name          | Type     | Description                                                                              | Default | Required |\n|---------------|----------|------------------------------------------------------------------------------------------|---------|----------|\n| `directory`   | `string` | Path to local directory for the WAL file.                                                | `./`    | yes      |\n| `max_backoff` | `string` | Max duration to retry requests on network errors (`UNAVAILABLE` or `DEADLINE_EXCEEDED`). | `1h`    | no       |\n```\n\n----------------------------------------\n\nTITLE: Defining import.http Block in Alloy\nDESCRIPTION: This snippet demonstrates the basic structure of an import.http block in Alloy. It includes the label and url parameters, which are essential for retrieving a module from an HTTP server.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/import.http.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nimport.http \"LABEL\" {\n  url = URL\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of pyroscope.relabel Component in Alloy\nDESCRIPTION: Basic syntax for configuring a pyroscope.relabel component, showing the required forward_to parameter and rule block structure.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.relabel.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\npyroscope.relabel \"<LABEL>\" {\n    forward_to = <RECEIVER_LIST>\n\n    rule {\n        ...\n    }\n\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Local File Configuration Block in Alloy\nDESCRIPTION: Example of a labeled local.file block that configures file access with environment variable and secret handling.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/syntax.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file \"token\" {\n  filename  = sys.env(\"TOKEN_FILE_PATH\") // Use an expression to read from an env var.\n  is_secret = true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Operator ScrapeConfigs in Alloy\nDESCRIPTION: This snippet demonstrates how to configure the prometheus.operator.scrapeconfigs component in Alloy. It specifies a label and sets up forwarding of scraped metrics to receivers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.operator.scrapeconfigs.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.operator.scrapeconfigs \"<LABEL>\" {\n    forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logs Block for Datadog Exporter in YAML\nDESCRIPTION: The logs block configures settings for the logs exporter, including endpoint, compression, and batching options.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.datadog.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nlogs:\n  endpoint: \"https://http-intake.logs.datadoghq.com\"\n  use_compression: true\n  compression_level: 6\n  batch_wait: 5\n```\n\n----------------------------------------\n\nTITLE: Basic Filelog Receiver Configuration in Alloy\nDESCRIPTION: Basic configuration structure for the filelog receiver showing required include and output fields.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.filelog.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.filelog \"<LABEL>\" {\n  include = [...]\n  output {\n    logs    = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.exporter.splunkhec in Alloy\nDESCRIPTION: Example configuration for the otelcol.exporter.splunkhec component in Alloy. It sets up a Splunk HEC exporter with a token and endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.splunkhec.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.splunkhec \"LABEL\" {\n    splunk {\n        token = \"YOUR_SPLUNK_TOKEN\"\n    }\n    client {\n        endpoint = \"http://splunk.yourdomain.com:8088\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring stage.decolorize in loki.process\nDESCRIPTION: Demonstrates how to add the stage.decolorize block to loki.process. This stage strips ANSI color codes from log lines for easier parsing.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nstage.decolorize {}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Node Resource Detection with Default Settings in Alloy\nDESCRIPTION: This snippet shows the default configuration for Kubernetes node resource detection, which uses the K8S_NODE_NAME environment variable. No explicit kubernetes_node block is needed as defaults are applied automatically.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.resourcedetection.md#2025-04-22_snippet_11\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.resourcedetection \"default\" {\n  detectors = [\"kubernetes_node\"]\n\n  output {\n    logs    = [otelcol.exporter.otlp.default.input]\n    metrics = [otelcol.exporter.otlp.default.input]\n    traces  = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic OTel Collector File Storage Usage\nDESCRIPTION: Basic usage example showing how to declare a file storage component with a label.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.storage.file.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.storage.file \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Using the fmt Command in Grafana Alloy\nDESCRIPTION: Basic usage syntax for the fmt command to format Grafana Alloy configuration files. The command can read from a file or standard input, and supports flags for writing changes back to disk or testing for proper formatting.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/cli/fmt.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nalloy fmt [<FLAG> ...] <FILE_NAME>\n```\n\n----------------------------------------\n\nTITLE: Configuring File Stats Receiver with Batch Processing and OTLP Export in Alloy\nDESCRIPTION: This snippet demonstrates how to configure the OpenTelemetry Collector to collect file stats from log files, process them in batches, and export to an OTLP endpoint. It includes settings for file inclusion patterns and environment variable usage for the OTLP endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.file_stats.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.file_stats \"default\" {\n  include = \"/var/log/**/*.log\"\n\n  output {\n    metrics = [otelcol.processor.batch.default.input]\n  }\n}\n\notelcol.processor.batch \"default\" {\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n  }\n}\n\notelcol.exporter.otlp \"default\" {\n  client {\n    endpoint = sys.env(\"OTLP_ENDPOINT\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kustomize for Grafana Alloy\nDESCRIPTION: YAML configuration for Kustomize to generate a ConfigMap for Grafana Alloy without appending a hash to the name, preventing unintended rolling updates.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/kubernetes.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nconfigMapGenerator:\n  - name: alloy\n    files:\n      - config.alloy\n    options:\n      disableNameSuffixHash: true\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of discovery.lightsail in Alloy\nDESCRIPTION: Demonstrates the basic syntax for using the discovery.lightsail component in an Alloy configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.lightsail.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.lightsail \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying IAM Permission for API Gateway Discovery\nDESCRIPTION: This text block specifies the IAM permission required to discover tagged API Gateway REST APIs using the CloudWatch exporter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.cloudwatch.md#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n\"apigateway:GET\"\n```\n\n----------------------------------------\n\nTITLE: Using Standard Library Functions in Alloy Configuration\nDESCRIPTION: Demonstrates two examples of standard library function calls in Alloy: retrieving an environment variable with sys.env() and accessing a JSON property using encoding.from_json() with nested operations.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/expressions/function_calls.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nsys.env(\"HOME\")\nencoding.from_json(local.file.cfg.content)[\"namespace\"]\n```\n\n----------------------------------------\n\nTITLE: GeoIP Processing with ASN Database\nDESCRIPTION: Configuration for GeoIP processing using ASN database to extract autonomous system information from IP addresses.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\nloki.process \"example\" {\n    stage.json {\n        expressions = {ip = \"client_ip\"}\n    }\n\n    stage.geoip {\n        source  = \"ip\"\n        db      = \"/path/to/db/GeoIP2-ASN.mmdb\"\n        db_type = \"asn\"\n    }\n\n    stage.labels {\n        values = {\n            geoip_autonomous_system_number       = \"\",\n            geoip_autonomous_system_organization = \"\",\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining an Argument Block in Alloy\nDESCRIPTION: Basic syntax for declaring an argument block in Alloy. The block must be labeled with the argument name.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/argument.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nargument \"ARGUMENT_NAME\" {}\n```\n\n----------------------------------------\n\nTITLE: Grafana Agent to Alloy Migration Admonition in Hugo Shortcode\nDESCRIPTION: A Hugo shortcode that displays a caution admonition informing users that Grafana Agent is deprecated and being replaced by Grafana Alloy, including support dates and a link to learn more about the migration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/shared/agent-deprecation.md#2025-04-22_snippet_0\n\nLANGUAGE: hugo\nCODE:\n```\n{{< admonition type=\"caution\" >}}\nGrafana Alloy is the new name for our distribution of the OTel collector.\nGrafana Agent has been deprecated and is in Long-Term Support (LTS) through October 31, 2025. Grafana Agent will reach an End-of-Life (EOL) on November 1, 2025.\nRead more about why we recommend migrating to [Grafana Alloy][alloy].\n\n[alloy]: https://grafana.com/blog/2024/04/09/grafana-alloy-opentelemetry-collector-with-prometheus-pipelines/\n{{< /admonition >}}\n```\n\n----------------------------------------\n\nTITLE: Service Collector Configuration\nDESCRIPTION: Parameters for collecting Windows service metrics, including options for V2 collector and API-based collection instead of WMI queries.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.windows.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Name                  | Type     | Description                                           | Default   | Required |\n| --------------------- | -------- | ----------------------------------------------------- | --------- | -------- |\n| `enable_v2_collector` | `string` | Enable V2 service collector.                          | `\"false\"` | no       |\n| `use_api`             | `string` | Use API calls to collect service data instead of WMI. | `\"false\"` | no       |\n| `where_clause`        | `string` | WQL 'where' clause to use in WMI metrics query.       | `\"\"`      | no       |\n```\n\n----------------------------------------\n\nTITLE: Setting DD_URL Environment Variable for Exclusive Alloy Forwarding\nDESCRIPTION: Sets the DD_URL environment variable to direct the Datadog Agent to send metrics only to Grafana Alloy instead of the default Datadog endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/datadog-traces-metrics.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nDD_DD_URL='{\"http://<DATADOG_RECEIVER_HOST>:<DATADOG_RECEIVER_PORT>\": [\"datadog-receiver\"]}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Profile Collection in Alloy\nDESCRIPTION: Demonstrates how to configure custom profile collection endpoints with required path and enabled settings. Supports delta profiling with automatic seconds parameter addition.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.scrape.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprofile.custom \"<PROFILE_TYPE>\" {\n  enabled = true\n  path    = \"<PROFILE_PATH>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing otelcol.exporter.prometheus Component in Alloy\nDESCRIPTION: Basic usage of the otelcol.exporter.prometheus component, specifying a label and forward_to argument.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.prometheus.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.prometheus \"LABEL\" {\n  forward_to = [...]\n}\n```\n\n----------------------------------------\n\nTITLE: Using local.file_match in Alloy\nDESCRIPTION: Basic usage of the local.file_match component to discover files on the local filesystem using glob patterns.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/local/local.file_match.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file_match \"LABEL\" {\n  path_targets = [{\"__path__\" = DOUBLESTAR_PATH}]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Pyroscope eBPF Component in Alloy\nDESCRIPTION: Basic configuration structure for setting up a pyroscope.ebpf profiling job. Requires targets list for grouping profiles by container ID and forward_to list to specify profile receivers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.ebpf.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\npyroscope.ebpf \"<LABEL>\" {\n  targets    = <TARGET_LIST>\n  forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Running Grafana Alloy with Configuration File\nDESCRIPTION: This command runs Grafana Alloy using a specified configuration file. The <BINARY_FILE_PATH> should be replaced with the actual path to the Alloy binary.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/first-components-and-stdlib.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n<BINARY_FILE_PATH> run config.alloy\n```\n\n----------------------------------------\n\nTITLE: Label Keep Stage Configuration in Alloy\nDESCRIPTION: Sets up a stage to retain only specified labels from log entries, filtering out all others.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_12\n\nLANGUAGE: alloy\nCODE:\n```\nstage.label_keep {\n    values = [ \"kubernetes_pod_name\", \"kubernetes_pod_container_name\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Basic OTLP Receiver Configuration in Alloy\nDESCRIPTION: Basic configuration structure for the OTLP receiver showing the available configuration blocks for gRPC and HTTP servers, along with output configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.otlp.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"LABEL\" {\n  grpc { ... }\n  http { ... }\n\n  output {\n    metrics = [...]\n    logs    = [...]\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Tracing Block for Meta-monitoring in Alloy\nDESCRIPTION: Sets up the tracing configuration block to define sampling ratio and forwarding destination for Alloy's internal traces. This block can only appear once in a configuration file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/metamonitoring.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\ntracing {\n  sampling_fraction = <SAMPLING_FRACTION>\n  write_to          = [<TRACES_RECEIVER_LIST>]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring loki.process Component in Alloy\nDESCRIPTION: Demonstrates the basic structure for configuring a loki.process component. It includes the forward_to argument and placeholders for adding processing stages.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.process \"<LABEL>\" {\n  forward_to = <RECEIVER_LIST>\n\n  stage.<STAGENAME> {\n    ...\n  }\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of discovery.process in Alloy\nDESCRIPTION: Demonstrates the basic syntax for using the discovery.process component in Alloy. This snippet shows the minimal required configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.process.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.process \"<LABEL>\" {\n\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Secret Values in Alloy\nDESCRIPTION: Demonstrates how to use convert.nonsensitive() to transform a secret value back into a plain text string. This should only be used when certain the value isn't sensitive, as the result will be displayed in plain text in the UI and API calls.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/convert.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\n// Assuming `sensitive_value` is a secret:\n\n> sensitive_value\n(secret)\n> convert.nonsensitive(sensitive_value)\n\"Hello, world!\"\n```\n\n----------------------------------------\n\nTITLE: Adding Resource Attributes as Metric Datapoint Attributes\nDESCRIPTION: Demonstrates configuring the transform processor to add resource attributes as metric datapoint attributes after the spanmetrics connector, enabling the attributes to be visible as metric labels.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.spanmetrics.md#2025-04-22_snippet_10\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  http {}\n  grpc {}\n\n  output {\n    traces  = [otelcol.connector.spanmetrics.default.input]\n  }\n}\n\notelcol.connector.spanmetrics \"default\" {\n  histogram {\n    explicit {}\n  }\n\n  dimension {\n    name = \"special.attr\"\n  }\n  output {\n    metrics = [otelcol.processor.transform.default.input]\n  }\n}\n\n// Insert resource attributes as metric data point attributes.\notelcol.processor.transform \"default\" {\n  error_mode = \"ignore\"\n\n  metric_statements {\n    context = \"datapoint\"\n    statements = [\n      // \"insert\" means that a metric datapoint attribute will be inserted\n      // only if an attribute with the same key does not already exist.\n      `merge_maps(attributes, resource.attributes, \"insert\")`,\n    ]\n  }\n\n  output {\n    metrics = [otelcol.exporter.prometheus.default.input]\n  }\n}\n\notelcol.exporter.prometheus \"default\" {\n  forward_to = [prometheus.remote_write.mimir.receiver]\n}\n\nprometheus.remote_write \"mimir\" {\n  endpoint {\n    url = \"http://mimir:9009/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Security Context in Kubernetes Helm Values\nDESCRIPTION: YAML configuration snippet for setting up non-root user security context in Grafana Alloy's Helm chart values.yaml. Configures the container to run with UID and GID 473.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/nonroot.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nalloy:\n  securityContext:\n    runAsUser: 473\n    runAsGroup: 473\n```\n\n----------------------------------------\n\nTITLE: Processing Different Resource Attributes - Output Metrics\nDESCRIPTION: Example showing two resulting metric resources created from spans with different k8s.pod.name attributes. The connector creates separate metric resources to preserve all unique resource attributes.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.spanmetrics.md#2025-04-22_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resourceMetrics\": [\n    {\n      \"resource\": {\n        \"attributes\": [\n          {\n            \"key\": \"service.name\",\n            \"value\": { \"stringValue\": \"TestSvcName\" }\n          },\n          {\n            \"key\": \"k8s.pod.name\",\n            \"value\": { \"stringValue\": \"first\" }\n          }\n        ]\n      },\n      \"scopeMetrics\": [\n        {\n          \"scope\": {\n            \"name\": \"spanmetricsconnector\"\n          },\n          \"metrics\": [\n            {\n              \"name\": \"calls\",\n              \"sum\": {\n                \"dataPoints\": [\n                  {\n                    \"attributes\": [\n                      {\n                        \"key\": \"service.name\",\n                        \"value\": { \"stringValue\": \"TestSvcName\" }\n                      },\n                      {\n                        \"key\": \"span.name\",\n                        \"value\": { \"stringValue\": \"TestSpan\" }\n                      },\n                      {\n                        \"key\": \"span.kind\",\n                        \"value\": { \"stringValue\": \"SPAN_KIND_UNSPECIFIED\" }\n                      },\n                      {\n                        \"key\": \"status.code\",\n                        \"value\": { \"stringValue\": \"STATUS_CODE_UNSET\" }\n                      }\n                    ],\n                    \"startTimeUnixNano\": \"1702582936761872000\",\n                    \"timeUnixNano\": \"1702582936761872012\",\n                    \"asInt\": \"1\"\n                  }\n                ],\n                \"aggregationTemporality\": 2,\n                \"isMonotonic\": true\n              }\n            }\n          ]\n        }\n      ]\n    },\n    {\n      \"resource\": {\n        \"attributes\": [\n          {\n            \"key\": \"service.name\",\n            \"value\": { \"stringValue\": \"TestSvcName\" }\n          },\n          {\n            \"key\": \"k8s.pod.name\",\n            \"value\": { \"stringValue\": \"second\" }\n          }\n        ]\n      },\n      \"scopeMetrics\": [\n        {\n          \"scope\": {\n            \"name\": \"spanmetricsconnector\"\n          },\n          \"metrics\": [\n            {\n              \"name\": \"calls\",\n              \"sum\": {\n                \"dataPoints\": [\n                  {\n                    \"attributes\": [\n                      {\n                        \"key\": \"service.name\",\n                        \"value\": { \"stringValue\": \"TestSvcName\" }\n                      },\n                      {\n                        \"key\": \"span.name\",\n                        \"value\": { \"stringValue\": \"TestSpan\" }\n                      },\n                      {\n                        \"key\": \"span.kind\",\n                        \"value\": { \"stringValue\": \"SPAN_KIND_UNSPECIFIED\" }\n                      },\n                      {\n                        \"key\": \"status.code\",\n                        \"value\": { \"stringValue\": \"STATUS_CODE_UNSET\" }\n                      }\n                    ],\n                    \"startTimeUnixNano\": \"1702582936761872000\",\n                    \"timeUnixNano\": \"1702582936761872012\",\n                    \"asInt\": \"1\"\n                  }\n                ],\n                \"aggregationTemporality\": 2,\n                \"isMonotonic\": true\n              }\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Livedebugging in Alloy\nDESCRIPTION: Configuration block to enable real-time data streaming to the Alloy UI for debugging purposes.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-syslog-messages.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\nlivedebugging {\n  enabled = true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Operator Probes in Alloy\nDESCRIPTION: This snippet demonstrates how to configure the prometheus.operator.probes component in Alloy. It specifies a label and sets the forward_to argument to a list of MetricsReceivers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.operator.probes.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.operator.probes \"<LABEL>\" {\n    forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Running Grafana Alloy with CLI Command\nDESCRIPTION: Basic usage syntax for the Grafana Alloy run command. The command requires a path name argument and accepts optional flags to customize behavior.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/cli/run.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nalloy run [<FLAG> ...] <PATH_NAME>\n```\n\n----------------------------------------\n\nTITLE: OpenCensus Receiver with Authentication\nDESCRIPTION: Example showing how to enable basic authentication for the OpenCensus receiver using environment variables for credentials.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.opencensus.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.opencensus \"default\" {\n  auth = otelcol.auth.basic.creds.handler\n}\n\notelcol.auth.basic \"creds\" {\n    username = sys.env(\"USERNAME\")\n    password = sys.env(\"PASSWORD\")\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Prometheus Configuration with Error Bypassing\nDESCRIPTION: Command to convert a Prometheus configuration with the --bypass-errors flag, which allows non-critical issues to be bypassed during conversion. This can be useful for troubleshooting but should be used with caution.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-prometheus.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=prometheus --bypass-errors --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON Strings to Alloy Values with encoding.from_json\nDESCRIPTION: Shows how to decode a string representing JSON into an Alloy value. This is commonly used to decode the output of a local.file component. The function fails if the string argument provided cannot be parsed as JSON.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/encoding.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\n> encoding.from_json(\"15\")\n15\n\n> encoding.from_json(\"[1, 2, 3]\")\n[1, 2, 3]\n\n> encoding.from_json(\"null\")\nnull\n\n> encoding.from_json(\"{\\\"key\\\": \\\"value\\\"}\")\n{\n  key = \"value\",\n}\n\n> encoding.from_json(local.file.some_file.content)\n\"Hello, world!\"\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Field Selector Configuration\nDESCRIPTION: Configuration block for filtering Kubernetes services using field selectors.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/prometheus-metrics.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\nselectors {\n  role  = \"service\"\n  field = \"<FIELD_SELECTOR>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of remote.http Component in Alloy\nDESCRIPTION: Demonstrates the basic syntax for defining a remote.http component in Alloy, which requires a label and URL to poll.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/remote/remote.http.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nremote.http \"<LABEL>\" {\n  url = \"<URL_TO_POLL>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Alternative Blackbox Exporter Configuration with Target List\nDESCRIPTION: Alternative configuration using a target list instead of individual target blocks.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.blackbox.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.blackbox \"<LABEL>\" {\n  targets = <TARGET_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying Alloy with Kubernetes Monitoring Helm Chart\nDESCRIPTION: Command to deploy Alloy in the 'meta' namespace using the Kubernetes Monitoring Helm chart with custom values.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nhelm install --values ./k8s-monitoring-values.yml k8s grafana/k8s-monitoring -n meta --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Adding Query Parameters to Metrics Path in JSON\nDESCRIPTION: JSON example showing how to append query parameters to the metrics path using the __param_<name> syntax in the discovery.http response.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.http.md#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\n   {\n      \"labels\" : {\n         \"__metrics_path__\" : \"/health\",\n         \"__scheme__\" : \"https\",\n         \"__scrape_interval__\" : \"60s\",\n         \"__scrape_timeout__\" : \"10s\",\n         \"__param_target_data\": \"prometheus\",\n         \"service\" : \"custom-api-service\"\n      },\n      \"targets\" : [\n         \"custom-api:443\"\n      ]\n   },\n]\n```\n\n----------------------------------------\n\nTITLE: Basic Git Import Block Structure - Alloy\nDESCRIPTION: Basic structure for importing custom components from a Git repository. Requires repository and path arguments, with optional revision specification.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/import.git.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nimport.git \"NAMESPACE\" {\n  repository = \"GIT_REPOSTORY\"\n  path       = \"PATH_TO_MODULE\"\n}\n```\n\n----------------------------------------\n\nTITLE: OTLP HTTP Endpoint Configuration\nDESCRIPTION: Configuration block for enabling HTTP/1.1 protocol support on port 4318 for receiving OTLP data.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-data.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\nhttp {\n  endpoint = \"<HOST>:4318\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic OTLP Receiver Configuration in Alloy\nDESCRIPTION: Basic configuration template for an OpenTelemetry Protocol receiver component that can handle metrics, logs, and traces data streams.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-data.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"<LABEL>\" {\n  output {\n    metrics = [<COMPONENT_INPUT_LIST>]\n    logs    = [<COMPONENT_INPUT_LIST>]\n    traces  = [<COMPONENT_INPUT_LIST>]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP-based Service Discovery in Alloy\nDESCRIPTION: Basic usage example of the discovery.http component in Alloy, specifying the URL to scrape for service discovery.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.http.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.http \"<LABEL>\" {\n  url = \"<URL>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing MySQL Exporter in Alloy\nDESCRIPTION: Basic usage of the prometheus.exporter.mysql component. It requires a label and a data source name for connecting to the MySQL server.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.mysql.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.mysql \"<LABEL>\" {\n    data_source_name = \"<DATA_SOURCE_NAME>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using file.path_join to Combine Path Elements in Alloy\nDESCRIPTION: The file.path_join function joins multiple path elements into a single path, using OS-specific separators. It can handle empty inputs and automatically handles separators between path segments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/file.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\n> file.path_join()\n\"\"\n```\n\nLANGUAGE: alloy\nCODE:\n```\n> file.path_join(\"this/is\", \"a/path\")\n\"this/is/a/path\"\n```\n\n----------------------------------------\n\nTITLE: Encoding Strings to Base64 with encoding.to_base64 in Alloy\nDESCRIPTION: Demonstrates how to encode an original string into a RFC4648-compliant Base64 encoded string.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/encoding.md#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n> encoding.to_base64(\"string123!?$*&()'-=@~\")\nc3RyaW5nMTIzIT8kKiYoKSctPUB+\n```\n\n----------------------------------------\n\nTITLE: Registering OpenTelemetry Component in Grafana Alloy\nDESCRIPTION: Initializes and registers a new OpenTelemetry processor component with Alloy runtime. Defines the component name, stability level, arguments structure, and build function.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/add-otel-component.md#2025-04-22_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nimport (\n    ...\n    \"github.com/open-telemetry/opentelemetry-collector-contrib/processor/exampleprocessor\"\n    ...\n    )\n\nfunc init() {\n        component.Register(component.Registration{\n        Name:      \"otelcol.processor.example\",\n        Stability: featuregate.StabilityGenerallyAvailable,\n        Args:      Arguments{},\n        Exports:   otelcol.ConsumerExports{},\n        Build: func(opts component.Options, args component.Arguments) (component.Component, error) {\n            return processor.New(opts, exampleprocessor.NewFactory(), args.(Arguments))\n        },\n    })\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment for Grafana Alloy Tutorial\nDESCRIPTION: Bash commands to create a directory structure and files for the Grafana Alloy tutorial. This includes creating a tutorial directory and necessary configuration files.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/send-logs-to-loki.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir alloy-tutorial\ncd alloy-tutorial\ntouch docker-compose.yml\n```\n\n----------------------------------------\n\nTITLE: Azure AKS Resource Attributes Configuration\nDESCRIPTION: Configuration block for Azure Kubernetes Service (AKS) resource attributes including cloud platform, provider, and cluster name information. The cluster name is derived from the Azure IMDS infrastructure resource group field.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.resourcedetection.md#2025-04-22_snippet_3\n\nLANGUAGE: hcl\nCODE:\n```\naks {\n  resource_attributes {\n    cloud.platform { enabled = true }\n    cloud.provider { enabled = true }\n    k8s.cluster.name { enabled = false }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Batch Processor Configuration\nDESCRIPTION: Basic configuration structure for the OpenTelemetry batch processor showing required output block configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.batch.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.batch \"LABEL\" {\n  output {\n    metrics = [...]\n    logs    = [...]\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Redis Exporter Configuration in Alloy\nDESCRIPTION: Basic configuration syntax for setting up a Redis exporter with a required Redis address parameter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.redis.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.redis \"<LABEL>\" {\n    redis_addr = \"<REDIS_ADDRESS>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Live Debugging in Alloy\nDESCRIPTION: Alloy configuration block to enable live debugging, which streams real-time data to the Alloy UI for troubleshooting.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-logs-over-tcp.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\nlivedebugging {\n  enabled = true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring EC2 Instance Discovery in Alloy\nDESCRIPTION: Basic usage of the discovery.ec2 component in Alloy. This snippet shows the minimal configuration required to set up EC2 instance discovery.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.ec2.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.ec2 \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Converting OpenTelemetry Collector Configuration to Grafana Alloy\nDESCRIPTION: This command converts an OpenTelemetry Collector configuration file to a Grafana Alloy configuration. It takes input and output paths as parameters.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-otelcol.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=otelcol --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Configuring LiveDebugging in Grafana Alloy\nDESCRIPTION: Configuration block to enable live debugging in Grafana Alloy, which streams real-time data to the Alloy UI for troubleshooting.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-logs-from-file.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\nlivedebugging {\n  enabled = true\n}\n```\n\n----------------------------------------\n\nTITLE: Concatenating Arrays with array.concat in Alloy\nDESCRIPTION: The array.concat function concatenates one or more lists of values into a single list. Each argument must be a list value, and elements within the list can be of any type.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/array.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\n> array.concat([])\n[]\n\n> array.concat([1, 2], [3, 4])\n[1, 2, 3, 4]\n\n> array.concat([1, 2], [], [bool, null])\n[1, 2, bool, null]\n\n> array.concat([[1, 2], [3, 4]], [[5, 6]])\n[[1, 2], [3, 4], [5, 6]]\n```\n\n----------------------------------------\n\nTITLE: Custom Targets Scrape Configuration\nDESCRIPTION: Configuration for collecting metrics from custom targets without service discovery.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/prometheus-metrics.md#2025-04-22_snippet_11\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"<SCRAPE_LABEL>\" {\n  targets    = [<TARGET_LIST>]\n  forward_to = [prometheus.remote_write.<REMOTE_WRITE_LABEL>.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring File Stats Receiver in Alloy\nDESCRIPTION: Basic configuration example for the otelcol.receiver.file_stats component. Shows how to specify a glob pattern for file monitoring and configure the metrics output.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.file_stats.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.file_stats \"LABEL\" {\n  include = \"GLOB_PATTERN\"\n\n  output {\n    metrics = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: OracleDB Connection String Format\nDESCRIPTION: Shows the format for the connection_string argument used in the prometheus.exporter.oracledb component. It includes the structure for specifying user, password, server, and service name, along with optional parameters.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.oracledb.md#2025-04-22_snippet_1\n\nLANGUAGE: conn\nCODE:\n```\noracle://user:pass@server/service_name[?OPTION1=VALUE1[&OPTIONn=VALUEn]...]\n```\n\n----------------------------------------\n\nTITLE: Running Grafana Alloy in Windows Docker Container\nDESCRIPTION: This shell command runs Grafana Alloy in a Windows Docker container. It mounts a configuration file, exposes port 12345, and specifies runtime arguments including the HTTP listen address and storage path.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/docker.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker run \\\n  -v \"<CONFIG_FILE_PATH>:C:\\Program Files\\GrafanaLabs\\Alloy\\config.alloy\" \\\n  -p 12345:12345 \\\n  grafana/alloy:nanoserver-1809 \\\n    run --server.http.listen-addr=0.0.0.0:12345 \"--storage.path=C:\\ProgramData\\GrafanaLabs\\Alloy\\data\" \\\n    \"C:\\Program Files\\GrafanaLabs\\Alloy\\config.alloy\"\n```\n\n----------------------------------------\n\nTITLE: Configuring loki.source.windowsevent in Alloy\nDESCRIPTION: Basic usage structure for configuring the loki.source.windowsevent component. It specifies the eventlog_name and forward_to parameters.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.windowsevent.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.windowsevent \"<LABEL>\" {\n  eventlog_name = \"<EVENTLOG_NAME>\"\n  forward_to    = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Labeled Block in Alloy\nDESCRIPTION: Demonstrates the pattern for creating a labeled block structure that includes a block name and label with nested elements.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/syntax.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nBLOCK_NAME \"BLOCK_LABEL\" {\n  // Block body can contain attributes and nested unlabeled blocks\n  IDENTIFIER = EXPRESSION // Attribute\n\n  NESTED_BLOCK_NAME {\n    // Nested block body\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Batch Processor\nDESCRIPTION: Configuration for the otelcol.processor.batch component to batch OpenTelemetry data before export.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-data.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.batch \"<PROCESSOR_LABEL>\" {\n  output {\n    metrics = [otelcol.exporter.otlp.<EXPORTER_LABEL>.input]\n    logs    = [otelcol.exporter.otlp.<EXPORTER_LABEL>.input]\n    traces  = [otelcol.exporter.otlp.>EXPORTER_LABEL>.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Jaeger Remote Sampling Extension Configuration\nDESCRIPTION: Basic configuration example showing how to define a Jaeger remote sampling extension component in Alloy configuration language. The component requires a source block for configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.extension.jaeger_remote_sampling.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.extension.jaeger_remote_sampling \"LABEL\" {\n  source {\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Regex Replacement in Templates\nDESCRIPTION: Applies regular expression patterns to transform strings using capture groups and direct replacements with regexReplaceAll and regexReplaceAllLiteral functions.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_43\n\nLANGUAGE: alloy\nCODE:\n```\nstage.template {\n    source   = \"output\"\n    template = `{{ regexReplaceAll \"(a*)bc\" .Value \"${1}a\" }}`\n}\nstage.template {\n    source   = \"output\"\n    template = `{{ regexReplaceAllLiteral \"(ts=)\" .Value \"timestamp=\" }}`\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.processor.attributes in Alloy\nDESCRIPTION: Basic usage example of the otelcol.processor.attributes component in Alloy configuration. It shows how to define the component with a label and configure its output.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.attributes.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.attributes \"LABEL\" {\n  output {\n    metrics = [...]\n    logs    = [...]\n    traces  = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Decoding Base64 Strings with encoding.from_base64 in Alloy\nDESCRIPTION: Demonstrates how to decode a RFC4648-compliant Base64-encoded string into the original string. The function fails if the provided string contains invalid Base64 data.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/encoding.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n> encoding.from_base64(\"dGFuZ2VyaW5l\")\ntangerine\n```\n\n----------------------------------------\n\nTITLE: Decoding URL-safe Base64 Strings with encoding.from_URLbase64 in Alloy\nDESCRIPTION: Shows how to decode a RFC4648-compliant Base64 URL safe encoded string into the original string. The function fails if the provided string contains invalid Base64 data.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/encoding.md#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n> encoding.from_URLbase64(\"c3RyaW5nMTIzIT8kKiYoKSctPUB-\")\nstring123!?$*&()'-=@~\n```\n\n----------------------------------------\n\nTITLE: Configuring Write Relabel Rules in Grafana Alloy\nDESCRIPTION: The write_relabel_config block allows you to define relabeling rules that transform input metrics. All configuration arguments are optional with sensible defaults. Multiple blocks are processed in order from top to bottom.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/shared/reference/components/write_relabel_config.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nwrite_relabel_config:\n  action: replace         # The relabeling action to perform (default: replace)\n  modulus: 10            # Used for hashmod action\n  regex: \"(.*)\"          # RE2 expression with capture group support\n  replacement: \"$1\"      # Value for replace operations, supports capture groups\n  separator: \";\"         # Separator for source_labels values\n  source_labels:         # List of labels to select\n    - label1\n    - label2\n  target_label: output   # Destination label for results\n```\n\n----------------------------------------\n\nTITLE: Resulting Alloy Syntax Output\nDESCRIPTION: The Alloy syntax configuration that is generated from the Jsonnet example. It shows how attributes, blocks, and expressions are represented in the Alloy syntax format.\nSOURCE: https://github.com/grafana/alloy/blob/main/operations/alloy-syntax-jsonnet/README.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nattr_1 = \"Hello, world\"\nsome_block \"foobar\" {\n  expr = sys.env(\"HOME\")\n  inner_attr_1 = [0, 1, 2, 3]\n  inner_attr_2 = {\n    \"first_name\" = \"John\",\n    \"last_name\" = \"Smith\",\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: YAML Target File Example for discovery.file\nDESCRIPTION: Example of a YAML file format that can be used with discovery.file. It shows an alternative way to define targets and labels for discovery.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.file.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- targets:\n  - 127.0.0.1:9999\n  - 127.0.0.1:10101\n  labels:\n    job: worker\n- targets:\n  - 127.0.0.1:9090\n  labels:\n    job: prometheus\n```\n\n----------------------------------------\n\nTITLE: Complete Kubernetes Events Collection Example\nDESCRIPTION: Full example showing how to collect events from kube-system namespace and forward them to a Loki write component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.kubernetes_events.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.kubernetes_events \"example\" {\n  // Only watch for events in the kube-system namespace.\n  namespaces = [\"kube-system\"]\n\n  forward_to = [loki.write.local.receiver]\n}\n\nloki.write \"local\" {\n  endpoint {\n    url = sys.env(\"LOKI_URL\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of otelcol.auth.sigv4 in Alloy\nDESCRIPTION: Shows the basic syntax for using the otelcol.auth.sigv4 component in Alloy configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.auth.sigv4.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.auth.sigv4 \"LABEL\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Configuration Using Alloy CLI Command\nDESCRIPTION: Shell command that uses the Alloy CLI to convert a Grafana Agent Static configuration file to an Alloy configuration file. The command takes the input path and specifies the output path for the converted configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-static.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=static --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Squid Exporter in Alloy\nDESCRIPTION: Basic configuration for the prometheus.exporter.squid component, specifying the Squid address to collect metrics from.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.squid.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.squid \"<LABEL>\" {\n    address = \"<SQUID_ADDRESS>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Reloading Alloy Configuration Using curl\nDESCRIPTION: Uses the curl command to trigger a configuration reload via Alloy's HTTP API without requiring a full service restart. This allows for configuration changes to take effect immediately.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/send-metrics-to-prometheus.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:12345/-/reload\n```\n\n----------------------------------------\n\nTITLE: Converting Integrations Next Configurations\nDESCRIPTION: Shell command to convert Grafana Agent Static configurations that use the Integrations Next feature to Alloy format. This requires the extra-args flag to enable the integrations-next feature.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-static.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=static --extra-args=\"-enable-features=integrations-next\" --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Using Coalesce Function in Alloy\nDESCRIPTION: Examples demonstrating coalesce function usage to find first non-empty value among multiple arguments. Shows cases with string literals and environment variable fallbacks.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/coalesce.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\n> coalesce(\"a\", \"b\")\na\n> coalesce(\"\", \"b\")\nb\n> coalesce(sys.env(\"DOES_NOT_EXIST\"), \"c\")\nc\n```\n\n----------------------------------------\n\nTITLE: Complete Custom Targets Metrics Collection Example\nDESCRIPTION: Full example configuration for collecting metrics from multiple custom endpoints with different configurations.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/prometheus-metrics.md#2025-04-22_snippet_12\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"custom_targets\" {\n  targets = [\n    {\n      __address__ = \"prometheus:9090\",\n    },\n    {\n      __address__ = \"mimir:8080\",\n      __scheme__  = \"https\",\n    },\n    {\n      __address__      = \"custom-application:80\",\n      __metrics_path__ = \"/custom-metrics–path\",\n    },\n    {\n      __address__ = \"alloy:12345\",\n      application = \"alloy\",\n      environment = \"production\",\n    },\n  ]\n\n  forward_to = [prometheus.remote_write.default.receiver]\n}\n\nprometheus.remote_write \"default\" {\n  endpoint {\n    url = \"http://localhost:9090/api/prom/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Reloading Grafana Alloy Configuration\nDESCRIPTION: Examples demonstrating how to reload the Grafana Alloy configuration using the /-/reload endpoint. The first example shows a successful reload, while the second shows an error during reload.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/http/_index.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ curl localhost:12345/-/reload\nconfig reloaded\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ curl localhost:12345/-/reload\nerror during the initial load: /Users/user1/Desktop/git.alloy:13:1: Failed to build component: loading custom component controller: custom component config not found in the registry, namespace: \"math\", componentName: \"add\"\n```\n\n----------------------------------------\n\nTITLE: Compacting Telemetry Data with otelcol.processor.groupbyattrs\nDESCRIPTION: This example shows how to use otelcol.processor.groupbyattrs with its default configuration to compact telemetry data. It demonstrates the component definition for data compaction.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.groupbyattrs.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.groupbyattrs \"default\" {\n  output {\n    metrics = [otelcol.exporter.otlp.default.input]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Final Prometheus Metrics After All Relabeling Rules\nDESCRIPTION: Shows the final set of metrics after applying all relabeling rules, including the removal of the 'instance' label.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.relabel.md#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nmetric_a{host = \"localhost/development\", __address__ = \"localhost\", app = \"backend\"}  2\nmetric_a{host = \"cluster_a/production\",  __address__ = \"cluster_a\", app = \"backend\"}  9\n```\n\n----------------------------------------\n\nTITLE: Configuring Consul Agent Discovery in Alloy\nDESCRIPTION: Basic configuration for the discovery.consulagent component, specifying the Consul server to connect to.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.consulagent.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.consulagent \"<LABEL>\" {\n  server = \"<CONSUL_SERVER>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Object and Array Elements in Alloy\nDESCRIPTION: Demonstrates different ways to access elements in objects and arrays using square bracket notation and dot notation. Square brackets can access array elements by index or object fields by name, while dot notation can access object fields directly.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/expressions/operators.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nobj[\"app\"]\narr[1]\n\nobj.app\nlocal.file.token.content\n```\n\n----------------------------------------\n\nTITLE: Git Import from Directory Example - Alloy\nDESCRIPTION: Example demonstrating how to import components from a directory in a Git repository rather than a single file. Shows accessing components from the imported module.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/import.git.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nimport.git \"math\" {\n  repository = \"https://github.com/wildum/module.git\"\n  revision   = \"master\"\n  path       = \"modules\"\n}\n\nmath.add \"default\" {\n  a = 15\n  b = 45\n}\n```\n\n----------------------------------------\n\nTITLE: TCP Syslog Exporter Configuration without TLS\nDESCRIPTION: Example configuration for sending data to a syslog server over TCP without TLS encryption, using RFC5424-compliant messages.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.syslog.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.syslog \"default\" {\n  endpoint = \"localhost\"\n  tls {\n      insecure             = true\n      insecure_skip_verify = true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubeadm Permissions in YAML\nDESCRIPTION: This YAML configuration sets up the required permissions for the Kubeadm detector to access the kubeadm-config ConfigMap in the kube-system namespace. It defines a Role and RoleBinding for the OpenTelemetry Collector.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.resourcedetection.md#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: otel-collector\n  namespace: kube-system\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    resourceNames: [\"kubeadm-config\"]\n    verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: otel-collector-rolebinding\n  namespace: kube-system\nsubjects:\n- kind: ServiceAccount\n  name: default\n  namespace: default\nroleRef:\n  kind: Role\n  name: otel-collector\n  apiGroup: rbac.authorization.k8s.io\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Exporter in Alloy\nDESCRIPTION: Basic configuration snippet for setting up a Kafka metrics exporter in Alloy. The component requires a label identifier and a list of Kafka URIs to connect to for metrics collection.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.kafka.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.kafka \"<LABEL>\" {\n    kafka_uris = \"<KAFKA_URI_LIST>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace for Grafana Alloy\nDESCRIPTION: This command creates a dedicated namespace in your Kubernetes cluster where Grafana Alloy will be installed.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/kubernetes.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create namespace <NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Configuring Experimental Feature Flag in Grafana Alloy\nDESCRIPTION: Configuration property required to enable experimental features in Grafana Alloy. The stability level must be explicitly set to 'experimental' to use experimental features.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/shared/stability/experimental_feature.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nstability.level=experimental\n```\n\n----------------------------------------\n\nTITLE: Using loki.secretfilter in Alloy Configuration\nDESCRIPTION: Basic usage syntax for the loki.secretfilter component in an Alloy configuration file. It defines how to instantiate the component and specify the receiver list for forwarding log entries.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.secretfilter.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.secretfilter \"<LABEL>\" {\n    forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Multiline Raw String in Alloy Syntax\nDESCRIPTION: Example of a multiline raw string in Alloy syntax, which preserves all line breaks and formatting exactly as written.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/expressions/types_and_values.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\n`Hello,\n\"world\"!`\n```\n\n----------------------------------------\n\nTITLE: Basic Prometheus Self-Exporter Configuration in Alloy\nDESCRIPTION: Basic syntax for configuring the prometheus.exporter.self component with a label identifier.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.self.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.self \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Complete OpenStack Discovery and Prometheus Configuration Example\nDESCRIPTION: Comprehensive example showing OpenStack discovery configuration with Prometheus scraping and remote write setup in Grafana Alloy.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.openstack.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.openstack \"example\" {\n  role   = \"<OPENSTACK_ROLE>\"\n  region = \"<OPENSTACK_REGION>\"\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.openstack.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Relative Module Import in Alloy\nDESCRIPTION: Example of importing a module relative to the current module's path using file.path_join and module_path. This allows for modular component composition within a Git imported module.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/import.file.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\nimport.file \"lib\" {\n  filename = file.path_join(module_path, \"lib.alloy\")\n}\n\ndeclare \"add\" {\n  argument \"a\" {}\n  argument \"b\" {}\n\n  lib.plus \"default\" {\n    a = argument.a.value\n    b = argument.b.value\n  }\n\n  export \"output\" {\n    value = lib.plus.default.sum\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Security Context for Privileged Access in Grafana Alloy\nDESCRIPTION: YAML configuration for setting up privileged security context to allow Grafana Alloy to access container logs and host telemetry data. This grants the container root-level access and privileged mode.\nSOURCE: https://github.com/grafana/alloy/blob/main/operations/helm/charts/alloy/README.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprivileged: true\nrunAsUser: 0\n```\n\n----------------------------------------\n\nTITLE: Mapping Extracted Values to Structured Metadata\nDESCRIPTION: Configures a structured_metadata stage that adds data from the extracted values map to log entries as structured metadata, using either direct mapping or specifying source values.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_35\n\nLANGUAGE: alloy\nCODE:\n```\nstage.structured_metadata {\n    values = {\n      env  = \"\",         // Sets up an 'env' property to structured metadata, based on the 'env' extracted value.\n      user = \"username\", // Sets up a 'user' property to structured metadata, based on the 'username' extracted value.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana Alloy Server Log Level in YAML\nDESCRIPTION: This YAML snippet configures the log level for the Grafana Alloy server. It uses an environment variable ${TEST} to set the log_level, allowing for dynamic configuration based on the deployment environment.\nSOURCE: https://github.com/grafana/alloy/blob/main/internal/static/config/encoder/test_encoding_utf8bom.txt#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nserver:\n  log_level: ${TEST}\n```\n\n----------------------------------------\n\nTITLE: Configuring Local File Matching in Grafana Alloy\nDESCRIPTION: Configuration for the local.file_match component which discovers log files on the filesystem using glob patterns. It specifies the path targets to monitor and how often to sync the filesystem.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-logs-from-file.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file_match \"local_files\" {\n    path_targets = [{\"__path__\" = \"/temp/logs/*.log\", \"job\" = \"python\", \"hostname\" = constants.hostname}]\n    sync_period  = \"5s\"\n}\n```\n\n----------------------------------------\n\nTITLE: Prometheus Metrics After First Relabeling Rule\nDESCRIPTION: Shows the transformed metrics after applying the first relabeling rule, which adds a new 'host' label.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.relabel.md#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nmetric_a{host = \"localhost/development\", __address__ = \"localhost\", instance = \"development\", app = \"frontend\"} 10\nmetric_a{host = \"localhost/development\", __address__ = \"localhost\", instance = \"development\", app = \"backend\"}  2\nmetric_a{host = \"cluster_a/production\",  __address__ = \"cluster_a\", instance = \"production\",  app = \"frontend\"} 7\nmetric_a{host = \"cluster_a/production\",  __address__ = \"cluster_a\", instance = \"production\",  app = \"backend\"}  9\nmetric_a{host = \"cluster_b/production\",  __address__ = \"cluster_a\", instance = \"production\",  app = \"database\"} 4\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Scraping in Alloy\nDESCRIPTION: Set up the prometheus.scrape component to collect Windows metrics and forward them.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-windows.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.scrape \"example\" {\n  targets    = prometheus.exporter.windows.default.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Alloy Roles in Kubernetes Monitoring Helm Chart\nDESCRIPTION: YAML configuration for defining the Alloy roles in the Kubernetes Monitoring Helm chart. This configuration deploys Alloy with the capability to collect logs only.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\nalloy-singleton:\n  enabled: false\n\nalloy-metrics:\n  enabled: false\n\nalloy-logs:\n  enabled: true\n  alloy:\n    mounts:\n      varlog: false\n    clustering:\n      enabled: true\n\nalloy-profiles:\n  enabled: false\n\nalloy-receiver:\n  enabled: false\n```\n\n----------------------------------------\n\nTITLE: Installing Grafana Alloy Helm Chart\nDESCRIPTION: Command to install the Grafana Alloy chart with a specified release name. By default, this installs Grafana Alloy using a DaemonSet controller, which can be changed using controller.type configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/operations/helm/charts/alloy/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install my-release grafana/alloy\n```\n\n----------------------------------------\n\nTITLE: Inspecting WAL Files with Promtool Command\nDESCRIPTION: This example shows how to use the Promtool command to inspect Write-Ahead Log (WAL) files to identify which metrics were sent by a specific Prometheus instance. This is useful for troubleshooting out of order errors.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.remote_write.md#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n./promtool tsdb dump --match='{__name__=\"otelcol_connector_spanmetrics_duration_seconds_bucket\", http_method=\"GET\", job=\"ExampleJobName\"}' /path/to/wal/\n```\n\n----------------------------------------\n\nTITLE: Using loki.rules.kubernetes in Alloy\nDESCRIPTION: Basic usage of the loki.rules.kubernetes component in Alloy configuration. It requires specifying a label and the address of the Loki ruler.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.rules.kubernetes.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.rules.kubernetes \"<LABEL>\" {\n  address = \"<LOKI_RULER_URL>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Unix Exporter Configuration in Alloy\nDESCRIPTION: Basic usage example showing the minimal configuration structure for prometheus.exporter.unix component\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.unix.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.unix \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Objects and Arrays in Alloy\nDESCRIPTION: Example of using curly braces to define an object with application and namespace properties, and square brackets to define an array with different data types including a mathematical expression.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/expressions/operators.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nobj = { app = \"alloy\", namespace = \"dev\" }\narr = [1, true, 7 * (1+1), 3]\n```\n\n----------------------------------------\n\nTITLE: Creating New Values with Template Processing\nDESCRIPTION: Adds a new key-value pair to the extracted map using Go's text/template syntax, creating data that wasn't previously present.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_36\n\nLANGUAGE: alloy\nCODE:\n```\nstage.template {\n    source   = \"new_key\"\n    template = \"hello_world\"\n}\n```\n\n----------------------------------------\n\nTITLE: Excluding Spans Based on Resources in OpenTelemetry Collector\nDESCRIPTION: This configuration shows how to exclude spans from processing based on resource attributes. It uses a strict match type and defines a resource key-value pair for exclusion. The example notes that this configuration is not applicable to metrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.attributes.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.attributes \"default\" {\n    exclude {\n        match_type = \"strict\"\n        resource {\n            key = \"host.type\"\n            value = \"n1-standard-1\"\n        }\n    }\n    action {\n        key = \"credit_card\"\n        action = \"delete\"\n    }\n    action {\n        key = \"duplicate_key\"\n        action = \"delete\"\n    }\n    output {\n        logs    = [otelcol.exporter.otlp.default.input]\n        traces  = [otelcol.exporter.otlp.default.input]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Puppet Module Dependencies for Grafana Alloy Installation\nDESCRIPTION: JSON configuration specifying required Puppet module dependencies for installing Grafana Alloy. It specifies version requirements for the puppetlabs/apt and puppetlabs/yumrepo_core modules.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/puppet.md#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"name\": \"puppetlabs/apt\",\n\"version_requirement\": \">= 4.1.0 <= 7.0.0\"\n},\n{\n\"name\": \"puppetlabs/yumrepo_core\",\n\"version_requirement\": \"<= 2.0.0\"\n}\n```\n\n----------------------------------------\n\nTITLE: Raw String Declaration in Alloy Syntax\nDESCRIPTION: Example of a raw string declaration in Alloy syntax, enclosed in backticks. Raw strings don't support escape sequences.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/expressions/types_and_values.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\n`Hello, \"world\"!`\n```\n\n----------------------------------------\n\nTITLE: Embedding Grafana Alloy Configuration in values.yaml\nDESCRIPTION: YAML snippet for embedding Grafana Alloy configuration directly in the Helm chart's values.yaml file. It sets logging level and format.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/kubernetes.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nalloy:\n  configMap:\n    content: |-\n      // Write your Alloy config here:\n      logging {\n        level = \"info\"\n        format = \"logfmt\"\n      }\n```\n\n----------------------------------------\n\nTITLE: Retrieving Environment Variables with sys.env in Alloy\nDESCRIPTION: Demonstrates how to use the sys.env function to access environment variables from the system. Shows both successful retrieval of an existing variable (HOME) and handling of non-existent variables which return empty strings.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/sys.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\n> sys.env(\"HOME\")\n\"/home/alloy\"\n\n> sys.env(\"DOES_NOT_EXIST\")\n\"\"\n```\n\n----------------------------------------\n\nTITLE: Converting Maps to JSON Strings with encoding.to_json in Alloy\nDESCRIPTION: Demonstrates how to encode a map into a JSON string. This is commonly used to encode component configurations that expect JSON strings. The function fails if the input argument cannot be parsed as a JSON string.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/encoding.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\n> encoding.to_json({\"modules\"={\"http_2xx\"={\"prober\"=\"http\",\"timeout\"=\"5s\",\"http\"={\"headers\"={\"Authorization\"=sys.env(\"TEST_VAR\")}}}}})\n\"{\\\"modules\\\":{\\\"http_2xx\\\":{\\\"http\\\":{\\\"headers\\\":{\\\"Authorization\\\":\\\"Hello!\\\"}},\\\"prober\\\":\\\"http\\\",\\\"timeout\\\":\\\"5s\\\"}}}\"\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of prometheus.exporter.process in Alloy\nDESCRIPTION: Demonstrates the basic syntax for using the prometheus.exporter.process component in Alloy configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.process.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.process \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Datadog Agent YAML for Exclusive Alloy Forwarding\nDESCRIPTION: Updates the Datadog Agent YAML configuration to send metrics only to Grafana Alloy. This replaces the default Datadog endpoint with the Alloy receiver endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/datadog-traces-metrics.md#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ndd_url: http://<DATADOG_RECEIVER_HOST>:<DATADOG_RECEIVER_PORT>\n```\n\n----------------------------------------\n\nTITLE: Transforming Existing Values with Templates\nDESCRIPTION: Modifies an existing extracted field by transforming its value using a template with the ToLower function and adding a suffix to standardize data format.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_37\n\nLANGUAGE: alloy\nCODE:\n```\nstage.template {\n    source   = \"app\"\n    template = \"{{ ToLower .Value }}_some_suffix\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Remote Write\nDESCRIPTION: Configuration for sending metrics to a Prometheus server.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-linux.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.remote_write \"local\" {\n  endpoint {\n    url = \"http://prometheus:9090/api/v1/write\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Target Format for discovery.file YAML Configuration in SNMP\nDESCRIPTION: Example YAML file format for use with discovery.file component when discovering SNMP targets, showing how to specify targets and their associated labels such as name, module, and authentication method.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.snmp.md#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n- targets:\n  - localhost:161\n  labels:\n    name: t1\n    module: default\n    auth: public_v2\n- targets:\n  - localhost:161\n  labels:\n    name: t2\n    module: default\n    auth: public_v2\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Remote Write in Alloy\nDESCRIPTION: Configure the prometheus.remote_write component to send metrics to a Prometheus server.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-windows.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"http://localhost:9090/api/v1/write\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Node Logs Collection in Kubernetes Monitoring Helm Chart\nDESCRIPTION: YAML configuration for disabling the collection of node logs in the Kubernetes Monitoring Helm chart.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nnodeLogs:\n  enabled: false\n```\n\n----------------------------------------\n\nTITLE: Basic Loki Echo Usage\nDESCRIPTION: Basic syntax for using the loki.echo component with a label.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.echo.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.echo \"<LABEL>\" {}\n```\n\n----------------------------------------\n\nTITLE: Example prometheus.exporter.kafka Component Arguments Table\nDESCRIPTION: Markdown table showing all available configuration arguments for the Kafka Prometheus exporter, including their types, descriptions, default values, and whether they are required.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.kafka.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Name                          | Type            | Description                                                                                                                                                            | Default | Required |\n| ----------------------------- | --------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | -------- |\n| `kafka_uris`                  | `array(string)` | Address array (host:port) of Kafka server.                                                                                                                                             |         | yes      |\n| `instance`                    | `string`        | The`instance`label for metrics, default is the hostname:port of the first `kafka_uris`. You must manually provide the instance value if there is more than one string in `kafka_uris`. |         | no       |\n```\n\n----------------------------------------\n\nTITLE: Running Ansible Playbook to Install Grafana Alloy\nDESCRIPTION: This shell command executes the Ansible playbook to install Grafana Alloy on the target hosts as defined in the inventory.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/ansible.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nansible-playbook alloy.yml\n```\n\n----------------------------------------\n\nTITLE: Target Definition Format for SNMP in YAML\nDESCRIPTION: Example YAML file format for defining SNMP targets when using local.file to provide targets to prometheus.exporter.snmp, showing how to specify name, address, module, and authentication parameters.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.snmp.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n- name: t1\n  address: localhost:161\n  module: default\n  auth: public_v2\n- name: t2\n  address: localhost:161\n  module: default\n  auth: public_v2\n```\n\n----------------------------------------\n\nTITLE: Array Construction in Alloy Syntax\nDESCRIPTION: Example of constructing an array with a sequence of comma-separated values enclosed in square brackets.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/expressions/types_and_values.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\n[0, 1, 2, 3]\n```\n\n----------------------------------------\n\nTITLE: Converting Vault Secret Data to Non-Sensitive String in Alloy\nDESCRIPTION: Example of using the convert.nonsensitive function to convert a Vault secret data value into a non-sensitive string that can be used with components that don't support secrets.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/remote/remote.vault.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nconvert.nonsensitive(remote.vault.LABEL.data.KEY_NAME)\n```\n\n----------------------------------------\n\nTITLE: Defining Kubernetes Node Permissions in YAML\nDESCRIPTION: This YAML snippet defines the necessary permissions for the Kubernetes Node detector to access node resources. It creates a ClusterRole with 'get' and 'list' permissions for nodes.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.resourcedetection.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nkind: ClusterRole\nmetadata:\n  name: alloy\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"get\", \"list\"]\n```\n\n----------------------------------------\n\nTITLE: Deploying Grafana Stack with Docker Compose\nDESCRIPTION: Start the Grafana stack using Docker Compose in the windows directory of the cloned repository.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-windows.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd alloy-scenarios/windows\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Generating Diagnostic Report During Conversion\nDESCRIPTION: Command to convert a Grafana Agent Static configuration while generating a diagnostic report to help identify potential issues or warnings in the conversion process.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-static.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=static --report=<OUTPUT_REPORT_PATH> --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Replacing Substrings with Template Functions\nDESCRIPTION: Uses the Replace template function to substitute specific instances of text within a string, with control over the number of replacements.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_41\n\nLANGUAGE: alloy\nCODE:\n```\nstage.template {\n    source   = \"output\"\n    template = `{{ Replace .Value \"loki\" \"Loki\" 2 }}`\n}\n```\n\n----------------------------------------\n\nTITLE: Structuring Azure Function App Logs in JSON\nDESCRIPTION: This JSON structure represents log records for Azure Function Apps. It includes multiple log entries with detailed information about function execution, host environment, and metadata loading events.\nSOURCE: https://github.com/grafana/alloy/blob/main/internal/component/loki/source/azure_event_hubs/internal/parser/testdata/function_app_logs_message.txt#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"records\": [\n\t{ \"time\": \"2023-03-08T12:06:46Z\",\n\t\"resourceId\": \"AZURE-FUNC-APP\",\n\t\"category\": \"FunctionAppLogs\",\n\t\"operationName\": \"Microsoft.Web/sites/functions/log\",\n\t\"level\": \"Informational\",\n\t\"location\": \"My Location\",\n\t\"properties\": {\n\t'appName':'',\n\t'roleInstance':'123123123123',\n\t'message':'Loading functions metadata',\n\t'category':'Host.Startup',\n\t'hostVersion':'X.XX.X.X',\n\t'hostInstanceId':'myInstance',\n\t'level':'Information',\n\t'levelId':2,\n\t'processId':155,\n\t'eventId':3143,\n\t'eventName':'FunctionMetadataManagerLoadingFunctionsMetadata'\n\t}\n\t},\n\t{ \"time\": \"2023-03-08T12:06:47Z\",\n\t\"resourceId\": \"AZURE-FUNC-APP-2\",\n\t\"category\": \"FunctionAppLogs\",\n\t\"operationName\": \"Microsoft.Web/sites/functions/log\",\n\t\"level\": \"Informational\",\n\t\"location\": \"My Location\",\n\t\"properties\": {\n\t'appName':'',\n\t'roleInstance':'123123123123',\n\t'message':'Loading functions metadata',\n\t'category':'Host.Startup',\n\t'hostVersion':'X.XX.X.X',\n\t'hostInstanceId':'myInstance',\n\t'level':'Information',\n\t'levelId':2,\n\t'processId':155,\n\t'eventId':3143,\n\t'eventName':'FunctionMetadataManagerLoadingFunctionsMetadata'\n\t}\n\t}\n]}\n```\n\n----------------------------------------\n\nTITLE: Converting to Uppercase with string.to_upper in Alloy\nDESCRIPTION: The string.to_upper function converts all lowercase letters in a string to uppercase.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/string.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\n> string.to_upper(\"hello\")\n\"HELLO\"\n```\n\n----------------------------------------\n\nTITLE: Processing Metrics by Name Pattern in Alloy\nDESCRIPTION: Configures an attribute processor that filters metrics based on metric name regex patterns. It adds or updates a label called \"important_label\" with value \"label_val\" for metrics with names starting with \"counter\", targeting only metric signals.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.attributes.md#2025-04-22_snippet_10\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.processor.attributes \"default\" {\n\tinclude {\n\t\tmatch_type = \"regexp\"\n\t\tmetric_names = [\"counter.*\"]\n\t}\n\taction {\n\t\tkey = \"important_label\"\n\t\taction = \"upsert\"\n\t\tvalue = \"label_val\"\n\t}\n\n    output {\n        metrics = [otelcol.exporter.otlp.default.input]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Fixed Tenant ID in Alloy Pipeline\nDESCRIPTION: Demonstrates how to set a fixed tenant ID value in the pipeline using the stage.tenant block.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_45\n\nLANGUAGE: alloy\nCODE:\n```\nstage.tenant {\n    value = \"team-a\"\n}\n```\n\n----------------------------------------\n\nTITLE: Template-based Replace Operation\nDESCRIPTION: Shows how to use Go template functions in replace operations to transform captured values.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_32\n\nLANGUAGE: alloy\nCODE:\n```\nstage.replace {\n    expression = \"^(?P<ip>\\\\S+) (?P<identd>\\\\S+) (?P<user>\\\\S+) \\\\[(?P<timestamp>[\\\\w:/]+\\\\s[+\\\\-]\\\\d{4})\\\\]\"\n    replace    = \"{{ .Value | ToUpper }}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana Alloy Network Listen Address\nDESCRIPTION: Command line argument to configure the network listen address for Grafana Alloy's HTTP server. This allows other machines to access the UI for debugging purposes. The listen address can be set to a specific IP or 0.0.0.0 to listen on all interfaces.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/windows.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n--server.http.listen-addr=LISTEN_ADDR:12345\n```\n\n----------------------------------------\n\nTITLE: Configuring Helm Chart to Use Existing ConfigMap\nDESCRIPTION: YAML configuration for the Helm chart's values.yaml to use an existing ConfigMap instead of creating a new one.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/kubernetes.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nalloy:\n  configMap:\n    create: false\n    name: alloy-config\n    key: config.alloy\n```\n\n----------------------------------------\n\nTITLE: Configuring IONOS Cloud Discovery in Grafana Alloy\nDESCRIPTION: Basic usage of the discovery.ionos component to retrieve scrape targets from the IONOS Cloud API. It requires specifying a datacenter ID.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.ionos.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.ionos \"<LABEL>\" {\n    datacenter_id = \"<DATACENTER_ID>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using array.combine_maps with Discovery and Exporter Components in Alloy\nDESCRIPTION: Examples of using array.combine_maps function with Kubernetes discovery and Prometheus exporter components to combine target information.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/array.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\n> array.combine_maps(discovery.kubernetes.k8s_pods.targets, prometheus.exporter.postgres, [\"instance\"])\n\n> array.combine_maps(prometheus.exporter.redis.default.targets, [{\"instance\"=\"1.1.1.1\", \"testLabelKey\" = \"testLabelVal\"}], [\"instance\"])\n```\n\n----------------------------------------\n\nTITLE: Executing Log Generation Commands in Bash\nDESCRIPTION: These bash commands generate sample log entries with different log levels (info, warn, debug) and append them to a log file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/logs-and-relabeling-basics.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\necho 'level=info msg=\"INFO: This is an info level log!\"' >> /tmp/alloy-logs/log.log\necho 'level=warn msg=\"WARN: This is a warn level log!\"' >> /tmp/alloy-logs/log.log\necho 'level=debug msg=\"DEBUG: This is a debug level log!\"' >> /tmp/alloy-logs/log.log\n```\n\n----------------------------------------\n\nTITLE: Reinstalling Alloy Homebrew Formula\nDESCRIPTION: Command to reinstall the Grafana Alloy Formula after modifying the service configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/macos.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nbrew reinstall --formula alloy\n```\n\n----------------------------------------\n\nTITLE: Combining Maps with array.combine_maps in Alloy\nDESCRIPTION: The array.combine_maps function joins two arrays of maps if certain keys have matching values in both maps. It takes three arguments: two lists of maps and an array of keys to match. It's useful for combining labels from different Prometheus discovery or exporter components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/array.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\n> array.combine_maps([{\"instance\"=\"1.1.1.1\", \"team\"=\"A\"}], [{\"instance\"=\"1.1.1.1\", \"cluster\"=\"prod\"}], [\"instance\"])\n[{\"instance\"=\"1.1.1.1\", \"team\"=\"A\", \"cluster\"=\"prod\"}]\n\n// Second map overrides the team in the first map\n> array.combine_maps([{\"instance\"=\"1.1.1.1\", \"team\"=\"A\"}], [{\"instance\"=\"1.1.1.1\", \"team\"=\"B\"}], [\"instance\"])\n[{\"instance\"=\"1.1.1.1\", \"team\"=\"B\"}]\n\n// If multiple maps from the first argument match with multiple maps from the second argument, different combinations will be created.\n> array.combine_maps([{\"instance\"=\"1.1.1.1\", \"team\"=\"A\"}, {\"instance\"=\"1.1.1.1\", \"team\"=\"B\"}], [{\"instance\"=\"1.1.1.1\", \"cluster\"=\"prod\"}, {\"instance\"=\"1.1.1.1\", \"cluster\"=\"ops\"}], [\"instance\"])\n[{\"instance\"=\"1.1.1.1\", \"team\"=\"A\", \"cluster\"=\"prod\"}, {\"instance\"=\"1.1.1.1\", \"team\"=\"A\", \"cluster\"=\"ops\"}, {\"instance\"=\"1.1.1.1\", \"team\"=\"B\", \"cluster\"=\"prod\"}, {\"instance\"=\"1.1.1.1\", \"team\"=\"B\", \"cluster\"=\"ops\"}]\n```\n\n----------------------------------------\n\nTITLE: Defining a Plus Component in Alloy\nDESCRIPTION: Definition of a custom 'plus' component in Alloy that adds two numbers. This is used as a library component imported by other modules.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/import.file.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\ndeclare \"plus\" {\n  argument \"a\" {}\n  argument \"b\" {}\n\n  export \"sum\" {\n    value = argument.a.value + argument.b.value\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cluster Events Collection in Kubernetes Monitoring Helm Chart\nDESCRIPTION: YAML configuration for enabling the collection of cluster events from the 'meta' and 'prod' namespaces in the Kubernetes Monitoring Helm chart.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nclusterEvents:\n  enabled: true\n  collector: alloy-logs\n  namespaces:\n    - meta\n    - prod\n```\n\n----------------------------------------\n\nTITLE: Prometheus Metrics After Second Relabeling Rule\nDESCRIPTION: Demonstrates the metrics after applying the second relabeling rule, which keeps only metrics with the 'backend' app label.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.relabel.md#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nmetric_a{host = \"localhost/development\", __address__ = \"localhost\", instance = \"development\", app = \"backend\"}  2\nmetric_a{host = \"cluster_a/production\",  __address__ = \"cluster_a\", instance = \"production\",  app = \"backend\"}  9\n```\n\n----------------------------------------\n\nTITLE: Scheduled Task Collector Configuration\nDESCRIPTION: Configuration parameters for filtering Windows scheduled tasks using regular expressions. Allows including and excluding specific tasks based on pattern matching.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.windows.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Name      | Type     | Description                             | Default  | Required |\n| --------- | -------- | --------------------------------------- | -------- | -------- |\n| `exclude` | `string` | Regular expression of tasks to exclude. | `\"^$\"` | no       |\n| `include` | `string` | Regular expression of tasks to include. | `\"^.+$\"` | no       |\n```\n\n----------------------------------------\n\nTITLE: JSON Target File Example for discovery.file\nDESCRIPTION: Example of a JSON file format that can be used with discovery.file. It demonstrates how to define targets and labels for discovery.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.file.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"targets\": [ \"127.0.0.1:9091\", \"127.0.0.1:9092\" ],\n    \"labels\": {\n      \"environment\": \"dev\"\n    }\n  },\n  {\n    \"targets\": [ \"127.0.0.1:9093\" ],\n    \"labels\": {\n      \"environment\": \"prod\"\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Disabling Grafana Alloy Data Collection with Ansible\nDESCRIPTION: This YAML snippet demonstrates how to use Ansible to install Grafana Alloy with data reporting disabled. It includes the Alloy role and sets a custom argument to disable reporting.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/data-collection.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Install Alloy\n  hosts: all\n  become: true\n  tasks:\n    - name: Install Alloy\n      ansible.builtin.include_role:\n        name: grafana.grafana.alloy\n      vars:\n        alloy_env_file_vars:\n          CUSTOM_ARGS: \"--disable-reporting\"\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespaces\nDESCRIPTION: Command to create the 'meta' and 'prod' namespaces in the Kubernetes cluster.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create namespace meta && \\\nkubectl create namespace prod\n```\n\n----------------------------------------\n\nTITLE: Cloning Grafana Alloy Scenarios Repository\nDESCRIPTION: Shell commands to clone the Grafana Alloy scenarios repository containing monitoring examples.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-linux.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/grafana/alloy-scenarios.git\n```\n\n----------------------------------------\n\nTITLE: Defining Metric Block Structure in Markdown\nDESCRIPTION: This code snippet defines the structure of a 'metric' block used for configuring individual metrics. It includes a table with columns for property name, type, description, default value, and whether it's required.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.vcenter.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n#### metric block\n\nName | Type | Description | Default | Required\n---- | ---- | ----------- | ------- | --------\n`enabled` | `boolean` | Whether to enable the metric. | `true` | no\n```\n\n----------------------------------------\n\nTITLE: Defining Kafka Metadata Retry Configuration in YAML\nDESCRIPTION: This YAML snippet defines the structure of the 'retry' block used to configure Kafka metadata retrieval retry behavior. It specifies two parameters: max_retries for the number of retry attempts, and backoff for the wait time between retries.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/shared/reference/components/otelcol-kafka-metadata-retry.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nretry:\n  max_retries: 3\n  backoff: \"250ms\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Processing and Relabeling in Alloy\nDESCRIPTION: This Alloy configuration sets up log processing to extract the 'level' from log lines, add it as a label, and send the processed logs to Loki. It uses local.file_match, loki.source.file, loki.process, loki.relabel, and loki.write components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/logs-and-relabeling-basics.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\nlocal.file_match \"tmplogs\" {\n    path_targets = [{\"__path__\" = \"/tmp/alloy-logs/*.log\"}]\n}\n\nloki.source.file \"local_files\" {\n    targets    = local.file_match.tmplogs.targets\n    forward_to = [loki.process.add_new_label.receiver]\n}\n\nloki.process \"add_new_label\" {\n    stage.logfmt {\n        mapping = {\n            \"extracted_level\" = \"level\",\n        }\n    }\n\n    stage.labels {\n        values = {\n            \"level\" = \"extracted_level\",\n        }\n    }\n\n    forward_to = [loki.relabel.add_static_label.receiver]\n}\n\nloki.relabel \"add_static_label\" {\n    forward_to = [loki.write.local_loki.receiver]\n\n    rule {\n        target_label = \"os\"\n        replacement  = constants.os\n    }\n}\n\nloki.write \"local_loki\" {\n    endpoint {\n        url = \"http://localhost:3100/loki/api/v1/push\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Diagnostic Report During Configuration Conversion\nDESCRIPTION: This command converts an OpenTelemetry Collector configuration and outputs a diagnostic report. The --report flag specifies where to save the conversion diagnostics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-otelcol.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=otelcol --report=<OUTPUT_REPORT_PATH> --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Basic Loki Journal Source Configuration in Alloy\nDESCRIPTION: Basic usage template for configuring a loki.source.journal component with required forward_to parameter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.journal.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.journal \"<LABEL>\" {\n  forward_to    = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: Redirecting Grafana Alloy Logs on Linux/macOS\nDESCRIPTION: This command starts Grafana Alloy and redirects both stdout and stderr to a specified output file. Replace <BINARY_PATH>, <CONFIG_PATH>, and <OUTPUT_FILE> with appropriate values.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/binary.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n<BINARY_PATH> run <CONFIG_PATH> &> <OUTPUT_FILE>\n```\n\n----------------------------------------\n\nTITLE: Complete Declare.Dynamic Block for Redis Exporter in Alloy\nDESCRIPTION: Full example of the declare.dynamic proposal handling Redis targets, showing how to define the component with arguments and the special 'id' identifier for creating sub-pipelines.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/design/1443-dynamic-pipelines.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\n// declare.dynamic \"maps\" each target to a sub-pipeline.\n// Each sub-pipeline has 1 exporter, 1 relabel, and 1 scraper.\n// Internally, maybe one way this can be done via serializing the pipeline to a string and then importing it as a module?\ndeclare.dynamic \"redis_exporter\" {\n  argument \"input_targets\" {\n    optional = false\n    comment = \"We will create a sub-pipeline for each target in input_targets.\"\n  }\n\n  argument \"output_metrics\" {\n    optional = false\n    comment = \"All the metrics gathered from all pipelines.\"\n  }\n\n  // \"id\" is a special identifier for every \"sub-pipeline\".\n  // The number of \"sub-pipelines\" is equal to len(input_targets).\n  prometheus.exporter.redis id {\n    redis_addr = input_targets[\"__address__\"]\n  }\n\n  discovery.relabel id {\n    targets = prometheus.exporter.redis[id].targets\n    // Add a label which comes from the discovery component.\n    rule {\n      target_label = \"filepath\"\n      // __meta_filepath comes from discovery.file\n      replacement  = input_targets[\"__meta_filepath\"]\n    }\n  }\n\n  prometheus.scrape id {\n    targets = prometheus.exporter.redis[id].targets\n    forward_to = output_metrics\n  }\n\n}\ndiscovery.file \"default\" {\n  files = [\"/Users/batman/Desktop/redis_addresses.yaml\"]\n}\n\ndeclare.dynamic.redis_exporter \"default\" {\n  input_targets = discovery.file.default.targets\n  output_metrics = [prometheus.remote_write.mimir.receiver]\n}\n\nprometheus.remote_write \"mimir\" {\n  endpoint {\n    url = \"https://prometheus-prod-05-gb-south-0.grafana.net/api/prom/push\"\n    basic_auth {\n      username = \"\"\n      password = \"\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Linux Capabilities in Kubernetes\nDESCRIPTION: YAML configuration for Helm values to set necessary Linux capabilities for pyroscope.java to function properly in a Kubernetes environment.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.java.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nalloy:\n  securityContext:\n    runAsUser: 0\n    runAsNonRoot: false\n    capabilities:\n      add:\n        - PERFMON\n        - SYS_PTRACE\n        - SYS_RESOURCE\n        - SYS_ADMIN\n```\n\n----------------------------------------\n\nTITLE: Getting Alloy Pod Name\nDESCRIPTION: Command to get the name of the Alloy pod deployed in the 'meta' namespace and store it in an environment variable.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nexport POD_NAME=$(kubectl get pods --namespace meta -l \"app.kubernetes.io/name=alloy-logs,app.kubernetes.io/instance=k8s\" -o jsonpath=\"{.items[0].metadata.name}\")\n```\n\n----------------------------------------\n\nTITLE: Restarting Alloy Service After Formula Changes\nDESCRIPTION: Command to restart the Grafana Alloy service after reinstalling the modified formula.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/macos.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nbrew services restart alloy\n```\n\n----------------------------------------\n\nTITLE: Example of Prometheus Metrics Exposition Format with Timestamp\nDESCRIPTION: Demonstrates a sample in the Prometheus metrics exposition format with an explicit timestamp of 1395066363000. This format is referenced in the context of timestamp staleness tracking.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.scrape.md#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nhttp_requests_total{method=\"post\",code=\"200\"} 1027 1395066363000\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana Alloy Helm Values for Deployment\nDESCRIPTION: YAML configuration for deploying Grafana Alloy using Helm. It sets up Alloy as a StatefulSet with clustering enabled and 2 replicas.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-operator.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nalloy:\n  configMap:\n    create: true\n  clustering:\n    enabled: true\ncontroller:\n  type: 'statefulset'\n  replicas: 2\ncrds:\n  create: false\n```\n\n----------------------------------------\n\nTITLE: Installing Debug Symbols for System Libraries in Ubuntu\nDESCRIPTION: Command to install debug symbols for the C library in Ubuntu, which helps resolve symbols when profiling applications that use system libraries.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.ebpf.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napt install libc6-dbg\n```\n\n----------------------------------------\n\nTITLE: Encoding Strings to URL-safe Base64 with encoding.to_URLbase64 in Alloy\nDESCRIPTION: Shows how to encode an original string into a RFC4648-compliant URL safe Base64 encoded string.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/encoding.md#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n> encoding.to_URLbase64(\"string123!?$*&()'-=@~\")\nc3RyaW5nMTIzIT8kKiYoKSctPUB-\n```\n\n----------------------------------------\n\nTITLE: Configuring Exporter Block for Datadog Exporter in YAML\nDESCRIPTION: The exporter block configures general metric exporter settings, including options for resource attributes and instrumentation scope metadata.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.datadog.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nexporter:\n  resource_attributes_as_tags: false\n  instrumentation_scope_metadata_as_tags: false\n```\n\n----------------------------------------\n\nTITLE: Filesystem Type Exclusion Patterns by OS\nDESCRIPTION: Regular expression patterns for excluding filesystem types based on different operating systems\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.unix.md#2025-04-22_snippet_1\n\nLANGUAGE: linux\nCODE:\n```\n^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\n```\n\nLANGUAGE: osx\nCODE:\n```\n^(autofs|devfs)$\n```\n\nLANGUAGE: bsd\nCODE:\n```\n^devfs$\n```\n\n----------------------------------------\n\nTITLE: Bypassing Errors During OpenTelemetry Configuration Conversion\nDESCRIPTION: This command converts an OpenTelemetry Collector configuration while bypassing non-critical errors. The --bypass-errors flag enables best-effort conversion when the original configuration has issues.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-otelcol.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=otelcol --bypass-errors --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Verifying Docker Container Status\nDESCRIPTION: Command to check the status of deployed Docker containers to ensure they are running properly.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-docker-containers.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker ps\n```\n\n----------------------------------------\n\nTITLE: Basic Convert Command Usage in Shell\nDESCRIPTION: Shows the basic syntax for using the convert command with optional flags and required filename argument. The command converts configuration files from supported formats to Alloy configuration format.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/cli/convert.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert [<FLAG> ...] <FILE_NAME>\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Structure\nDESCRIPTION: Documentation detailing the relationships between telemetry components, organized into sections for Prometheus, Loki, and OpenTelemetry interfaces. Each section uses collapsible sections to group related components by namespace.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/compatibility/_index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{{< collapse title=\"otelcol\" >}}\n- [otelcol.exporter.prometheus](../components/otelcol/otelcol.exporter.prometheus)\n{{< /collapse >}}\n\n{{< collapse title=\"prometheus\" >}}\n- [prometheus.operator.podmonitors](../components/prometheus/prometheus.operator.podmonitors)\n- [prometheus.operator.probes](../components/prometheus/prometheus.operator.probes)\n- [prometheus.operator.scrapeconfigs](../components/prometheus/prometheus.operator.scrapeconfigs)\n- [prometheus.operator.servicemonitors](../components/prometheus/prometheus.operator.servicemonitors)\n- [prometheus.receive_http](../components/prometheus/prometheus.receive_http)\n- [prometheus.relabel](../components/prometheus/prometheus.relabel)\n- [prometheus.scrape](../components/prometheus/prometheus.scrape)\n{{< /collapse >}}\n```\n\n----------------------------------------\n\nTITLE: Defining AWS MSK SASL Authentication Parameters in YAML\nDESCRIPTION: This YAML snippet defines the structure and arguments for the 'aws_msk' configuration block used in SASL authentication for AWS MSK. It specifies two required string parameters: 'region' for the AWS region of the MSK cluster, and 'broker_addr' for the MSK address used in authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/shared/reference/components/otelcol-kafka-authentication-sasl-aws_msk.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nName          | Type     | Description                                   | Default | Required\n--------------|----------|-----------------------------------------------|---------|---------\n`region`      | `string` | AWS region the MSK cluster is based in.       |         | yes\n`broker_addr` | `string` | MSK address to connect to for authentication. |         | yes\n```\n\n----------------------------------------\n\nTITLE: Configuring LiveDebugging in Grafana Alloy\nDESCRIPTION: Alloy configuration block to enable live debugging, which streams real-time data from components to the Alloy UI.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-structured-logs.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\nlivedebugging {\n  enabled = true\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Promtail Configuration Example\nDESCRIPTION: Command example that demonstrates how to convert a specific Promtail configuration file to a Grafana Alloy configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-promtail.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=promtail --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.receiver.syslog in Alloy\nDESCRIPTION: Basic configuration structure for the otelcol.receiver.syslog component in Alloy. It shows how to set up TCP and UDP receivers and configure the output for logs.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.receiver.syslog.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.syslog \"LABEL\" {\n  tcp { ... }\n  udp { ... }\n\n  output {\n    logs    = [...]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Redis Container with Docker\nDESCRIPTION: This command starts a Docker container running Redis, exposing it on port 6379.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/first-components-and-stdlib.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker container run -d --name alloy-redis -p 6379:6379 --rm redis\n```\n\n----------------------------------------\n\nTITLE: Collecting Kubernetes Pod Logs via API\nDESCRIPTION: Comprehensive configuration to collect logs from Kubernetes pods through the Kubernetes API, including discovery, relabeling, sourcing, and processing of log data before forwarding to Loki.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/logs-in-kubernetes.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\n// discovery.kubernetes allows you to find scrape targets from Kubernetes resources.\n// It watches cluster state and ensures targets are continually synced with what is currently running in your cluster.\ndiscovery.kubernetes \"pod\" {\n  role = \"pod\"\n}\n\n// discovery.relabel rewrites the label set of the input targets by applying one or more relabeling rules.\n// If no rules are defined, then the input targets are exported as-is.\ndiscovery.relabel \"pod_logs\" {\n  targets = discovery.kubernetes.pod.targets\n\n  // Label creation - \"namespace\" field from \"__meta_kubernetes_namespace\"\n  rule {\n    source_labels = [\"__meta_kubernetes_namespace\"]\n    action = \"replace\"\n    target_label = \"namespace\"\n  }\n\n  // Label creation - \"pod\" field from \"__meta_kubernetes_pod_name\"\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_name\"]\n    action = \"replace\"\n    target_label = \"pod\"\n  }\n\n  // Label creation - \"container\" field from \"__meta_kubernetes_pod_container_name\"\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_container_name\"]\n    action = \"replace\"\n    target_label = \"container\"\n  }\n\n  // Label creation -  \"app\" field from \"__meta_kubernetes_pod_label_app_kubernetes_io_name\"\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_label_app_kubernetes_io_name\"]\n    action = \"replace\"\n    target_label = \"app\"\n  }\n\n  // Label creation -  \"job\" field from \"__meta_kubernetes_namespace\" and \"__meta_kubernetes_pod_container_name\"\n  // Concatenate values __meta_kubernetes_namespace/__meta_kubernetes_pod_container_name\n  rule {\n    source_labels = [\"__meta_kubernetes_namespace\", \"__meta_kubernetes_pod_container_name\"]\n    action = \"replace\"\n    target_label = \"job\"\n    separator = \"/\"\n    replacement = \"$1\"\n  }\n\n  // Label creation - \"container\" field from \"__meta_kubernetes_pod_uid\" and \"__meta_kubernetes_pod_container_name\"\n  // Concatenate values __meta_kubernetes_pod_uid/__meta_kubernetes_pod_container_name.log\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_uid\", \"__meta_kubernetes_pod_container_name\"]\n    action = \"replace\"\n    target_label = \"__path__\"\n    separator = \"/\"\n    replacement = \"/var/log/pods/*$1/*.log\"\n  }\n\n  // Label creation -  \"container_runtime\" field from \"__meta_kubernetes_pod_container_id\"\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_container_id\"]\n    action = \"replace\"\n    target_label = \"container_runtime\"\n    regex = \"^(\\\\S+):\\\\/\\\\/.+$\"\n    replacement = \"$1\"\n  }\n}\n\n// loki.source.kubernetes tails logs from Kubernetes containers using the Kubernetes API.\nloki.source.kubernetes \"pod_logs\" {\n  targets    = discovery.relabel.pod_logs.output\n  forward_to = [loki.process.pod_logs.receiver]\n}\n\n// loki.process receives log entries from other Loki components, applies one or more processing stages,\n// and forwards the results to the list of receivers in the component's arguments.\nloki.process \"pod_logs\" {\n  stage.static_labels {\n      values = {\n        cluster = \"<CLUSTER_NAME>\",\n      }\n  }\n\n  forward_to = [loki.write.<WRITE_COMPONENT_NAME>.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Build Image Tags Reference\nDESCRIPTION: Lists the available build image variants with their Docker Hub naming conventions. Includes standard Linux, boringcrypto, and Windows variants.\nSOURCE: https://github.com/grafana/alloy/blob/main/tools/build-image/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* `grafana/alloy-build-image:vX.Y.Z` (for building Linux containers)\n* `grafana/alloy-build-image:vX.Y.Z-boringcrypto` (for building Linux containers with boringcrypto)\n* `grafana/alloy-build-image:vX.Y.Z-windows` (for building Windows containers)\n```\n\n----------------------------------------\n\nTITLE: Creating Linux User for Grafana Alloy\nDESCRIPTION: This command creates a system user named 'alloy' without a home directory and with /bin/false as the shell. It's used for running Grafana Alloy as a service.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/binary.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo useradd --no-create-home --shell /bin/false alloy\n```\n\n----------------------------------------\n\nTITLE: Pipeline Flow Diagram for OpenTelemetry Data\nDESCRIPTION: A plaintext diagram showing the flow of metrics, logs, and traces through the OpenTelemetry pipeline, from the OTLP receiver through the batch processor to the OTLP exporter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-to-lgtm-stack.md#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nMetrics, Logs, Traces: OTLP Receiver → batch processor → OTLP Exporter\n```\n\n----------------------------------------\n\nTITLE: Joining Strings with string.join in Alloy\nDESCRIPTION: The string.join function concatenates all items in an array into a single string, using a specified separator between each item.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/string.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nstring.join(list, separator)\n```\n\nLANGUAGE: alloy\nCODE:\n```\n> string.join([\"foo\", \"bar\", \"baz\"], \"-\")\n\"foo-bar-baz\"\n> string.join([\"foo\", \"bar\", \"baz\"], \", \")\n\"foo, bar, baz\"\n> string.join([\"foo\"], \", \")\n\"foo\"\n```\n\n----------------------------------------\n\nTITLE: Starting Grafana Alloy from Command Line\nDESCRIPTION: This command starts Grafana Alloy using the binary path and configuration file path. Replace <BINARY_PATH> with the path to the Alloy binary file and <CONFIG_PATH> with the path to the configuration file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/binary.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n<BINARY_PATH> run <CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Declare.Dynamic Block Syntax for Collection Processing in Alloy\nDESCRIPTION: Example of the proposed 'declare.dynamic' block syntax that would create a custom component for processing collections, with input and output arguments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/design/1443-dynamic-pipelines.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\ndeclare.dynamic \"ex1\" {\n  argument \"input_targets\" {\n    optional = false\n    comment = \"We will create a sub-pipeline for each target in input_targets.\"\n  }\n\n  argument \"output_metrics\" {\n    optional = false\n    comment = \"All the metrics gathered from all pipelines.\"\n  }\n\n  // A sub-pipeline consisting of components which process each target.\n  ...\n}\n\ndeclare.dynamic.ex1 \"default\" {\n  input_targets = discovery.file.default.targets\n  output_metrics = [prometheus.remote_write.mimir.receiver]\n}\n```\n\n----------------------------------------\n\nTITLE: Using fsnotify Detector Reference\nDESCRIPTION: Reference for the fsnotify detector which uses filesystem events to detect file changes. It requires OS-level filesystem event support and includes polling as a fallback mechanism.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/shared/reference/components/local-file-arguments-text.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nlocal.file\n```\n\n----------------------------------------\n\nTITLE: AWS Firehose Common Attributes Configuration\nDESCRIPTION: Example JSON configuration for X-Amz-Firehose-Common-Attributes header to set static labels in the AWS Firehose delivery stream configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.awsfirehose.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"commonAttributes\": {\n    \"lbl_label1\": \"value1\",\n    \"lbl_label2\": \"value2\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Basic Expressions in Grafana Alloy Configuration\nDESCRIPTION: This snippet shows examples of basic expressions in Grafana Alloy configuration. It includes a string literal and a boolean literal as simple expressions that can be assigned to attributes.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/expressions/_index.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n\"Hello, world!\"\n```\n\nLANGUAGE: plaintext\nCODE:\n```\ntrue\n```\n\n----------------------------------------\n\nTITLE: Sample Diagnostic Report Output from Conversion\nDESCRIPTION: Example output of a diagnostic report generated during the conversion of a Prometheus configuration to Grafana Alloy. It shows how Prometheus components were mapped to Alloy components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-prometheus.md#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n(Info) Converted scrape_configs job_name \"prometheus\" into...\n  A prometheus.scrape.prometheus component\n(Info) Converted 1 remote_write[s] \"grafana-cloud\" into...\n  A prometheus.remote_write.default component\n```\n\n----------------------------------------\n\nTITLE: Deploying the Grafana Stack with Docker Compose\nDESCRIPTION: Commands to navigate to the mail-house example directory and start the Docker containers for the Grafana stack.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-structured-logs.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd alloy-scenarios/mail-house\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of prometheus.exporter.consul in Alloy\nDESCRIPTION: Shows the basic syntax for using the prometheus.exporter.consul component in Alloy configurations. This snippet demonstrates how to declare the component with a label.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.consul.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.consul \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Configuration Example Command\nDESCRIPTION: This is the command to convert the example OpenTelemetry Collector configuration to a Grafana Alloy configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-otelcol.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=otelcol --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Cloning the Alloy Scenarios Repository\nDESCRIPTION: Command to clone the Grafana Alloy scenarios repository containing complete examples of Alloy deployments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-structured-logs.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/grafana/alloy-scenarios.git\n```\n\n----------------------------------------\n\nTITLE: Example Collection Output from Discovery Component in Alloy\nDESCRIPTION: Example of a list of targets produced by a discovery component in Alloy, showing the structure of targets with address and instance information.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/design/1443-dynamic-pipelines.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\n[\n  {\"__address__\" = \"redis-one:9115\", \"instance\" = \"one\"},\n  {\"__address__\" = \"redis-two:9116\", \"instance\" = \"two\"},\n]\n```\n\n----------------------------------------\n\nTITLE: Silent Installation Command for Windows\nDESCRIPTION: Command to perform a silent installation of Grafana Alloy on Windows systems. Requires specifying the path to the installer executable.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/windows.md#2025-04-22_snippet_0\n\nLANGUAGE: cmd\nCODE:\n```\n<PATH_TO_INSTALLER> /S\n```\n\n----------------------------------------\n\nTITLE: Custom Format Timestamp Parsing\nDESCRIPTION: Demonstrates parsing a custom timestamp format with milliseconds and timezone offset.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_49\n\nLANGUAGE: alloy\nCODE:\n```\nstage.timestamp {\n    source = \"time\"\n    format = \"2006-01-02T15:04:05,000-07:00\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of prometheus.exporter.memcached in Alloy\nDESCRIPTION: Demonstrates the basic syntax for using the prometheus.exporter.memcached component in an Alloy configuration file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.memcached.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.memcached \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Grafana Alloy Service\nDESCRIPTION: This environment file defines variables used by the Grafana Alloy systemd service. It includes the configuration file path and custom arguments. Replace <CONFIG_PATH> with the path to your configuration file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/binary.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n## Path:\n## Description: Grafana Alloy settings\n## Type:        string\n## Default:     \"\"\n## ServiceRestart: alloy\n#\n# Command line options for alloy\n#\n# The configuration file holding the Grafana Alloy configuration.\nCONFIG_FILE=\"<CONFIG_PATH>\"\n\n# User-defined arguments to pass to the run command.\nCUSTOM_ARGS=\"\"\n\n# Restart on system upgrade. Defaults to true.\nRESTART_ON_UPGRADE=true\n```\n\n----------------------------------------\n\nTITLE: Example JSON Log Structure\nDESCRIPTION: Sample JSON log entry showing the expected structure with timestamp, secret flag, level, and message fields.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/processing-logs.md#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"log\": {\n        \"is_secret\": \"true\",\n        \"level\": \"info\",\n        \"message\": \"This is a secret message!\"\n    },\n    \"timestamp\": \"2023-11-16T06:01:50Z\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic GitHub Exporter Configuration in Alloy\nDESCRIPTION: Basic configuration example showing how to initialize a GitHub exporter component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.github.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.github \"<LABEL>\" {\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Promtail Configuration to Grafana Alloy\nDESCRIPTION: Command for converting a Promtail configuration file to a Grafana Alloy configuration using the 'convert' CLI command with the --source-format flag.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-promtail.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=promtail --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Sample Input Trace in JSON\nDESCRIPTION: This JSON snippet represents a sample input trace with resource spans, scope spans, and events. It includes various attributes at different levels of the trace structure.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.connector.spanlogs.md#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resourceSpans\": [\n    {\n      \"resource\": {\n        \"attributes\": [\n          {\n            \"key\": \"service.name\",\n            \"value\": { \"stringValue\": \"TestSvcName\" }\n          },\n          {\n            \"key\": \"res_attribute1\",\n            \"value\": { \"intValue\": \"78\" }\n          },\n          {\n            \"key\": \"unused_res_attribute1\",\n            \"value\": { \"stringValue\": \"str\" }\n          },\n          {\n            \"key\": \"res_account_id\",\n            \"value\": { \"intValue\": \"2245\" }\n          }\n        ]\n      },\n      \"scopeSpans\": [\n        {\n          \"spans\": [\n            {\n              \"trace_id\": \"7bba9f33312b3dbb8b2c2c62bb7abe2d\",\n              \"span_id\": \"086e83747d0e381e\",\n              \"name\": \"TestSpan\",\n              \"attributes\": [\n                {\n                  \"key\": \"attribute1\",\n                  \"value\": { \"intValue\": \"78\" }\n                },\n                {\n                  \"key\": \"unused_attribute1\",\n                  \"value\": { \"intValue\": \"78\" }\n                },\n                {\n                  \"key\": \"account_id\",\n                  \"value\": { \"intValue\": \"2245\" }\n                }\n              ],\n              \"events\": [\n                {\n                  \"name\": \"log\",\n                  \"attributes\": [\n                    {\n                      \"key\": \"log.severity\",\n                      \"value\": { \"stringValue\": \"INFO\" }\n                    },\n                    {\n                      \"key\": \"log.message\",\n                      \"value\": { \"stringValue\": \"TestLogMessage\" }\n                    }\n                  ]\n                }\n              ]\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Memory Management Environment Variables\nDESCRIPTION: Configuration examples for Go memory management variables including GOMEMLIMIT with various unit suffixes.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/cli/environment-variables.md#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nGOMEMLIMIT=9GiB\nGOGC=100\nGOGC=off\n```\n\n----------------------------------------\n\nTITLE: Viewing Grafana Alloy Service Logs\nDESCRIPTION: Command to view the Grafana Alloy service logs using journalctl\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/linux.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nsudo journalctl -u alloy\n```\n\n----------------------------------------\n\nTITLE: Trimming Prefix with string.trim_prefix in Alloy\nDESCRIPTION: The string.trim_prefix function removes a specified prefix from the start of a string. If the string doesn't start with the prefix, it returns the original string.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/string.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\n> string.trim_prefix(\"helloworld\", \"hello\")\n\"world\"\n```\n\n----------------------------------------\n\nTITLE: Azure AD Cloud Configuration Parameters\nDESCRIPTION: Configuration table defining the cloud parameter for Azure AD integration. Specifies the available cloud environments including public, China, and government options.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/shared/reference/components/azuread-block.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nName    | Type     | Description      | Default         | Required\n--------|----------|------------------|-----------------|---------\n`cloud` | `string` | The Azure Cloud. | `\"AzurePublic\"` | no\n```\n\n----------------------------------------\n\nTITLE: Setting Log Level in Alloy Configuration\nDESCRIPTION: This snippet shows how to set the log level attribute in an Alloy configuration. It demonstrates the basic syntax for setting an attribute value.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/_index.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nlog_level = \"debug\"\n```\n\n----------------------------------------\n\nTITLE: Markdown Table of Tail Sampling Block Configuration\nDESCRIPTION: A comprehensive table showing the hierarchical structure of supported blocks within the tail sampling processor configuration, including policy types and their descriptions.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.tail_sampling.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Hierarchy                                                     | Block                    | Description                                                                                                 | Required |\n| ------------------------------------------------------------- | ------------------------ | ----------------------------------------------------------------------------------------------------------- | -------- |\n| policy                                                        | [policy][]               | Policies used to make a sampling decision.                                                                  | yes      |\n| policy > latency                                              | [latency][]              | The policy samples based on the duration of the trace.                                                      | no       |\n| policy > numeric_attribute                                    | [numeric_attribute][]    | The policy samples based on the number attributes (resource and record).                                    | no       |\n```\n\n----------------------------------------\n\nTITLE: Installing Grafana Alloy on macOS via Homebrew\nDESCRIPTION: This command installs Grafana Alloy on macOS using Homebrew. It fetches and installs the package from the previously added Grafana tap.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/macos.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nbrew install grafana/grafana/alloy\n```\n\n----------------------------------------\n\nTITLE: Deploying Grafana Stack with Docker Compose\nDESCRIPTION: Commands to navigate to the logs-file directory and start the Docker containers for the Grafana stack deployment.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-logs-from-file.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd alloy-scenarios/logs-file\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Configuring Histograms Block for Datadog Exporter in YAML\nDESCRIPTION: The histograms block sets up histogram reporting options, including mode and aggregation metrics settings.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.datadog.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nhistograms:\n  mode: \"distributions\"\n  send_aggregation_metrics: false\n```\n\n----------------------------------------\n\nTITLE: Cloning Grafana Alloy Scenarios Repository\nDESCRIPTION: Command to clone the Grafana Alloy scenarios repository containing example deployments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-syslog-messages.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/grafana/alloy-scenarios.git\n```\n\n----------------------------------------\n\nTITLE: Creating Configuration File for Grafana Alloy\nDESCRIPTION: Bash command to create an empty configuration file for Grafana Alloy that will contain the component definitions for log collection and processing.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/send-logs-to-loki.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntouch config.alloy\n```\n\n----------------------------------------\n\nTITLE: Curl Command for Log Insertion\nDESCRIPTION: Command to send a test log entry to the configured HTTP endpoint with current timestamp.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/processing-logs.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl localhost:9999/loki/api/v1/raw -XPOST -H \"Content-Type: application/json\" -d '{\"log\": {\"is_secret\": \"false\", \"level\": \"debug\", \"message\": \"This is a debug message!\"}, \"timestamp\":  \"'\"$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\"'\"}'\n```\n\n----------------------------------------\n\nTITLE: Verifying Grafana Alloy Service Status\nDESCRIPTION: This shell command checks the status of the Grafana Alloy service on the target machine to verify that it is active and running correctly.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/ansible.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemctl status alloy.service\n```\n\n----------------------------------------\n\nTITLE: Configuring Clustering in values.yaml for Grafana Alloy\nDESCRIPTION: This YAML snippet shows how to enable clustering by setting clustering.enabled to true inside the alloy block in your values.yaml file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/clustering/_index.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nalloy:\n  clustering:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of otelcol.exporter.otlp in Alloy\nDESCRIPTION: Demonstrates the basic configuration for the otelcol.exporter.otlp component, specifying the endpoint for the gRPC server.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.otlp.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlp \"LABEL\" {\n  client {\n    endpoint = \"HOST:PORT\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Object Construction with Multiple Key-Value Pairs in Alloy Syntax\nDESCRIPTION: Example of constructing an object with multiple key-value pairs enclosed in curly braces, using commas to separate pairs.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/expressions/types_and_values.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\n{\n  first_name = \"John\",\n  last_name  = \"Doe\",\n}\n```\n\n----------------------------------------\n\nTITLE: Debugging Environment Variables\nDESCRIPTION: Examples of GODEBUG settings for controlling debugging variables in the Go runtime, including X.509 certificates and DNS resolver options.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/cli/environment-variables.md#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nGODEBUG=x509usefallbackroots=1\nGODEBUG=netdns=go\nGODEBUG=netdns=1\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Component in Alloy\nDESCRIPTION: Definition of a custom component in Alloy that adds two numbers. This component takes two arguments 'a' and 'b', and exports their sum as 'sum'.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/import.file.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\ndeclare \"add\" {\n  argument \"a\" {}\n  argument \"b\" {}\n\n  export \"sum\" {\n    value = argument.a.value + argument.b.value\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Binary Execution Permissions on Unix-like Systems\nDESCRIPTION: Command to make the Grafana Alloy binary executable on Linux, macOS, or FreeBSD systems. The command uses chmod to set execution permissions for the specified binary path.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/binary.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nchmod +x <BINARY_PATH>\n```\n\n----------------------------------------\n\nTITLE: Importing GPG Key for Debian/Ubuntu\nDESCRIPTION: Commands to import the GPG key and add the Grafana package repository for Debian/Ubuntu systems.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo mkdir -p /etc/apt/keyrings/\nwget -q -O - https://apt.grafana.com/gpg.key | gpg --dearmor | sudo tee /etc/apt/keyrings/grafana.gpg > /dev/null\necho \"deb [signed-by=/etc/apt/keyrings/grafana.gpg] https://apt.grafana.com stable main\" | sudo tee /etc/apt/sources.list.d/grafana.list\n```\n\n----------------------------------------\n\nTITLE: Trimming Suffix with string.trim_suffix in Alloy\nDESCRIPTION: The string.trim_suffix function removes a specified suffix from the end of a string.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/string.md#2025-04-22_snippet_8\n\nLANGUAGE: alloy\nCODE:\n```\n> string.trim_suffix(\"helloworld\", \"world\")\n\"hello\"\n```\n\n----------------------------------------\n\nTITLE: Creating Unlabeled Block in Alloy\nDESCRIPTION: Shows the pattern for creating an unlabeled block structure that can contain attributes and nested blocks.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/syntax.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nBLOCK_NAME {\n  // Block body can contain attributes and nested unlabeled blocks\n  IDENTIFIER = EXPRESSION // Attribute\n\n  NESTED_BLOCK_NAME {\n    // Nested block body\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using prometheus.remote_write sample-stats Command in Grafana Alloy CLI\nDESCRIPTION: This command reads a Prometheus Write-Ahead Log (WAL) directory and collects sample information for metrics. It reports timestamp data and sample counts for each metric, with optional filtering via the --selector flag.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/cli/tools.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nalloy tools prometheus.remote_write sample-stats [<FLAG> ...] <WAL_DIRECTORY>\n```\n\n----------------------------------------\n\nTITLE: Splitting Strings with string.split in Alloy\nDESCRIPTION: The string.split function divides a string into a list of substrings at all occurrences of a specified separator.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/string.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\nsplit(list, separator)\n```\n\nLANGUAGE: alloy\nCODE:\n```\n> string.split(\"foo,bar,baz\", \",\" )\n[\"foo\", \"bar\", \"baz\"]\n\n> string.split(\"foo\", \",\")\n[\"foo\"]\n\n> string.split(\"\", \",\")\n[\"\"]\n\n```\n\n----------------------------------------\n\nTITLE: Converting Grafana Agent Static Configuration to Grafana Alloy\nDESCRIPTION: Command to convert a Grafana Agent Static configuration file to a Grafana Alloy configuration. The command specifies the source format as static, an output path for the generated configuration, and the input path of the original configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-static.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=static --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Object Construction with Quoted Keys in Alloy Syntax\nDESCRIPTION: Example of constructing an object with keys that aren't valid identifiers, requiring double quotes around the keys.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/expressions/types_and_values.md#2025-04-22_snippet_8\n\nLANGUAGE: alloy\nCODE:\n```\n{\n  \"app.kubernetes.io/name\"     = \"mysql\",\n  \"app.kubernetes.io/instance\" = \"mysql-abcxyz\",\n  namespace                    = \"default\",\n}\n```\n\n----------------------------------------\n\nTITLE: Example of Detailed Verbosity Output\nDESCRIPTION: Demonstrates the log output format for 'detailed' verbosity level in the debug exporter, showing comprehensive information about resource spans and individual spans.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.debug.md#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nts=2024-06-13T11:24:13.782957Z level=info msg=TracesExporter component_path=/ component_id=otelcol.exporter.debug.default \"resource spans\"=1 spans=2\nts=2024-06-13T11:24:13.783101Z level=info msg=\"ResourceSpans #0\nResource SchemaURL: https://opentelemetry.io/schemas/1.4.0\nResource attributes:\n     -> service.name: Str(telemetrygen)\nScopeSpans #0\nScopeSpans SchemaURL:\nInstrumentationScope telemetrygen\nSpan #0\n    Trace ID       : 3bde5d3ee82303571bba6e1136781fe4\n    Parent ID      : 5e9dcf9bac4acc1f\n    ID             : 2cf3ef2899aba35c\n    Name           : okey-dokey\n    Kind           : Server\n    Start time     : 2023-11-11 04:49:03.509369393 +0000 UTC\n    End time       : 2023-11-11 04:49:03.50949377 +0000 UTC\n    Status code    : Unset\n    Status message :\nAttributes:\n     -> net.peer.ip: Str(1.2.3.4)\n     -> peer.service: Str(telemetrygen-client)\nSpan #1\n    Trace ID       : 3bde5d3ee82303571bba6e1136781fe4\n    Parent ID      :\n    ID             : 5e9dcf9bac4acc1f\n    Name           : lets-go\n    Kind           : Client\n    Start time     : 2023-11-11 04:49:03.50935117 +0000 UTC\n    End time       : 2023-11-11 04:49:03.50949377 +0000 UTC\n    Status code    : Unset\n    Status message :\nAttributes:\n     -> net.peer.ip: Str(1.2.3.4)\n     -> peer.service: Str(telemetrygen-server)\n        {\"kind\": \"exporter\", \"data_type\": \"traces\", \"name\": \"debug\"}\"\n```\n\n----------------------------------------\n\nTITLE: Stopping Grafana Alloy Service\nDESCRIPTION: Command to stop the systemd service for Grafana Alloy on all Linux distributions before uninstalling.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemctl stop alloy\n```\n\n----------------------------------------\n\nTITLE: Deploying Loki with Helm\nDESCRIPTION: Command to deploy Loki in the 'meta' namespace using the Grafana Helm chart with custom values.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nhelm install --values loki-values.yml loki grafana/loki -n meta\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom MSSQL Prometheus Metrics in YAML\nDESCRIPTION: This YAML configuration defines the default metrics collected by the MSSQL Prometheus exporter. It includes various metrics such as local time, connections, deadlocks, errors, and memory usage. The configuration specifies metric names, types, help text, and SQL queries to collect the data.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.mssql.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ncollector_name: mssql_standard\n\nmetrics:\n  - metric_name: mssql_local_time_seconds\n    type: gauge\n    help: 'Local time in seconds since epoch (Unix time).'\n    values: [unix_time]\n    query: |\n      SELECT DATEDIFF(second, '19700101', GETUTCDATE()) AS unix_time\n  - metric_name: mssql_connections\n    type: gauge\n    help: 'Number of active connections.'\n    key_labels:\n      - db\n    values: [count]\n    query: |\n      SELECT DB_NAME(sp.dbid) AS db, COUNT(sp.spid) AS count\n      FROM sys.sysprocesses sp\n      GROUP BY DB_NAME(sp.dbid)\n  #\n  # Collected from sys.dm_os_performance_counters\n  #\n  - metric_name: mssql_deadlocks_total\n    type: counter\n    help: 'Number of lock requests that resulted in a deadlock.'\n    values: [cntr_value]\n    query: |\n      SELECT cntr_value\n      FROM sys.dm_os_performance_counters WITH (NOLOCK)\n      WHERE counter_name = 'Number of Deadlocks/sec' AND instance_name = '_Total'\n  - metric_name: mssql_user_errors_total\n    type: counter\n    help: 'Number of user errors.'\n    values: [cntr_value]\n    query: |\n      SELECT cntr_value\n      FROM sys.dm_os_performance_counters WITH (NOLOCK)\n      WHERE counter_name = 'Errors/sec' AND instance_name = 'User Errors'\n  - metric_name: mssql_kill_connection_errors_total\n    type: counter\n    help: 'Number of severe errors that caused SQL Server to kill the connection.'\n    values: [cntr_value]\n    query: |\n      SELECT cntr_value\n      FROM sys.dm_os_performance_counters WITH (NOLOCK)\n      WHERE counter_name = 'Errors/sec' AND instance_name = 'Kill Connection Errors'\n  - metric_name: mssql_page_life_expectancy_seconds\n    type: gauge\n    help: 'The minimum number of seconds a page will stay in the buffer pool on this node without references.'\n    values: [cntr_value]\n    query: |\n      SELECT top(1) cntr_value\n      FROM sys.dm_os_performance_counters WITH (NOLOCK)\n      WHERE counter_name = 'Page life expectancy'\n  - metric_name: mssql_batch_requests_total\n    type: counter\n    help: 'Number of command batches received.'\n    values: [cntr_value]\n    query: |\n      SELECT cntr_value\n      FROM sys.dm_os_performance_counters WITH (NOLOCK)\n      WHERE counter_name = 'Batch Requests/sec'\n  - metric_name: mssql_log_growths_total\n    type: counter\n    help: 'Number of times the transaction log has been expanded, per database.'\n    key_labels:\n      - db\n    values: [cntr_value]\n    query: |\n      SELECT rtrim(instance_name) AS db, cntr_value\n      FROM sys.dm_os_performance_counters WITH (NOLOCK)\n      WHERE counter_name = 'Log Growths' AND instance_name <> '_Total'\n  - metric_name: mssql_buffer_cache_hit_ratio\n    type: gauge\n    help: 'Ratio of requests that hit the buffer cache'\n    values: [BufferCacheHitRatio]\n    query: |\n      SELECT (a.cntr_value * 1.0 / b.cntr_value) * 100.0 as BufferCacheHitRatio\n      FROM sys.dm_os_performance_counters  a\n      JOIN  (SELECT cntr_value, OBJECT_NAME\n          FROM sys.dm_os_performance_counters\n          WHERE counter_name = 'Buffer cache hit ratio base'\n              AND OBJECT_NAME = 'SQLServer:Buffer Manager') b ON  a.OBJECT_NAME = b.OBJECT_NAME\n      WHERE a.counter_name = 'Buffer cache hit ratio'\n      AND a.OBJECT_NAME = 'SQLServer:Buffer Manager'\n\n  - metric_name: mssql_checkpoint_pages_sec\n    type: gauge\n    help: 'Checkpoint Pages Per Second'\n    values: [cntr_value]\n    query: |\n      SELECT cntr_value\n      FROM sys.dm_os_performance_counters\n      WHERE [counter_name] = 'Checkpoint pages/sec'\n  #\n  # Collected from sys.dm_io_virtual_file_stats\n  #\n  - metric_name: mssql_io_stall_seconds_total\n    type: counter\n    help: 'Stall time in seconds per database and I/O operation.'\n    key_labels:\n      - db\n    value_label: operation\n    values:\n      - read\n      - write\n    query_ref: mssql_io_stall\n\n  #\n  # Collected from sys.dm_os_process_memory\n  #\n  - metric_name: mssql_resident_memory_bytes\n    type: gauge\n    help: 'SQL Server resident memory size (AKA working set).'\n    values: [resident_memory_bytes]\n    query_ref: mssql_process_memory\n\n  - metric_name: mssql_virtual_memory_bytes\n    type: gauge\n    help: 'SQL Server committed virtual memory size.'\n    values: [virtual_memory_bytes]\n    query_ref: mssql_process_memory\n\n  - metric_name: mssql_available_commit_memory_bytes\n    type: gauge\n    help: 'SQL Server available to be committed memory size.'\n    values: [available_commit_limit_bytes]\n    query_ref: mssql_process_memory\n\n  - metric_name: mssql_memory_utilization_percentage\n    type: gauge\n    help: 'The percentage of committed memory that is in the working set.'\n    values: [memory_utilization_percentage]\n    query_ref: mssql_process_memory\n\n  - metric_name: mssql_page_fault_count_total\n    type: counter\n    help: 'The number of page faults that were incurred by the SQL Server process.'\n    values: [page_fault_count]\n    query_ref: mssql_process_memory\n\n  #\n  # Collected from sys.dm_os_sys_info\n  #\n  - metric_name: mssql_server_total_memory_bytes\n    type: gauge\n    help: 'SQL Server committed memory in the memory manager.'\n    values: [committed_memory_bytes]\n    query_ref: mssql_os_sys_info\n\n  - metric_name: mssql_server_target_memory_bytes\n    type: gauge\n    help: 'SQL Server target committed memory set for the memory manager.'\n    values: [committed_memory_target_bytes]\n    query_ref: mssql_os_sys_info\n\n  #\n  # Collected from sys.dm_os_sys_memory\n  #\n  - metric_name: mssql_os_memory\n    type: gauge\n    help: 'OS physical memory, used and available.'\n    value_label: 'state'\n    values: [used, available]\n    query: |\n      SELECT\n        (total_physical_memory_kb - available_physical_memory_kb) * 1024 AS used,\n        available_physical_memory_kb * 1024 AS available\n      FROM sys.dm_os_sys_memory\n  - metric_name: mssql_os_page_file\n    type: gauge\n    help: 'OS page file, used and available.'\n    value_label: 'state'\n    values: [used, available]\n    query: |\n      SELECT\n        (total_page_file_kb - available_page_file_kb) * 1024 AS used,\n        available_page_file_kb * 1024 AS available\n      FROM sys.dm_os_sys_memory\nqueries:\n  # Populates `mssql_io_stall` and `mssql_io_stall_total`\n  - query_name: mssql_io_stall\n    query: |\n      SELECT\n        cast(DB_Name(a.database_id) as varchar) AS [db],\n        sum(io_stall_read_ms) / 1000.0 AS [read],\n        sum(io_stall_write_ms) / 1000.0 AS [write]\n      FROM\n        sys.dm_io_virtual_file_stats(null, null) a\n      INNER JOIN sys.master_files b ON a.database_id = b.database_id AND a.file_id = b.file_id\n      GROUP BY a.database_id\n  # Populates `mssql_resident_memory_bytes`, `mssql_virtual_memory_bytes`, mssql_available_commit_memory_bytes,\n  # and `mssql_memory_utilization_percentage`, and `mssql_page_fault_count_total`\n  - query_name: mssql_process_memory\n    query: |\n      SELECT\n        physical_memory_in_use_kb * 1024 AS resident_memory_bytes,\n        virtual_address_space_committed_kb * 1024 AS virtual_memory_bytes,\n        available_commit_limit_kb * 1024 AS available_commit_limit_bytes,\n        memory_utilization_percentage,\n        page_fault_count\n      FROM sys.dm_os_process_memory\n  # Populates `mssql_server_total_memory_bytes` and `mssql_server_target_memory_bytes`.\n  - query_name: mssql_os_sys_info\n    query: |\n      SELECT\n        committed_kb * 1024 AS committed_memory_bytes,\n        committed_target_kb * 1024 AS committed_memory_target_bytes\n      FROM sys.dm_os_sys_info\n```\n\n----------------------------------------\n\nTITLE: Updating Repositories for RHEL/Fedora\nDESCRIPTION: Command to update package repositories on RHEL/Fedora systems before installing Grafana Alloy.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nyum update\n```\n\n----------------------------------------\n\nTITLE: Updating go.mod Replace Directives for OpenTelemetry Modules\nDESCRIPTION: Shows how to update the replace directives in go.mod to point to the latest commit of the forked release branch of the OpenTelemetry Collector.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/updating-otel/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngo mod edit -replace=go.opentelemetry.io/collector=github.com/grafana/opentelemetry-collector@asdf123jkl\n```\n\n----------------------------------------\n\nTITLE: Listing OpenTelemetry Dependencies in Alloy\nDESCRIPTION: Shows the key OpenTelemetry module dependencies used in the Alloy project, including exporters, extensions, and core OpenTelemetry packages.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/updating-otel/README.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/jaegerexporter\ngithub.com/open-telemetry/opentelemetry-collector-contrib/extension/sigv4authextension\ngo.opentelemetry.io/collector\ngo.opentelemetry.io/collector/component\ngo.opentelemetry.io/otel\ngo.opentelemetry.io/otel/metric\ngo.opentelemetry.io/otel/sdk\n```\n\n----------------------------------------\n\nTITLE: Enabling Grafana Alloy Auto-start\nDESCRIPTION: Command to configure Grafana Alloy service to automatically start on system boot\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/linux.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemctl enable alloy.service\n```\n\n----------------------------------------\n\nTITLE: Upgrading Helm Installation with New Configuration\nDESCRIPTION: This Bash command upgrades an existing Helm installation to use the updated values.yaml file with clustering enabled.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/clustering/_index.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade <RELEASE_NAME> -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Reloading Systemd Service Files\nDESCRIPTION: This command reloads the systemd service files after making changes to the service configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/binary.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemctl daemon-reload\n```\n\n----------------------------------------\n\nTITLE: Replacing Substrings with string.replace in Alloy\nDESCRIPTION: The string.replace function searches a string for a substring and replaces each occurrence with a specified replacement string.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/string.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nstring.replace(string, substring, replacement)\n```\n\nLANGUAGE: alloy\nCODE:\n```\n> string.replace(\"1 + 2 + 3\", \"+\", \"-\")\n\"1 - 2 - 3\"\n```\n\n----------------------------------------\n\nTITLE: Azure Cloud Environment Options\nDESCRIPTION: List of supported Azure cloud environment values that can be used in the configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/shared/reference/components/azuread-block.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n* `\"AzurePublic\"`\n* `\"AzureChina\"`\n* `\"AzureGovernment\"`\n```\n\n----------------------------------------\n\nTITLE: Defining a foreach Block in Alloy\nDESCRIPTION: Basic structure of a foreach block in Alloy, showing the required arguments and template block.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/foreach.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nforeach \"<LABEL>\" {\n  collection = [...]\n  var        = \"<VAR_NAME>\"\n  template {\n    ...\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying Grafana Tempo\nDESCRIPTION: Command to deploy Grafana Tempo in the 'prod' namespace using the Grafana Helm chart.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nhelm install tempo grafana/tempo-distributed -n prod\n```\n\n----------------------------------------\n\nTITLE: Creating Grafana Alloy Release Branch\nDESCRIPTION: Instructions for creating and pushing a release branch for a new version of Grafana Alloy. The branch name should follow the format 'release/VERSION_PREFIX', such as 'release/v0.31'.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/release/01-create-release-branch.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nrelease/VERSION_PREFIX\n```\n\nLANGUAGE: bash\nCODE:\n```\nrelease/v0.31\n```\n\nLANGUAGE: bash\nCODE:\n```\nrelease/v0.31-rc.0\n```\n\nLANGUAGE: bash\nCODE:\n```\nrelease/v0.31.0\n```\n\n----------------------------------------\n\nTITLE: Reloading Alloy Configuration with curl\nDESCRIPTION: This command uses the Alloy API to reload the configuration without requiring a system service restart. It allows configuration changes to take effect immediately.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/send-logs-to-loki.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:12345/-/reload\n```\n\n----------------------------------------\n\nTITLE: Trimming Whitespace with string.trim_space in Alloy\nDESCRIPTION: The string.trim_space function removes any whitespace characters from the start and end of a string.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/string.md#2025-04-22_snippet_9\n\nLANGUAGE: alloy\nCODE:\n```\n> string.trim_space(\"  hello\\n\\n\")\n\"hello\"\n```\n\n----------------------------------------\n\nTITLE: Converting to Lowercase with string.to_lower in Alloy\nDESCRIPTION: The string.to_lower function converts all uppercase letters in a string to lowercase.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/string.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\n> string.to_lower(\"HELLO\")\n\"hello\"\n```\n\n----------------------------------------\n\nTITLE: Reloading Grafana Alloy Configuration on Linux\nDESCRIPTION: This command reloads the Grafana Alloy configuration file after making changes. It uses systemctl to reload the alloy service.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/linux.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemctl reload alloy\n```\n\n----------------------------------------\n\nTITLE: Converting Promtail Configuration with Error Bypass\nDESCRIPTION: Command for converting a Promtail configuration to Grafana Alloy with the --bypass-errors flag to allow non-critical issues to be ignored during conversion.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-promtail.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=promtail --bypass-errors --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Grafana Alloy from macOS via Homebrew\nDESCRIPTION: This command completely removes Grafana Alloy from a macOS system. It uninstalls the package and removes all associated files managed by Homebrew.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/macos.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nbrew uninstall grafana/grafana/alloy\n```\n\n----------------------------------------\n\nTITLE: Configuring Regular Expression for Netstat Fields in Grafana Alloy\nDESCRIPTION: Default regular expression string for the 'fields' parameter in the netstat collector. It specifies which network statistics fields to include in the collection.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.unix.md#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n\"^(.*_(InErrors|InErrs)|Ip_Forwarding|Ip(6|Ext)_(InOctets|OutOctets)|Icmp6?_(InMsgs|OutMsgs)|TcpExt_(Listen.*|Syncookies.*|TCPSynRetrans|TCPTimeouts)|Tcp_(ActiveOpens|InSegs|OutSegs|OutRsts|PassiveOpens|RetransSegs|CurrEstab)|Udp6?_(InDatagrams|OutDatagrams|NoPorts|RcvbufErrors|SndbufErrors))$\"\n```\n\n----------------------------------------\n\nTITLE: Data Flow Pipeline for Other Grafana Platforms\nDESCRIPTION: A plaintext diagram showing how data flows through the OpenTelemetry pipeline to different Grafana components: metrics to Mimir/Prometheus, logs to Loki, and traces to the OTLP exporter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-to-lgtm-stack.md#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nMetrics: OTel → batch processor → Mimir or Prometheus remote write\nLogs: OTel → batch processor → Loki exporter\nTraces: OTel → batch processor → OTel exporter\n```\n\n----------------------------------------\n\nTITLE: Tidying Go Module Dependencies\nDESCRIPTION: Command for tidying up the go.mod and go.sum files after adding or updating dependencies. This should be done before committing changes.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/contributing.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngo mod tidy\n```\n\n----------------------------------------\n\nTITLE: Importing OpenTelemetry Component into Alloy\nDESCRIPTION: Adds the new OpenTelemetry processor component to the Alloy components list by importing it in the all.go file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/add-otel-component.md#2025-04-22_snippet_3\n\nLANGUAGE: go\nCODE:\n```\n_ \"github.com/grafana/alloy/internal/component/otelcol/processor/example\"                 // Import otelcol.processor.example\n```\n\n----------------------------------------\n\nTITLE: Starting Grafana Alloy Service\nDESCRIPTION: Command to start the Grafana Alloy service using systemd\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/linux.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemctl start alloy\n```\n\n----------------------------------------\n\nTITLE: Restarting Grafana Alloy Service on Linux\nDESCRIPTION: This command restarts the Grafana Alloy service after making changes to the configuration or environment files. It uses systemctl to restart the alloy service.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/linux.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemctl restart alloy\n```\n\n----------------------------------------\n\nTITLE: Creating ConfigMap for Grafana Alloy Configuration\nDESCRIPTION: Kubernetes command to create a ConfigMap named 'alloy-config' from a local configuration file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/kubernetes.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create configmap --namespace <NAMESPACE> alloy-config \"--from-file=config.alloy=./config.alloy\"\n```\n\n----------------------------------------\n\nTITLE: Example Diagnostic Report Output\nDESCRIPTION: Sample output from a diagnostic report when converting from Grafana Agent Static to Grafana Alloy, warning the user to review command line flags that might need configuration in Alloy.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-static.md#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n(Warning) Please review your agent command line flags and ensure they are set in your {{< param \"PRODUCT_NAME\" >}} configuration file where necessary.\n```\n\n----------------------------------------\n\nTITLE: Invalid Cyclic Reference Example in Alloy\nDESCRIPTION: This code snippet shows an invalid configuration with a cyclic dependency between two components, where component 'a' references 'b' and 'b' references 'a', which is not allowed in the component graph.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/component_controller.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\n// INVALID: cyclic reference between local.file.a and local.file.b:\nlocal.file \"a\" {\n  filename = local.file.b.content\n}\nlocal.file \"b\" {\n  filename = local.file.a.content\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging in Grafana Alloy\nDESCRIPTION: Basic logging configuration example showing how to set the log level and format. This snippet demonstrates setting the log level to 'info' and using the 'logfmt' format for log output.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/config-blocks/logging.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nlogging {\n  level  = \"info\"\n  format = \"logfmt\"\n}\n```\n\n----------------------------------------\n\nTITLE: Restarting Alloy Service on macOS using Homebrew\nDESCRIPTION: Command to restart the Grafana Alloy service after configuration changes using Homebrew services.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/macos.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbrew services restart alloy\n```\n\n----------------------------------------\n\nTITLE: Build Image Version Tag Format\nDESCRIPTION: Example of the Git tag format used to trigger new build image creation in CI.\nSOURCE: https://github.com/grafana/alloy/blob/main/tools/build-image/README.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nbuild-image/v0.1.0\n```\n\n----------------------------------------\n\nTITLE: Port-forwarding Alloy Pod\nDESCRIPTION: Command to set up port-forwarding for the Alloy pod to access the Alloy UI locally on port 12345.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nkubectl --namespace meta port-forward $POD_NAME 12345\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Document\nDESCRIPTION: A structured changelog document detailing version history, breaking changes, features, enhancements, and bugfixes for Grafana Alloy\nSOURCE: https://github.com/grafana/alloy/blob/main/CHANGELOG.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Changelog\n\n> _Contributors should read our [contributors guide][] for instructions on how\n> to update the changelog._\n\nThis document contains a historical list of changes between releases. Only\nchanges that impact end-user behavior are listed; changes to documentation or\ninternal API changes are not present.\n\nMain (unreleased)\n-----------------\n\n### Features\n\n- Add the `otelcol.storage.file` extension to support persistent sending queues and `otelcol.receiver.filelog` file state tracking between restarts. (@dehaansa)\n\n- Add `otelcol.exporter.googlecloud` community component to export metrics, traces, and logs to Google Cloud. (@motoki317)\n\n- Add support to configure basic authentication for alloy http server. (@kalleep)\n\n### Enhancements\n\n- Add binary version to constants exposed in configuration file syntatx. (@adlots)\n\n- Update `loki.secretfilter` to include metrics about redactions (@kelnage)\n\n- (_Experimental_) Various changes to the experimental component `database_observability.mysql`:\n  - `schema_table`: add support for index expressions (@cristiangreco)\n  - `query_sample`: enable opt-in support to extract unredacted sql query (sql_text) (@matthewnolf)\n  - `query_tables`: improve queries parsing (@cristiangreco)\n  - `query_tables`: add support for prepared statements (@cristiangreco)\n  - make tidbparser the default choice (@cristiangreco)\n\n- Mixin dashboards improvements: added minimum cluster size to Cluster Overview dashboard, fixed units in OpenTelemetry dashboard, fixed slow components evaluation time units in Controller dashboard and updated Prometheus dashboard to correctly aggregate across instances. (@thampiotr)\n\n- Reduced the lag time during targets handover in a cluster in `prometheus.scrape` components by reducing thread contention. (@thampiotr)\n\n- Pretty print diagnostic errors when using `alloy run` (@kalleep)\n\n### Bugfixes\n\n- Fix `otelcol.exporter.prometheus` dropping valid exemplars. (@github-vincent-miszczak)\n\n- Fix `loki.source.podlogs` add missing labels `__meta_kubernetes_namespace` and `__meta_kubernetes_pod_label_*`. (@kalleep)\n\n- Fix `otelcol.receiver.filelog` documentation's default value for `start_at`. (@petewall)\n\n### Other changes\n\n- Update the zap logging adapter used by `otelcol` components to log arrays and objects. (@dehaansa)\n\n- Updated Windows install script to add DisplayVersion into registry on install (@enessene)\n\nv1.8.1\n-----------------\n\n### Bugfixes\n\n- `rfc3164_default_to_current_year` argument was not fully added to `loki.source.syslog` (@dehaansa)\n\n- Fix issue with `remoteCfg` service stopping immediately and logging noop error if not configured (@dehaansa)\n\n- Fix potential race condition in `remoteCfg` service metrics registration (@kalleep)\n\n- Fix panic in `prometheus.exporter.postgres` when using minimal url as data source name. (@kalleep)\n\nv1.8.0\n-----------------\n\n### Breaking changes\n\n- Removed `open_port` and `executable_name` from top level configuration of Beyla component. Removed `enabled` argument from `network` block. (@marctc)\n\n- Breaking changes from the OpenTelemetry Collector v0.122 update: (@wildum)\n  - `otelcol.exporter.splunkhec`: `min_size_items` and `max_size_items` were replaced by `min_size`, `max_size` and `sizer` in the `batcher` block to allow\n  users to configure the size of the batch in a more flexible way.\n  - The telemetry level of Otel components is no longer configurable. The `level` argument in the `debug_metrics` block is kept to avoid breaking changes but it is not used anymore.\n  - `otelcol.processor.tailsampling` changed the unit of the decision timer metric from microseconds to milliseconds. (change unit of otelcol_processor_tail_sampling_sampling_decision_timer_latency)\n  - `otelcol.processor.deltatocumulative`: rename `otelcol_deltatocumulative_datapoints_processed` to `otelcol_deltatocumulative_datapoints` and remove the metrics `otelcol_deltatocumulative_streams_evicted`, `otelcol_deltatocumulative_datapoints_dropped` and `otelcol_deltatocumulative_gaps_length`.\n  - The `regex` attribute was removed from `otelcol.processor.k8sattributes`. The extract-patterns function from `otelcol.processor.transform` can be used instead.\n  - The default value of `metrics_flush_interval` in `otelcol.connector.servicegraph` was changed from `0s` to `60s`.\n  - `s3_partition` in `otelcol.exporter.awss3` was replaced by `s3_partition_format`.\n\n- (_Experimental_) `prometheus.write.queue` metric names changed to align better with prometheus standards. (@mattdurham)\n\n### Features\n\n- Add `otelcol.receiver.awscloudwatch` component to receive logs from AWS CloudWatch and forward them to other `otelcol.*` components. (@wildum)\n- Add `loki.enrich` component to enrich logs using labels from `discovery.*` components. (@v-zhuravlev)\n- Add string concatenation for secrets type (@ravishankar15)\n- Add support for environment variables to OpenTelemetry Collector config. (@jharvey10)\n- Replace graph in Alloy UI with a new version that supports modules and data flow visualization. (@wildum)\n- Added `--cluster.wait-for-size` and `--cluster.wait-timeout` flags which allow to specify the minimum cluster size\n  required before components that use clustering begin processing traffic to ensure adequate cluster capacity is\n  available. (@thampiotr)\n- Add `trace_printer` to `beyla.ebpf` component to print trace information in a specific format. (@marctc)\n- Add support for live debugging and graph in the UI for components imported via remotecfg. (@wildum)\n```\n\n----------------------------------------\n\nTITLE: Deploying Grafana with Helm\nDESCRIPTION: Command to deploy Grafana in the 'meta' namespace using the Grafana Helm chart with custom values.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nhelm install --values grafana-values.yml grafana grafana/grafana --namespace meta\n```\n\n----------------------------------------\n\nTITLE: Example Output of Grafana Alloy Service Status\nDESCRIPTION: This shell output shows what a successful Grafana Alloy service status should look like, including active state, process information, and resource usage.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/ansible.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nalloy.service - Grafana Alloy\n  Loaded: loaded (/etc/systemd/system/alloy.service; enabled; vendor preset: enabled)\n  Active: active (running) since Wed 2022-07-20 09:56:15 UTC; 36s ago\nMain PID: 3176 (alloy-linux-amd)\n  Tasks: 8 (limit: 515)\n  Memory: 92.5M\n    CPU: 380ms\n  CGroup: /system.slice/alloy.service\n    └─3176 /usr/local/bin/alloy-linux-amd64 --config.file=/etc/grafana-cloud/alloy-config.yaml\n```\n\n----------------------------------------\n\nTITLE: Accessing System Information Constants in Alloy\nDESCRIPTION: Examples demonstrating how to access system information constants in Alloy, including hostname, operating system, and architecture. These constants provide information about the environment where Grafana Alloy is running.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/constants.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\n> constants.hostname\n\"my-hostname\"\n\n> constants.os\n\"linux\"\n\n> constants.arch\n\"amd64\"\n```\n\n----------------------------------------\n\nTITLE: Mount Points Exclusion Patterns by OS\nDESCRIPTION: Regular expression patterns for excluding mount points based on different operating systems\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.unix.md#2025-04-22_snippet_2\n\nLANGUAGE: linux\nCODE:\n```\n^/(dev|proc|run/credentials/.+|sys|var/lib/docker/.+)($|/)\n```\n\nLANGUAGE: osx\nCODE:\n```\n^/(dev)($|/)\n```\n\nLANGUAGE: bsd\nCODE:\n```\n^/(dev)($|/)\n```\n\n----------------------------------------\n\nTITLE: Specifying Required IAM Permissions for CloudWatch Exporter\nDESCRIPTION: This text block lists the core IAM permissions required for the CloudWatch exporter to function, including access to tags, CloudWatch metrics, and listing metrics.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.cloudwatch.md#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n\"tag:GetResources\",\n\"cloudwatch:GetMetricData\",\n\"cloudwatch:GetMetricStatistics\",\n\"cloudwatch:ListMetrics\"\n```\n\n----------------------------------------\n\nTITLE: Comparing Number Types in Alloy Syntax\nDESCRIPTION: Examples of number comparisons in Alloy syntax, demonstrating how integers, floating-point values, and scientific notation are treated as a single 'number' type.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/expressions/types_and_values.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\n3    == 3.00     // true\n5.0  == (10 / 2) // true\n1e+2 == 100      // true\n2e-3 == 0.002    // true\n```\n\n----------------------------------------\n\nTITLE: Creating Grafana Alloy Configuration File\nDESCRIPTION: Alloy configuration file content for setting logging level and format. This is used when creating a separate ConfigMap.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/kubernetes.md#2025-04-22_snippet_3\n\nLANGUAGE: alloy\nCODE:\n```\n// Write your Alloy config here:\nlogging {\n  level = \"info\"\n  format = \"logfmt\"\n}\n```\n\n----------------------------------------\n\nTITLE: Text File Collector Configuration\nDESCRIPTION: Configuration for ingesting metrics from .prom files in a specified directory. Files must end with a line feed to be properly recognized.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.windows.md#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n| Name                  | Type     | Description                                        | Default       | Required |\n| --------------------- | -------- | -------------------------------------------------- | ------------- | -------- |\n| `text_file_directory` | `string` | The directory containing the files to be ingested. | __see below__ | no       |\n```\n\n----------------------------------------\n\nTITLE: Sample Windows Event Log Entry in JSON Format\nDESCRIPTION: An example of a Windows security event log entry in JSON format that would be processed by the Loki pipeline. This shows the raw input before any processing stages are applied.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_51\n\nLANGUAGE: text\nCODE:\n```\n{\"event_id\": 1, \"Overwritten\": \"old\", \"message\": \"\"Special privileges assigned to new logon.\\r\\n\\r\\nSubject:\\r\\n\\tSecurity ID:\\t\\tS-1-1-1\\r\\n\\tAccount Name:\\t\\tSYSTEM\\r\\n\\tAccount Domain:\\t\\tNT AUTHORITY\\r\\n\\tLogon ID:\\t\\t0xAAA\\r\\n\\r\\nPrivileges:\\t\\tSeAssignPrimaryTokenPrivilege\\r\\n\\t\\t\\tSeTcbPrivilege\\r\\n\\t\\t\\tSeSecurityPrivilege\\r\\n\\t\\t\\tSeTakeOwnershipPrivilege\\r\\n\\t\\t\\tSeLoadDriverPrivilege\\r\\n\\t\\t\\tSeBackupPrivilege\\r\\n\\t\\t\\tSeRestorePrivilege\\r\\n\\t\\t\\tSeDebugPrivilege\\r\\n\\t\\t\\tSeAuditPrivilege\\r\\n\\t\\t\\tSeSystemEnvironmentPrivilege\\r\\n\\t\\t\\tSeImpersonatePrivilege\\r\\n\\t\\t\\tSeDelegateSessionUserImpersonatePrivilege\"\"}\n```\n\n----------------------------------------\n\nTITLE: Removing Grafana Repository on SUSE/openSUSE\nDESCRIPTION: Optional command to remove the Grafana repository from SUSE/openSUSE systems after uninstalling.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nsudo zypper removerepo grafana\n```\n\n----------------------------------------\n\nTITLE: Configuring Git for GPG Signature Verification\nDESCRIPTION: Commands to configure Git to always sign commits and tags with GPG. This is recommended for ensuring the authenticity of releases.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/release/03-tag-release.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit config --global commit.gpgSign true\ngit config --global tag.gpgSign true\n```\n\n----------------------------------------\n\nTITLE: Verifying Docker Container Status\nDESCRIPTION: Check the status of the deployed Docker containers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-windows.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker ps\n```\n\n----------------------------------------\n\nTITLE: Updating Grafana Helm Chart Repository\nDESCRIPTION: This command updates the local Helm chart repository to ensure you have the latest version of the Grafana charts.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/kubernetes.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Cloning Alloy Scenarios Repository\nDESCRIPTION: Command to clone the Grafana Alloy scenarios repository containing example deployments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-logs-over-tcp.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/grafana/alloy-scenarios.git\n```\n\n----------------------------------------\n\nTITLE: Sample Diagnostic Report from Conversion\nDESCRIPTION: This is an example of the diagnostic report generated when converting the OpenTelemetry Collector configuration. It shows which components were successfully converted.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-otelcol.md#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\n(Info) Converted receiver/otlp into otelcol.receiver.otlp.default\n(Info) Converted processor/memory_limiter into otelcol.processor.memory_limiter.default\n(Info) Converted exporter/otlp into otelcol.exporter.otlp.default\n\nA configuration file was generated successfully.\n```\n\n----------------------------------------\n\nTITLE: Installing Grafana Alloy on Debian/Ubuntu\nDESCRIPTION: Command to install Grafana Alloy on Debian/Ubuntu systems using apt-get.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get install alloy\n```\n\n----------------------------------------\n\nTITLE: Obtaining Memory Profiles in Grafana Alloy using HTTP Requests\nDESCRIPTION: Commands to collect heap and goroutine profiles from Grafana Alloy via HTTP requests. These profiles are useful for diagnosing memory leaks, especially when comparing profiles taken during low and high memory usage.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/troubleshoot/profile.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl localhost:12345/debug/pprof/heap -o heap.pprof\ncurl localhost:12345/debug/pprof/goroutine -o goroutine.pprof\n```\n\n----------------------------------------\n\nTITLE: Adding the Grafana Helm Repository\nDESCRIPTION: Command to add the Grafana Helm repository to use for installing Loki, Grafana, and Alloy.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nhelm repo add grafana https://grafana.github.io/helm-charts\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Network Matching in Prometheus Discovery\nDESCRIPTION: New 'match_first_network' attribute for discovery.docker to match the first network if a container has multiple networks defined, avoiding duplicate targets.\nSOURCE: https://github.com/grafana/alloy/blob/main/CHANGELOG.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndiscovery.docker:\n  match_first_network: true\n```\n\n----------------------------------------\n\nTITLE: Getting Grafana Pod Name\nDESCRIPTION: Command to get the name of the Grafana pod deployed in the 'meta' namespace and store it in an environment variable.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nexport POD_NAME=$(kubectl get pods --namespace meta -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\" -o jsonpath=\"{.items[0].metadata.name}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Diagnostic Report During Prometheus Conversion\nDESCRIPTION: Command to convert a Prometheus configuration while generating a diagnostic report using the --report flag. This provides detailed information about the conversion process for troubleshooting.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-prometheus.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=prometheus --report=<OUTPUT_REPORT_PATH> --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Trimming String Content with Template Functions\nDESCRIPTION: Demonstrates string trimming operations by removing specified characters, whitespace, or prefixes from extracted values.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_42\n\nLANGUAGE: alloy\nCODE:\n```\nstage.template {\n    source   = \"output\"\n    template = `{{ Trim .Value \",. \" }}`\n}\nstage.template {\n    source   = \"output\"\n    template = \"{{ TrimSpace .Value }}\"\n}\nstage.template {\n    source   = \"output\"\n    template = `{{ TrimPrefix .Value \"--\" }}`\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Output Block Logs Configuration Parameters in Grafana Alloy\nDESCRIPTION: Documentation of the output block configuration for logs forwarding in Grafana Alloy. The block supports the logs argument which accepts a list of otelcol.Consumer components to receive the log data. By default, the logs list is empty and telemetry data is dropped if not configured.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/shared/reference/components/output-block-logs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nName   | Type                     | Description                        | Default | Required\n-------|--------------------------|------------------------------------|---------|---------\n`logs` | `list(otelcol.Consumer)` | List of consumers to send logs to. | `[]`    | no\n```\n\n----------------------------------------\n\nTITLE: Adding Grafana Helm Repository and Updating\nDESCRIPTION: Commands to add the Grafana chart repository to Helm and update the local repositories cache. This is a prerequisite before installing any Grafana Helm charts.\nSOURCE: https://github.com/grafana/alloy/blob/main/operations/helm/charts/alloy/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Multiline Array Construction in Alloy Syntax\nDESCRIPTION: Example of constructing an array with values on separate lines for readability. Note the comma after the final value when the closing bracket is on a different line.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/expressions/types_and_values.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\n[\n  0,\n  1,\n  2,\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Heroku Log Drain\nDESCRIPTION: Command to add a Heroku log drain pointing to your Loki instance using the Heroku CLI.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.heroku.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nheroku drains:add [http|https]://HOSTNAME:PORT/heroku/api/v1/drain -a HEROKU_APP_NAME\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Collector Transform Statements Configuration\nDESCRIPTION: New statements block addition in otelcol.processor.transform for transformations that don't require explicit context specification.\nSOURCE: https://github.com/grafana/alloy/blob/main/CHANGELOG.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\notelcol.processor.transform:\n  statements:\n    # Transformation statements here\n```\n\n----------------------------------------\n\nTITLE: Announcing Stable or Patch Release in Slack\nDESCRIPTION: This snippet provides a template for announcing a Stable or Patch Release of Grafana Alloy in the Grafana Labs Community #alloy Slack channel. It includes placeholders for the release version and links to the release and changelog.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/release/08-announce-release.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n:alloy: Grafana Alloy RELEASE_VERSION is now available! :alloy:\nRelease: https://github.com/grafana/alloy/releases/tag/RELEASE_VERSION\nFull changelog: https://github.com/grafana/alloy/blob/RELEASE_VERSION/CHANGELOG.md\n```\n\n----------------------------------------\n\nTITLE: Stopping Docker Stack\nDESCRIPTION: Stop the Docker stack when finished exploring the example.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-windows.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker compose down\n```\n\n----------------------------------------\n\nTITLE: Checking Out and Updating Release Branch in Git\nDESCRIPTION: Commands for checking out the release branch and ensuring it's up to date with the remote repository before tagging a release.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/release/03-tag-release.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout release-VERSION_PREFIX\ngit fetch origin \ngit pull origin \n```\n\n----------------------------------------\n\nTITLE: Removing Grafana Repository on Debian/Ubuntu\nDESCRIPTION: Optional command to remove the Grafana repository from Debian/Ubuntu systems after uninstalling.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nsudo rm -i /etc/apt/sources.list.d/grafana.list\n```\n\n----------------------------------------\n\nTITLE: RFC3339 Timestamp Parsing Configuration\nDESCRIPTION: Configures timestamp parsing for RFC3339 formatted time values from the extracted data map.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_48\n\nLANGUAGE: alloy\nCODE:\n```\nstage.timestamp {\n    source = \"time\"\n    format = \"RFC3339\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Grafana Alloy on SUSE/openSUSE\nDESCRIPTION: Command to install Grafana Alloy on SUSE/openSUSE systems using zypper.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nsudo zypper -r grafana install alloy\n```\n\n----------------------------------------\n\nTITLE: Compiling Grafana Alloy from Source\nDESCRIPTION: Command sequence for cloning the Grafana Alloy repository and building it using Make. Shows how to clone the repository, build the project, and run it with a configuration file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/contributing.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/grafana/alloy.git\n$ cd alloy\n$ make alloy\n$ ./build/alloy run <CONFIG_FILE>\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Compose for Local Grafana Stack\nDESCRIPTION: Bash command to start the Docker Compose services in detached mode, which launches the Grafana, Loki, and Prometheus containers in the background.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/send-logs-to-loki.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Basic Building and Testing Commands for Grafana Alloy\nDESCRIPTION: Commands for quickly building and testing changes in Grafana Alloy. Includes commands to build the project, run it with a configuration file, and run linting and tests.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/contributing.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# For building:\ngo build .\n./alloy run <CONFIG_FILE>\n\n# For testing:\nmake lint test # Make sure all the tests pass before you commit and push :)\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenTelemetry Component Interface Methods\nDESCRIPTION: Implements required interface methods for the OpenTelemetry processor including default settings, validation, conversion, and various helper methods for configuration handling.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/add-otel-component.md#2025-04-22_snippet_2\n\nLANGUAGE: go\nCODE:\n```\n// Compile time assertions\nvar (\n    _ processor.Arguments = Arguments{}\n    _ syntax.Defaulter = &Arguments{}\n    _ syntax.Validator = &Arguments{}\n)\n\n// DefaultArguments holds default settings for Arguments.\nvar DefaultArguments = Arguments{\n    AffectLogs: true,\n    AffectMetrics: true,\n    AffectTraces: true,\n}\n\n// SetToDefault implements syntax.Defaulter.\nfunc (args *Arguments) SetToDefault() {\n    *args = DefaultArguments\n    args.DebugMetrics.SetToDefault()\n}\n\n// Validate implements syntax.Validator.\nfunc (args *Arguments) Validate() error {\n    if args.Attribute == \"\" {\n        return fmt.Errorf(\"attribute must not be empty\")\n    }\n    return nil\n}\n\n// Convert implements processor.Arguments.\nfunc (args Arguments) Convert() (otelcomponent.Config, error) {\n    return &exampleprocessor.Config{\n        Attribute: args.Attribute,\n        Value: args.Value,\n        AffectLogs: args.AffectLogs,\n        AffectMetrics: args.AffectMetrics,\n        AffectTraces: args.AffectTraces,\n    }, nil\n}\n\n// Extensions implements processor.Arguments.\nfunc (args Arguments) Extensions() map[otelcomponent.ID]otelcomponent.Component {\n    return nil\n}\n\n// Exporters implements processor.Arguments.\nfunc (args Arguments) Exporters() map[pipeline.Signal]map[otelcomponent.ID]otelcomponent.Component {\n    return nil\n}\n\n// NextConsumers implements processor.Arguments.\nfunc (args Arguments) NextConsumers() *otelcol.ConsumerArguments {\n    return args.Output\n}\n\n// DebugMetricsConfig implements processor.Arguments.\nfunc (args Arguments) DebugMetricsConfig() otelcolCfg.DebugMetricsArguments {\n    return args.DebugMetrics\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning Alloy Scenarios Repository\nDESCRIPTION: Clone the Grafana Alloy scenarios repository containing example deployments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-windows.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/grafana/alloy-scenarios.git\n```\n\n----------------------------------------\n\nTITLE: Upgrading Grafana Alloy on macOS via Homebrew\nDESCRIPTION: This command upgrades an existing installation of Grafana Alloy on macOS to the latest version available through Homebrew.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/macos.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nbrew upgrade grafana/grafana/alloy\n```\n\n----------------------------------------\n\nTITLE: Defining Grafana Alloy Tutorials Page in Hugo Markdown\nDESCRIPTION: This snippet sets up the front matter for a Hugo-based documentation page for Grafana Alloy tutorials. It defines metadata such as the canonical URL, description, menu title, and page weight.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/_index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ncanonical: https://grafana.com/docs/alloy/latest/tutorials/\ndescription: Learn how to use Grafana Alloy\nmenuTitle: Tutorials\ntitle: Grafana Alloy tutorials\nweight: 200\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring Process Exporter in Grafana Alloy\nDESCRIPTION: An example configuration for the process exporter component showing how to use multiple matcher blocks to track different processes. The configuration demonstrates the block tag pattern for nested slice values.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/writing-exporter-components.md#2025-04-22_snippet_0\n\nLANGUAGE: grafana-alloy\nCODE:\n```\nprometheus.exporter.process \"example\" {\n  track_children = false\n  matcher {\n    comm = [\"alloy\"]\n  }\n  matcher {\n    comm = [\"firefox\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Conversion Command on Example Configuration\nDESCRIPTION: Command to convert the example Prometheus configuration to a Grafana Alloy configuration. This transforms the YAML format to Alloy's native configuration format.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-prometheus.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nalloy convert --source-format=prometheus --output=<OUTPUT_CONFIG_PATH> <INPUT_CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Cluster with kind\nDESCRIPTION: Command to create a local Kubernetes cluster using kind with the provided configuration file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkind create cluster --config kind.yml\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Compose Environment\nDESCRIPTION: Command to start the Docker Compose environment for Grafana Alloy and its components in detached mode with build option.\nSOURCE: https://github.com/grafana/alloy/blob/main/example/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up --build -d\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Metric Format in OpenMetrics\nDESCRIPTION: Illustrates the format of a Prometheus metric following the OpenMetrics standard, including metric name, labels, value, and optional timestamp.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.relabel.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n<metric name>{<label_1>=<label_val_1>, <label_2>=<label_val_2> ...} <value> [timestamp]\n```\n\n----------------------------------------\n\nTITLE: Configuring otelcol.exporter.debug in River\nDESCRIPTION: Basic usage of the otelcol.exporter.debug component in River configuration language. This snippet shows how to declare the component with a label.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.debug.md#2025-04-22_snippet_0\n\nLANGUAGE: river\nCODE:\n```\notelcol.exporter.debug \"LABEL\" { }\n```\n\n----------------------------------------\n\nTITLE: System Metrics Collection Reference Table\nDESCRIPTION: Markdown table listing various system metric collectors, their descriptions, supported operating systems, and default enablement status\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.unix.md#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n| `meminfo_numa`     | Exposes memory statistics from `/proc/meminfo_numa`.                                                                                                                            | Linux                                                              | no                 |\n| `meminfo`          | Exposes memory statistics.                                                                                                                                      | Darwin, Dragonfly, FreeBSD, Linux, OpenBSD, NetBSD                 | yes                |\n| `mountstats`       | Exposes filesystem statistics from `/proc/self/mountstats`. Exposes detailed NFS client statistics.                                                             | Linux                                                              | no                 |\n| `netclass`         | Exposes network interface info from `/sys/class/net`.                                                                                                          | Linux                                                              | yes                |\n```\n\n----------------------------------------\n\nTITLE: Adding Community Component Shortcode in Documentation\nDESCRIPTION: Markdown code for including the community component description shortcode below the heading 1.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/adding-community-components.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n{{< docs/shared lookup=\"stability/community.md\" source=\"alloy\" version=\"<ALLOY_VERSION>\" >}}\n```\n\n----------------------------------------\n\nTITLE: Executing Helm Chart Updates Command\nDESCRIPTION: Command to run from the operations/helm directory to update documentation and rebuild tests\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/release/06-update-helm-charts.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake docs rebuild-tests\n```\n\n----------------------------------------\n\nTITLE: Setting GPG_TTY Environment Variable for Mac/Linux\nDESCRIPTION: Command to set the GPG_TTY environment variable in shell profile files. This resolves issues with GPG passphrase prompts on macOS or Linux when using encrypted GPG keys.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/release/03-tag-release.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport GPG_TTY=$(tty)\n```\n\n----------------------------------------\n\nTITLE: Stopping Grafana Alloy Service\nDESCRIPTION: Command to stop the Grafana Alloy service\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/linux.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemctl stop alloy\n```\n\n----------------------------------------\n\nTITLE: SMB Server Configuration\nDESCRIPTION: Deprecated configuration for SMB server collectors, maintaining backwards compatibility with a no-op enabled_list parameter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.windows.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Name           | Type           | Description                                      | Default | Required |\n| -------------- | -------------- | ------------------------------------------------ | ------- | -------- |\n| `enabled_list` | `list(string)` | Deprecated (no-op), a list of collectors to use. | `[]`    | no       |\n```\n\n----------------------------------------\n\nTITLE: Updating Repositories for SUSE/openSUSE\nDESCRIPTION: Command to update package repositories on SUSE/openSUSE systems before installing Grafana Alloy.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nsudo zypper update\n```\n\n----------------------------------------\n\nTITLE: Specifying Compression Methods for OpenTelemetry Collector\nDESCRIPTION: This snippet lists the supported string values for the 'compression' argument in OpenTelemetry Collector configuration. It includes various compression algorithms and options for disabling compression.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/shared/reference/components/otelcol-compression-field.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n\"gzip\"\n\"zlib\"\n\"deflate\"\n\"snappy\"\n\"zstd\"\n\"none\"\n\"\"\n```\n\n----------------------------------------\n\nTITLE: Stopping Redis Container\nDESCRIPTION: This command stops the previously started Redis container.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/first-components-and-stdlib.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker container stop alloy-redis\n```\n\n----------------------------------------\n\nTITLE: Invalid Self-Reference Example in Alloy\nDESCRIPTION: This code snippet demonstrates an invalid configuration where a component attempts to reference itself, which violates the directed acyclic graph requirement of the component controller.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/component_controller.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\n// INVALID: local.file.some_file can't reference itself:\nlocal.file \"self_reference\" {\n  filename = local.file.self_reference.content\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Foreach Block in Alloy\nDESCRIPTION: Example of the proposed 'foreach' block syntax that would iterate over a collection and create sub-pipelines for each item. Demonstrates the structure with component scoping.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/design/1443-dynamic-pipelines.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\n// All components in the sub-pipeline will be scoped under \"foreach.default/1/...\".\n// Here, \"1\" is sub-pipeline number 1.\n// This way component names won't clash with other sub-pipelines from the same foreach, \n// and with the names of components outside of the foreach.\nforeach \"default\" {\n    \n  // \"collection\" is what the for loop will iterate over.\n  collection = discovery.file.default.targets\n\n  // Each item in the collection will be accessible via the \"target\" variable.\n  // E.g. `target[\"__address__\"]`.\n  var = \"target\"\n\n  // A sub-pipeline consisting of components which process each target.\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Grafana Alloy on Debian/Ubuntu\nDESCRIPTION: Command to uninstall Grafana Alloy on Debian/Ubuntu systems using apt-get.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get remove alloy\n```\n\n----------------------------------------\n\nTITLE: Configuring Server Log Level with Environment Variable in YAML\nDESCRIPTION: This YAML configuration snippet sets the server log level using an environment variable placeholder. The '${TEST}' syntax allows for dynamic configuration based on the 'TEST' environment variable.\nSOURCE: https://github.com/grafana/alloy/blob/main/internal/static/config/encoder/test_encoding_utf8.txt#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nserver:\n  log_level: ${TEST}\n```\n\n----------------------------------------\n\nTITLE: Enabling Community Components Configuration Flag\nDESCRIPTION: CLI flag configuration required to enable community-maintained components in Grafana Alloy. This setting must be explicitly enabled to use community components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/shared/stability/community.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n--feature.community-components.enabled\n```\n\n----------------------------------------\n\nTITLE: Sample Promtail Configuration\nDESCRIPTION: Example Promtail configuration file that defines a client endpoint and scrape config for log files in /var/log/.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-promtail.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nclients:\n  - url: http://localhost/loki/api/v1/push\nscrape_configs:\n  - job_name: example\n    static_configs:\n      - targets:\n          - localhost\n        labels:\n          __path__: /var/log/*.log\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Attributes in Alloy\nDESCRIPTION: Example of a basic attribute key-value pair used to set a URL in Grafana Alloy configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/first-components-and-stdlib.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nurl = \"http://localhost:9090\"\n```\n\n----------------------------------------\n\nTITLE: Deploying Docker Stack\nDESCRIPTION: Commands to navigate to the syslog directory and start the Docker containers for the Grafana stack.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-syslog-messages.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd alloy-scenarios/syslog\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Alloy Pipeline Execution Output\nDESCRIPTION: Terminal output showing the successful startup and initialization of the OpenTelemetry pipeline components, including WAL replay and server startup messages.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-to-lgtm-stack.md#2025-04-22_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\n./alloy run alloy-config.alloy\n./alloy run alloy-config.alloy\nts=2023-05-09T09:37:15.300959Z level=info msg=\"running usage stats reporter\"\nts=2023-05-09T09:37:15.300958Z level=info msg=\"now listening for http traffic\" addr=127.0.0.1:12345\nts=2023-05-09T09:37:15.301104Z level=info trace_id=6466516c9e1a556422df7a84c0ade6b0 msg=\"starting complete graph evaluation\"\nts=2023-05-09T09:37:15.301307Z level=info trace_id=6466516c9e1a556422df7a84c0ade6b0 msg=\"finished node evaluation\" node_id=loki.write.grafana_cloud_logs duration=188.209µs\nts=2023-05-09T09:37:15.301334Z level=info trace_id=6466516c9e1a556422df7a84c0ade6b0 msg=\"finished node evaluation\" node_id=otelcol.exporter.loki.grafana_cloud_logs duration=18.791µs\nts=2023-05-09T09:37:15.303138Z component=prometheus.remote_write.grafana_cloud_metrics level=info subcomponent=wal msg=\"replaying WAL, this may take a while\" dir=data-alloy/prometheus.remote_write.grafana_cloud_metrics/wal\nts=2023-05-09T09:37:15.303257Z component=prometheus.remote_write.grafana_cloud_metrics level=info subcomponent=wal msg=\"WAL segment loaded\" segment=0 maxSegment=1\nts=2023-05-09T09:37:15.303302Z component=prometheus.remote_write.grafana_cloud_metrics level=info subcomponent=wal msg=\"WAL segment loaded\" segment=1 maxSegment=1\nts=2023-05-09T09:37:15.303507Z component=prometheus.remote_write.grafana_cloud_metrics subcomponent=rw level=info remote_name=7f623a url=https://prometheus-us-central1.grafana.net/api/prom/push msg=\"Starting WAL watcher\" queue=7f623a\nts=2023-05-09T09:37:15.303515Z component=prometheus.remote_write.grafana_cloud_metrics subcomponent=rw level=info remote_name=7f623a url=https://prometheus-us-central1.grafana.net/api/prom/push msg=\"Starting scraped metadata watcher\"\nts=2023-05-09T09:37:15.303522Z level=info trace_id=6466516c9e1a556422df7a84c0ade6b0 msg=\"finished node evaluation\" node_id=prometheus.remote_write.grafana_cloud_metrics duration=2.181958ms\nts=2023-05-09T09:37:15.303557Z level=info trace_id=6466516c9e1a556422df7a84c0ade6b0 msg=\"finished node evaluation\" node_id=otelcol.exporter.prometheus.grafana_cloud_metrics duration=30.083µs\nts=2023-05-09T09:37:15.303611Z component=prometheus.remote_write.grafana_cloud_metrics subcomponent=rw level=info remote_name=7f623a url=https://prometheus-us-central1.grafana.net/api/prom/push msg=\"Replaying WAL\" queue=7f623a\nts=2023-05-09T09:37:15.303618Z level=info trace_id=6466516c9e1a556422df7a84c0ade6b0 msg=\"finished node evaluation\" node_id=otelcol.auth.basic.grafana_cloud_traces duration=52.5µs\nts=2023-05-09T09:37:15.303694Z level=info trace_id=6466516c9e1a556422df7a84c0ade6b0 msg=\"finished node evaluation\" node_id=otelcol.exporter.otlp.grafana_cloud_traces duration=70.375µs\nts=2023-05-09T09:37:15.303782Z component=otelcol.processor.memory_limiter.default level=info msg=\"Memory limiter configured\" limit_mib=150 spike_limit_mib=30 check_interval=1s\nts=2023-05-09T09:37:15.303802Z level=info trace_id=6466516c9e1a556422df7a84c0ade6b0 msg=\"finished node evaluation\" node_id=otelcol.processor.memory_limiter.default duration=100.334µs\nts=2023-05-09T09:37:15.303853Z level=info trace_id=6466516c9e1a556422df7a84c0ade6b0 msg=\"finished node evaluation\" node_id=otelcol.processor.batch.default duration=44.75µs\nts=2023-05-09T09:37:15.303948Z level=info trace_id=6466516c9e1a556422df7a84c0ade6b0 msg=\"finished node evaluation\" node_id=otelcol.receiver.otlp.default duration=87.333µs\nts=2023-05-09T09:37:15.303968Z level=info trace_id=6466516c9e1a556422df7a84c0ade6b0 msg=\"finished node evaluation\" node_id=tracing duration=10.792µs\nts=2023-05-09T09:37:15.303981Z level=info trace_id=6466516c9e1a556422df7a84c0ade6b0 msg=\"finished node evaluation\" node_id=logging duration=9µs\nts=2023-05-09T09:37:15.303987Z level=info trace_id=6466516c9e1a556422df7a84c0ade6b0 msg=\"finished complete graph evaluation\" duration=2.960333ms\nts=2023-05-09T09:37:15.304Z level=info msg=\"scheduling loaded components\"\nts=2023-05-09T09:37:15.304109Z component=otelcol.receiver.otlp.default level=info msg=\"Starting GRPC server\" endpoint=0.0.0.0:4317\nts=2023-05-09T09:37:15.304234Z component=otelcol.receiver.otlp.default level=info msg=\"Starting HTTP server\" endpoint=0.0.0.0:4318\n```\n\n----------------------------------------\n\nTITLE: Upgrading Grafana Alloy Helm Chart\nDESCRIPTION: Command to upgrade the Grafana Alloy installation using Helm. It requires specifying the namespace, release name, and path to the values.yaml file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/kubernetes.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nhelm upgrade --namespace <NAMESPACE> <RELEASE_NAME> grafana/alloy -f <VALUES_PATH>\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Compose with Alloy Profile\nDESCRIPTION: Command to start the Docker Compose environment with the Alloy profile enabled, which includes Alloy services alongside other Grafana components.\nSOURCE: https://github.com/grafana/alloy/blob/main/example/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose --profile=alloy up --build -d\n```\n\n----------------------------------------\n\nTITLE: Running Grafana Alloy with Experimental Components\nDESCRIPTION: Command to run Grafana Alloy with the experimental Datadog receiver. The stability.level flag is required as the otelcol.receiver.datadog component is currently experimental.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/datadog-traces-metrics.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nalloy run config.alloy --stability.level=experimental\n```\n\n----------------------------------------\n\nTITLE: Using prometheus.remote_write target-stats Command in Grafana Alloy CLI\nDESCRIPTION: This command reads a Prometheus Write-Ahead Log (WAL) directory and collects metric cardinality information for a specific target. It requires both job and instance flags to identify the target.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/cli/tools.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nalloy tools prometheus.remote_write target-stats --job JOB --instance INSTANCE WAL_DIRECTORY\n```\n\n----------------------------------------\n\nTITLE: Installing Grafana Alloy on RHEL/Fedora\nDESCRIPTION: Command to install Grafana Alloy on RHEL/Fedora systems using dnf.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nsudo dnf install alloy\n```\n\n----------------------------------------\n\nTITLE: Stopping the Docker Compose Environment\nDESCRIPTION: Optional command to shut down the Grafana stack when finished with the example.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-docker-containers.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker compose down\n```\n\n----------------------------------------\n\nTITLE: Setting Server Log Level in YAML Configuration\nDESCRIPTION: This YAML snippet configures the server's log level. It uses a variable or placeholder ${TEST} to set the log level, allowing for dynamic configuration based on the environment or other factors.\nSOURCE: https://github.com/grafana/alloy/blob/main/internal/static/config/encoder/test_encoding_utf16le.txt#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nserver:\n  log_level: ${TEST}\n```\n\n----------------------------------------\n\nTITLE: Using Poll Detector Reference\nDESCRIPTION: Reference for the poll detector which performs periodic file checks based on a configured poll_frequency setting.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/shared/reference/components/local-file-arguments-text.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\npoll_frequency\n```\n\n----------------------------------------\n\nTITLE: Importing GPG Key for SUSE/openSUSE\nDESCRIPTION: Commands to import the GPG key and add the Grafana package repository for SUSE and openSUSE systems.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nwget -q -O gpg.key https://rpm.grafana.com/gpg.key\nsudo rpm --import gpg.key\nsudo zypper addrepo https://rpm.grafana.com grafana\n```\n\n----------------------------------------\n\nTITLE: Stopping Docker Compose with Alloy Profile\nDESCRIPTION: Command to stop and remove all containers in the Docker Compose environment, including Alloy services.\nSOURCE: https://github.com/grafana/alloy/blob/main/example/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose --profile=alloy down\n```\n\n----------------------------------------\n\nTITLE: Structuring Grafana Alloy Tutorials Content with Hugo Shortcodes\nDESCRIPTION: This snippet demonstrates the use of Hugo shortcodes to create dynamic content for the Grafana Alloy tutorials page. It includes a parameter for the full product name and a section shortcode.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/tutorials/_index.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n# {{% param \"FULL_PRODUCT_NAME\" %}} tutorials\n\nThis section provides a set of step-by-step tutorials that show how to use {{< param \"PRODUCT_NAME\" >}}.\n\n{{< section >}}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Version Formats in Markdown\nDESCRIPTION: This snippet shows examples of different version formats used in Grafana Alloy, including stable releases, patch releases, and release candidates. It also demonstrates how to create a version prefix.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/release/concepts/version.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- Examples\n  - For example, `v0.31.0` is the Stable Release `VERSION` for the v0.31.0 release.\n  - For example, `v0.31.1` is the first Patch Release `VERSION` for the v0.31.0 release.\n  - For example, `v0.31.0-rc.0` is the first Release Candidate `VERSION` for the v0.31.0 release.\n\n## Version Prefix\n\nTo determine the `VERSION PREFIX`, use only the major and minor version `vX.Y`.\n\n- Examples\n  - `v0.31`\n```\n\n----------------------------------------\n\nTITLE: PromQL Query for Computing Recommended max_shards Value\nDESCRIPTION: This PromQL query calculates the recommended max_shards value for each remote write endpoint by taking the 90th percentile of desired shards over a seven-day period, multiplying by 4 to add room for spikes, and ensuring the value is at least 50.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.remote_write.md#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nclamp_min(\n    (\n        # Calculate the 90th percentile desired shards over the last seven-day period.\n        # If you're running {{< param \"PRODUCT_NAME\" >}} for less than seven days, then\n        # reduce the [7d] period to cover only the time range since when you deployed it.\n        ceil(quantile_over_time(0.9, prometheus_remote_storage_shards_desired[7d]))\n\n        # Add room for spikes.\n        * 4\n    ),\n    # We recommend setting max_shards to a value of no less than 50, as in the default configuration.\n    50\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Helm Tests Command\nDESCRIPTION: This command generates Helm tests by iterating through value.yaml files in the charts/alloy/ci directory and creating separate directories under charts/alloy/tests.\nSOURCE: https://github.com/grafana/alloy/blob/main/operations/helm/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmake generate-helm-tests\n```\n\n----------------------------------------\n\nTITLE: Removing Grafana Repository on RHEL/Fedora\nDESCRIPTION: Optional command to remove the Grafana repository from RHEL/Fedora systems after uninstalling.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nsudo rm -i /etc/yum.repos.d/rpm.grafana.repo\n```\n\n----------------------------------------\n\nTITLE: Diagnostic Report Example from Promtail Conversion\nDESCRIPTION: Sample diagnostic report content showing warnings about tracing setup and metrics differences when converting from Promtail to Grafana Alloy.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-promtail.md#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n(Warning) If you have a tracing set up for Promtail, it cannot be migrated to {{< param \"PRODUCT_NAME\" >}} automatically. Refer to the documentation on how to configure tracing in {{< param \"PRODUCT_NAME\" >}}.\n(Warning) The metrics from {{< param \"PRODUCT_NAME\" >}} are different from the metrics emitted by Promtail. If you rely on Promtail's metrics, you must change your configuration, for example, your alerts and dashboards.\n```\n\n----------------------------------------\n\nTITLE: Component Reference in Markdown\nDESCRIPTION: References the prometheus.exporter.redis component and its compatibility with Target consumers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.redis.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\nprometheus.exporter.redis\n```\n\n----------------------------------------\n\nTITLE: Cloning Grafana Alloy Scenarios Repository\nDESCRIPTION: Command to clone the Grafana Alloy scenarios repository which contains complete examples of Alloy deployments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-logs-from-file.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/grafana/alloy-scenarios.git\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Kafka Error Backoff Configuration\nDESCRIPTION: New error_backoff block in otelcol.receiver.kafka to configure retry behavior for failed requests.\nSOURCE: https://github.com/grafana/alloy/blob/main/CHANGELOG.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\notelcol.receiver.kafka:\n  error_backoff:\n    # Retry configuration here\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Grafana Alloy on RHEL/Fedora\nDESCRIPTION: Command to uninstall Grafana Alloy on RHEL/Fedora systems using dnf.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nsudo dnf remove alloy\n```\n\n----------------------------------------\n\nTITLE: Setting OTEL_EXPORTER_ENDPOINT for Linux Testing\nDESCRIPTION: Environment variable setup required for running tests on Linux systems. This sets the OpenTelemetry exporter endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/internal/cmd/integration-tests/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nOTEL_EXPORTER_ENDPOINT=172.17.0.1:4318\n```\n\n----------------------------------------\n\nTITLE: Cloning the Alloy Scenarios Repository\nDESCRIPTION: Command to clone the Grafana Alloy scenarios repository which contains complete examples of deployments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-docker-containers.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/grafana/alloy-scenarios.git\n```\n\n----------------------------------------\n\nTITLE: Redirecting Grafana Alloy Logs on Windows\nDESCRIPTION: This command starts Grafana Alloy on Windows and redirects both stdout and stderr to a specified output file. Replace <BINARY_PATH>, <CONFIG_PATH>, and <OUTPUT_FILE> with appropriate values.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/binary.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n<BINARY_PATH> run <CONFIG_PATH> 1> <OUTPUT_FILE> 2>&1\n```\n\n----------------------------------------\n\nTITLE: Modifying Log Lines with Current Entry Reference\nDESCRIPTION: Uses the special Entry reference to prepend application context to the current log line and sets it as the output message.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.process.md#2025-04-22_snippet_39\n\nLANGUAGE: alloy\nCODE:\n```\nstage.template {\n    source   = \"message\"\n    template = \"{{.app }}: {{ .Entry }}\"\n}\nstage.output {\n    source = \"message\"\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Healthy Status Endpoint in Grafana Alloy\nDESCRIPTION: Examples showing how to query the /-/healthy endpoint. The first example shows a successful health check response, while the second shows an unhealthy component response.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/http/_index.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ curl localhost:12345/-/healthy\nAll Alloy components are healthy.\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ curl localhost:12345/-/healthy\nunhealthy components: math.add\n```\n\n----------------------------------------\n\nTITLE: Setting up Custom Environment Variable for Kubernetes Node Detection in YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure a custom environment variable 'my_custom_var' in a Kubernetes workload to be used with the custom node_from_env_var setting in the resource detector.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.resourcedetection.md#2025-04-22_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\n        env:\n          - name: my_custom_var\n            valueFrom:\n              fieldRef:\n                fieldPath: spec.nodeName\n```\n\n----------------------------------------\n\nTITLE: Configuring loki.source.kubernetes in Alloy\nDESCRIPTION: Basic configuration for the loki.source.kubernetes component to tail logs from Kubernetes containers. Specify a label, targets, and forward_to receivers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/loki/loki.source.kubernetes.md#2025-04-22_snippet_0\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.kubernetes \"<LABEL>\" {\n  targets    = <TARGET_LIST>\n  forward_to = <RECEIVER_LIST>\n}\n```\n\n----------------------------------------\n\nTITLE: OTLP Exporter with Default Bearer Authentication\nDESCRIPTION: Example demonstrating how to configure OTLP exporter with bearer token authentication using environment variable as token source. Uses default Bearer scheme.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.auth.bearer.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.exporter.otlp \"example\" {\n  client {\n    endpoint = \"my-otlp-grpc-server:4317\"\n    auth     = otelcol.auth.bearer.creds.handler\n  }\n}\n\notelcol.auth.bearer \"creds\" {\n  token = sys.env(\"API_KEY\")\n}\n```\n\n----------------------------------------\n\nTITLE: Stopping Docker Compose Environment\nDESCRIPTION: Command to stop and remove all containers in the Docker Compose environment.\nSOURCE: https://github.com/grafana/alloy/blob/main/example/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose down\n```\n\n----------------------------------------\n\nTITLE: Feature List in Markdown\nDESCRIPTION: List of main features included in Alloy v1.0.0, formatted as markdown bullet points\nSOURCE: https://github.com/grafana/alloy/blob/main/CHANGELOG.md#2025-04-22_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\n### Features\n\n- Support for programmable pipelines using a rich expression-based syntax.\n\n- Over 130 components for processing, transforming, and exporting telemetry\n  data.\n\n- Native support for Kubernetes and Prometheus Operator without needing to\n  deploy or learn a separate Kubernetes operator.\n\n- Support for creating and sharing custom components.\n\n- Support for forming a cluster of Alloy instances for automatic workload\n  distribution.\n\n- (_Public preview_) Support for receiving configuration from a server for\n  centralized configuration management.\n\n- A built-in UI for visualizing and debugging pipelines.\n```\n\n----------------------------------------\n\nTITLE: Design Document Template in Markdown\nDESCRIPTION: Template for creating design documents for Alloy proposals, including sections for abstract, problem statement, proposal details, pros/cons analysis, alternative solutions, compatibility considerations, implementation plan, and related issues.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/design/908-proposal-process.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Proposal: [Title]\n\n* Author(s): [List of proposal authors]\n* Last updated: [Date]\n* Original issue: https://github.com/grafana/alloy/issues/NNNN\n\n## Abstract\n\n[A short summary of the proposal.]\n\n## Problem\n\n[A description of the necessary context and the problem being solved by the proposed change. The problem statement should explain why a solution is necessary.]\n\n## Proposal\n\n[A description of the proposed change with a level of detail sufficient to evaluate the tradeoffs and alternatives.]\n\n## Pros and cons\n\n[A list of trade offs, advantages, and disadvantages introduced by the proposed solution.]\n\n## Alternative solutions\n\n[If applicable, a discussion of alternative approches. Alternative approaches should include pros and cons, and a rationale for why the alternative approach was not selected.]\n\n## Compatibility\n\n[A discussion of the change with regard to backwards compatibility.]\n\n## Implementation\n\n[A description of the steps in the implementation, who will do them, and when.]\n\n## Related open issues\n\n[If applicable, a discussion of issues relating to this proposal for which the author does not know the solution. This section may be omitted if there are none.]\n```\n\n----------------------------------------\n\nTITLE: Stopping the Grafana Stack\nDESCRIPTION: Optional command to shut down the Grafana stack when finished with the example.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-structured-logs.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker compose down\n```\n\n----------------------------------------\n\nTITLE: Generating Versioned Files in Grafana Alloy (Bash)\nDESCRIPTION: After updating the VERSION file, this command generates the versioned files for the Grafana Alloy project. It's used in both the 'release/' and 'main' branch update processes.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/release/02-update-version-in-code.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake generate-versioned-files\n```\n\n----------------------------------------\n\nTITLE: Stopping Docker Containers\nDESCRIPTION: Command to stop and remove the Docker containers when finished with the example.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-logs-over-tcp.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker compose down\n```\n\n----------------------------------------\n\nTITLE: Markdown Proposal Template Structure\nDESCRIPTION: A standardized markdown template for project proposals that includes sections for proposal metadata, problem description, solution details, implementation plan, and related considerations.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/design/template.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Proposal: [Title]\n\n* Author(s): [List of proposal authors]\n* Last updated: [Date]\n* Original issue: https://github.com/grafana/alloy/issues/NNNN\n\n## Abstract\n\n[A short summary of the proposal.]\n\n## Problem\n\n[A description of the necessary context and the problem being solved by the proposed change. The problem statement should explain why a solution is necessary.]\n\n## Proposal\n\n[A description of the proposed change with a level of detail sufficient to evaluate the tradeoffs and alternatives.]\n\n## Pros and cons\n\n[A list of trade offs, advantages, and disadvantages introduced by the proposed solution.]\n\n## Alternative solutions\n\n[If applicable, a discussion of alternative approches. Alternative approaches should include pros and cons, and a rationale for why the alternative approach was not selected.]\n\n## Compatibility\n\n[A discussion of the change with regard to backwards compatibility.]\n\n## Implementation\n\n[A description of the steps in the implementation, who will do them, and when.]\n\n## Related open issues\n\n[If applicable, a discussion of issues relating to this proposal for which the author does not know the solution. This section may be omitted if there are none.]\n```\n\n----------------------------------------\n\nTITLE: Verifying Docker Container Status\nDESCRIPTION: Command to check the status of running Docker containers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-logs-over-tcp.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker ps\n```\n\n----------------------------------------\n\nTITLE: Starting Grafana Alloy on macOS using Homebrew\nDESCRIPTION: This command starts the Grafana Alloy service using Homebrew's services management. It initializes Alloy as a launchd service.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/macos.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbrew services start alloy\n```\n\n----------------------------------------\n\nTITLE: Verifying Grafana Alloy Pods Are Running\nDESCRIPTION: This command checks if the Grafana Alloy pods are running correctly in the specified namespace after installation.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/kubernetes.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods --namespace <NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Updating Repositories for Debian/Ubuntu\nDESCRIPTION: Command to update package repositories on Debian/Ubuntu systems before installing Grafana Alloy.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get update\n```\n\n----------------------------------------\n\nTITLE: Enabling Native Histogram Support in Prometheus Scraping\nDESCRIPTION: New 'scrape_native_histograms' argument for prometheus.scrape to explicitly control native histogram support. Enabled by default when scrape_protocols starts with PrometheusProto.\nSOURCE: https://github.com/grafana/alloy/blob/main/CHANGELOG.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nprometheus.scrape:\n  scrape_native_histograms: true\n```\n\n----------------------------------------\n\nTITLE: Executing Integration Tests in Go\nDESCRIPTION: Command to run integration tests for the Alloy project. This command executes all tests in the project directory.\nSOURCE: https://github.com/grafana/alloy/blob/main/internal/cmd/integration-tests/README.md#2025-04-22_snippet_0\n\nLANGUAGE: go\nCODE:\n```\ngo run .\n```\n\n----------------------------------------\n\nTITLE: Proposal Example in Markdown\nDESCRIPTION: Example of a minimal but sufficient proposal format for Grafana Alloy, demonstrating how to propose using the OpenTelemetry Collector batch processor component.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/design/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nI would like to use the batch processor component from OpenTelemetry\\nCollector in Grafana Alloy to reduce the number of outgoing network requests.\\nPlease add the batch processor as a new Alloy component called\\n`otelcol.processor.batch`.\n```\n\n----------------------------------------\n\nTITLE: Cloning the Alloy Scenarios Repository\nDESCRIPTION: Command to clone the Grafana Alloy scenarios repository which contains complete examples of Alloy deployments.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/grafana/alloy-scenarios.git\n```\n\n----------------------------------------\n\nTITLE: Generating Service List Column in Hugo for Grafana Alloy\nDESCRIPTION: This Hugo shortcode creates a column list layout for displaying a list of services. It uses the {{SERVICE_LIST}} placeholder to insert the actual service list content.\nSOURCE: https://github.com/grafana/alloy/blob/main/internal/static/integrations/cloudwatch_exporter/docs/template.md#2025-04-22_snippet_0\n\nLANGUAGE: hugo\nCODE:\n```\n{{< column-list >}}\n\n{{SERVICE_LIST}}\n\n{{< /column-list >}}\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Grafana Alloy on SUSE/openSUSE\nDESCRIPTION: Command to uninstall Grafana Alloy on SUSE/openSUSE systems using zypper.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nsudo zypper remove alloy\n```\n\n----------------------------------------\n\nTITLE: Experimental Prometheus Queue Parallelism Block Configuration\nDESCRIPTION: Breaking change in prometheus.write.queue where parallelism was changed from an attribute to a block to enable dynamic scaling.\nSOURCE: https://github.com/grafana/alloy/blob/main/CHANGELOG.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprometheus.write.queue:\n  parallelism:\n    # Configuration block for dynamic scaling\n```\n\n----------------------------------------\n\nTITLE: Defining Pyroscope Component Header in Markdown\nDESCRIPTION: Markdown frontmatter and heading definition for the pyroscope components documentation page in Grafana Alloy. Includes metadata like canonical URL, description, and weight for page ordering.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/_index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ncanonical: https://grafana.com/docs/alloy/latest/reference/components/pyroscope/\ndescription: Learn about the pyroscope components in Grafana Alloy\ntitle: pyroscope\nweight: 100\n---\n\n# `pyroscope`\n```\n\n----------------------------------------\n\nTITLE: Restarting Grafana Alloy on macOS using Homebrew\nDESCRIPTION: This command restarts the Grafana Alloy service using Homebrew's services management. It stops and then starts the Alloy service.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/macos.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nbrew services restart alloy\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana Alloy for Log Collection\nDESCRIPTION: YAML configuration for deploying Grafana Alloy as a DaemonSet for log collection. It mounts /var/log from the host for accessing log files.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/migrate/from-operator.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nalloy:\n  configMap:\n    create: true\n  clustering:\n    enabled: false\n  controller:\n    type: 'daemonset'\n  mounts:\n    varlog: true\n```\n\n----------------------------------------\n\nTITLE: Announcing Release Candidate Version (RCV) in Slack\nDESCRIPTION: This snippet provides a template for announcing a Release Candidate Version (RCV) of Grafana Alloy in the Grafana Labs Community #alloy Slack channel. It includes placeholders for the release version, links to the release and changelog, and information about the upcoming stable release.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/release/08-announce-release.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n:alloy: Grafana Alloy RELEASE_VERSION is now available! :alloy:\nRelease: https://github.com/grafana/alloy/releases/tag/RELEASE_VERSION\nFull changelog: https://github.com/grafana/alloy/blob/RELEASE_VERSION/CHANGELOG.md\nWe'll be publishing STABLE_RELEASE_VERSION on STABLE_RELEASE_DATE if we haven't heard about any major issues.\n```\n\n----------------------------------------\n\nTITLE: Verifying Grafana Alloy Service Status on macOS\nDESCRIPTION: This optional command checks the status of the Grafana Alloy service using Homebrew's services management. It provides information about whether the service is running.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/macos.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nbrew services info alloy\n```\n\n----------------------------------------\n\nTITLE: Configuring Web Crawler Access with robots.txt\nDESCRIPTION: A robots.txt file that specifies rules for web crawlers. This configuration allows all user agents unrestricted access to the entire website, as it includes no disallow directives.\nSOURCE: https://github.com/grafana/alloy/blob/main/internal/web/ui/public/robots.txt#2025-04-22_snippet_0\n\nLANGUAGE: robotstxt\nCODE:\n```\nUser-agent: *\nDisallow:\n```\n\n----------------------------------------\n\nTITLE: Adding Community Badge in Markdown Documentation\nDESCRIPTION: Markdown code for adding a community badge to component documentation below the frontmatter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/adding-community-components.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<span class=\"badge docs-labels__stage docs-labels__item\">Community</span>\n```\n\n----------------------------------------\n\nTITLE: Restarting Grafana Alloy Service\nDESCRIPTION: Command to restart the Grafana Alloy service\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/linux.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemctl restart alloy\n```\n\n----------------------------------------\n\nTITLE: Adding Grafana Homebrew Tap for macOS Installation\nDESCRIPTION: This command adds the Grafana Homebrew tap, which is necessary for installing Grafana Alloy on macOS. It allows access to Grafana-specific packages through Homebrew.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/macos.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbrew tap grafana/grafana\n```\n\n----------------------------------------\n\nTITLE: Verifying Docker Container Status\nDESCRIPTION: Command to check the status of the deployed Docker containers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-structured-logs.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker ps\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Grafana Alloy Documentation\nDESCRIPTION: YAML configuration block that defines metadata for the Grafana Alloy getting started documentation page, including canonical URL, aliases, description, title and weight for page ordering.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/_index.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncanonical: https://grafana.com/docs/alloy/latest/get-started/\naliases:\n  - ./concepts/ # /docs/alloy/latest/concepts/\ndescription: Get started with Grafana Alloy\ntitle: Get started\nweight: 40\n```\n\n----------------------------------------\n\nTITLE: Editing Alloy Homebrew Formula\nDESCRIPTION: Command to edit the Grafana Alloy Homebrew Formula to customize the service configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/configure/macos.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nbrew edit alloy\n```\n\n----------------------------------------\n\nTITLE: Stopping Grafana Alloy on macOS using Homebrew\nDESCRIPTION: This command stops the Grafana Alloy service using Homebrew's services management. It terminates the running Alloy service.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/run/macos.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nbrew services stop  alloy\n```\n\n----------------------------------------\n\nTITLE: Installing GPG on Debian-based Linux VMs\nDESCRIPTION: Command to install GPG on Debian-based cloud Virtual Machines that might not have it installed by default.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt install gpg\n```\n\n----------------------------------------\n\nTITLE: Deploying Grafana Stack with Docker Compose\nDESCRIPTION: Commands to navigate to the logs-tcp directory and start the Docker containers for the Grafana stack.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-logs-over-tcp.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd alloy-scenarios/logs-tcp\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Generating Updated CloudWatch Integration Documentation\nDESCRIPTION: Command to generate an updated list of supported services for the CloudWatch integration. The output should replace the existing list in the specified documentation file.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nmake generate-cloudwatch-integration\n```\n\n----------------------------------------\n\nTITLE: Previewing Grafana Alloy Documentation Website\nDESCRIPTION: Command to launch a local preview of the Grafana Alloy documentation website. This uses Docker to serve the content and automatically refreshes when changes are made to the 'sources' directory.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmake docs\n```\n\n----------------------------------------\n\nTITLE: Component Documentation Structure in Markdown\nDESCRIPTION: Structured documentation showing OpenTelemetry consumers and Pyroscope components organized by namespaces. The document uses Hugo shortcodes for collapsible sections and includes links to detailed component documentation.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/compatibility/_index.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n### OpenTelemetry `otelcol.Consumer` Consumers\n\nThe following components, grouped by namespace, _consume_ OpenTelemetry `otelcol.Consumer`.\n\n{{< collapse title=\"beyla\" >}}\n- [beyla.ebpf](../components/beyla/beyla.ebpf)\n{{< /collapse >}}\n\n{{< collapse title=\"faro\" >}}\n- [faro.receiver](../components/faro/faro.receiver)\n{{< /collapse >}}\n\n{{< collapse title=\"otelcol\" >}}\n- [otelcol.connector.host_info](../components/otelcol/otelcol.connector.host_info)\n- [otelcol.connector.servicegraph](../components/otelcol/otelcol.connector.servicegraph)\n[...additional components...]\n{{< /collapse >}}\n```\n\n----------------------------------------\n\nTITLE: Stopping Docker Containers\nDESCRIPTION: Command to shut down the Grafana stack when finished with the example.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-logs-from-file.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker compose down\n```\n\n----------------------------------------\n\nTITLE: Checking CloudWatch Integration Documentation\nDESCRIPTION: Command to verify if the CloudWatch integration documentation needs updating. This should be run from within the 'docs/' folder before making any changes to the CloudWatch docs.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmake check-cloudwatch-integration\n```\n\n----------------------------------------\n\nTITLE: Stopping Docker Stack\nDESCRIPTION: Command to stop and remove the Docker containers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-syslog-messages.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker compose down\n```\n\n----------------------------------------\n\nTITLE: Starting K6 Trace Generator with Docker Compose\nDESCRIPTION: Commands to start a K6 trace generator container that simulates an application instrumented for tracing, configured to send traces to host.docker.internal:4320.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/updating-otel/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd docs/developer/updating-otel/k6-trace-gen/\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: GitHub Repository Path Reference in Markdown\nDESCRIPTION: A reference to the original Jaeger repository path that was forked for this implementation, specifying the exact version (v1.38.1) and file path for the original strategy store implementation.\nSOURCE: https://github.com/grafana/alloy/blob/main/internal/component/otelcol/extension/jaeger_remote_sampling/internal/jaegerremotesampling/internal/source/strategy_store/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nhttps://github.com/jaegertracing/jaeger/blob/v1.38.1/plugin/sampling/strategystore/static/strategy_store.go\n```\n\n----------------------------------------\n\nTITLE: Importing GPG Key for RHEL/Fedora\nDESCRIPTION: Commands to import the GPG key and add the Grafana package repository for RHEL and Fedora systems.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/linux.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nwget -q -O gpg.key https://rpm.grafana.com/gpg.key\nsudo rpm --import gpg.key\necho -e '[grafana]\\nname=grafana\\nbaseurl=https://rpm.grafana.com\\nrepo_gpgcheck=1\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://rpm.grafana.com/gpg.key\\nsslverify=1\\nsslcacert=/etc/pki/tls/certs/ca-bundle.crt' | sudo tee /etc/yum.repos.d/grafana.repo\n```\n\n----------------------------------------\n\nTITLE: Simple Object Construction in Alloy Syntax\nDESCRIPTION: Example of constructing a simple object with a single key-value pair.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/get-started/configuration-syntax/expressions/types_and_values.md#2025-04-22_snippet_7\n\nLANGUAGE: alloy\nCODE:\n```\n{ name = \"John\" }\n```\n\n----------------------------------------\n\nTITLE: Installing and Enabling the Alloy Service with Chef\nDESCRIPTION: This Chef recipe installs the Alloy package and enables/starts the service. It flushes the package cache before installation on RHEL-based systems and automatically restarts the service after installation.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/chef.md#2025-04-22_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\npackage 'alloy' do\n  action :install\n  flush_cache [ :before ] if platform_family?('amazon', 'rhel', 'fedora')\n  notifies :restart, 'service[alloy]', :delayed\nend\n\nservice 'alloy' do\n  service_name 'alloy'\n  action [:enable, :start]\nend\n```\n\n----------------------------------------\n\nTITLE: Updating Generated Reference Documentation\nDESCRIPTION: Command to update automatically generated sections of the Grafana Alloy reference documentation. This should be run periodically to ensure the reference docs are up-to-date.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmake generate-docs\n```\n\n----------------------------------------\n\nTITLE: Restarting Grafana Alloy Service on macOS\nDESCRIPTION: This command restarts the Grafana Alloy service on macOS after an upgrade. It ensures that the new version is properly initialized and running.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/macos.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nbrew services restart alloy\n```\n\n----------------------------------------\n\nTITLE: Version Details in Markdown\nDESCRIPTION: Version marker and heading for Alloy v1.0.0 release notes\nSOURCE: https://github.com/grafana/alloy/blob/main/CHANGELOG.md#2025-04-22_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\nv1.0.0\n------\n```\n\n----------------------------------------\n\nTITLE: Including Grafana Alloy Class in Puppet Module\nDESCRIPTION: Simple Ruby snippet showing how to include the grafana_alloy class in a Puppet module's init.pp file to activate the Grafana Alloy installation functionality.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/set-up/install/puppet.md#2025-04-22_snippet_2\n\nLANGUAGE: ruby\nCODE:\n```\ninclude grafana_alloy::grafana_alloy\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Discovery for Same-Node Pods in Alloy\nDESCRIPTION: This example shows how to limit Kubernetes discovery to Pods on the same node as the Grafana Alloy instance. It's useful for DaemonSet deployments and includes Kubernetes discovery, Prometheus scraping, and remote write configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/discovery/discovery.kubernetes.md#2025-04-22_snippet_4\n\nLANGUAGE: alloy\nCODE:\n```\ndiscovery.kubernetes \"k8s_pods\" {\n  role = \"pod\"\n  selectors {\n    role = \"pod\"\n    field = \"spec.nodeName=\" + coalesce(sys.env(\"HOSTNAME\"), constants.hostname)\n  }\n}\n\nprometheus.scrape \"demo\" {\n  targets    = discovery.kubernetes.k8s_pods.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenShift Route for Exposing Alloy with gRPC Over HTTP/2\nDESCRIPTION: YAML configuration for an OpenShift Route that exposes Grafana Alloy's gRPC endpoint externally. This setup enables communication between external Alloy instances and the one deployed in OpenShift using the h2c protocol.\nSOURCE: https://github.com/grafana/alloy/blob/main/operations/helm/charts/alloy/README.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nkind: Route\napiVersion: route.openshift.io/v1\nmetadata:\n  name: route-otlp-alloy-h2c\nspec:\n  to:\n    kind: Service\n    name: test-grpc-h2c\n    weight: 100\n  port:\n    targetPort: otlp-grpc\n  tls:\n    termination: edge\n    insecureEdgeTerminationPolicy: Redirect\n  wildcardPolicy: None\n```\n\n----------------------------------------\n\nTITLE: Creating Prometheus Labels from OTLP Resource Attributes in Alloy\nDESCRIPTION: This example demonstrates how to use otelcol.processor.transform to convert specific OTLP resource attributes to metric datapoint attributes before using otelcol.exporter.prometheus.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.prometheus.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.receiver.otlp \"default\" {\n  grpc {}\n\n  output {\n    metrics = [otelcol.processor.transform.default.input]\n  }\n}\n\notelcol.processor.transform \"default\" {\n  error_mode = \"ignore\"\n\n  metric_statements {\n    context = \"datapoint\"\n\n    statements = [\n      `set(attributes[\"key1\"], resource.attributes[\"key1\"])`,\n      `set(attributes[\"key2\"], resource.attributes[\"key2\"])`,\n    ]\n  }\n\n  output {\n    metrics = [otelcol.exporter.prometheus.default.input]\n  }\n}\n\notelcol.exporter.prometheus \"default\" {\n  forward_to = [prometheus.remote_write.mimir.receiver]\n}\n\nprometheus.remote_write \"mimir\" {\n  endpoint {\n    url = \"http://mimir:9009/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation for Breaking Changes Guidelines\nDESCRIPTION: Structured documentation detailing the complete process of handling breaking changes in Grafana Alloy, including pre-requisites, considerations, decision-making framework, communication guidelines, and tracking mechanisms.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/breaking-changes.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Handling Breaking Changes\n\n## Before you begin\n\n* Make sure you've read [the backwards-compatibility document][backwards-compatibility-doc].\n* Write down in what way your change will break users. Think of cases where the\n  upgrade of Alloy to a new version would require a manual step beyond just\n  replacing the binary to keep all the functionality working.\n\n## Considerations\n\nUse the list of considerations below to generate ideas / approaches on how the\nchange could be handled:\n\n* **Can we make this change non-breaking in Alloy code?**\n\n  Sometimes it's possible to make the change non-breaking with some extra\n  effort. For example:\n\n    * If a metric that is used on our official dashboard was renamed, we can add\n      an OR to a query and the dashboard can support both: new and old metric.\n    * If a CLI flag or a config option was renamed, we can add an alias that\n      allows us to use both names.\n\n  **NOTE:** If you choose this option, we may still want to make the breaking\n  change in the next major\n  release. [Make sure that you correctly track it][tracking-breaking-changes].\n\n* **Is this change covered by our backwards-compatibility RFC guarantees?**\n    * **Is this change out-of-scope of our guarantees?**\n        * For example: metrics that are not used on our official dashboards.\n    * **Is this change called out as an exception?**\n        * For example: non-stable functionality or breaking changes in the\n          upstream.\n\n  **NOTE:** If the change is not a breaking change by our definition, but it\n  would still cause breakage and/or frustration with our users, we may still\n  communicate it to the users. See the [communicating the breaking changes section][communicating].\n\n* **Can this change wait for the next major release?**\n\n  Sometimes changes are urgent, e.g. security fixes or users blocked on them,\n  but sometimes they can wait for the next major release. Also, consider how far\n  in the future is the next major release.\n\n  **NOTE:** If you do pick this option, make sure you\n  correctly [track the work necessary for the next major release][tracking-breaking-changes].\n\n* **Can we fork / update an existing fork to make this change non-breaking?**\n\n  Sometimes it may be preferable to bring back the old behaviour or handle the\n  breaking change in a backwards-compatible way in the upstream library.\n  Consider this as an option.\n\n  **NOTE:** If you choose this option, we may still want to make the breaking\n  change in the next major\n  release. [Make sure that you correctly track it][tracking-breaking-changes].\n\n* **Should we first deprecate / give a warning and make the breaking change\n  later?**\n\n  It may be a good idea for certain breaking changes to give a heads-up / add a\n  warning in one release and make the breaking change later. Consider this as an\n  option.\n\n* **Do we have an idea how widely the impacted feature is used and how many\n  users will be impacted?**\n\n  Sometimes this can help decide which approach to take.\n\n* **Do we need to discuss this with maintainers / a wider community?**\n\n  If there is no exception, no way to make the change non-breaking and we cannot\n  wait for the next major release, we may need to discuss this further and\n  consider options not listed here.\n\n## Decide the best approach\n\nWe'd typically prefer options in the following order:\n\n1. Not make a breaking change at all - via code change in Alloy or in a fork or\n   upstream\n2. If not possible, we'd prefer to wait for the next major release\n3. If not possible, we'd consider using our backwards-compatibility scope\n   definition / exceptions / out-of-scope options\n4. If not possible, we'd likely need a wider discussion about this change and\n   communicate with users\n\n## Communicating the breaking changes\n\nCurrently, we use our CHANGELOG.md to communicate the presence of breaking\nchanges. They are also included on a GitHub release page under the notable\nchanges section. For changes that require manual steps to migrate, we must also\ninclude a migration guide.\n\n## Tracking work that needs to be done for the next major release\n\nIf we need to plan some work for the next major release, it's essential to track\nit correctly, so it's not lost, and we don't miss the rare opportunity to make\nsome breaking changes.\n\nWe currently use GitHub issues assigned to 2.0 milestone to track the issues\nplanned for the next release.\n\n[tracking-breaking-changes]: #tracking-work-that-needs-to-be-done-for-the-next-major-release\n[backwards-compatibility-doc]: https://grafana.com/docs/alloy/latest/introduction/backward-compatibility/\n[communicating]: #communicating-the-breaking-changes\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Operator Scrape Configuration Parameters in Markdown\nDESCRIPTION: This snippet defines a markdown table that lists and describes the configuration parameters for Prometheus Operator scraping. It includes the parameter name, data type, description, default value, and whether it's required.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/shared/reference/components/prom-operator-scrape.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nName                      | Type       | Description                                                                                                                  | Default | Required\n--------------------------|------------|------------------------------------------------------------------------------------------------------------------------------|---------|---------\n`default_scrape_interval` | `duration` | The default interval between scraping targets. Used as the default if the target resource doesn't provide a scrape interval. | `1m`    | no\n`default_scrape_timeout`  | `duration` | The default timeout for scrape requests. Used as the default if the target resource doesn't provide a scrape timeout.        | `10s`   | no\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Exporter and Prometheus Scrape in Alloy\nDESCRIPTION: This snippet demonstrates how to set up a Kafka exporter, configure Prometheus scraping, and send metrics to a remote write endpoint. It includes setting up the Kafka exporter, defining a scrape job, and configuring remote write with authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.kafka.md#2025-04-22_snippet_2\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.kafka \"example\" {\n  kafka_uris = [\"localhost:9200\"]\n}\n\n// Configure a prometheus.scrape component to send metrics to.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.kafka.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Docker Container Status\nDESCRIPTION: Command to verify the status of running Docker containers.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-syslog-messages.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker ps\n```\n\n----------------------------------------\n\nTITLE: Checking Docker Container Status\nDESCRIPTION: Command to verify the status of running Docker containers in the deployment.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-logs-from-file.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker ps\n```\n\n----------------------------------------\n\nTITLE: Port-forwarding Grafana Pod\nDESCRIPTION: Command to set up port-forwarding for the Grafana pod to access the Grafana UI locally on port 3000.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-kubernetes-logs.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nkubectl --namespace meta port-forward $POD_NAME 3000\n```\n\n----------------------------------------\n\nTITLE: Complete GitHub Exporter Setup with Remote Write\nDESCRIPTION: Comprehensive example demonstrating GitHub exporter configuration with Prometheus scraping and remote write setup. Includes API token configuration, repository targeting, and remote write endpoint configuration.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/prometheus/prometheus.exporter.github.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nprometheus.exporter.github \"example\" {\n  api_token_file = \"/etc/github-api-token\"\n  repositories   = [\"grafana/alloy\"]\n}\n\n// Configure a prometheus.scrape component to collect github metrics.\nprometheus.scrape \"demo\" {\n  targets    = prometheus.exporter.github.example.targets\n  forward_to = [prometheus.remote_write.demo.receiver]\n}\n\nprometheus.remote_write \"demo\" {\n  endpoint {\n    url = \"<PROMETHEUS_REMOTE_WRITE_URL>\"\n\n    basic_auth {\n      username = \"<USERNAME>\"\n      password = \"<PASSWORD>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Debug Files for Symbol Resolution in Bash\nDESCRIPTION: Commands to create separate debug files for stripped binaries to ensure symbols can be resolved properly. This process creates a debug file, strips the original binary, and links them together.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.ebpf.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nobjcopy --only-keep-debug elf elf.debug\nstrip elf -o elf.stripped\nobjcopy --add-gnu-debuglink=elf.debug elf.stripped elf.debuglink\n```\n\n----------------------------------------\n\nTITLE: Configuring Blackbox Exporter in Grafana Alloy\nDESCRIPTION: An example configuration for the blackbox exporter component showing how to use named blocks with the label tag. The configuration demonstrates setting a config file and defining a target with properties.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/writing-exporter-components.md#2025-04-22_snippet_1\n\nLANGUAGE: grafana-alloy\nCODE:\n```\nprometheus.exporter.blackbox \"example\" {\n    config_file = \"blackbox_modules.yml\"\n\n    target {\n        name    = \"example\"\n        address = \"http://example.com\"\n        module  = \"http_2xx\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Listing MIT-licensed Files in Grafana Alloy\nDESCRIPTION: Specifies the path to a file that is licensed under MIT rather than the default Apache-2.0 license. This indicates that the gitleaks.toml file in the secretfilter component has a different license than the rest of the project.\nSOURCE: https://github.com/grafana/alloy/blob/main/LICENSING.md#2025-04-22_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\ninternal/component/loki/secretfilter/gitleaks.toml\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Source API in Alloy\nDESCRIPTION: Alloy configuration block for the loki.source.api component, which receives log entries over HTTP and forwards them to other Loki components.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-logs-over-tcp.md#2025-04-22_snippet_5\n\nLANGUAGE: alloy\nCODE:\n```\nloki.source.api \"loki_push_api\" {\n    http {\n        listen_address = \"0.0.0.0\"\n        listen_port = 9999\n    }\n    forward_to = [\n        loki.process.labels.receiver,\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up Kubernetes Environment Variable for Node Detection in YAML\nDESCRIPTION: This YAML snippet shows how to add the required K8S_NODE_NAME environment variable to a Kubernetes workload for the node detector to properly identify the node name.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.resourcedetection.md#2025-04-22_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\n        env:\n          - name: K8S_NODE_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: spec.nodeName\n```\n\n----------------------------------------\n\nTITLE: Disabled Profiling Endpoints for Pyroscope\nDESCRIPTION: Lists the endpoints that will not be scraped due to explicit disabling in the configuration. Block and mutex profiling endpoints are excluded from collection.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/pyroscope/pyroscope.scrape.md#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nhttp://localhost:12345/debug/pprof/block\nhttp://localhost:12345/debug/pprof/mutex\n```\n\n----------------------------------------\n\nTITLE: Managing Go Dependencies in Grafana Alloy\nDESCRIPTION: Commands for adding or updating dependencies using Go modules. Shows how to select specific versions of dependencies and tidy up the go.mod and go.sum files.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/contributing.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Pick the latest tagged release.\ngo get example.com/some/module/pkg@latest\n\n# Pick a specific version.\ngo get example.com/some/module/pkg@vX.Y.Z\n```\n\n----------------------------------------\n\nTITLE: Example of Basic and Normal Verbosity Output\nDESCRIPTION: Shows the log output format for 'basic' and 'normal' verbosity levels in the debug exporter.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.exporter.debug.md#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nts=2024-06-13T11:24:13.782957Z level=info msg=TracesExporter component_path=/ component_id=otelcol.exporter.debug.default \"resource spans\": 1, spans: 2\n```\n\n----------------------------------------\n\nTITLE: Adding Basic Authentication to Loki Endpoint\nDESCRIPTION: Configuration block for adding basic authentication to a Loki endpoint. This is used when the Loki endpoint requires username and password authentication.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/logs-in-kubernetes.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\nbasic_auth {\n  username = \"<USERNAME>\"\n  password = \"<PASSWORD>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing System Dependencies for Linux Compilation\nDESCRIPTION: Commands for installing the required systemd headers on Debian-based Linux distributions. These headers are needed for compiling Loki components in Grafana Alloy.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/contributing.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install libsystemd-dev\n```\n\n----------------------------------------\n\nTITLE: Tagging and Pushing a Release\nDESCRIPTION: Commands for creating a signed Git tag for the release and pushing it to the remote repository. This triggers GitHub Tasks to create release assets.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/developer/release/03-tag-release.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit tag -s VERSION\ngit push origin VERSION\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Write in Alloy\nDESCRIPTION: Alloy configuration for the loki.write component that writes logs to a Loki destination endpoint.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/monitor/monitor-docker-containers.md#2025-04-22_snippet_10\n\nLANGUAGE: alloy\nCODE:\n```\nloki.write \"local\" {\n  endpoint {\n    url = \"http://loki:3100/loki/api/v1/push\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Authentication for OTLP Export\nDESCRIPTION: Configuration for basic authentication using otelcol.auth.basic component to secure OTLP data export.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/collect/opentelemetry-data.md#2025-04-22_snippet_1\n\nLANGUAGE: alloy\nCODE:\n```\notelcol.auth.basic \"<BASIC_AUTH_LABEL>\" {\n  username = \"<USERNAME>\"\n  password = \"<PASSWORD>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Rebuilding Tests Command\nDESCRIPTION: This command rebuilds the tests directory, which must be run before submitting a PR to ensure the directory is up-to-date.\nSOURCE: https://github.com/grafana/alloy/blob/main/operations/helm/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmake rebuild-tests\n```\n\n----------------------------------------\n\nTITLE: Setting Up MySQL Database Observability\nDESCRIPTION: Experimental component for collecting MySQL performance data, including health reporting.\nSOURCE: https://github.com/grafana/alloy/blob/main/CHANGELOG.md#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ndatabase_observability.mysql:\n  dsn: \"user:password@tcp(localhost:3306)/\"\n  collect_interval: 10s\n```\n\n----------------------------------------\n\nTITLE: Component Reference in Markdown\nDESCRIPTION: References to the OpenTelemetry tail sampling processor component name and its compatible interfaces.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/components/otelcol/otelcol.processor.tail_sampling.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\notelcol.processor.tail_sampling\n```\n\n----------------------------------------\n\nTITLE: Trimming Characters with string.trim in Alloy\nDESCRIPTION: The string.trim function removes specified characters from the start and end of a string.\nSOURCE: https://github.com/grafana/alloy/blob/main/docs/sources/reference/stdlib/string.md#2025-04-22_snippet_6\n\nLANGUAGE: alloy\nCODE:\n```\nstring.trim(string, str_character_set)\n```\n\nLANGUAGE: alloy\nCODE:\n```\n> string.trim(\"?!hello?!\", \"!?\")\n\"hello\"\n\n> string.trim(\"foobar\", \"far\")\n\"oob\"\n\n> string.trim(\"   hello! world.!  \", \"! \")\n\"hello! world.\"\n```"
  }
]
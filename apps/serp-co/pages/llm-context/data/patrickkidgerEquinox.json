[
  {
    "owner": "patrick-kidger",
    "repo": "equinox",
    "content": "TITLE: Equinox Automatic Differentiation Functions\nDESCRIPTION: Collection of automatic differentiation functions including gradients, Jacobians, Hessians, and custom derivatives implementations.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/transformations.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nequinox.filter_grad\nequinox.filter_value_and_grad\nequinox.filter_jvp\nequinox.filter_vjp\nequinox.filter_jacfwd\nequinox.filter_jacrev\nequinox.filter_hessian\n```\n\n----------------------------------------\n\nTITLE: Filtering with Partition and Combine\nDESCRIPTION: Demonstrates how to use partition and combine functions to separate a model into array and non-array components for JAX transformations. This approach explicitly separates parameters from static components before applying JAX operations.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/all-of-equinox.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@ft.partial(jax.jit, static_argnums=1)  # `static` must be a PyTree of non-arrays.\n@jax.grad  # differentiates with respect to `params`, as it is the first argument\ndef loss(params, static, x, y):\n    model = eqx.combine(params, static)\n    pred_y = jax.vmap(model)(x)\n    return jax.numpy.mean((y - pred_y) ** 2)\n\nparams, static = eqx.partition(model, eqx.is_array)\nloss(params, static, x, y)\n```\n\n----------------------------------------\n\nTITLE: Training Loop for Score-Based Diffusion Model\nDESCRIPTION: Implements the main training loop with optimization steps, model updates, and progress tracking. Includes hyperparameter settings for the model architecture, optimization process, and sampling.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/score_based_diffusion.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@eqx.filter_jit\ndef make_step(model, weight, int_beta, data, t1, key, opt_state, opt_update):\n    loss_fn = eqx.filter_value_and_grad(batch_loss_fn)\n    loss, grads = loss_fn(model, weight, int_beta, data, t1, key)\n    updates, opt_state = opt_update(grads, opt_state)\n    model = eqx.apply_updates(model, updates)\n    key = jr.split(key, 1)[0]\n    return loss, model, key, opt_state\n\n\ndef main(\n    # Model hyperparameters\n    patch_size=4,\n    hidden_size=64,\n    mix_patch_size=512,\n    mix_hidden_size=512,\n    num_blocks=4,\n    t1=10.0,\n    # Optimisation hyperparameters\n    num_steps=1_000_000,\n    lr=3e-4,\n    batch_size=256,\n    print_every=10_000,\n    # Sampling hyperparameters\n    dt0=0.1,\n    sample_size=10,\n    # Seed\n    seed=5678,\n):\n    key = jr.PRNGKey(seed)\n    model_key, train_key, loader_key, sample_key = jr.split(key, 4)\n    data = mnist()\n    data_mean = jnp.mean(data)\n    data_std = jnp.std(data)\n    data_max = jnp.max(data)\n    data_min = jnp.min(data)\n    data_shape = data.shape[1:]\n    data = (data - data_mean) / data_std\n\n    model = Mixer2d(\n        data_shape,\n        patch_size,\n        hidden_size,\n        mix_patch_size,\n        mix_hidden_size,\n        num_blocks,\n        t1,\n        key=model_key,\n    )\n    int_beta = lambda t: t  # Try experimenting with other options here!\n    weight = lambda t: 1 - jnp.exp(\n        -int_beta(t)\n    )  # Just chosen to upweight the region near t=0.\n\n    opt = optax.adabelief(lr)\n    # Optax will update the floating-point JAX arrays in the model.\n    opt_state = opt.init(eqx.filter(model, eqx.is_inexact_array))\n\n    total_value = 0\n    total_size = 0\n    for step, data in zip(\n        range(num_steps), dataloader(data, batch_size, key=loader_key)\n    ):\n        value, model, train_key, opt_state = make_step(\n            model, weight, int_beta, data, t1, train_key, opt_state, opt.update\n        )\n        total_value += value.item()\n        total_size += 1\n        if (step % print_every) == 0 or step == num_steps - 1:\n            print(f\"Step={step} Loss={total_value / total_size}\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Neural Network with Equinox Module\nDESCRIPTION: Demonstrates how to create a neural network class using Equinox Module that can be treated as a PyTree. The model includes linear layers with trainable parameters and shows how to apply JAX transformations for loss calculation and gradient descent.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/all-of-equinox.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport equinox as eqx\nimport jax\n\nclass NeuralNetwork(eqx.Module):\n    layers: list\n    extra_bias: jax.Array\n\n    def __init__(self, key):\n        key1, key2, key3 = jax.random.split(key, 3)\n        # These contain trainable parameters.\n        self.layers = [eqx.nn.Linear(2, 8, key=key1),\n                       eqx.nn.Linear(8, 8, key=key2),\n                       eqx.nn.Linear(8, 2, key=key3)]\n        # This is also a trainable parameter.\n        self.extra_bias = jax.numpy.ones(2)\n\n    def __call__(self, x):\n        for layer in self.layers[:-1]:\n            x = jax.nn.relu(layer(x))\n        return self.layers[-1](x) + self.extra_bias\n\n@jax.jit  # compile this function to make it run fast.\n@jax.grad  # differentiate all floating-point arrays in `model`.\ndef loss(model, x, y):\n    pred_y = jax.vmap(model)(x)  # vectorise the model over a batch of data\n    return jax.numpy.mean((y - pred_y) ** 2)  # L2 loss\n\nx_key, y_key, model_key = jax.random.split(jax.random.PRNGKey(0), 3)\n# Example data\nx = jax.random.normal(x_key, (100, 2))\ny = jax.random.normal(y_key, (100, 2))\nmodel = NeuralNetwork(model_key)\n# Compute gradients\ngrads = loss(model, x, y)\n# Perform gradient descent\nlearning_rate = 0.1\nnew_model = jax.tree_util.tree_map(lambda m, g: m - learning_rate * g, model, grads)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Training GAN Models with JAX and Equinox\nDESCRIPTION: This snippet initializes the generator and discriminator models, sets up optimizers, and starts the training process. It uses JAX's random number generation and Equinox's neural network and optimization utilities.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/deep_convolutional_gan.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nkey = jr.PRNGKey(2003)\n\nkey, gen_key, disc_key = jr.split(key, 3)\n\ngenerator = Generator(input_shape=latent_size, output_shape=image_size, key=gen_key)\ndiscriminator = Discriminator(input_shape=image_size, key=disc_key)\n\ngenerator_state = eqx.nn.State(generator)\ndiscriminator_state = eqx.nn.State(discriminator)\n\ngenerator_optimizer = optax.adam(lr, b1=beta1, b2=beta2)\ndiscriminator_optimizer = optax.adam(lr, b1=beta1, b2=beta2)\n\ngenerator_opt_state = generator_optimizer.init(eqx.filter(generator, eqx.is_array))\ndiscriminator_opt_state = discriminator_optimizer.init(\n    eqx.filter(discriminator, eqx.is_array)\n)\n\n(\n    generator,\n    discriminator,\n    generator_state,\n    discriminator_state,\n    generator_losses,\n    discriminator_losses,\n    key,\n) = train(\n    generator,\n    discriminator,\n    generator_optimizer,\n    discriminator_optimizer,\n    generator_opt_state,\n    discriminator_opt_state,\n    generator_state,\n    discriminator_state,\n    dataloader,\n    num_steps,\n    key,\n)\n```\n\n----------------------------------------\n\nTITLE: Complete Training Function with Equinox and JAX\nDESCRIPTION: A comprehensive training function that combines gradient computation, optimization, and model updates in a JIT-compiled step function. It uses Equinox filtering to handle non-array parameters and implements periodic evaluation on the test dataset while training.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/mnist.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef train(\n    model: CNN,\n    trainloader: torch.utils.data.DataLoader,\n    testloader: torch.utils.data.DataLoader,\n    optim: optax.GradientTransformation,\n    steps: int,\n    print_every: int,\n) -> CNN:\n    # Just like earlier: It only makes sense to train the arrays in our model,\n    # so filter out everything else.\n    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n\n    # Always wrap everything -- computing gradients, running the optimiser, updating\n    # the model -- into a single JIT region. This ensures things run as fast as\n    # possible.\n    @eqx.filter_jit\n    def make_step(\n        model: CNN,\n        opt_state: PyTree,\n        x: Float[Array, \"batch 1 28 28\"],\n        y: Int[Array, \" batch\"],\n    ):\n        loss_value, grads = eqx.filter_value_and_grad(loss)(model, x, y)\n        updates, opt_state = optim.update(\n            grads, opt_state, eqx.filter(model, eqx.is_array)\n        )\n        model = eqx.apply_updates(model, updates)\n        return model, opt_state, loss_value\n\n    # Loop over our training dataset as many times as we need.\n    def infinite_trainloader():\n        while True:\n            yield from trainloader\n\n    for step, (x, y) in zip(range(steps), infinite_trainloader()):\n        # PyTorch dataloaders give PyTorch tensors by default,\n        # so convert them to NumPy arrays.\n        x = x.numpy()\n        y = y.numpy()\n        model, opt_state, train_loss = make_step(model, opt_state, x, y)\n        if (step % print_every) == 0 or (step == steps - 1):\n            test_loss, test_accuracy = evaluate(model, testloader)\n            print(\n                f\"{step=}, train_loss={train_loss.item()}, \"\n                f\"test_loss={test_loss.item()}, test_accuracy={test_accuracy.item()}\"\n            )\n    return model\n```\n\n----------------------------------------\n\nTITLE: Implementing Loss and Sampling Functions for Score-Based Diffusion\nDESCRIPTION: Defines functions for computing loss during training and generating samples during inference. It includes the analytical computation of score function and ODE-based sampling using diffrax.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/score_based_diffusion.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef single_loss_fn(model, weight, int_beta, data, t, key):\n    mean = data * jnp.exp(-0.5 * int_beta(t))\n    var = jnp.maximum(1 - jnp.exp(-int_beta(t)), 1e-5)\n    std = jnp.sqrt(var)\n    noise = jr.normal(key, data.shape)\n    y = mean + std * noise\n    pred = model(t, y)\n    return weight(t) * jnp.mean((pred + noise / std) ** 2)\n\n\ndef batch_loss_fn(model, weight, int_beta, data, t1, key):\n    batch_size = data.shape[0]\n    tkey, losskey = jr.split(key)\n    losskey = jr.split(losskey, batch_size)\n    # Low-discrepancy sampling over t to reduce variance\n    t = jr.uniform(tkey, (batch_size,), minval=0, maxval=t1 / batch_size)\n    t = t + (t1 / batch_size) * jnp.arange(batch_size)\n    loss_fn = ft.partial(single_loss_fn, model, weight, int_beta)\n    loss_fn = jax.vmap(loss_fn)\n    return jnp.mean(loss_fn(data, t, losskey))\n\n\n@eqx.filter_jit\ndef single_sample_fn(model, int_beta, data_shape, dt0, t1, key):\n    def drift(t, y, args):\n        _, beta = jax.jvp(int_beta, (t,), (jnp.ones_like(t),))\n        return -0.5 * beta * (y + model(t, y))\n\n    term = dfx.ODETerm(drift)\n    solver = dfx.Tsit5()\n    t0 = 0\n    y1 = jr.normal(key, data_shape)\n    # reverse time, solve from t1 to t0\n    sol = dfx.diffeqsolve(term, solver, t1, t0, -dt0, y1)\n    return sol.ys[0]\n```\n\n----------------------------------------\n\nTITLE: Training BERT Classifier on SST2 Dataset with JAX\nDESCRIPTION: Training loop for the BERT classifier on the SST2 dataset using JAX and Equinox. The code sets up an optimizer with Adam and gradient clipping, replicates the model across devices, and processes batches with a progress bar. It distributes computation across multiple devices for efficiency.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/bert.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nepochs = 3\nbatch_size = 32\nlearning_rate = 1e-5\n\nnum_devices = jax.device_count()\nassert (\n    batch_size % num_devices == 0\n), \"The batch size must be a multiple of the number of devices.\"\n\ntx = optax.adam(learning_rate=learning_rate)\ntx = optax.chain(optax.clip_by_global_norm(1.0), tx)\nopt_state = tx.init(classifier_chkpt)\n\np_make_step = eqx.filter_pmap(functools.partial(make_step, tx=tx), axis_name=\"devices\")\n\n# Replicate across devices.\nopt_state = jax.device_put_replicated(opt_state, jax.local_devices())\nmodel = jax.device_put_replicated(classifier_chkpt, jax.local_devices())\ntrain_key = jax.device_put_replicated(train_key, jax.local_devices())\n\nfor epoch in range(epochs):\n    with tqdm.tqdm(\n        ds[\"train\"].iter(batch_size=batch_size, drop_last_batch=True),\n        total=ds[\"train\"].num_rows // batch_size,\n        unit=\"steps\",\n        desc=f\"Epoch {epoch+1}/{epochs}\",\n    ) as tqdm_epoch:\n        for batch in tqdm_epoch:\n            token_ids, token_type_ids = batch[\"input_ids\"], batch[\"token_type_ids\"]\n            label = batch[\"label\"]\n\n            # Split batch across devices.\n            token_ids = einops.rearrange(\n                token_ids, \"(b1 b2) s -> b1 b2 s\", b1=num_devices\n            )\n            token_type_ids = einops.rearrange(\n                token_type_ids, \"(b1 b2) s -> b1 b2 s\", b1=num_devices\n            )\n            label = einops.rearrange(label, \"(b1 b2) -> b1 b2\", b1=num_devices)\n\n            inputs = {\n                \"token_ids\": token_ids,\n                \"segment_ids\": token_type_ids,\n                \"label\": label,\n            }\n            loss, model, opt_state, train_key = p_make_step(\n                model, inputs, opt_state, train_key\n            )\n\n            tqdm_epoch.set_postfix(loss=np.sum(loss).item())\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Functions for Vision Transformer in Python\nDESCRIPTION: Defines functions for training the Vision Transformer model using JAX and Equinox, including gradient computation, model updates, and a training loop with loss tracking. It uses softmax cross-entropy loss and the Adam optimizer.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/vision_transformer.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@eqx.filter_value_and_grad\ndef compute_grads(\n    model: VisionTransformer, images: jnp.ndarray, labels: jnp.ndarray, key\n):\n    logits = jax.vmap(model, in_axes=(0, None, 0))(images, True, key)\n    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n\n    return jnp.mean(loss)\n\n\n@eqx.filter_jit\ndef step_model(\n    model: VisionTransformer,\n    optimizer: optax.GradientTransformation,\n    state: optax.OptState,\n    images: jnp.ndarray,\n    labels: jnp.ndarray,\n    key,\n):\n    loss, grads = compute_grads(model, images, labels, key)\n    updates, new_state = optimizer.update(grads, state, model)\n\n    model = eqx.apply_updates(model, updates)\n\n    return model, new_state, loss\n\n\ndef train(\n    model: VisionTransformer,\n    optimizer: optax.GradientTransformation,\n    state: optax.OptState,\n    data_loader: torch.utils.data.DataLoader,\n    num_steps: int,\n    print_every: int = 1000,\n    key=None,\n):\n    losses = []\n\n    def infinite_trainloader():\n        while True:\n            yield from data_loader\n\n    for step, batch in zip(range(num_steps), infinite_trainloader()):\n        images, labels = batch\n\n        images = images.numpy()\n        labels = labels.numpy()\n\n        key, *subkeys = jr.split(key, num=batch_size + 1)\n        subkeys = jnp.array(subkeys)\n\n        (model, state, loss) = step_model(\n            model, optimizer, state, images, labels, subkeys\n        )\n\n        losses.append(loss)\n\n        if (step % print_every) == 0 or step == num_steps - 1:\n            print(f\"Step: {step}/{num_steps}, Loss: {loss}.\")\n\n    return model, state, losses\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing CIFAR10 Dataset with PyTorch in Python\nDESCRIPTION: Sets up data loading and preprocessing for the CIFAR10 dataset using torchvision, including data augmentation for training, normalization, and creating DataLoader objects for both training and testing.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/vision_transformer.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntransform_train = transforms.Compose(\n    [\n        transforms.RandomCrop(32, padding=4),\n        transforms.Resize((height, width)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ]\n)\n\ntransform_test = transforms.Compose(\n    [\n        transforms.Resize((height, width)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ]\n)\n\ntrain_dataset = torchvision.datasets.CIFAR10(\n    \"CIFAR\",\n    train=True,\n    download=True,\n    transform=transform_train,\n)\n\ntest_dataset = torchvision.datasets.CIFAR10(\n    \"CIFAR\",\n    train=False,\n    download=True,\n    transform=transform_test,\n)\n\ntrainloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n)\n\ntestloader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Training and Evaluation Functions for BERT in JAX\nDESCRIPTION: Definition of loss computation, training step, and evaluation functions for BERT using JAX's functional approach. The code leverages JAX's automatic differentiation, device parallelism through pmap, and vectorization through vmap for efficient training.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/bert.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@eqx.filter_value_and_grad\ndef compute_loss(classifier, inputs, key):\n    batch_size = inputs[\"token_ids\"].shape[0]\n    batched_keys = jax.random.split(key, num=batch_size)\n    logits = jax.vmap(classifier, in_axes=(0, None, 0))(inputs, True, batched_keys)\n    return jnp.mean(\n        optax.softmax_cross_entropy_with_integer_labels(\n            logits=logits, labels=inputs[\"label\"]\n        )\n    )\n\n\ndef make_step(model, inputs, opt_state, key, tx):\n    key, new_key = jax.random.split(key)\n    loss, grads = compute_loss(model, inputs, key)\n    grads = jax.lax.pmean(grads, axis_name=\"devices\")\n\n    updates, opt_state = tx.update(grads, opt_state, model)\n    model = eqx.apply_updates(model, updates)\n    return loss, model, opt_state, new_key\n\n\ndef make_eval_step(model, inputs):\n    return jax.vmap(functools.partial(model, enable_dropout=False))(inputs)\n\n\np_make_eval_step = eqx.filter_pmap(make_eval_step)\n```\n\n----------------------------------------\n\nTITLE: Implementing UNet Class for Score-Based Diffusion in Python\nDESCRIPTION: Defines the main UNet class for score-based diffusion models. It includes time embedding, down and up sampling blocks, middle blocks, and final convolution layers. The architecture is flexible and can be customized with various parameters.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/unet.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass UNet(eqx.Module):\n    time_pos_emb: SinusoidalPosEmb\n    mlp: eqx.nn.MLP\n    first_conv: eqx.nn.Conv2d\n    down_res_blocks: list[list[ResnetBlock]]\n    mid_block1: ResnetBlock\n    mid_block2: ResnetBlock\n    ups_res_blocks: list[list[ResnetBlock]]\n    final_conv_layers: list[Union[Callable, eqx.nn.LayerNorm, eqx.nn.Conv2d]]\n\n    def __init__(\n        self,\n        data_shape: tuple[int, int, int],\n        is_biggan: bool,\n        dim_mults: list[int],\n        hidden_size: int,\n        heads: int,\n        dim_head: int,\n        dropout_rate: float,\n        num_res_blocks: int,\n        attn_resolutions: list[int],\n        *,\n        key,\n    ):\n        keys = jax.random.split(key, 7)\n        del key\n\n        data_channels, in_height, in_width = data_shape\n\n        dims = [hidden_size] + [hidden_size * m for m in dim_mults]\n        in_out = list(exact_zip(dims[:-1], dims[1:]))\n\n        self.time_pos_emb = SinusoidalPosEmb(hidden_size)\n        self.mlp = eqx.nn.MLP(\n            hidden_size,\n            hidden_size,\n            4 * hidden_size,\n            1,\n            activation=jax.nn.silu,\n            key=keys[0],\n        )\n        self.first_conv = eqx.nn.Conv2d(\n            data_channels, hidden_size, kernel_size=3, padding=1, key=keys[1]\n        )\n\n        h, w = in_height, in_width\n        self.down_res_blocks = []\n        num_keys = len(in_out) * num_res_blocks - 1\n        keys_resblock = jr.split(keys[2], num_keys)\n        i = 0\n        for ind, (dim_in, dim_out) in enumerate(in_out):\n            if h in attn_resolutions and w in attn_resolutions:\n                is_attn = True\n            else:\n                is_attn = False\n            res_blocks = [\n                ResnetBlock(\n                    dim_in=dim_in,\n                    dim_out=dim_out,\n                    is_biggan=is_biggan,\n                    up=False,\n                    down=False,\n                    time_emb_dim=hidden_size,\n                    dropout_rate=dropout_rate,\n                    is_attn=is_attn,\n                    heads=heads,\n                    dim_head=dim_head,\n                    key=keys_resblock[i],\n                )\n            ]\n            i += 1\n            for _ in range(num_res_blocks - 2):\n                res_blocks.append(\n                    ResnetBlock(\n                        dim_in=dim_out,\n                        dim_out=dim_out,\n                        is_biggan=is_biggan,\n                        up=False,\n                        down=False,\n                        time_emb_dim=hidden_size,\n                        dropout_rate=dropout_rate,\n                        is_attn=is_attn,\n                        heads=heads,\n                        dim_head=dim_head,\n                        key=keys_resblock[i],\n                    )\n                )\n                i += 1\n            if ind < (len(in_out) - 1):\n                res_blocks.append(\n                    ResnetBlock(\n                        dim_in=dim_out,\n                        dim_out=dim_out,\n                        is_biggan=is_biggan,\n                        up=False,\n                        down=True,\n                        time_emb_dim=hidden_size,\n                        dropout_rate=dropout_rate,\n                        is_attn=is_attn,\n                        heads=heads,\n                        dim_head=dim_head,\n                        key=keys_resblock[i],\n                    )\n                )\n                h, w = h // 2, w // 2\n            self.down_res_blocks.append(res_blocks)\n```\n\n----------------------------------------\n\nTITLE: Training GAN Models with JAX and Equinox\nDESCRIPTION: This snippet shows the core training loop for a GAN, updating both the generator and discriminator models. It includes loss calculation, optimization steps, and periodic logging of progress.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/deep_convolutional_gan.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimages, labels = batch\n\n(\n    discriminator_loss,\n    discriminator,\n    discriminator_opt_state,\n    generator_state,\n    discriminator_state,\n    key,\n) = step_discriminator(\n    discriminator,\n    generator,\n    images.numpy(),\n    discriminator_optimizer,\n    discriminator_opt_state,\n    generator_state,\n    discriminator_state,\n    key,\n)\n\n(\n    generator_loss,\n    generator,\n    generator_opt_state,\n    discriminator_state,\n    generator_state,\n    key,\n) = step_generator(\n    generator,\n    discriminator,\n    generator_optimizer,\n    generator_opt_state,\n    discriminator_state,\n    generator_state,\n    key,\n)\n\ngenerator_losses.append(generator_loss)\ndiscriminator_losses.append(discriminator_loss)\n\nif (step % print_every) == 0 or step == num_steps - 1:\n    print(\n        f\"Step: {step}/{num_steps}, Generator loss: {generator_loss}, \"\n        f\"Discriminator loss: {discriminator_loss}\"\n    )\n\nreturn (\n    generator,\n    discriminator,\n    generator_state,\n    discriminator_state,\n    generator_losses,\n    discriminator_losses,\n    key,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-GPU Training Step with Memory Donation in JAX and Equinox\nDESCRIPTION: This snippet defines a training step function that uses memory donation and array sharding for efficient multi-GPU parallelism. It applies gradients and updates the model parameters across multiple devices.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/parallelism.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@eqx.filter_jit(donate=\"all\")\ndef train_step(model, opt_state, x, y, sharding):\n    replicated = sharding.replicate()\n    model, opt_state = eqx.filter_shard((model, opt_state), replicated)\n    x, y = eqx.filter_shard((x, y), sharding)\n\n    grads = eqx.filter_grad(compute_loss)(model, x, y)\n    updates, opt_state = optim.update(grads, opt_state)\n    model = eqx.apply_updates(model, updates)\n\n    model, opt_state = eqx.filter_shard((model, opt_state), replicated)\n\n    return model, opt_state\n```\n\n----------------------------------------\n\nTITLE: UNet Model Implementation with JAX/Equinox\nDESCRIPTION: A UNet model implementation featuring ResNet blocks, attention mechanisms, and time embeddings. The model includes downsampling and upsampling paths, middle blocks, and final convolution layers. It handles both forward and backward passes with optional key-based randomization.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/unet.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nself.mid_block1 = ResnetBlock(\n    dim_in=mid_dim,\n    dim_out=mid_dim,\n    is_biggan=is_biggan,\n    up=False,\n    down=False,\n    time_emb_dim=hidden_size,\n    dropout_rate=dropout_rate,\n    is_attn=True,\n    heads=heads,\n    dim_head=dim_head,\n    key=keys[3],\n)\n\ndef __call__(self, t, y, *, key=None):\n    t = self.time_pos_emb(t)\n    t = self.mlp(t)\n    h = self.first_conv(y)\n    hs = [h]\n    for res_blocks in self.down_res_blocks:\n        for res_block in res_blocks:\n            key, subkey = key_split_allowing_none(key)\n            h = res_block(h, t, key=subkey)\n            hs.append(h)\n\n    key, subkey = key_split_allowing_none(key)\n    h = self.mid_block1(h, t, key=subkey)\n    key, subkey = key_split_allowing_none(key)\n    h = self.mid_block2(h, t, key=subkey)\n\n    for res_blocks in self.ups_res_blocks:\n        for res_block in res_blocks:\n            key, subkey = key_split_allowing_none(key)\n            if res_block.up:\n                h = res_block(h, t, key=subkey)\n            else:\n                h = res_block(jnp.concatenate((h, hs.pop()), axis=0), t, key=subkey)\n\n    assert len(hs) == 0\n\n    for layer in self.final_conv_layers:\n        h = layer(h)\n    return h\n```\n\n----------------------------------------\n\nTITLE: Implementing BERT Encoder Forward Pass in JAX\nDESCRIPTION: Code snippet showing the forward pass through BERT encoder layers. It processes input through each layer sequentially, collects layer outputs, and performs BERT pooling by extracting the first token (CLS) from the last layer and applying a pooler with tanh activation.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/bert.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfor layer in self.layers:\n    cl_key, l_key = (None, None) if l_key is None else jax.random.split(l_key)\n    x = layer(x, mask, enable_dropout=enable_dropout, key=cl_key)\n    layer_outputs.append(x)\n\n# BERT pooling.\n# The first token in the last layer is the embedding of the \"[CLS]\" token.\nfirst_token_last_layer = x[..., 0, :]\npooled = self.pooler(first_token_last_layer)\npooled = jnp.tanh(pooled)\n\nreturn {\"embeddings\": embeddings, \"layers\": layer_outputs, \"pooled\": pooled}\n```\n\n----------------------------------------\n\nTITLE: Implementing Transformer Feed Forward Block in Python with Equinox\nDESCRIPTION: Implementation of a transformer feed-forward block that applies a multi-layer perceptron to the input, followed by GELU activation, projection back to the input size, dropout, residual connection, and layer normalization.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/bert.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass FeedForwardBlock(eqx.Module):\n    \"\"\"A single transformer feed forward block.\"\"\"\n\n    mlp: eqx.nn.Linear\n    output: eqx.nn.Linear\n    layernorm: eqx.nn.LayerNorm\n    dropout: eqx.nn.Dropout\n\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        dropout_rate: float,\n        key: jax.random.PRNGKey,\n    ):\n        mlp_key, output_key = jax.random.split(key)\n        self.mlp = eqx.nn.Linear(\n            in_features=hidden_size, out_features=intermediate_size, key=mlp_key\n        )\n        self.output = eqx.nn.Linear(\n            in_features=intermediate_size, out_features=hidden_size, key=output_key\n        )\n\n        self.layernorm = eqx.nn.LayerNorm(shape=hidden_size)\n        self.dropout = eqx.nn.Dropout(dropout_rate)\n\n    def __call__(\n        self,\n        inputs: Float[Array, \" hidden_size\"],\n        enable_dropout: bool = True,\n        key: Optional[jax.random.PRNGKey] = None,\n    ) -> Float[Array, \" hidden_size\"]:\n        # Feed-forward.\n        hidden = self.mlp(inputs)\n        hidden = jax.nn.gelu(hidden)\n\n        # Project back to input size.\n        output = self.output(hidden)\n        output = self.dropout(output, inference=not enable_dropout, key=key)\n\n        # Residual and layer norm.\n        output += inputs\n        output = self.layernorm(output)\n\n        return output\n```\n\n----------------------------------------\n\nTITLE: Training Loop with JAX Transformations\nDESCRIPTION: Example showing how to use JAX transformations (jit, grad, vmap) with Equinox models for computing loss and gradients.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/index.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\n@jax.grad\ndef loss_fn(model, x, y):\n    pred_y = jax.vmap(model)(x)\n    return jax.numpy.mean((y - pred_y) ** 2)\n\nbatch_size, in_size, out_size = 32, 2, 3\nmodel = Linear(in_size, out_size, key=jax.random.PRNGKey(0))\nx = jax.numpy.zeros((batch_size, in_size))\ny = jax.numpy.zeros((batch_size, out_size))\ngrads = loss_fn(model, x, y)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Training Vision Transformer Model in Python\nDESCRIPTION: Creates and trains a Vision Transformer model for CIFAR10 using the Adam optimizer with specified hyperparameters. Initializes the model with random keys and trains it for the defined number of steps.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/vision_transformer.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nkey = jr.PRNGKey(2003)\n\nmodel = VisionTransformer(\n    embedding_dim=embedding_dim,\n    hidden_dim=hidden_dim,\n    num_heads=num_heads,\n    num_layers=num_layers,\n    dropout_rate=dropout_rate,\n    patch_size=patch_size,\n    num_patches=num_patches,\n    num_classes=num_classes,\n    key=key,\n)\n\noptimizer = optax.adamw(\n    learning_rate=lr,\n    b1=beta1,\n    b2=beta2,\n)\n\nstate = optimizer.init(eqx.filter(model, eqx.is_inexact_array))\n\nmodel, state, losses = train(model, optimizer, state, trainloader, num_steps, key=key)\n```\n\n----------------------------------------\n\nTITLE: Implementing AttentionBlock Module for Vision Transformer in Python\nDESCRIPTION: Creates the AttentionBlock class using Equinox which implements the core transformer attention mechanism. It includes multi-head attention, layer normalization, MLP layers, and dropout for regularization.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/vision_transformer.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass AttentionBlock(eqx.Module):\n    layer_norm1: eqx.nn.LayerNorm\n    layer_norm2: eqx.nn.LayerNorm\n    attention: eqx.nn.MultiheadAttention\n    linear1: eqx.nn.Linear\n    linear2: eqx.nn.Linear\n    dropout1: eqx.nn.Dropout\n    dropout2: eqx.nn.Dropout\n\n    def __init__(\n        self,\n        input_shape: int,\n        hidden_dim: int,\n        num_heads: int,\n        dropout_rate: float,\n        key: PRNGKeyArray,\n    ):\n        key1, key2, key3 = jr.split(key, 3)\n\n        self.layer_norm1 = eqx.nn.LayerNorm(input_shape)\n        self.layer_norm2 = eqx.nn.LayerNorm(input_shape)\n        self.attention = eqx.nn.MultiheadAttention(num_heads, input_shape, key=key1)\n\n        self.linear1 = eqx.nn.Linear(input_shape, hidden_dim, key=key2)\n        self.linear2 = eqx.nn.Linear(hidden_dim, input_shape, key=key3)\n        self.dropout1 = eqx.nn.Dropout(dropout_rate)\n        self.dropout2 = eqx.nn.Dropout(dropout_rate)\n\n    def __call__(\n        self,\n        x: Float[Array, \"num_patches embedding_dim\"],\n        enable_dropout: bool,\n        key: PRNGKeyArray,\n    ) -> Float[Array, \"num_patches embedding_dim\"]:\n        input_x = jax.vmap(self.layer_norm1)(x)\n        x = x + self.attention(input_x, input_x, input_x)\n\n        input_x = jax.vmap(self.layer_norm2)(x)\n        input_x = jax.vmap(self.linear1)(input_x)\n        input_x = jax.nn.gelu(input_x)\n\n        key1, key2 = jr.split(key, num=2)\n\n        input_x = self.dropout1(input_x, inference=not enable_dropout, key=key1)\n        input_x = jax.vmap(self.linear2)(input_x)\n        input_x = self.dropout2(input_x, inference=not enable_dropout, key=key2)\n\n        x = x + input_x\n\n        return x\n```\n\n----------------------------------------\n\nTITLE: Implementing Complete VisionTransformer Model with Equinox in Python\nDESCRIPTION: Defines the complete VisionTransformer class that combines patch embedding, positional encoding, CLS token, multiple attention blocks, and a classification head for image classification tasks using the transformer architecture.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/vision_transformer.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass VisionTransformer(eqx.Module):\n    patch_embedding: PatchEmbedding\n    positional_embedding: jnp.ndarray\n    cls_token: jnp.ndarray\n    attention_blocks: list[AttentionBlock]\n    dropout: eqx.nn.Dropout\n    mlp: eqx.nn.Sequential\n    num_layers: int\n\n    def __init__(\n        self,\n        embedding_dim: int,\n        hidden_dim: int,\n        num_heads: int,\n        num_layers: int,\n        dropout_rate: float,\n        patch_size: int,\n        num_patches: int,\n        num_classes: int,\n        key: PRNGKeyArray,\n    ):\n        key1, key2, key3, key4, key5 = jr.split(key, 5)\n\n        self.patch_embedding = PatchEmbedding(channels, embedding_dim, patch_size, key1)\n\n        self.positional_embedding = jr.normal(key2, (num_patches + 1, embedding_dim))\n\n        self.cls_token = jr.normal(key3, (1, embedding_dim))\n\n        self.num_layers = num_layers\n\n        self.attention_blocks = [\n            AttentionBlock(embedding_dim, hidden_dim, num_heads, dropout_rate, key4)\n            for _ in range(self.num_layers)\n        ]\n\n        self.dropout = eqx.nn.Dropout(dropout_rate)\n\n        self.mlp = eqx.nn.Sequential(\n            [\n                eqx.nn.LayerNorm(embedding_dim),\n                eqx.nn.Linear(embedding_dim, num_classes, key=key5),\n            ]\n        )\n\n    def __call__(\n        self,\n        x: Float[Array, \"channels height width\"],\n        enable_dropout: bool,\n        key: PRNGKeyArray,\n    ) -> Float[Array, \"num_classes\"]:\n        x = self.patch_embedding(x)\n\n        x = jnp.concatenate((self.cls_token, x), axis=0)\n\n        x += self.positional_embedding[\n            : x.shape[0]\n        ]  # Slice to the same length as x, as the positional embedding may be longer.\n\n        dropout_key, *attention_keys = jr.split(key, num=self.num_layers + 1)\n\n        x = self.dropout(x, inference=not enable_dropout, key=dropout_key)\n\n        for block, attention_key in zip(self.attention_blocks, attention_keys):\n            x = block(x, enable_dropout, key=attention_key)\n\n        x = x[0]  # Select the CLS token.\n        x = self.mlp(x)\n\n        return x\n```\n\n----------------------------------------\n\nTITLE: Implementing GAN Generator with Equinox\nDESCRIPTION: Defines the Generator class using Equinox modules to create a network that transforms random noise into MNIST-like images through a series of transpose convolutions with batch normalization and ReLU activations.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/deep_convolutional_gan.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass Generator(eqx.Module):\n    layers: list[Union[eqx.nn.ConvTranspose2d, eqx.nn.BatchNorm, Callable]]\n\n    def __init__(\n        self,\n        input_shape: int,\n        output_shape: tuple[int, int, int],\n        key: jr.PRNGKey,\n    ):\n        keys = jr.split(key, 5)\n\n        height, width, channels = output_shape\n\n        self.layers = [\n            eqx.nn.ConvTranspose2d(\n                in_channels=input_shape,\n                out_channels=width * 8,\n                kernel_size=4,\n                stride=1,\n                padding=0,\n                use_bias=False,\n                key=keys[0],\n            ),\n            eqx.nn.BatchNorm(input_size=width * 8, axis_name=\"batch\"),\n            jax.nn.relu,\n            eqx.nn.ConvTranspose2d(\n                in_channels=width * 8,\n                out_channels=width * 4,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                use_bias=False,\n                key=keys[1],\n            ),\n            eqx.nn.BatchNorm(input_size=width * 4, axis_name=\"batch\"),\n            jax.nn.relu,\n            eqx.nn.ConvTranspose2d(\n                in_channels=width * 4,\n                out_channels=width * 2,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                use_bias=False,\n                key=keys[2],\n            ),\n            eqx.nn.BatchNorm(input_size=width * 2, axis_name=\"batch\"),\n            jax.nn.relu,\n            eqx.nn.ConvTranspose2d(\n                in_channels=width * 2,\n                out_channels=width,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                use_bias=False,\n                key=keys[3],\n            ),\n            eqx.nn.BatchNorm(input_size=width, axis_name=\"batch\"),\n            jax.nn.relu,\n            eqx.nn.ConvTranspose2d(\n                in_channels=width,\n                out_channels=channels,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                use_bias=False,\n                key=keys[4],\n            ),\n            jax.nn.tanh,\n        ]\n\n    def __call__(self, x, state):\n        for layer in self.layers:\n            if isinstance(layer, eqx.nn.BatchNorm):\n                x, state = layer(x, state)\n            else:\n                x = layer(x)\n\n        return x, state\n```\n\n----------------------------------------\n\nTITLE: Implementing BERT Encoder in Python with Equinox\nDESCRIPTION: Implementation of the complete BERT encoder which combines the embedder block with multiple transformer layers and a pooler. It processes token, position, and segment IDs through the embedding layer and then through a series of transformer layers, with masking for padding tokens.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/bert.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass Encoder(eqx.Module):\n    \"\"\"Full BERT encoder.\"\"\"\n\n    embedder_block: EmbedderBlock\n    layers: List[TransformerLayer]\n    pooler: eqx.nn.Linear\n\n    def __init__(\n        self,\n        vocab_size: int,\n        max_length: int,\n        type_vocab_size: int,\n        embedding_size: int,\n        hidden_size: int,\n        intermediate_size: int,\n        num_layers: int,\n        num_heads: int,\n        dropout_rate: float,\n        attention_dropout_rate: float,\n        key: jax.random.PRNGKey,\n    ):\n        embedder_key, layer_key, pooler_key = jax.random.split(key, num=3)\n        self.embedder_block = EmbedderBlock(\n            vocab_size=vocab_size,\n            max_length=max_length,\n            type_vocab_size=type_vocab_size,\n            embedding_size=embedding_size,\n            hidden_size=hidden_size,\n            dropout_rate=dropout_rate,\n            key=embedder_key,\n        )\n\n        layer_keys = jax.random.split(layer_key, num=num_layers)\n        self.layers = []\n        for layer_key in layer_keys:\n            self.layers.append(\n                TransformerLayer(\n                    hidden_size=hidden_size,\n                    intermediate_size=intermediate_size,\n                    num_heads=num_heads,\n                    dropout_rate=dropout_rate,\n                    attention_dropout_rate=attention_dropout_rate,\n                    key=layer_key,\n                )\n            )\n\n        self.pooler = eqx.nn.Linear(\n            in_features=hidden_size, out_features=hidden_size, key=pooler_key\n        )\n\n    def __call__(\n        self,\n        token_ids: Int[Array, \" seq_len\"],\n        position_ids: Int[Array, \" seq_len\"],\n        segment_ids: Int[Array, \" seq_len\"],\n        *,\n        enable_dropout: bool = False,\n        key: Optional[jax.random.PRNGKey] = None,\n    ) -> Dict[str, Array]:\n        emb_key, l_key = (None, None) if key is None else jax.random.split(key)\n\n        embeddings = self.embedder_block(\n            token_ids=token_ids,\n            position_ids=position_ids,\n            segment_ids=segment_ids,\n            enable_dropout=enable_dropout,\n            key=emb_key,\n        )\n\n        # We assume that all 0-values should be masked out.\n        mask = jnp.asarray(token_ids != 0, dtype=jnp.int32)\n\n        x = embeddings\n        layer_outputs = []\n```\n\n----------------------------------------\n\nTITLE: Implementing Transformer Layer in Python with Equinox\nDESCRIPTION: Implementation of a complete transformer layer that combines an attention block and a feed-forward block. It first processes inputs through the attention mechanism and then through the feed-forward network, with appropriate dropout and masking support.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/bert.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass TransformerLayer(eqx.Module):\n    \"\"\"A single transformer layer.\"\"\"\n\n    attention_block: AttentionBlock\n    ff_block: FeedForwardBlock\n\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        num_heads: int,\n        dropout_rate: float,\n        attention_dropout_rate: float,\n        key: jax.random.PRNGKey,\n    ):\n        attention_key, ff_key = jax.random.split(key)\n\n        self.attention_block = AttentionBlock(\n            hidden_size=hidden_size,\n            num_heads=num_heads,\n            dropout_rate=dropout_rate,\n            attention_dropout_rate=attention_dropout_rate,\n            key=attention_key,\n        )\n        self.ff_block = FeedForwardBlock(\n            hidden_size=hidden_size,\n            intermediate_size=intermediate_size,\n            dropout_rate=dropout_rate,\n            key=ff_key,\n        )\n\n    def __call__(\n        self,\n        inputs: Float[Array, \"seq_len hidden_size\"],\n        mask: Optional[Int[Array, \" seq_len\"]] = None,\n        *,\n        enable_dropout: bool = False,\n        key: Optional[jax.random.PRNGKey] = None,\n    ) -> Float[Array, \"seq_len hidden_size\"]:\n        attn_key, ff_key = (None, None) if key is None else jax.random.split(key)\n        attention_output = self.attention_block(\n            inputs, mask, enable_dropout=enable_dropout, key=attn_key\n        )\n        seq_len = inputs.shape[0]\n        ff_keys = None if ff_key is None else jax.random.split(ff_key, num=seq_len)\n        output = jax.vmap(self.ff_block, in_axes=(0, None, 0))(\n            attention_output, enable_dropout, ff_keys\n        )\n        return output\n```\n\n----------------------------------------\n\nTITLE: Converting Equinox Models to Init-Apply Pattern\nDESCRIPTION: Creates a function that takes an Equinox model and returns separate init and apply functions compatible with the init/apply pattern. The init function returns the trainable parameters, while the apply function combines parameters with static elements to execute the model.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/init_apply.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport equinox as eqx\n\n\ndef make_mlp(in_size, out_size, width_size, depth, *, key):\n    mlp = eqx.nn.MLP(\n        in_size, out_size, width_size, depth, key=key\n    )  # insert your model here\n    params, static = eqx.partition(mlp, eqx.is_inexact_array)\n\n    def init_fn():\n        return params\n\n    def apply_fn(_params, x):\n        model = eqx.combine(_params, static)\n        return model(x)\n\n    return init_fn, apply_fn\n```\n\n----------------------------------------\n\nTITLE: Implementing LinearTimeSelfAttention in Python\nDESCRIPTION: Defines a linear time self-attention module. It uses group normalization, multi-head attention, and convolutional layers to process input features.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/unet.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass LinearTimeSelfAttention(eqx.Module):\n    group_norm: eqx.nn.GroupNorm\n    heads: int\n    to_qkv: eqx.nn.Conv2d\n    to_out: eqx.nn.Conv2d\n\n    def __init__(\n        self,\n        dim,\n        key,\n        heads=4,\n        dim_head=32,\n    ):\n        keys = jax.random.split(key, 2)\n        self.group_norm = eqx.nn.GroupNorm(min(dim // 4, 32), dim)\n        self.heads = heads\n        hidden_dim = dim_head * heads\n        self.to_qkv = eqx.nn.Conv2d(dim, hidden_dim * 3, 1, key=keys[0])\n        self.to_out = eqx.nn.Conv2d(hidden_dim, dim, 1, key=keys[1])\n\n    def __call__(self, x):\n        c, h, w = x.shape\n        x = self.group_norm(x)\n        qkv = self.to_qkv(x)\n        q, k, v = rearrange(\n            qkv, \"(qkv heads c) h w -> qkv heads c (h w)\", heads=self.heads, qkv=3\n        )\n        k = jax.nn.softmax(k, axis=-1)\n        context = jnp.einsum(\"hdn,hen->hde\", k, v)\n        out = jnp.einsum(\"hde,hdn->hen\", context, q)\n        out = rearrange(\n            out, \"heads c (h w) -> (heads c) h w\", heads=self.heads, h=h, w=w\n        )\n        return self.to_out(out)\n```\n\n----------------------------------------\n\nTITLE: Implementing MLP-Mixer Architecture for Score Modeling\nDESCRIPTION: Defines the MLP-Mixer architecture used for parameterizing the score model. It includes MixerBlock for feature mixing and Mixer2d for handling 2D images with time embeddings.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/score_based_diffusion.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MixerBlock(eqx.Module):\n    patch_mixer: eqx.nn.MLP\n    hidden_mixer: eqx.nn.MLP\n    norm1: eqx.nn.LayerNorm\n    norm2: eqx.nn.LayerNorm\n\n    def __init__(\n        self, num_patches, hidden_size, mix_patch_size, mix_hidden_size, *, key\n    ):\n        tkey, ckey = jr.split(key, 2)\n        self.patch_mixer = eqx.nn.MLP(\n            num_patches, num_patches, mix_patch_size, depth=1, key=tkey\n        )\n        self.hidden_mixer = eqx.nn.MLP(\n            hidden_size, hidden_size, mix_hidden_size, depth=1, key=ckey\n        )\n        self.norm1 = eqx.nn.LayerNorm((hidden_size, num_patches))\n        self.norm2 = eqx.nn.LayerNorm((num_patches, hidden_size))\n\n    def __call__(self, y):\n        y = y + jax.vmap(self.patch_mixer)(self.norm1(y))\n        y = einops.rearrange(y, \"c p -> p c\")\n        y = y + jax.vmap(self.hidden_mixer)(self.norm2(y))\n        y = einops.rearrange(y, \"p c -> c p\")\n        return y\n\n\nclass Mixer2d(eqx.Module):\n    conv_in: eqx.nn.Conv2d\n    conv_out: eqx.nn.ConvTranspose2d\n    blocks: list\n    norm: eqx.nn.LayerNorm\n    t1: float\n\n    def __init__(\n        self,\n        img_size,\n        patch_size,\n        hidden_size,\n        mix_patch_size,\n        mix_hidden_size,\n        num_blocks,\n        t1,\n        *,\n        key,\n    ):\n        input_size, height, width = img_size\n        assert (height % patch_size) == 0\n        assert (width % patch_size) == 0\n        num_patches = (height // patch_size) * (width // patch_size)\n        inkey, outkey, *bkeys = jr.split(key, 2 + num_blocks)\n\n        self.conv_in = eqx.nn.Conv2d(\n            input_size + 1, hidden_size, patch_size, stride=patch_size, key=inkey\n        )\n        self.conv_out = eqx.nn.ConvTranspose2d(\n            hidden_size, input_size, patch_size, stride=patch_size, key=outkey\n        )\n        self.blocks = [\n            MixerBlock(\n                num_patches, hidden_size, mix_patch_size, mix_hidden_size, key=bkey\n            )\n            for bkey in bkeys\n        ]\n        self.norm = eqx.nn.LayerNorm((hidden_size, num_patches))\n        self.t1 = t1\n\n    def __call__(self, t, y):\n        t = jnp.array(t / self.t1)\n        _, height, width = y.shape\n        t = einops.repeat(t, \"-> 1 h w\", h=height, w=width)\n        y = jnp.concatenate([y, t])\n        y = self.conv_in(y)\n        _, patch_height, patch_width = y.shape\n        y = einops.rearrange(y, \"c h w -> c (h w)\")\n        for block in self.blocks:\n            y = block(y)\n        y = self.norm(y)\n        y = einops.rearrange(y, \"c (h w) -> c h w\", h=patch_height, w=patch_width)\n        return self.conv_out(y)\n```\n\n----------------------------------------\n\nTITLE: Implementing Transformer Attention Block in Python with Equinox\nDESCRIPTION: Implementation of a transformer attention block using multi-head attention mechanism. It processes inputs through self-attention, applies dropout, adds residual connections, and performs layer normalization. It also includes masking functionality for attention.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/bert.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass AttentionBlock(eqx.Module):\n    \"\"\"A single transformer attention block.\"\"\"\n\n    attention: eqx.nn.MultiheadAttention\n    layernorm: eqx.nn.LayerNorm\n    dropout: eqx.nn.Dropout\n    num_heads: int = eqx.field(static=True)\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n        dropout_rate: float,\n        attention_dropout_rate: float,\n        key: jax.random.PRNGKey,\n    ):\n        self.num_heads = num_heads\n        self.attention = eqx.nn.MultiheadAttention(\n            num_heads=num_heads,\n            query_size=hidden_size,\n            use_query_bias=True,\n            use_key_bias=True,\n            use_value_bias=True,\n            use_output_bias=True,\n            dropout_p=attention_dropout_rate,\n            key=key,\n        )\n        self.layernorm = eqx.nn.LayerNorm(shape=hidden_size)\n        self.dropout = eqx.nn.Dropout(dropout_rate)\n\n    def __call__(\n        self,\n        inputs: Float[Array, \"seq_len hidden_size\"],\n        mask: Optional[Int[Array, \" seq_len\"]],\n        enable_dropout: bool = False,\n        key: \"jax.random.PRNGKey\" = None,\n    ) -> Float[Array, \"seq_len hidden_size\"]:\n        if mask is not None:\n            mask = self.make_self_attention_mask(mask)\n        attention_key, dropout_key = (\n            (None, None) if key is None else jax.random.split(key)\n        )\n\n        attention_output = self.attention(\n            query=inputs,\n            key_=inputs,\n            value=inputs,\n            mask=mask,\n            inference=not enable_dropout,\n            key=attention_key,\n        )\n\n        result = attention_output\n        result = self.dropout(result, inference=not enable_dropout, key=dropout_key)\n        result = result + inputs\n        result = jax.vmap(self.layernorm)(result)\n        return result\n\n    def make_self_attention_mask(\n        self, mask: Int[Array, \" seq_len\"]\n    ) -> Float[Array, \"num_heads seq_len seq_len\"]:\n        \"\"\"Create self-attention mask from sequence-level mask.\"\"\"\n        mask = jnp.multiply(\n            jnp.expand_dims(mask, axis=-1), jnp.expand_dims(mask, axis=-2)\n        )\n        mask = jnp.expand_dims(mask, axis=-3)\n        mask = jnp.repeat(mask, repeats=self.num_heads, axis=-3)\n        return mask.astype(jnp.float32)\n```\n\n----------------------------------------\n\nTITLE: Implementing GAN Training Functions with JAX and Equinox\nDESCRIPTION: Defines gradient computation and training step functions for both generator and discriminator components of the GAN using JAX's functional paradigm and Equinox utilities for filtering and JIT compilation.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/deep_convolutional_gan.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@eqx.filter_value_and_grad(has_aux=True)\ndef compute_grads_discriminator(\n    discriminator,\n    generator,\n    fake_labels,\n    real_batch,\n    real_labels,\n    discriminator_state,\n    generator_state,\n    key,\n):\n    key, subkey = jr.split(key)\n    noise = jr.normal(subkey, (batch_size, latent_size, 1, 1))\n\n    fake_batch, generator_state = jax.vmap(\n        generator, axis_name=\"batch\", in_axes=(0, None), out_axes=(0, None)\n    )(noise, generator_state)\n\n    pred_y, discriminator_state = jax.vmap(\n        discriminator, axis_name=\"batch\", in_axes=(0, None), out_axes=(0, None)\n    )(fake_batch, discriminator_state)\n    loss1 = optax.sigmoid_binary_cross_entropy(pred_y, fake_labels).mean()\n\n    pred_y, discriminator_state = jax.vmap(\n        discriminator, axis_name=\"batch\", in_axes=(0, None), out_axes=(0, None)\n    )(real_batch, discriminator_state)\n    loss2 = optax.sigmoid_binary_cross_entropy(pred_y, real_labels).mean()\n\n    loss = (loss1 + loss2) / 2\n\n    return loss, (discriminator_state, generator_state, key)\n\n\n@eqx.filter_value_and_grad(has_aux=True)\ndef compute_grads_generator(\n    generator, discriminator, real_labels, discriminator_state, generator_state, key\n):\n    key, subkey = jr.split(key)\n    noise = jr.normal(subkey, (batch_size, latent_size, 1, 1))\n\n    fake_batch, generator_state = jax.vmap(\n        generator, axis_name=\"batch\", in_axes=(0, None), out_axes=(0, None)\n    )(noise, generator_state)\n\n    pred_y, discriminator_state = jax.vmap(\n        discriminator, axis_name=\"batch\", in_axes=(0, None), out_axes=(0, None)\n    )(fake_batch, discriminator_state)\n    loss = optax.sigmoid_binary_cross_entropy(pred_y, real_labels).mean()\n\n    return loss, (discriminator_state, generator_state, key)\n\n\n@eqx.filter_jit\ndef step_discriminator(\n    discriminator: Discriminator,\n    generator: Generator,\n    real_batch: jnp.ndarray,\n    discriminator_optimizer: optax.GradientTransformation,\n    discriminator_opt_state: optax.OptState,\n    generator_state: eqx.nn.State,\n    discriminator_state: eqx.nn.State,\n    key: jr.PRNGKey,\n):\n    fake_labels = jnp.zeros(batch_size)\n    real_labels = jnp.ones(batch_size)\n\n    (\n        (\n            loss,\n            (discriminator_state, generator_state, key),\n        ),\n        grads,\n    ) = compute_grads_discriminator(\n        discriminator,\n        generator,\n        fake_labels,\n        real_batch,\n        real_labels,\n        discriminator_state,\n        generator_state,\n        key,\n    )\n\n    updates, opt_state = discriminator_optimizer.update(grads, discriminator_opt_state)\n    discriminator = eqx.apply_updates(discriminator, updates)\n\n    return loss, discriminator, opt_state, generator_state, discriminator_state, key\n\n\n@eqx.filter_jit\ndef step_generator(\n    generator: Generator,\n    discriminator: Discriminator,\n    generator_optimizer: optax.GradientTransformation,\n    generator_opt_state: optax.OptState,\n    discriminator_state: eqx.nn.State,\n    generator_state: eqx.nn.State,\n    key: jr.PRNGKey,\n):\n    real_labels = jnp.ones(batch_size)\n\n    (\n        (\n            loss,\n            (discriminator_state, generator_state, key),\n        ),\n        grads,\n    ) = compute_grads_generator(\n        generator, discriminator, real_labels, discriminator_state, generator_state, key\n    )\n\n    updates, opt_state = generator_optimizer.update(grads, generator_opt_state)\n    generator = eqx.apply_updates(generator, updates)\n\n    return loss, generator, opt_state, discriminator_state, generator_state, key\n\n\ndef train(\n    generator: Generator,\n    discriminator: Discriminator,\n    generator_optimizer: optax.GradientTransformation,\n    discriminator_optimizer: optax.GradientTransformation,\n    generator_opt_state: optax.OptState,\n    discriminator_opt_state: optax.OptState,\n    generator_state: eqx.nn.State,\n    discriminator_state: eqx.nn.State,\n    data_loader: torch.utils.data.DataLoader,\n    num_steps: int,\n    key: jr.PRNGKey,\n    print_every: int = 1000,\n):\n    generator_losses = []\n    discriminator_losses = []\n\n    def infinite_trainloader():\n        while True:\n            yield from data_loader\n\n    for step, batch in zip(range(num_steps), infinite_trainloader()):\n```\n\n----------------------------------------\n\nTITLE: Initializing and Training a Model with Stateful Layers in Equinox\nDESCRIPTION: Demonstrates how to initialize a model with stateful layers using eqx.nn.make_with_state, which separates the initial parameters from the initial state. Then performs training iterations with state management.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/stateful.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndataset_size = 10\nlearning_rate = 3e-4\nsteps = 5\nseed = 5678\n\nkey = jr.PRNGKey(seed)\nmkey, xkey, xkey2 = jr.split(key, 3)\n\nmodel, state = eqx.nn.make_with_state(Model)(mkey)\n\nxs = jr.normal(xkey, (dataset_size, 3))\nys = jnp.sin(xs) + 1\noptim = optax.adam(learning_rate)\nopt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n\nfor _ in range(steps):\n    # Full-batch gradient descent in this simple example.\n    model, state, opt_state = make_step(model, state, opt_state, xs, ys)\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Loop for RNN Spiral Classifier with Equinox and JAX\nDESCRIPTION: Define the main training function including loss computation, optimization, and evaluation. Uses Equinox's filter_value_and_grad and filter_jit for efficient computation.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/train_rnn.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef main(\n    dataset_size=10000,\n    batch_size=32,\n    learning_rate=3e-3,\n    steps=200,\n    hidden_size=16,\n    depth=1,\n    seed=5678,\n):\n    data_key, model_key = jrandom.split(jrandom.PRNGKey(seed), 2)\n    xs, ys = get_data(dataset_size, key=data_key)\n    iter_data = dataloader((xs, ys), batch_size)\n\n    model = RNN(in_size=2, out_size=1, hidden_size=hidden_size, key=model_key)\n\n    @eqx.filter_value_and_grad\n    def compute_loss(model, x, y):\n        pred_y = jax.vmap(model)(x)\n        # Trains with respect to binary cross-entropy\n        return -jnp.mean(y * jnp.log(pred_y) + (1 - y) * jnp.log(1 - pred_y))\n\n    # Important for efficiency whenever you use JAX: wrap everything into a single JIT\n    # region.\n    @eqx.filter_jit\n    def make_step(model, x, y, opt_state):\n        loss, grads = compute_loss(model, x, y)\n        updates, opt_state = optim.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return loss, model, opt_state\n\n    optim = optax.adam(learning_rate)\n    opt_state = optim.init(model)\n    for step, (x, y) in zip(range(steps), iter_data):\n        loss, model, opt_state = make_step(model, x, y, opt_state)\n        loss = loss.item()\n        print(f\"step={step}, loss={loss}\")\n\n    pred_ys = jax.vmap(model)(xs)\n    num_correct = jnp.sum((pred_ys > 0.5) == ys)\n    final_accuracy = (num_correct / dataset_size).item()\n    print(f\"final_accuracy={final_accuracy}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Functions with Stateful Operations in Equinox\nDESCRIPTION: Defines functions for computing loss and performing training steps with stateful operations. Uses vmap with appropriate in_axes and out_axes to handle batched data while keeping model state unbatched.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/stateful.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef compute_loss(model, state, xs, ys):\n    batch_model = jax.vmap(\n        model, axis_name=\"batch\", in_axes=(0, None), out_axes=(0, None)\n    )\n    pred_ys, state = batch_model(xs, state)\n    loss = jnp.mean((pred_ys - ys) ** 2)\n    return loss, state\n\n\n@eqx.filter_jit\ndef make_step(model, state, opt_state, xs, ys):\n    grads, state = eqx.filter_grad(compute_loss, has_aux=True)(model, state, xs, ys)\n    updates, opt_state = optim.update(grads, opt_state)\n    model = eqx.apply_updates(model, updates)\n    return model, state, opt_state\n```\n\n----------------------------------------\n\nTITLE: Implementing ResnetBlock for U-Net in Python\nDESCRIPTION: Defines a ResNet block for the U-Net architecture. It includes options for upsampling, downsampling, time embedding, and attention mechanisms. The block uses group normalization, convolutions, and skip connections.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/unet.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass ResnetBlock(eqx.Module):\n    dim_out: int\n    is_biggan: bool\n    up: bool\n    down: bool\n    dropout_rate: float\n    time_emb_dim: int\n    mlp_layers: list[Union[Callable, eqx.nn.Linear]]\n    scaling: Union[None, Callable, eqx.nn.ConvTranspose2d, eqx.nn.Conv2d]\n    block1_groupnorm: eqx.nn.GroupNorm\n    block1_conv: eqx.nn.Conv2d\n    block2_layers: list[\n        Union[eqx.nn.GroupNorm, eqx.nn.Dropout, eqx.nn.Conv2d, Callable]\n    ]\n    res_conv: eqx.nn.Conv2d\n    attn: Optional[Residual]\n\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        is_biggan,\n        up,\n        down,\n        time_emb_dim,\n        dropout_rate,\n        is_attn,\n        heads,\n        dim_head,\n        *,\n        key,\n    ):\n        keys = jax.random.split(key, 7)\n        self.dim_out = dim_out\n        self.is_biggan = is_biggan\n        self.up = up\n        self.down = down\n        self.dropout_rate = dropout_rate\n        self.time_emb_dim = time_emb_dim\n\n        self.mlp_layers = [\n            jax.nn.silu,\n            eqx.nn.Linear(time_emb_dim, dim_out, key=keys[0]),\n        ]\n        self.block1_groupnorm = eqx.nn.GroupNorm(min(dim_in // 4, 32), dim_in)\n        self.block1_conv = eqx.nn.Conv2d(dim_in, dim_out, 3, padding=1, key=keys[1])\n        self.block2_layers = [\n            eqx.nn.GroupNorm(min(dim_out // 4, 32), dim_out),\n            jax.nn.silu,\n            eqx.nn.Dropout(dropout_rate),\n            eqx.nn.Conv2d(dim_out, dim_out, 3, padding=1, key=keys[2]),\n        ]\n\n        assert not self.up or not self.down\n\n        if is_biggan:\n            if self.up:\n                self.scaling = upsample_2d\n            elif self.down:\n                self.scaling = downsample_2d\n            else:\n                self.scaling = None\n        else:\n            if self.up:\n                self.scaling = eqx.nn.ConvTranspose2d(\n                    dim_in,\n                    dim_in,\n                    kernel_size=4,\n                    stride=2,\n                    padding=1,\n                    key=keys[3],\n                )\n            elif self.down:\n                self.scaling = eqx.nn.Conv2d(\n                    dim_in,\n                    dim_in,\n                    kernel_size=3,\n                    stride=2,\n                    padding=1,\n                    key=keys[4],\n                )\n            else:\n                self.scaling = None\n        self.res_conv = eqx.nn.Conv2d(dim_in, dim_out, kernel_size=1, key=keys[5])\n\n        if is_attn:\n            self.attn = Residual(\n                LinearTimeSelfAttention(\n                    dim_out,\n                    heads=heads,\n                    dim_head=dim_head,\n                    key=keys[6],\n                )\n            )\n        else:\n            self.attn = None\n\n    def __call__(self, x, t, *, key):\n        C, _, _ = x.shape\n        h = jax.nn.silu(self.block1_groupnorm(x))\n        if self.up or self.down:\n            h = self.scaling(h)  # pyright: ignore\n            x = self.scaling(x)  # pyright: ignore\n        h = self.block1_conv(h)\n\n        for layer in self.mlp_layers:\n            t = layer(t)\n        h = h + t[..., None, None]\n        for layer in self.block2_layers:\n            if isinstance(layer, eqx.nn.Dropout):\n                h = layer(h, key=key)\n            else:\n                h = layer(h)\n\n        if C != self.dim_out or self.up or self.down:\n            x = self.res_conv(x)\n\n        out = (h + x) / jnp.sqrt(2)\n        if self.attn is not None:\n            out = self.attn(out)\n        return out\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing MNIST Dataset\nDESCRIPTION: Functions for downloading, loading, and processing the MNIST dataset. Includes data normalization and a dataloader for batch processing during training.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/score_based_diffusion.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef mnist():\n    filename = \"train-images-idx3-ubyte.gz\"\n    url_dir = \"https://storage.googleapis.com/cvdf-datasets/mnist\"\n    target_dir = os.getcwd() + \"/data/mnist\"\n    url = f\"{url_dir}/{filename}\"\n    target = f\"{target_dir}/{filename}\"\n\n    if not os.path.exists(target):\n        os.makedirs(target_dir, exist_ok=True)\n        urllib.request.urlretrieve(url, target)\n        print(f\"Downloaded {url} to {target}\")\n\n    with gzip.open(target, \"rb\") as fh:\n        _, batch, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n        shape = (batch, 1, rows, cols)\n        return jnp.array(array.array(\"B\", fh.read()), dtype=jnp.uint8).reshape(shape)\n\n\ndef dataloader(data, batch_size, *, key):\n    dataset_size = data.shape[0]\n    indices = jnp.arange(dataset_size)\n    while True:\n        key, subkey = jr.split(key, 2)\n        perm = jr.permutation(subkey, indices)\n        start = 0\n        end = batch_size\n        while end < dataset_size:\n            batch_perm = perm[start:end]\n            yield data[batch_perm]\n            start = end\n            end = start + batch_size\n```\n\n----------------------------------------\n\nTITLE: Implementing Selective Parameter Training in Equinox\nDESCRIPTION: The main function that demonstrates how to selectively train parameters in an Equinox model. It sets up an MLP, creates a filter specification to identify trainable parameters, and implements a training loop that only updates the parameters of the final layer.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/frozen_layer.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef main(\n    dataset_size=10000,\n    batch_size=256,\n    learning_rate=3e-3,\n    steps=1000,\n    width_size=8,\n    depth=1,\n    seed=5678,\n):\n    data_key, loader_key, model_key = jrandom.split(jrandom.PRNGKey(seed), 3)\n    data = get_data(dataset_size, key=data_key)\n    data_iter = dataloader(data, batch_size, key=loader_key)\n\n    # Step 1\n    model = eqx.nn.MLP(\n        in_size=1, out_size=1, width_size=width_size, depth=depth, key=model_key\n    )\n\n    # Step 2\n    filter_spec = jtu.tree_map(lambda _: False, model)\n    filter_spec = eqx.tree_at(\n        lambda tree: (tree.layers[-1].weight, tree.layers[-1].bias),\n        filter_spec,\n        replace=(True, True),\n    )\n\n    # Step 3\n    @eqx.filter_jit\n    def make_step(model, x, y, opt_state):\n        @eqx.filter_grad\n        def loss(diff_model, static_model, x, y):\n            model = eqx.combine(diff_model, static_model)\n            pred_y = jax.vmap(model)(x)\n            return jnp.mean((y - pred_y) ** 2)\n\n        diff_model, static_model = eqx.partition(model, filter_spec)\n        grads = loss(diff_model, static_model, x, y)\n        updates, opt_state = optim.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return model, opt_state\n\n    # And now let's train for a short while -- in exactly the usual way -- and see what\n    # happens. We keep the original model around to compare to later.\n    original_model = model\n    optim = optax.sgd(learning_rate)\n    opt_state = optim.init(model)\n    for step, (x, y) in zip(range(steps), data_iter):\n        model, opt_state = make_step(model, x, y, opt_state)\n    print(\n        f\"Parameters of first layer at initialisation:\\n\"\n        f\"{jtu.tree_leaves(original_model.layers[0])}\\n\"\n    )\n    print(\n        f\"Parameters of first layer at end of training:\\n\"\n        f\"{jtu.tree_leaves(model.layers[0])}\\n\"\n    )\n    print(\n        f\"Parameters of last layer at initialisation:\\n\"\n        f\"{jtu.tree_leaves(original_model.layers[-1])}\\n\"\n    )\n    print(\n        f\"Parameters of last layer at end of training:\\n\"\n        f\"{jtu.tree_leaves(model.layers[-1])}\\n\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting Up Device Sharding and Running Multi-GPU Training Loop in JAX\nDESCRIPTION: This snippet demonstrates how to set up device sharding for multi-GPU training, move data to devices, and run the training loop using the previously defined train_step function.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/parallelism.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnum_devices = len(jax.devices())\ndevices = mesh_utils.create_device_mesh((num_devices, 1))\nsharding = jshard.PositionalSharding(devices)\nreplicated = sharding.replicate()\n\nmodel = eqx.filter_shard(model, replicated)\nfor step, (x, y) in zip(\n    range(1, num_steps + 1), train_dataloader((xs, ys), batch_size)\n):\n    x, y = eqx.filter_shard((x, y), sharding)\n    model, opt_state = train_step(model, opt_state, x, y, sharding)\n```\n\n----------------------------------------\n\nTITLE: Equinox Additional Transformation Functions\nDESCRIPTION: Supplementary transformation functions including checkpointing, closure conversion, vectorization (vmap), parallelization (pmap), and pure callbacks.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/transformations.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nequinox.filter_checkpoint\nequinox.filter_closure_convert\nequinox.filter_vmap\nequinox.filter_pmap\nequinox.filter_pure_callback\n```\n\n----------------------------------------\n\nTITLE: Improving Compilation Speed with Scan-over-Layers in Equinox\nDESCRIPTION: Showcases a technique to improve compilation speed by using vmap to create multiple layers with the same structure and then iterating over them using jax.lax.scan, demonstrated with a model of ten sequential MLPs.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/tricks.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport equinox as eqx\nimport jax.lax as lax\nimport jax.random as jr\n\nclass TenMLPs(eqx.Module):\n    mlps: eqx.nn.MLP\n  \n    def __init__(self, size, key):\n        keys = jr.split(key, 10)\n        make_mlp = lambda k: eqx.nn.MLP(size, size, size, 1, key=k)\n        self.mlps = eqx.filter_vmap(make_mlp)(keys)\n        # Store a single `eqx.nn.MLP` object, with each of its parameters (JAX\n        # arrays) having an extra length-10 dimension at the start.\n        #\n        # This works because `make_mlp` is just a function returning a PyTree,\n        # and `eqx.filter_vmap` operates on functions between PyTrees.\n        #\n        # `self.mlps` should not be called directly: it should only be\n        # used inside a `vmap` or `scan`, to first unpeel the extra dimension.\n  \n    def __call__(self, x):\n        dynamic_mlps, static_mlps = eqx.partition(self.mlps, eqx.is_array)\n        # `dynamic_mlps` has all the parameters, with their extra length-10\n        # dimension.\n        # `static_mlps` has all the non-parameters, e.g. activation functions,\n        # which are shared across all layers.\n    \n        def f(_x, _dynamic_mlp):\n            mlp = eqx.combine(_dynamic_mlp, static_mlps)\n            return mlp(_x), None\n    \n        out, _ = lax.scan(f, x, dynamic_mlps)\n        return out\n\nsize = 3\nkey = jr.PRNGKey(0)\nmodel_key, data_key = jr.split(key, 2)\nmodel = TenMLPs(size, model_key)\nx = jr.normal(data_key, (size,))\nmodel(x)\n```\n\n----------------------------------------\n\nTITLE: Using Vmap with Stateful Layers in Equinox\nDESCRIPTION: This example builds on the Counter layer to demonstrate how to use stateful layers with JAX's vmap functionality. It creates a Model class that contains both regular and vmap'd stateful counters, showing how to properly manage state when using filter_vmap with stateful components.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/stateful.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax.random as jr\n\nclass Model(eqx.Module):\n    linear: eqx.nn.Linear\n    counter: Counter\n    v_counter: Counter\n\n    def __init__(self, key):\n        # Not-stateful layer\n        self.linear = eqx.nn.Linear(2, 2, key=key)\n        # Stateful layer.\n        self.counter = Counter()\n        # Vmap'd stateful layer. (Whose initial state will include a batch dimension.)\n        self.v_counter = eqx.filter_vmap(Counter, axis_size=2)()\n\n    def __call__(self, x: Array, state: eqx.nn.State) -> tuple[Array, eqx.nn.State]:\n        # This bit happens as normal.\n        assert x.shape == (2,)\n        x = self.linear(x)\n        x, state = self.counter(x, state)\n\n        # For the vmap, we have to restrict our state to just those states we want to\n        # vmap, and then update the overall state again afterwards.\n        #\n        # After all, the state for `self.counter` isn't expecting to be batched, so we\n        # have to remove that.\n        substate = state.substate(self.v_counter)\n        x, substate = eqx.filter_vmap(self.v_counter)(x, substate)\n        state = state.update(substate)\n\n        return x, state\n\nkey = jr.PRNGKey(0)\nmodel, state = eqx.nn.make_with_state(Model)(key)\nx = jnp.array([5.0, -1.0])\nmodel(x, state)\n```\n\n----------------------------------------\n\nTITLE: Evaluating BERT Classifier on SST2 Validation Set\nDESCRIPTION: Evaluation of the trained BERT classifier on the SST2 validation set. The code processes validation batches, computes model predictions, compares them with ground truth labels, and calculates the accuracy. It uses the same device parallelization technique as in training for efficiency.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/bert.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\noutputs = []\nfor batch in tqdm.tqdm(\n    ds[\"validation\"].iter(batch_size=batch_size),\n    unit=\"steps\",\n    total=np.ceil(ds[\"validation\"].num_rows / batch_size),\n    desc=\"Validation\",\n):\n    token_ids, token_type_ids = batch[\"input_ids\"], batch[\"token_type_ids\"]\n    label = batch[\"label\"]\n\n    # Split batch across devices.\n    token_ids = einops.rearrange(token_ids, \"(b1 b2) s -> b1 b2 s\", b1=num_devices)\n    token_type_ids = einops.rearrange(\n        token_type_ids, \"(b1 b2) s -> b1 b2 s\", b1=num_devices\n    )\n\n    inputs = {\"token_ids\": token_ids, \"segment_ids\": token_type_ids}\n\n    # Compare predicted class with label.\n    output = p_make_eval_step(model, inputs)\n    output = map(float, np.argmax(output.reshape(-1, 2), axis=-1) == label)\n    outputs.extend(output)\n\nprint(f\"Accuracy: {100 * np.sum(outputs) / len(outputs):.2f}%\")\n```\n\n----------------------------------------\n\nTITLE: Implementing BERT Embedder Block in Python with Equinox\nDESCRIPTION: Implementation of the BERT embedder which combines token, segment, and position embeddings. It embeds input tokens along with segment and position information into vectors, then applies layer normalization and dropout.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/bert.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass EmbedderBlock(eqx.Module):\n    \"\"\"BERT embedder.\"\"\"\n\n    token_embedder: eqx.nn.Embedding\n    segment_embedder: eqx.nn.Embedding\n    position_embedder: eqx.nn.Embedding\n    layernorm: eqx.nn.LayerNorm\n    dropout: eqx.nn.Dropout\n\n    def __init__(\n        self,\n        vocab_size: int,\n        max_length: int,\n        type_vocab_size: int,\n        embedding_size: int,\n        hidden_size: int,\n        dropout_rate: float,\n        key: jax.random.PRNGKey,\n    ):\n        token_key, segment_key, position_key = jax.random.split(key, 3)\n\n        self.token_embedder = eqx.nn.Embedding(\n            num_embeddings=vocab_size, embedding_size=embedding_size, key=token_key\n        )\n        self.segment_embedder = eqx.nn.Embedding(\n            num_embeddings=type_vocab_size,\n            embedding_size=embedding_size,\n            key=segment_key,\n        )\n        self.position_embedder = eqx.nn.Embedding(\n            num_embeddings=max_length, embedding_size=embedding_size, key=position_key\n        )\n        self.layernorm = eqx.nn.LayerNorm(shape=hidden_size)\n        self.dropout = eqx.nn.Dropout(dropout_rate)\n\n    def __call__(\n        self,\n        token_ids: Int[Array, \" seq_len\"],\n        position_ids: Int[Array, \" seq_len\"],\n        segment_ids: Int[Array, \" seq_len\"],\n        enable_dropout: bool = False,\n        key: Optional[jax.random.PRNGKey] = None,\n    ) -> Float[Array, \"seq_len hidden_size\"]:\n        tokens = jax.vmap(self.token_embedder)(token_ids)\n        segments = jax.vmap(self.segment_embedder)(segment_ids)\n        positions = jax.vmap(self.position_embedder)(position_ids)\n        embedded_inputs = tokens + segments + positions\n        embedded_inputs = jax.vmap(self.layernorm)(embedded_inputs)\n        embedded_inputs = self.dropout(\n            embedded_inputs, inference=not enable_dropout, key=key\n        )\n        return embedded_inputs\n```\n\n----------------------------------------\n\nTITLE: Serializing Equinox Model with Hyperparameters to a Single File\nDESCRIPTION: Implements a function that saves both hyperparameters (as JSON) and model weights (using Equinox serialization) to a single file, with the hyperparameters on the first line.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/serialisation.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef save(filename, hyperparams, model):\n    with open(filename, \"wb\") as f:\n        hyperparam_str = json.dumps(hyperparams)\n        f.write((hyperparam_str + \"\\n\").encode())\n        eqx.tree_serialise_leaves(f, model)\n\n\nsave(\"model.eqx\", hyperparameters, model)\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Stateful Models in Equinox\nDESCRIPTION: Shows how to use a trained stateful model for inference. Sets the model to inference mode and creates a partial function that includes the final state, allowing the model to be called without explicitly passing the state.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/stateful.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninference_model = eqx.nn.inference_mode(model)\ninference_model = eqx.Partial(inference_model, state=state)\n\n\n@eqx.filter_jit\ndef evaluate(model, xs):\n    # discard state\n    out, _ = jax.vmap(model)(xs)\n    return out\n\n\ntest_dataset_size = 5\ntest_xs = jr.normal(xkey2, (test_dataset_size, 3))\npred_test_ys = evaluate(inference_model, test_xs)\n```\n\n----------------------------------------\n\nTITLE: Defining a Model with Stateful Layers in Equinox\nDESCRIPTION: Creates a neural network model that combines stateful layers (BatchNorm and SpectralNorm) with standard layers (Linear). The model threads state through the forward pass, demonstrating how to handle stateful operations.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/stateful.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# This model is just a weird mish-mash of stateful and non-stateful layers for\n# demonstration purposes, it isn't doing any clever.\nclass Model(eqx.Module):\n    norm1: eqx.nn.BatchNorm\n    spectral_linear: eqx.nn.SpectralNorm[eqx.nn.Linear]\n    norm2: eqx.nn.BatchNorm\n    linear1: eqx.nn.Linear\n    linear2: eqx.nn.Linear\n\n    def __init__(self, key):\n        key1, key2, key3, key4 = jr.split(key, 4)\n        self.norm1 = eqx.nn.BatchNorm(input_size=3, axis_name=\"batch\")\n        self.spectral_linear = eqx.nn.SpectralNorm(\n            layer=eqx.nn.Linear(in_features=3, out_features=32, key=key1),\n            weight_name=\"weight\",\n            key=key2,\n        )\n        self.norm2 = eqx.nn.BatchNorm(input_size=32, axis_name=\"batch\")\n        self.linear1 = eqx.nn.Linear(in_features=32, out_features=32, key=key3)\n        self.linear2 = eqx.nn.Linear(in_features=32, out_features=3, key=key4)\n\n    def __call__(self, x, state):\n        x, state = self.norm1(x, state)\n        x, state = self.spectral_linear(x, state)\n        x = jax.nn.relu(x)\n        x, state = self.norm2(x, state)\n        x = self.linear1(x)\n        x = jax.nn.relu(x)\n        x = self.linear2(x)\n        return x, state\n```\n\n----------------------------------------\n\nTITLE: Filtering with Filtered Transformations\nDESCRIPTION: Shows how to use Equinox's filtered transformation decorators as a convenience wrapper for handling models with mixed array and non-array components. This approach simplifies the filtering process with a cleaner syntax.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/all-of-equinox.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@eqx.filter_jit\n@eqx.filter_grad\ndef loss(model, x, y):\n    pred_y = jax.vmap(model)(x)\n    return jax.numpy.mean((y - pred_y) ** 2)\n\nloss(model, x, y)\n```\n\n----------------------------------------\n\nTITLE: Creating a Model Factory Function with Hyperparameters in Equinox\nDESCRIPTION: Defines a model factory function that constructs an MLP based on provided hyperparameters, setting up a pattern for recreating model structure during deserialization.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/serialisation.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\n\n\ndef make(*, key, size, width, depth, use_tanh=False):\n    if use_tanh:\n        activation = jnp.tanh\n    else:\n        activation = jax.nn.relu\n    # (This is not meant to be a realistically useful model.)\n    return eqx.nn.MLP(\n        in_size=size,\n        out_size=1,\n        width_size=width,\n        depth=depth,\n        activation=activation,\n        key=key,\n    )\n\n\nhyperparameters = {\"size\": 5, \"width\": 10, \"depth\": 3, \"use_tanh\": True}\nmodel = make(key=jr.PRNGKey(0), **hyperparameters)\n```\n\n----------------------------------------\n\nTITLE: Using the Init-Apply Pattern with Equinox Models\nDESCRIPTION: Demonstrates how to use the converted init/apply functions in practice. This example creates an MLP model, initializes its parameters, applies the model to data, manually updates parameters, and then reapplies the model to show that parameter updates affect the output.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/init_apply.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nimport jax.random as jrandom\nimport jax.tree_util as jtu\n\n\ndef main(in_size=2, seed=5678):\n    key = jrandom.PRNGKey(seed)\n\n    init_fn, apply_fn = make_mlp(\n        in_size=in_size, out_size=1, width_size=8, depth=1, key=key\n    )\n\n    x = jnp.arange(in_size)  # sample data\n    params = init_fn()\n    y1 = apply_fn(params, x)\n    params = jtu.tree_map(lambda p: p + 1, params)  # \"stochastic gradient descent\"\n    y2 = apply_fn(params, x)\n    assert y1 != y2\n\n\nmain()\n```\n\n----------------------------------------\n\nTITLE: Implementing PatchEmbedding Module for Vision Transformer in Python\nDESCRIPTION: Defines the PatchEmbedding class using Equinox that converts images into embedded patches, which is the first step in a Vision Transformer. It rearranges the input image into patches and projects them to the desired embedding dimension.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/vision_transformer.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass PatchEmbedding(eqx.Module):\n    linear: eqx.nn.Embedding\n    patch_size: int\n\n    def __init__(\n        self,\n        input_channels: int,\n        output_shape: int,\n        patch_size: int,\n        key: PRNGKeyArray,\n    ):\n        self.patch_size = patch_size\n\n        self.linear = eqx.nn.Linear(\n            self.patch_size**2 * input_channels,\n            output_shape,\n            key=key,\n        )\n\n    def __call__(\n        self, x: Float[Array, \"channels height width\"]\n    ) -> Float[Array, \"num_patches embedding_dim\"]:\n        x = einops.rearrange(\n            x,\n            \"c (h ph) (w pw) -> (h w) (c ph pw)\",\n            ph=self.patch_size,\n            pw=self.patch_size,\n        )\n        x = jax.vmap(self.linear)(x)\n\n        return x\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using PReLU Activation in Equinox\nDESCRIPTION: This snippet demonstrates the usage of the PReLU (Parametric Rectified Linear Unit) activation function in Equinox. PReLU is an advanced form of ReLU that introduces a learnable parameter for negative inputs.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/activations.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nequinox.nn.PReLU(\n    __init__\n    __call__\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Model Surgery with tree_at in Equinox\nDESCRIPTION: Demonstrates how to replace the final layer in an MLP using Equinox's tree_at function, showcasing the simplicity of model surgery with the model-as-PyTree approach.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/tricks.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmlp = eqx.nn.MLP(...)\nnew_final_layer = eqx.nn.Linear(...)\nwhere = lambda m: m.layers[-1]\nnew_mlp = eqx.tree_at(where, mlp, new_final_layer)\n```\n\n----------------------------------------\n\nTITLE: Standard Training Loop Pattern in Equinox\nDESCRIPTION: Shows a common pattern for training loops in Equinox using filter_jit for efficient model updates during training iteration.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/tricks.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@eqx.filter_jit\ndef make_step(model, opt_state, x, y):\n    ...\n    return update_model, update_opt_state\n\nmodel = ...\nopt_state = ...\n\nfor batch_x, batch_y in dataloader(...):\n    model, opt_state = make_step(model, opt_state, batch_x, batch_y)\n```\n\n----------------------------------------\n\nTITLE: Defining RNN Model Class with Equinox for Spiral Classification\nDESCRIPTION: Create an RNN model class using Equinox, including a GRU cell and a linear layer with custom bias handling.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/train_rnn.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass RNN(eqx.Module):\n    hidden_size: int\n    cell: eqx.Module\n    linear: eqx.nn.Linear\n    bias: jax.Array\n\n    def __init__(self, in_size, out_size, hidden_size, *, key):\n        ckey, lkey = jrandom.split(key)\n        self.hidden_size = hidden_size\n        self.cell = eqx.nn.GRUCell(in_size, hidden_size, key=ckey)\n        self.linear = eqx.nn.Linear(hidden_size, out_size, use_bias=False, key=lkey)\n        self.bias = jnp.zeros(out_size)\n\n    def __call__(self, input):\n        hidden = jnp.zeros((self.hidden_size,))\n\n        def f(carry, inp):\n            return self.cell(inp, carry), None\n\n        out, _ = lax.scan(f, hidden, input)\n        # sigmoid because we're performing binary classification\n        return jax.nn.sigmoid(self.linear(out) + self.bias)\n```\n\n----------------------------------------\n\nTITLE: Implementing GAN Discriminator with Equinox\nDESCRIPTION: Defines the Discriminator class using Equinox modules to create a network that evaluates whether images are real or generated, using convolutional layers with batch normalization and PReLU activations.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/deep_convolutional_gan.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass Discriminator(eqx.Module):\n    layers: list[Union[eqx.nn.Conv2d, eqx.nn.PReLU, eqx.nn.BatchNorm, Callable]]\n\n    def __init__(\n        self,\n        input_shape: tuple[int, int, int],\n        key: jr.PRNGKey,\n    ):\n        keys = jr.split(key, 5)\n\n        height, width, channels = input_shape\n\n        self.layers = [\n            eqx.nn.Conv2d(\n                in_channels=channels,\n                out_channels=width,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                use_bias=False,\n                key=keys[0],\n            ),\n            eqx.nn.PReLU(0.2),\n            eqx.nn.Conv2d(\n                in_channels=width,\n                out_channels=width * 2,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                use_bias=False,\n                key=keys[1],\n            ),\n            eqx.nn.BatchNorm(width * 2, axis_name=\"batch\"),\n            eqx.nn.PReLU(0.2),\n            eqx.nn.Conv2d(\n                in_channels=width * 2,\n                out_channels=width * 4,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                use_bias=False,\n                key=keys[2],\n            ),\n            eqx.nn.BatchNorm(width * 4, axis_name=\"batch\"),\n            eqx.nn.PReLU(0.2),\n            eqx.nn.Conv2d(\n                in_channels=width * 4,\n                out_channels=width * 8,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                use_bias=False,\n                key=keys[3],\n            ),\n            eqx.nn.BatchNorm(width * 8, axis_name=\"batch\"),\n            eqx.nn.PReLU(0.2),\n            eqx.nn.Conv2d(\n                in_channels=width * 8,\n                out_channels=1,\n                kernel_size=4,\n                stride=1,\n                padding=0,\n                use_bias=False,\n                key=keys[4],\n            ),\n        ]\n\n    def __call__(self, x, state):\n        for layer in self.layers:\n            if isinstance(layer, eqx.nn.BatchNorm):\n                x, state = layer(x, state=state)\n            else:\n                x = layer(x)\n\n        return x, state\n```\n\n----------------------------------------\n\nTITLE: Running the Parameter Freezing Example\nDESCRIPTION: Executes the main function to demonstrate that only the parameters of the final layer are updated during training, while the parameters of the first layer remain unchanged.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/frozen_layer.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmain()\n```\n\n----------------------------------------\n\nTITLE: JIT-compiled Loss Function and Accuracy Computation with Equinox\nDESCRIPTION: Functions for evaluating model performance. `compute_accuracy` calculates the average prediction accuracy on a batch by comparing model predictions with ground truth labels. Both functions use `eqx.filter_jit` to handle non-array components in the model.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/mnist.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nloss = eqx.filter_jit(loss)  # JIT our loss function from earlier!\n\n\n@eqx.filter_jit\ndef compute_accuracy(\n    model: CNN, x: Float[Array, \"batch 1 28 28\"], y: Int[Array, \" batch\"]\n) -> Float[Array, \"\"]:\n    \"\"\"This function takes as input the current model\n    and computes the average accuracy on a batch.\n    \"\"\"\n    pred_y = jax.vmap(model)(x)\n    pred_y = jnp.argmax(pred_y, axis=1)\n    return jnp.mean(y == pred_y)\n```\n\n----------------------------------------\n\nTITLE: Defining BertClassifier Class in Equinox\nDESCRIPTION: Implementation of a BERT classifier using Equinox. The class includes an encoder for text representation, a classifier head for prediction, and dropout for regularization. The forward pass processes token IDs, segment IDs, and position IDs through the encoder and classifier head.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/bert.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass BertClassifier(eqx.Module):\n    \"\"\"BERT classifier.\"\"\"\n\n    encoder: Encoder\n    classifier_head: eqx.nn.Linear\n    dropout: eqx.nn.Dropout\n\n    def __init__(self, config: Mapping, num_classes: int, key: jax.random.PRNGKey):\n        encoder_key, head_key = jax.random.split(key)\n\n        self.encoder = Encoder(\n            vocab_size=config[\"vocab_size\"],\n            max_length=config[\"max_position_embeddings\"],\n            type_vocab_size=config[\"type_vocab_size\"],\n            embedding_size=config[\"hidden_size\"],\n            hidden_size=config[\"hidden_size\"],\n            intermediate_size=config[\"intermediate_size\"],\n            num_layers=config[\"num_hidden_layers\"],\n            num_heads=config[\"num_attention_heads\"],\n            dropout_rate=config[\"hidden_dropout_prob\"],\n            attention_dropout_rate=config[\"attention_probs_dropout_prob\"],\n            key=encoder_key,\n        )\n        self.classifier_head = eqx.nn.Linear(\n            in_features=config[\"hidden_size\"], out_features=num_classes, key=head_key\n        )\n        self.dropout = eqx.nn.Dropout(config[\"hidden_dropout_prob\"])\n\n    def __call__(\n        self,\n        inputs: Dict[str, Int[Array, \" seq_len\"]],\n        enable_dropout: bool = True,\n        key: jax.random.PRNGKey = None,\n    ) -> Float[Array, \" num_classes\"]:\n        seq_len = inputs[\"token_ids\"].shape[-1]\n        position_ids = jnp.arange(seq_len)\n\n        e_key, d_key = (None, None) if key is None else jax.random.split(key)\n\n        pooled_output = self.encoder(\n            token_ids=inputs[\"token_ids\"],\n            segment_ids=inputs[\"segment_ids\"],\n            position_ids=position_ids,\n            enable_dropout=enable_dropout,\n            key=e_key,\n        )[\"pooled\"]\n        pooled_output = self.dropout(\n            pooled_output, inference=not enable_dropout, key=d_key\n        )\n\n        return self.classifier_head(pooled_output)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Stateful Counter Layer in Equinox\nDESCRIPTION: This example demonstrates how to create a custom stateful layer using the StateIndex class. The Counter class increments a counter each time it's called and adds the current count to the input. The example shows initialization, usage with make_with_state, and tracking the counter's state.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/stateful.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport equinox as eqx\nimport jax.numpy as jnp\nfrom jaxtyping import Array\n\nclass Counter(eqx.Module):\n    index: eqx.nn.StateIndex\n\n    def __init__(self):\n        init_state = jnp.array(0)\n        self.index = eqx.nn.StateIndex(init_state)\n\n    def __call__(self, x: Array, state: eqx.nn.State) -> tuple[Array, eqx.nn.State]:\n        value = state.get(self.index)\n        new_x = x + value\n        new_state = state.set(self.index, value + 1)\n        return new_x, new_state\n\ncounter, state = eqx.nn.make_with_state(Counter)()\nx = jnp.array(2.3)\n\nnum_calls = state.get(counter.index)\nprint(f\"Called {num_calls} times.\")  # 0\n\n_, state = counter(x, state)\nnum_calls = state.get(counter.index)\nprint(f\"Called {num_calls} times.\")  # 1\n\n_, state = counter(x, state)\nnum_calls = state.get(counter.index)\nprint(f\"Called {num_calls} times.\")  # 2\n```\n\n----------------------------------------\n\nTITLE: Detecting Recompilation in JAX with assert_max_traces\nDESCRIPTION: Using equinox.debug.assert_max_traces to detect when a function is being recompiled, which helps identify performance issues with JIT compilation.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/faq.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@eqx.filter_jit\n@eqx.debug.assert_max_traces(max_traces=1)\ndef your_function(x, y, z):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Evaluating Vision Transformer Model on CIFAR10 Test Data in Python\nDESCRIPTION: Evaluates the trained Vision Transformer model on the CIFAR10 test dataset by calculating prediction accuracy. It disables dropout during inference and computes the percentage of correctly predicted image classes.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/vision_transformer.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\naccuracies = []\n\nfor batch in range(len(test_dataset) // batch_size):\n    images, labels = next(iter(testloader))\n\n    logits = jax.vmap(functools.partial(model, enable_dropout=False))(\n        images.numpy(), key=jax.random.split(key, num=batch_size)\n    )\n\n    predictions = jnp.argmax(logits, axis=-1)\n\n    accuracy = jnp.mean(predictions == labels.numpy())\n\n    accuracies.append(accuracy)\n\nprint(f\"Accuracy: {np.sum(accuracies) / len(accuracies) * 100}%\")\n```\n\n----------------------------------------\n\nTITLE: Generating and Visualizing Synthetic MNIST Digits with Trained GAN\nDESCRIPTION: This snippet uses the trained generator to create synthetic MNIST digits, evaluates them with the discriminator, and visualizes the top-rated generated images. It employs JAX's vectorized operations and matplotlib for plotting.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/deep_convolutional_gan.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nkey, subkey = jr.split(key)\nnoise = jr.normal(subkey, (1000, latent_size, 1, 1))\n\n\n@eqx.filter_jit\ndef evaluate(model, xx):\n    out, _ = jax.vmap(model)(xx)\n    return out\n\n\ninference_gen = eqx.nn.inference_mode(generator)\ninference_gen = eqx.Partial(inference_gen, state=generator_state)\n\ngenerated_images = evaluate(inference_gen, noise)\n\ninference_discriminator = eqx.nn.inference_mode(discriminator)\ninference_discriminator = eqx.Partial(\n    inference_discriminator, state=discriminator_state\n)\n\nlogits = evaluate(inference_discriminator, generated_images)\n\nplot_sample = generated_images[\n    jax.lax.top_k(jax.nn.sigmoid(logits).squeeze(), 32)[1]\n].transpose(0, 2, 3, 1)\n\nfig, axes = plt.subplots(nrows=4, ncols=8, figsize=(8, 4))\nfig.suptitle(\"Top generated fake images\", y=1.02, fontsize=14)\n\nfor ax, image in zip(sum(axes.tolist(), []), plot_sample):\n    ax.imshow(image, cmap=\"gray\")\n    ax.set_axis_off()\n\nplt.subplots_adjust(wspace=0.1, hspace=0.1)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-GPU Evaluation Step in JAX and Equinox\nDESCRIPTION: This snippet shows how to implement an evaluation step for multi-GPU setups, including a dataloader for evaluation and a function to compute the loss without donating model parameters.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/parallelism.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef eval_dataloader(arrays, batch_size):\n    dataset_size = arrays[0].shape[0]\n    assert all(array.shape[0] == dataset_size for array in arrays)\n    start = 0\n    end = batch_size\n    while start < dataset_size:\n        yield tuple(array[start:end] for array in arrays)\n        start = end\n        end = start + batch_size\n\n\n@eqx.filter_jit(donate=\"all-except-first\")\ndef evaluate(model, x, y, sharding):\n    replicated = sharding.replicate()\n    model = eqx.filter_shard(model, replicated)\n    x, y = eqx.filter_shard((x, y), sharding)\n    return compute_loss(model, x, y)\n\n\nloss = 0\nnum_batches = 0\nfor x, y in eval_dataloader((xs, ys), batch_size):\n    loss = loss + evaluate(model, x, y, sharding).item()\n    num_batches = num_batches + 1\nprint(f\"train loss={loss/num_batches}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing __check_init__ for Validation in Equinox Modules\nDESCRIPTION: Demonstrates how to use the __check_init__ method in an abstract base class to validate invariants after initialization. This method is specific to Equinox and runs automatically after object creation.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/pattern.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass AbstractPolynomialInterpolation(AbstractInterpolation)\n    coeffs: AbstractVar[Array]\n\n    def __check_init__(self):\n        if not jnp.issubdtype(self.coeffs.dtype, jnp.floating):\n            raise ValueError(\"Coefficients must be floating-point!\")\n\n    ...\n```\n\n----------------------------------------\n\nTITLE: Low-Overhead Training Loop with Manual PyTree Flattening in Equinox\nDESCRIPTION: Demonstrates how to optimize a training loop by manually flattening and unflattening PyTrees to reduce the overhead of entering and exiting JIT regions.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/tricks.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@eqx.filter_jit\ndef make_step(flat_model, flat_opt_state, x, y):\n    model = jax.tree_util.tree_unflatten(treedef_model, flat_model)\n    opt_state = jax.tree_util.tree_unflatten(treedef_opt_state, flat_opt_state)\n    ...\n    flat_update_model = jax.tree_util.tree_leaves(update_model)\n    flat_update_opt_state = jax.tree_util.tree_leaves(update_opt_state)\n    return flat_update_model, flat_update_opt_state\n\nmodel = ...\nopt_state = ...\nflat_model, treedef_model = jax.tree_util.tree_flatten(model)\nflat_opt_state, treedef_opt_state = jax.tree_util.tree_flatten(opt_state)\n\nfor batch_x, batch_y in dataloader(...):\n    flat_model, flat_opt_state = make_step(flat_model, flat_opt_state, batch_x, batch_y)\nmodel = jax.tree_util.tree_unflatten(treedef_model, flat_model)\n```\n\n----------------------------------------\n\nTITLE: Defining CNN Model Architecture with Equinox\nDESCRIPTION: This snippet defines a CNN class using Equinox, including convolutional layers, pooling, and fully connected layers for MNIST classification.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/mnist.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass CNN(eqx.Module):\n    layers: list\n\n    def __init__(self, key):\n        key1, key2, key3, key4 = jax.random.split(key, 4)\n        # Standard CNN setup: convolutional layer, followed by flattening,\n        # with a small MLP on top.\n        self.layers = [\n            eqx.nn.Conv2d(1, 3, kernel_size=4, key=key1),\n            eqx.nn.MaxPool2d(kernel_size=2),\n            jax.nn.relu,\n            jnp.ravel,\n            eqx.nn.Linear(1728, 512, key=key2),\n            jax.nn.sigmoid,\n            eqx.nn.Linear(512, 64, key=key3),\n            jax.nn.relu,\n            eqx.nn.Linear(64, 10, key=key4),\n            jax.nn.log_softmax,\n        ]\n\n    def __call__(self, x: Float[Array, \"1 28 28\"]) -> Float[Array, \"10\"]:\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n\nkey, subkey = jax.random.split(key, 2)\nmodel = CNN(subkey)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Issues with Cooperative Multiple Inheritance\nDESCRIPTION: Illustrates potential problems with cooperative multiple inheritance in Python, where not all __init__ methods are called due to improper use of super(). This example shows why the abstract+final pattern is preferred in Equinox.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/pattern.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass A:\n    def __init__(self, x):\n        self.x = x\n        # Not calling super().__init__, because the superclass is just `object`, right?\n\nclass AA:\n    def __init__(...):\n        super().__init__(...)  # Being a good citizen.\n        ...  # Do anything else that needs to happen.\n\nclass B(A, AA):\n    pass\n\nB()  # AA.__init__ is not called.\n```\n\n----------------------------------------\n\nTITLE: Handling Batch Dimensions with JAX vmap and Equinox\nDESCRIPTION: Comparison between PyTorch batch handling and Equinox approach using jax.vmap. Shows how to apply a linear layer to batched inputs in both frameworks.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/faq.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nlinear = torch.nn.Linear(input_size, output_size)\n\ny = linear(x)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport equinox as eqx\nkey = jax.random.PRNGKey(seed=0)\nlinear = eqx.nn.Linear(input_size, output_size, key=key)\n\ny = jax.vmap(linear)(x)\n```\n\n----------------------------------------\n\nTITLE: Using Equinox Model with JAX Operations\nDESCRIPTION: Demonstrates how to use the defined Equinox model with JAX operations like jit, grad, and vmap. Includes defining a loss function and computing gradients.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\n@jax.grad\ndef loss_fn(model, x, y):\n    pred_y = jax.vmap(model)(x)\n    return jax.numpy.mean((y - pred_y) ** 2)\n\nbatch_size, in_size, out_size = 32, 2, 3\nmodel = Linear(in_size, out_size, key=jax.random.PRNGKey(0))\nx = jax.numpy.zeros((batch_size, in_size))\ny = jax.numpy.zeros((batch_size, out_size))\ngrads = loss_fn(model, x, y)\n```\n\n----------------------------------------\n\nTITLE: Defining Loss Function and Cross-Entropy for MNIST CNN\nDESCRIPTION: This snippet defines the loss function and cross-entropy calculation for the MNIST CNN model using JAX operations.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/mnist.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef loss(\n    model: CNN, x: Float[Array, \"batch 1 28 28\"], y: Int[Array, \" batch\"]\n) -> Float[Array, \"\"]:\n    pred_y = jax.vmap(model)(x)\n    return cross_entropy(y, pred_y)\n\n\ndef cross_entropy(\n    y: Int[Array, \" batch\"], pred_y: Float[Array, \"batch 10\"]\n) -> Float[Array, \"\"]:\n    pred_y = jnp.take_along_axis(pred_y, jnp.expand_dims(y, 1), axis=1)\n    return -jnp.mean(pred_y)\n\n\n# Example loss\nloss_value = loss(model, dummy_x, dummy_y)\nprint(loss_value.shape)  # scalar loss\n# Example inference\noutput = jax.vmap(model)(dummy_x)\nprint(output.shape)  # batch of predictions\n```\n\n----------------------------------------\n\nTITLE: Importing Equinox Internal Module in Python\nDESCRIPTION: Shows how to import the semi-public internal namespace of Equinox which contains advanced, undocumented features primarily for downstream libraries.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/equinox/internal/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport equinox.internal as eqxi\n```\n\n----------------------------------------\n\nTITLE: Implementing an Abstract Optimizer Interface in Equinox\nDESCRIPTION: This example demonstrates using abstract base classes to define an optimizer interface with an Adam implementation. It shows how to define abstract methods that must be implemented by concrete subclasses.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/pattern.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass AbstractOptimiser(eqx.Module):\n    @abc.abstractmethod\n    def init(self, params):\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def update(self, params, grads, state):\n        raise NotImplementedError\n\nclass Adam(AbstractOptimiser):\n    learning_rate: float\n    beta1: float = 0.9\n    beta2: float = 0.999\n\n    def init(self, params):\n        ...  # some implementation\n        return initial_state\n\n    def update(self, params, grads, state):\n        ...  # some implementation\n        return new_params, new_state\n\n@eqx.filter_jit\ndef make_step(params, data, opt_state, optimiser: AbstractOptimiser):\n    grads = eqx.filter_grad(compute_loss)(params, data)\n    new_params, new_opt_state = optimiser.update(params, grads, opt_state)\n    return new_params, new_opt_state\n\ndef train(params, dataloader, optimiser: AbstractOptimiser):\n    opt_state = optimiser.init(params)\n    for data in dataloader:\n        params, opt_state = make_step(params, data, opt_state, optimiser)\n    return params\n\nparams = ...  # some model\ndataloader = ...  # some dataloader\noptimiser = Adam(learning_rate=3e-4)\ntrain(params, dataloader, optimiser)\n```\n\n----------------------------------------\n\nTITLE: Using Equinox Filter Functions for Gradient Computation\nDESCRIPTION: This snippet demonstrates the use of Equinox's filter functions to compute gradients on the model parameters while handling non-differentiable components.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/mnist.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# This will work too!\nvalue, grads = eqx.filter_value_and_grad(loss)(model, dummy_x, dummy_y)\nprint(value)\n```\n\n----------------------------------------\n\nTITLE: Equinox Custom Derivative Functions\nDESCRIPTION: Functions for defining custom forward and backward derivatives, including filter_custom_jvp and filter_custom_vjp with their respective definition methods.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/transformations.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nequinox.filter_custom_jvp.def_jvp\nequinox.filter_custom_vjp.def_fwd\nequinox.filter_custom_vjp.def_bwd\n```\n\n----------------------------------------\n\nTITLE: Defining a Linear Layer in Equinox\nDESCRIPTION: Example of defining a linear layer using Equinox's PyTorch-like syntax. The class inherits from eqx.Module and implements initialization and forward pass methods.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport equinox as eqx\nimport jax\n\nclass Linear(eqx.Module):\n    weight: jax.Array\n    bias: jax.Array\n\n    def __init__(self, in_size, out_size, key):\n        wkey, bkey = jax.random.split(key)\n        self.weight = jax.random.normal(wkey, (out_size, in_size))\n        self.bias = jax.random.normal(bkey, (out_size,))\n\n    def __call__(self, x):\n        return self.weight @ x + self.bias\n```\n\n----------------------------------------\n\nTITLE: Creating and Evaluating Model Ensembles with filter_vmap in Equinox\nDESCRIPTION: Demonstrates how to create and use ensembles of models by vectorizing model initialization and evaluation with filter_vmap, supporting both shared and per-model input data.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/tricks.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nkey = jax.random.PRNGKey(0)\nkeys = jax.random.split(key, 8)\n\n# Create an ensemble of models\n@eqx.filter_vmap\ndef make_ensemble(key):\n    return eqx.nn.MLP(2, 2, 2, 2, key=key)\n\nmlp_ensemble = make_ensemble(keys)\n\n# Evaluate each member of the ensemble on the same data\n@eqx.filter_vmap(in_axes=(eqx.if_array(0), None))\ndef evaluate_ensemble(model, x):\n    return model(x)\n\nevaluate_ensemble(mlp_ensemble, jax.random.normal(key, (2,)))\n\n# Evaluate each member of the ensemble on different data\n@eqx.filter_vmap\ndef evaluate_per_ensemble(model, x):\n    return model(x)\n\nevaluate_per_ensemble(mlp_ensemble, jax.random.normal(key, (8, 2)))\n```\n\n----------------------------------------\n\nTITLE: Fixing Optax TypeError with Equinox Models\nDESCRIPTION: Example showing how to properly initialize an Optax optimizer with an Equinox model by filtering for inexact arrays. This solves the common error when Optax attempts to operate on non-array components of a model.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/faq.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\noptim = optax.adam(learning_rate)\noptim.init(eqx.filter(model, eqx.is_inexact_array))\n```\n\n----------------------------------------\n\nTITLE: Initializing BERT Model with Tiny-BERT Configuration\nDESCRIPTION: Initialization of a BERT classifier using the Tiny-BERT configuration (2-layer BERT model with 128 hidden size). This snippet sets up the model with a specific random seed and prepares it for training or fine-tuning on a specific task.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/bert.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Tiny-BERT config.\nbert_config = {\n    \"vocab_size\": 30522,\n    \"hidden_size\": 128,\n    \"num_hidden_layers\": 2,\n    \"num_attention_heads\": 2,\n    \"hidden_act\": \"gelu\",\n    \"intermediate_size\": 512,\n    \"hidden_dropout_prob\": 0.1,\n    \"attention_probs_dropout_prob\": 0.1,\n    \"max_position_embeddings\": 512,\n    \"type_vocab_size\": 2,\n    \"initializer_range\": 0.02,\n}\n\nkey = jax.random.PRNGKey(5678)\nmodel_key, train_key = jax.random.split(key)\nclassifier = BertClassifier(config=bert_config, num_classes=2, key=model_key)\n```\n\n----------------------------------------\n\nTITLE: Model Evaluation Function for Test Dataset\nDESCRIPTION: Function that evaluates a model's performance on the test dataset, computing both average loss and accuracy. It iterates through a PyTorch dataloader, converts tensors to NumPy arrays, and calls the JIT-compiled loss and accuracy functions.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/mnist.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate(model: CNN, testloader: torch.utils.data.DataLoader):\n    \"\"\"This function evaluates the model on the test dataset,\n    computing both the average loss and the average accuracy.\n    \"\"\"\n    avg_loss = 0\n    avg_acc = 0\n    for x, y in testloader:\n        x = x.numpy()\n        y = y.numpy()\n        # Note that all the JAX operations happen inside `loss` and `compute_accuracy`,\n        # and both have JIT wrappers, so this is fast.\n        avg_loss += loss(model, x, y)\n        avg_acc += compute_accuracy(model, x, y)\n    return avg_loss / len(testloader), avg_acc / len(testloader)\n```\n\n----------------------------------------\n\nTITLE: Importing Equinox Transposed Convolutional Layer Class\nDESCRIPTION: Reference to the Equinox transposed convolutional neural network base class with initialization and call methods.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/conv.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n::: equinox.nn.ConvTranspose\n    options:\n        members:\n            - __init__\n            - __call__\n```\n\n----------------------------------------\n\nTITLE: Loading Pretrained BERT Checkpoint in Equinox\nDESCRIPTION: Code for loading a pretrained BERT checkpoint into the initialized model. The checkpoint contains pretrained weights that significantly improve training performance compared to randomly initialized weights.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/bert.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Download the checkpoint from\n# https://github.com/patrick-kidger/equinox/blob/main/examples/bert_checkpoint.eqx\nclassifier_chkpt = eqx.tree_deserialise_leaves(\"bert_checkpoint.eqx\", classifier)\n```\n\n----------------------------------------\n\nTITLE: Tokenizing and Loading SST2 Dataset for BERT\nDESCRIPTION: Preparation of the SST2 (Stanford Sentiment Treebank) dataset for BERT training. It uses the Hugging Face tokenizer and dataset library to load, tokenize, and format the data for JAX processing.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/bert.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntokenizer = AutoTokenizer.from_pretrained(\n    \"google/bert_uncased_L-2_H-128_A-2\", model_max_length=128\n)\n\n\ndef tokenize(example):\n    return tokenizer(example[\"sentence\"], padding=\"max_length\", truncation=True)\n\n\nds = load_dataset(\"sst2\")\nds = ds.map(tokenize, batched=True)\nds.set_format(type=\"jax\", columns=[\"input_ids\", \"token_type_ids\", \"label\"])\n```\n\n----------------------------------------\n\nTITLE: Initializing and Calling 2D Adaptive Max Pooling Layer in Equinox\nDESCRIPTION: Definition of the AdaptiveMaxPool2d class in Equinox, including initialization and call methods for 2D adaptive max pooling.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/pool.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nequinox.nn.AdaptiveMaxPool2d\n```\n\n----------------------------------------\n\nTITLE: Inference Mode Context Manager Usage in Python\nDESCRIPTION: Context manager that configures whether neural networks are in training or inference mode. Affects behavior of layers like Dropout and BatchNorm.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/inference.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: equinox.nn.inference_mode\n```\n\n----------------------------------------\n\nTITLE: Implementing Linear Layer with Equinox\nDESCRIPTION: Definition of a linear layer class using Equinox Module. Implements weight and bias initialization using JAX random number generation.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport equinox as eqx\nimport jax\n\nclass Linear(eqx.Module):\n    weight: jax.Array\n    bias: jax.Array\n\n    def __init__(self, in_size, out_size, key):\n        wkey, bkey = jax.random.split(key)\n        self.weight = jax.random.normal(wkey, (out_size, in_size))\n        self.bias = jax.random.normal(bkey, (out_size,))\n\n    def __call__(self, x):\n        return self.weight @ x + self.bias\n```\n\n----------------------------------------\n\nTITLE: Initializing and Calling 3D Adaptive Average Pooling Layer in Equinox\nDESCRIPTION: Definition of the AdaptiveAvgPool3d class in Equinox, including initialization and call methods for 3D adaptive average pooling.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/pool.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nequinox.nn.AdaptiveAvgPool3d\n```\n\n----------------------------------------\n\nTITLE: Initializing Weights with Truncated Normal Distribution in Equinox\nDESCRIPTION: Demonstrates how to replace the weights of all linear layers in an arbitrary model with custom initialization from a truncated normal distribution.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/tricks.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef trunc_init(weight: jax.Array, key: jax.random.PRNGKey) -> jax.Array:\n  out, in_ = weight.shape\n  stddev = math.sqrt(1 / in_)\n  return stddev * jax.random.truncated_normal(key, shape=(out, in_), lower=-2, upper=2)\n\ndef init_linear_weight(model, init_fn, key):\n  is_linear = lambda x: isinstance(x, eqx.nn.Linear)\n  get_weights = lambda m: [x.weight\n                           for x in jax.tree_util.tree_leaves(m, is_leaf=is_linear)\n                           if is_linear(x)]\n  weights = get_weights(model)\n  new_weights = [init_fn(weight, subkey)\n                 for weight, subkey in zip(weights, jax.random.split(key, len(weights)))]\n  new_model = eqx.tree_at(get_weights, model, new_weights)\n  return new_model\n\nmodel = ... # any PyTree\nkey = jax.random.PRNGKey(...)\nnew_model = init_linear_weight(model, trunc_init, key)\n```\n\n----------------------------------------\n\nTITLE: Creating a Neural Network with Non-Array Components\nDESCRIPTION: Shows how to create a neural network that includes non-array components like activation functions in the PyTree structure. This example sets up the foundation for demonstrating filtering techniques.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/all-of-equinox.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport equinox as eqx\nimport functools as ft\nimport jax\n\nclass NeuralNetwork2(eqx.Module):\n    layers: list\n\n    def __init__(self, key):\n        key1, key2 = jax.random.split(key)\n        self.layers = [eqx.nn.Linear(2, 8, key=key1),\n                       jax.nn.relu,\n                       eqx.nn.Linear(8, 2, key=key2)]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\nx_key, y_key, model_key = jax.random.split(jax.random.PRNGKey(0), 3)\nx, y = jax.random.normal(x_key, (100, 2)), jax.random.normal(y_key, (100, 2))\nmodel = NeuralNetwork2(model_key)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Calling Pool Layer in Equinox\nDESCRIPTION: Definition of the base Pool class in Equinox, including initialization and call methods.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/pool.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nequinox.nn.Pool\n```\n\n----------------------------------------\n\nTITLE: Creating Non-Trainable Arrays in Equinox (Like PyTorch Buffers)\nDESCRIPTION: Implementation of non-trainable parameters in Equinox models using jax.lax.stop_gradient, equivalent to PyTorch's registered buffers.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/faq.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass Model(eqx.Module):\n    buffer: Array\n    param: Array\n\n    def __call__(self, x):\n        return self.param * x + jax.lax.stop_gradient(self.buffer)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing MNIST Dataset with PyTorch\nDESCRIPTION: Sets up the MNIST dataset using torchvision transforms for normalization and resizing, then creates a DataLoader with batching and shuffling for efficient training.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/deep_convolutional_gan.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntransform = transforms.Compose(\n    [\n        transforms.Resize((height, width)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,)),\n    ]\n)\n\ndata = torchvision.datasets.MNIST(root=\"./data\", transform=transform, download=True)\n\ndataloader = torch.utils.data.DataLoader(\n    data, batch_size=batch_size, shuffle=True, num_workers=2\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Calling Adaptive Pooling Layer in Equinox\nDESCRIPTION: Definition of the base AdaptivePool class in Equinox, including initialization and call methods for adaptive pooling.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/pool.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nequinox.nn.AdaptivePool\n```\n\n----------------------------------------\n\nTITLE: Initializing and Calling 3D Max Pooling Layer in Equinox\nDESCRIPTION: Definition of the MaxPool3d class in Equinox, including initialization and call methods for 3D max pooling.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/pool.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nequinox.nn.MaxPool3d\n```\n\n----------------------------------------\n\nTITLE: Custom Parameter Initialization for a Linear Layer in Equinox\nDESCRIPTION: Shows how to replace the weight matrix of a linear layer with a custom initialization using tree_at for model surgery.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/tricks.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlinear = eqx.nn.Linear(...)\nnew_weight = jax.random.normal(...)\nwhere = lambda l: l.weight\nnew_linear = eqx.tree_at(where, linear, new_weight)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Calling 3D Adaptive Max Pooling Layer in Equinox\nDESCRIPTION: Definition of the AdaptiveMaxPool3d class in Equinox, including initialization and call methods for 3D adaptive max pooling.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/pool.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nequinox.nn.AdaptiveMaxPool3d\n```\n\n----------------------------------------\n\nTITLE: Initializing and Calling 1D Max Pooling Layer in Equinox\nDESCRIPTION: Definition of the MaxPool1d class in Equinox, including initialization and call methods for 1D max pooling.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/pool.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nequinox.nn.MaxPool1d\n```\n\n----------------------------------------\n\nTITLE: Deserializing Equinox Model with Hyperparameters from a Single File\nDESCRIPTION: Implements a function that loads hyperparameters from the first line of a file as JSON, reconstructs the model skeleton, then deserializes the model weights to populate that skeleton.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/serialisation.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef load(filename):\n    with open(filename, \"rb\") as f:\n        hyperparams = json.loads(f.readline().decode())\n        model = make(key=jr.PRNGKey(0), **hyperparams)\n        return eqx.tree_deserialise_leaves(f, model)\n\n\nnewmodel = load(\"model.eqx\")\n\n# Check that it's loaded correctly:\nassert model.layers[1].weight[2, 2] == newmodel.layers[1].weight[2, 2]\n```\n\n----------------------------------------\n\nTITLE: Initializing and Calling 2D Adaptive Average Pooling Layer in Equinox\nDESCRIPTION: Definition of the AdaptiveAvgPool2d class in Equinox, including initialization and call methods for 2D adaptive average pooling.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/pool.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nequinox.nn.AdaptiveAvgPool2d\n```\n\n----------------------------------------\n\nTITLE: Initializing and Calling 2D Average Pooling Layer in Equinox\nDESCRIPTION: Definition of the AvgPool2d class in Equinox, including initialization and call methods for 2D average pooling.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/pool.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nequinox.nn.AvgPool2d\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using Equinox Embedding Layer in Python\nDESCRIPTION: This snippet showcases the Embedding class from the equinox.nn module. It includes the __init__ method for initializing the embedding layer and the __call__ method for using it in a neural network.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/embedding.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: equinox.nn.Embedding\n    options:\n        members:\n            - __init__\n            - __call__\n```\n\n----------------------------------------\n\nTITLE: Initializing and Calling 1D Adaptive Average Pooling Layer in Equinox\nDESCRIPTION: Definition of the AdaptiveAvgPool1d class in Equinox, including initialization and call methods for 1D adaptive average pooling.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/pool.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nequinox.nn.AdaptiveAvgPool1d\n```\n\n----------------------------------------\n\nTITLE: Initializing Optax Optimizer for CNN Training\nDESCRIPTION: Initializes the AdamW optimizer from Optax with a predefined learning rate for training the CNN model.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/mnist.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\noptim = optax.adamw(LEARNING_RATE)\n```\n\n----------------------------------------\n\nTITLE: Defining GAN Hyperparameters\nDESCRIPTION: Configures essential hyperparameters for GAN training including learning rate, Adam optimizer parameters, batch size, number of training steps, and network architecture dimensions.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/deep_convolutional_gan.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Hyperparameters\nlr = 0.0002\nbeta1 = 0.5\nbeta2 = 0.999\nbatch_size = 32\nnum_steps = 100000\nimage_size = (64, 64, 1)\nheight, width, channels = image_size\nlatent_size = 100\n```\n\n----------------------------------------\n\nTITLE: Importing Equinox 1D Transposed Convolutional Layer Class\nDESCRIPTION: Reference to the Equinox 1D transposed convolutional neural network class, showing the initialization method.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/conv.md#2025-04-22_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n::: equinox.nn.ConvTranspose1d\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Generating and Displaying Image Samples using Jax and Matplotlib in Python\nDESCRIPTION: This code generates image samples using Jax's random number generation and vmap functionality. It then applies transformations, clips the values, reshapes the data, and displays the resulting image using matplotlib.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/score_based_diffusion.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntotal_value = 0\ntotal_size = 0\n\nsample_key = jr.split(sample_key, sample_size**2)\nsample_fn = ft.partial(single_sample_fn, model, int_beta, data_shape, dt0, t1)\nsample = jax.vmap(sample_fn)(sample_key)\nsample = data_mean + data_std * sample\nsample = jnp.clip(sample, data_min, data_max)\nsample = einops.rearrange(\n    sample, \"(n1 n2) 1 h w -> (n1 h) (n2 w)\", n1=sample_size, n2=sample_size\n)\nplt.imshow(sample, cmap=\"Greys\")\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Creating an Interpolation Hierarchy with Abstract Attributes in Equinox\nDESCRIPTION: This example shows how to create an interpolation class hierarchy with intermediate abstract classes that provide partial implementations. It demonstrates the use of eqx.AbstractVar for abstract attributes.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/pattern.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass AbstractInterpolation(eqx.Module):\n    @abc.abstractmethod\n    def __call__(self, x: Array) -> Array:\n        raise NotImplementedError\n\n\nclass AbstractPolynomialInterpolation(AbstractInterpolation)\n    coeffs: eqx.AbstractVar[Array]\n\n    def degree(self) -> int:\n        return len(self.coeffs)\n\n    def __call__(self, x: Array) -> Array:\n        return jnp.polyval(self.coeffs, x)\n\n\nclass CubicInterpolation(AbstractPolynomialInterpolation):\n    coeffs: Array\n\n    def __init__(self, ts: Array, xs: Array):\n        self.coeffs = ...  # some implementation\n```\n\n----------------------------------------\n\nTITLE: Importing Equinox 2D Transposed Convolutional Layer Class\nDESCRIPTION: Reference to the Equinox 2D transposed convolutional neural network class, showing the initialization method.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/conv.md#2025-04-22_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n::: equinox.nn.ConvTranspose2d\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Implementing Dataloader and Data Generation for Spiral Classification\nDESCRIPTION: Define functions for creating a simple dataloader and generating toy dataset of clockwise and anticlockwise spirals.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/train_rnn.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef dataloader(arrays, batch_size):\n    dataset_size = arrays[0].shape[0]\n    assert all(array.shape[0] == dataset_size for array in arrays)\n    indices = np.arange(dataset_size)\n    while True:\n        perm = np.random.permutation(indices)\n        start = 0\n        end = batch_size\n        while end <= dataset_size:\n            batch_perm = perm[start:end]\n            yield tuple(array[batch_perm] for array in arrays)\n            start = end\n            end = start + batch_size\n\n\ndef get_data(dataset_size, *, key):\n    t = jnp.linspace(0, 2 * math.pi, 16)\n    offset = jrandom.uniform(key, (dataset_size, 1), minval=0, maxval=2 * math.pi)\n    x1 = jnp.sin(t + offset) / (1 + t)\n    x2 = jnp.cos(t + offset) / (1 + t)\n    y = jnp.ones((dataset_size, 1))\n\n    half_dataset_size = dataset_size // 2\n    x1 = x1.at[:half_dataset_size].multiply(-1)\n    y = y.at[:half_dataset_size].set(0)\n    x = jnp.stack([x1, x2], axis=-1)\n\n    return x, y\n```\n\n----------------------------------------\n\nTITLE: Initializing and Calling 1D Average Pooling Layer in Equinox\nDESCRIPTION: Definition of the AvgPool1d class in Equinox, including initialization and call methods for 1D average pooling.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/pool.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nequinox.nn.AvgPool1d\n```\n\n----------------------------------------\n\nTITLE: Importing Equinox 1D Convolutional Layer Class\nDESCRIPTION: Reference to the Equinox 1D convolutional neural network class, showing the initialization method.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/conv.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n::: equinox.nn.Conv1d\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing MNIST Dataset with PyTorch\nDESCRIPTION: This snippet uses PyTorch's torchvision to load and preprocess the MNIST dataset, creating data loaders for training and testing.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/mnist.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnormalise_data = torchvision.transforms.Compose(\n    [\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize((0.5,), (0.5,)),\n    ]\n)\ntrain_dataset = torchvision.datasets.MNIST(\n    \"MNIST\",\n    train=True,\n    download=True,\n    transform=normalise_data,\n)\ntest_dataset = torchvision.datasets.MNIST(\n    \"MNIST\",\n    train=False,\n    download=True,\n    transform=normalise_data,\n)\ntrainloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n)\ntestloader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=BATCH_SIZE, shuffle=True\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Calling 3D Average Pooling Layer in Equinox\nDESCRIPTION: Definition of the AvgPool3d class in Equinox, including initialization and call methods for 3D average pooling.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/pool.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nequinox.nn.AvgPool3d\n```\n\n----------------------------------------\n\nTITLE: Importing Equinox 2D Convolutional Layer Class\nDESCRIPTION: Reference to the Equinox 2D convolutional neural network class, showing the initialization method.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/conv.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n::: equinox.nn.Conv2d\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Vision Transformer Implementation in Equinox\nDESCRIPTION: Imports necessary libraries for building a Vision Transformer model, including Equinox for the model architecture, JAX for tensor operations, einops for tensor manipulations, optax for optimization, and PyTorch for dataset loading.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/vision_transformer.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport functools\n\nimport einops  # https://github.com/arogozhnikov/einops\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport numpy as np\nimport optax  # https://github.com/deepmind/optax\n\n# We'll use PyTorch to load the dataset.\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom jaxtyping import Array, Float, PRNGKeyArray\n```\n\n----------------------------------------\n\nTITLE: Initializing and Calling 2D Max Pooling Layer in Equinox\nDESCRIPTION: Definition of the MaxPool2d class in Equinox, including initialization and call methods for 2D max pooling.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/pool.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nequinox.nn.MaxPool2d\n```\n\n----------------------------------------\n\nTITLE: Equinox Enumeration Class Members\nDESCRIPTION: Enumerations in Equinox with two key members: 'where' for conditional operations and 'promote' for type promotion.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/enumerations.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nequinox.Enumeration\n    - where\n    - promote\n```\n\n----------------------------------------\n\nTITLE: Creating Symmetric Weight Matrices in Linear Layers with Equinox\nDESCRIPTION: Shows how to make all linear layers have symmetric weight matrices by implementing a custom Symmetric module and applying it to the model parameters.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/tricks.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Library code\n\nclass Symmetric(eqx.Module):\n  matrix: jax.Array\n\n  def get(self):\n    return 0.5 * (self.matrix + self.matrix.T)\n\ndef is_symmetric(x):\n    return isinstance(x, Symmetric)\n\ndef maybe_symmetric(x):\n    if is_symmetric(x):\n        return x.get()\n    else:\n        return x  # leave everything else unchanged\n\ndef resolve_symmetric(model):\n    return jax.tree_util.tree_map(maybe_symmetric, model, is_leaf=is_symmetric)\n\n# User code\n\nmodel = ...   # any PyTree\nis_linear = lambda x: isinstance(x, eqx.nn.Linear)\nget_weights = lambda m: [x.weight\n                         for x in jax.tree_util.tree_leaves(m, is_leaf=is_linear)\n                         if is_linear(x)]\nsymmetric_model = eqx.tree_at(get_weights, model, replace_fn=Symmetric)\n\n@eqx.filter_grad\ndef loss_fn(model, x, y):\n  model = resolve_symmetric(model)\n  pred_y = jax.vmap(model)(x)\n  return jnp.sum((y - pred_y)**2)\n\ngrads = loss_fn(symmetric_model, ...)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Calling 1D Adaptive Max Pooling Layer in Equinox\nDESCRIPTION: Definition of the AdaptiveMaxPool1d class in Equinox, including initialization and call methods for 1D adaptive max pooling.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/pool.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nequinox.nn.AdaptiveMaxPool1d\n```\n\n----------------------------------------\n\nTITLE: Equinox Normalization Layer References\nDESCRIPTION: Documentation references for normalization layers in the equinox.nn module, including initialization and call methods for each normalization type.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/normalisation.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: equinox.nn.LayerNorm\n    options:\n        members:\n            - __init__\n            - __call__\n\n::: equinox.nn.RMSNorm\n    options:\n        members:\n            - __init__\n            - __call__\n\n::: equinox.nn.GroupNorm\n    options:\n        members:\n            - __init__\n            - __call__\n\n::: equinox.nn.BatchNorm\n    options:\n        members:\n            - __init__\n            - __call__\n\n::: equinox.nn.SpectralNorm\n    options:\n        members:\n            - __init__\n            - __call__\n\n::: equinox.nn.WeightNorm\n    options:\n        members:\n            - __init__\n            - __call__\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Equinox-based CNN on MNIST\nDESCRIPTION: This snippet imports the necessary libraries for building and training a CNN on MNIST using Equinox, JAX, and PyTorch for data loading.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/mnist.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\nimport optax  # https://github.com/deepmind/optax\nimport torch  # https://pytorch.org\nimport torchvision  # https://pytorch.org\nfrom jaxtyping import Array, Float, Int, PyTree  # https://github.com/google/jaxtyping\n```\n\n----------------------------------------\n\nTITLE: Importing Equinox Convolutional Layer Classes\nDESCRIPTION: References to the Equinox convolutional neural network classes including standard convolutions and transposed convolutions of various dimensions.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/conv.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n::: equinox.nn.Conv\n    options:\n        members:\n            - __init__\n            - __call__\n```\n\n----------------------------------------\n\nTITLE: Documenting LSTMCell in Equinox\nDESCRIPTION: Documentation format for the LSTMCell implementation in Equinox, displaying the structure for showing initialization and call methods.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/rnn.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n::: equinox.nn.LSTMCell\n    options:\n        members:\n        members:\n            - __init__\n            - __call__\n```\n\n----------------------------------------\n\nTITLE: Configuring Hyperparameters for Vision Transformer in Python\nDESCRIPTION: Defines the hyperparameters for training a Vision Transformer model, including learning rate, dropout rate, batch size, model architecture dimensions, and dataset characteristics for CIFAR10.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/vision_transformer.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Hyperparameters\nlr = 0.0001\ndropout_rate = 0.1\nbeta1 = 0.9\nbeta2 = 0.999\nbatch_size = 64\npatch_size = 4\nnum_patches = 64\nnum_steps = 100000\nimage_size = (32, 32, 3)\nembedding_dim = 512\nhidden_dim = 256\nnum_heads = 8\nnum_layers = 6\nheight, width, channels = image_size\nnum_classes = 10\n```\n\n----------------------------------------\n\nTITLE: Importing Equinox 3D Transposed Convolutional Layer Class\nDESCRIPTION: Reference to the Equinox 3D transposed convolutional neural network class, showing the initialization method.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/conv.md#2025-04-22_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n::: equinox.nn.ConvTranspose3d\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Equinox GAN Implementation\nDESCRIPTION: Sets up necessary imports for building a GAN, including Equinox for neural networks, JAX for numerical computing, and PyTorch for dataset handling.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/deep_convolutional_gan.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Callable\nfrom typing import Union\n\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\nimport optax\n\n# We'll use PyTorch to load the dataset.\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiple Inheritance with Abstract Classes in Equinox\nDESCRIPTION: Shows how to use multiple inheritance with abstract classes in Equinox to create a complex class hierarchy for a differential equation solver. This example demonstrates the diamond inheritance pattern.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/pattern.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass AbstractSolver(eqx.Module):\n    @abc.abstractmethod\n    def step(...):\n        raise NotImplementedError\n\nclass AbstractAdaptiveSolver(AbstractSolver):\n    tolerance: eqx.AbstractVar[float]\n\nclass AbstractImplicitSolver(AbstractSolver):\n    root_finder: eqx.AbstractVar[AbstractRootFinder]\n\nclass ImplicitEuler(AbstractAdaptiveSolver, AbstractImplicitSolver):\n    tolerance: float\n    root_finder: AbstractRootFinder = Newton()\n\n    def step(...):\n        ...  # some implementation\n\nsolver = ImplicitEuler(tolerance=1e-3)  # this can be instantiated\n```\n\n----------------------------------------\n\nTITLE: Equinox Filter JIT Compilation Functions\nDESCRIPTION: Functions for just-in-time compilation including filter_jit, filter_make_jaxpr, filter_eval_shape, and filter_shard. These provide automatic handling of dynamic/static arguments for JAX arrays.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/transformations.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nequinox.filter_jit\nequinox.filter_make_jaxpr\nequinox.filter_eval_shape\nequinox.filter_shard\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Score-Based Diffusion Model\nDESCRIPTION: Imports necessary Python libraries including diffrax for differential equation solving, einops for tensor manipulation, equinox for neural network building, JAX for numerical computing, and visualization libraries.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/score_based_diffusion.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport array\nimport functools as ft\nimport gzip\nimport os\nimport struct\nimport urllib.request\n\nimport diffrax as dfx  # https://github.com/patrick-kidger/diffrax\nimport einops  # https://github.com/arogozhnikov/einops\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\nimport optax  # https://github.com/deepmind/optax\n```\n\n----------------------------------------\n\nTITLE: Initializing SinusoidalPosEmb Module in Python\nDESCRIPTION: Defines a sinusoidal positional embedding module. It initializes an embedding vector based on the input dimension, using sine and cosine functions for encoding positions.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/unet.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass SinusoidalPosEmb(eqx.Module):\n    emb: jax.Array\n\n    def __init__(self, dim):\n        half_dim = dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        self.emb = jnp.exp(jnp.arange(half_dim) * -emb)\n\n    def __call__(self, x):\n        emb = x * self.emb\n        emb = jnp.concatenate((jnp.sin(emb), jnp.cos(emb)), axis=-1)\n        return emb\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for BERT Language Model Implementation in Python\nDESCRIPTION: This code imports all necessary libraries for implementing a BERT language model with Equinox. It includes functional programming utilities, typing, Equinox, JAX, NumPy, Optax, and HuggingFace libraries for datasets and tokenization.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/bert.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nfrom typing import Dict, List, Mapping, Optional\n\nimport einops  # https://github.com/arogozhnikov/einops\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport optax  # https://github.com/deepmind/optax\nfrom datasets import load_dataset  # https://github.com/huggingface/datasets\nfrom jaxtyping import Array, Float, Int  # https://github.com/google/jaxtyping\nfrom tqdm import notebook as tqdm  # https://github.com/tqdm/tqdm\nfrom transformers import AutoTokenizer  # https://github.com/huggingface/transformers\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Up Toy Problem for Multi-GPU Parallelism in JAX\nDESCRIPTION: This snippet imports necessary libraries, sets up hyperparameters, generates synthetic data, and initializes the model and optimizer for a multi-GPU parallelism example using JAX and Equinox.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/parallelism.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport equinox as eqx\nimport jax\nimport jax.experimental.mesh_utils as mesh_utils\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.sharding as jshard\nimport numpy as np\nimport optax  # https://github.com/deepmind/optax\n\n\n# Hyperparameters\ndataset_size = 64\nchannel_size = 4\nhidden_size = 32\ndepth = 1\nlearning_rate = 3e-4\nnum_steps = 10\nbatch_size = 16  # must be a multiple of our number of devices.\n\n# Generate some synthetic data\nxs = np.random.normal(size=(dataset_size, channel_size))\nys = np.sin(xs)\n\nmodel = eqx.nn.MLP(channel_size, channel_size, hidden_size, depth, key=jr.PRNGKey(6789))\noptim = optax.adam(learning_rate)\nopt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n\n\n# Loss function for a batch of data\ndef compute_loss(model, x, y):\n    pred_y = jax.vmap(model)(x)\n    return jnp.mean((y - pred_y) ** 2)\n\n\n# Simple dataloader; randomly slices our dataset and shuffles between epochs.\n# In NumPy for speed, as our dataset is small enough to fit entirely in host memory.\n#\n# For larger datasets (that require loading from disk) then use PyTorch's `DataLoader`\n# or TensorFlow's `tf.data`.\ndef train_dataloader(arrays, batch_size):\n    dataset_size = arrays[0].shape[0]\n    assert all(array.shape[0] == dataset_size for array in arrays)\n    indices = np.arange(dataset_size)\n    while True:\n        perm = np.random.permutation(indices)\n        start = 0\n        end = batch_size\n        while end <= dataset_size:\n            batch_perm = perm[start:end]\n            yield tuple(array[batch_perm] for array in arrays)\n            start = end\n            end = start + batch_size\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Stateful Operations in Equinox\nDESCRIPTION: Imports the necessary libraries for working with stateful operations in Equinox, including JAX for array operations and Optax for optimization.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/stateful.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport optax  # https://github.com/deepmind/optax\n```\n\n----------------------------------------\n\nTITLE: Correctly Using Non-Array Modules with JAX Scan\nDESCRIPTION: Solution for using modules with non-JAX components in jax.lax.scan by partitioning the model into static and array parts, then recombining during execution.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/faq.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef rollout(mlp, xs):\n    arr, static = eqx.partition(mlp, eqx.is_array)\n    def step(carry, x):\n        mlp = eqx.combine(carry, static)\n        val = mlp(x)\n        carry, _ = eqx.partition(mlp, eqx.is_array)\n        return carry, [val]\n\n    _, scan_out = jax.lax.scan(\n        step,\n        arr,\n        xs\n    )\n    return scan_out\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Equinox Parameter Freezing\nDESCRIPTION: Imports the necessary libraries for building and training a neural network with frozen parameters, including Equinox, JAX, and Optax for optimization.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/frozen_layer.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jrandom\nimport jax.tree_util as jtu\nimport optax  # https://github.com/deepmind/optax\n```\n\n----------------------------------------\n\nTITLE: Documenting GRUCell in Equinox\nDESCRIPTION: Documentation format for the GRUCell implementation in Equinox, showing the structure for displaying initialization and call methods.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/rnn.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n::: equinox.nn.GRUCell\n    options:\n        members:\n            - __init__\n            - __call__\n```\n\n----------------------------------------\n\nTITLE: Simple Approach to Monitor JAX Compilation\nDESCRIPTION: A simple method to detect when JAX is recompiling a function by adding a print statement, which executes only during tracing/compilation.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/faq.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@eqx.filter_jit\ndef your_function(x, y, z):\n    print(\"Compiling!\")\n    ...  # rest of your code here\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for RNN Training with Equinox\nDESCRIPTION: Import necessary libraries including Equinox, JAX, NumPy, and Optax for building and training an RNN model.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/train_rnn.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport math\n\nimport equinox as eqx\nimport jax\nimport jax.lax as lax\nimport jax.numpy as jnp\nimport jax.random as jrandom\nimport numpy as np\nimport optax  # https://github.com/deepmind/optax\n```\n\n----------------------------------------\n\nTITLE: Creating Toy Data and Dataloader for Training\nDESCRIPTION: Defines functions to generate synthetic linear data and a dataloader that yields batches of data for training. The data follows a simple linear relationship y = 5x - 2.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/frozen_layer.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Toy data\ndef get_data(dataset_size, *, key):\n    x = jrandom.normal(key, (dataset_size, 1))\n    y = 5 * x - 2\n    return x, y\n\n\n# Toy dataloader\ndef dataloader(arrays, batch_size, *, key):\n    dataset_size = arrays[0].shape[0]\n    assert all(array.shape[0] == dataset_size for array in arrays)\n    indices = jnp.arange(dataset_size)\n    while True:\n        perm = jrandom.permutation(key, indices)\n        (key,) = jrandom.split(key, 1)\n        start = 0\n        end = batch_size\n        while end < dataset_size:\n            batch_perm = perm[start:end]\n            yield tuple(array[batch_perm] for array in arrays)\n            start = end\n            end = start + batch_size\n```\n\n----------------------------------------\n\nTITLE: Inspecting MNIST Data Shape and Content\nDESCRIPTION: This snippet demonstrates how to inspect the shape and content of the MNIST data loaded using PyTorch data loaders.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/mnist.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndummy_x, dummy_y = next(iter(trainloader))\ndummy_x = dummy_x.numpy()\ndummy_y = dummy_y.numpy()\nprint(dummy_x.shape)  # 64x1x28x28\nprint(dummy_y.shape)  # 64\nprint(dummy_y)\n```\n\n----------------------------------------\n\nTITLE: Method Invocation Example with AbstractPolynomialInterpolation\nDESCRIPTION: This snippet demonstrates calling a method on an AbstractPolynomialInterpolation instance, showing how the abstract/final pattern ensures you know exactly which implementation is being called.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/pattern.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef foo(interp: AbstractPolynomialInterpolation)\n    ... = interp.degree()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Runtime Error Handling in Python\nDESCRIPTION: This example demonstrates how Equinox's error handling differs from JAX's checkify. In this case, an error is raised when 'h' is negative, preventing an infinite loop, unlike checkify which would only raise errors after completing the computation.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/errors.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nif h < 0:\n    error()\nwhile t < t_max:\n    t += h\n```\n\n----------------------------------------\n\nTITLE: Visualizing MNIST Sample Images with Matplotlib\nDESCRIPTION: Extracts a batch of images from the dataloader and plots them in a grid to visualize the MNIST digits that will be used for training the GAN.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/deep_convolutional_gan.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimages, labels = next(iter(dataloader))\n\nplot_sample = images.permute(0, 2, 3, 1)\n\nfig, axes = plt.subplots(nrows=4, ncols=8, figsize=(8, 4))\nfig.suptitle(\"Sample Images\", y=1.02, fontsize=14)\n\nfor ax, image in zip(sum(axes.tolist(), []), plot_sample):\n    ax.imshow(image, cmap=\"gray\")\n    ax.set_axis_off()\n\nplt.subplots_adjust(wspace=0.1, hspace=0.1)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Anti-pattern: Splitting Initialization Across Multiple Classes\nDESCRIPTION: This example demonstrates an anti-pattern where initialization is split across multiple classes in a hierarchy, making the code less readable and more prone to bugs.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/pattern.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass AbstractPolynomialInterpolation(AbstractInterpolation)\n    coeffs: Array\n\n    def __init__(self, coeffs: Array):\n        self.coeffs = coeffs\n\n    def degree(self) -> int:\n        return len(self.coeffs)\n\n    def __call__(self, x: Array) -> Array:\n        return jnp.polyval(self.coeffs, x)\n\n\nclass CubicInterpolation(AbstractPolynomialInterpolation):\n    def __init__(self, ts: Array, xs: Array):\n        coeffs = ...  # some implementation\n        super().__init__(coeffs)\n```\n\n----------------------------------------\n\nTITLE: Automatic Parent Class __check_init__ Calling in Equinox (Python)\nDESCRIPTION: Shows how __check_init__ is automatically called for parent classes in Equinox modules, eliminating the need for explicit super() calls.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/module/advanced_fields.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Parent(eqx.Module):\n    def __check_init__(self):\n        print(\"Parent\")\n\nclass Child(Parent):\n    def __check_init__(self):\n        print(\"Child\")\n\nChild()  # prints out both Child and Parent\n```\n\n----------------------------------------\n\nTITLE: Implementing Invariant Checking in Equinox Module (Python)\nDESCRIPTION: Demonstrates how to use the __check_init__ method in Equinox to check invariants after initialization. This method is automatically called and cannot be overridden by subclasses.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/module/advanced_fields.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Positive(eqx.Module):\n    x: int\n\n    def __check_init__(self):\n        if self.x <= 0:\n            raise ValueError(\"Oh no!\")\n```\n\n----------------------------------------\n\nTITLE: Setting Hyperparameters for MNIST CNN Training\nDESCRIPTION: This snippet defines hyperparameters for training the CNN, including batch size, learning rate, number of steps, and random seed.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/mnist.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nBATCH_SIZE = 64\nLEARNING_RATE = 3e-4\nSTEPS = 300\nPRINT_EVERY = 30\nSEED = 5678\n\nkey = jax.random.PRNGKey(SEED)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Assignment Restriction in __check_init__ (Python)\nDESCRIPTION: Illustrates that assignment is not allowed within the __check_init__ method of Equinox modules, enforcing its use for invariant checking only.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/module/advanced_fields.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MyModule(eqx.Module):\n    foo: int\n\n    def __check_init__(self):\n        self.foo = 1  # will raise an error\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Strict Module Subclassing in Equinox\nDESCRIPTION: This snippet shows how Equinox enforces the abstract/final pattern with the strict=True flag, preventing field definition across multiple classes in a hierarchy.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/pattern.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Foo(eqx.Module, strict=True):\n    x: int\n# ...so far so good...\n\nclass Bar(Foo, strict=True):\n    y: int\n# ...error raised here: can't define fields in two different classes.\n```\n\n----------------------------------------\n\nTITLE: Executing CNN Training with Configured Parameters\nDESCRIPTION: Executes the training function with the model and data loaders, using the initialized optimizer and predefined training parameters like steps and print frequency.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/mnist.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmodel = train(model, trainloader, testloader, optim, STEPS, PRINT_EVERY)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating __post_init__ Limitation in Dataclasses (Python)\nDESCRIPTION: Illustrates a potential bug when using __post_init__ in dataclasses, where a subclass's __init__ method can bypass the parent's __post_init__ checks.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/module/advanced_fields.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Parent(eqx.Module):\n    x: int\n\n    def __post_init__(self):\n        if self.x <= 0:\n            raise ValueError(\"Oh no!\")\n\nclass Child(Parent):\n    x_as_str: str\n\n    def __init__(self, x):\n        self.x = x\n        self.x_as_str = str(x)\n\nChild(-1)  # No error!\n```\n\n----------------------------------------\n\nTITLE: Demonstrating TypeError with JAX and Equinox Functions\nDESCRIPTION: Example showing how regular jax.jit fails with Equinox models containing non-JAX types (like functions), while equinox.filter_jit handles them correctly.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/faq.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport equinox as eqx\n\ndef loss_fn(model, x, y):\n    return ((model(x) - y) ** 2).mean()\n\nmodel = eqx.nn.Lambda(lambda x: x)\nmodel = eqx.nn.MLP(2, 2, 2, 2, key=jax.random.PRNGKey(0))\n\nx = jax.numpy.arange(2)\ny = x * x\n\ntry:\n    jax.jit(loss_fn)(model, x, y) # error\nexcept TypeError as e:\n    print(e)\n\neqx.filter_jit(loss_fn)(model, x, y) # ok\n```\n\n----------------------------------------\n\nTITLE: Executing Model Evaluation\nDESCRIPTION: A simple code snippet that calls the evaluate function with the model and testloader to get performance metrics.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/mnist.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nevaluate(model, testloader)\n```\n\n----------------------------------------\n\nTITLE: Incorrect Way to Share Layers in Equinox\nDESCRIPTION: Example of improper layer sharing in Equinox that creates two separate layers with the same initial values instead of truly shared parameters.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/faq.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Buggy code!\nclass Module(eqx.Module):\n    linear1: eqx.nn.Linear\n    linear2: eqx.nn.Linear\n\n    def __init__(...):\n        shared_linear = eqx.nn.Linear(...)\n        self.linear1 = shared_linear\n        self.linear2 = shared_linear\n```\n\n----------------------------------------\n\nTITLE: Setting up Equinox Development Environment\nDESCRIPTION: Instructions for cloning and installing Equinox in development mode from a forked repository.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/your-username-here/equinox.git\ncd equinox\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Incorrect Use of Non-Array Modules with JAX Transformations\nDESCRIPTION: Example showing incorrect usage of modules with JAX transformations like scan, which will result in a TypeError for non-JAX types.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/faq.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmlp = eqx.nn.MLP(...)\n\ndef rollout(mlp, xs):\n    def step(carry, x):\n        mlp = carry\n        val = mlp(x)\n        carry = mlp\n        return carry, [val]\n\n    _, scan_out = jax.lax.scan(\n        step,\n        [mlp],\n        xs\n    )\n\n    return scan_out\n\nkey, subkey = jax.random.split(key)\nvals = rollout(mlp, jax.random.normal(key=subkey, shape=(200, 3)))\n```\n\n----------------------------------------\n\nTITLE: Citing Equinox in Academic Work\nDESCRIPTION: BibTeX citation for the Equinox library, to be used when referencing the library in academic papers or research.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{kidger2021equinox,\n    author={Patrick Kidger and Cristian Garcia},\n    title={{E}quinox: neural networks in {JAX} via callable {P}y{T}rees and filtered transformations},\n    year={2021},\n    journal={Differentiable Programming workshop at Neural Information Processing Systems 2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit Hooks\nDESCRIPTION: Commands to install and setup pre-commit hooks for code formatting, linting with ruff, and typechecking with pyright.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Example Causing JIT Compilation Slowdown\nDESCRIPTION: Code pattern that causes slow compilation in JAX due to loop unrolling during tracing. This pattern should be avoided in favor of jax.lax.scan for iterative operations.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/faq.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f(x):\n    for i in range(100):\n        x = my_complicated_function(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Installing Equinox using pip\nDESCRIPTION: Command to install Equinox using pip package manager. Requires Python 3.10 or higher.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install equinox\n```\n\n----------------------------------------\n\nTITLE: Pushing Changes to Fork\nDESCRIPTION: Command to push local changes back to the forked repository.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/CONTRIBUTING.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit push\n```\n\n----------------------------------------\n\nTITLE: Running the RNN Training for Spiral Classification\nDESCRIPTION: Execute the main function to train the RNN model on the spiral classification task.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/train_rnn.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmain()  # All right, let's run the code.\n```\n\n----------------------------------------\n\nTITLE: Installing Equinox via pip\nDESCRIPTION: Command to install Equinox package using pip package manager. Requires Python 3.10+.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install equinox\n```\n\n----------------------------------------\n\nTITLE: Academic Citation for DCGAN Paper\nDESCRIPTION: BibTeX citation for the original DCGAN paper 'Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks' by Radford et al.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/deep_convolutional_gan.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{radford2015unsupervised,\n  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},\n  title = {Unsupervised Representation Learning with Deep Convolutional Generative\n  Adversarial Networks},\n  url = {http://arxiv.org/abs/1511.06434},\n  year = 2015\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Main Function in Python\nDESCRIPTION: This code snippet calls the main() function, which likely contains the implementation for the image generation and display process.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/examples/score_based_diffusion.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmain()\n```\n\n----------------------------------------\n\nTITLE: Clearing Internal Caches in Equinox (Python)\nDESCRIPTION: The clear_caches function is used to clear internal caches in the Equinox library. It takes no arguments and returns None. This function is primarily used for debugging purposes or when you need to free up memory.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/caches.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: equinox.clear_caches\n```\n\n----------------------------------------\n\nTITLE: Building and Serving Documentation\nDESCRIPTION: Commands to install documentation dependencies, build the documentation, and serve it locally for preview.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/CONTRIBUTING.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install -e '.[docs]'\nmkdocs build\nmkdocs serve\n```\n\n----------------------------------------\n\nTITLE: Handling NaNs with Double Where Trick in JAX\nDESCRIPTION: A technique to prevent NaNs when using operations like jnp.log or jnp.sqrt with jnp.where conditionals. This approach brackets the computation with a where clause on both sides to prevent NaNs from being created during backpropagation.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/debug.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsafe_x = jnp.where(x > 0, x, 1)\ny = jnp.where(x > 0, jnp.log(safe_x), 0)\n```\n\n----------------------------------------\n\nTITLE: Citing Equinox Library in BibTeX Format\nDESCRIPTION: This BibTeX entry provides the correct citation format for the Equinox library paper. It includes author names, title, year, and publication venue information.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/.citation.md#2025-04-22_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{kidger2021equinox,\n    author={Patrick Kidger and Cristian Garcia},\n    title={{E}quinox: neural networks in {JAX} via callable {P}y{T}rees and filtered transformations},\n    year={2021},\n    journal={Differentiable Programming workshop at Neural Information Processing Systems 2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Equinox 3D Convolutional Layer Class\nDESCRIPTION: Reference to the Equinox 3D convolutional neural network class, showing the initialization method.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/docs/api/nn/conv.md#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n::: equinox.nn.Conv3d\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Running Tests for Equinox\nDESCRIPTION: Commands to install pytest and run the test suite to verify changes.\nSOURCE: https://github.com/patrick-kidger/equinox/blob/main/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install pytest\npytest\n```"
  }
]
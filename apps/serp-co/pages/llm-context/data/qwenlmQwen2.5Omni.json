[
  {
    "owner": "qwenlm",
    "repo": "qwen2.5-omni",
    "content": "TITLE: Using Qwen2.5-Omni with Transformers\nDESCRIPTION: Example code demonstrating how to use the Qwen2.5-Omni model for multimodal processing including text, audio, image, and video inputs.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport soundfile as sf\n\nfrom transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\nfrom qwen_omni_utils import process_mm_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\", torch_dtype=\"auto\", device_map=\"auto\")\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n# model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2.5-Omni-7B\",\n#     torch_dtype=\"auto\",\n#     device_map=\"auto\",\n#     attn_implementation=\"flash_attention_2\",\n# )\n\nprocessor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\n\nconversation = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"video\", \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4\"},\n        ],\n    },\n]\n\n# set use audio in video\nUSE_AUDIO_IN_VIDEO = True\n\n# Preparation for inference\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n\n# Inference: Generation of the output text and audio\ntext_ids, audio = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ntext = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(text)\nsf.write(\n    \"output.wav\",\n    audio.reshape(-1).detach().cpu().numpy(),\n    samplerate=24000,\n)\n```\n\n----------------------------------------\n\nTITLE: Processing Inputs and Generating Output with Qwen2.5-Omni in Python\nDESCRIPTION: Prepares the conversation input, processes multimodal information, runs inference, and generates text and audio outputs. Uses the processor and model to handle the input and produce the results.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README_CN.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Preparation for inference\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n\n# Inference: Generation of the output text and audio\ntext_ids, audio = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ntext = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(text)\nsf.write(\n    \"output.wav\",\n    audio.reshape(-1).detach().cpu().numpy(),\n    samplerate=24000,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Qwen2.5-Omni Model and Processor in Python\nDESCRIPTION: Loads the Qwen2.5-Omni model and processor from pretrained weights. Includes an optional configuration for using flash attention 2 for improved inference speed and lower memory usage.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README_CN.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# default: Load the model on the available device(s)\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\", torch_dtype=\"auto\", device_map=\"auto\")\n\n# 我们建议启用 flash_attention_2 以获取更快的推理速度以及更低的显存占用.\n# model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2.5-Omni-7B\",\n#     torch_dtype=\"auto\",\n#     device_map=\"auto\",\n#     attn_implementation=\"flash_attention_2\",\n# )\n\nprocessor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\n```\n\n----------------------------------------\n\nTITLE: Qwen2Omni Implementation with Multi-Modal Processing\nDESCRIPTION: Demonstrates how to use Qwen2Omni for processing images, videos, and audio inputs. Shows various input formats including local files, URLs, and base64 encoded media.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/qwen-omni-utils/README.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import Qwen2_5OmniForConditionalGeneration, AutoProcessor\nfrom qwen_omni_utils import process_mm_info\n\n\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\nmessages = [\n    # Image\n    ## Local file path\n    [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}],\n    ## Image URL\n    [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"http://path/to/your/image.jpg\"}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}],\n    ## Base64 encoded image\n    [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"data:image;base64,/9j/...\"}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}],\n    ## PIL.Image.Image\n    [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": pil_image}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}],\n    ## Model dynamically adjusts image size, specify dimensions if required.\n    [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\", \"resized_height\": 280, \"resized_width\": 420}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}],\n    # Video\n    ## Local video path\n    [{\"role\": \"user\", \"content\": [{\"type\": \"video\", \"video\": \"file:///path/to/video1.mp4\"}, {\"type\": \"text\", \"text\": \"Describe this video.\"}]}],\n    ## Local video frames\n    [{\"role\": \"user\", \"content\": [{\"type\": \"video\", \"video\": [\"file:///path/to/extracted_frame1.jpg\", \"file:///path/to/extracted_frame2.jpg\", \"file:///path/to/extracted_frame3.jpg\"],}, {\"type\": \"text\", \"text\": \"Describe this video.\"},],}],\n    ## Model dynamically adjusts video nframes, video height and width. specify args if required.\n    [{\"role\": \"user\", \"content\": [{\"type\": \"video\", \"video\": \"file:///path/to/video1.mp4\", \"fps\": 2.0, \"resized_height\": 280, \"resized_width\": 280}, {\"type\": \"text\", \"text\": \"Describe this video.\"}]}],\n    # Audio\n    ## Local audio path\n    [{\"role\": \"user\", \"content\": [{\"type\": \"audio\", \"audio\": \"file:///path/to/audio1.wav\"}, {\"type\": \"text\", \"text\": \"Describe this audio.\"}]}],\n    ## Numpy format audio\n    [{\"role\": \"user\", \"content\": [{\"type\": \"audio\", \"audio\": numpy_audio}, {\"type\": \"text\", \"text\": \"Describe this audio.\"}]}],\n    ## Remote audio\n    [{\"role\": \"user\", \"content\": [{\"type\": \"audio\", \"audio\": \"https://path/to/audio.wav\"}, {\"type\": \"text\", \"text\": \"Describe this audio.\"}]}],\n]\n\nprocessor = AutoProcessor.from_pretrained(model_path)\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(model_path, torch_dtype=\"auto\", device_map=\"auto\")\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\naudios, images, videos = process_mm_info(messages)\ninputs = processor(text=text, images=images, videos=videos, audio=audios, padding=True, return_tensors=\"pt\")\nprint(inputs)\ngenerated_ids, generate_wav = model.generate(**inputs)\nprint(generated_ids)\n```\n\n----------------------------------------\n\nTITLE: Implementing Video Inference Function\nDESCRIPTION: Defines a function to process video content with the Qwen2.5-Omni model. Handles multimodal inputs including video and text, and returns generated responses.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/cookbooks/video_information_extracting.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom qwen_omni_utils import process_mm_info\n\ndef inference(video_path, prompt, sys_prompt):\n    messages = [\n        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": sys_prompt}]},\n        {\"role\": \"user\", \"content\": [\n                {\"type\": \"video\", \"video\": video_path},\n                {\"type\": \"text\", \"text\": prompt},\n            ]\n        },\n    ]\n    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    audios, images, videos = process_mm_info(messages, use_audio_in_video=False)\n    inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=False)\n    inputs = inputs.to(model.device).to(model.dtype)\n\n    output = model.generate(**inputs, use_audio_in_video=False, return_audio=False)\n\n    text = processor.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n    return text\n```\n\n----------------------------------------\n\nTITLE: Batch Inference with Qwen2.5-Omni\nDESCRIPTION: Example of batch processing with mixed input types including text, images, audio, and video.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Sample messages for batch inference\n\n# Conversation with video only\nconversation1 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"video\", \"video\": \"/path/to/video.mp4\"},\n        ]\n    }\n]\n\n# Conversation with audio only\nconversation2 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"audio\", \"audio\": \"/path/to/audio.wav\"},\n        ]\n    }\n]\n\n# Conversation with pure text\nconversation3 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"who are you?\"\n    }\n]\n\n\n# Conversation with mixed media\nconversation4 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"/path/to/image.jpg\"},\n            {\"type\": \"video\", \"video\": \"/path/to/video.mp4\"},\n            {\"type\": \"audio\", \"audio\": \"/path/to/audio.wav\"},\n            {\"type\": \"text\", \"text\": \"What are the elements can you see and hear in these medias?\"},\n        ],\n    }\n]\n\n# Combine messages for batch processing\nconversations = [conversation1, conversation2, conversation3, conversation4]\n\n# set use audio in video\nUSE_AUDIO_IN_VIDEO = True\n\n# Preparation for batch inference\ntext = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n\n# Batch Inference\ntext_ids = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)\ntext = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(text)\n```\n\n----------------------------------------\n\nTITLE: API Inference with Qwen2.5-Omni Python\nDESCRIPTION: Example of using the OpenAI API to interact with Qwen2.5-Omni. This code snippet demonstrates how to set up the client, create a chat completion, and process the response including text and audio output.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport base64\nimport numpy as np\nimport soundfile as sf\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"your_api_key\",\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"video_url\", \"video_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4\"},\n        ],\n    },\n]\n\n# Qwen-Omni only supports stream mode\ncompletion = client.chat.completions.create(\n    model=\"qwen-omni-turbo\",\n    messages=messages,\n    modalities=[\"text\", \"audio\"],\n    audio={\n        \"voice\": \"Cherry\", # Cherry, Ethan, Serena, Chelsie is available\n        \"format\": \"wav\"\n    },\n    stream=True,\n    stream_options={\"include_usage\": True}\n)\n\ntext = []\naudio_string = \"\"\nfor chunk in completion:\n    if chunk.choices:\n        if hasattr(chunk.choices[0].delta, \"audio\"):\n            try:\n                audio_string += chunk.choices[0].delta.audio[\"data\"]\n            except Exception as e:\n                text.append(chunk.choices[0].delta.audio[\"transcript\"])\n    else:\n        print(chunk.usage)\n\nprint(\"\".join(text))\nwav_bytes = base64.b64decode(audio_string)\nwav_array = np.frombuffer(wav_bytes, dtype=np.int16)\nsf.write(\"output.wav\", wav_array, samplerate=24000)\n```\n\n----------------------------------------\n\nTITLE: Loading Qwen2.5-Omni with Flash-Attention 2 Python\nDESCRIPTION: Demonstrates how to load and run the Qwen2.5-Omni model using Flash-Attention 2 for improved performance. The 'attn_implementation' parameter is set to 'flash_attention_2' when loading the model.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import Qwen2_5OmniForConditionalGeneration\n\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-Omni-7B\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"flash_attention_2\",\n)\n```\n\n----------------------------------------\n\nTITLE: Customizing Qwen2.5-Omni Conversation Template Python\nDESCRIPTION: Example of customizing the conversation template for Qwen2.5-Omni to control output or modify personality settings. This approach is suggested when prompt settings are not supported for audio output.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are a shopping guide, now responsible for introducing various products.\"},\n        ],\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Sure, I got it.\"},\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Who are you?\"},\n        ],\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Preparing Multimodal Conversation Input for Qwen2.5-Omni in Python\nDESCRIPTION: Sets up a conversation structure with system and user messages, including a video input. Defines a flag for using audio in video processing.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README_CN.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconversation = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"video\", \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4\"},\n        ],\n    },\n]\n\n# set use audio in video\nUSE_AUDIO_IN_VIDEO = True\n```\n\n----------------------------------------\n\nTITLE: Qwen2VL Implementation for Vision Language Processing\nDESCRIPTION: Shows how to use Qwen2VL for processing images and videos. Includes examples of different input formats and size specifications.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/qwen-omni-utils/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom qwen_omni_utils import process_vision_info\n\n\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\nmessages = [\n    # Image\n    ## Local file path\n    [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}],\n    ## Image URL\n    [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"http://path/to/your/image.jpg\"}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}],\n    ## Base64 encoded image\n    [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"data:image;base64,/9j/...\"}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}],\n    ## PIL.Image.Image\n    [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": pil_image}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}],\n    ## Model dynamically adjusts image size, specify dimensions if required.\n    [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\", \"resized_height\": 280, \"resized_width\": 420}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}],\n    # Video\n    ## Local video path\n    [{\"role\": \"user\", \"content\": [{\"type\": \"video\", \"video\": \"file:///path/to/video1.mp4\"}, {\"type\": \"text\", \"text\": \"Describe this video.\"}]}],\n    ## Local video frames\n    [{\"role\": \"user\", \"content\": [{\"type\": \"video\", \"video\": [\"file:///path/to/extracted_frame1.jpg\", \"file:///path/to/extracted_frame2.jpg\", \"file:///path/to/extracted_frame3.jpg\"],}, {\"type\": \"text\", \"text\": \"Describe this video.\"},],}],\n    ## Model dynamically adjusts video nframes, video height and width. specify args if required.\n    [{\"role\": \"user\", \"content\": [{\"type\": \"video\", \"video\": \"file:///path/to/video1.mp4\", \"fps\": 2.0, \"resized_height\": 280, \"resized_width\": 280}, {\"type\": \"text\", \"text\": \"Describe this video.\"}]}],\n]\n\nprocessor = AutoProcessor.from_pretrained(model_path)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(model_path, torch_dtype=\"auto\", device_map=\"auto\")\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nimages, videos = process_vision_info(messages)\ninputs = processor(text=text, images=images, videos=videos, padding=True, return_tensors=\"pt\")\nprint(inputs)\ngenerated_ids = model.generate(**inputs)\nprint(generated_ids)\n```\n\n----------------------------------------\n\nTITLE: Qwen2.5VL Implementation with Advanced Video Processing\nDESCRIPTION: Demonstrates Qwen2.5VL usage with support for video token limits and advanced processing options. Includes environment variable configuration for video processing.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/qwen-omni-utils/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\nfrom qwen_omni_utils import process_vision_info\n\n\n# You can set the maximum tokens for a video through the environment variable VIDEO_MAX_PIXELS\n# based on the maximum tokens that the model can accept. \n# export VIDEO_MAX_PIXELS = 32000 * 28 * 28 * 0.9\n\n\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\nmessages = [\n    # Image\n    ## Local file path\n    [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}],\n    ## Image URL\n    [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"http://path/to/your/image.jpg\"}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}],\n    ## Base64 encoded image\n    [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"data:image;base64,/9j/...\"}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}],\n    ## PIL.Image.Image\n    [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": pil_image}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}],\n    ## Model dynamically adjusts image size, specify dimensions if required.\n    [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\", \"resized_height\": 280, \"resized_width\": 420}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}],\n    # Video\n    ## Local video path\n    [{\"role\": \"user\", \"content\": [{\"type\": \"video\", \"video\": \"file:///path/to/video1.mp4\"}, {\"type\": \"text\", \"text\": \"Describe this video.\"}]}],\n    ## Local video frames\n    [{\"role\": \"user\", \"content\": [{\"type\": \"video\", \"video\": [\"file:///path/to/extracted_frame1.jpg\", \"file:///path/to/extracted_frame2.jpg\", \"file:///path/to/extracted_frame3.jpg\"],}, {\"type\": \"text\", \"text\": \"Describe this video.\"},],}],\n    ## Model dynamically adjusts video nframes, video height and width. specify args if required.\n    [{\"role\": \"user\", \"content\": [{\"type\": \"video\", \"video\": \"file:///path/to/video1.mp4\", \"fps\": 2.0, \"resized_height\": 280, \"resized_width\": 280}, {\"type\": \"text\", \"text\": \"Describe this video.\"}]}],\n]\n\nprocessor = AutoProcessor.from_pretrained(model_path)\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_path, torch_dtype=\"auto\", device_map=\"auto\")\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nimages, videos, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\ninputs = processor(text=text, images=images, videos=videos, padding=True, return_tensors=\"pt\", **video_kwargs)\nprint(inputs)\ngenerated_ids = model.generate(**inputs)\nprint(generated_ids)\n```\n\n----------------------------------------\n\nTITLE: Changing Voice Type in Qwen2.5-Omni Python\nDESCRIPTION: Demonstrates how to change the voice type of the output audio in Qwen2.5-Omni. The model supports two voice types: Chelsie (female) and Ethan (male). The 'speaker' parameter is used to specify the desired voice type.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntext_ids, audio = model.generate(**inputs, speaker=\"Chelsie\")\n```\n\nLANGUAGE: python\nCODE:\n```\ntext_ids, audio = model.generate(**inputs, speaker=\"Ethan\")\n```\n\n----------------------------------------\n\nTITLE: Querying Qwen2.5-Omni API with Multimodal Input\nDESCRIPTION: Example curl command demonstrating how to query the Qwen2.5-Omni API with multimodal inputs including image, audio, and text. Shows the JSON structure required for sending multiple content types in a single request.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n    \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png\"}},\n        {\"type\": \"audio_url\", \"audio_url\": {\"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/cough.wav\"}},\n        {\"type\": \"text\", \"text\": \"What is the text in the illustrate ans what it the sound in the audio?\"}\n    ]}\n    ]\n    }'\n```\n\n----------------------------------------\n\nTITLE: Loading Qwen2.5-Omni Model and Processor\nDESCRIPTION: Initializes the Qwen2.5-Omni model and processor with specific configurations including flash attention and bfloat16 precision.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/cookbooks/video_information_extracting.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\n\nmodel_path = \"Qwen/Qwen2.5-Omni-7B\"\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n)\nprocessor = Qwen2_5OmniProcessor.from_pretrained(model_path)\n```\n\n----------------------------------------\n\nTITLE: Loading Qwen2.5-Omni Model and Processor\nDESCRIPTION: Loads the Qwen2.5-Omni-7B model and its associated processor from Hugging Face. Configures the model to use bfloat16 precision, automatic device mapping, and Flash Attention 2 for optimal performance.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/cookbooks/screen_recording_interaction.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\n\nmodel_path = \"Qwen/Qwen2.5-Omni-7B\"\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n)\nprocessor = Qwen2_5OmniProcessor.from_pretrained(model_path)\n```\n\n----------------------------------------\n\nTITLE: Basic Qwen2.5-Omni Model Usage\nDESCRIPTION: Code snippet showing how to import and use Qwen2.5-Omni model with Transformers and qwen_omni_utils\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README_CN.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport soundfile as sf\n\nfrom transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\nfrom qwen_omni_utils import process_mm_info\n```\n\n----------------------------------------\n\nTITLE: Processing Video Questions\nDESCRIPTION: Three example implementations of video analysis using the inference function to answer specific questions about drink bottles in a shopping video.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/cookbooks/video_information_extracting.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nvideo_path = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/shopping.mp4\"\nprompt = \"How many kind of drinks can you see in the video?\"\n\ndisplay(Video(video_path, width=640, height=360))\n\nresponse = inference(video_path, prompt=prompt, sys_prompt=\"You are a helpful assistant.\")\nprint(response[0])\n```\n\n----------------------------------------\n\nTITLE: Understanding Browser in Video with Qwen2.5-Omni\nDESCRIPTION: Demonstrates using Qwen2.5-Omni to identify the browser used in a screen recording by processing the video and a specific prompt asking about the browser.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/cookbooks/screen_recording_interaction.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nvideo_path = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/screen.mp4\"\nprompt = \"What the browser is used in this video?\"\n\ndisplay(Video(video_path, width=640, height=360))\n\n## Use a local HuggingFace model to inference.\nresponse = inference(video_path, prompt=prompt, sys_prompt=\"You are a helpful assistant.\")\nprint(response[0])\n```\n\n----------------------------------------\n\nTITLE: Extracting Paper Authors with OCR using Qwen2.5-Omni\nDESCRIPTION: Shows how to use Qwen2.5-Omni's OCR capabilities to identify the authors of a paper shown in a screen recording by processing the video with a specific prompt.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/cookbooks/screen_recording_interaction.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nvideo_path = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/screen.mp4\"\nprompt = \"Who is the authors of this paper?\"\n\ndisplay(Video(video_path, width=640, height=360))\n\n## Use a local HuggingFace model to inference.\nresponse = inference(video_path, prompt=prompt, sys_prompt=\"You are a helpful assistant.\")\nprint(response[0])\n```\n\n----------------------------------------\n\nTITLE: Summarizing Paper Content with Qwen2.5-Omni\nDESCRIPTION: Demonstrates using Qwen2.5-Omni to generate a concise summary of a research paper shown in a screen recording by processing the video with a summarization prompt.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/cookbooks/screen_recording_interaction.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nvideo_path = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/screen.mp4\"\nprompt = \"Summarize this paper in short.\"\n\ndisplay(Video(video_path, width=640, height=360))\n\n## Use a local HuggingFace model to inference.\nresponse = inference(video_path, prompt=prompt, sys_prompt=\"You are a helpful assistant.\")\nprint(response[0])\n```\n\n----------------------------------------\n\nTITLE: Translating Paper Abstract to Chinese with Qwen2.5-Omni\nDESCRIPTION: Shows how to use Qwen2.5-Omni to translate the abstract of a research paper from English to Chinese by processing the screen recording and a translation prompt.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/cookbooks/screen_recording_interaction.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nvideo_path = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/screen.mp4\"\nprompt = \"Please trranslate the abstract of paper into Chinese.\"\n\ndisplay(Video(video_path, width=640, height=360))\n\n## Use a local HuggingFace model to inference.\nresponse = inference(video_path, prompt=prompt, sys_prompt=\"You are a helpful assistant.\")\nprint(response[0])\n```\n\n----------------------------------------\n\nTITLE: Running Qwen2.5-Omni Inference with vLLM\nDESCRIPTION: Commands for running inference with Qwen2.5-Omni using vLLM. Provides examples for text-only output on single and multiple GPUs, as well as audio output generation with customizable voice types.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n# git clone -b qwen2_omni_public https://github.com/fyabc/vllm.git\n# cd vllm\n# git checkout 729feed3ec2beefe63fda30a345ef363d08062f8\n# cd examples/offline_inference/qwen2_5_omni/\n\n# only text output for single GPU\npython end2end.py --model Qwen/Qwen2.5-Omni-7B --prompt audio-in-video-v2 --enforce-eager --thinker-only\n\n# only text output for multi GPUs (example in 4 GPUs)\npython end2end.py --model Qwen/Qwen2.5-Omni-7B --prompt audio-in-video-v2 --enforce-eager --thinker-only --thinker-devices [0,1,2,3] --thinker-gpu-memory-utilization 0.9 \n\n# audio output for single GPU\npython end2end.py --model Qwen/Qwen2.5-Omni-7B --prompt audio-in-video-v2 --enforce-eager --do-wave --voice-type Chelsie --warmup-voice-type Chelsie --output-dir output_wav\n\n# audio output for multi GPUs (example in 4 GPUs)\npython end2end.py --model Qwen/Qwen2.5-Omni-7B --prompt audio-in-video-v2 --enforce-eager --do-wave --voice-type Chelsie --warmup-voice-type Chelsie --thinker-devices [0,1] --talker-devices [2] --code2wav-devices [3] --thinker-gpu-memory-utilization 0.9 --talker-gpu-memory-utilization 0.9 --output-dir output_wav\n```\n\n----------------------------------------\n\nTITLE: Serving Qwen2.5-Omni with vLLM API\nDESCRIPTION: Commands to serve the Qwen2.5-Omni model through vLLM's API server. Provides examples for both single GPU and multi-GPU configurations, which enables text-only inference through an API endpoint.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n# for single GPU\nVLLM_USE_V1=0 vllm serve /path/to/Qwen2.5-Omni-7B/ --port 8000 --host 127.0.0.1 --dtype bfloat16\n# for multi GPUs (example in 4 GPUs)\nVLLM_USE_V1=0 vllm serve /path/to/Qwen2.5-Omni-7B/ --port 8000 --host 127.0.0.1 --dtype bfloat16 -tp 4\n```\n\n----------------------------------------\n\nTITLE: Installing Transformers from Source for Qwen2.5-Omni\nDESCRIPTION: Commands to install the latest version of Transformers from source to use Qwen2.5-Omni model, along with the accelerate package for improved performance.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip uninstall transformers\npip install git+https://github.com/huggingface/transformers\npip install accelerate\n```\n\n----------------------------------------\n\nTITLE: Installing Transformers from Source\nDESCRIPTION: Commands to install the latest Transformers version from source code to support Qwen2.5-Omni model\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README_CN.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip uninstall transformers\npip install git+https://github.com/huggingface/transformers\npip install accelerate\n```\n\n----------------------------------------\n\nTITLE: Installing Flash-Attention 2 for Qwen2.5-Omni Bash\nDESCRIPTION: Command to install the latest version of Flash-Attention 2, which can be used to speed up generation in Qwen2.5-Omni. This requires compatible hardware and can only be used with torch.float16 or torch.bfloat16 models.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install -U flash-attn --no-build-isolation\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installation of necessary Python packages including transformers, qwen-omni-utils, openai, and flash-attn.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/cookbooks/video_information_extracting.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install git+https://github.com/huggingface/transformers\n!pip install qwen-omni-utils\n!pip install openai\n!pip install flash-attn --no-build-isolation\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Qwen2.5-Omni\nDESCRIPTION: Installs necessary Python packages including transformers, qwen-omni-utils, openai, and flash-attn required to run the Qwen2.5-Omni model.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/cookbooks/screen_recording_interaction.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install git+https://github.com/huggingface/transformers\n!pip install qwen-omni-utils\n!pip install openai\n!pip install flash-attn --no-build-isolation\n```\n\n----------------------------------------\n\nTITLE: Installing Qwen-Omni Utils with Decord\nDESCRIPTION: Command to install qwen-omni-utils with decord for faster video loading.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install qwen-omni-utils[decord] -U\n```\n\n----------------------------------------\n\nTITLE: Installing Qwen Omni Utils Package\nDESCRIPTION: Installation command for Qwen Omni utilities package with decord support for faster video processing\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README_CN.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install qwen-omni-utils[decord] -U\n```\n\n----------------------------------------\n\nTITLE: Installing Core Dependencies for Qwen 2.5 Omni Project in Python\nDESCRIPTION: This code snippet lists the core dependencies required for the Qwen 2.5 Omni project. It includes packages for gradio, audio processing, model utilities, and transformers. The dependencies are specified with their versions to ensure compatibility.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/requirements_web_demo.txt#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# Core dependencies\ngradio==5.23.1\ngradio_client==1.8.0\nqwen-omni-utils==0.0.4\nlibrosa==0.11.0\nffmpeg==1.4\nffmpeg-python==0.2.0\nsoundfile==0.13.1\nmodelscope_studio==1.2.2\ngit+https://github.com/huggingface/transformers\naccelerate\nav\n```\n\n----------------------------------------\n\nTITLE: Installing Qwen-Omni Utils Package\nDESCRIPTION: Simple pip installation command for the qwen-omni-utils package.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/qwen-omni-utils/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install qwen-omni-utils\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM for Qwen2.5-Omni Support\nDESCRIPTION: Commands to clone and install the specific vLLM version that supports Qwen2.5-Omni. This requires installing from a specific branch and commit to ensure compatibility with the model.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ngit clone -b qwen2_omni_public https://github.com/fyabc/vllm.git\ncd vllm\ngit checkout 729feed3ec2beefe63fda30a345ef363d08062f8\npip install setuptools_scm torchdiffeq resampy x_transformers qwen-omni-utils accelerate\npip install -r requirements/cuda.txt \npip install .\npip install git+https://github.com/huggingface/transformers\n```\n\n----------------------------------------\n\nTITLE: FFmpeg Installation Note\nDESCRIPTION: Reference to FFmpeg requirement for handling audio and visual input with the toolkit.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nffmpeg\n```\n\n----------------------------------------\n\nTITLE: Running Qwen2.5-Omni with Docker Container\nDESCRIPTION: Commands for running the Qwen2.5-Omni model using the pre-built Docker image. Includes instructions for launching the Docker container with GPU access and starting the web demo with optional FlashAttention-2 support.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --gpus all --ipc=host --network=host --rm --name qwen2.5-omni -it qwenllm/qwen-omni:2.5-cu121 bash\n```\n\n----------------------------------------\n\nTITLE: Launching Qwen2.5-Omni Web Demo with FlashAttention-2\nDESCRIPTION: Command to start the Qwen2.5-Omni web demo with FlashAttention-2 enabled for improved performance and memory efficiency.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nbash docker/docker_web_demo.sh --checkpoint /path/to/Qwen2.5-Omni-7B --flash-attn2\n```\n\n----------------------------------------\n\nTITLE: Launching Qwen2.5-Omni Web Demo in Docker\nDESCRIPTION: Commands for starting the Qwen2.5-Omni web demo inside the Docker container. Provides options for enabling FlashAttention-2 for improved performance.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nbash docker/docker_web_demo.sh --checkpoint /path/to/Qwen2.5-Omni-7B\n```\n\n----------------------------------------\n\nTITLE: Launching Qwen2.5-Omni Web Demo with FlashAttention-2 Bash\nDESCRIPTION: Command to start the Qwen2.5-Omni web demo with FlashAttention-2 enabled for enhanced performance, especially in multi-image and video processing scenarios.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython web_demo.py --flash-attn2\n```\n\n----------------------------------------\n\nTITLE: Launching Qwen2.5-Omni Web Demo without FlashAttention-2 Bash\nDESCRIPTION: Command to start the Qwen2.5-Omni web demo using the standard attention implementation, without FlashAttention-2.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npython web_demo.py\n```\n\n----------------------------------------\n\nTITLE: Installing Optional Flash Attention Dependency for Qwen 2.5 Omni Project in Python\nDESCRIPTION: This code snippet shows an optional dependency for flash attention in the Qwen 2.5 Omni project. It is commented out by default and can be uncommented if flash attention support is needed.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/requirements_web_demo.txt#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Optional dependency\n# Uncomment the following line if you need flash-attn\n# flash-attn==2.7.4.post1\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for Qwen2.5-Omni Web Demo Bash\nDESCRIPTION: Command to install the required dependencies for running the Qwen2.5-Omni web demo. This ensures all necessary packages are available before launching the demo.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements_web_demo.txt\n```\n\n----------------------------------------\n\nTITLE: Importing Video Display Module\nDESCRIPTION: Imports IPython's Video module for displaying video content in the notebook.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/cookbooks/video_information_extracting.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Video\n```\n\n----------------------------------------\n\nTITLE: Importing Video Display Module\nDESCRIPTION: Imports the Video class from IPython.display to enable showing videos within the notebook interface.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/cookbooks/screen_recording_interaction.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Video\n```\n\n----------------------------------------\n\nTITLE: HTML Table Structure for Multimodality Performance\nDESCRIPTION: HTML table showing performance comparison of various models on OmniBench dataset across Speech, Sound Event, and Music categories.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-0lax\">Datasets</th>\n    <th class=\"tg-0lax\">Model</th>\n    <th class=\"tg-0lax\">Performance</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"10\">OmniBench<br>Speech | Sound Event | Music | Avg</td>\n    <td class=\"tg-0lax\">Gemini-1.5-Pro</td>\n    <td class=\"tg-0lax\">42.67%|42.26%|46.23%|42.91%</td>\n  </tr>\n  <!-- Additional rows omitted for brevity -->\n</tbody></table>\n```\n\n----------------------------------------\n\nTITLE: HTML Table Structure for Audio Performance\nDESCRIPTION: HTML table displaying performance metrics for various audio processing tasks including ASR, S2TT, SER, VSC, and Music across different models and datasets.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-0lax\">Datasets</th>\n    <th class=\"tg-0lax\">Model</th>\n    <th class=\"tg-0lax\">Performance</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">ASR</td>\n  </tr>\n  <!-- Additional rows omitted for brevity -->\n</tbody></table>\n```\n\n----------------------------------------\n\nTITLE: Citing Qwen2.5-Omni in Research\nDESCRIPTION: BibTeX citation for the Qwen2.5-Omni technical report, useful for referencing the model in academic papers and research.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/README.md#2025-04-21_snippet_22\n\nLANGUAGE: BibTeX\nCODE:\n```\n@article{Qwen2.5-Omni,\n  title={Qwen2.5-Omni Technical Report},\n  author={Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, Junyang Lin},\n  journal={arXiv preprint arXiv:2503.20215},\n  year={2025}\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Inference Function for Qwen2.5-Omni\nDESCRIPTION: Defines a function that processes video inputs with specified prompts and system instructions to generate responses from the Qwen2.5-Omni model. It handles multimodal inputs including video and text.\nSOURCE: https://github.com/qwenlm/qwen2.5-omni/blob/main/cookbooks/screen_recording_interaction.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom qwen_omni_utils import process_mm_info\n\n# @title inference function\ndef inference(video_path, prompt, sys_prompt):\n    messages = [\n        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": sys_prompt}]},\n        {\"role\": \"user\", \"content\": [\n                {\"type\": \"video\", \"video\": video_path},\n                {\"type\": \"text\", \"text\": prompt},\n            ]\n        },\n    ]\n    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    # image_inputs, video_inputs = process_vision_info([messages])\n    audios, images, videos = process_mm_info(messages, use_audio_in_video=False)\n    inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=False)\n    inputs = inputs.to(model.device).to(model.dtype)\n\n    output = model.generate(**inputs, use_audio_in_video=False, return_audio=False)\n\n    text = processor.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n    return text\n```"
  }
]
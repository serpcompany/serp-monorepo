[
  {
    "owner": "sktime",
    "repo": "pytorch-forecasting",
    "content": "TITLE: Complete Example of Time Series Forecasting with TemporalFusionTransformer\nDESCRIPTION: A comprehensive example demonstrating the full workflow for time series forecasting using PyTorch Forecasting. It includes dataset creation, dataloader setup, model definition with TemporalFusionTransformer, learning rate finding, and model training with early stopping.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/getting-started.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport lightning.pytorch as pl\nfrom lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\nfrom lightning.pytorch.tuner import Tuner\nfrom pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n\n# load data\ndata = ...\n\n# define dataset\nmax_encoder_length = 36\nmax_prediction_length = 6\ntraining_cutoff = \"YYYY-MM-DD\"  # day for cutoff\n\ntraining = TimeSeriesDataSet(\n    data[lambda x: x.date < training_cutoff],\n    time_idx= ...,\n    target= ...,\n    # weight=\"weight\",\n    group_ids=[ ... ],\n    max_encoder_length=max_encoder_length,\n    max_prediction_length=max_prediction_length,\n    static_categoricals=[ ... ],\n    static_reals=[ ... ],\n    time_varying_known_categoricals=[ ... ],\n    time_varying_known_reals=[ ... ],\n    time_varying_unknown_categoricals=[ ... ],\n    time_varying_unknown_reals=[ ... ],\n)\n\n# create validation and training dataset\nvalidation = TimeSeriesDataSet.from_dataset(training, data, min_prediction_idx=training.index.time.max() + 1, stop_randomization=True)\nbatch_size = 128\ntrain_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\nval_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n\n# define trainer with early stopping\nearly_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=1, verbose=False, mode=\"min\")\nlr_logger = LearningRateMonitor()\ntrainer = pl.Trainer(\n    max_epochs=100,\n    accelerator=\"auto\",\n    gradient_clip_val=0.1,\n    limit_train_batches=30,\n    callbacks=[lr_logger, early_stop_callback],\n)\n\n# create the model\ntft = TemporalFusionTransformer.from_dataset(\n    training,\n    learning_rate=0.03,\n    hidden_size=32,\n    attention_head_size=1,\n    dropout=0.1,\n    hidden_continuous_size=16,\n    output_size=7,\n    loss=QuantileLoss(),\n    log_interval=2,\n    reduce_on_plateau_patience=4\n)\nprint(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n\n# find optimal learning rate (set limit_train_batches to 1.0 and log_interval = -1)\nres = Tuner(trainer).lr_find(\n    tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, early_stop_threshold=1000.0, max_lr=0.3,\n)\n\nprint(f\"suggested learning rate: {res.suggestion()}\")\nfig = res.plot(show=True, suggest=True)\nfig.show()\n\n# fit the model\ntrainer.fit(\n    tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader,\n)\n```\n\n----------------------------------------\n\nTITLE: Training a Time Series Forecasting Model with PyTorch Forecasting\nDESCRIPTION: Complete example demonstrating how to prepare time series data, create a TimeSeriesDataSet, set up a Temporal Fusion Transformer model, and train it using PyTorch Lightning. Includes dataset configuration, dataloader creation, model initialization, and training process with learning rate finding.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/README.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# imports for training\nimport lightning.pytorch as pl\nfrom lightning.pytorch.loggers import TensorBoardLogger\nfrom lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n# import dataset, network to train and metric to optimize\nfrom pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, QuantileLoss\nfrom lightning.pytorch.tuner import Tuner\n\n# load data: this is pandas dataframe with at least a column for\n# * the target (what you want to predict)\n# * the timeseries ID (which should be a unique string to identify each timeseries)\n# * the time of the observation (which should be a monotonically increasing integer)\ndata = ...\n\n# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\nmax_encoder_length = 36\nmax_prediction_length = 6\ntraining_cutoff = \"YYYY-MM-DD\"  # day for cutoff\n\ntraining = TimeSeriesDataSet(\n    data[lambda x: x.date <= training_cutoff],\n    time_idx= ...,  # column name of time of observation\n    target= ...,  # column name of target to predict\n    group_ids=[ ... ],  # column name(s) for timeseries IDs\n    max_encoder_length=max_encoder_length,  # how much history to use\n    max_prediction_length=max_prediction_length,  # how far to predict into future\n    # covariates static for a timeseries ID\n    static_categoricals=[ ... ],\n    static_reals=[ ... ],\n    # covariates known and unknown in the future to inform prediction\n    time_varying_known_categoricals=[ ... ],\n    time_varying_known_reals=[ ... ],\n    time_varying_unknown_categoricals=[ ... ],\n    time_varying_unknown_reals=[ ... ],\n)\n\n# create validation dataset using the same normalization techniques as for the training dataset\nvalidation = TimeSeriesDataSet.from_dataset(training, data, min_prediction_idx=training.index.time.max() + 1, stop_randomization=True)\n\n# convert datasets to dataloaders for training\nbatch_size = 128\ntrain_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\nval_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n\n# create PyTorch Lighning Trainer with early stopping\nearly_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=1, verbose=False, mode=\"min\")\nlr_logger = LearningRateMonitor()\ntrainer = pl.Trainer(\n    max_epochs=100,\n    accelerator=\"auto\",  # run on CPU, if on multiple GPUs, use strategy=\"ddp\"\n    gradient_clip_val=0.1,\n    limit_train_batches=30,  # 30 batches per epoch\n    callbacks=[lr_logger, early_stop_callback],\n    logger=TensorBoardLogger(\"lightning_logs\")\n)\n\n# define network to train - the architecture is mostly inferred from the dataset, so that only a few hyperparameters have to be set by the user\ntft = TemporalFusionTransformer.from_dataset(\n    # dataset\n    training,\n    # architecture hyperparameters\n    hidden_size=32,\n    attention_head_size=1,\n    dropout=0.1,\n    hidden_continuous_size=16,\n    # loss metric to optimize\n    loss=QuantileLoss(),\n    # logging frequency\n    log_interval=2,\n    # optimizer parameters\n    learning_rate=0.03,\n    reduce_on_plateau_patience=4\n)\nprint(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n\n# find the optimal learning rate\nres = Tuner(trainer).lr_find(\n    tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, early_stop_threshold=1000.0, max_lr=0.3,\n)\n# and plot the result - always visually confirm that the suggested learning rate makes sense\nprint(f\"suggested learning rate: {res.suggestion()}\")\nfig = res.plot(show=True, suggest=True)\nfig.show()\n\n# fit the model on the data - redefine the model with the correct learning rate if necessary\ntrainer.fit(\n    tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing FullyConnectedClassificationModel for Time Series Classification\nDESCRIPTION: Defines a custom classification model that extends BaseModel for time series classification tasks. The model uses a fully connected network and CrossEntropy loss to predict categorical targets.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_forecasting.metrics import CrossEntropy\n\n\nclass FullyConnectedClassificationModel(BaseModel):\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        hidden_size: int,\n        n_hidden_layers: int,\n        n_classes: int,\n        loss=CrossEntropy(),\n        **kwargs,\n    ):\n        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n        self.save_hyperparameters()\n        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n        super().__init__(**kwargs)\n        self.network = FullyConnectedModule(\n            input_size=self.hparams.input_size,\n            output_size=self.hparams.output_size * self.hparams.n_classes,\n            hidden_size=self.hparams.hidden_size,\n            n_hidden_layers=self.hparams.n_hidden_layers,\n        )\n\n    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        # x is a batch generated based on the TimeSeriesDataset\n        batch_size = x[\"encoder_cont\"].size(0)\n        network_input = x[\"encoder_cont\"].squeeze(-1)\n        prediction = self.network(network_input)\n        # RESHAPE output to batch_size x n_decoder_timesteps x n_classes\n        prediction = prediction.unsqueeze(-1).view(batch_size, -1, self.hparams.n_classes)\n\n        # rescale predictions into target space\n        prediction = self.transform_output(prediction, target_scale=x[\"target_scale\"])\n\n        # We need to return a named tuple that at least contains the prediction.\n        # The parameter can be directly forwarded from the input.\n        # The conversion to a named tuple can be directly achieved with the `to_network_output` function.\n        return self.to_network_output(prediction=prediction)\n\n    @classmethod\n    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n        assert isinstance(dataset.target_normalizer, NaNLabelEncoder), \"target normalizer has to encode categories\"\n        new_kwargs = {\n            \"n_classes\": len(\n                dataset.target_normalizer.classes_\n            ),  # ADD number of classes as encoded by the target normalizer\n            \"output_size\": dataset.max_prediction_length,\n            \"input_size\": dataset.max_encoder_length,\n        }\n        new_kwargs.update(kwargs)  # use to pass real hyperparameters and override defaults set by dataset\n        # example for dataset validation\n        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n        assert (\n            len(dataset._time_varying_known_categoricals) == 0\n            and len(dataset._time_varying_known_reals) == 0\n            and len(dataset._time_varying_unknown_categoricals) == 0\n            and len(dataset._static_categoricals) == 0\n            and len(dataset._static_reals) == 0\n            and len(dataset._time_varying_unknown_reals) == 1\n        ), \"Only covariate should be in 'time_varying_unknown_reals'\"\n\n        return super().from_dataset(dataset, **new_kwargs)\n\n\nmodel = FullyConnectedClassificationModel.from_dataset(classification_dataset, hidden_size=10, n_hidden_layers=2)\nprint(ModelSummary(model, max_depth=-1))\nmodel.hparams\n```\n\n----------------------------------------\n\nTITLE: Defining FullyConnectedForDistributionLossModel in PyTorch\nDESCRIPTION: This snippet defines a custom neural network model for time series forecasting. It uses a fully connected architecture and predicts parameters for a normal distribution. The model includes methods for initialization, forward pass, and dataset compatibility checks.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom copy import copy\n\nfrom pytorch_forecasting.metrics import NormalDistributionLoss\n\n\nclass FullyConnectedForDistributionLossModel(BaseModel):  # we inherit the `from_dataset` method\n    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int, **kwargs):\n        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n        self.save_hyperparameters()\n        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n        super().__init__(**kwargs)\n        self.network = FullyConnectedMultiOutputModule(\n            input_size=self.hparams.input_size,\n            output_size=self.hparams.output_size,\n            hidden_size=self.hparams.hidden_size,\n            n_hidden_layers=self.hparams.n_hidden_layers,\n            n_outputs=2,  # <<<<<<<< we predict two outputs for mean and scale of the normal distribution\n        )\n        self.loss = NormalDistributionLoss()\n\n    @classmethod\n    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n        new_kwargs = {\n            \"output_size\": dataset.max_prediction_length,\n            \"input_size\": dataset.max_encoder_length,\n        }\n        new_kwargs.update(kwargs)  # use to pass real hyperparameters and override defaults set by dataset\n        # example for dataset validation\n        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n        assert (\n            len(dataset._time_varying_known_categoricals) == 0\n            and len(dataset._time_varying_known_reals) == 0\n            and len(dataset._time_varying_unknown_categoricals) == 0\n            and len(dataset._static_categoricals) == 0\n            and len(dataset._static_reals) == 0\n            and len(dataset._time_varying_unknown_reals) == 1\n            and dataset._time_varying_unknown_reals[0] == dataset.target\n        ), \"Only covariate should be the target in 'time_varying_unknown_reals'\"\n\n        return super().from_dataset(dataset, **new_kwargs)\n\n    def forward(self, x: Dict[str, torch.Tensor], n_samples: int = None) -> Dict[str, torch.Tensor]:\n        # x is a batch generated based on the TimeSeriesDataset\n        network_input = x[\"encoder_cont\"].squeeze(-1)\n        prediction = self.network(network_input)  # shape batch_size x n_decoder_steps x 2\n        # we need to scale the parameters to real space\n        prediction = self.transform_output(\n            prediction=prediction,\n            target_scale=x[\"target_scale\"],\n        )\n        if n_samples is not None:\n            # sample from distribution\n            prediction = self.loss.sample(prediction, n_samples)\n        # The conversion to a named tuple can be directly achieved with the `to_network_output` function.\n        return self.to_network_output(prediction=prediction)\n\n\nmodel = FullyConnectedForDistributionLossModel.from_dataset(dataset, hidden_size=10, n_hidden_layers=2)\nprint(ModelSummary(model, max_depth=-1))\nmodel.hparams\n```\n\n----------------------------------------\n\nTITLE: Implementing an LSTM Autoregressive Model in PyTorch Forecasting\nDESCRIPTION: A complete implementation of an LSTM-based autoregressive model for time series forecasting. The model inherits from AutoRegressiveBaseModel and includes methods for encoding time series data, decoding predictions, and handling both training and inference modes.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.nn.utils import rnn\n\nfrom pytorch_forecasting.models.base_model import AutoRegressiveBaseModel\nfrom pytorch_forecasting.models.nn import LSTM\n\n\nclass LSTMModel(AutoRegressiveBaseModel):\n    def __init__(\n        self,\n        target: str,\n        target_lags: Dict[str, Dict[str, int]],\n        n_layers: int,\n        hidden_size: int,\n        dropout: float = 0.1,\n        **kwargs,\n    ):\n        # arguments target and target_lags are required for autoregressive models\n        # even though target_lags cannot be used without covariates\n        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n        self.save_hyperparameters()\n        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n        super().__init__(**kwargs)\n\n        # use version of LSTM that can handle zero-length sequences\n        self.lstm = LSTM(\n            hidden_size=self.hparams.hidden_size,\n            input_size=1,\n            num_layers=self.hparams.n_layers,\n            dropout=self.hparams.dropout,\n            batch_first=True,\n        )\n        self.output_layer = nn.Linear(self.hparams.hidden_size, 1)\n\n    def encode(self, x: Dict[str, torch.Tensor]):\n        # we need at least one encoding step as because the target needs to be lagged by one time step\n        # because we use the custom LSTM, we do not have to require encoder lengths of > 1\n        # but can handle lengths of >= 1\n        assert x[\"encoder_lengths\"].min() >= 1\n        input_vector = x[\"encoder_cont\"].clone()\n        # lag target by one\n        input_vector[..., self.target_positions] = torch.roll(\n            input_vector[..., self.target_positions], shifts=1, dims=1\n        )\n        input_vector = input_vector[:, 1:]  # first time step cannot be used because of lagging\n\n        # determine effective encoder_length length\n        effective_encoder_lengths = x[\"encoder_lengths\"] - 1\n        # run through LSTM network\n        _, hidden_state = self.lstm(\n            input_vector, lengths=effective_encoder_lengths, enforce_sorted=False  # passing the lengths directly\n        )  # second ouput is not needed (hidden state)\n        return hidden_state\n\n    def decode(self, x: Dict[str, torch.Tensor], hidden_state):\n        # again lag target by one\n        input_vector = x[\"decoder_cont\"].clone()\n        input_vector[..., self.target_positions] = torch.roll(\n            input_vector[..., self.target_positions], shifts=1, dims=1\n        )\n        # but this time fill in missing target from encoder_cont at the first time step instead of throwing it away\n        last_encoder_target = x[\"encoder_cont\"][\n            torch.arange(x[\"encoder_cont\"].size(0), device=x[\"encoder_cont\"].device),\n            x[\"encoder_lengths\"] - 1,\n            self.target_positions.unsqueeze(-1),\n        ].T\n        input_vector[:, 0, self.target_positions] = last_encoder_target\n\n        if self.training:  # training mode\n            lstm_output, _ = self.lstm(input_vector, hidden_state, lengths=x[\"decoder_lengths\"], enforce_sorted=False)\n\n            # transform into right shape\n            prediction = self.output_layer(lstm_output)\n            prediction = self.transform_output(prediction, target_scale=x[\"target_scale\"])\n\n            # predictions are not yet rescaled\n            return prediction\n\n        else:  # prediction mode\n            target_pos = self.target_positions\n\n            def decode_one(idx, lagged_targets, hidden_state):\n                x = input_vector[:, [idx]]\n                # overwrite at target positions\n                x[:, 0, target_pos] = lagged_targets[-1]  # take most recent target (i.e. lag=1)\n                lstm_output, hidden_state = self.lstm(x, hidden_state)\n                # transform into right shape\n                prediction = self.output_layer(lstm_output)[:, 0]  # take first timestep\n                return prediction, hidden_state\n\n            # make predictions which are fed into next step\n            output = self.decode_autoregressive(\n                decode_one,\n                first_target=input_vector[:, 0, target_pos],\n                first_hidden_state=hidden_state,\n                target_scale=x[\"target_scale\"],\n                n_decoder_steps=input_vector.size(1),\n            )\n\n            # predictions are already rescaled\n            return output\n\n    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        hidden_state = self.encode(x)  # encode to hidden state\n        output = self.decode(x, hidden_state)  # decode leveraging hidden state\n\n        return self.to_network_output(prediction=output)\n\n\nmodel = LSTMModel.from_dataset(dataset, n_layers=2, hidden_size=10)\nprint(ModelSummary(model, max_depth=-1))\nmodel.hparams\n```\n\n----------------------------------------\n\nTITLE: Implementing a Multi-Output Neural Network Module for Quantile Prediction\nDESCRIPTION: A custom fully connected neural network module that supports multiple outputs such as quantile prediction. The network takes an input tensor and produces an output with an additional dimension for the number of quantiles or other multiple outputs.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch import nn\n\n\nclass FullyConnectedMultiOutputModule(nn.Module):\n    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int, n_outputs: int):\n        super().__init__()\n\n        # input layer\n        module_list = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n        # hidden layers\n        for _ in range(n_hidden_layers):\n            module_list.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])\n        # output layer\n        self.n_outputs = n_outputs\n        module_list.append(\n            nn.Linear(hidden_size, output_size * n_outputs)\n        )  # <<<<<<<< modified: replaced output_size with output_size * n_outputs\n\n        self.sequential = nn.Sequential(*module_list)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x of shape: batch_size x n_timesteps_in\n        # output of shape batch_size x n_timesteps_out\n        return self.sequential(x).reshape(x.size(0), -1, self.n_outputs)  # <<<<<<<< modified: added reshape\n\n\n# test that network works as intended\nnetwork = FullyConnectedMultiOutputModule(input_size=5, output_size=2, hidden_size=10, n_hidden_layers=2, n_outputs=7)\nnetwork(torch.rand(20, 5)).shape  # <<<<<<<<<< instead of shape (20, 2), returning additional dimension for quantiles\n```\n\n----------------------------------------\n\nTITLE: Implementing FullyConnectedModelWithCovariates in PyTorch Forecasting\nDESCRIPTION: This code defines a FullyConnectedModelWithCovariates class that inherits from BaseModelWithCovariates. It implements a fully connected neural network that can handle both categorical and continuous covariates for time series forecasting.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict, List, Tuple\n\nfrom pytorch_forecasting.models.nn import MultiEmbedding\n\n\nclass FullyConnectedModelWithCovariates(BaseModelWithCovariates):\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        hidden_size: int,\n        n_hidden_layers: int,\n        x_reals: List[str],\n        x_categoricals: List[str],\n        embedding_sizes: Dict[str, Tuple[int, int]],\n        embedding_labels: Dict[str, List[str]],\n        static_categoricals: List[str],\n        static_reals: List[str],\n        time_varying_categoricals_encoder: List[str],\n        time_varying_categoricals_decoder: List[str],\n        time_varying_reals_encoder: List[str],\n        time_varying_reals_decoder: List[str],\n        embedding_paddings: List[str],\n        categorical_groups: Dict[str, List[str]],\n        **kwargs,\n    ):\n        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n        self.save_hyperparameters()\n        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n        super().__init__(**kwargs)\n\n        # create embedder - can be fed with x[\"encoder_cat\"] or x[\"decoder_cat\"] and will return\n        # dictionary of category names mapped to embeddings\n        self.input_embeddings = MultiEmbedding(\n            embedding_sizes=self.hparams.embedding_sizes,\n            categorical_groups=self.hparams.categorical_groups,\n            embedding_paddings=self.hparams.embedding_paddings,\n            x_categoricals=self.hparams.x_categoricals,\n            max_embedding_size=self.hparams.hidden_size,\n        )\n\n        # calculate the size of all concatenated embeddings + continous variables\n        n_features = sum(\n            embedding_size for classes_size, embedding_size in self.hparams.embedding_sizes.values()\n        ) + len(self.reals)\n\n        # create network that will be fed with continious variables and embeddings\n        self.network = FullyConnectedModule(\n            input_size=self.hparams.input_size * n_features,\n            output_size=self.hparams.output_size,\n            hidden_size=self.hparams.hidden_size,\n            n_hidden_layers=self.hparams.n_hidden_layers,\n        )\n\n    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        # x is a batch generated based on the TimeSeriesDataset\n        batch_size = x[\"encoder_lengths\"].size(0)\n        embeddings = self.input_embeddings(x[\"encoder_cat\"])  # returns dictionary with embedding tensors\n        network_input = torch.cat(\n            [x[\"encoder_cont\"]]\n            + [\n                emb\n                for name, emb in embeddings.items()\n                if name in self.encoder_variables or name in self.static_variables\n            ],\n            dim=-1,\n        )\n        prediction = self.network(network_input.view(batch_size, -1))\n\n        # rescale predictions into target space\n        prediction = self.transform_output(prediction, target_scale=x[\"target_scale\"])\n\n        # We need to return a dictionary that at least contains the prediction.\n        # The parameter can be directly forwarded from the input.\n        # The conversion to a named tuple can be directly achieved with the `to_network_output` function.\n        return self.to_network_output(prediction=prediction)\n\n    @classmethod\n    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n        new_kwargs = {\n            \"output_size\": dataset.max_prediction_length,\n            \"input_size\": dataset.max_encoder_length,\n        }\n        new_kwargs.update(kwargs)  # use to pass real hyperparameters and override defaults set by dataset\n        # example for dataset validation\n        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n\n        return super().from_dataset(dataset, **new_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Creating a Dataset with Covariates in PyTorch Forecasting\nDESCRIPTION: This snippet creates a sample dataset with categorical and real covariates using pandas and numpy. It then uses the TimeSeriesDataSet class from PyTorch Forecasting to create a dataset suitable for time series forecasting with covariates.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n\nfrom pytorch_forecasting import TimeSeriesDataSet\n\ntest_data_with_covariates = pd.DataFrame(\n    dict(\n        # as before\n        value=np.random.rand(30),\n        group=np.repeat(np.arange(3), 10),\n        time_idx=np.tile(np.arange(10), 3),\n        # now adding covariates\n        categorical_covariate=np.random.choice([\"a\", \"b\"], size=30),\n        real_covariate=np.random.rand(30),\n    )\n).astype(\n    dict(group=str)  # categorical covariates have to be of string type\n)\ntest_data_with_covariates\n\n# create the dataset from the pandas dataframe\ndataset_with_covariates = TimeSeriesDataSet(\n    test_data_with_covariates,\n    group_ids=[\"group\"],\n    target=\"value\",\n    time_idx=\"time_idx\",\n    min_encoder_length=5,\n    max_encoder_length=5,\n    min_prediction_length=2,\n    max_prediction_length=2,\n    time_varying_unknown_reals=[\"value\"],\n    time_varying_known_reals=[\"real_covariate\"],\n    time_varying_known_categoricals=[\"categorical_covariate\"],\n    static_categoricals=[\"group\"],\n)\n\nmodel = FullyConnectedModelWithCovariates.from_dataset(dataset_with_covariates, hidden_size=10, n_hidden_layers=2)\nprint(ModelSummary(model, max_depth=-1))  # print model summary\nmodel.hparams\n```\n\n----------------------------------------\n\nTITLE: Setting Up Multi-Target TimeSeriesDataSet\nDESCRIPTION: Creates a TimeSeriesDataSet configured for multiple target forecasting, using a MultiNormalizer to handle different normalization requirements for each target.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_forecasting.data.encoders import EncoderNormalizer, MultiNormalizer, TorchNormalizer\n\n# create the dataset from the pandas dataframe\nmulti_target_dataset = TimeSeriesDataSet(\n    multi_target_test_data,\n    group_ids=[\"group\"],\n    target=[\"target1\", \"target2\"],  # USING two targets\n    time_idx=\"time_idx\",\n    min_encoder_length=5,\n    max_encoder_length=5,\n    min_prediction_length=2,\n    max_prediction_length=2,\n    time_varying_unknown_reals=[\"target1\", \"target2\"],\n    target_normalizer=MultiNormalizer(\n        [EncoderNormalizer(), TorchNormalizer()]\n    ),  # Use the NaNLabelEncoder to encode categorical target\n)\n\nx, y = next(iter(multi_target_dataset.to_dataloader(batch_size=4)))\ny[0]  # target values are a list of targets\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Mean Absolute Error (MAE) Metric for PyTorch Forecasting\nDESCRIPTION: An implementation of a custom Mean Absolute Error (MAE) metric that inherits from the MultiHorizonMetric class in PyTorch Forecasting. This metric calculates the absolute difference between predictions and targets.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_forecasting.metrics import MultiHorizonMetric\n\n\nclass MAE(MultiHorizonMetric):\n    def loss(self, y_pred, target):\n        loss = (self.to_prediction(y_pred) - target).abs()\n        return loss\n```\n\n----------------------------------------\n\nTITLE: Creating TimeSeriesDataSet for Inference in PyTorch Forecasting\nDESCRIPTION: Use the TimeSeriesDataSet method of your training dataset to create datasets for running inference on new samples.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/faq.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nTimeSeriesDataSet\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom PyTorch Forecasting Model\nDESCRIPTION: Extends BaseModel to create a custom forecasting model with the fully connected network architecture.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\n\nfrom pytorch_forecasting.models import BaseModel\n\n\nclass FullyConnectedModel(BaseModel):\n    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int, **kwargs):\n        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n        self.save_hyperparameters()\n        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n        super().__init__(**kwargs)\n        self.network = FullyConnectedModule(\n            input_size=self.hparams.input_size,\n            output_size=self.hparams.output_size,\n            hidden_size=self.hparams.hidden_size,\n            n_hidden_layers=self.hparams.n_hidden_layers,\n        )\n\n    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        # x is a batch generated based on the TimeSeriesDataSet\n        network_input = x[\"encoder_cont\"].squeeze(-1)\n        prediction = self.network(network_input)\n\n        # rescale predictions into target space\n        prediction = self.transform_output(prediction, target_scale=x[\"target_scale\"])\n\n        # We need to return a dictionary that at least contains the prediction\n        # The parameter can be directly forwarded from the input.\n        # The conversion to a named tuple can be directly achieved with the `to_network_output` function.\n        return self.to_network_output(prediction=prediction)\n```\n\n----------------------------------------\n\nTITLE: Implementing FullyConnectedMultiTargetModel\nDESCRIPTION: Defines a custom model for multi-target forecasting that can handle multiple outputs simultaneously. The model supports different target sizes and can use multiple loss functions combined with the MultiLoss class.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Union\n\nfrom pytorch_forecasting.metrics import MAE, SMAPE, MultiLoss\nfrom pytorch_forecasting.utils import to_list\n\n\nclass FullyConnectedMultiTargetModel(BaseModel):\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        hidden_size: int,\n        n_hidden_layers: int,\n        target_sizes: Union[int, List[int]] = [],\n        **kwargs,\n    ):\n        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n        self.save_hyperparameters()\n        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n        super().__init__(**kwargs)\n        self.network = FullyConnectedModule(\n            input_size=self.hparams.input_size * len(to_list(self.hparams.target_sizes)),\n            output_size=self.hparams.output_size * sum(to_list(self.hparams.target_sizes)),\n            hidden_size=self.hparams.hidden_size,\n            n_hidden_layers=self.hparams.n_hidden_layers,\n        )\n\n    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        # x is a batch generated based on the TimeSeriesDataset\n        batch_size = x[\"encoder_cont\"].size(0)\n        network_input = x[\"encoder_cont\"].view(batch_size, -1)\n        prediction = self.network(network_input)\n        # RESHAPE output to batch_size x n_decoder_timesteps x sum_of_target_sizes\n        prediction = prediction.unsqueeze(-1).view(batch_size, self.hparams.output_size, sum(self.hparams.target_sizes))\n        # RESHAPE into list of batch_size x n_decoder_timesteps x target_sizes[i] where i=1..len(target_sizes)\n        stops = np.cumsum(self.hparams.target_sizes)\n        starts = stops - self.hparams.target_sizes\n        prediction = [prediction[..., start:stop] for start, stop in zip(starts, stops)]\n        if isinstance(self.hparams.target_sizes, int):  # only one target\n            prediction = prediction[0]\n\n        # rescale predictions into target space\n        prediction = self.transform_output(prediction, target_scale=x[\"target_scale\"])\n\n        # We need to return a named tuple that at least contains the prediction.\n        # The parameter can be directly forwarded from the input.\n        # The conversion to a named tuple can be directly achieved with the `to_network_output` function.\n        return self.to_network_output(prediction=prediction)\n\n    @classmethod\n    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n        # By default only handle targets of size one here, categorical targets would be of larger size\n        new_kwargs = {\n            \"target_sizes\": [1] * len(to_list(dataset.target)),\n            \"output_size\": dataset.max_prediction_length,\n            \"input_size\": dataset.max_encoder_length,\n        }\n        new_kwargs.update(kwargs)  # use to pass real hyperparameters and override defaults set by dataset\n        # example for dataset validation\n        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n        assert (\n            len(dataset._time_varying_known_categoricals) == 0\n            and len(dataset._time_varying_known_reals) == 0\n            and len(dataset._time_varying_unknown_categoricals) == 0\n            and len(dataset._static_categoricals) == 0\n            and len(dataset._static_reals) == 0\n            and len(dataset._time_varying_unknown_reals)\n            == len(dataset.target_names)  # Expect as as many unknown reals as targets\n        ), \"Only covariate should be in 'time_varying_unknown_reals'\"\n\n        return super().from_dataset(dataset, **new_kwargs)\n\n\nmodel = FullyConnectedMultiTargetModel.from_dataset(\n    multi_target_dataset,\n    hidden_size=10,\n    n_hidden_layers=2,\n    loss=MultiLoss(metrics=[MAE(), SMAPE()], weights=[2.0, 1.0]),\n)\nprint(ModelSummary(model, max_depth=-1))\nmodel.hparams\n```\n\n----------------------------------------\n\nTITLE: Reducing Mean Prediction Bias with Aggregation Metrics in PyTorch Forecasting\nDESCRIPTION: This example shows how to modify a loss metric to ensure predictions add up correctly using an AggregationMetric. This adds an additional loss calculated on the mean predictions and actuals, which helps ensure aggregated results are unbiased.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/metrics.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_forecasting.metrics import MAE, AggregationMetric\n\ncomposite_metric = MAE() + AggregationMetric(metric=MAE())\n```\n\n----------------------------------------\n\nTITLE: Implementing custom plotting function in PyTorch Forecasting\nDESCRIPTION: This snippet defines a custom plotting function for visualizing predictions, actuals, and attention in a time series forecasting model using matplotlib.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\n\ndef plot_prediction(\n    self,\n    x: Dict[str, torch.Tensor],\n    out: Dict[str, torch.Tensor],\n    idx: int,\n    plot_attention: bool = True,\n    add_loss_to_title: bool = False,\n    show_future_observed: bool = True,\n    ax=None,\n) -> plt.Figure:\n    \"\"\"\n    Plot actuals vs prediction and attention\n\n    Args:\n        x (Dict[str, torch.Tensor]): network input\n        out (Dict[str, torch.Tensor]): network output\n        idx (int): sample index\n        plot_attention: if to plot attention on secondary axis\n        add_loss_to_title: if to add loss to title. Default to False.\n        show_future_observed: if to show actuals for future. Defaults to True.\n        ax: matplotlib axes to plot on\n\n    Returns:\n        plt.Figure: matplotlib figure\n    \"\"\"\n    # plot prediction as normal\n    fig = super().plot_prediction(\n        x, out, idx=idx, add_loss_to_title=add_loss_to_title, show_future_observed=show_future_observed, ax=ax\n    )\n\n    # add attention on secondary axis\n    if plot_attention:\n        interpretation = self.interpret_output(out)\n        ax = fig.axes[0]\n        ax2 = ax.twinx()\n        ax2.set_ylabel(\"Attention\")\n        encoder_length = x[\"encoder_lengths\"][idx]\n        ax2.plot(\n            torch.arange(-encoder_length, 0),\n            interpretation[\"attention\"][idx, :encoder_length].detach().cpu(),\n            alpha=0.2,\n            color=\"k\",\n        )\n    fig.tight_layout()\n    return fig\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Model Prediction Shapes in Training vs Inference Modes\nDESCRIPTION: This code snippet shows how to examine the output shape of a PyTorch Forecasting model in both training and inference modes. It demonstrates the differences in prediction behavior when the model is set to evaluation mode for autoregressive prediction.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nx, y = next(iter(dataloader))\n\nprint(\n    \"prediction shape in training:\", model(x)[\"prediction\"].size()\n)  # batch_size x decoder time steps x 1 (1 for one target dimension)\nmodel.eval()  # set model into evaluation mode to use autoregressive prediction\nprint(\"prediction shape in inference:\", model(x)[\"prediction\"].size())  # should be the same as in training\n```\n\n----------------------------------------\n\nTITLE: Implementing Fully Connected Neural Network Module\nDESCRIPTION: Creates a PyTorch module with configurable fully connected layers for time series prediction.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch import nn\n\n\nclass FullyConnectedModule(nn.Module):\n    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int):\n        super().__init__()\n\n        # input layer\n        module_list = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n        # hidden layers\n        for _ in range(n_hidden_layers):\n            module_list.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])\n        # output layer\n        module_list.append(nn.Linear(hidden_size, output_size))\n\n        self.sequential = nn.Sequential(*module_list)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x of shape: batch_size x n_timesteps_in\n        # output of shape batch_size x n_timesteps_out\n        return self.sequential(x)\n\n\n# test that network works as intended\nnetwork = FullyConnectedModule(input_size=5, output_size=2, hidden_size=10, n_hidden_layers=2)\nx = torch.rand(20, 5)\nnetwork(x).shape\n```\n\n----------------------------------------\n\nTITLE: Using Custom Metrics with PyTorch Forecasting Models\nDESCRIPTION: This example shows how to use a custom metric (Mean Absolute Error) with a PyTorch Forecasting model. It demonstrates how to pass the custom metric to the model during initialization using the from_dataset method.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_forecasting.metrics import MAE\n\nmodel = FullyConnectedModel.from_dataset(dataset, hidden_size=10, n_hidden_layers=2, loss=MAE())\nmodel.hparams\n```\n\n----------------------------------------\n\nTITLE: Calculating Loss for Multi-Target Model\nDESCRIPTION: Computes the combined loss for the multi-target model using the configured loss function.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel.loss(out[\"prediction\"], y)\n```\n\n----------------------------------------\n\nTITLE: Combining Metrics by Addition in PyTorch Forecasting\nDESCRIPTION: This snippet demonstrates how to create a composite metric by adding SMAPE and MAE with a weight factor. This approach helps optimize one metric (SMAPE) while avoiding large outliers in another (MAE).\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/metrics.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_forecasting.metrics import SMAPE, MAE\n\ncomposite_metric = SMAPE() + 1e-4 * MAE()\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Forecasting via pip and conda\nDESCRIPTION: Installation instructions for PyTorch Forecasting on different operating systems, including Windows-specific PyTorch installation and optional MQF2 loss dependencies.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/README.md#2025-04-17_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install torch -f https://download.pytorch.org/whl/torch_stable.html\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install pytorch-forecasting\n```\n\nLANGUAGE: bash\nCODE:\n```\nconda install pytorch-forecasting pytorch -c pytorch>=1.7 -c conda-forge\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install pytorch-forecasting[mqf2]\n```\n\n----------------------------------------\n\nTITLE: Testing Model Training with Sample Batch in PyTorch Forecasting\nDESCRIPTION: This code generates a sample batch from the dataset and passes it through the model to test if the model can be trained. It demonstrates how to use the dataloader and feed data into the model.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nx, y = next(iter(dataset_with_covariates.to_dataloader(batch_size=4)))  # generate batch\nmodel(x)  # pass batch through model\n```\n\n----------------------------------------\n\nTITLE: Evaluating model output shapes in PyTorch\nDESCRIPTION: This code snippet shows how to evaluate the output shapes of a PyTorch model for both parameter prediction and sample prediction modes.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nx, y = next(iter(dataloader))\n\nprint(\"parameter predition shape: \", model(x)[\"prediction\"].size())\nmodel.eval()  # set model into eval mode for sampling\nprint(\"sample prediction shape: \", model(x, n_samples=200)[\"prediction\"].size())\n```\n\n----------------------------------------\n\nTITLE: Setting up minimal testing for PyTorch Forecasting models\nDESCRIPTION: This snippet shows how to set up a minimal test for a PyTorch Forecasting model using PyTorch Lightning's Trainer with fast_dev_run option.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import Trainer\n\nmodel = FullyConnectedForDistributionLossModel.from_dataset(dataset, hidden_size=10, n_hidden_layers=2, log_interval=1)\ntrainer = Trainer(fast_dev_run=True)\ntrainer.fit(model, train_dataloaders=dataloader, val_dataloaders=dataloader)\n```\n\n----------------------------------------\n\nTITLE: Predicting quantiles with PyTorch model\nDESCRIPTION: This snippet demonstrates how to use the model to predict quantiles using a specified number of samples.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nmodel.predict(dataloader, mode=\"quantiles\", mode_kwargs=dict(n_samples=100)).shape\n```\n\n----------------------------------------\n\nTITLE: Transforming Network Output Example in PyTorch Forecasting\nDESCRIPTION: Example of how to implement the forward method in a PyTorch Forecasting model to handle transformations after v0.9.0. The code shows the pattern for transforming normalized predictions back to their original scale and formatting them as network output.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/CHANGELOG.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, x):\n    normalized_prediction = self.module(x)\n    prediction = self.transform_output(prediction=normalized_prediction, target_scale=x[\"target_scale\"])\n    return self.to_network_output(prediction=prediction)\n```\n\n----------------------------------------\n\nTITLE: Creating Test Data and TimeSeriesDataSet\nDESCRIPTION: Generates sample time series data and creates a TimeSeriesDataSet for model training.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n\ntest_data = pd.DataFrame(\n    dict(\n        value=np.random.rand(30) - 0.5,\n        group=np.repeat(np.arange(3), 10),\n        time_idx=np.tile(np.arange(10), 3),\n    )\n)\n\nfrom pytorch_forecasting import TimeSeriesDataSet\n\n# create the dataset from the pandas dataframe\ndataset = TimeSeriesDataSet(\n    test_data,\n    group_ids=[\"group\"],\n    target=\"value\",\n    time_idx=\"time_idx\",\n    min_encoder_length=5,\n    max_encoder_length=5,\n    min_prediction_length=2,\n    max_prediction_length=2,\n    time_varying_unknown_reals=[\"value\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing custom logging in PyTorch Forecasting\nDESCRIPTION: This snippet shows how to implement custom logging functions for a PyTorch Forecasting model, including creating logs and handling epoch end events.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_forecasting.utils import detach\n\n\ndef create_log(self, x, y, out, batch_idx, **kwargs):\n    # log standard\n    log = super().create_log(x, y, out, batch_idx, **kwargs)\n    # calculate interpretations etc for latter logging\n    if self.log_interval > 0:\n        interpretation = self.interpret_output(\n            detach(out),\n            reduction=\"sum\",\n            attention_prediction_horizon=0,  # attention only for first prediction horizon\n        )\n        log[\"interpretation\"] = interpretation\n    return log\n\n\ndef on_epoch_end(self, outputs):\n    \"\"\"\n    Run at epoch end for training or validation\n    \"\"\"\n    if self.log_interval > 0:\n        self.log_interpretation(outputs)\n```\n\n----------------------------------------\n\nTITLE: Enhanced FullyConnectedModel Implementation\nDESCRIPTION: Extended version of the FullyConnectedModel with dataset validation and custom initialization.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass FullyConnectedModel(BaseModel):\n    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int, **kwargs):\n        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n        self.save_hyperparameters()\n        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n        super().__init__(**kwargs)\n        self.network = FullyConnectedModule(\n            input_size=self.hparams.input_size,\n            output_size=self.hparams.output_size,\n            hidden_size=self.hparams.hidden_size,\n            n_hidden_layers=self.hparams.n_hidden_layers,\n        )\n\n    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        # x is a batch generated based on the TimeSeriesDataSet\n        network_input = x[\"encoder_cont\"].squeeze(-1)\n        prediction = self.network(network_input).unsqueeze(-1)\n\n        # rescale predictions into target space\n        prediction = self.transform_output(prediction, target_scale=x[\"target_scale\"])\n\n        # We need to return a dictionary that at least contains the prediction.\n        # The parameter can be directly forwarded from the input.\n        # The conversion to a named tuple can be directly achieved with the `to_network_output` function.\n        return self.to_network_output(prediction=prediction)\n\n    @classmethod\n    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n        new_kwargs = {\n            \"output_size\": dataset.max_prediction_length,\n            \"input_size\": dataset.max_encoder_length,\n        }\n        new_kwargs.update(kwargs)  # use to pass real hyperparameters and override defaults set by dataset\n        # example for dataset validation\n        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n        assert (\n            len(dataset._time_varying_known_categoricals) == 0\n            and len(dataset._time_varying_known_reals) == 0\n            and len(dataset._time_varying_unknown_categoricals) == 0\n            and len(dataset._static_categoricals) == 0\n            and len(dataset._static_reals) == 0\n            and len(dataset._time_varying_unknown_reals) == 1\n            and dataset._time_varying_unknown_reals[0] == dataset.target\n        ), \"Only covariate should be the target in 'time_varying_unknown_reals'\"\n\n        return super().from_dataset(dataset, **new_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Accessing loss quantiles in PyTorch model\nDESCRIPTION: These snippets show how to access the quantiles used in the loss function of the PyTorch model.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nmodel.loss.quantiles\n```\n\nLANGUAGE: python\nCODE:\n```\nNormalDistributionLoss(quantiles=[0.2, 0.8]).quantiles\n```\n\n----------------------------------------\n\nTITLE: Checking Model Size in PyTorch Forecasting\nDESCRIPTION: Use the size() method to check the number of parameters in your model, which can help diagnose issues with model size and training performance.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/faq.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel.size()\n```\n\n----------------------------------------\n\nTITLE: Creating Multi-Target Test Data\nDESCRIPTION: Generates a test dataframe with multiple target columns for multi-target forecasting.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmulti_target_test_data = pd.DataFrame(\n    dict(\n        target1=np.random.rand(30),\n        target2=np.random.rand(30),\n        group=np.repeat(np.arange(3), 10),\n        time_idx=np.tile(np.arange(10), 3),\n    )\n)\nmulti_target_test_data\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Forecasting via pip\nDESCRIPTION: Command to install the PyTorch Forecasting package using pip package manager.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/getting-started.rst#2025-04-17_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install pytorch-forecasting\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Forecasting with pip\nDESCRIPTION: Command to install the PyTorch Forecasting package using pip package manager. This is the standard installation method for Python packages.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/index.rst#2025-04-17_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install pytorch-forecasting\n```\n\n----------------------------------------\n\nTITLE: Implementing end-of-training logging in PyTorch Forecasting\nDESCRIPTION: This snippet demonstrates how to implement logging at the end of model training, specifically for logging embeddings.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef on_fit_end(self):\n    \"\"\"\n    run at the end of training\n    \"\"\"\n    if self.log_interval > 0:\n        for name, emb in self.input_embeddings.items():\n            labels = self.hparams.embedding_labels[name]\n            self.logger.experiment.add_embedding(\n                emb.weight.data.cpu(), metadata=labels, tag=name, global_step=self.global_step\n            )\n```\n\n----------------------------------------\n\nTITLE: Accessing decoder lengths in PyTorch model\nDESCRIPTION: This snippet demonstrates how to access the decoder lengths from the input dictionary in a PyTorch model.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nx[\"decoder_lengths\"]\n```\n\n----------------------------------------\n\nTITLE: Testing Multi-Target Model Predictions\nDESCRIPTION: Demonstrates passing input data through the multi-target model and examining the output structure.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nout = model(x)\nout\n```\n\n----------------------------------------\n\nTITLE: Creating Classification Dataset with TimeSeriesDataSet\nDESCRIPTION: Sets up a TimeSeriesDataSet for classification tasks using a categorical target. The dataset uses NaNLabelEncoder to encode categorical target values.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# create the dataset from the pandas dataframe\nclassification_dataset = TimeSeriesDataSet(\n    classification_test_data,\n    group_ids=[\"group\"],\n    target=\"target\",  # SWITCHING to categorical target\n    time_idx=\"time_idx\",\n    min_encoder_length=5,\n    max_encoder_length=5,\n    min_prediction_length=2,\n    max_prediction_length=2,\n    time_varying_unknown_reals=[\"value\"],\n    target_normalizer=NaNLabelEncoder(),  # Use the NaNLabelEncoder to encode categorical target\n)\n\nx, y = next(iter(classification_dataset.to_dataloader(batch_size=4)))\ny[0]  # target values are encoded categories\n```\n\n----------------------------------------\n\nTITLE: Setting Log Interval for Learning Rate Finder in PyTorch Forecasting\nDESCRIPTION: Set the log_interval parameter to -1 in your model to avoid creating excessive plots and matplotlib warnings when running the learning rate finder.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/faq.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlog_interval=-1\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Forecasting via conda\nDESCRIPTION: Command to install PyTorch Forecasting using conda package manager, getting the package from conda-forge channel and PyTorch from the PyTorch channel.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/getting-started.rst#2025-04-17_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nconda install pytorch-forecasting pytorch>=1.7 -c pytorch -c conda-forge\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Forecasting with conda\nDESCRIPTION: Command to install PyTorch Forecasting using conda package manager. This method also ensures compatible versions of PyTorch (1.7 or higher) are installed, using both pytorch and conda-forge channels.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/index.rst#2025-04-17_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nconda install pytorch-forecasting pytorch>=1.7 -c pytorch -c conda-forge\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch on Windows\nDESCRIPTION: Command to install PyTorch on Windows systems using pip and the official PyTorch wheel repository.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/installation.rst#2025-04-17_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install torch -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n----------------------------------------\n\nTITLE: Testing Classification Model Prediction Shape\nDESCRIPTION: Demonstrates passing input data through the classification model and checking the output prediction shape.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# passing x through model\nmodel(x)[\"prediction\"].shape\n```\n\n----------------------------------------\n\nTITLE: Creating Classification Test Data\nDESCRIPTION: Generates sample data for classification tasks with categorical target values.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclassification_test_data = pd.DataFrame(\n    dict(\n        target=np.random.choice([\"A\", \"B\", \"C\"], size=30),  # CHANGING values to predict to a categorical\n        value=np.random.rand(30),  # INPUT values - see next section on covariates how to use categorical inputs\n        group=np.repeat(np.arange(3), 10),\n        time_idx=np.tile(np.arange(10), 3),\n    )\n)\nclassification_test_data\n```\n\n----------------------------------------\n\nTITLE: Installing via Conda\nDESCRIPTION: Command to install PyTorch Forecasting and PyTorch using conda from the pytorch and conda-forge channels.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/installation.rst#2025-04-17_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nconda install pytorch-forecasting pytorch>=2.0.0 -c pytorch -c conda-forge\n```\n\n----------------------------------------\n\nTITLE: Installing MQF2 loss extension\nDESCRIPTION: Command to install the optional multivariate quantile loss (MQF2) extension for PyTorch Forecasting.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/getting-started.rst#2025-04-17_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install pytorch-forecasting[mqf2]\n```\n\n----------------------------------------\n\nTITLE: Installing with MQF2 Loss Support\nDESCRIPTION: Command to install PyTorch Forecasting with multivariate quantile loss (MQF2) support.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/installation.rst#2025-04-17_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install pytorch-forecasting[mqf2]\n```\n\n----------------------------------------\n\nTITLE: Installing Latest Development Version\nDESCRIPTION: Command to install the latest development version directly from the GitHub repository.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/installation.rst#2025-04-17_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/sktime/pytorch-forecasting.git\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Forecasting with MQF2 loss support\nDESCRIPTION: Command to install PyTorch Forecasting with the multivariate quantile loss (MQF2) extra dependency. This is needed for applications requiring this specific loss function.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/index.rst#2025-04-17_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install pytorch-forecasting[mqf2]\n```\n\n----------------------------------------\n\nTITLE: Defining API Documentation Structure in reStructuredText\nDESCRIPTION: Sphinx documentation configuration that sets up the automatic API documentation generation. Uses autosummary directive to create documentation from docstrings, with a custom module template and recursive processing.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/api.rst#2025-04-17_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. currentmodule:: pytorch_forecasting\n\n.. autosummary::\n   :toctree: api\n   :template: custom-module-template.rst\n   :recursive:\n\n   data\n   models\n   metrics\n   utils\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Forecasting with CPU Support\nDESCRIPTION: Command to install PyTorch Forecasting with CPU support using pip and the PyTorch wheel repository.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/installation.rst#2025-04-17_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install pytorch-forecasting --extra-index-url https://download.pytorch.org/whl/cpu\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch on Windows via pip\nDESCRIPTION: Command to install PyTorch specifically for Windows users, using the official PyTorch wheel repository.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/getting-started.rst#2025-04-17_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install torch -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n----------------------------------------\n\nTITLE: Printing BaseModelWithCovariates Documentation in PyTorch Forecasting\nDESCRIPTION: This snippet imports the BaseModelWithCovariates class from pytorch_forecasting and prints its documentation.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_forecasting.models.base_model import BaseModelWithCovariates\n\nprint(BaseModelWithCovariates.__doc__)\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Lightning\nDESCRIPTION: Command to install the PyTorch Lightning library, which is a key dependency.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/installation.rst#2025-04-17_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install lightning\n```\n\n----------------------------------------\n\nTITLE: Cloning Repository for Development\nDESCRIPTION: Commands to clone the repository and configure remote settings for development.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/installation.rst#2025-04-17_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:<username>/sktime/pytorch-forecasting.git\ncd pytorch-forecasting\n```\n\n----------------------------------------\n\nTITLE: Version Numbers\nDESCRIPTION: Version numbers referenced in the changelog, showing progression from v0.10.1 to v1.3.0.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/CHANGELOG.md#2025-04-17_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nv1.3.0\nv1.2.0\nv1.1.1\nv1.1.0\nv1.0.0\nv0.10.3\nv0.10.2\nv0.10.1\n```\n\n----------------------------------------\n\nTITLE: Setting Up Warning Filters in Python\nDESCRIPTION: Configures Python to ignore warnings for cleaner output.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials/building.ipynb#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Git Remote\nDESCRIPTION: Commands to set up and verify remote repository configuration for development.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/installation.rst#2025-04-17_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ngit remote -v\ngit remote add upstream https://github.com/sktime/pytorch-forecasting.git\n```\n\n----------------------------------------\n\nTITLE: Jinja2 Template for PyTorch Forecasting Module Documentation\nDESCRIPTION: A Sphinx documentation template that generates structured documentation for Python modules. It divides content into sections for module attributes, functions, classes, exceptions, and submodules, using conditional blocks to display each section only if content exists.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/_templates/custom-module-template.rst#2025-04-17_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname.split(\".\")[-1] | escape | underline}}\n\n.. automodule:: {{ fullname }}\n\n   {% block attributes %}\n   {% if attributes %}\n   .. rubric:: Module Attributes\n\n   .. autosummary::\n      :toctree:\n   {% for item in attributes %}\n      {{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block functions %}\n   {% if functions %}\n   .. rubric:: {{ _('Functions') }}\n\n   .. autosummary::\n      :toctree:\n      :template: custom-base-template.rst\n   {% for item in functions %}\n      {{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block classes %}\n   {% if classes %}\n   .. rubric:: {{ _('Classes') }}\n\n   .. autosummary::\n      :toctree:\n      :template: custom-class-template.rst\n   {% for item in classes %}\n      {{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block exceptions %}\n   {% if exceptions %}\n   .. rubric:: {{ _('Exceptions') }}\n\n   .. autosummary::\n      :toctree:\n   {% for item in exceptions %}\n      {{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n{% block modules %}\n{% if all_modules %}\n.. rubric:: Modules\n\n.. autosummary::\n   :toctree:\n   :template: custom-module-template.rst\n   :recursive:\n{% for item in all_modules %}\n   {{ item }}\n{%- endfor %}\n{% endif %}\n{% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Module Documentation for PyTorch Forecasting\nDESCRIPTION: Sphinx documentation configuration that sets up autodoc module documentation for PyTorch Forecasting models. Uses a custom module template and enables recursive documentation building.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/models.rst#2025-04-17_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: pytorch_forecasting\n\n.. moduleautosummary::\n   :toctree: api/\n   :template: custom-module-template.rst\n   :recursive:\n\n   pytorch_forecasting.models\n```\n\n----------------------------------------\n\nTITLE: RST Tutorial Navigation Structure\nDESCRIPTION: ReStructuredText markup defining the tutorial navigation structure with a table of contents tree and links to individual tutorial pages.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/tutorials.rst#2025-04-17_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _tutorials:\n\nThe following tutorials can be also found as `notebooks on GitHub <https://github.com/sktime/pytorch-forecasting/tree/main/docs/source/tutorials>`_.\n\n.. toctree::\n   :titlesonly:\n   :maxdepth: 2\n\n   tutorials/stallion\n   tutorials/ar\n   tutorials/building\n   tutorials/deepar\n   tutorials/nhits\n```\n\n----------------------------------------\n\nTITLE: Creating Documentation Template with Jinja2 for PyTorch Forecasting\nDESCRIPTION: This template uses Jinja2 to generate documentation for objects in the PyTorch Forecasting library. It creates a header with the object name, sets the current module context, and then auto-documents the specified object.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/_templates/custom-base-template.rst#2025-04-17_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname.split(\".\")[-1] | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. auto{{ objtype }}:: {{ objname }}\n```\n\n----------------------------------------\n\nTITLE: Installing from Specific Branch\nDESCRIPTION: Command to install PyTorch Forecasting from a specific branch in the GitHub repository.\nSOURCE: https://github.com/sktime/pytorch-forecasting/blob/main/docs/source/installation.rst#2025-04-17_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/sktime/pytorch-forecasting.git@<branch_name>\n```"
  }
]